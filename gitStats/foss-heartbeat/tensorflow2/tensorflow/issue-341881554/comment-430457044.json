{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430457044", "html_url": "https://github.com/tensorflow/tensorflow/issues/20878#issuecomment-430457044", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20878", "id": 430457044, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDQ1NzA0NA==", "user": {"login": "kehuantiantang", "id": 5328587, "node_id": "MDQ6VXNlcjUzMjg1ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5328587?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kehuantiantang", "html_url": "https://github.com/kehuantiantang", "followers_url": "https://api.github.com/users/kehuantiantang/followers", "following_url": "https://api.github.com/users/kehuantiantang/following{/other_user}", "gists_url": "https://api.github.com/users/kehuantiantang/gists{/gist_id}", "starred_url": "https://api.github.com/users/kehuantiantang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kehuantiantang/subscriptions", "organizations_url": "https://api.github.com/users/kehuantiantang/orgs", "repos_url": "https://api.github.com/users/kehuantiantang/repos", "events_url": "https://api.github.com/users/kehuantiantang/events{/privacy}", "received_events_url": "https://api.github.com/users/kehuantiantang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T01:28:06Z", "updated_at": "2018-10-17T01:28:06Z", "author_association": "NONE", "body_html": "<blockquote>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=35597616\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dlml\">@dlml</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5328587\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kehuantiantang\">@kehuantiantang</a>: Please file separate issues with enough detail to replicate your issue.</p>\n</blockquote>\n<ul>\n<li>First, I retrain a network with Keras, the code seems like below, which is a simple keras code and can save weight to .h file,  then using function save_graph_to_file() to get .pb file.</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> keras.models <span class=\"pl-k\">import</span> Sequential, Model\n<span class=\"pl-k\">from</span> keras.models <span class=\"pl-k\">import</span> load_model\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> keras <span class=\"pl-k\">import</span> backend <span class=\"pl-k\">as</span> K\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> Conv2D, MaxPooling2D, Input\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> Activation, Dropout, Flatten, Dense\n<span class=\"pl-k\">from</span> keras.preprocessing.image <span class=\"pl-k\">import</span> ImageDataGenerator, array_to_img, img_to_array, load_img\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">from</span> keras <span class=\"pl-k\">import</span> optimizers\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CUDA_DEVICE_ORDER<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>PCI_BUS_ID<span class=\"pl-pds\">\"</span></span>\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>0<span class=\"pl-pds\">'</span></span>\n\ntrain_path<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos<span class=\"pl-pds\">'</span></span>\nval_path<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos<span class=\"pl-pds\">'</span></span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">200</span>\n\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">150</span>, <span class=\"pl-c1\">150</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input<span class=\"pl-pds\">'</span></span>)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(<span class=\"pl-c1\">input</span>)\nx <span class=\"pl-k\">=</span> MaxPooling2D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))(x)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> MaxPooling2D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))(x)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">64</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">64</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">64</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> Conv2D(<span class=\"pl-c1\">64</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> MaxPooling2D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))(x)\nx <span class=\"pl-k\">=</span> Flatten()(x)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> x = Dense(64, activation='relu')(x)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> x = Dense(2, activation='sigmoid', name = 'output')(x)</span>\nx <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>softmax<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>output<span class=\"pl-pds\">'</span></span>)(x)\nmodel <span class=\"pl-k\">=</span> Model(<span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>, <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span> x)\n\nsgd <span class=\"pl-k\">=</span> optimizers.SGD(<span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.000005</span>, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-6</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">nesterov</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>categorical_crossentropy<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>sgd,\n              <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])\n\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> this is the augmentation configuration we will use for training</span>\ntrain_datagen <span class=\"pl-k\">=</span> ImageDataGenerator(\n        <span class=\"pl-v\">shear_range</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.2</span>,\n        <span class=\"pl-v\">zoom_range</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.2</span>,\n        <span class=\"pl-v\">horizontal_flip</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> this is the augmentation configuration we will use for testing:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> only rescaling</span>\ntest_datagen <span class=\"pl-k\">=</span> ImageDataGenerator()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> this is a generator that will read pictures found in</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> subfolers of 'data/train', and indefinitely generate</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> batches of augmented image data</span>\ntrain_generator <span class=\"pl-k\">=</span> train_datagen.flow_from_directory(\n        train_path,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> this is the target directory</span>\n        <span class=\"pl-v\">target_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">150</span>, <span class=\"pl-c1\">150</span>),  <span class=\"pl-c\"><span class=\"pl-c\">#</span> all images will be resized to 150x150</span>\n        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n        <span class=\"pl-v\">class_mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>categorical<span class=\"pl-pds\">'</span></span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> since we use binary_crossentropy loss, we need binary labels</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> this is a similar generator, for validation data</span>\nvalidation_generator <span class=\"pl-k\">=</span> test_datagen.flow_from_directory(\n        val_path,\n        <span class=\"pl-v\">target_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">150</span>, <span class=\"pl-c1\">150</span>),\n        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n        <span class=\"pl-v\">class_mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>categorical<span class=\"pl-pds\">'</span></span>)\n\nmodel.fit_generator(\n        train_generator,\n        <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1340</span> <span class=\"pl-k\">//</span> batch_size,\n        <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span>epochs,\n        <span class=\"pl-v\">validation_data</span><span class=\"pl-k\">=</span>validation_generator,\n        <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n        <span class=\"pl-v\">validation_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span> <span class=\"pl-k\">//</span> batch_size)\nmodel.save(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>./weight/flower5.h5<span class=\"pl-pds\">'</span></span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> always save your weights after training or during training</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> convert</span>\noutput_names <span class=\"pl-k\">=</span> [node.op.name <span class=\"pl-k\">for</span> node <span class=\"pl-k\">in</span> model.outputs]\n\nexport_dir <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>./weight/<span class=\"pl-pds\">'</span></span>\nsess <span class=\"pl-k\">=</span> K.get_session()\nsave_graph_to_file(sess,  export_dir <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>flower5.pb<span class=\"pl-pds\">\"</span></span>, output_names)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">save_graph_to_file</span>(<span class=\"pl-smi\">sess</span>,  <span class=\"pl-smi\">graph_file_name</span>, <span class=\"pl-smi\">output_names</span>):\n    output_graph_def <span class=\"pl-k\">=</span> graph_util.convert_variables_to_constants(\n      sess,  sess.graph.as_graph_def(),  output_names)\n    <span class=\"pl-k\">with</span> gfile.FastGFile(graph_file_name, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>wb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n        f.write(output_graph_def.SerializeToString())</pre></div>\n<ul>\n<li>Then  covert .pb file to .tlite file like this:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre>IMAGE_SIZE=150\nFILE=flower5\ntoco \\\n  --graph_def_file=weight/<span class=\"pl-smi\">${FILE}</span>.pb \\\n  --output_file=weight/<span class=\"pl-smi\">${FILE}</span>.lite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --input_shape=1,<span class=\"pl-smi\">${IMAGE_SIZE}</span>,<span class=\"pl-smi\">${IMAGE_SIZE}</span>,3 \\\n  --input_array=input_1 \\\n  --output_array=output_1/Softmax \\\n  --inference_type=FLOAT \\\n  --input_data_type=FLOAT</pre></div>\n<ul>\n<li>I order to make sure the convert is correct when use toco, I used tflite file to predict some image and get predict result like that, there are five classes and the label is \"daisy dandelion roses sunflowers tulips\", which get correct softmax result, this means if I use a sunflower image and the model can get highest softmax result in sunflower.</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">PIL</span> <span class=\"pl-k\">import</span> Image\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> filepath='./weight/flower_gpu.tflite'</span>\nfilepath<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./weight/flower5.lite<span class=\"pl-pds\">'</span></span>\nval_path <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/home/sober/ftp_bk/Dataset/Tmp_Train/flo/<span class=\"pl-pds\">'</span></span>\nshape<span class=\"pl-k\">=</span>(<span class=\"pl-c1\">150</span>, <span class=\"pl-c1\">150</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Load TFLite model and allocate tensors.</span>\ninterpreter <span class=\"pl-k\">=</span> tf.contrib.lite.Interpreter(<span class=\"pl-v\">model_path</span><span class=\"pl-k\">=</span>filepath)\ninterpreter.allocate_tensors()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Get input and output tensors.</span>\ninput_details <span class=\"pl-k\">=</span> interpreter.get_input_details()\noutput_details <span class=\"pl-k\">=</span> interpreter.get_output_details()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Test model on random input data.</span>\ninput_shape <span class=\"pl-k\">=</span> input_details[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shape<span class=\"pl-pds\">'</span></span>]\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> change the following line to feed into your own data.</span>\n\n<span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> os.listdir(val_path):\n    dirpath <span class=\"pl-k\">=</span> val_path<span class=\"pl-k\">+</span> d <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> os.listdir(dirpath):\n        <span class=\"pl-v\">file</span> <span class=\"pl-k\">=</span> dirpath <span class=\"pl-k\">+</span> f\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-v\">file</span>)\n        im <span class=\"pl-k\">=</span> Image.open(<span class=\"pl-v\">file</span>)\n        im <span class=\"pl-k\">=</span> im.resize(shape, Image.<span class=\"pl-c1\">ANTIALIAS</span>)\n        im <span class=\"pl-k\">=</span> np.asarray(im)\n        im <span class=\"pl-k\">=</span> im[np.newaxis, :]\n        im <span class=\"pl-k\">=</span> np.asarray(im)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>         print(np.array2string(model.predict(np.asarray(im)), formatter={'float_kind':lambda x: \"%.2f\" % x}))</span>\n        input_data <span class=\"pl-k\">=</span> np.array(im, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n        interpreter.set_tensor(input_details[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>index<span class=\"pl-pds\">'</span></span>], input_data)\n\n        interpreter.invoke()\n        output_data <span class=\"pl-k\">=</span> interpreter.get_tensor(output_details[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>index<span class=\"pl-pds\">'</span></span>])\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>         output_data.sort()</span>\n        <span class=\"pl-c1\">print</span>(np.array2string(output_data, <span class=\"pl-v\">formatter</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float_kind<span class=\"pl-pds\">'</span></span>:<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%.2f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> x}))\n    \n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>daisy dandelion roses sunflowers tulips<span class=\"pl-pds\">\"</span></span></pre></div>\n<ul>\n<li>Finally, I put label and tflite file to android, and used android tensorflow demo,  I don't change the code and just replace the orgin graph.lite and label.txt file to my .lite .txt file, and test it. but the result shows in demo is just like \"0.2, 0.2, 0.2\" which change a little even though I took photos for sunflower.</li>\n</ul>\n<blockquote>\n<p><a href=\"https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2\" rel=\"nofollow\">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2</a></p>\n</blockquote>\n<ul>\n<li>I don't know how to fix it, because the model predict in computer is right but the result in andorid is<br>\nabnormal and not change when predict different object.</li>\n</ul>\n</blockquote>\n<p>the problem fixed by the input image and android input image is different, where image in training has not subtract mean and divide by std.</p>", "body_text": "@dlml and @kehuantiantang: Please file separate issues with enough detail to replicate your issue.\n\n\nFirst, I retrain a network with Keras, the code seems like below, which is a simple keras code and can save weight to .h file,  then using function save_graph_to_file() to get .pb file.\n\nfrom keras.models import Sequential, Model\nfrom keras.models import load_model\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.layers import Conv2D, MaxPooling2D, Input\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nimport os\nfrom keras import optimizers\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\ntrain_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\nval_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\nbatch_size = 16\nepochs = 200\n\ninput = Input(shape = (150, 150, 3), name = 'input')\nx = Conv2D(32, (3, 3), activation='relu')(input)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Conv2D(32, (3, 3), activation='relu')(x)\nx = Conv2D(32, (3, 3), activation='relu')(x)\nx = Conv2D(32, (3, 3), activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\n# x = Dense(64, activation='relu')(x)\n# x = Dense(2, activation='sigmoid', name = 'output')(x)\nx = Dense(5, activation='softmax', name = 'output')(x)\nmodel = Model(inputs = input, outputs= x)\n\nsgd = optimizers.SGD(lr=0.000005, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\n\n\n# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator()\n\n# this is a generator that will read pictures found in\n# subfolers of 'data/train', and indefinitely generate\n# batches of augmented image data\ntrain_generator = train_datagen.flow_from_directory(\n        train_path,  # this is the target directory\n        target_size=(150, 150),  # all images will be resized to 150x150\n        batch_size=batch_size,\n        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n\n# this is a similar generator, for validation data\nvalidation_generator = test_datagen.flow_from_directory(\n        val_path,\n        target_size=(150, 150),\n        batch_size=batch_size,\n        class_mode='categorical')\n\nmodel.fit_generator(\n        train_generator,\n        steps_per_epoch=1340 // batch_size,\n        epochs=epochs,\n        validation_data=validation_generator,\n        shuffle=True,\n        validation_steps=64 // batch_size)\nmodel.save('./weight/flower5.h5')  # always save your weights after training or during training\n\n# convert\noutput_names = [node.op.name for node in model.outputs]\n\nexport_dir = './weight/'\nsess = K.get_session()\nsave_graph_to_file(sess,  export_dir + \"flower5.pb\", output_names)\n\ndef save_graph_to_file(sess,  graph_file_name, output_names):\n    output_graph_def = graph_util.convert_variables_to_constants(\n      sess,  sess.graph.as_graph_def(),  output_names)\n    with gfile.FastGFile(graph_file_name, 'wb') as f:\n        f.write(output_graph_def.SerializeToString())\n\nThen  covert .pb file to .tlite file like this:\n\nIMAGE_SIZE=150\nFILE=flower5\ntoco \\\n  --graph_def_file=weight/${FILE}.pb \\\n  --output_file=weight/${FILE}.lite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\n  --input_array=input_1 \\\n  --output_array=output_1/Softmax \\\n  --inference_type=FLOAT \\\n  --input_data_type=FLOAT\n\nI order to make sure the convert is correct when use toco, I used tflite file to predict some image and get predict result like that, there are five classes and the label is \"daisy dandelion roses sunflowers tulips\", which get correct softmax result, this means if I use a sunflower image and the model can get highest softmax result in sunflower.\n\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nimport os\n# filepath='./weight/flower_gpu.tflite'\nfilepath='./weight/flower5.lite'\nval_path = '/home/sober/ftp_bk/Dataset/Tmp_Train/flo/'\nshape=(150, 150)\n# Load TFLite model and allocate tensors.\ninterpreter = tf.contrib.lite.Interpreter(model_path=filepath)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Test model on random input data.\ninput_shape = input_details[0]['shape']\n# change the following line to feed into your own data.\n\nfor d in os.listdir(val_path):\n    dirpath = val_path+ d + '/'\n    for f in os.listdir(dirpath):\n        file = dirpath + f\n        print(file)\n        im = Image.open(file)\n        im = im.resize(shape, Image.ANTIALIAS)\n        im = np.asarray(im)\n        im = im[np.newaxis, :]\n        im = np.asarray(im)\n#         print(np.array2string(model.predict(np.asarray(im)), formatter={'float_kind':lambda x: \"%.2f\" % x}))\n        input_data = np.array(im, dtype=np.float32)\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        interpreter.invoke()\n        output_data = interpreter.get_tensor(output_details[0]['index'])\n#         output_data.sort()\n        print(np.array2string(output_data, formatter={'float_kind':lambda x: \"%.2f\" % x}))\n    \n\"daisy dandelion roses sunflowers tulips\"\n\nFinally, I put label and tflite file to android, and used android tensorflow demo,  I don't change the code and just replace the orgin graph.lite and label.txt file to my .lite .txt file, and test it. but the result shows in demo is just like \"0.2, 0.2, 0.2\" which change a little even though I took photos for sunflower.\n\n\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2\n\n\nI don't know how to fix it, because the model predict in computer is right but the result in andorid is\nabnormal and not change when predict different object.\n\n\nthe problem fixed by the input image and android input image is different, where image in training has not subtract mean and divide by std.", "body": "> > @dlml and @kehuantiantang: Please file separate issues with enough detail to replicate your issue.\r\n> \r\n> * First, I retrain a network with Keras, the code seems like below, which is a simple keras code and can save weight to .h file,  then using function save_graph_to_file() to get .pb file.\r\n> \r\n> ```python\r\n> from keras.models import Sequential, Model\r\n> from keras.models import load_model\r\n> import tensorflow as tf\r\n> from keras import backend as K\r\n> from keras.layers import Conv2D, MaxPooling2D, Input\r\n> from keras.layers import Activation, Dropout, Flatten, Dense\r\n> from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n> import os\r\n> from keras import optimizers\r\n> os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\n> os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\r\n> \r\n> train_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\r\n> val_path='/home/sober/code/jupyter/ftp_code/AndroidTensorflow/tensorflow-for-poets-2/tf_files/flower_photos'\r\n> batch_size = 16\r\n> epochs = 200\r\n> \r\n> input = Input(shape = (150, 150, 3), name = 'input')\r\n> x = Conv2D(32, (3, 3), activation='relu')(input)\r\n> x = MaxPooling2D(pool_size=(2, 2))(x)\r\n> x = Conv2D(32, (3, 3), activation='relu')(x)\r\n> x = Conv2D(32, (3, 3), activation='relu')(x)\r\n> x = Conv2D(32, (3, 3), activation='relu')(x)\r\n> x = MaxPooling2D(pool_size=(2, 2))(x)\r\n> x = Conv2D(64, (3, 3), activation='relu')(x)\r\n> x = Conv2D(64, (3, 3), activation='relu')(x)\r\n> x = Conv2D(64, (3, 3), activation='relu')(x)\r\n> x = Conv2D(64, (3, 3), activation='relu')(x)\r\n> x = MaxPooling2D(pool_size=(2, 2))(x)\r\n> x = Flatten()(x)\r\n> # x = Dense(64, activation='relu')(x)\r\n> # x = Dense(2, activation='sigmoid', name = 'output')(x)\r\n> x = Dense(5, activation='softmax', name = 'output')(x)\r\n> model = Model(inputs = input, outputs= x)\r\n> \r\n> sgd = optimizers.SGD(lr=0.000005, decay=1e-6, momentum=0.9, nesterov=True)\r\n> model.compile(loss='categorical_crossentropy',\r\n>               optimizer=sgd,\r\n>               metrics=['accuracy'])\r\n> \r\n> \r\n> \r\n> # this is the augmentation configuration we will use for training\r\n> train_datagen = ImageDataGenerator(\r\n>         shear_range=0.2,\r\n>         zoom_range=0.2,\r\n>         horizontal_flip=True)\r\n> \r\n> # this is the augmentation configuration we will use for testing:\r\n> # only rescaling\r\n> test_datagen = ImageDataGenerator()\r\n> \r\n> # this is a generator that will read pictures found in\r\n> # subfolers of 'data/train', and indefinitely generate\r\n> # batches of augmented image data\r\n> train_generator = train_datagen.flow_from_directory(\r\n>         train_path,  # this is the target directory\r\n>         target_size=(150, 150),  # all images will be resized to 150x150\r\n>         batch_size=batch_size,\r\n>         class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\r\n> \r\n> # this is a similar generator, for validation data\r\n> validation_generator = test_datagen.flow_from_directory(\r\n>         val_path,\r\n>         target_size=(150, 150),\r\n>         batch_size=batch_size,\r\n>         class_mode='categorical')\r\n> \r\n> model.fit_generator(\r\n>         train_generator,\r\n>         steps_per_epoch=1340 // batch_size,\r\n>         epochs=epochs,\r\n>         validation_data=validation_generator,\r\n>         shuffle=True,\r\n>         validation_steps=64 // batch_size)\r\n> model.save('./weight/flower5.h5')  # always save your weights after training or during training\r\n> \r\n> # convert\r\n> output_names = [node.op.name for node in model.outputs]\r\n> \r\n> export_dir = './weight/'\r\n> sess = K.get_session()\r\n> save_graph_to_file(sess,  export_dir + \"flower5.pb\", output_names)\r\n> \r\n> def save_graph_to_file(sess,  graph_file_name, output_names):\r\n>     output_graph_def = graph_util.convert_variables_to_constants(\r\n>       sess,  sess.graph.as_graph_def(),  output_names)\r\n>     with gfile.FastGFile(graph_file_name, 'wb') as f:\r\n>         f.write(output_graph_def.SerializeToString())\r\n> ```\r\n> \r\n> * Then  covert .pb file to .tlite file like this:\r\n> \r\n> ```shell\r\n> IMAGE_SIZE=150\r\n> FILE=flower5\r\n> toco \\\r\n>   --graph_def_file=weight/${FILE}.pb \\\r\n>   --output_file=weight/${FILE}.lite \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF \\\r\n>   --output_format=TFLITE \\\r\n>   --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n>   --input_array=input_1 \\\r\n>   --output_array=output_1/Softmax \\\r\n>   --inference_type=FLOAT \\\r\n>   --input_data_type=FLOAT\r\n> ```\r\n> \r\n> * I order to make sure the convert is correct when use toco, I used tflite file to predict some image and get predict result like that, there are five classes and the label is \"daisy dandelion roses sunflowers tulips\", which get correct softmax result, this means if I use a sunflower image and the model can get highest softmax result in sunflower.\r\n> \r\n> ```python\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> from PIL import Image\r\n> import os\r\n> # filepath='./weight/flower_gpu.tflite'\r\n> filepath='./weight/flower5.lite'\r\n> val_path = '/home/sober/ftp_bk/Dataset/Tmp_Train/flo/'\r\n> shape=(150, 150)\r\n> # Load TFLite model and allocate tensors.\r\n> interpreter = tf.contrib.lite.Interpreter(model_path=filepath)\r\n> interpreter.allocate_tensors()\r\n> \r\n> # Get input and output tensors.\r\n> input_details = interpreter.get_input_details()\r\n> output_details = interpreter.get_output_details()\r\n> \r\n> # Test model on random input data.\r\n> input_shape = input_details[0]['shape']\r\n> # change the following line to feed into your own data.\r\n> \r\n> for d in os.listdir(val_path):\r\n>     dirpath = val_path+ d + '/'\r\n>     for f in os.listdir(dirpath):\r\n>         file = dirpath + f\r\n>         print(file)\r\n>         im = Image.open(file)\r\n>         im = im.resize(shape, Image.ANTIALIAS)\r\n>         im = np.asarray(im)\r\n>         im = im[np.newaxis, :]\r\n>         im = np.asarray(im)\r\n> #         print(np.array2string(model.predict(np.asarray(im)), formatter={'float_kind':lambda x: \"%.2f\" % x}))\r\n>         input_data = np.array(im, dtype=np.float32)\r\n>         interpreter.set_tensor(input_details[0]['index'], input_data)\r\n> \r\n>         interpreter.invoke()\r\n>         output_data = interpreter.get_tensor(output_details[0]['index'])\r\n> #         output_data.sort()\r\n>         print(np.array2string(output_data, formatter={'float_kind':lambda x: \"%.2f\" % x}))\r\n>     \r\n> \"daisy dandelion roses sunflowers tulips\"\r\n> ```\r\n> \r\n> * Finally, I put label and tflite file to android, and used android tensorflow demo,  I don't change the code and just replace the orgin graph.lite and label.txt file to my .lite .txt file, and test it. but the result shows in demo is just like \"0.2, 0.2, 0.2\" which change a little even though I took photos for sunflower.\r\n> \r\n> > https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2\r\n> \r\n> * I don't know how to fix it, because the model predict in computer is right but the result in andorid is\r\n>   abnormal and not change when predict different object.\r\n\r\nthe problem fixed by the input image and android input image is different, where image in training has not subtract mean and divide by std."}