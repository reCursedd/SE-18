{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18357", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18357/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18357/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18357/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18357", "id": 312667396, "node_id": "MDU6SXNzdWUzMTI2NjczOTY=", "number": 18357, "title": "Bug: GPU resources not released appropriately when graph is reset & session is closed", "user": {"login": "rishabhmalhotra", "id": 15898956, "node_id": "MDQ6VXNlcjE1ODk4OTU2", "avatar_url": "https://avatars3.githubusercontent.com/u/15898956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rishabhmalhotra", "html_url": "https://github.com/rishabhmalhotra", "followers_url": "https://api.github.com/users/rishabhmalhotra/followers", "following_url": "https://api.github.com/users/rishabhmalhotra/following{/other_user}", "gists_url": "https://api.github.com/users/rishabhmalhotra/gists{/gist_id}", "starred_url": "https://api.github.com/users/rishabhmalhotra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rishabhmalhotra/subscriptions", "organizations_url": "https://api.github.com/users/rishabhmalhotra/orgs", "repos_url": "https://api.github.com/users/rishabhmalhotra/repos", "events_url": "https://api.github.com/users/rishabhmalhotra/events{/privacy}", "received_events_url": "https://api.github.com/users/rishabhmalhotra/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-04-09T20:03:49Z", "updated_at": "2018-05-17T00:39:57Z", "closed_at": "2018-05-17T00:39:56Z", "author_association": "NONE", "body_html": "<p>Have I written custom code: Forked network arch for FasterRCNN from <code>https://github.com/endernewton/tf-faster-rcnn</code><br>\nOS Platform and Distribution: Ubuntu 16.04.4, x86_64 GNU/Linux<br>\nTensorFlow installed from:<br>\nBazel version: 0.5.2<br>\nCUDA/cuDNN version: CUDA V8.0.61, release 8.0<br>\nGPU model and memory: <code>Nvidia Tesla K80</code>, 12 GB memory<br>\nExact procedure to reproduce: Load <code>graph</code> and <code>net</code> in GPU memory, use <code>tf.reset_default_graph()</code> followed by <code>sess.close()</code>, GPU memory not freed as seen through <code>nvidia-smi</code></p>\n<hr>\n<p>Possible duplicate but re-opening here since it doesn't appear to have been resolved &amp; there's no way to re-open the previously filed ones:</p>\n<p>Versions Used/Tried:</p>\n<ul>\n<li>System: aws EC2 (ubuntu) (with p2.xlarge elastic GPU instance)</li>\n<li>GPU: <code>NVIDIA Tesla K80</code></li>\n<li>Tensorflow versions: 1.3.0, 1.5.5 (tried on both)</li>\n<li>CUDA Version: 8.0, 9.0 (tried on both)</li>\n</ul>\n<p>Calling <code>tf.reset_graph()</code> and <code>sess.close()</code> doesn't free GPU as seen using <code>nvidia-smi</code>.<br>\nIt might be possible that nvidia-smi doesn't update but in that case, there wouldn't be resource errors on subsequent code trying to run on the GPU as seen here:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/15898956/38519614-6ad30aec-3c0e-11e8-895e-b9b2d20dab17.png\"><img src=\"https://user-images.githubusercontent.com/15898956/38519614-6ad30aec-3c0e-11e8-895e-b9b2d20dab17.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>What i tried after this was to use a subprocess within my script to kill the previous processes occupying resources on the GPU as seen here:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/15898956/38519591-55817534-3c0e-11e8-8bae-51d91e20b98e.png\"><img src=\"https://user-images.githubusercontent.com/15898956/38519591-55817534-3c0e-11e8-8bae-51d91e20b98e.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nAlthough this works, it seems like incredibly bad practice &amp; I shouldn't be doing this.</p>\n<p>What I tried then was to use the <code>numba</code> host API for interacting with the GPU to shut down processes i.e. clear <code>current_context</code> and then clear deallocations from the GPU like this:<br>\n<code>from numba import cuda current_context = get_context(devnum=0) current_context.reset() cuda.current_context().deallocations.clear()</code></p>\n<p>When this didn't work out, as a last resort, i tried to use this:<br>\n<code>from numba import cuda cuda.gpus[0].numba.cuda.cudadrv.devices.reset()</code><br>\nwhich works but results in a substantial memory leak everytime it runs which implies that after running my code a few times, the leak accumulated to a large enough value to again give me  resource errors.</p>\n<hr>\n<p>Context: I am trying to deploy a deep learning model using a flask API. Since this is a pipeline with multiple computation graphs, I cannot afford to keep all of those in memory so i need to do something like this:</p>\n<ol>\n<li>Upload data &amp; store it</li>\n<li>Build Graph</li>\n<li>Run Inference on stored data</li>\n<li>Remove graph from memory</li>\n<li>Build new graph</li>\n<li>Run Inference on stored data again<br>\n.<br>\n.<br>\n....and so on</li>\n</ol>\n<hr>\n<p>What I suspect might be going on under the hood:<br>\nIt's possible that <code>tf.reset_graph()</code> frees memory but doesn't remove the actual process holding onto that chunk of memory in the GPU in which case it makes sense for <code>nvidia-smi</code> to still show me the PID occupying memory in the GPU. But shouldn't <code>tf.reset_graph()</code> followed by <code>sess.close()</code> be freeing the GPU entirely?</p>\n<hr>\n<p>Any help on the issue will be appreciated.</p>", "body_text": "Have I written custom code: Forked network arch for FasterRCNN from https://github.com/endernewton/tf-faster-rcnn\nOS Platform and Distribution: Ubuntu 16.04.4, x86_64 GNU/Linux\nTensorFlow installed from:\nBazel version: 0.5.2\nCUDA/cuDNN version: CUDA V8.0.61, release 8.0\nGPU model and memory: Nvidia Tesla K80, 12 GB memory\nExact procedure to reproduce: Load graph and net in GPU memory, use tf.reset_default_graph() followed by sess.close(), GPU memory not freed as seen through nvidia-smi\n\nPossible duplicate but re-opening here since it doesn't appear to have been resolved & there's no way to re-open the previously filed ones:\nVersions Used/Tried:\n\nSystem: aws EC2 (ubuntu) (with p2.xlarge elastic GPU instance)\nGPU: NVIDIA Tesla K80\nTensorflow versions: 1.3.0, 1.5.5 (tried on both)\nCUDA Version: 8.0, 9.0 (tried on both)\n\nCalling tf.reset_graph() and sess.close() doesn't free GPU as seen using nvidia-smi.\nIt might be possible that nvidia-smi doesn't update but in that case, there wouldn't be resource errors on subsequent code trying to run on the GPU as seen here:\n\nWhat i tried after this was to use a subprocess within my script to kill the previous processes occupying resources on the GPU as seen here:\n\nAlthough this works, it seems like incredibly bad practice & I shouldn't be doing this.\nWhat I tried then was to use the numba host API for interacting with the GPU to shut down processes i.e. clear current_context and then clear deallocations from the GPU like this:\nfrom numba import cuda current_context = get_context(devnum=0) current_context.reset() cuda.current_context().deallocations.clear()\nWhen this didn't work out, as a last resort, i tried to use this:\nfrom numba import cuda cuda.gpus[0].numba.cuda.cudadrv.devices.reset()\nwhich works but results in a substantial memory leak everytime it runs which implies that after running my code a few times, the leak accumulated to a large enough value to again give me  resource errors.\n\nContext: I am trying to deploy a deep learning model using a flask API. Since this is a pipeline with multiple computation graphs, I cannot afford to keep all of those in memory so i need to do something like this:\n\nUpload data & store it\nBuild Graph\nRun Inference on stored data\nRemove graph from memory\nBuild new graph\nRun Inference on stored data again\n.\n.\n....and so on\n\n\nWhat I suspect might be going on under the hood:\nIt's possible that tf.reset_graph() frees memory but doesn't remove the actual process holding onto that chunk of memory in the GPU in which case it makes sense for nvidia-smi to still show me the PID occupying memory in the GPU. But shouldn't tf.reset_graph() followed by sess.close() be freeing the GPU entirely?\n\nAny help on the issue will be appreciated.", "body": "Have I written custom code: Forked network arch for FasterRCNN from `https://github.com/endernewton/tf-faster-rcnn`\r\nOS Platform and Distribution: Ubuntu 16.04.4, x86_64 GNU/Linux\r\nTensorFlow installed from: \r\nBazel version: 0.5.2\r\nCUDA/cuDNN version: CUDA V8.0.61, release 8.0\r\nGPU model and memory: `Nvidia Tesla K80`, 12 GB memory\r\nExact procedure to reproduce: Load `graph` and `net` in GPU memory, use `tf.reset_default_graph()` followed by `sess.close()`, GPU memory not freed as seen through `nvidia-smi`\r\n______________________________________________________________________________________________\r\nPossible duplicate but re-opening here since it doesn't appear to have been resolved & there's no way to re-open the previously filed ones:\r\n\r\nVersions Used/Tried:\r\n - System: aws EC2 (ubuntu) (with p2.xlarge elastic GPU instance)\r\n - GPU: `NVIDIA Tesla K80`\r\n - Tensorflow versions: 1.3.0, 1.5.5 (tried on both)\r\n - CUDA Version: 8.0, 9.0 (tried on both)\r\n\r\nCalling `tf.reset_graph()` and `sess.close()` doesn't free GPU as seen using `nvidia-smi`. \r\nIt might be possible that nvidia-smi doesn't update but in that case, there wouldn't be resource errors on subsequent code trying to run on the GPU as seen here:\r\n![image](https://user-images.githubusercontent.com/15898956/38519614-6ad30aec-3c0e-11e8-895e-b9b2d20dab17.png)\r\n\r\nWhat i tried after this was to use a subprocess within my script to kill the previous processes occupying resources on the GPU as seen here:\r\n![image](https://user-images.githubusercontent.com/15898956/38519591-55817534-3c0e-11e8-8bae-51d91e20b98e.png)\r\nAlthough this works, it seems like incredibly bad practice & I shouldn't be doing this.\r\n\r\nWhat I tried then was to use the `numba` host API for interacting with the GPU to shut down processes i.e. clear `current_context` and then clear deallocations from the GPU like this:\r\n`from numba import cuda\r\ncurrent_context = get_context(devnum=0)\r\ncurrent_context.reset()\r\ncuda.current_context().deallocations.clear()`\r\n\r\nWhen this didn't work out, as a last resort, i tried to use this:\r\n`from numba import cuda\r\ncuda.gpus[0].numba.cuda.cudadrv.devices.reset()` \r\nwhich works but results in a substantial memory leak everytime it runs which implies that after running my code a few times, the leak accumulated to a large enough value to again give me  resource errors.\r\n\r\n___________________________________________________________________________________________________\r\n\r\nContext: I am trying to deploy a deep learning model using a flask API. Since this is a pipeline with multiple computation graphs, I cannot afford to keep all of those in memory so i need to do something like this:\r\n1) Upload data & store it\r\n2) Build Graph\r\n2) Run Inference on stored data\r\n3) Remove graph from memory\r\n4) Build new graph\r\n5) Run Inference on stored data again\r\n.\r\n.\r\n....and so on\r\n--------------------------------------------------------------------------------------------\r\nWhat I suspect might be going on under the hood:\r\n          It's possible that `tf.reset_graph()` frees memory but doesn't remove the actual process holding onto that chunk of memory in the GPU in which case it makes sense for `nvidia-smi` to still show me the PID occupying memory in the GPU. But shouldn't `tf.reset_graph()` followed by `sess.close()` be freeing the GPU entirely?\r\n\r\n----------\r\n\r\nAny help on the issue will be appreciated."}