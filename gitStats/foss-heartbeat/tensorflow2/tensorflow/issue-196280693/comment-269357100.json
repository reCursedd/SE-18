{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269357100", "html_url": "https://github.com/tensorflow/tensorflow/pull/6387#issuecomment-269357100", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6387", "id": 269357100, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTM1NzEwMA==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-27T17:35:10Z", "updated_at": "2016-12-27T17:35:10Z", "author_association": "MEMBER", "body_html": "<p>Great thanks for the new implementation, looks much more maintainable! Just a few more things.</p>\n<ul>\n<li>\n<p>You wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of <code>.get_shape()</code> why not use <code>tf.shape</code>, would that work?</p>\n</li>\n<li>\n<p>I understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.</p>\n</li>\n<li>\n<p>As for your error, it looks like <code>dims = logits.get_shape()[1]</code> is <code>None</code> and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try <code>tf.shape</code> instead?</p>\n</li>\n<li>\n<p>Finally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.</p>\n</li>\n</ul>\n<p>Great thanks again for doing all of this!</p>", "body_text": "Great thanks for the new implementation, looks much more maintainable! Just a few more things.\n\n\nYou wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of .get_shape() why not use tf.shape, would that work?\n\n\nI understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.\n\n\nAs for your error, it looks like dims = logits.get_shape()[1] is None and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try tf.shape instead?\n\n\nFinally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.\n\n\nGreat thanks again for doing all of this!", "body": "Great thanks for the new implementation, looks much more maintainable! Just a few more things.\r\n\r\n* You wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of `.get_shape()` why not use `tf.shape`, would that work?\r\n\r\n* I understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.\r\n\r\n* As for your error, it looks like `dims = logits.get_shape()[1]` is `None` and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try `tf.shape` instead?\r\n\r\n* Finally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.\r\n\r\nGreat thanks again for doing all of this!"}