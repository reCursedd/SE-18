{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/268667217", "html_url": "https://github.com/tensorflow/tensorflow/pull/6387#issuecomment-268667217", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6387", "id": 268667217, "node_id": "MDEyOklzc3VlQ29tbWVudDI2ODY2NzIxNw==", "user": {"login": "AndreasMadsen", "id": 505333, "node_id": "MDQ6VXNlcjUwNTMzMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/505333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreasMadsen", "html_url": "https://github.com/AndreasMadsen", "followers_url": "https://api.github.com/users/AndreasMadsen/followers", "following_url": "https://api.github.com/users/AndreasMadsen/following{/other_user}", "gists_url": "https://api.github.com/users/AndreasMadsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreasMadsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreasMadsen/subscriptions", "organizations_url": "https://api.github.com/users/AndreasMadsen/orgs", "repos_url": "https://api.github.com/users/AndreasMadsen/repos", "events_url": "https://api.github.com/users/AndreasMadsen/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreasMadsen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-21T23:04:25Z", "updated_at": "2016-12-21T23:05:34Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>It seems like a lot of the work is because you need sorting. But tf.nn.top_k can be used to sort. Did you consider using it? Could you try it to simplify the code?</p>\n</blockquote>\n<p>I think this is true, there are two issues as far as I'm aware:</p>\n<ul>\n<li><code>tf.nn.top_k</code> has no GPU implementation.</li>\n<li>I don't know how to define a gradient on a composite operator (not a kernel, but an graph).</li>\n</ul>\n<p>On the subject of simplifying code: I don't think subtracting the mean is particular important, I helps only very little in some situations. Unlike softmax, sparsemax mostly consists of additions and some multiplications (no <code>exp</code>). So this part could likely be removed without noticeable issues. In our experiments, we did in fact not include it.</p>", "body_text": "It seems like a lot of the work is because you need sorting. But tf.nn.top_k can be used to sort. Did you consider using it? Could you try it to simplify the code?\n\nI think this is true, there are two issues as far as I'm aware:\n\ntf.nn.top_k has no GPU implementation.\nI don't know how to define a gradient on a composite operator (not a kernel, but an graph).\n\nOn the subject of simplifying code: I don't think subtracting the mean is particular important, I helps only very little in some situations. Unlike softmax, sparsemax mostly consists of additions and some multiplications (no exp). So this part could likely be removed without noticeable issues. In our experiments, we did in fact not include it.", "body": "> It seems like a lot of the work is because you need sorting. But tf.nn.top_k can be used to sort. Did you consider using it? Could you try it to simplify the code?\r\n\r\nI think this is true, there are two issues as far as I'm aware:\r\n\r\n* `tf.nn.top_k` has no GPU implementation.\r\n* I don't know how to define a gradient on a composite operator (not a kernel, but an graph).\r\n\r\nOn the subject of simplifying code: I don't think subtracting the mean is particular important, I helps only very little in some situations. Unlike softmax, sparsemax mostly consists of additions and some multiplications (no `exp`). So this part could likely be removed without noticeable issues. In our experiments, we did in fact not include it."}