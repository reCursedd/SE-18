{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269395108", "html_url": "https://github.com/tensorflow/tensorflow/pull/6387#issuecomment-269395108", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6387", "id": 269395108, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTM5NTEwOA==", "user": {"login": "AndreasMadsen", "id": 505333, "node_id": "MDQ6VXNlcjUwNTMzMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/505333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreasMadsen", "html_url": "https://github.com/AndreasMadsen", "followers_url": "https://api.github.com/users/AndreasMadsen/followers", "following_url": "https://api.github.com/users/AndreasMadsen/following{/other_user}", "gists_url": "https://api.github.com/users/AndreasMadsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreasMadsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreasMadsen/subscriptions", "organizations_url": "https://api.github.com/users/AndreasMadsen/orgs", "repos_url": "https://api.github.com/users/AndreasMadsen/repos", "events_url": "https://api.github.com/users/AndreasMadsen/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreasMadsen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-27T23:01:02Z", "updated_at": "2016-12-27T23:01:02Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>You wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of .get_shape() why not use tf.shape, would that work?</p>\n</blockquote>\n<p>Thanks <code>tf.shape</code> works, I was just not aware of it.</p>\n<blockquote>\n<p>I understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.</p>\n</blockquote>\n<blockquote>\n<p>Finally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.</p>\n</blockquote>\n<p>Yes, once I get the hand-written backprop version to work I will benchmark the diffrent versions.</p>\n<blockquote>\n<p>As for your error, it looks like dims = logits.get_shape()[1] is None and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try tf.shape instead?</p>\n</blockquote>\n<p>I changed it to  <code>tf.shape</code>, now I get a strange recursion error: <a href=\"https://gist.github.com/AndreasMadsen/8dca7e8b3ead6b5b339fd4a8c5bc8b03\">full error message</a>.</p>", "body_text": "You wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of .get_shape() why not use tf.shape, would that work?\n\nThanks tf.shape works, I was just not aware of it.\n\nI understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.\n\n\nFinally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.\n\nYes, once I get the hand-written backprop version to work I will benchmark the diffrent versions.\n\nAs for your error, it looks like dims = logits.get_shape()[1] is None and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try tf.shape instead?\n\nI changed it to  tf.shape, now I get a strange recursion error: full error message.", "body": "> You wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of .get_shape() why not use tf.shape, would that work?\r\n\r\nThanks `tf.shape` works, I was just not aware of it.\r\n\r\n> I understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.\r\n\r\n> Finally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.\r\n\r\nYes, once I get the hand-written backprop version to work I will benchmark the diffrent versions.\r\n\r\n> As for your error, it looks like dims = logits.get_shape()[1] is None and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try tf.shape instead?\r\n\r\nI changed it to  `tf.shape`, now I get a strange recursion error: [full error message](https://gist.github.com/AndreasMadsen/8dca7e8b3ead6b5b339fd4a8c5bc8b03)."}