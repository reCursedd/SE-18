{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269914966", "html_url": "https://github.com/tensorflow/tensorflow/pull/6387#issuecomment-269914966", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6387", "id": 269914966, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTkxNDk2Ng==", "user": {"login": "AndreasMadsen", "id": 505333, "node_id": "MDQ6VXNlcjUwNTMzMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/505333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreasMadsen", "html_url": "https://github.com/AndreasMadsen", "followers_url": "https://api.github.com/users/AndreasMadsen/followers", "following_url": "https://api.github.com/users/AndreasMadsen/following{/other_user}", "gists_url": "https://api.github.com/users/AndreasMadsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreasMadsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreasMadsen/subscriptions", "organizations_url": "https://api.github.com/users/AndreasMadsen/orgs", "repos_url": "https://api.github.com/users/AndreasMadsen/repos", "events_url": "https://api.github.com/users/AndreasMadsen/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreasMadsen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-01T19:03:47Z", "updated_at": "2017-01-02T08:56:10Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>But as for your benchmarks: you should always first run at least one iteration and throw away the timing, only later start true measurements. The graph does a fair bit of one-time allocations, especially with Defun, no need to measure them.</p>\n</blockquote>\n<p>I've changed it such it runs it 100 times before it measures anything. It also doesn't measure the memory transfer anymore.</p>\n<blockquote>\n<p>I think there is something I don't understand about how you specify sizes. A normal softmax from vectors of size 512 into 16K labels easily fits into any GPU, so I'm clearly not understanding something here.</p>\n</blockquote>\n<p>It was a matter of time, not memory. I created a job script that I can submit to our HPC system so that is not a problem, but it literally takes 12h to run the benchmark.</p>\n<blockquote>\n<p>Did you look at the TF timeline to see what takes so much time?</p>\n</blockquote>\n<p>No I have not, I want to setup the benchmarks correctly before I profile it.</p>\n<hr>\n<p>Benchmark results:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"left\"><a href=\"https://github.com/AndreasMadsen/tensorflow/commit/b9679dbc6a7ba7302a9d6c0f1541459b95a014b2\">kernel CPU</a></th>\n<th align=\"left\"><a href=\"https://github.com/AndreasMadsen/tensorflow/commit/b9679dbc6a7ba7302a9d6c0f1541459b95a014b2\">kernel GPU</a></th>\n<th align=\"left\"><a href=\"https://github.com/AndreasMadsen/tensorflow/commit/6be1549ce6e930f407f7ab59503b43072a53263d\">name_scope CPU</a></th>\n<th align=\"left\"><a href=\"https://github.com/AndreasMadsen/tensorflow/commit/1e5e47065cdd299e05fe8d34feafff6729efd2c5\">Defun CPU</a></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">(1000, 10) sparsemax</td>\n<td align=\"left\">0.096 \u00b1 0.0008</td>\n<td align=\"left\">0.079 \u00b1 0.0009</td>\n<td align=\"left\">0.325 \u00b1 0.0032</td>\n<td align=\"left\">0.336 \u00b1 0.0030</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 10) sparsemax_grad</td>\n<td align=\"left\">0.163 \u00b1 0.0009</td>\n<td align=\"left\">0.106 \u00b1 0.0014</td>\n<td align=\"left\">0.536 \u00b1 0.0061</td>\n<td align=\"left\">0.685 \u00b1 0.0045</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 10) sparsemax_loss</td>\n<td align=\"left\">0.131 \u00b1 0.0020</td>\n<td align=\"left\">0.104 \u00b1 0.0015</td>\n<td align=\"left\">0.356 \u00b1 0.0039</td>\n<td align=\"left\">0.399 \u00b1 0.0039</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 10) sparsemax_loss_grad</td>\n<td align=\"left\">0.129 \u00b1 0.0031</td>\n<td align=\"left\">0.088 \u00b1 0.0005</td>\n<td align=\"left\">0.573 \u00b1 0.0040</td>\n<td align=\"left\">0.950 \u00b1 0.0050</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 100) sparsemax</td>\n<td align=\"left\">0.554 \u00b1 0.0571</td>\n<td align=\"left\">0.213 \u00b1 0.0013</td>\n<td align=\"left\">1.494 \u00b1 0.0316</td>\n<td align=\"left\">1.493 \u00b1 0.0291</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 100) sparsemax_grad</td>\n<td align=\"left\">1.029 \u00b1 0.0267</td>\n<td align=\"left\">0.237 \u00b1 0.0029</td>\n<td align=\"left\">2.399 \u00b1 0.0495</td>\n<td align=\"left\">3.067 \u00b1 0.0318</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 100) sparsemax_loss</td>\n<td align=\"left\">0.844 \u00b1 0.0182</td>\n<td align=\"left\">0.277 \u00b1 0.0034</td>\n<td align=\"left\">1.758 \u00b1 0.0616</td>\n<td align=\"left\">1.735 \u00b1 0.0159</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 100) sparsemax_loss_grad</td>\n<td align=\"left\">0.994 \u00b1 0.0278</td>\n<td align=\"left\">0.227 \u00b1 0.0009</td>\n<td align=\"left\">2.632 \u00b1 0.0245</td>\n<td align=\"left\">3.723 \u00b1 0.0316</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 512) sparsemax</td>\n<td align=\"left\">3.238 \u00b1 0.0351</td>\n<td align=\"left\">1.746 \u00b1 0.0010</td>\n<td align=\"left\">5.973 \u00b1 0.0187</td>\n<td align=\"left\">5.618 \u00b1 0.0239</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 512) sparsemax_grad</td>\n<td align=\"left\">3.918 \u00b1 0.0288</td>\n<td align=\"left\">1.793 \u00b1 0.0019</td>\n<td align=\"left\">8.079 \u00b1 0.0312</td>\n<td align=\"left\">11.630 \u00b1 0.0235</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 512) sparsemax_loss</td>\n<td align=\"left\">3.480 \u00b1 0.0181</td>\n<td align=\"left\">1.673 \u00b1 0.0016</td>\n<td align=\"left\">6.218 \u00b1 0.0433</td>\n<td align=\"left\">6.309 \u00b1 0.0219</td>\n</tr>\n<tr>\n<td align=\"left\">(1000, 512) sparsemax_loss_grad</td>\n<td align=\"left\">3.887 \u00b1 0.0627</td>\n<td align=\"left\">1.754 \u00b1 0.0048</td>\n<td align=\"left\">8.893 \u00b1 0.0575</td>\n<td align=\"left\">13.813 \u00b1 0.0495</td>\n</tr>\n<tr>\n<td align=\"left\">(512, 16384) sparsemax</td>\n<td align=\"left\">60.092 \u00b1 1.4167</td>\n<td align=\"left\">633.671 \u00b1 0.2451</td>\n<td align=\"left\">86.176 \u00b1 0.9363</td>\n<td align=\"left\">91.747 \u00b1 0.9430</td>\n</tr>\n<tr>\n<td align=\"left\">(512, 16384) sparsemax_grad</td>\n<td align=\"left\">69.699 \u00b1 0.9295</td>\n<td align=\"left\">634.124 \u00b1 0.3004</td>\n<td align=\"left\">115.631 \u00b1 1.1464</td>\n<td align=\"left\">187.159 \u00b1 1.0400</td>\n</tr>\n<tr>\n<td align=\"left\">(512, 16384) sparsemax_loss</td>\n<td align=\"left\">61.021 \u00b1 0.7513</td>\n<td align=\"left\">632.289 \u00b1 0.2675</td>\n<td align=\"left\">94.075 \u00b1 0.6780</td>\n<td align=\"left\">97.617 \u00b1 0.2923</td>\n</tr>\n<tr>\n<td align=\"left\">(512, 16384) sparsemax_loss_grad</td>\n<td align=\"left\">67.598 \u00b1 1.7163</td>\n<td align=\"left\">633.777 \u00b1 0.3879</td>\n<td align=\"left\">125.269 \u00b1 0.8877</td>\n<td align=\"left\">213.390 \u00b1 1.1162</td>\n</tr>\n</tbody>\n</table>", "body_text": "But as for your benchmarks: you should always first run at least one iteration and throw away the timing, only later start true measurements. The graph does a fair bit of one-time allocations, especially with Defun, no need to measure them.\n\nI've changed it such it runs it 100 times before it measures anything. It also doesn't measure the memory transfer anymore.\n\nI think there is something I don't understand about how you specify sizes. A normal softmax from vectors of size 512 into 16K labels easily fits into any GPU, so I'm clearly not understanding something here.\n\nIt was a matter of time, not memory. I created a job script that I can submit to our HPC system so that is not a problem, but it literally takes 12h to run the benchmark.\n\nDid you look at the TF timeline to see what takes so much time?\n\nNo I have not, I want to setup the benchmarks correctly before I profile it.\n\nBenchmark results:\n\n\n\n\nkernel CPU\nkernel GPU\nname_scope CPU\nDefun CPU\n\n\n\n\n(1000, 10) sparsemax\n0.096 \u00b1 0.0008\n0.079 \u00b1 0.0009\n0.325 \u00b1 0.0032\n0.336 \u00b1 0.0030\n\n\n(1000, 10) sparsemax_grad\n0.163 \u00b1 0.0009\n0.106 \u00b1 0.0014\n0.536 \u00b1 0.0061\n0.685 \u00b1 0.0045\n\n\n(1000, 10) sparsemax_loss\n0.131 \u00b1 0.0020\n0.104 \u00b1 0.0015\n0.356 \u00b1 0.0039\n0.399 \u00b1 0.0039\n\n\n(1000, 10) sparsemax_loss_grad\n0.129 \u00b1 0.0031\n0.088 \u00b1 0.0005\n0.573 \u00b1 0.0040\n0.950 \u00b1 0.0050\n\n\n(1000, 100) sparsemax\n0.554 \u00b1 0.0571\n0.213 \u00b1 0.0013\n1.494 \u00b1 0.0316\n1.493 \u00b1 0.0291\n\n\n(1000, 100) sparsemax_grad\n1.029 \u00b1 0.0267\n0.237 \u00b1 0.0029\n2.399 \u00b1 0.0495\n3.067 \u00b1 0.0318\n\n\n(1000, 100) sparsemax_loss\n0.844 \u00b1 0.0182\n0.277 \u00b1 0.0034\n1.758 \u00b1 0.0616\n1.735 \u00b1 0.0159\n\n\n(1000, 100) sparsemax_loss_grad\n0.994 \u00b1 0.0278\n0.227 \u00b1 0.0009\n2.632 \u00b1 0.0245\n3.723 \u00b1 0.0316\n\n\n(1000, 512) sparsemax\n3.238 \u00b1 0.0351\n1.746 \u00b1 0.0010\n5.973 \u00b1 0.0187\n5.618 \u00b1 0.0239\n\n\n(1000, 512) sparsemax_grad\n3.918 \u00b1 0.0288\n1.793 \u00b1 0.0019\n8.079 \u00b1 0.0312\n11.630 \u00b1 0.0235\n\n\n(1000, 512) sparsemax_loss\n3.480 \u00b1 0.0181\n1.673 \u00b1 0.0016\n6.218 \u00b1 0.0433\n6.309 \u00b1 0.0219\n\n\n(1000, 512) sparsemax_loss_grad\n3.887 \u00b1 0.0627\n1.754 \u00b1 0.0048\n8.893 \u00b1 0.0575\n13.813 \u00b1 0.0495\n\n\n(512, 16384) sparsemax\n60.092 \u00b1 1.4167\n633.671 \u00b1 0.2451\n86.176 \u00b1 0.9363\n91.747 \u00b1 0.9430\n\n\n(512, 16384) sparsemax_grad\n69.699 \u00b1 0.9295\n634.124 \u00b1 0.3004\n115.631 \u00b1 1.1464\n187.159 \u00b1 1.0400\n\n\n(512, 16384) sparsemax_loss\n61.021 \u00b1 0.7513\n632.289 \u00b1 0.2675\n94.075 \u00b1 0.6780\n97.617 \u00b1 0.2923\n\n\n(512, 16384) sparsemax_loss_grad\n67.598 \u00b1 1.7163\n633.777 \u00b1 0.3879\n125.269 \u00b1 0.8877\n213.390 \u00b1 1.1162", "body": "> But as for your benchmarks: you should always first run at least one iteration and throw away the timing, only later start true measurements. The graph does a fair bit of one-time allocations, especially with Defun, no need to measure them.\r\n\r\nI've changed it such it runs it 100 times before it measures anything. It also doesn't measure the memory transfer anymore.\r\n\r\n> I think there is something I don't understand about how you specify sizes. A normal softmax from vectors of size 512 into 16K labels easily fits into any GPU, so I'm clearly not understanding something here.\r\n\r\nIt was a matter of time, not memory. I created a job script that I can submit to our HPC system so that is not a problem, but it literally takes 12h to run the benchmark. \r\n\r\n> Did you look at the TF timeline to see what takes so much time? \r\n\r\nNo I have not, I want to setup the benchmarks correctly before I profile it.\r\n\r\n----\r\n\r\nBenchmark results:\r\n\r\n|                                  | [kernel CPU](https://github.com/AndreasMadsen/tensorflow/commit/b9679dbc6a7ba7302a9d6c0f1541459b95a014b2)      | [kernel GPU](https://github.com/AndreasMadsen/tensorflow/commit/b9679dbc6a7ba7302a9d6c0f1541459b95a014b2)       | [name_scope CPU](https://github.com/AndreasMadsen/tensorflow/commit/6be1549ce6e930f407f7ab59503b43072a53263d)   | [Defun CPU](https://github.com/AndreasMadsen/tensorflow/commit/1e5e47065cdd299e05fe8d34feafff6729efd2c5)        |\r\n|:---------------------------------|:----------------|:-----------------|:-----------------|:-----------------|\r\n| (1000, 10) sparsemax             | 0.096 \u00b1 0.0008  | 0.079 \u00b1 0.0009   | 0.325 \u00b1 0.0032   | 0.336 \u00b1 0.0030   |\r\n| (1000, 10) sparsemax_grad        | 0.163 \u00b1 0.0009  | 0.106 \u00b1 0.0014   | 0.536 \u00b1 0.0061   | 0.685 \u00b1 0.0045   |\r\n| (1000, 10) sparsemax_loss        | 0.131 \u00b1 0.0020  | 0.104 \u00b1 0.0015   | 0.356 \u00b1 0.0039   | 0.399 \u00b1 0.0039   |\r\n| (1000, 10) sparsemax_loss_grad   | 0.129 \u00b1 0.0031  | 0.088 \u00b1 0.0005   | 0.573 \u00b1 0.0040   | 0.950 \u00b1 0.0050   |\r\n| (1000, 100) sparsemax            | 0.554 \u00b1 0.0571  | 0.213 \u00b1 0.0013   | 1.494 \u00b1 0.0316   | 1.493 \u00b1 0.0291   |\r\n| (1000, 100) sparsemax_grad       | 1.029 \u00b1 0.0267  | 0.237 \u00b1 0.0029   | 2.399 \u00b1 0.0495   | 3.067 \u00b1 0.0318   |\r\n| (1000, 100) sparsemax_loss       | 0.844 \u00b1 0.0182  | 0.277 \u00b1 0.0034   | 1.758 \u00b1 0.0616   | 1.735 \u00b1 0.0159   |\r\n| (1000, 100) sparsemax_loss_grad  | 0.994 \u00b1 0.0278  | 0.227 \u00b1 0.0009   | 2.632 \u00b1 0.0245   | 3.723 \u00b1 0.0316   |\r\n| (1000, 512) sparsemax            | 3.238 \u00b1 0.0351  | 1.746 \u00b1 0.0010   | 5.973 \u00b1 0.0187   | 5.618 \u00b1 0.0239   |\r\n| (1000, 512) sparsemax_grad       | 3.918 \u00b1 0.0288  | 1.793 \u00b1 0.0019   | 8.079 \u00b1 0.0312   | 11.630 \u00b1 0.0235  |\r\n| (1000, 512) sparsemax_loss       | 3.480 \u00b1 0.0181  | 1.673 \u00b1 0.0016   | 6.218 \u00b1 0.0433   | 6.309 \u00b1 0.0219   |\r\n| (1000, 512) sparsemax_loss_grad  | 3.887 \u00b1 0.0627  | 1.754 \u00b1 0.0048   | 8.893 \u00b1 0.0575   | 13.813 \u00b1 0.0495  |\r\n| (512, 16384) sparsemax           | 60.092 \u00b1 1.4167 | 633.671 \u00b1 0.2451 | 86.176 \u00b1 0.9363  | 91.747 \u00b1 0.9430  |\r\n| (512, 16384) sparsemax_grad      | 69.699 \u00b1 0.9295 | 634.124 \u00b1 0.3004 | 115.631 \u00b1 1.1464 | 187.159 \u00b1 1.0400 |\r\n| (512, 16384) sparsemax_loss      | 61.021 \u00b1 0.7513 | 632.289 \u00b1 0.2675 | 94.075 \u00b1 0.6780  | 97.617 \u00b1 0.2923  |\r\n| (512, 16384) sparsemax_loss_grad | 67.598 \u00b1 1.7163 | 633.777 \u00b1 0.3879 | 125.269 \u00b1 0.8877 | 213.390 \u00b1 1.1162 |\r\n"}