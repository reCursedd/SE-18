{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/424261030", "html_url": "https://github.com/tensorflow/tensorflow/issues/19903#issuecomment-424261030", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19903", "id": 424261030, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDI2MTAzMA==", "user": {"login": "jonasrauber", "id": 5837385, "node_id": "MDQ6VXNlcjU4MzczODU=", "avatar_url": "https://avatars1.githubusercontent.com/u/5837385?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jonasrauber", "html_url": "https://github.com/jonasrauber", "followers_url": "https://api.github.com/users/jonasrauber/followers", "following_url": "https://api.github.com/users/jonasrauber/following{/other_user}", "gists_url": "https://api.github.com/users/jonasrauber/gists{/gist_id}", "starred_url": "https://api.github.com/users/jonasrauber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jonasrauber/subscriptions", "organizations_url": "https://api.github.com/users/jonasrauber/orgs", "repos_url": "https://api.github.com/users/jonasrauber/repos", "events_url": "https://api.github.com/users/jonasrauber/events{/privacy}", "received_events_url": "https://api.github.com/users/jonasrauber/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-25T08:57:18Z", "updated_at": "2018-09-25T08:57:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1710528\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bhack\">@bhack</a>. For many common real-world tasks related to warm-starting (fine-tuning existing networks, pretraining on a different dataset, etc.), it is important to restore not just the variables trained with backprop (i.e. the <code>trainable_variables</code>) but also the variables <em>trained</em> using dataset statistics (e.g. accumulators, moving averages, etc.). Forgetting to restore them is such a common source of problems that are hard to notice; ideally it should be a very simple to switch between restoring <code>trainable_variables</code> and all <em>trainable variables</em> (i.e. including accumulators, \u2026) (as you notice: I even think it's a misnomer to only refer to some of them as <em>trainable</em> variables).</p>", "body_text": "I agree with @bhack. For many common real-world tasks related to warm-starting (fine-tuning existing networks, pretraining on a different dataset, etc.), it is important to restore not just the variables trained with backprop (i.e. the trainable_variables) but also the variables trained using dataset statistics (e.g. accumulators, moving averages, etc.). Forgetting to restore them is such a common source of problems that are hard to notice; ideally it should be a very simple to switch between restoring trainable_variables and all trainable variables (i.e. including accumulators, \u2026) (as you notice: I even think it's a misnomer to only refer to some of them as trainable variables).", "body": "I agree with @bhack. For many common real-world tasks related to warm-starting (fine-tuning existing networks, pretraining on a different dataset, etc.), it is important to restore not just the variables trained with backprop (i.e. the `trainable_variables`) but also the variables _trained_ using dataset statistics (e.g. accumulators, moving averages, etc.). Forgetting to restore them is such a common source of problems that are hard to notice; ideally it should be a very simple to switch between restoring `trainable_variables` and all _trainable variables_ (i.e. including accumulators, \u2026) (as you notice: I even think it's a misnomer to only refer to some of them as _trainable_ variables).\r\n"}