{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/337099932", "html_url": "https://github.com/tensorflow/tensorflow/issues/13761#issuecomment-337099932", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13761", "id": 337099932, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzA5OTkzMg==", "user": {"login": "kevinj22", "id": 20320568, "node_id": "MDQ6VXNlcjIwMzIwNTY4", "avatar_url": "https://avatars1.githubusercontent.com/u/20320568?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kevinj22", "html_url": "https://github.com/kevinj22", "followers_url": "https://api.github.com/users/kevinj22/followers", "following_url": "https://api.github.com/users/kevinj22/following{/other_user}", "gists_url": "https://api.github.com/users/kevinj22/gists{/gist_id}", "starred_url": "https://api.github.com/users/kevinj22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kevinj22/subscriptions", "organizations_url": "https://api.github.com/users/kevinj22/orgs", "repos_url": "https://api.github.com/users/kevinj22/repos", "events_url": "https://api.github.com/users/kevinj22/events{/privacy}", "received_events_url": "https://api.github.com/users/kevinj22/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-17T02:35:24Z", "updated_at": "2017-10-17T12:27:09Z", "author_association": "NONE", "body_html": "<p>That is true, the further apart the classification indexes the larger the weight of that error would be. In sentiment classification lets say 1 represents \"Hate it!\" and 5 represents \"Love it!\". In this case the squared error might be useful.</p>\n<p>I recently saw <a href=\"https://arxiv.org/abs/1702.05659\" rel=\"nofollow\">https://arxiv.org/abs/1702.05659</a> which states that loss functions other than the most popular log loss sometimes produce better results for classification asks. It would be nice to have the option to try.</p>", "body_text": "That is true, the further apart the classification indexes the larger the weight of that error would be. In sentiment classification lets say 1 represents \"Hate it!\" and 5 represents \"Love it!\". In this case the squared error might be useful.\nI recently saw https://arxiv.org/abs/1702.05659 which states that loss functions other than the most popular log loss sometimes produce better results for classification asks. It would be nice to have the option to try.", "body": "That is true, the further apart the classification indexes the larger the weight of that error would be. In sentiment classification lets say 1 represents \"Hate it!\" and 5 represents \"Love it!\". In this case the squared error might be useful.\r\n\r\nI recently saw https://arxiv.org/abs/1702.05659 which states that loss functions other than the most popular log loss sometimes produce better results for classification asks. It would be nice to have the option to try."}