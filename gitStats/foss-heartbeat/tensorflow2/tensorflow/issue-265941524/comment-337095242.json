{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/337095242", "html_url": "https://github.com/tensorflow/tensorflow/issues/13761#issuecomment-337095242", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13761", "id": 337095242, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzA5NTI0Mg==", "user": {"login": "kevinj22", "id": 20320568, "node_id": "MDQ6VXNlcjIwMzIwNTY4", "avatar_url": "https://avatars1.githubusercontent.com/u/20320568?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kevinj22", "html_url": "https://github.com/kevinj22", "followers_url": "https://api.github.com/users/kevinj22/followers", "following_url": "https://api.github.com/users/kevinj22/following{/other_user}", "gists_url": "https://api.github.com/users/kevinj22/gists{/gist_id}", "starred_url": "https://api.github.com/users/kevinj22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kevinj22/subscriptions", "organizations_url": "https://api.github.com/users/kevinj22/orgs", "repos_url": "https://api.github.com/users/kevinj22/repos", "events_url": "https://api.github.com/users/kevinj22/events{/privacy}", "received_events_url": "https://api.github.com/users/kevinj22/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-17T02:02:52Z", "updated_at": "2017-10-17T02:13:36Z", "author_association": "NONE", "body_html": "<p>I am attempting to implement the one hot cnn as described here : <a href=\"https://arxiv.org/pdf/1412.1058.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1412.1058.pdf</a></p>\n<p>In the paper Rie Johnson mentions they minimize square loss. I also read <a href=\"https://arxiv.org/abs/1702.05659\" rel=\"nofollow\">https://arxiv.org/abs/1702.05659</a> which states that loss functions other than the most popular log loss sometimes produce better results.</p>\n<p>I had the CNN working with cross entropy:</p>\n<pre><code> losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.targets, logits=self.logits,\n                name=\"xentropy\")\n            self.loss = tf.reduce_mean(losses)\n</code></pre>\n<p>but wish to try some other loss functions to see if I can get an accuracy boost.</p>\n<p>If I change my loss to:</p>\n<pre><code>self.loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=self.targets, predictions=self.predictions))\n</code></pre>\n<p>This is the error message I receive:</p>\n<pre><code>Traceback (most recent call last):\n  File \"cnn_Sparse_TwoFilt.py\", line 261, in &lt;module&gt;\n    train_op = optimizer.minimize(cnn.loss, global_step=global_step)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 322, in minimize\n    ([str(v) for _, v in grads_and_vars], loss))\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"&lt;tf.Variable 'conv-maxpool-1/W:0' shape=(28395, 50) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'conv-maxpool-1/b:0' shape=(50,) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'conv-maxpool-2/W:0' shape=(56790, 50) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'conv-maxpool-2/b:0' shape=(50,) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'output/W:0' shape=(100, 20) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'output/b:0' shape=(20,) dtype=float32_ref&gt;\"] and loss Tensor(\"loss/Mean:0\", shape=(), dtype=float32).\n</code></pre>\n<p>This seems like a similar issue to: <a href=\"https://stackoverflow.com/questions/40294271/tensorflow-minimize-l2-loss-on-int64-data-without-casting-to-float32-because-ca\" rel=\"nofollow\">https://stackoverflow.com/questions/40294271/tensorflow-minimize-l2-loss-on-int64-data-without-casting-to-float32-because-ca</a> which there is no answer to. The author of that post also posted here: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"140957377\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1511\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1511/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1511\">#1511</a> but she received instructions to go to stackoverflow, which I assume resulted in the first link.</p>", "body_text": "I am attempting to implement the one hot cnn as described here : https://arxiv.org/pdf/1412.1058.pdf\nIn the paper Rie Johnson mentions they minimize square loss. I also read https://arxiv.org/abs/1702.05659 which states that loss functions other than the most popular log loss sometimes produce better results.\nI had the CNN working with cross entropy:\n losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.targets, logits=self.logits,\n                name=\"xentropy\")\n            self.loss = tf.reduce_mean(losses)\n\nbut wish to try some other loss functions to see if I can get an accuracy boost.\nIf I change my loss to:\nself.loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=self.targets, predictions=self.predictions))\n\nThis is the error message I receive:\nTraceback (most recent call last):\n  File \"cnn_Sparse_TwoFilt.py\", line 261, in <module>\n    train_op = optimizer.minimize(cnn.loss, global_step=global_step)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 322, in minimize\n    ([str(v) for _, v in grads_and_vars], loss))\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv-maxpool-1/W:0' shape=(28395, 50) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-1/b:0' shape=(50,) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-2/W:0' shape=(56790, 50) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-2/b:0' shape=(50,) dtype=float32_ref>\", \"<tf.Variable 'output/W:0' shape=(100, 20) dtype=float32_ref>\", \"<tf.Variable 'output/b:0' shape=(20,) dtype=float32_ref>\"] and loss Tensor(\"loss/Mean:0\", shape=(), dtype=float32).\n\nThis seems like a similar issue to: https://stackoverflow.com/questions/40294271/tensorflow-minimize-l2-loss-on-int64-data-without-casting-to-float32-because-ca which there is no answer to. The author of that post also posted here: #1511 but she received instructions to go to stackoverflow, which I assume resulted in the first link.", "body": "I am attempting to implement the one hot cnn as described here : https://arxiv.org/pdf/1412.1058.pdf\r\n\r\nIn the paper Rie Johnson mentions they minimize square loss. I also read https://arxiv.org/abs/1702.05659 which states that loss functions other than the most popular log loss sometimes produce better results.\r\n\r\nI had the CNN working with cross entropy:\r\n\r\n```\r\n losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.targets, logits=self.logits,\r\n                name=\"xentropy\")\r\n            self.loss = tf.reduce_mean(losses)\r\n```\r\n\r\nbut wish to try some other loss functions to see if I can get an accuracy boost. \r\n\r\nIf I change my loss to:\r\n\r\n```\r\nself.loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=self.targets, predictions=self.predictions))\r\n```\r\n\r\nThis is the error message I receive:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"cnn_Sparse_TwoFilt.py\", line 261, in <module>\r\n    train_op = optimizer.minimize(cnn.loss, global_step=global_step)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 322, in minimize\r\n    ([str(v) for _, v in grads_and_vars], loss))\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv-maxpool-1/W:0' shape=(28395, 50) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-1/b:0' shape=(50,) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-2/W:0' shape=(56790, 50) dtype=float32_ref>\", \"<tf.Variable 'conv-maxpool-2/b:0' shape=(50,) dtype=float32_ref>\", \"<tf.Variable 'output/W:0' shape=(100, 20) dtype=float32_ref>\", \"<tf.Variable 'output/b:0' shape=(20,) dtype=float32_ref>\"] and loss Tensor(\"loss/Mean:0\", shape=(), dtype=float32).\r\n```\r\n\r\nThis seems like a similar issue to: https://stackoverflow.com/questions/40294271/tensorflow-minimize-l2-loss-on-int64-data-without-casting-to-float32-because-ca which there is no answer to. The author of that post also posted here: https://github.com/tensorflow/tensorflow/issues/1511 but she received instructions to go to stackoverflow, which I assume resulted in the first link. \r\n"}