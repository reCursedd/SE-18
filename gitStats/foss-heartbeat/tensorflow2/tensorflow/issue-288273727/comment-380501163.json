{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/380501163", "html_url": "https://github.com/tensorflow/tensorflow/issues/16087#issuecomment-380501163", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16087", "id": 380501163, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDUwMTE2Mw==", "user": {"login": "markpwoodward", "id": 6820773, "node_id": "MDQ6VXNlcjY4MjA3NzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6820773?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markpwoodward", "html_url": "https://github.com/markpwoodward", "followers_url": "https://api.github.com/users/markpwoodward/followers", "following_url": "https://api.github.com/users/markpwoodward/following{/other_user}", "gists_url": "https://api.github.com/users/markpwoodward/gists{/gist_id}", "starred_url": "https://api.github.com/users/markpwoodward/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markpwoodward/subscriptions", "organizations_url": "https://api.github.com/users/markpwoodward/orgs", "repos_url": "https://api.github.com/users/markpwoodward/repos", "events_url": "https://api.github.com/users/markpwoodward/events{/privacy}", "received_events_url": "https://api.github.com/users/markpwoodward/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-11T15:47:53Z", "updated_at": "2018-04-11T15:47:53Z", "author_association": "NONE", "body_html": "<p>+1<br>\nTo compare train and validation metrics \"apples\" to \"apples\", I run two evaluations, one on the validation set and one on a fixed subset of training data. To do this I call the evaluate's in a CheckpointSaverListener passed to estimator.train(...., saving_listeners), but two EvalSpec's passed to train_and_evalute(..., eval_spec) would be cleaner. Alternatively, EvalSpec could take a list of input_fn's and corresponding name's. Whichever you think is easier/better for the api.</p>", "body_text": "+1\nTo compare train and validation metrics \"apples\" to \"apples\", I run two evaluations, one on the validation set and one on a fixed subset of training data. To do this I call the evaluate's in a CheckpointSaverListener passed to estimator.train(...., saving_listeners), but two EvalSpec's passed to train_and_evalute(..., eval_spec) would be cleaner. Alternatively, EvalSpec could take a list of input_fn's and corresponding name's. Whichever you think is easier/better for the api.", "body": "+1\r\nTo compare train and validation metrics \"apples\" to \"apples\", I run two evaluations, one on the validation set and one on a fixed subset of training data. To do this I call the evaluate's in a CheckpointSaverListener passed to estimator.train(...., saving_listeners), but two EvalSpec's passed to train_and_evalute(..., eval_spec) would be cleaner. Alternatively, EvalSpec could take a list of input_fn's and corresponding name's. Whichever you think is easier/better for the api."}