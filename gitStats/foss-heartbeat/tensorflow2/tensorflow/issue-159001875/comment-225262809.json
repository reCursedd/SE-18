{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225262809", "html_url": "https://github.com/tensorflow/tensorflow/issues/2714#issuecomment-225262809", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2714", "id": 225262809, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTI2MjgwOQ==", "user": {"login": "zhengwy888", "id": 1190730, "node_id": "MDQ6VXNlcjExOTA3MzA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1190730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhengwy888", "html_url": "https://github.com/zhengwy888", "followers_url": "https://api.github.com/users/zhengwy888/followers", "following_url": "https://api.github.com/users/zhengwy888/following{/other_user}", "gists_url": "https://api.github.com/users/zhengwy888/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhengwy888/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhengwy888/subscriptions", "organizations_url": "https://api.github.com/users/zhengwy888/orgs", "repos_url": "https://api.github.com/users/zhengwy888/repos", "events_url": "https://api.github.com/users/zhengwy888/events{/privacy}", "received_events_url": "https://api.github.com/users/zhengwy888/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-10T18:40:11Z", "updated_at": "2016-06-10T18:40:11Z", "author_association": "NONE", "body_html": "<p>Unfortunately there is not much output. I have added a minimal script to reproduce the hanging on dynamic_rnn and the exception on rnn().</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> collections\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> variable_scope <span class=\"pl-k\">as</span> vs\n<span class=\"pl-k\">from</span> tensorflow.python.ops.math_ops <span class=\"pl-k\">import</span> sigmoid\n<span class=\"pl-k\">from</span> tensorflow.python.ops.math_ops <span class=\"pl-k\">import</span> tanh\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> array_ops\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> init_ops\n\n_LSTMStateTuple <span class=\"pl-k\">=</span> collections.namedtuple(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>LSTMStateTuple<span class=\"pl-pds\">\"</span></span>, (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>c<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h<span class=\"pl-pds\">\"</span></span>))\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">LSTMStateTuple</span>(<span class=\"pl-e\">_LSTMStateTuple</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.</span>\n<span class=\"pl-s\">  Stores two elements: `(c, h)`, in that order.</span>\n<span class=\"pl-s\">  Only used when `state_is_tuple=True`.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c1\">__slots__</span> <span class=\"pl-k\">=</span> ()\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">BasicLSTMCell</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">rnn_cell</span>.<span class=\"pl-e\">RNNCell</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Basic LSTM recurrent network cell.</span>\n<span class=\"pl-s\">  The implementation is based on: http://arxiv.org/abs/1409.2329.</span>\n<span class=\"pl-s\">  We add forget_bias (default: 1) to the biases of the forget gate in order to</span>\n<span class=\"pl-s\">  reduce the scale of forgetting in the beginning of the training.</span>\n<span class=\"pl-s\">  It does not allow cell clipping, a projection layer, and does not</span>\n<span class=\"pl-s\">  use peep-hole connections: it is the basic baseline.</span>\n<span class=\"pl-s\">  For advanced models, please use the full LSTMCell that follows.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-smi\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n               <span class=\"pl-smi\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">activation</span><span class=\"pl-k\">=</span>tanh, <span class=\"pl-smi\">add_summary</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Initialize the basic LSTM cell.</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">      num_units: int, The number of units in the LSTM cell.</span>\n<span class=\"pl-s\">      forget_bias: float, The bias added to forget gates (see above).</span>\n<span class=\"pl-s\">      input_size: Deprecated and unused.</span>\n<span class=\"pl-s\">      state_is_tuple: If True, accepted and returned states are 2-tuples of</span>\n<span class=\"pl-s\">        the `c_state` and `m_state`.  By default (False), they are concatenated</span>\n<span class=\"pl-s\">        along the column axis.  This default behavior will soon be deprecated.</span>\n<span class=\"pl-s\">      activation: Activation function of the inner states.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> state_is_tuple:\n      tf.logging.warn(\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%s</span>: Using a concatenated state is slower and will soon be <span class=\"pl-pds\">\"</span></span>\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>deprecated.  Use state_is_tuple=True.<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">self</span>)\n    <span class=\"pl-k\">if</span> input_size <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n      tf.logging.warn(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%s</span>: The input_size parameter is deprecated.<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">self</span>)\n    <span class=\"pl-c1\">self</span>._input_size <span class=\"pl-k\">=</span> num_units <span class=\"pl-k\">if</span> input_size <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> input_size\n    <span class=\"pl-c1\">self</span>._num_units <span class=\"pl-k\">=</span> num_units\n    <span class=\"pl-c1\">self</span>._forget_bias <span class=\"pl-k\">=</span> forget_bias\n    <span class=\"pl-c1\">self</span>._state_is_tuple <span class=\"pl-k\">=</span> state_is_tuple\n    <span class=\"pl-c1\">self</span>._activation <span class=\"pl-k\">=</span> activation\n    <span class=\"pl-c1\">self</span>._is_training <span class=\"pl-k\">=</span> is_training\n    <span class=\"pl-c1\">self</span>._add_summary <span class=\"pl-k\">=</span> add_summary\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">input_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._input_size\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> (LSTMStateTuple(<span class=\"pl-c1\">self</span>._num_units, <span class=\"pl-c1\">self</span>._num_units)\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._state_is_tuple <span class=\"pl-k\">else</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>._num_units)\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">output_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Long short-term memory cell (LSTM).<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__name__</span>) <span class=\"pl-k\">as</span> sc:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> \"BasicLSTMCell\"</span>\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>called with timestep<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> sc.name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> reuse <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(sc.reuse))\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> Parameters of gates are concatenated into one multiply for efficiency.</span>\n      <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._state_is_tuple:\n        c, h <span class=\"pl-k\">=</span> state\n      <span class=\"pl-k\">else</span>:\n        c, h <span class=\"pl-k\">=</span> array_ops.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, state)\n      concat <span class=\"pl-k\">=</span> tf.nn.rnn_cell._linear([inputs, h], <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>._num_units, <span class=\"pl-c1\">True</span>)\n\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> i = input_gate, j = new_input, f = forget_gate, o = output_gate</span>\n      i, j, f, o <span class=\"pl-k\">=</span> array_ops.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, concat)\n\n      new_c <span class=\"pl-k\">=</span> (c <span class=\"pl-k\">*</span> tf.nn.sigmoid(f <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>._forget_bias) <span class=\"pl-k\">+</span> sigmoid(i) <span class=\"pl-k\">*</span>\n               <span class=\"pl-c1\">self</span>._activation(j))\n      new_h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._activation(new_c) <span class=\"pl-k\">*</span> sigmoid(o)\n      <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._is_training:\n          <span class=\"pl-c\"><span class=\"pl-c\">#</span>timestep = get_timestep()</span>\n          timestep <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n          tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i/<span class=\"pl-c1\">%03d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> timestep, i)\n          tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>j/<span class=\"pl-c1\">%03d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> timestep, j)\n          tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f/<span class=\"pl-c1\">%03d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> timestep, f)\n          tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>o/<span class=\"pl-c1\">%03d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> timestep, o)\n          tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>c/<span class=\"pl-c1\">%03d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> timestep, c)\n          tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h/<span class=\"pl-c1\">%03d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> timestep, h)\n\n      <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._state_is_tuple:\n        new_state <span class=\"pl-k\">=</span> LSTMStateTuple(new_c, new_h)\n      <span class=\"pl-k\">else</span>:\n        new_state <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">1</span>, [new_c, new_h])\n      <span class=\"pl-k\">return</span> new_h, new_state\n\ntime_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\nnum_units <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\ninput_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n\ninput_values <span class=\"pl-k\">=</span> np.random.randn(time_steps, batch_size, input_size)\n\nsequence_length <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(batch_size))\nsummary_writer <span class=\"pl-k\">=</span> tf.train.SummaryWriter(\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>graph_test/<span class=\"pl-pds\">'</span></span>)\n\nconcat_inputs <span class=\"pl-k\">=</span> tf.placeholder(\n  tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(time_steps, batch_size, input_size))\ninputs <span class=\"pl-k\">=</span> [tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(batch_size, input_size)) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(time_steps)]\ncell <span class=\"pl-k\">=</span> BasicLSTMCell(num_units, <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">add_summary</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">False</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> dynamic_rnn</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this hangs</span>\n    outputs, state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n      cell, <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>concat_inputs, \n      <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_length)\n    feeds <span class=\"pl-k\">=</span> {}\n    feeds[concat_inputs] <span class=\"pl-k\">=</span> input_values\n    feeds[sequence_length] <span class=\"pl-k\">=</span> np.array([<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">5</span>])    \n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> rnn</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> InvalidArgumentError</span>\n    outputs, state <span class=\"pl-k\">=</span> tf.nn.rnn(\n        cell, inputs, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_length)\n    feeds <span class=\"pl-k\">=</span> {}\n    <span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> inputs:\n        feeds[var] <span class=\"pl-k\">=</span> np.random.randn(batch_size, input_size)\n    feeds[sequence_length] <span class=\"pl-k\">=</span> np.array([<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">5</span>])    \n\n\nmerged_summary <span class=\"pl-k\">=</span> tf.merge_all_summaries()\nto_fetch <span class=\"pl-k\">=</span> outputs\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(to_fetch, <span class=\"pl-c1\">list</span>):\n    to_fetch.append(merged_summary)\n<span class=\"pl-k\">else</span>:\n    to_fetch <span class=\"pl-k\">=</span> [to_fetch, merged_summary]\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> sess:\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize</span>\n  tf.initialize_all_variables().run()\n  fetches <span class=\"pl-k\">=</span> sess.run(to_fetch, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feeds)\n  summary <span class=\"pl-k\">=</span> fetches[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n  summary_writer.add_summary(summary, <span class=\"pl-c1\">1</span>)\n</pre></div>", "body_text": "Unfortunately there is not much output. I have added a minimal script to reproduce the hanging on dynamic_rnn and the exception on rnn().\nimport numpy as np\nimport collections\nimport tensorflow as tf\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\n\n_LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\nclass LSTMStateTuple(_LSTMStateTuple):\n  \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n  Stores two elements: `(c, h)`, in that order.\n  Only used when `state_is_tuple=True`.\n  \"\"\"\n  __slots__ = ()\n\nclass BasicLSTMCell(tf.nn.rnn_cell.RNNCell):\n  \"\"\"Basic LSTM recurrent network cell.\n  The implementation is based on: http://arxiv.org/abs/1409.2329.\n  We add forget_bias (default: 1) to the biases of the forget gate in order to\n  reduce the scale of forgetting in the beginning of the training.\n  It does not allow cell clipping, a projection layer, and does not\n  use peep-hole connections: it is the basic baseline.\n  For advanced models, please use the full LSTMCell that follows.\n  \"\"\"\n\n  def __init__(self, num_units, forget_bias=1.0, input_size=None, is_training=True,\n               state_is_tuple=False, activation=tanh, add_summary=True):\n    \"\"\"Initialize the basic LSTM cell.\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (see above).\n      input_size: Deprecated and unused.\n      state_is_tuple: If True, accepted and returned states are 2-tuples of\n        the `c_state` and `m_state`.  By default (False), they are concatenated\n        along the column axis.  This default behavior will soon be deprecated.\n      activation: Activation function of the inner states.\n    \"\"\"\n    if not state_is_tuple:\n      tf.logging.warn(\n          \"%s: Using a concatenated state is slower and will soon be \"\n          \"deprecated.  Use state_is_tuple=True.\" % self)\n    if input_size is not None:\n      tf.logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n    self._input_size = num_units if input_size is None else input_size\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._activation = activation\n    self._is_training = is_training\n    self._add_summary = add_summary\n\n  @property\n  def input_size(self):\n    return self._input_size\n  @property\n  def state_size(self):\n    return (LSTMStateTuple(self._num_units, self._num_units)\n            if self._state_is_tuple else 2 * self._num_units)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n    with vs.variable_scope(scope or type(self).__name__) as sc:  # \"BasicLSTMCell\"\n      print(\"called with timestep\" + sc.name + \" reuse \" + str(sc.reuse))\n      # Parameters of gates are concatenated into one multiply for efficiency.\n      if self._state_is_tuple:\n        c, h = state\n      else:\n        c, h = array_ops.split(1, 2, state)\n      concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, True)\n\n      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      i, j, f, o = array_ops.split(1, 4, concat)\n\n      new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + sigmoid(i) *\n               self._activation(j))\n      new_h = self._activation(new_c) * sigmoid(o)\n      if self._is_training:\n          #timestep = get_timestep()\n          timestep = 1\n          tf.histogram_summary(\"i/%03d\" % timestep, i)\n          tf.histogram_summary(\"j/%03d\" % timestep, j)\n          tf.histogram_summary(\"f/%03d\" % timestep, f)\n          tf.histogram_summary(\"o/%03d\" % timestep, o)\n          tf.histogram_summary(\"c/%03d\" % timestep, c)\n          tf.histogram_summary(\"h/%03d\" % timestep, h)\n\n      if self._state_is_tuple:\n        new_state = LSTMStateTuple(new_c, new_h)\n      else:\n        new_state = array_ops.concat(1, [new_c, new_h])\n      return new_h, new_state\n\ntime_steps = 8\nnum_units = 3\ninput_size = 5\nbatch_size = 2\n\ninput_values = np.random.randn(time_steps, batch_size, input_size)\n\nsequence_length = tf.placeholder(tf.int32, shape=(batch_size))\nsummary_writer = tf.train.SummaryWriter(\n    'graph_test/')\n\nconcat_inputs = tf.placeholder(\n  tf.float32, shape=(time_steps, batch_size, input_size))\ninputs = [tf.placeholder(tf.float32, shape=(batch_size, input_size)) for _ in range(time_steps)]\ncell = BasicLSTMCell(num_units, forget_bias=False, is_training=True, add_summary=True)\n\nif False:\n    # dynamic_rnn\n    # this hangs\n    outputs, state = tf.nn.dynamic_rnn(\n      cell, inputs=concat_inputs, \n      time_major=True, dtype=tf.float32, sequence_length=sequence_length)\n    feeds = {}\n    feeds[concat_inputs] = input_values\n    feeds[sequence_length] = np.array([2,5])    \nelse:\n    # rnn\n    # InvalidArgumentError\n    outputs, state = tf.nn.rnn(\n        cell, inputs, dtype=tf.float32, sequence_length=sequence_length)\n    feeds = {}\n    for var in inputs:\n        feeds[var] = np.random.randn(batch_size, input_size)\n    feeds[sequence_length] = np.array([2,5])    \n\n\nmerged_summary = tf.merge_all_summaries()\nto_fetch = outputs\nif isinstance(to_fetch, list):\n    to_fetch.append(merged_summary)\nelse:\n    to_fetch = [to_fetch, merged_summary]\n\nwith tf.Session(\"\") as sess:\n  # Initialize\n  tf.initialize_all_variables().run()\n  fetches = sess.run(to_fetch, feed_dict=feeds)\n  summary = fetches[-1]\n  summary_writer.add_summary(summary, 1)", "body": "Unfortunately there is not much output. I have added a minimal script to reproduce the hanging on dynamic_rnn and the exception on rnn().\n\n``` python\nimport numpy as np\nimport collections\nimport tensorflow as tf\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\n\n_LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\nclass LSTMStateTuple(_LSTMStateTuple):\n  \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n  Stores two elements: `(c, h)`, in that order.\n  Only used when `state_is_tuple=True`.\n  \"\"\"\n  __slots__ = ()\n\nclass BasicLSTMCell(tf.nn.rnn_cell.RNNCell):\n  \"\"\"Basic LSTM recurrent network cell.\n  The implementation is based on: http://arxiv.org/abs/1409.2329.\n  We add forget_bias (default: 1) to the biases of the forget gate in order to\n  reduce the scale of forgetting in the beginning of the training.\n  It does not allow cell clipping, a projection layer, and does not\n  use peep-hole connections: it is the basic baseline.\n  For advanced models, please use the full LSTMCell that follows.\n  \"\"\"\n\n  def __init__(self, num_units, forget_bias=1.0, input_size=None, is_training=True,\n               state_is_tuple=False, activation=tanh, add_summary=True):\n    \"\"\"Initialize the basic LSTM cell.\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (see above).\n      input_size: Deprecated and unused.\n      state_is_tuple: If True, accepted and returned states are 2-tuples of\n        the `c_state` and `m_state`.  By default (False), they are concatenated\n        along the column axis.  This default behavior will soon be deprecated.\n      activation: Activation function of the inner states.\n    \"\"\"\n    if not state_is_tuple:\n      tf.logging.warn(\n          \"%s: Using a concatenated state is slower and will soon be \"\n          \"deprecated.  Use state_is_tuple=True.\" % self)\n    if input_size is not None:\n      tf.logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n    self._input_size = num_units if input_size is None else input_size\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._activation = activation\n    self._is_training = is_training\n    self._add_summary = add_summary\n\n  @property\n  def input_size(self):\n    return self._input_size\n  @property\n  def state_size(self):\n    return (LSTMStateTuple(self._num_units, self._num_units)\n            if self._state_is_tuple else 2 * self._num_units)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n    with vs.variable_scope(scope or type(self).__name__) as sc:  # \"BasicLSTMCell\"\n      print(\"called with timestep\" + sc.name + \" reuse \" + str(sc.reuse))\n      # Parameters of gates are concatenated into one multiply for efficiency.\n      if self._state_is_tuple:\n        c, h = state\n      else:\n        c, h = array_ops.split(1, 2, state)\n      concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, True)\n\n      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n      i, j, f, o = array_ops.split(1, 4, concat)\n\n      new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + sigmoid(i) *\n               self._activation(j))\n      new_h = self._activation(new_c) * sigmoid(o)\n      if self._is_training:\n          #timestep = get_timestep()\n          timestep = 1\n          tf.histogram_summary(\"i/%03d\" % timestep, i)\n          tf.histogram_summary(\"j/%03d\" % timestep, j)\n          tf.histogram_summary(\"f/%03d\" % timestep, f)\n          tf.histogram_summary(\"o/%03d\" % timestep, o)\n          tf.histogram_summary(\"c/%03d\" % timestep, c)\n          tf.histogram_summary(\"h/%03d\" % timestep, h)\n\n      if self._state_is_tuple:\n        new_state = LSTMStateTuple(new_c, new_h)\n      else:\n        new_state = array_ops.concat(1, [new_c, new_h])\n      return new_h, new_state\n\ntime_steps = 8\nnum_units = 3\ninput_size = 5\nbatch_size = 2\n\ninput_values = np.random.randn(time_steps, batch_size, input_size)\n\nsequence_length = tf.placeholder(tf.int32, shape=(batch_size))\nsummary_writer = tf.train.SummaryWriter(\n    'graph_test/')\n\nconcat_inputs = tf.placeholder(\n  tf.float32, shape=(time_steps, batch_size, input_size))\ninputs = [tf.placeholder(tf.float32, shape=(batch_size, input_size)) for _ in range(time_steps)]\ncell = BasicLSTMCell(num_units, forget_bias=False, is_training=True, add_summary=True)\n\nif False:\n    # dynamic_rnn\n    # this hangs\n    outputs, state = tf.nn.dynamic_rnn(\n      cell, inputs=concat_inputs, \n      time_major=True, dtype=tf.float32, sequence_length=sequence_length)\n    feeds = {}\n    feeds[concat_inputs] = input_values\n    feeds[sequence_length] = np.array([2,5])    \nelse:\n    # rnn\n    # InvalidArgumentError\n    outputs, state = tf.nn.rnn(\n        cell, inputs, dtype=tf.float32, sequence_length=sequence_length)\n    feeds = {}\n    for var in inputs:\n        feeds[var] = np.random.randn(batch_size, input_size)\n    feeds[sequence_length] = np.array([2,5])    \n\n\nmerged_summary = tf.merge_all_summaries()\nto_fetch = outputs\nif isinstance(to_fetch, list):\n    to_fetch.append(merged_summary)\nelse:\n    to_fetch = [to_fetch, merged_summary]\n\nwith tf.Session(\"\") as sess:\n  # Initialize\n  tf.initialize_all_variables().run()\n  fetches = sess.run(to_fetch, feed_dict=feeds)\n  summary = fetches[-1]\n  summary_writer.add_summary(summary, 1)\n\n```\n"}