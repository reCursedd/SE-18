{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9901", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9901/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9901/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9901/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9901", "id": 228575222, "node_id": "MDU6SXNzdWUyMjg1NzUyMjI=", "number": 9901, "title": "tf.gradients runtime scales suboptimally with size of the graph", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-05-14T22:37:07Z", "updated_at": "2018-01-25T16:11:36Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><code>tf.gradients</code> can be inefficient on large graphs and it runtime increases with size of the graph, even when the amount of work it needs to do is constant. This inefficiency is apparent when trying to differentiate small parts of large graph many times.</p>\n<p>Discovered when trying to scale to 8 GPUs using data parallelism using 8 identical copies of model -- time spent inside gradients grows for each new replica even though replicas are identical and independent. We are calling <code>tf.gradients</code> many times (calling tf.gradients on parts of model in order to do memory saving gradients <a href=\"https://arxiv.org/abs/1604.06174\" rel=\"nofollow\">trick</a>), our largest models spend &gt;2 hours inside <code>tf.gradients</code>.</p>\n<p>I've profiled the runs and saw that most of the time is spent inside</p>\n<p><code>_MarkReachedOps(from_ops, reached_ops)</code> inside <code>gradients_impl.py</code></p>\n<p>It's called as follows</p>\n<pre><code>  reached_ops = [False] * (graph._last_id + 1)\n  for op in to_ops:\n    reached_ops[op._id] = True\n</code></pre>\n<p>You can see that it's using Python list initialized with the size of the entire graph so this initialization step would grow with size of the graph.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/26038340/102545da-38bb-11e7-873b-b57bc628ae7c.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/26038340/102545da-38bb-11e7-873b-b57bc628ae7c.png\" alt=\"screenshot 2017-05-14 15 35 58\" style=\"max-width:100%;\"></a></p>\n<p>Profile of the <code>_MarkReachedOps</code> when calling when calling tf gradients 560 times, with each gradient call adding 35 nodes on average, and total size of the graph being 200k nodes</p>\n<pre><code>Line #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n   101                                           @profile\n   102                                           def _MarkReachedOps(from_ops, reached_ops):\n   103                                             \"\"\"Mark all ops reached from \"from_ops\".\n   104                                           \n   105                                             Args:\n   106                                               from_ops: list of Operations.\n   107                                               reached_ops: list of booleans, indexed by operation id.\n   108                                             \"\"\"\n   109       568         1967      3.5      0.0    queue = collections.deque()\n   110       568         4835      8.5      0.0    queue.extend(from_ops)\n   111  39912648     14661885      0.4      8.4    while queue:\n   112  39912080     17079278      0.4      9.8      op = queue.popleft()\n   113  39912080     41203709      1.0     23.7      if not reached_ops[op._id]:\n   114  28997056     21267924      0.7     12.2        reached_ops[op._id] = True\n   115  58483932     42196549      0.7     24.2        for output in op.outputs:\n   116  29486876     37595214      1.3     21.6          queue.extend(output.consumers())\n</code></pre>\n<p>Possible solutions could be a more efficient implementation of <code>_PendingCount</code>, or a different algorithm for <code>tf.gradients</code> which is more efficient for large graphs</p>", "body_text": "tf.gradients can be inefficient on large graphs and it runtime increases with size of the graph, even when the amount of work it needs to do is constant. This inefficiency is apparent when trying to differentiate small parts of large graph many times.\nDiscovered when trying to scale to 8 GPUs using data parallelism using 8 identical copies of model -- time spent inside gradients grows for each new replica even though replicas are identical and independent. We are calling tf.gradients many times (calling tf.gradients on parts of model in order to do memory saving gradients trick), our largest models spend >2 hours inside tf.gradients.\nI've profiled the runs and saw that most of the time is spent inside\n_MarkReachedOps(from_ops, reached_ops) inside gradients_impl.py\nIt's called as follows\n  reached_ops = [False] * (graph._last_id + 1)\n  for op in to_ops:\n    reached_ops[op._id] = True\n\nYou can see that it's using Python list initialized with the size of the entire graph so this initialization step would grow with size of the graph.\n\nProfile of the _MarkReachedOps when calling when calling tf gradients 560 times, with each gradient call adding 35 nodes on average, and total size of the graph being 200k nodes\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n   101                                           @profile\n   102                                           def _MarkReachedOps(from_ops, reached_ops):\n   103                                             \"\"\"Mark all ops reached from \"from_ops\".\n   104                                           \n   105                                             Args:\n   106                                               from_ops: list of Operations.\n   107                                               reached_ops: list of booleans, indexed by operation id.\n   108                                             \"\"\"\n   109       568         1967      3.5      0.0    queue = collections.deque()\n   110       568         4835      8.5      0.0    queue.extend(from_ops)\n   111  39912648     14661885      0.4      8.4    while queue:\n   112  39912080     17079278      0.4      9.8      op = queue.popleft()\n   113  39912080     41203709      1.0     23.7      if not reached_ops[op._id]:\n   114  28997056     21267924      0.7     12.2        reached_ops[op._id] = True\n   115  58483932     42196549      0.7     24.2        for output in op.outputs:\n   116  29486876     37595214      1.3     21.6          queue.extend(output.consumers())\n\nPossible solutions could be a more efficient implementation of _PendingCount, or a different algorithm for tf.gradients which is more efficient for large graphs", "body": "`tf.gradients` can be inefficient on large graphs and it runtime increases with size of the graph, even when the amount of work it needs to do is constant. This inefficiency is apparent when trying to differentiate small parts of large graph many times.\r\n\r\nDiscovered when trying to scale to 8 GPUs using data parallelism using 8 identical copies of model -- time spent inside gradients grows for each new replica even though replicas are identical and independent. We are calling `tf.gradients` many times (calling tf.gradients on parts of model in order to do memory saving gradients [trick](https://arxiv.org/abs/1604.06174)), our largest models spend >2 hours inside `tf.gradients`.\r\n\r\nI've profiled the runs and saw that most of the time is spent inside\r\n\r\n`_MarkReachedOps(from_ops, reached_ops)` inside `gradients_impl.py`\r\n\r\nIt's called as follows\r\n\r\n```\r\n  reached_ops = [False] * (graph._last_id + 1)\r\n  for op in to_ops:\r\n    reached_ops[op._id] = True\r\n```\r\nYou can see that it's using Python list initialized with the size of the entire graph so this initialization step would grow with size of the graph.\r\n\r\n![screenshot 2017-05-14 15 35 58](https://cloud.githubusercontent.com/assets/23068/26038340/102545da-38bb-11e7-873b-b57bc628ae7c.png)\r\n\r\n\r\nProfile of the `_MarkReachedOps` when calling when calling tf gradients 560 times, with each gradient call adding 35 nodes on average, and total size of the graph being 200k nodes\r\n\r\n```\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n   101                                           @profile\r\n   102                                           def _MarkReachedOps(from_ops, reached_ops):\r\n   103                                             \"\"\"Mark all ops reached from \"from_ops\".\r\n   104                                           \r\n   105                                             Args:\r\n   106                                               from_ops: list of Operations.\r\n   107                                               reached_ops: list of booleans, indexed by operation id.\r\n   108                                             \"\"\"\r\n   109       568         1967      3.5      0.0    queue = collections.deque()\r\n   110       568         4835      8.5      0.0    queue.extend(from_ops)\r\n   111  39912648     14661885      0.4      8.4    while queue:\r\n   112  39912080     17079278      0.4      9.8      op = queue.popleft()\r\n   113  39912080     41203709      1.0     23.7      if not reached_ops[op._id]:\r\n   114  28997056     21267924      0.7     12.2        reached_ops[op._id] = True\r\n   115  58483932     42196549      0.7     24.2        for output in op.outputs:\r\n   116  29486876     37595214      1.3     21.6          queue.extend(output.consumers())\r\n```\r\n\r\nPossible solutions could be a more efficient implementation of `_PendingCount`, or a different algorithm for `tf.gradients` which is more efficient for large graphs"}