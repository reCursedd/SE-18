{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23634", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23634/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23634/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23634/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23634", "id": 379232723, "node_id": "MDU6SXNzdWUzNzkyMzI3MjM=", "number": 23634, "title": "Incorrect loss calculation after v1.10: fitting tf.Dataset in Keras.", "user": {"login": "skiler07", "id": 6961787, "node_id": "MDQ6VXNlcjY5NjE3ODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6961787?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skiler07", "html_url": "https://github.com/skiler07", "followers_url": "https://api.github.com/users/skiler07/followers", "following_url": "https://api.github.com/users/skiler07/following{/other_user}", "gists_url": "https://api.github.com/users/skiler07/gists{/gist_id}", "starred_url": "https://api.github.com/users/skiler07/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skiler07/subscriptions", "organizations_url": "https://api.github.com/users/skiler07/orgs", "repos_url": "https://api.github.com/users/skiler07/repos", "events_url": "https://api.github.com/users/skiler07/events{/privacy}", "received_events_url": "https://api.github.com/users/skiler07/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-09T16:33:10Z", "updated_at": "2018-11-15T10:30:03Z", "closed_at": "2018-11-15T10:30:03Z", "author_association": "NONE", "body_html": "<p>In 1.12, we can pass <code>tf.Dataset</code> into our Keras models. For instance, I'm modelling a binary classification with high class imbalance.</p>\n<pre><code>def _parse_func(record):\n    keys_to_features = {} # some schema\n\n    parsed = tf.parse_single_example(record, keys_to_features)\n\n    # Build the X output\n    x = parsed['x']  \n    y = parsed['y']\n    w = parsed['w'] * DEBUG_WEIGHT\n\n    return x, y, w\n</code></pre>\n<p>where <code>DEBUG_WEIGHT</code> is changed from 1 to 10000, with no affect on the loss.<br>\nCreate <code>tf.Dataset</code></p>\n<pre><code>def input_fn(f, b=BATCH_SIZE):\n    \"\"\"\n    :param f: A list of file names e.g. ['data-0.tf-record', 'data-1.tf-record'] to read\n    :param b: Batch size, defaults to BATCH_SIZE in hparams.py\n    :return: And infinitely iterable data set using tf records of tf.data.Dataset class\n    \"\"\"\n    d = tf.data.TFRecordDataset(filenames=f)\n    d = d.map(map_func=_parse_func)\n    d = d.repeat()\n    d = d.batch(batch_size=b)\n    return d\n</code></pre>\n<p>But when I fit the model using Keras, no matter what weight is passed, it seems ignored.</p>\n<p>E.g.</p>\n<pre><code>    train_data = input_fn(f=['path_to_tf_records'])\n\n    params = {\n        'loss': 'binary_crossentropy',\n        'optimizer': optimizer,\n        'metrics': metrics,\n    }\n    model.compile(**params)\n\n    model.fit(x=train_data.make_one_shot_iterator(),\n              epochs=50,\n              steps_per_epoch=10000)\n</code></pre>\n<p>Interestingly, if I pass <code>class_weight</code> to the <code>fit</code>, then I get the message that both <code>sample_weights</code> and <code>class_weight</code> are provided. So why doesn't the loss change?</p>\n<p>The loss is incredibly small and it doesn't change with weight.</p>\n<p>Epoch 1/30<br>\n2018-11-09 16:24:10.605651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br>\n461/9375 [&gt;.............................] - ETA: 2:01:50 - loss: 53.4499 - binary_accuracy: 0.8183 - binary_crossentropy: 0.4290 - mean_absolute_error: 0.3171</p>\n<h2>Sidenote</h2>\n<p>When I used numpy arrays with <code>tf.keras.utils.Sequence</code> class as a generator, using <code>fit_generator</code>, the loss was also not changing properly.</p>\n<h3>Takeaways</h3>\n<p>Clearly, the weight is not passed to the loss function correctly. Even if I use <code>fit_generator()</code> the loss differs in tensor flow &gt; 1.10. Additionally, workers must be set to 0 i.e. <code>workers=0</code> for the generator to work which is incredibly slow during training.</p>", "body_text": "In 1.12, we can pass tf.Dataset into our Keras models. For instance, I'm modelling a binary classification with high class imbalance.\ndef _parse_func(record):\n    keys_to_features = {} # some schema\n\n    parsed = tf.parse_single_example(record, keys_to_features)\n\n    # Build the X output\n    x = parsed['x']  \n    y = parsed['y']\n    w = parsed['w'] * DEBUG_WEIGHT\n\n    return x, y, w\n\nwhere DEBUG_WEIGHT is changed from 1 to 10000, with no affect on the loss.\nCreate tf.Dataset\ndef input_fn(f, b=BATCH_SIZE):\n    \"\"\"\n    :param f: A list of file names e.g. ['data-0.tf-record', 'data-1.tf-record'] to read\n    :param b: Batch size, defaults to BATCH_SIZE in hparams.py\n    :return: And infinitely iterable data set using tf records of tf.data.Dataset class\n    \"\"\"\n    d = tf.data.TFRecordDataset(filenames=f)\n    d = d.map(map_func=_parse_func)\n    d = d.repeat()\n    d = d.batch(batch_size=b)\n    return d\n\nBut when I fit the model using Keras, no matter what weight is passed, it seems ignored.\nE.g.\n    train_data = input_fn(f=['path_to_tf_records'])\n\n    params = {\n        'loss': 'binary_crossentropy',\n        'optimizer': optimizer,\n        'metrics': metrics,\n    }\n    model.compile(**params)\n\n    model.fit(x=train_data.make_one_shot_iterator(),\n              epochs=50,\n              steps_per_epoch=10000)\n\nInterestingly, if I pass class_weight to the fit, then I get the message that both sample_weights and class_weight are provided. So why doesn't the loss change?\nThe loss is incredibly small and it doesn't change with weight.\nEpoch 1/30\n2018-11-09 16:24:10.605651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n461/9375 [>.............................] - ETA: 2:01:50 - loss: 53.4499 - binary_accuracy: 0.8183 - binary_crossentropy: 0.4290 - mean_absolute_error: 0.3171\nSidenote\nWhen I used numpy arrays with tf.keras.utils.Sequence class as a generator, using fit_generator, the loss was also not changing properly.\nTakeaways\nClearly, the weight is not passed to the loss function correctly. Even if I use fit_generator() the loss differs in tensor flow > 1.10. Additionally, workers must be set to 0 i.e. workers=0 for the generator to work which is incredibly slow during training.", "body": "In 1.12, we can pass `tf.Dataset` into our Keras models. For instance, I'm modelling a binary classification with high class imbalance. \r\n\r\n```\r\ndef _parse_func(record):\r\n    keys_to_features = {} # some schema\r\n\r\n    parsed = tf.parse_single_example(record, keys_to_features)\r\n\r\n    # Build the X output\r\n    x = parsed['x']  \r\n    y = parsed['y']\r\n    w = parsed['w'] * DEBUG_WEIGHT\r\n\r\n    return x, y, w\r\n```\r\nwhere `DEBUG_WEIGHT` is changed from 1 to 10000, with no affect on the loss.\r\nCreate `tf.Dataset`\r\n\r\n```\r\ndef input_fn(f, b=BATCH_SIZE):\r\n    \"\"\"\r\n    :param f: A list of file names e.g. ['data-0.tf-record', 'data-1.tf-record'] to read\r\n    :param b: Batch size, defaults to BATCH_SIZE in hparams.py\r\n    :return: And infinitely iterable data set using tf records of tf.data.Dataset class\r\n    \"\"\"\r\n    d = tf.data.TFRecordDataset(filenames=f)\r\n    d = d.map(map_func=_parse_func)\r\n    d = d.repeat()\r\n    d = d.batch(batch_size=b)\r\n    return d\r\n```\r\n\r\nBut when I fit the model using Keras, no matter what weight is passed, it seems ignored.\r\n\r\nE.g.\r\n\r\n```\r\n    train_data = input_fn(f=['path_to_tf_records'])\r\n\r\n    params = {\r\n        'loss': 'binary_crossentropy',\r\n        'optimizer': optimizer,\r\n        'metrics': metrics,\r\n    }\r\n    model.compile(**params)\r\n\r\n    model.fit(x=train_data.make_one_shot_iterator(),\r\n              epochs=50,\r\n              steps_per_epoch=10000)\r\n```\r\nInterestingly, if I pass `class_weight` to the `fit`, then I get the message that both `sample_weights` and `class_weight` are provided. So why doesn't the loss change?\r\n\r\nThe loss is incredibly small and it doesn't change with weight.\r\n\r\nEpoch 1/30\r\n2018-11-09 16:24:10.605651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n 461/9375 [>.............................] - ETA: 2:01:50 - loss: 53.4499 - binary_accuracy: 0.8183 - binary_crossentropy: 0.4290 - mean_absolute_error: 0.3171\r\n\r\n\r\n## Sidenote\r\nWhen I used numpy arrays with `tf.keras.utils.Sequence` class as a generator, using `fit_generator`, the loss was also not changing properly. \r\n\r\n\r\n### Takeaways\r\n\r\nClearly, the weight is not passed to the loss function correctly. Even if I use `fit_generator()` the loss differs in tensor flow > 1.10. Additionally, workers must be set to 0 i.e. `workers=0` for the generator to work which is incredibly slow during training."}