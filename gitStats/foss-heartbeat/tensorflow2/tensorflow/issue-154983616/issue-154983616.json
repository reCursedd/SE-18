{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2385", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2385/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2385/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2385/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2385", "id": 154983616, "node_id": "MDU6SXNzdWUxNTQ5ODM2MTY=", "number": 2385, "title": "Hard to understand error message ", "user": {"login": "ushnish", "id": 3603839, "node_id": "MDQ6VXNlcjM2MDM4Mzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3603839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ushnish", "html_url": "https://github.com/ushnish", "followers_url": "https://api.github.com/users/ushnish/followers", "following_url": "https://api.github.com/users/ushnish/following{/other_user}", "gists_url": "https://api.github.com/users/ushnish/gists{/gist_id}", "starred_url": "https://api.github.com/users/ushnish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ushnish/subscriptions", "organizations_url": "https://api.github.com/users/ushnish/orgs", "repos_url": "https://api.github.com/users/ushnish/repos", "events_url": "https://api.github.com/users/ushnish/events{/privacy}", "received_events_url": "https://api.github.com/users/ushnish/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-05-16T08:29:23Z", "updated_at": "2016-05-18T19:47:40Z", "closed_at": "2016-05-18T19:47:40Z", "author_association": "NONE", "body_html": "<p>x is a list of list of tensors, and im passing in one list at a time to the rnn but I get this hard to understand error message</p>\n<p>Traceback (most recent call last):<br>\nFile \"autoencoder_m.py\", line 60, in <br>\noutputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 127, in rnn<br>\narray_ops.pack([batch_size, cell.output_size]), inputs[0].dtype)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 241, in pack<br>\nreturn gen_array_ops._pack(values, name=name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 916, in _pack<br>\nreturn _op_def_lib.apply_op(\"Pack\", values=values, name=name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 379, in apply_op<br>\nassert dtype is not None, \"Should not fail if dtype is None\"<br>\n<strong>AssertionError: Should not fail if dtype is None</strong></p>\n<p>Does this mean that x[i][0] is of type None? I already checked that it is <code>&lt;dtype: 'float32'&gt;</code></p>\n<p>This is the code I am trying to run, basically trying to run rnn on multiple gpu in the following manner.</p>\n<pre><code>x = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ny = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ns = [tf.placeholder(tf.int32, shape=[batch_size]) for _ in xrange(num_gpu)]\ncell = BasicLSTMCell(num_units = hidden_neurons, input_size = [batch_size, input_width])\ninitial_state = tf.zeros([batch_size, hidden_neurons * 2]) \ndropout_cell = DropoutWrapper(cell, input_keep_prob=0.9, output_keep_prob=1.0)\ninitial_state = tf.zeros([batch_size, cell.state_size])\n\ngpu_grads = []  \nlosses = []\nopt = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.99, beta2=0.97)\n\nfor i in xrange(num_gpu):\n        gpu_id = '/gpu:'+str(i)\n        with tf.device(gpu_id), tf.name_scope(gpu_id) as scope:\n                print x[i][0].dtype\n                outputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)\n</code></pre>\n<p>After this I get average of gradients similar to cifar10 multi gpu tutorial example.</p>", "body_text": "x is a list of list of tensors, and im passing in one list at a time to the rnn but I get this hard to understand error message\nTraceback (most recent call last):\nFile \"autoencoder_m.py\", line 60, in \noutputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 127, in rnn\narray_ops.pack([batch_size, cell.output_size]), inputs[0].dtype)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 241, in pack\nreturn gen_array_ops._pack(values, name=name)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 916, in _pack\nreturn _op_def_lib.apply_op(\"Pack\", values=values, name=name)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 379, in apply_op\nassert dtype is not None, \"Should not fail if dtype is None\"\nAssertionError: Should not fail if dtype is None\nDoes this mean that x[i][0] is of type None? I already checked that it is <dtype: 'float32'>\nThis is the code I am trying to run, basically trying to run rnn on multiple gpu in the following manner.\nx = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ny = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ns = [tf.placeholder(tf.int32, shape=[batch_size]) for _ in xrange(num_gpu)]\ncell = BasicLSTMCell(num_units = hidden_neurons, input_size = [batch_size, input_width])\ninitial_state = tf.zeros([batch_size, hidden_neurons * 2]) \ndropout_cell = DropoutWrapper(cell, input_keep_prob=0.9, output_keep_prob=1.0)\ninitial_state = tf.zeros([batch_size, cell.state_size])\n\ngpu_grads = []  \nlosses = []\nopt = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.99, beta2=0.97)\n\nfor i in xrange(num_gpu):\n        gpu_id = '/gpu:'+str(i)\n        with tf.device(gpu_id), tf.name_scope(gpu_id) as scope:\n                print x[i][0].dtype\n                outputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)\n\nAfter this I get average of gradients similar to cifar10 multi gpu tutorial example.", "body": "x is a list of list of tensors, and im passing in one list at a time to the rnn but I get this hard to understand error message\n\nTraceback (most recent call last):\n  File \"autoencoder_m.py\", line 60, in <module>\n    outputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 127, in rnn\n    array_ops.pack([batch_size, cell.output_size]), inputs[0].dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 241, in pack\n    return gen_array_ops._pack(values, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 916, in _pack\n    return _op_def_lib.apply_op(\"Pack\", values=values, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 379, in apply_op\n    assert dtype is not None, \"Should not fail if dtype is None\"\n**AssertionError: Should not fail if dtype is None**\n\nDoes this mean that x[i][0] is of type None? I already checked that it is `<dtype: 'float32'>`\n\nThis is the code I am trying to run, basically trying to run rnn on multiple gpu in the following manner.\n\n```\nx = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ny = [[tf.placeholder(tf.float32, shape=[batch_size, input_width]) for _ in xrange(max_sequence_length)] for _ in xrange(num_gpu)]\ns = [tf.placeholder(tf.int32, shape=[batch_size]) for _ in xrange(num_gpu)]\ncell = BasicLSTMCell(num_units = hidden_neurons, input_size = [batch_size, input_width])\ninitial_state = tf.zeros([batch_size, hidden_neurons * 2]) \ndropout_cell = DropoutWrapper(cell, input_keep_prob=0.9, output_keep_prob=1.0)\ninitial_state = tf.zeros([batch_size, cell.state_size])\n\ngpu_grads = []  \nlosses = []\nopt = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.99, beta2=0.97)\n\nfor i in xrange(num_gpu):\n        gpu_id = '/gpu:'+str(i)\n        with tf.device(gpu_id), tf.name_scope(gpu_id) as scope:\n                print x[i][0].dtype\n                outputs, state = rnn.rnn(cell=dropout_cell, inputs = x[i], initial_state=initial_state, sequence_length=s[i], scope = scope)\n```\n\nAfter this I get average of gradients similar to cifar10 multi gpu tutorial example.\n"}