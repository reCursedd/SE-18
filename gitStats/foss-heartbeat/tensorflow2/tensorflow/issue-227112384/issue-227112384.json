{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9764", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9764/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9764/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9764/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9764", "id": 227112384, "node_id": "MDU6SXNzdWUyMjcxMTIzODQ=", "number": 9764, "title": "how to call AttentionWrapper?", "user": {"login": "ehsanasgari", "id": 8551117, "node_id": "MDQ6VXNlcjg1NTExMTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/8551117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ehsanasgari", "html_url": "https://github.com/ehsanasgari", "followers_url": "https://api.github.com/users/ehsanasgari/followers", "following_url": "https://api.github.com/users/ehsanasgari/following{/other_user}", "gists_url": "https://api.github.com/users/ehsanasgari/gists{/gist_id}", "starred_url": "https://api.github.com/users/ehsanasgari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ehsanasgari/subscriptions", "organizations_url": "https://api.github.com/users/ehsanasgari/orgs", "repos_url": "https://api.github.com/users/ehsanasgari/repos", "events_url": "https://api.github.com/users/ehsanasgari/events{/privacy}", "received_events_url": "https://api.github.com/users/ehsanasgari/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473173351, "node_id": "MDU6TGFiZWw0NzMxNzMzNTE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install", "name": "type:build/install", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-05-08T17:11:03Z", "updated_at": "2018-08-17T10:14:57Z", "closed_at": "2017-06-16T22:42:40Z", "author_association": "NONE", "body_html": "<p>I am trying to write a simple seq2seq model with attention. But  it gets the following error:</p>\n<pre><code>attn_cell = tf.contrib.seq2seq.AttentionWrapper(\nAttributeError: 'module' object has no attribute 'AttentionWrapper'\n</code></pre>\n<p>How should I call AttentionWrapper?</p>\n<p>Here is my code:</p>\n<pre><code>    T=1000\n    N=100\n    input = tf.placeholder(tf.float32, shape=(N, T, 512), name=\"input_matrix\")\n    seq_lengths = tf.placeholder(tf.int32, shape=(N), name=\"input_lengths\")\n\n    cell= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\n    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell,input, parallel_iterations=32, swap_memory=True, dtype=tf.float32)\n\n    # Attention Mechanisms. Bahdanau is additive style attention\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n        num_units = 100, # depth of query mechanism\n        memory = encoder_outputs, # hidden states to attend (output of RNN)\n        #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories\n        normalize=False, # normalize energy term\n        name='BahdanauAttention')\n\n    cell_out= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\n        # Attention Wrapper: adds the attention mechanism to the cell\n\n    # Attention Wrapper: adds the attention mechanism to the cell\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n        cell = cell,# Instance of RNNCell\n        attention_mechanism = attn_mech, # Instance of AttentionMechanism\n        attention_size = 100, # Int, depth of attention (output) tensor\n        attention_history=False, # whether to store history in final output\n        name=\"attention_wrapper\")\n \n    # TrainingHelper does no sampling, only uses inputs\n    helper = tf.contrib.seq2seq.TrainingHelper(\n        inputs = x, # decoder inputs\n        sequence_length = seq_len_dec, # decoder input length\n        name = \"decoder_training_helper\")\n \n    # Decoder setup\n    decoder = tf.contrib.seq2seq.BasicDecoder(\n              cell = attn_cell,\n              helper = helper, # A Helper instance\n              initial_state = encoder_final_state, # initial state of decoder\n              output_layer = None) # instance of tf.layers.Layer, like Dense\n \n    # Perform dynamic decoding with decoder object\n    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n</code></pre>", "body_text": "I am trying to write a simple seq2seq model with attention. But  it gets the following error:\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(\nAttributeError: 'module' object has no attribute 'AttentionWrapper'\n\nHow should I call AttentionWrapper?\nHere is my code:\n    T=1000\n    N=100\n    input = tf.placeholder(tf.float32, shape=(N, T, 512), name=\"input_matrix\")\n    seq_lengths = tf.placeholder(tf.int32, shape=(N), name=\"input_lengths\")\n\n    cell= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\n    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell,input, parallel_iterations=32, swap_memory=True, dtype=tf.float32)\n\n    # Attention Mechanisms. Bahdanau is additive style attention\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n        num_units = 100, # depth of query mechanism\n        memory = encoder_outputs, # hidden states to attend (output of RNN)\n        #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories\n        normalize=False, # normalize energy term\n        name='BahdanauAttention')\n\n    cell_out= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\n        # Attention Wrapper: adds the attention mechanism to the cell\n\n    # Attention Wrapper: adds the attention mechanism to the cell\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n        cell = cell,# Instance of RNNCell\n        attention_mechanism = attn_mech, # Instance of AttentionMechanism\n        attention_size = 100, # Int, depth of attention (output) tensor\n        attention_history=False, # whether to store history in final output\n        name=\"attention_wrapper\")\n \n    # TrainingHelper does no sampling, only uses inputs\n    helper = tf.contrib.seq2seq.TrainingHelper(\n        inputs = x, # decoder inputs\n        sequence_length = seq_len_dec, # decoder input length\n        name = \"decoder_training_helper\")\n \n    # Decoder setup\n    decoder = tf.contrib.seq2seq.BasicDecoder(\n              cell = attn_cell,\n              helper = helper, # A Helper instance\n              initial_state = encoder_final_state, # initial state of decoder\n              output_layer = None) # instance of tf.layers.Layer, like Dense\n \n    # Perform dynamic decoding with decoder object\n    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)", "body": "I am trying to write a simple seq2seq model with attention. But  it gets the following error: \r\n\r\n```\r\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\nAttributeError: 'module' object has no attribute 'AttentionWrapper'\r\n```\r\n How should I call AttentionWrapper?\r\n\r\n Here is my code:\r\n\r\n```\r\n    T=1000\r\n    N=100\r\n    input = tf.placeholder(tf.float32, shape=(N, T, 512), name=\"input_matrix\")\r\n    seq_lengths = tf.placeholder(tf.int32, shape=(N), name=\"input_lengths\")\r\n\r\n    cell= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\r\n    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell,input, parallel_iterations=32, swap_memory=True, dtype=tf.float32)\r\n\r\n    # Attention Mechanisms. Bahdanau is additive style attention\r\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(\r\n        num_units = 100, # depth of query mechanism\r\n        memory = encoder_outputs, # hidden states to attend (output of RNN)\r\n        #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories\r\n        normalize=False, # normalize energy term\r\n        name='BahdanauAttention')\r\n\r\n    cell_out= MultiRNNCell([DeviceWrapper(ResidualWrapper(LSTMCell(num_units=512)),device='/gpu:%d' %(i+1)) for i in range(2)])\r\n        # Attention Wrapper: adds the attention mechanism to the cell\r\n\r\n    # Attention Wrapper: adds the attention mechanism to the cell\r\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n        cell = cell,# Instance of RNNCell\r\n        attention_mechanism = attn_mech, # Instance of AttentionMechanism\r\n        attention_size = 100, # Int, depth of attention (output) tensor\r\n        attention_history=False, # whether to store history in final output\r\n        name=\"attention_wrapper\")\r\n \r\n    # TrainingHelper does no sampling, only uses inputs\r\n    helper = tf.contrib.seq2seq.TrainingHelper(\r\n        inputs = x, # decoder inputs\r\n        sequence_length = seq_len_dec, # decoder input length\r\n        name = \"decoder_training_helper\")\r\n \r\n    # Decoder setup\r\n    decoder = tf.contrib.seq2seq.BasicDecoder(\r\n              cell = attn_cell,\r\n              helper = helper, # A Helper instance\r\n              initial_state = encoder_final_state, # initial state of decoder\r\n              output_layer = None) # instance of tf.layers.Layer, like Dense\r\n \r\n    # Perform dynamic decoding with decoder object\r\n    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\r\n```"}