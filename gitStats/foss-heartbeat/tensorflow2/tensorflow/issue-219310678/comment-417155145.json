{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417155145", "html_url": "https://github.com/tensorflow/tensorflow/issues/8957#issuecomment-417155145", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8957", "id": 417155145, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzE1NTE0NQ==", "user": {"login": "joe-antognini", "id": 7061933, "node_id": "MDQ6VXNlcjcwNjE5MzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7061933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joe-antognini", "html_url": "https://github.com/joe-antognini", "followers_url": "https://api.github.com/users/joe-antognini/followers", "following_url": "https://api.github.com/users/joe-antognini/following{/other_user}", "gists_url": "https://api.github.com/users/joe-antognini/gists{/gist_id}", "starred_url": "https://api.github.com/users/joe-antognini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joe-antognini/subscriptions", "organizations_url": "https://api.github.com/users/joe-antognini/orgs", "repos_url": "https://api.github.com/users/joe-antognini/repos", "events_url": "https://api.github.com/users/joe-antognini/events{/privacy}", "received_events_url": "https://api.github.com/users/joe-antognini/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-30T00:58:16Z", "updated_at": "2018-08-30T00:58:16Z", "author_association": "NONE", "body_html": "<p>If I understand correctly, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19514328\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xiao-dong\">@xiao-dong</a>, this issue comes up when you try to replicate your model across multiple GPUs.  Suppose you create a model on CPU with <code>net = tf.layers.dense(net, units=128)</code>.  Behind the scenes <code>tf.layers</code> is going to use <code>get_variable</code> to create a variable which may be called, say, <code>dense/kernel</code>.</p>\n<p>Now if you try to do this on multiple GPUs you need to put the same variables on all the GPUs.  The naive way to do this might be to use a loop that looks like this:</p>\n<pre><code>for i in range(num_gpus):\n  with tf.device('/gpu:{}'.format(i)):\n    tf.layers.dense(net, units=128)\n</code></pre>\n<p>But this is going to fail, because on the second iteration of the loop, <code>get_variable</code> is going to try to reuse the same <code>dense/kernel</code> variable as before, but won't be able to because it's not shared.  If you were to simply wrap this with</p>\n<pre><code>for i in range(num_gpus):\n  with tf.variable_scope('my_scope', reuse=True), tf.device('/gpu:{}'.format(i)):\n    tf.layers.dense(net, units=128)\n</code></pre>\n<p>this also will not work because <code>get_variable</code> will try to reuse a variable that does not exist on the first iteration of the loop.  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a>'s solution is to not reuse the variable on the first iteration of the loop, but then reuse it on all others:</p>\n<pre><code>for i in range(num_gpus):\n  with tf.variable_scope('my_scope', reuse=(i &gt; 0)), tf.device('/gpu:{}'.format(i)):\n    tf.layers.dense(net, units=128)\n</code></pre>", "body_text": "If I understand correctly, @xiao-dong, this issue comes up when you try to replicate your model across multiple GPUs.  Suppose you create a model on CPU with net = tf.layers.dense(net, units=128).  Behind the scenes tf.layers is going to use get_variable to create a variable which may be called, say, dense/kernel.\nNow if you try to do this on multiple GPUs you need to put the same variables on all the GPUs.  The naive way to do this might be to use a loop that looks like this:\nfor i in range(num_gpus):\n  with tf.device('/gpu:{}'.format(i)):\n    tf.layers.dense(net, units=128)\n\nBut this is going to fail, because on the second iteration of the loop, get_variable is going to try to reuse the same dense/kernel variable as before, but won't be able to because it's not shared.  If you were to simply wrap this with\nfor i in range(num_gpus):\n  with tf.variable_scope('my_scope', reuse=True), tf.device('/gpu:{}'.format(i)):\n    tf.layers.dense(net, units=128)\n\nthis also will not work because get_variable will try to reuse a variable that does not exist on the first iteration of the loop.  @lukaszkaiser's solution is to not reuse the variable on the first iteration of the loop, but then reuse it on all others:\nfor i in range(num_gpus):\n  with tf.variable_scope('my_scope', reuse=(i > 0)), tf.device('/gpu:{}'.format(i)):\n    tf.layers.dense(net, units=128)", "body": "If I understand correctly, @xiao-dong, this issue comes up when you try to replicate your model across multiple GPUs.  Suppose you create a model on CPU with `net = tf.layers.dense(net, units=128)`.  Behind the scenes `tf.layers` is going to use `get_variable` to create a variable which may be called, say, `dense/kernel`.\r\n\r\nNow if you try to do this on multiple GPUs you need to put the same variables on all the GPUs.  The naive way to do this might be to use a loop that looks like this:\r\n\r\n```\r\nfor i in range(num_gpus):\r\n  with tf.device('/gpu:{}'.format(i)):\r\n    tf.layers.dense(net, units=128)\r\n```\r\n\r\nBut this is going to fail, because on the second iteration of the loop, `get_variable` is going to try to reuse the same `dense/kernel` variable as before, but won't be able to because it's not shared.  If you were to simply wrap this with\r\n\r\n```\r\nfor i in range(num_gpus):\r\n  with tf.variable_scope('my_scope', reuse=True), tf.device('/gpu:{}'.format(i)):\r\n    tf.layers.dense(net, units=128)\r\n```\r\n\r\nthis also will not work because `get_variable` will try to reuse a variable that does not exist on the first iteration of the loop.  @lukaszkaiser's solution is to not reuse the variable on the first iteration of the loop, but then reuse it on all others:\r\n\r\n```\r\nfor i in range(num_gpus):\r\n  with tf.variable_scope('my_scope', reuse=(i > 0)), tf.device('/gpu:{}'.format(i)):\r\n    tf.layers.dense(net, units=128)\r\n```"}