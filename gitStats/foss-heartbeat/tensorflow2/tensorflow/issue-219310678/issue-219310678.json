{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8957", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8957/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8957/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8957/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8957", "id": 219310678, "node_id": "MDU6SXNzdWUyMTkzMTA2Nzg=", "number": 8957, "title": "Tensorflow multi-GPU training and variable scope", "user": {"login": "jesryu", "id": 25464992, "node_id": "MDQ6VXNlcjI1NDY0OTky", "avatar_url": "https://avatars0.githubusercontent.com/u/25464992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jesryu", "html_url": "https://github.com/jesryu", "followers_url": "https://api.github.com/users/jesryu/followers", "following_url": "https://api.github.com/users/jesryu/following{/other_user}", "gists_url": "https://api.github.com/users/jesryu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jesryu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jesryu/subscriptions", "organizations_url": "https://api.github.com/users/jesryu/orgs", "repos_url": "https://api.github.com/users/jesryu/repos", "events_url": "https://api.github.com/users/jesryu/events{/privacy}", "received_events_url": "https://api.github.com/users/jesryu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-04-04T16:35:14Z", "updated_at": "2018-09-04T04:47:18Z", "closed_at": "2017-04-04T18:28:09Z", "author_association": "NONE", "body_html": "<h3>Context</h3>\n<p>I'm working on a detector model on multiple GPUs using Tensorflow 1.0. As suggested <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\">here</a>, Gradients are computed on multiple GPUs individually and are averaged on CPU. To share the trainable variables (e.g. weights and biases) across the GPU towers, the <code>reuse</code> flag is turned on using <code>tf.get_variable_scope().reuse_variables()</code>, as in the cifar10 example. The difference is that I am using an <code>AdamOptimizer</code> instead of <code>GradientDescentOptimizer</code>.</p>\n<h3>Problem</h3>\n<p>When I run the training job, it prints out a long stacktrace and raise the following error at <code>opt.apply_gradients()</code>:</p>\n<pre><code>ValueError: Variable conv1_1/kernel/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n</code></pre>\n<p>Looking into the source code I found that the <code>AdamOptimizer</code> is creating a number of zero-initialized slots within the <code>_create_slots()</code> method, wherein it calls the <code>_zeros_slot()</code>. This calls a separate module called the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L62\"><code>slot_creator</code></a> (source code linked).</p>\n<p><strong>In <code>line 62</code> of the <code>slot_creator</code>, it uses <code>variable_scope.get_variable()</code>. This used to be <code>tf.Variable()</code> in 0.12.</strong></p>\n<p>My understanding of variable scopes is that <code>variable_scope.get_variable()</code> would fail to create a variable <strong>if <code>reuse</code> flag is on`. See <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L669\">here</a> for source code.</strong></p>\n<p>But the cifar10 example by Tensorflow creators seems to suggest enabling reuse to share variables across the GPU towers using <code>tf.get_variable_scope().reuse_variables()</code>. This happens <strong>before</strong> we average and apply the gradients. It looks like Tensorflow 1.0 refuses to create variables for the <code>AdamOptimizer</code>.</p>\n<p>This happens for all optimizers that directly or indirectly call the <code>slot_creator</code> module.</p>\n<h3>Question</h3>\n<p>As a quick fix, I added a custom function into the <code>VariableScope</code> class to disable the <code>_reuse</code> flag right before calling <code>opt.apply_gradients</code>. However, I am sure there is a merit to forcing the <code>reuse</code> flag to be only set to <code>True</code>. I am not sure what the better workaround would be. Any suggestions?</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I created question <a href=\"http://stackoverflow.com/questions/43212725/tensorflow-multi-gpu-training-and-variable-scope\" rel=\"nofollow\">here</a></p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04 LTS</p>\n<p>Installed version of CUDA and cuDNN: 8.0 and 5.1 respectively<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<p>The commit hash (<code>git rev-parse HEAD</code>): <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/1cb96893a64f59b7265f9def9968f7bed1e57662/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/1cb96893a64f59b7265f9def9968f7bed1e57662\"><tt>1cb9689</tt></a>#diff-a25cc0fc9d475568e0069e5dc0b67d85 (see <code>slot_creator</code>)</p>", "body_text": "Context\nI'm working on a detector model on multiple GPUs using Tensorflow 1.0. As suggested here, Gradients are computed on multiple GPUs individually and are averaged on CPU. To share the trainable variables (e.g. weights and biases) across the GPU towers, the reuse flag is turned on using tf.get_variable_scope().reuse_variables(), as in the cifar10 example. The difference is that I am using an AdamOptimizer instead of GradientDescentOptimizer.\nProblem\nWhen I run the training job, it prints out a long stacktrace and raise the following error at opt.apply_gradients():\nValueError: Variable conv1_1/kernel/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n\nLooking into the source code I found that the AdamOptimizer is creating a number of zero-initialized slots within the _create_slots() method, wherein it calls the _zeros_slot(). This calls a separate module called the slot_creator (source code linked).\nIn line 62 of the slot_creator, it uses variable_scope.get_variable(). This used to be tf.Variable() in 0.12.\nMy understanding of variable scopes is that variable_scope.get_variable() would fail to create a variable if reuse flag is on`. See here for source code.\nBut the cifar10 example by Tensorflow creators seems to suggest enabling reuse to share variables across the GPU towers using tf.get_variable_scope().reuse_variables(). This happens before we average and apply the gradients. It looks like Tensorflow 1.0 refuses to create variables for the AdamOptimizer.\nThis happens for all optimizers that directly or indirectly call the slot_creator module.\nQuestion\nAs a quick fix, I added a custom function into the VariableScope class to disable the _reuse flag right before calling opt.apply_gradients. However, I am sure there is a merit to forcing the reuse flag to be only set to True. I am not sure what the better workaround would be. Any suggestions?\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI created question here\nEnvironment info\nOperating System: Ubuntu 14.04 LTS\nInstalled version of CUDA and cuDNN: 8.0 and 5.1 respectively\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nThe commit hash (git rev-parse HEAD): 1cb9689#diff-a25cc0fc9d475568e0069e5dc0b67d85 (see slot_creator)", "body": "### Context\r\n\r\nI'm working on a detector model on multiple GPUs using Tensorflow 1.0. As suggested [here](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py), Gradients are computed on multiple GPUs individually and are averaged on CPU. To share the trainable variables (e.g. weights and biases) across the GPU towers, the `reuse` flag is turned on using `tf.get_variable_scope().reuse_variables()`, as in the cifar10 example. The difference is that I am using an `AdamOptimizer` instead of `GradientDescentOptimizer`.\r\n\r\n\r\n### Problem\r\n\r\nWhen I run the training job, it prints out a long stacktrace and raise the following error at `opt.apply_gradients()`:\r\n\r\n```\r\nValueError: Variable conv1_1/kernel/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\n\r\nLooking into the source code I found that the `AdamOptimizer` is creating a number of zero-initialized slots within the `_create_slots()` method, wherein it calls the `_zeros_slot()`. This calls a separate module called the [`slot_creator`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L62) (source code linked).\r\n\r\n**In `line 62` of the `slot_creator`, it uses `variable_scope.get_variable()`. This used to be `tf.Variable()` in 0.12.**\r\n\r\nMy understanding of variable scopes is that `variable_scope.get_variable()` would fail to create a variable **if `reuse` flag is on`. See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L669) for source code.**\r\n\r\nBut the cifar10 example by Tensorflow creators seems to suggest enabling reuse to share variables across the GPU towers using `tf.get_variable_scope().reuse_variables()`. This happens **before** we average and apply the gradients. It looks like Tensorflow 1.0 refuses to create variables for the `AdamOptimizer`.\r\n\r\nThis happens for all optimizers that directly or indirectly call the `slot_creator` module.\r\n\r\n### Question\r\nAs a quick fix, I added a custom function into the `VariableScope` class to disable the `_reuse` flag right before calling `opt.apply_gradients`. However, I am sure there is a merit to forcing the `reuse` flag to be only set to `True`. I am not sure what the better workaround would be. Any suggestions?\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI created question [here](http://stackoverflow.com/questions/43212725/tensorflow-multi-gpu-training-and-variable-scope)\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5.1 respectively\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nThe commit hash (`git rev-parse HEAD`): 1cb96893a64f59b7265f9def9968f7bed1e57662#diff-a25cc0fc9d475568e0069e5dc0b67d85 (see `slot_creator`)"}