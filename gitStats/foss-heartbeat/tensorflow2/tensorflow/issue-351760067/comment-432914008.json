{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/432914008", "html_url": "https://github.com/tensorflow/tensorflow/issues/21693#issuecomment-432914008", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21693", "id": 432914008, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjkxNDAwOA==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-25T04:52:02Z", "updated_at": "2018-10-25T04:52:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hey <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a>! I recently made a big update to TF Scala (<a href=\"https://github.com/eaplatanios/tensorflow_scala/pull/131\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/eaplatanios/tensorflow_scala/pull/131/hovercard\">PR</a>) that makes all of the graph construction API statically-typed (finally :))). This raised the following question that relates to this issue: whenever a shape is used when a tensor is expected, I implicitly convert the shape to a tensor (similarly to what the Python API does). However, I was converting to <code>Tensor[Long]</code> previously, because I had assumed shapes are represented as <code>int64</code> tensors. If I keep doing that though I run into issues like the one described above.</p>\n<p>So, what should be the convention for implicit conversion of shapes to tensors? Should they be converted to <code>int32</code> tensors or <code>int64</code> tensors?</p>\n<p>I believe it would be good to make a decision about this and stick with it, given the awkward handling of <code>int32</code> tensors when it comes to GPUs. Also, what is currently happening in the Python API? Is it <code>int32</code> or <code>int64</code> that is used for these implicit conversions?</p>\n<p>Thanks! :)</p>", "body_text": "Hey @alextp! I recently made a big update to TF Scala (PR) that makes all of the graph construction API statically-typed (finally :))). This raised the following question that relates to this issue: whenever a shape is used when a tensor is expected, I implicitly convert the shape to a tensor (similarly to what the Python API does). However, I was converting to Tensor[Long] previously, because I had assumed shapes are represented as int64 tensors. If I keep doing that though I run into issues like the one described above.\nSo, what should be the convention for implicit conversion of shapes to tensors? Should they be converted to int32 tensors or int64 tensors?\nI believe it would be good to make a decision about this and stick with it, given the awkward handling of int32 tensors when it comes to GPUs. Also, what is currently happening in the Python API? Is it int32 or int64 that is used for these implicit conversions?\nThanks! :)", "body": "Hey @alextp! I recently made a big update to TF Scala ([PR](https://github.com/eaplatanios/tensorflow_scala/pull/131)) that makes all of the graph construction API statically-typed (finally :))). This raised the following question that relates to this issue: whenever a shape is used when a tensor is expected, I implicitly convert the shape to a tensor (similarly to what the Python API does). However, I was converting to `Tensor[Long]` previously, because I had assumed shapes are represented as `int64` tensors. If I keep doing that though I run into issues like the one described above. \r\n\r\nSo, what should be the convention for implicit conversion of shapes to tensors? Should they be converted to `int32` tensors or `int64` tensors?\r\n\r\nI believe it would be good to make a decision about this and stick with it, given the awkward handling of `int32` tensors when it comes to GPUs. Also, what is currently happening in the Python API? Is it `int32` or `int64` that is used for these implicit conversions?\r\n\r\nThanks! :)"}