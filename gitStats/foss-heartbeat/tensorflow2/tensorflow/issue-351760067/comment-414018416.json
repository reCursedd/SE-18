{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/414018416", "html_url": "https://github.com/tensorflow/tensorflow/issues/21693#issuecomment-414018416", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21693", "id": 414018416, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDAxODQxNg==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-18T00:29:27Z", "updated_at": "2018-08-18T00:31:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For now I have temporarily changed my API so all shape inputs to RandomUniform are cast to int32 automatically. I believe that allowing int64 is not an elegant solution given the whole weirdness with int32 tensor support in GPUs. I believe that it would be more appropriate if TensorFlow constrained shape-related tensors to be int32 (e.g., that argument to the random uniform op), or documented somewhere what the deal is with int32 vs int64. I have some understanding currently of the underlying reasons, but it easily leads to suboptimal performance that is hard to debug. My MT model from processing 9K words per second while training to 25K once I forced that int32 cast <strong>(note that this is a 3x performance gain achieved by adding a cast -- also note that I have 4 GPUs on that machine and now I can train 4 models in parallel at 25K words per second each, whereas before I would get ~4K words per second each because the CPU was becoming a constraint)</strong>. This was due to colocation constraints propagating but I don't believe something like that should happen so easily.</p>", "body_text": "For now I have temporarily changed my API so all shape inputs to RandomUniform are cast to int32 automatically. I believe that allowing int64 is not an elegant solution given the whole weirdness with int32 tensor support in GPUs. I believe that it would be more appropriate if TensorFlow constrained shape-related tensors to be int32 (e.g., that argument to the random uniform op), or documented somewhere what the deal is with int32 vs int64. I have some understanding currently of the underlying reasons, but it easily leads to suboptimal performance that is hard to debug. My MT model from processing 9K words per second while training to 25K once I forced that int32 cast (note that this is a 3x performance gain achieved by adding a cast -- also note that I have 4 GPUs on that machine and now I can train 4 models in parallel at 25K words per second each, whereas before I would get ~4K words per second each because the CPU was becoming a constraint). This was due to colocation constraints propagating but I don't believe something like that should happen so easily.", "body": "For now I have temporarily changed my API so all shape inputs to RandomUniform are cast to int32 automatically. I believe that allowing int64 is not an elegant solution given the whole weirdness with int32 tensor support in GPUs. I believe that it would be more appropriate if TensorFlow constrained shape-related tensors to be int32 (e.g., that argument to the random uniform op), or documented somewhere what the deal is with int32 vs int64. I have some understanding currently of the underlying reasons, but it easily leads to suboptimal performance that is hard to debug. My MT model from processing 9K words per second while training to 25K once I forced that int32 cast **(note that this is a 3x performance gain achieved by adding a cast -- also note that I have 4 GPUs on that machine and now I can train 4 models in parallel at 25K words per second each, whereas before I would get ~4K words per second each because the CPU was becoming a constraint)**. This was due to colocation constraints propagating but I don't believe something like that should happen so easily."}