{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/382500520", "html_url": "https://github.com/tensorflow/tensorflow/pull/15109#issuecomment-382500520", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15109", "id": 382500520, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjUwMDUyMA==", "user": {"login": "bowang", "id": 425637, "node_id": "MDQ6VXNlcjQyNTYzNw==", "avatar_url": "https://avatars3.githubusercontent.com/u/425637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bowang", "html_url": "https://github.com/bowang", "followers_url": "https://api.github.com/users/bowang/followers", "following_url": "https://api.github.com/users/bowang/following{/other_user}", "gists_url": "https://api.github.com/users/bowang/gists{/gist_id}", "starred_url": "https://api.github.com/users/bowang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bowang/subscriptions", "organizations_url": "https://api.github.com/users/bowang/orgs", "repos_url": "https://api.github.com/users/bowang/repos", "events_url": "https://api.github.com/users/bowang/events{/privacy}", "received_events_url": "https://api.github.com/users/bowang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-18T19:22:54Z", "updated_at": "2018-04-18T19:22:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> , I know it has been a while. Does the use case in my previous comment makes sense?<br>\nWith this feature we can get rid of the unnecessary variable scope when training on multiple GPUs and generate the same graph as we do on a single GPU.</p>", "body_text": "Hi @asimshankar , I know it has been a while. Does the use case in my previous comment makes sense?\nWith this feature we can get rid of the unnecessary variable scope when training on multiple GPUs and generate the same graph as we do on a single GPU.", "body": "Hi @asimshankar , I know it has been a while. Does the use case in my previous comment makes sense?\r\nWith this feature we can get rid of the unnecessary variable scope when training on multiple GPUs and generate the same graph as we do on a single GPU."}