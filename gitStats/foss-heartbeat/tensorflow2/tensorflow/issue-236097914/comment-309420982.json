{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309420982", "html_url": "https://github.com/tensorflow/tensorflow/issues/10723#issuecomment-309420982", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10723", "id": 309420982, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTQyMDk4Mg==", "user": {"login": "GD06", "id": 13307515, "node_id": "MDQ6VXNlcjEzMzA3NTE1", "avatar_url": "https://avatars2.githubusercontent.com/u/13307515?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GD06", "html_url": "https://github.com/GD06", "followers_url": "https://api.github.com/users/GD06/followers", "following_url": "https://api.github.com/users/GD06/following{/other_user}", "gists_url": "https://api.github.com/users/GD06/gists{/gist_id}", "starred_url": "https://api.github.com/users/GD06/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GD06/subscriptions", "organizations_url": "https://api.github.com/users/GD06/orgs", "repos_url": "https://api.github.com/users/GD06/repos", "events_url": "https://api.github.com/users/GD06/events{/privacy}", "received_events_url": "https://api.github.com/users/GD06/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-19T12:09:04Z", "updated_at": "2017-06-19T12:09:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2613663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/byronyi\">@byronyi</a><br>\nThanks for your kind remind! I am sorry that I didn't claim the problem very clearly. To be more specific, my question is that what's the difference causing so significant performance difference. To my poor understanding, both of programs resulting in the second and fourth column data aggregate gradients on one GPU and use NCCL for the reduction and they have the same amount of data transfer. The program using replicated variable updates needs to transfer gradients from each GPU and aggregated gradients to each GPU. While the program using parameter server for variable updates needs to transfer gradients from each GPU and updated gradients from the parameter server. The only difference is that whether each GPU has a local copy of weights. I think the overhead of multiGPU mainly comes from the communication. However, with the amount of transferred data the same, these two programs still have so large performance difference.</p>\n<p>Above stated is exactly what I am confused. I doubt I have an incorrect understanding of these two modes. Thanks for your patience!</p>", "body_text": "@byronyi\nThanks for your kind remind! I am sorry that I didn't claim the problem very clearly. To be more specific, my question is that what's the difference causing so significant performance difference. To my poor understanding, both of programs resulting in the second and fourth column data aggregate gradients on one GPU and use NCCL for the reduction and they have the same amount of data transfer. The program using replicated variable updates needs to transfer gradients from each GPU and aggregated gradients to each GPU. While the program using parameter server for variable updates needs to transfer gradients from each GPU and updated gradients from the parameter server. The only difference is that whether each GPU has a local copy of weights. I think the overhead of multiGPU mainly comes from the communication. However, with the amount of transferred data the same, these two programs still have so large performance difference.\nAbove stated is exactly what I am confused. I doubt I have an incorrect understanding of these two modes. Thanks for your patience!", "body": "@byronyi \r\nThanks for your kind remind! I am sorry that I didn't claim the problem very clearly. To be more specific, my question is that what's the difference causing so significant performance difference. To my poor understanding, both of programs resulting in the second and fourth column data aggregate gradients on one GPU and use NCCL for the reduction and they have the same amount of data transfer. The program using replicated variable updates needs to transfer gradients from each GPU and aggregated gradients to each GPU. While the program using parameter server for variable updates needs to transfer gradients from each GPU and updated gradients from the parameter server. The only difference is that whether each GPU has a local copy of weights. I think the overhead of multiGPU mainly comes from the communication. However, with the amount of transferred data the same, these two programs still have so large performance difference.\r\n\r\nAbove stated is exactly what I am confused. I doubt I have an incorrect understanding of these two modes. Thanks for your patience! "}