{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10723", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10723/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10723/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10723/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10723", "id": 236097914, "node_id": "MDU6SXNzdWUyMzYwOTc5MTQ=", "number": 10723, "title": "Poor Scalability of TensorFlow MultiGPU Training on a Single Machine [Performance Bug]", "user": {"login": "GD06", "id": 13307515, "node_id": "MDQ6VXNlcjEzMzA3NTE1", "avatar_url": "https://avatars2.githubusercontent.com/u/13307515?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GD06", "html_url": "https://github.com/GD06", "followers_url": "https://api.github.com/users/GD06/followers", "following_url": "https://api.github.com/users/GD06/following{/other_user}", "gists_url": "https://api.github.com/users/GD06/gists{/gist_id}", "starred_url": "https://api.github.com/users/GD06/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GD06/subscriptions", "organizations_url": "https://api.github.com/users/GD06/orgs", "repos_url": "https://api.github.com/users/GD06/repos", "events_url": "https://api.github.com/users/GD06/events{/privacy}", "received_events_url": "https://api.github.com/users/GD06/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-06-15T07:14:49Z", "updated_at": "2018-10-03T15:52:49Z", "closed_at": "2017-06-17T00:01:37Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Debian 3.16.7</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>: source, hash: b1e174e</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>: v1.1.0-rc2-119-gb1e174e 1.1.0-rc2</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:  0.4.5-jdk7</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>: CUDA 8.0, cudnn-v5.1</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>: Titan X with 12 GiB</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>: ./tf_multiGPU.py</p>\n</li>\n<li>\n<p><strong>tensorflow/benchmark version</strong>: source, hash: 9165a70</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Recently, we are testing the TensorFlow scalability on multiGPU machines. We use the official scripts provided in the <a href=\"https://www.tensorflow.org/performance/benchmarks\" rel=\"nofollow\">benchmarks</a> using codes from GitHub repository tensorflow/benchmark. We execute the script according to the official website to test the scalability of TensorFlow on the machine equipped with 8 Titan X GPUs. We test the model VGG16 with batch size equaling to 64.</p>\n<p>The results are shown in the following table:</p>\n<table>\n<thead>\n<tr>\n<th>Num of GPUs</th>\n<th>Throughput (images/sec)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>83.13</td>\n</tr>\n<tr>\n<td>2</td>\n<td>155.06</td>\n</tr>\n<tr>\n<td>3</td>\n<td>211.8</td>\n</tr>\n<tr>\n<td>4</td>\n<td>278.51</td>\n</tr>\n<tr>\n<td>5</td>\n<td>265.53</td>\n</tr>\n<tr>\n<td>6</td>\n<td>268.19</td>\n</tr>\n<tr>\n<td>7</td>\n<td>272.8</td>\n</tr>\n<tr>\n<td>8</td>\n<td>302.27</td>\n</tr>\n</tbody>\n</table>\n<p>We are surprised to find that the total throughput of 5 GPUs is smaller than 4 GPUs, which means TensorFlow incurs significant overheads when the number of GPU is larger than 4. Because I don't know whether this performance issue belongs to the tensorflow/tensorflow repository or tensorflow/benchmark repository, I submit this issue here looking forward to the official response. The script for reproducing this issue can be found in Source code/logs section with the results.</p>\n<p>The scalability is strongly related to the topology of GPU interconnection. In our machine, we have a tree topology for GPUs. The topology details can be found at the <a href=\"https://gist.github.com/583d689fd16ee24f1924b83ca3dea5b9.git\">nv-topo-matrix.txt</a></p>\n<p>We believe this is a performance bug since if more GPUs can not achieve higher throughput, at least they should obtain similar throughput.</p>\n<h3>Source code / logs</h3>\n<p>Source code: <a href=\"https://gist.github.com/b4024e1d8bfbbdf0cf917798d677aaae.git\">tf_multiGPU.py</a><br>\nThe log after executing the script above: <a href=\"https://drive.google.com/file/d/0Bw3_V-EwBVToYXVqZDkzQkN6c3M/view?usp=sharing\" rel=\"nofollow\">https://drive.google.com/file/d/0Bw3_V-EwBVToYXVqZDkzQkN6c3M/view?usp=sharing</a></p>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 3.16.7\n\n\nTensorFlow installed from (source or binary): source, hash: b1e174e\n\n\nTensorFlow version (use command below): v1.1.0-rc2-119-gb1e174e 1.1.0-rc2\n\n\nBazel version (if compiling from source):  0.4.5-jdk7\n\n\nCUDA/cuDNN version: CUDA 8.0, cudnn-v5.1\n\n\nGPU model and memory: Titan X with 12 GiB\n\n\nExact command to reproduce: ./tf_multiGPU.py\n\n\ntensorflow/benchmark version: source, hash: 9165a70\n\n\nDescribe the problem\nRecently, we are testing the TensorFlow scalability on multiGPU machines. We use the official scripts provided in the benchmarks using codes from GitHub repository tensorflow/benchmark. We execute the script according to the official website to test the scalability of TensorFlow on the machine equipped with 8 Titan X GPUs. We test the model VGG16 with batch size equaling to 64.\nThe results are shown in the following table:\n\n\n\nNum of GPUs\nThroughput (images/sec)\n\n\n\n\n1\n83.13\n\n\n2\n155.06\n\n\n3\n211.8\n\n\n4\n278.51\n\n\n5\n265.53\n\n\n6\n268.19\n\n\n7\n272.8\n\n\n8\n302.27\n\n\n\nWe are surprised to find that the total throughput of 5 GPUs is smaller than 4 GPUs, which means TensorFlow incurs significant overheads when the number of GPU is larger than 4. Because I don't know whether this performance issue belongs to the tensorflow/tensorflow repository or tensorflow/benchmark repository, I submit this issue here looking forward to the official response. The script for reproducing this issue can be found in Source code/logs section with the results.\nThe scalability is strongly related to the topology of GPU interconnection. In our machine, we have a tree topology for GPUs. The topology details can be found at the nv-topo-matrix.txt\nWe believe this is a performance bug since if more GPUs can not achieve higher throughput, at least they should obtain similar throughput.\nSource code / logs\nSource code: tf_multiGPU.py\nThe log after executing the script above: https://drive.google.com/file/d/0Bw3_V-EwBVToYXVqZDkzQkN6c3M/view?usp=sharing", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 3.16.7\r\n- **TensorFlow installed from (source or binary)**: source, hash: b1e174e\r\n- **TensorFlow version (use command below)**: v1.1.0-rc2-119-gb1e174e 1.1.0-rc2\r\n- **Bazel version (if compiling from source)**:  0.4.5-jdk7\r\n- **CUDA/cuDNN version**: CUDA 8.0, cudnn-v5.1\r\n- **GPU model and memory**: Titan X with 12 GiB\r\n- **Exact command to reproduce**: ./tf_multiGPU.py\r\n\r\n- **tensorflow/benchmark version**: source, hash: 9165a70\r\n\r\n### Describe the problem\r\nRecently, we are testing the TensorFlow scalability on multiGPU machines. We use the official scripts provided in the [benchmarks](https://www.tensorflow.org/performance/benchmarks) using codes from GitHub repository tensorflow/benchmark. We execute the script according to the official website to test the scalability of TensorFlow on the machine equipped with 8 Titan X GPUs. We test the model VGG16 with batch size equaling to 64.\r\n\r\nThe results are shown in the following table:\r\n\r\n| Num of GPUs | Throughput (images/sec) |\r\n|-------------|-------------------------|\r\n| 1           | 83.13                   |\r\n| 2           | 155.06                  |\r\n| 3           | 211.8                   |\r\n| 4           | 278.51                  |\r\n| 5           | 265.53                  |\r\n| 6           | 268.19                  |\r\n| 7           | 272.8                   |\r\n| 8           | 302.27                  |\r\n\r\nWe are surprised to find that the total throughput of 5 GPUs is smaller than 4 GPUs, which means TensorFlow incurs significant overheads when the number of GPU is larger than 4. Because I don't know whether this performance issue belongs to the tensorflow/tensorflow repository or tensorflow/benchmark repository, I submit this issue here looking forward to the official response. The script for reproducing this issue can be found in Source code/logs section with the results.\r\n\r\nThe scalability is strongly related to the topology of GPU interconnection. In our machine, we have a tree topology for GPUs. The topology details can be found at the [nv-topo-matrix.txt](https://gist.github.com/583d689fd16ee24f1924b83ca3dea5b9.git)\r\n\r\nWe believe this is a performance bug since if more GPUs can not achieve higher throughput, at least they should obtain similar throughput. \r\n\r\n### Source code / logs\r\nSource code: [tf_multiGPU.py](https://gist.github.com/b4024e1d8bfbbdf0cf917798d677aaae.git)\r\nThe log after executing the script above: https://drive.google.com/file/d/0Bw3_V-EwBVToYXVqZDkzQkN6c3M/view?usp=sharing"}