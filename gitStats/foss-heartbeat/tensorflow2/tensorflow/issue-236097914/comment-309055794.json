{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309055794", "html_url": "https://github.com/tensorflow/tensorflow/issues/10723#issuecomment-309055794", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10723", "id": 309055794, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTA1NTc5NA==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-16T15:24:13Z", "updated_at": "2017-06-16T15:24:13Z", "author_association": "MEMBER", "body_html": "<p>I would have to look closer but I do not find this completely surprising but your number is lower than I would expect given my experience doing the benchmarks..  I believe VGG16 has a lot of parameters.  Did you try different parameter server approaches?  On the DGX-1 replicated (with NCCL) was the best approach for VGG16.  parameter_server on gpu was only a good option on AWS where they have GPUDirect.    I would try the following options when using VGG16:</p>\n<pre><code># My guess is with your ENV this will most likely to give you the best scaling for VGG16\n- variable_update = replicated  local_parameter_device = gpu  use_nccl = true\nor\n# this is a good fall back in most environments.  \n- variable_update = parameter_server  local_parameter_device = cpu\n</code></pre>\n<p>If your Titan X setup is more similar to a DGX-1 in topology, I no longer have access and did not research it then the nccl setup will work best.  If peering is not setup perfectly then cpu is often the best option.  Based on your starting number I would guess you should be able to get over 500 images/sec with one of the options that works best with your topology.</p>\n<p>Also VGG16 does not scale as well as the other models as the GPUs speedup.  Here is the <a href=\"https://www.tensorflow.org/performance/benchmarks#results\" rel=\"nofollow\">graph</a>, with real data it starts to dip and you only hit something like 5.8 or so speedup.  On K80s (much slower GPUs)  the scaling is over 6x on GCE.  I also do not see many people showing VGG16 distributed numbers and from what I have read it is the same reason.  The large number of parameters makes it difficult to scale.</p>\n<p>NCCL is suppose to \"magically\" solve these types of problems but that does not seem to be the case currently.  My testing showed that the best performance requires trying a few options.</p>\n<p>I have run other platforms and this can happen on them as well if using an variable placement option that does not work with with the hardware platform.</p>\n<p>I want to stress, these are guesses based on my experience testing on a variety of systems but not remotely all of them.  When it is a system I can access, it is much easier to speak in absolutes.  I hope you get a better speedup trying the other options.</p>", "body_text": "I would have to look closer but I do not find this completely surprising but your number is lower than I would expect given my experience doing the benchmarks..  I believe VGG16 has a lot of parameters.  Did you try different parameter server approaches?  On the DGX-1 replicated (with NCCL) was the best approach for VGG16.  parameter_server on gpu was only a good option on AWS where they have GPUDirect.    I would try the following options when using VGG16:\n# My guess is with your ENV this will most likely to give you the best scaling for VGG16\n- variable_update = replicated  local_parameter_device = gpu  use_nccl = true\nor\n# this is a good fall back in most environments.  \n- variable_update = parameter_server  local_parameter_device = cpu\n\nIf your Titan X setup is more similar to a DGX-1 in topology, I no longer have access and did not research it then the nccl setup will work best.  If peering is not setup perfectly then cpu is often the best option.  Based on your starting number I would guess you should be able to get over 500 images/sec with one of the options that works best with your topology.\nAlso VGG16 does not scale as well as the other models as the GPUs speedup.  Here is the graph, with real data it starts to dip and you only hit something like 5.8 or so speedup.  On K80s (much slower GPUs)  the scaling is over 6x on GCE.  I also do not see many people showing VGG16 distributed numbers and from what I have read it is the same reason.  The large number of parameters makes it difficult to scale.\nNCCL is suppose to \"magically\" solve these types of problems but that does not seem to be the case currently.  My testing showed that the best performance requires trying a few options.\nI have run other platforms and this can happen on them as well if using an variable placement option that does not work with with the hardware platform.\nI want to stress, these are guesses based on my experience testing on a variety of systems but not remotely all of them.  When it is a system I can access, it is much easier to speak in absolutes.  I hope you get a better speedup trying the other options.", "body": "I would have to look closer but I do not find this completely surprising but your number is lower than I would expect given my experience doing the benchmarks..  I believe VGG16 has a lot of parameters.  Did you try different parameter server approaches?  On the DGX-1 replicated (with NCCL) was the best approach for VGG16.  parameter_server on gpu was only a good option on AWS where they have GPUDirect.    I would try the following options when using VGG16:\r\n\r\n```\r\n# My guess is with your ENV this will most likely to give you the best scaling for VGG16\r\n- variable_update = replicated  local_parameter_device = gpu  use_nccl = true\r\nor\r\n# this is a good fall back in most environments.  \r\n- variable_update = parameter_server  local_parameter_device = cpu\r\n```\r\n\r\nIf your Titan X setup is more similar to a DGX-1 in topology, I no longer have access and did not research it then the nccl setup will work best.  If peering is not setup perfectly then cpu is often the best option.  Based on your starting number I would guess you should be able to get over 500 images/sec with one of the options that works best with your topology.  \r\n\r\nAlso VGG16 does not scale as well as the other models as the GPUs speedup.  Here is the [graph](https://www.tensorflow.org/performance/benchmarks#results), with real data it starts to dip and you only hit something like 5.8 or so speedup.  On K80s (much slower GPUs)  the scaling is over 6x on GCE.  I also do not see many people showing VGG16 distributed numbers and from what I have read it is the same reason.  The large number of parameters makes it difficult to scale.  \r\n\r\nNCCL is suppose to \"magically\" solve these types of problems but that does not seem to be the case currently.  My testing showed that the best performance requires trying a few options.  \r\n\r\nI have run other platforms and this can happen on them as well if using an variable placement option that does not work with with the hardware platform.\r\n\r\nI want to stress, these are guesses based on my experience testing on a variety of systems but not remotely all of them.  When it is a system I can access, it is much easier to speak in absolutes.  I hope you get a better speedup trying the other options.  \r\n\r\n"}