{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299676879", "html_url": "https://github.com/tensorflow/tensorflow/issues/9370#issuecomment-299676879", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9370", "id": 299676879, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY3Njg3OQ==", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-07T01:51:42Z", "updated_at": "2017-05-07T01:51:42Z", "author_association": "NONE", "body_html": "<p>May I suggest that this gets documented very clearly as a limitation, perhaps even preventing the user from accidentally using RNNParamsSaver with a bidirectional RNN? I understand the temptation to mark this as contributions welcome and set it aside, but I really do think it's a very serious bug. What makes it particularly problematic is that it's completely silent, so a user may suddenly see their model behaving inexplicably without any idea why. The only reason I stumbled onto this is because I noticed the size difference by accident, otherwise I would've merrily used it without realizing there's a hidden bug, and I routinely checkpoint models on one GPU type and resume on another, so it's not an unlikely scenario. Just my two cents. Of course ideally the problem would get solved.</p>", "body_text": "May I suggest that this gets documented very clearly as a limitation, perhaps even preventing the user from accidentally using RNNParamsSaver with a bidirectional RNN? I understand the temptation to mark this as contributions welcome and set it aside, but I really do think it's a very serious bug. What makes it particularly problematic is that it's completely silent, so a user may suddenly see their model behaving inexplicably without any idea why. The only reason I stumbled onto this is because I noticed the size difference by accident, otherwise I would've merrily used it without realizing there's a hidden bug, and I routinely checkpoint models on one GPU type and resume on another, so it's not an unlikely scenario. Just my two cents. Of course ideally the problem would get solved.", "body": "May I suggest that this gets documented very clearly as a limitation, perhaps even preventing the user from accidentally using RNNParamsSaver with a bidirectional RNN? I understand the temptation to mark this as contributions welcome and set it aside, but I really do think it's a very serious bug. What makes it particularly problematic is that it's completely silent, so a user may suddenly see their model behaving inexplicably without any idea why. The only reason I stumbled onto this is because I noticed the size difference by accident, otherwise I would've merrily used it without realizing there's a hidden bug, and I routinely checkpoint models on one GPU type and resume on another, so it's not an unlikely scenario. Just my two cents. Of course ideally the problem would get solved."}