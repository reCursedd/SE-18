{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9225", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9225/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9225/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9225/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9225", "id": 221893239, "node_id": "MDU6SXNzdWUyMjE4OTMyMzk=", "number": 9225, "title": "Restoring RNN model from checkpoint fails when hidden layer width is not equal to input/output width", "user": {"login": "glarchev", "id": 958267, "node_id": "MDQ6VXNlcjk1ODI2Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/958267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glarchev", "html_url": "https://github.com/glarchev", "followers_url": "https://api.github.com/users/glarchev/followers", "following_url": "https://api.github.com/users/glarchev/following{/other_user}", "gists_url": "https://api.github.com/users/glarchev/gists{/gist_id}", "starred_url": "https://api.github.com/users/glarchev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glarchev/subscriptions", "organizations_url": "https://api.github.com/users/glarchev/orgs", "repos_url": "https://api.github.com/users/glarchev/repos", "events_url": "https://api.github.com/users/glarchev/events{/privacy}", "received_events_url": "https://api.github.com/users/glarchev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-14T21:17:25Z", "updated_at": "2017-04-15T07:06:57Z", "closed_at": "2017-04-15T07:06:51Z", "author_association": "NONE", "body_html": "<p>Hi, I'm not sure whether this is actually a bug, or I'm doing something wrong, so please treat accordingly.</p>\n<p>I have an RNN model defined as following:</p>\n<pre><code>import tensorflow as tf\n\nnn_layers = 3\n\ndef RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=0.0):\n    # Input data shape: (batch_size, n_steps, input_size)\n    # Output data shape: (batch_size, output_size)\n    # Permute, reshape and split to get n_steps tensors of shape (batch_size, input_size)\n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1, input_size])\n    x = tf.split(axis=0, num_or_size_splits=n_steps, value=x)\n    \n    # define RNN structure\n    \n    #layer_cell = tf.contrib.rnn.BasicLSTMCell(nn_hidden, forget_bias=forget_bias)\n    layer_cell = tf.contrib.rnn.GRUCell(nn_hidden)\n    \n    cell = tf.contrib.rnn.DropoutWrapper(layer_cell, output_keep_prob=keep_prob)\n    cell = tf.contrib.rnn.MultiRNNCell([cell] * nn_layers)\n    output, state = tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\n    \n    pred = tf.matmul(output[-1], weights[\"out\"]) + out_biases[\"out\"]\n    \n    \n    return pred\n\ndef vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob, forget_bias=0.0):\n    \n    # Define weights -- output layer\n    weights = {\n        'out': tf.Variable(tf.random_normal([nn_hidden, output_size]), name=\"smax_w\")\n    }\n    out_biases = {\n        'out': tf.Variable(tf.random_normal([output_size]), name=\"smax_b\")\n    }\n    \n    pred = RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=forget_bias)\n    \n    return pred\n\n</code></pre>\n<p>I use the vs3_RNN method for both training and testing. The size of my input and output vectors (input_size, output_size) is 500. When I set the width of my hidden layer (nn_hidden) to 500, everything works great. However, when I set it to something else, I can train the model and save the checkpoint fine -- but when I try to restore the model from checkpoint, I get an error message.</p>\n<p>Here's a stack trace with nn_hidden equal to 600:</p>\n<pre><code>Traceback (most recent call last):\n  File \"thdvector/vs3_service.py\", line 43, in &lt;module&gt;\n    saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1439, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\n\nCaused by op u'save/Assign', defined at:\n  File \"thdvector/vs3_service.py\", line 41, in &lt;module&gt;\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\n\n\n</code></pre>\n<p>I have tried this with either GRUCell or BasicLSTMCell, and I get the error message regardless.</p>\n<p>Here's the code that run that tries to restore the checkpoint and fails:</p>\n<pre><code>x = tf.placeholder(\"float\", [1, n_steps, input_size], name=\"x_in\")\n    \n    pred = vs_model.vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob=testing_keep_prob, forget_bias=forget_bias)\n    \n    init = tf.global_variables_initializer()\n    \n    with tf.Session() as sess:\n        sess.run(init)\n        print(\"Variables initialized\")\n        \n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(META)\n        saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\n        print(\"Model restored from \" + str(SAVEDIR))\n        ...\n</code></pre>\n<p>Thanks.</p>", "body_text": "Hi, I'm not sure whether this is actually a bug, or I'm doing something wrong, so please treat accordingly.\nI have an RNN model defined as following:\nimport tensorflow as tf\n\nnn_layers = 3\n\ndef RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=0.0):\n    # Input data shape: (batch_size, n_steps, input_size)\n    # Output data shape: (batch_size, output_size)\n    # Permute, reshape and split to get n_steps tensors of shape (batch_size, input_size)\n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1, input_size])\n    x = tf.split(axis=0, num_or_size_splits=n_steps, value=x)\n    \n    # define RNN structure\n    \n    #layer_cell = tf.contrib.rnn.BasicLSTMCell(nn_hidden, forget_bias=forget_bias)\n    layer_cell = tf.contrib.rnn.GRUCell(nn_hidden)\n    \n    cell = tf.contrib.rnn.DropoutWrapper(layer_cell, output_keep_prob=keep_prob)\n    cell = tf.contrib.rnn.MultiRNNCell([cell] * nn_layers)\n    output, state = tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\n    \n    pred = tf.matmul(output[-1], weights[\"out\"]) + out_biases[\"out\"]\n    \n    \n    return pred\n\ndef vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob, forget_bias=0.0):\n    \n    # Define weights -- output layer\n    weights = {\n        'out': tf.Variable(tf.random_normal([nn_hidden, output_size]), name=\"smax_w\")\n    }\n    out_biases = {\n        'out': tf.Variable(tf.random_normal([output_size]), name=\"smax_b\")\n    }\n    \n    pred = RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=forget_bias)\n    \n    return pred\n\n\nI use the vs3_RNN method for both training and testing. The size of my input and output vectors (input_size, output_size) is 500. When I set the width of my hidden layer (nn_hidden) to 500, everything works great. However, when I set it to something else, I can train the model and save the checkpoint fine -- but when I try to restore the model from checkpoint, I get an error message.\nHere's a stack trace with nn_hidden equal to 600:\nTraceback (most recent call last):\n  File \"thdvector/vs3_service.py\", line 43, in <module>\n    saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1439, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\n\nCaused by op u'save/Assign', defined at:\n  File \"thdvector/vs3_service.py\", line 41, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\n\n\n\nI have tried this with either GRUCell or BasicLSTMCell, and I get the error message regardless.\nHere's the code that run that tries to restore the checkpoint and fails:\nx = tf.placeholder(\"float\", [1, n_steps, input_size], name=\"x_in\")\n    \n    pred = vs_model.vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob=testing_keep_prob, forget_bias=forget_bias)\n    \n    init = tf.global_variables_initializer()\n    \n    with tf.Session() as sess:\n        sess.run(init)\n        print(\"Variables initialized\")\n        \n        saver = tf.train.Saver()\n        #saver = tf.train.import_meta_graph(META)\n        saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\n        print(\"Model restored from \" + str(SAVEDIR))\n        ...\n\nThanks.", "body": "Hi, I'm not sure whether this is actually a bug, or I'm doing something wrong, so please treat accordingly.\r\n\r\nI have an RNN model defined as following:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nnn_layers = 3\r\n\r\ndef RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=0.0):\r\n    # Input data shape: (batch_size, n_steps, input_size)\r\n    # Output data shape: (batch_size, output_size)\r\n    # Permute, reshape and split to get n_steps tensors of shape (batch_size, input_size)\r\n    x = tf.transpose(x, [1, 0, 2])\r\n    x = tf.reshape(x, [-1, input_size])\r\n    x = tf.split(axis=0, num_or_size_splits=n_steps, value=x)\r\n    \r\n    # define RNN structure\r\n    \r\n    #layer_cell = tf.contrib.rnn.BasicLSTMCell(nn_hidden, forget_bias=forget_bias)\r\n    layer_cell = tf.contrib.rnn.GRUCell(nn_hidden)\r\n    \r\n    cell = tf.contrib.rnn.DropoutWrapper(layer_cell, output_keep_prob=keep_prob)\r\n    cell = tf.contrib.rnn.MultiRNNCell([cell] * nn_layers)\r\n    output, state = tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\r\n    \r\n    pred = tf.matmul(output[-1], weights[\"out\"]) + out_biases[\"out\"]\r\n    \r\n    \r\n    return pred\r\n\r\ndef vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob, forget_bias=0.0):\r\n    \r\n    # Define weights -- output layer\r\n    weights = {\r\n        'out': tf.Variable(tf.random_normal([nn_hidden, output_size]), name=\"smax_w\")\r\n    }\r\n    out_biases = {\r\n        'out': tf.Variable(tf.random_normal([output_size]), name=\"smax_b\")\r\n    }\r\n    \r\n    pred = RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=forget_bias)\r\n    \r\n    return pred\r\n\r\n```\r\n\r\nI use the vs3_RNN method for both training and testing. The size of my input and output vectors (input_size, output_size) is 500. When I set the width of my hidden layer (nn_hidden) to 500, everything works great. However, when I set it to something else, I can train the model and save the checkpoint fine -- but when I try to restore the model from checkpoint, I get an error message.\r\n\r\nHere's a stack trace with nn_hidden equal to 600:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"thdvector/vs3_service.py\", line 43, in <module>\r\n    saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1439, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\r\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\r\n\r\nCaused by op u'save/Assign', defined at:\r\n  File \"thdvector/vs3_service.py\", line 41, in <module>\r\n    saver = tf.train.Saver()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\r\n    assign_ops.append(saveable.restore(tensors, shapes))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\r\n    self.op.get_shape().is_fully_defined())\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\r\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\r\n\r\n\r\n```\r\n\r\nI have tried this with either GRUCell or BasicLSTMCell, and I get the error message regardless.\r\n\r\nHere's the code that run that tries to restore the checkpoint and fails:\r\n\r\n```\r\nx = tf.placeholder(\"float\", [1, n_steps, input_size], name=\"x_in\")\r\n    \r\n    pred = vs_model.vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob=testing_keep_prob, forget_bias=forget_bias)\r\n    \r\n    init = tf.global_variables_initializer()\r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        print(\"Variables initialized\")\r\n        \r\n        saver = tf.train.Saver()\r\n        #saver = tf.train.import_meta_graph(META)\r\n        saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\r\n        print(\"Model restored from \" + str(SAVEDIR))\r\n        ...\r\n```\r\n\r\nThanks."}