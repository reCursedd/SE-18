{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9129", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9129/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9129/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9129/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9129", "id": 220875686, "node_id": "MDU6SXNzdWUyMjA4NzU2ODY=", "number": 9129, "title": "tensorflow1.1 rnn lstm:ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights....", "user": {"login": "yidan216home", "id": 23524011, "node_id": "MDQ6VXNlcjIzNTI0MDEx", "avatar_url": "https://avatars3.githubusercontent.com/u/23524011?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yidan216home", "html_url": "https://github.com/yidan216home", "followers_url": "https://api.github.com/users/yidan216home/followers", "following_url": "https://api.github.com/users/yidan216home/following{/other_user}", "gists_url": "https://api.github.com/users/yidan216home/gists{/gist_id}", "starred_url": "https://api.github.com/users/yidan216home/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yidan216home/subscriptions", "organizations_url": "https://api.github.com/users/yidan216home/orgs", "repos_url": "https://api.github.com/users/yidan216home/repos", "events_url": "https://api.github.com/users/yidan216home/events{/privacy}", "received_events_url": "https://api.github.com/users/yidan216home/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-04-11T08:28:24Z", "updated_at": "2017-05-11T11:30:26Z", "closed_at": "2017-04-12T01:32:08Z", "author_association": "NONE", "body_html": "<p>Environment info</p>\n<p>Operating System: Ubuntu 14.04.5 LTS</p>\n<p>Installed version of CUDA and cuDNN:<br>\nNo CUDA, I use CPU-only.</p>\n<p>Pip version: pip 1.5.4<br>\nPython version: 2.7.6<br>\nOperating System: Ubuntu 14.04.5 LTS<br>\nTensorflow version: tensorflow-1.1.0rc0-cp27-none-linux_x86_64 , CPU-only<br>\nDescription:</p>\n<p>I was testing the tutorial example of LSTM .<br>\nmy main function  train_rnn_classify.py:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nfrom rnn_model import RNN_Model\nimport data_helper\n\n\nflags =tf.app.flags\nFLAGS = flags.FLAGS\n\n\nflags.DEFINE_integer('batch_size',64,'the batch_size of the training procedure')\nflags.DEFINE_float('lr',0.1,'the learning rate')\nflags.DEFINE_float('lr_decay',0.6,'the learning rate decay')\nflags.DEFINE_integer('vocabulary_size',20000,'vocabulary_size')\nflags.DEFINE_integer('emdedding_dim',128,'embedding dim')\nflags.DEFINE_integer('hidden_neural_size',128,'LSTM hidden neural size')\nflags.DEFINE_integer('hidden_layer_num',1,'LSTM hidden layer num')\nflags.DEFINE_string('dataset_path','/home/hadoop/lstm/subj0.pkl','dataset path')\nflags.DEFINE_integer('max_len',40,'max_len of training sentence')\nflags.DEFINE_integer('valid_num',100,'epoch num of validation')\nflags.DEFINE_integer('checkpoint_num',1000,'epoch num of checkpoint')\nflags.DEFINE_float('init_scale',0.1,'init scale')\nflags.DEFINE_integer('class_num',2,'class num')\nflags.DEFINE_float('keep_prob',0.5,'dropout rate')\nflags.DEFINE_integer('num_epoch',60,'num epoch')\nflags.DEFINE_integer('max_decay_epoch',30,'num epoch')\nflags.DEFINE_integer('max_grad_norm',5,'max_grad_norm')\nflags.DEFINE_string('out_dir',os.path.abspath(os.path.join(os.path.curdir,\"runs\")),'output directory')\nflags.DEFINE_integer('check_point_every',10,'checkpoint every num epoch ')\n\nclass Config(object):\n\n    hidden_neural_size=FLAGS.hidden_neural_size\n    vocabulary_size=FLAGS.vocabulary_size\n    embed_dim=FLAGS.emdedding_dim\n    hidden_layer_num=FLAGS.hidden_layer_num\n    class_num=FLAGS.class_num\n    keep_prob=FLAGS.keep_prob\n    lr = FLAGS.lr\n    lr_decay = FLAGS.lr_decay\n    batch_size=FLAGS.batch_size\n    num_step = FLAGS.max_len\n    max_grad_norm=FLAGS.max_grad_norm\n    num_epoch = FLAGS.num_epoch\n    max_decay_epoch = FLAGS.max_decay_epoch\n    valid_num=FLAGS.valid_num\n    out_dir=FLAGS.out_dir\n    checkpoint_every = FLAGS.check_point_every\n\n\ndef evaluate(model,session,data,global_steps=None,summary_writer=None):\n\n\n    correct_num=0\n    total_num=len(data[0])\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\n\n         fetches = model.correct_num\n         feed_dict={}\n         feed_dict[model.input_data]=x\n         feed_dict[model.target]=y\n         feed_dict[model.mask_x]=mask_x\n         model.assign_new_batch_size(session,len(x))\n         state = session.run(model._initial_state)\n         for i , (c,h) in enumerate(model._initial_state):\n            feed_dict[c]=state[i].c\n            feed_dict[h]=state[i].h\n         count=session.run(fetches,feed_dict)\n         correct_num+=count\n\n    accuracy=float(correct_num)/total_num\n    dev_summary = tf.scalar_summary('dev_accuracy',accuracy)\n    dev_summary = session.run(dev_summary)\n    if summary_writer:\n        summary_writer.add_summary(dev_summary,global_steps)\n        summary_writer.flush()\n    return accuracy\n\ndef run_epoch(model,session,data,global_steps,valid_model,valid_data,train_summary_writer,valid_summary_writer=None):\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\n\n        feed_dict={}\n        feed_dict[model.input_data]=x\n        feed_dict[model.target]=y\n        feed_dict[model.mask_x]=mask_x\n        model.assign_new_batch_size(session,len(x))\n        fetches = [model.cost,model.accuracy,model.train_op,model.summary]\n        state = session.run(model._initial_state)\n        for i , (c,h) in enumerate(model._initial_state):\n            feed_dict[c]=state[i].c\n            feed_dict[h]=state[i].h\n        cost,accuracy,_,summary = session.run(fetches,feed_dict)\n        train_summary_writer.add_summary(summary,global_steps)\n        train_summary_writer.flush()\n        valid_accuracy=evaluate(valid_model,session,valid_data,global_steps,valid_summary_writer)\n        if(global_steps%100==0):\n            print(\"the %i step, train cost is: %f and the train accuracy is %f and the valid accuracy is %f\"%(global_steps,cost,accuracy,valid_accuracy))\n        global_steps+=1\n\n    return global_steps\n\n\n\n\n\ndef train_step():\n\n    print(\"loading the dataset...\")\n    config = Config()\n    eval_config=Config()\n    eval_config.keep_prob=1.0\n\n    train_data,valid_data,test_data=data_helper.load_data(FLAGS.max_len,batch_size=config.batch_size)\n\n    print(\"begin training\")\n\n    # gpu_config=tf.ConfigProto()\n    # gpu_config.gpu_options.allow_growth=True\n    with tf.Graph().as_default(), tf.Session() as session:\n        initializer = tf.random_uniform_initializer(-1*FLAGS.init_scale,1*FLAGS.init_scale)\n        with tf.variable_scope(\"model\",reuse=None,initializer=initializer):\n            model = RNN_Model(config=config,is_training=True)\n\n        with tf.variable_scope(\"model\",reuse=True,initializer=initializer):\n            valid_model = RNN_Model(config=eval_config,is_training=False)\n            test_model = RNN_Model(config=eval_config,is_training=False)\n\n        #add summary\n        # train_summary_op = tf.merge_summary([model.loss_summary,model.accuracy])\n        train_summary_dir = os.path.join(config.out_dir,\"summaries\",\"train\")\n        train_summary_writer =  tf.train.SummaryWriter(train_summary_dir,session.graph)\n\n        # dev_summary_op = tf.merge_summary([valid_model.loss_summary,valid_model.accuracy])\n        dev_summary_dir = os.path.join(eval_config.out_dir,\"summaries\",\"dev\")\n        dev_summary_writer =  tf.train.SummaryWriter(dev_summary_dir,session.graph)\n\n        #add checkpoint\n        checkpoint_dir = os.path.abspath(os.path.join(config.out_dir, \"checkpoints\"))\n        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        saver = tf.train.Saver(tf.all_variables())\n\n\n        tf.initialize_all_variables().run()\n        global_steps=1\n        begin_time=int(time.time())\n\n        for i in range(config.num_epoch):\n            print(\"the %d epoch training...\"%(i+1))\n            lr_decay = config.lr_decay ** max(i-config.max_decay_epoch,0.0)\n            model.assign_new_lr(session,config.lr*lr_decay)\n            global_steps=run_epoch(model,session,train_data,global_steps,valid_model,valid_data,train_summary_writer,dev_summary_writer)\n\n            if i% config.checkpoint_every==0:\n                path = saver.save(session,checkpoint_prefix,global_steps)\n                print(\"Saved model chechpoint to{}\\n\".format(path))\n\n        print(\"the train is finished\")\n        end_time=int(time.time())\n        print(\"training takes %d seconds already\\n\"%(end_time-begin_time))\n        test_accuracy=evaluate(test_model,session,test_data)\n        print(\"the test data accuracy is %f\"%test_accuracy)\n        print(\"program end!\")\n\n\n\ndef main(_):\n    train_step()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()\n</code></pre>\n<p>model code rnn_model.py   :</p>\n<pre><code>`\nimport tensorflow as tf\n\nimport numpy as np\n\n\n\nclass RNN_Model(object):\n\n\n\n\n\n\n\n    def __init__(self,config,is_training=True):\n\n\n\n        self.keep_prob=config.keep_prob\n\n        self.batch_size=tf.Variable(0,dtype=tf.int32,trainable=False)\n\n\n\n        num_step=config.num_step\n\n        self.input_data=tf.placeholder(tf.int32,[None,num_step])\n\n        self.target = tf.placeholder(tf.int64,[None])\n\n        self.mask_x = tf.placeholder(tf.float32,[num_step,None])\n\n\n\n        class_num=config.class_num\n\n        hidden_neural_size=config.hidden_neural_size\n\n        vocabulary_size=config.vocabulary_size\n\n        embed_dim=config.embed_dim\n\n        hidden_layer_num=config.hidden_layer_num\n\n        self.new_batch_size = tf.placeholder(tf.int32,shape=[],name=\"new_batch_size\")\n\n        self._batch_size_update = tf.assign(self.batch_size,self.new_batch_size)\n\n\n\n        #build LSTM network\n\n\n\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_neural_size,forget_bias=0.0,state_is_tuple=True)\n\n        if self.keep_prob&lt;1:\n\n            lstm_cell =  tf.contrib.rnn.DropoutWrapper(\n\n                lstm_cell,output_keep_prob=self.keep_prob\n\n            )\n\n\n\n        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell]*hidden_layer_num,state_is_tuple=True)\n\n\n\n        self._initial_state = cell.zero_state(self.batch_size,dtype=tf.float32)\n\n\n\n        #embedding layer\n\n        with tf.device(\"/cpu:0\"),tf.name_scope(\"embedding_layer\"):\n\n            embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\n\n            inputs=tf.nn.embedding_lookup(embedding,self.input_data)\n\n\n\n        if self.keep_prob&lt;1:\n\n            inputs = tf.nn.dropout(inputs,self.keep_prob)\n\n\n\n        out_put=[]\n\n        state=self._initial_state\n\n        with tf.variable_scope(\"LSTM_layer\"):\n\n            for time_step in range(num_step):\n\n                if time_step&gt;0: tf.get_variable_scope().reuse_variables()\n\n                (cell_output,state)=cell(inputs[:,time_step,:],state)\n\n                out_put.append(cell_output)\n\n\n\n        out_put=out_put*self.mask_x[:,:,None]\n\n\n\n        with tf.name_scope(\"mean_pooling_layer\"):\n\n\n\n            out_put=tf.reduce_sum(out_put,0)/(tf.reduce_sum(self.mask_x,0)[:,None])\n\n\n\n        with tf.name_scope(\"Softmax_layer_and_output\"):\n\n            softmax_w = tf.get_variable(\"softmax_w\",[hidden_neural_size,class_num],dtype=tf.float32)\n\n            softmax_b = tf.get_variable(\"softmax_b\",[class_num],dtype=tf.float32)\n\n            self.logits = tf.matmul(out_put,softmax_w)+softmax_b\n\n\n\n        with tf.name_scope(\"loss\"):\n\n            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target,logits=self.logits+1e-10,)\n\n            self.cost = tf.reduce_mean(self.loss)\n\n\n\n        with tf.name_scope(\"accuracy\"):\n\n            self.prediction = tf.argmax(self.logits,1)\n\n            correct_prediction = tf.equal(self.prediction,self.target)\n\n            self.correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\n\n\n\n        #add summary\n\n        loss_summary = tf.contrib.deprecated.scalar_summary(\"loss\",self.cost)\n\n        #add summary\n\n        accuracy_summary=tf.contrib.deprecated.scalar_summary(\"accuracy_summary\",self.accuracy)\n\n\n\n        if not is_training:\n\n            return\n\n\n\n        self.globle_step = tf.Variable(0,name=\"globle_step\",trainable=False)\n\n        self.lr = tf.Variable(0.0,trainable=False)\n\n\n\n        tvars = tf.trainable_variables()\n\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n\n                                      config.max_grad_norm)\n\n\n\n\n\n        # Keep track of gradient values and sparsity (optional)\n\n        grad_summaries = []\n\n        for g, v in zip(grads, tvars):\n\n            if g is not None:\n\n                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n\n                sparsity_summary = tf.contrib.deprecated.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n\n                grad_summaries.append(grad_hist_summary)\n\n                grad_summaries.append(sparsity_summary)\n\n        self.grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n\n\n        self.summary =tf.summary.merge([loss_summary,accuracy_summary,self.grad_summaries_merged])\n\n\n\n\n\n\n\n        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n\n        optimizer.apply_gradients(zip(grads, tvars))\n\n        self.train_op=optimizer.apply_gradients(zip(grads, tvars))\n\n\n\n        self.new_lr = tf.placeholder(tf.float32,shape=[],name=\"new_learning_rate\")\n\n        self._lr_update = tf.assign(self.lr,self.new_lr)\n\n\n\n    def assign_new_lr(self,session,lr_value):\n\n        session.run(self._lr_update,feed_dict={self.new_lr:lr_value})\n\n    def assign_new_batch_size(self,session,batch_size_value):\n\n        session.run(self._batch_size_update,feed_dict={self.new_batch_size:batch_size_value})`\n</code></pre>\n<p>data handle code  data_helper.py:</p>\n<pre><code>\nimport numpy as np\n\nimport cPickle as pkl\n\n\n\n#file path\n\ndataset_path='/home/hadoop/lstm/subj0.pkl'\n\n\n\ndef set_dataset_path(path):\n\n    dataset_path=path\n\n\n\n\n\n\n\n\n\ndef load_data(max_len,batch_size,n_words=20000,valid_portion=0.1,sort_by_len=True):\n\n    f=open(dataset_path,'rb')\n\n    print ('load data from %s',dataset_path)\n\n    train_set = np.array(pkl.load(f))\n\n    test_set = np.array(pkl.load(f))\n\n    f.close()\n\n\n\n    train_set_x,train_set_y = train_set\n\n\n\n\n\n\n\n\n\n    #train_set length\n\n    n_samples= len(train_set_x)\n\n    #shuffle and generate train and valid dataset\n\n    sidx = np.random.permutation(n_samples)\n\n    n_train = int(np.round(n_samples * (1. - valid_portion)))\n\n    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n\n    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n\n    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n\n    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n\n\n\n\n\n    train_set = (train_set_x, train_set_y)\n\n    valid_set = (valid_set_x, valid_set_y)\n\n\n\n\n\n    #remove unknow words\n\n    def remove_unk(x):\n\n        return [[1 if w &gt;= n_words else w for w in sen] for sen in x]\n\n\n\n    test_set_x, test_set_y = test_set\n\n    valid_set_x, valid_set_y = valid_set\n\n    train_set_x, train_set_y = train_set\n\n\n\n    train_set_x = remove_unk(train_set_x)\n\n    valid_set_x = remove_unk(valid_set_x)\n\n    test_set_x = remove_unk(test_set_x)\n\n\n\n\n\n\n\n    def len_argsort(seq):\n\n        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n\n\n\n    if sort_by_len:\n\n        sorted_index = len_argsort(test_set_x)\n\n        test_set_x = [test_set_x[i] for i in sorted_index]\n\n        test_set_y = [test_set_y[i] for i in sorted_index]\n\n\n\n        sorted_index = len_argsort(valid_set_x)\n\n        valid_set_x = [valid_set_x[i] for i in sorted_index]\n\n        valid_set_y = [valid_set_y[i] for i in sorted_index]\n\n\n\n\n\n        sorted_index = len_argsort(train_set_x)\n\n        train_set_x = [train_set_x[i] for i in sorted_index]\n\n        train_set_y = [train_set_y[i] for i in sorted_index]\n\n\n\n    train_set=(train_set_x,train_set_y)\n\n    valid_set=(valid_set_x,valid_set_y)\n\n    test_set=(test_set_x,test_set_y)\n\n\n\n\n\n\n\n\n\n    new_train_set_x=np.zeros([len(train_set[0]),max_len])\n\n    new_train_set_y=np.zeros(len(train_set[0]))\n\n\n\n    new_valid_set_x=np.zeros([len(valid_set[0]),max_len])\n\n    new_valid_set_y=np.zeros(len(valid_set[0]))\n\n\n\n    new_test_set_x=np.zeros([len(test_set[0]),max_len])\n\n    new_test_set_y=np.zeros(len(test_set[0]))\n\n\n\n    mask_train_x=np.zeros([max_len,len(train_set[0])])\n\n    mask_test_x=np.zeros([max_len,len(test_set[0])])\n\n    mask_valid_x=np.zeros([max_len,len(valid_set[0])])\n\n\n\n\n\n\n\n    def padding_and_generate_mask(x,y,new_x,new_y,new_mask_x):\n\n\n\n        for i,(x,y) in enumerate(zip(x,y)):\n\n            #whether to remove sentences with length larger than maxlen\n\n            if len(x)&lt;=max_len:\n\n                new_x[i,0:len(x)]=x\n\n                new_mask_x[0:len(x),i]=1\n\n                new_y[i]=y\n\n            else:\n\n                new_x[i]=(x[0:max_len])\n\n                new_mask_x[:,i]=1\n\n                new_y[i]=y\n\n        new_set =(new_x,new_y,new_mask_x)\n\n        del new_x,new_y\n\n        return new_set\n\n\n\n    train_set=padding_and_generate_mask(train_set[0],train_set[1],new_train_set_x,new_train_set_y,mask_train_x)\n\n    test_set=padding_and_generate_mask(test_set[0],test_set[1],new_test_set_x,new_test_set_y,mask_test_x)\n\n    valid_set=padding_and_generate_mask(valid_set[0],valid_set[1],new_valid_set_x,new_valid_set_y,mask_valid_x)\n\n\n\n    return train_set,valid_set,test_set\n\n\n\n\n\n#return batch dataset\n\ndef batch_iter(data,batch_size):\n\n\n\n    #get dataset and label\n\n    x,y,mask_x=data\n\n    x=np.array(x)\n\n    y=np.array(y)\n\n    data_size=len(x)\n\n    num_batches_per_epoch=int((data_size-1)/batch_size)\n\n    for batch_index in range(num_batches_per_epoch):\n\n        start_index=batch_index*batch_size\n\n        end_index=min((batch_index+1)*batch_size,data_size)\n\n        return_x = x[start_index:end_index]\n\n        return_y = y[start_index:end_index]\n\n        return_mask_x = mask_x[:,start_index:end_index]\n\n        # if(len(return_x)&lt;batch_size):\n\n        #     print(len(return_x))\n\n        #     print return_x\n\n        #     print return_y\n\n        #     print return_mask_x\n\n        #     import sys\n\n        #     sys.exit(0)\n\n        yield (return_x,return_y,return_mask_x)\n</code></pre>\n<p>When I open a terminal and run<br>\n<code>  python train_rnn_classify.py</code><br>\nthen  has error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"train_rnn_classify.py\", line 176, in &lt;module&gt;\n    tf.app.run()\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"train_rnn_classify.py\", line 172, in main\n    train_step()\n  File \"train_rnn_classify.py\", line 128, in train_step\n    valid_model = RNN_Model(config=eval_config,is_training=False)\n  File \"/home/hadoop/lstm/rnn_model.py\", line 51, in __init__\n    (cell_output,state)=cell(inputs[:,time_step,:],state)\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 953, in __call__\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\n  File \"/home/hadoop/anaconda2/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 93, in _checked_scope\n    \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\nValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'model/LSTM_layer/multi_rnn_cell/cell_0/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.\n</code></pre>\n<p>Why can't I run this example?How to solve this  problem?<br>\nThank you all for your kind help!!!</p>", "body_text": "Environment info\nOperating System: Ubuntu 14.04.5 LTS\nInstalled version of CUDA and cuDNN:\nNo CUDA, I use CPU-only.\nPip version: pip 1.5.4\nPython version: 2.7.6\nOperating System: Ubuntu 14.04.5 LTS\nTensorflow version: tensorflow-1.1.0rc0-cp27-none-linux_x86_64 , CPU-only\nDescription:\nI was testing the tutorial example of LSTM .\nmy main function  train_rnn_classify.py:\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nfrom rnn_model import RNN_Model\nimport data_helper\n\n\nflags =tf.app.flags\nFLAGS = flags.FLAGS\n\n\nflags.DEFINE_integer('batch_size',64,'the batch_size of the training procedure')\nflags.DEFINE_float('lr',0.1,'the learning rate')\nflags.DEFINE_float('lr_decay',0.6,'the learning rate decay')\nflags.DEFINE_integer('vocabulary_size',20000,'vocabulary_size')\nflags.DEFINE_integer('emdedding_dim',128,'embedding dim')\nflags.DEFINE_integer('hidden_neural_size',128,'LSTM hidden neural size')\nflags.DEFINE_integer('hidden_layer_num',1,'LSTM hidden layer num')\nflags.DEFINE_string('dataset_path','/home/hadoop/lstm/subj0.pkl','dataset path')\nflags.DEFINE_integer('max_len',40,'max_len of training sentence')\nflags.DEFINE_integer('valid_num',100,'epoch num of validation')\nflags.DEFINE_integer('checkpoint_num',1000,'epoch num of checkpoint')\nflags.DEFINE_float('init_scale',0.1,'init scale')\nflags.DEFINE_integer('class_num',2,'class num')\nflags.DEFINE_float('keep_prob',0.5,'dropout rate')\nflags.DEFINE_integer('num_epoch',60,'num epoch')\nflags.DEFINE_integer('max_decay_epoch',30,'num epoch')\nflags.DEFINE_integer('max_grad_norm',5,'max_grad_norm')\nflags.DEFINE_string('out_dir',os.path.abspath(os.path.join(os.path.curdir,\"runs\")),'output directory')\nflags.DEFINE_integer('check_point_every',10,'checkpoint every num epoch ')\n\nclass Config(object):\n\n    hidden_neural_size=FLAGS.hidden_neural_size\n    vocabulary_size=FLAGS.vocabulary_size\n    embed_dim=FLAGS.emdedding_dim\n    hidden_layer_num=FLAGS.hidden_layer_num\n    class_num=FLAGS.class_num\n    keep_prob=FLAGS.keep_prob\n    lr = FLAGS.lr\n    lr_decay = FLAGS.lr_decay\n    batch_size=FLAGS.batch_size\n    num_step = FLAGS.max_len\n    max_grad_norm=FLAGS.max_grad_norm\n    num_epoch = FLAGS.num_epoch\n    max_decay_epoch = FLAGS.max_decay_epoch\n    valid_num=FLAGS.valid_num\n    out_dir=FLAGS.out_dir\n    checkpoint_every = FLAGS.check_point_every\n\n\ndef evaluate(model,session,data,global_steps=None,summary_writer=None):\n\n\n    correct_num=0\n    total_num=len(data[0])\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\n\n         fetches = model.correct_num\n         feed_dict={}\n         feed_dict[model.input_data]=x\n         feed_dict[model.target]=y\n         feed_dict[model.mask_x]=mask_x\n         model.assign_new_batch_size(session,len(x))\n         state = session.run(model._initial_state)\n         for i , (c,h) in enumerate(model._initial_state):\n            feed_dict[c]=state[i].c\n            feed_dict[h]=state[i].h\n         count=session.run(fetches,feed_dict)\n         correct_num+=count\n\n    accuracy=float(correct_num)/total_num\n    dev_summary = tf.scalar_summary('dev_accuracy',accuracy)\n    dev_summary = session.run(dev_summary)\n    if summary_writer:\n        summary_writer.add_summary(dev_summary,global_steps)\n        summary_writer.flush()\n    return accuracy\n\ndef run_epoch(model,session,data,global_steps,valid_model,valid_data,train_summary_writer,valid_summary_writer=None):\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\n\n        feed_dict={}\n        feed_dict[model.input_data]=x\n        feed_dict[model.target]=y\n        feed_dict[model.mask_x]=mask_x\n        model.assign_new_batch_size(session,len(x))\n        fetches = [model.cost,model.accuracy,model.train_op,model.summary]\n        state = session.run(model._initial_state)\n        for i , (c,h) in enumerate(model._initial_state):\n            feed_dict[c]=state[i].c\n            feed_dict[h]=state[i].h\n        cost,accuracy,_,summary = session.run(fetches,feed_dict)\n        train_summary_writer.add_summary(summary,global_steps)\n        train_summary_writer.flush()\n        valid_accuracy=evaluate(valid_model,session,valid_data,global_steps,valid_summary_writer)\n        if(global_steps%100==0):\n            print(\"the %i step, train cost is: %f and the train accuracy is %f and the valid accuracy is %f\"%(global_steps,cost,accuracy,valid_accuracy))\n        global_steps+=1\n\n    return global_steps\n\n\n\n\n\ndef train_step():\n\n    print(\"loading the dataset...\")\n    config = Config()\n    eval_config=Config()\n    eval_config.keep_prob=1.0\n\n    train_data,valid_data,test_data=data_helper.load_data(FLAGS.max_len,batch_size=config.batch_size)\n\n    print(\"begin training\")\n\n    # gpu_config=tf.ConfigProto()\n    # gpu_config.gpu_options.allow_growth=True\n    with tf.Graph().as_default(), tf.Session() as session:\n        initializer = tf.random_uniform_initializer(-1*FLAGS.init_scale,1*FLAGS.init_scale)\n        with tf.variable_scope(\"model\",reuse=None,initializer=initializer):\n            model = RNN_Model(config=config,is_training=True)\n\n        with tf.variable_scope(\"model\",reuse=True,initializer=initializer):\n            valid_model = RNN_Model(config=eval_config,is_training=False)\n            test_model = RNN_Model(config=eval_config,is_training=False)\n\n        #add summary\n        # train_summary_op = tf.merge_summary([model.loss_summary,model.accuracy])\n        train_summary_dir = os.path.join(config.out_dir,\"summaries\",\"train\")\n        train_summary_writer =  tf.train.SummaryWriter(train_summary_dir,session.graph)\n\n        # dev_summary_op = tf.merge_summary([valid_model.loss_summary,valid_model.accuracy])\n        dev_summary_dir = os.path.join(eval_config.out_dir,\"summaries\",\"dev\")\n        dev_summary_writer =  tf.train.SummaryWriter(dev_summary_dir,session.graph)\n\n        #add checkpoint\n        checkpoint_dir = os.path.abspath(os.path.join(config.out_dir, \"checkpoints\"))\n        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        saver = tf.train.Saver(tf.all_variables())\n\n\n        tf.initialize_all_variables().run()\n        global_steps=1\n        begin_time=int(time.time())\n\n        for i in range(config.num_epoch):\n            print(\"the %d epoch training...\"%(i+1))\n            lr_decay = config.lr_decay ** max(i-config.max_decay_epoch,0.0)\n            model.assign_new_lr(session,config.lr*lr_decay)\n            global_steps=run_epoch(model,session,train_data,global_steps,valid_model,valid_data,train_summary_writer,dev_summary_writer)\n\n            if i% config.checkpoint_every==0:\n                path = saver.save(session,checkpoint_prefix,global_steps)\n                print(\"Saved model chechpoint to{}\\n\".format(path))\n\n        print(\"the train is finished\")\n        end_time=int(time.time())\n        print(\"training takes %d seconds already\\n\"%(end_time-begin_time))\n        test_accuracy=evaluate(test_model,session,test_data)\n        print(\"the test data accuracy is %f\"%test_accuracy)\n        print(\"program end!\")\n\n\n\ndef main(_):\n    train_step()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()\n\nmodel code rnn_model.py   :\n`\nimport tensorflow as tf\n\nimport numpy as np\n\n\n\nclass RNN_Model(object):\n\n\n\n\n\n\n\n    def __init__(self,config,is_training=True):\n\n\n\n        self.keep_prob=config.keep_prob\n\n        self.batch_size=tf.Variable(0,dtype=tf.int32,trainable=False)\n\n\n\n        num_step=config.num_step\n\n        self.input_data=tf.placeholder(tf.int32,[None,num_step])\n\n        self.target = tf.placeholder(tf.int64,[None])\n\n        self.mask_x = tf.placeholder(tf.float32,[num_step,None])\n\n\n\n        class_num=config.class_num\n\n        hidden_neural_size=config.hidden_neural_size\n\n        vocabulary_size=config.vocabulary_size\n\n        embed_dim=config.embed_dim\n\n        hidden_layer_num=config.hidden_layer_num\n\n        self.new_batch_size = tf.placeholder(tf.int32,shape=[],name=\"new_batch_size\")\n\n        self._batch_size_update = tf.assign(self.batch_size,self.new_batch_size)\n\n\n\n        #build LSTM network\n\n\n\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_neural_size,forget_bias=0.0,state_is_tuple=True)\n\n        if self.keep_prob<1:\n\n            lstm_cell =  tf.contrib.rnn.DropoutWrapper(\n\n                lstm_cell,output_keep_prob=self.keep_prob\n\n            )\n\n\n\n        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell]*hidden_layer_num,state_is_tuple=True)\n\n\n\n        self._initial_state = cell.zero_state(self.batch_size,dtype=tf.float32)\n\n\n\n        #embedding layer\n\n        with tf.device(\"/cpu:0\"),tf.name_scope(\"embedding_layer\"):\n\n            embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\n\n            inputs=tf.nn.embedding_lookup(embedding,self.input_data)\n\n\n\n        if self.keep_prob<1:\n\n            inputs = tf.nn.dropout(inputs,self.keep_prob)\n\n\n\n        out_put=[]\n\n        state=self._initial_state\n\n        with tf.variable_scope(\"LSTM_layer\"):\n\n            for time_step in range(num_step):\n\n                if time_step>0: tf.get_variable_scope().reuse_variables()\n\n                (cell_output,state)=cell(inputs[:,time_step,:],state)\n\n                out_put.append(cell_output)\n\n\n\n        out_put=out_put*self.mask_x[:,:,None]\n\n\n\n        with tf.name_scope(\"mean_pooling_layer\"):\n\n\n\n            out_put=tf.reduce_sum(out_put,0)/(tf.reduce_sum(self.mask_x,0)[:,None])\n\n\n\n        with tf.name_scope(\"Softmax_layer_and_output\"):\n\n            softmax_w = tf.get_variable(\"softmax_w\",[hidden_neural_size,class_num],dtype=tf.float32)\n\n            softmax_b = tf.get_variable(\"softmax_b\",[class_num],dtype=tf.float32)\n\n            self.logits = tf.matmul(out_put,softmax_w)+softmax_b\n\n\n\n        with tf.name_scope(\"loss\"):\n\n            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target,logits=self.logits+1e-10,)\n\n            self.cost = tf.reduce_mean(self.loss)\n\n\n\n        with tf.name_scope(\"accuracy\"):\n\n            self.prediction = tf.argmax(self.logits,1)\n\n            correct_prediction = tf.equal(self.prediction,self.target)\n\n            self.correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\n\n\n\n        #add summary\n\n        loss_summary = tf.contrib.deprecated.scalar_summary(\"loss\",self.cost)\n\n        #add summary\n\n        accuracy_summary=tf.contrib.deprecated.scalar_summary(\"accuracy_summary\",self.accuracy)\n\n\n\n        if not is_training:\n\n            return\n\n\n\n        self.globle_step = tf.Variable(0,name=\"globle_step\",trainable=False)\n\n        self.lr = tf.Variable(0.0,trainable=False)\n\n\n\n        tvars = tf.trainable_variables()\n\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n\n                                      config.max_grad_norm)\n\n\n\n\n\n        # Keep track of gradient values and sparsity (optional)\n\n        grad_summaries = []\n\n        for g, v in zip(grads, tvars):\n\n            if g is not None:\n\n                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n\n                sparsity_summary = tf.contrib.deprecated.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n\n                grad_summaries.append(grad_hist_summary)\n\n                grad_summaries.append(sparsity_summary)\n\n        self.grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n\n\n        self.summary =tf.summary.merge([loss_summary,accuracy_summary,self.grad_summaries_merged])\n\n\n\n\n\n\n\n        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n\n        optimizer.apply_gradients(zip(grads, tvars))\n\n        self.train_op=optimizer.apply_gradients(zip(grads, tvars))\n\n\n\n        self.new_lr = tf.placeholder(tf.float32,shape=[],name=\"new_learning_rate\")\n\n        self._lr_update = tf.assign(self.lr,self.new_lr)\n\n\n\n    def assign_new_lr(self,session,lr_value):\n\n        session.run(self._lr_update,feed_dict={self.new_lr:lr_value})\n\n    def assign_new_batch_size(self,session,batch_size_value):\n\n        session.run(self._batch_size_update,feed_dict={self.new_batch_size:batch_size_value})`\n\ndata handle code  data_helper.py:\n\nimport numpy as np\n\nimport cPickle as pkl\n\n\n\n#file path\n\ndataset_path='/home/hadoop/lstm/subj0.pkl'\n\n\n\ndef set_dataset_path(path):\n\n    dataset_path=path\n\n\n\n\n\n\n\n\n\ndef load_data(max_len,batch_size,n_words=20000,valid_portion=0.1,sort_by_len=True):\n\n    f=open(dataset_path,'rb')\n\n    print ('load data from %s',dataset_path)\n\n    train_set = np.array(pkl.load(f))\n\n    test_set = np.array(pkl.load(f))\n\n    f.close()\n\n\n\n    train_set_x,train_set_y = train_set\n\n\n\n\n\n\n\n\n\n    #train_set length\n\n    n_samples= len(train_set_x)\n\n    #shuffle and generate train and valid dataset\n\n    sidx = np.random.permutation(n_samples)\n\n    n_train = int(np.round(n_samples * (1. - valid_portion)))\n\n    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n\n    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n\n    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n\n    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n\n\n\n\n\n    train_set = (train_set_x, train_set_y)\n\n    valid_set = (valid_set_x, valid_set_y)\n\n\n\n\n\n    #remove unknow words\n\n    def remove_unk(x):\n\n        return [[1 if w >= n_words else w for w in sen] for sen in x]\n\n\n\n    test_set_x, test_set_y = test_set\n\n    valid_set_x, valid_set_y = valid_set\n\n    train_set_x, train_set_y = train_set\n\n\n\n    train_set_x = remove_unk(train_set_x)\n\n    valid_set_x = remove_unk(valid_set_x)\n\n    test_set_x = remove_unk(test_set_x)\n\n\n\n\n\n\n\n    def len_argsort(seq):\n\n        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n\n\n\n    if sort_by_len:\n\n        sorted_index = len_argsort(test_set_x)\n\n        test_set_x = [test_set_x[i] for i in sorted_index]\n\n        test_set_y = [test_set_y[i] for i in sorted_index]\n\n\n\n        sorted_index = len_argsort(valid_set_x)\n\n        valid_set_x = [valid_set_x[i] for i in sorted_index]\n\n        valid_set_y = [valid_set_y[i] for i in sorted_index]\n\n\n\n\n\n        sorted_index = len_argsort(train_set_x)\n\n        train_set_x = [train_set_x[i] for i in sorted_index]\n\n        train_set_y = [train_set_y[i] for i in sorted_index]\n\n\n\n    train_set=(train_set_x,train_set_y)\n\n    valid_set=(valid_set_x,valid_set_y)\n\n    test_set=(test_set_x,test_set_y)\n\n\n\n\n\n\n\n\n\n    new_train_set_x=np.zeros([len(train_set[0]),max_len])\n\n    new_train_set_y=np.zeros(len(train_set[0]))\n\n\n\n    new_valid_set_x=np.zeros([len(valid_set[0]),max_len])\n\n    new_valid_set_y=np.zeros(len(valid_set[0]))\n\n\n\n    new_test_set_x=np.zeros([len(test_set[0]),max_len])\n\n    new_test_set_y=np.zeros(len(test_set[0]))\n\n\n\n    mask_train_x=np.zeros([max_len,len(train_set[0])])\n\n    mask_test_x=np.zeros([max_len,len(test_set[0])])\n\n    mask_valid_x=np.zeros([max_len,len(valid_set[0])])\n\n\n\n\n\n\n\n    def padding_and_generate_mask(x,y,new_x,new_y,new_mask_x):\n\n\n\n        for i,(x,y) in enumerate(zip(x,y)):\n\n            #whether to remove sentences with length larger than maxlen\n\n            if len(x)<=max_len:\n\n                new_x[i,0:len(x)]=x\n\n                new_mask_x[0:len(x),i]=1\n\n                new_y[i]=y\n\n            else:\n\n                new_x[i]=(x[0:max_len])\n\n                new_mask_x[:,i]=1\n\n                new_y[i]=y\n\n        new_set =(new_x,new_y,new_mask_x)\n\n        del new_x,new_y\n\n        return new_set\n\n\n\n    train_set=padding_and_generate_mask(train_set[0],train_set[1],new_train_set_x,new_train_set_y,mask_train_x)\n\n    test_set=padding_and_generate_mask(test_set[0],test_set[1],new_test_set_x,new_test_set_y,mask_test_x)\n\n    valid_set=padding_and_generate_mask(valid_set[0],valid_set[1],new_valid_set_x,new_valid_set_y,mask_valid_x)\n\n\n\n    return train_set,valid_set,test_set\n\n\n\n\n\n#return batch dataset\n\ndef batch_iter(data,batch_size):\n\n\n\n    #get dataset and label\n\n    x,y,mask_x=data\n\n    x=np.array(x)\n\n    y=np.array(y)\n\n    data_size=len(x)\n\n    num_batches_per_epoch=int((data_size-1)/batch_size)\n\n    for batch_index in range(num_batches_per_epoch):\n\n        start_index=batch_index*batch_size\n\n        end_index=min((batch_index+1)*batch_size,data_size)\n\n        return_x = x[start_index:end_index]\n\n        return_y = y[start_index:end_index]\n\n        return_mask_x = mask_x[:,start_index:end_index]\n\n        # if(len(return_x)<batch_size):\n\n        #     print(len(return_x))\n\n        #     print return_x\n\n        #     print return_y\n\n        #     print return_mask_x\n\n        #     import sys\n\n        #     sys.exit(0)\n\n        yield (return_x,return_y,return_mask_x)\n\nWhen I open a terminal and run\n  python train_rnn_classify.py\nthen  has error:\nTraceback (most recent call last):\n  File \"train_rnn_classify.py\", line 176, in <module>\n    tf.app.run()\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"train_rnn_classify.py\", line 172, in main\n    train_step()\n  File \"train_rnn_classify.py\", line 128, in train_step\n    valid_model = RNN_Model(config=eval_config,is_training=False)\n  File \"/home/hadoop/lstm/rnn_model.py\", line 51, in __init__\n    (cell_output,state)=cell(inputs[:,time_step,:],state)\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 953, in __call__\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\n  File \"/home/hadoop/anaconda2/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 93, in _checked_scope\n    \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\nValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'model/LSTM_layer/multi_rnn_cell/cell_0/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.\n\nWhy can't I run this example?How to solve this  problem?\nThank you all for your kind help!!!", "body": "Environment info\r\n\r\nOperating System: Ubuntu 14.04.5 LTS\r\n\r\nInstalled version of CUDA and cuDNN:\r\nNo CUDA, I use CPU-only.\r\n\r\nPip version: pip 1.5.4\r\nPython version: 2.7.6\r\nOperating System: Ubuntu 14.04.5 LTS\r\nTensorflow version: tensorflow-1.1.0rc0-cp27-none-linux_x86_64 , CPU-only\r\nDescription:\r\n\r\nI was testing the tutorial example of LSTM .\r\nmy main function  train_rnn_classify.py:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport datetime\r\nfrom rnn_model import RNN_Model\r\nimport data_helper\r\n\r\n\r\nflags =tf.app.flags\r\nFLAGS = flags.FLAGS\r\n\r\n\r\nflags.DEFINE_integer('batch_size',64,'the batch_size of the training procedure')\r\nflags.DEFINE_float('lr',0.1,'the learning rate')\r\nflags.DEFINE_float('lr_decay',0.6,'the learning rate decay')\r\nflags.DEFINE_integer('vocabulary_size',20000,'vocabulary_size')\r\nflags.DEFINE_integer('emdedding_dim',128,'embedding dim')\r\nflags.DEFINE_integer('hidden_neural_size',128,'LSTM hidden neural size')\r\nflags.DEFINE_integer('hidden_layer_num',1,'LSTM hidden layer num')\r\nflags.DEFINE_string('dataset_path','/home/hadoop/lstm/subj0.pkl','dataset path')\r\nflags.DEFINE_integer('max_len',40,'max_len of training sentence')\r\nflags.DEFINE_integer('valid_num',100,'epoch num of validation')\r\nflags.DEFINE_integer('checkpoint_num',1000,'epoch num of checkpoint')\r\nflags.DEFINE_float('init_scale',0.1,'init scale')\r\nflags.DEFINE_integer('class_num',2,'class num')\r\nflags.DEFINE_float('keep_prob',0.5,'dropout rate')\r\nflags.DEFINE_integer('num_epoch',60,'num epoch')\r\nflags.DEFINE_integer('max_decay_epoch',30,'num epoch')\r\nflags.DEFINE_integer('max_grad_norm',5,'max_grad_norm')\r\nflags.DEFINE_string('out_dir',os.path.abspath(os.path.join(os.path.curdir,\"runs\")),'output directory')\r\nflags.DEFINE_integer('check_point_every',10,'checkpoint every num epoch ')\r\n\r\nclass Config(object):\r\n\r\n    hidden_neural_size=FLAGS.hidden_neural_size\r\n    vocabulary_size=FLAGS.vocabulary_size\r\n    embed_dim=FLAGS.emdedding_dim\r\n    hidden_layer_num=FLAGS.hidden_layer_num\r\n    class_num=FLAGS.class_num\r\n    keep_prob=FLAGS.keep_prob\r\n    lr = FLAGS.lr\r\n    lr_decay = FLAGS.lr_decay\r\n    batch_size=FLAGS.batch_size\r\n    num_step = FLAGS.max_len\r\n    max_grad_norm=FLAGS.max_grad_norm\r\n    num_epoch = FLAGS.num_epoch\r\n    max_decay_epoch = FLAGS.max_decay_epoch\r\n    valid_num=FLAGS.valid_num\r\n    out_dir=FLAGS.out_dir\r\n    checkpoint_every = FLAGS.check_point_every\r\n\r\n\r\ndef evaluate(model,session,data,global_steps=None,summary_writer=None):\r\n\r\n\r\n    correct_num=0\r\n    total_num=len(data[0])\r\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\r\n\r\n         fetches = model.correct_num\r\n         feed_dict={}\r\n         feed_dict[model.input_data]=x\r\n         feed_dict[model.target]=y\r\n         feed_dict[model.mask_x]=mask_x\r\n         model.assign_new_batch_size(session,len(x))\r\n         state = session.run(model._initial_state)\r\n         for i , (c,h) in enumerate(model._initial_state):\r\n            feed_dict[c]=state[i].c\r\n            feed_dict[h]=state[i].h\r\n         count=session.run(fetches,feed_dict)\r\n         correct_num+=count\r\n\r\n    accuracy=float(correct_num)/total_num\r\n    dev_summary = tf.scalar_summary('dev_accuracy',accuracy)\r\n    dev_summary = session.run(dev_summary)\r\n    if summary_writer:\r\n        summary_writer.add_summary(dev_summary,global_steps)\r\n        summary_writer.flush()\r\n    return accuracy\r\n\r\ndef run_epoch(model,session,data,global_steps,valid_model,valid_data,train_summary_writer,valid_summary_writer=None):\r\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\r\n\r\n        feed_dict={}\r\n        feed_dict[model.input_data]=x\r\n        feed_dict[model.target]=y\r\n        feed_dict[model.mask_x]=mask_x\r\n        model.assign_new_batch_size(session,len(x))\r\n        fetches = [model.cost,model.accuracy,model.train_op,model.summary]\r\n        state = session.run(model._initial_state)\r\n        for i , (c,h) in enumerate(model._initial_state):\r\n            feed_dict[c]=state[i].c\r\n            feed_dict[h]=state[i].h\r\n        cost,accuracy,_,summary = session.run(fetches,feed_dict)\r\n        train_summary_writer.add_summary(summary,global_steps)\r\n        train_summary_writer.flush()\r\n        valid_accuracy=evaluate(valid_model,session,valid_data,global_steps,valid_summary_writer)\r\n        if(global_steps%100==0):\r\n            print(\"the %i step, train cost is: %f and the train accuracy is %f and the valid accuracy is %f\"%(global_steps,cost,accuracy,valid_accuracy))\r\n        global_steps+=1\r\n\r\n    return global_steps\r\n\r\n\r\n\r\n\r\n\r\ndef train_step():\r\n\r\n    print(\"loading the dataset...\")\r\n    config = Config()\r\n    eval_config=Config()\r\n    eval_config.keep_prob=1.0\r\n\r\n    train_data,valid_data,test_data=data_helper.load_data(FLAGS.max_len,batch_size=config.batch_size)\r\n\r\n    print(\"begin training\")\r\n\r\n    # gpu_config=tf.ConfigProto()\r\n    # gpu_config.gpu_options.allow_growth=True\r\n    with tf.Graph().as_default(), tf.Session() as session:\r\n        initializer = tf.random_uniform_initializer(-1*FLAGS.init_scale,1*FLAGS.init_scale)\r\n        with tf.variable_scope(\"model\",reuse=None,initializer=initializer):\r\n            model = RNN_Model(config=config,is_training=True)\r\n\r\n        with tf.variable_scope(\"model\",reuse=True,initializer=initializer):\r\n            valid_model = RNN_Model(config=eval_config,is_training=False)\r\n            test_model = RNN_Model(config=eval_config,is_training=False)\r\n\r\n        #add summary\r\n        # train_summary_op = tf.merge_summary([model.loss_summary,model.accuracy])\r\n        train_summary_dir = os.path.join(config.out_dir,\"summaries\",\"train\")\r\n        train_summary_writer =  tf.train.SummaryWriter(train_summary_dir,session.graph)\r\n\r\n        # dev_summary_op = tf.merge_summary([valid_model.loss_summary,valid_model.accuracy])\r\n        dev_summary_dir = os.path.join(eval_config.out_dir,\"summaries\",\"dev\")\r\n        dev_summary_writer =  tf.train.SummaryWriter(dev_summary_dir,session.graph)\r\n\r\n        #add checkpoint\r\n        checkpoint_dir = os.path.abspath(os.path.join(config.out_dir, \"checkpoints\"))\r\n        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\r\n        if not os.path.exists(checkpoint_dir):\r\n            os.makedirs(checkpoint_dir)\r\n        saver = tf.train.Saver(tf.all_variables())\r\n\r\n\r\n        tf.initialize_all_variables().run()\r\n        global_steps=1\r\n        begin_time=int(time.time())\r\n\r\n        for i in range(config.num_epoch):\r\n            print(\"the %d epoch training...\"%(i+1))\r\n            lr_decay = config.lr_decay ** max(i-config.max_decay_epoch,0.0)\r\n            model.assign_new_lr(session,config.lr*lr_decay)\r\n            global_steps=run_epoch(model,session,train_data,global_steps,valid_model,valid_data,train_summary_writer,dev_summary_writer)\r\n\r\n            if i% config.checkpoint_every==0:\r\n                path = saver.save(session,checkpoint_prefix,global_steps)\r\n                print(\"Saved model chechpoint to{}\\n\".format(path))\r\n\r\n        print(\"the train is finished\")\r\n        end_time=int(time.time())\r\n        print(\"training takes %d seconds already\\n\"%(end_time-begin_time))\r\n        test_accuracy=evaluate(test_model,session,test_data)\r\n        print(\"the test data accuracy is %f\"%test_accuracy)\r\n        print(\"program end!\")\r\n\r\n\r\n\r\ndef main(_):\r\n    train_step()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n```\r\nmodel code rnn_model.py   :\r\n```\r\n`\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\n\r\n\r\n\r\nclass RNN_Model(object):\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    def __init__(self,config,is_training=True):\r\n\r\n\r\n\r\n        self.keep_prob=config.keep_prob\r\n\r\n        self.batch_size=tf.Variable(0,dtype=tf.int32,trainable=False)\r\n\r\n\r\n\r\n        num_step=config.num_step\r\n\r\n        self.input_data=tf.placeholder(tf.int32,[None,num_step])\r\n\r\n        self.target = tf.placeholder(tf.int64,[None])\r\n\r\n        self.mask_x = tf.placeholder(tf.float32,[num_step,None])\r\n\r\n\r\n\r\n        class_num=config.class_num\r\n\r\n        hidden_neural_size=config.hidden_neural_size\r\n\r\n        vocabulary_size=config.vocabulary_size\r\n\r\n        embed_dim=config.embed_dim\r\n\r\n        hidden_layer_num=config.hidden_layer_num\r\n\r\n        self.new_batch_size = tf.placeholder(tf.int32,shape=[],name=\"new_batch_size\")\r\n\r\n        self._batch_size_update = tf.assign(self.batch_size,self.new_batch_size)\r\n\r\n\r\n\r\n        #build LSTM network\r\n\r\n\r\n\r\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_neural_size,forget_bias=0.0,state_is_tuple=True)\r\n\r\n        if self.keep_prob<1:\r\n\r\n            lstm_cell =  tf.contrib.rnn.DropoutWrapper(\r\n\r\n                lstm_cell,output_keep_prob=self.keep_prob\r\n\r\n            )\r\n\r\n\r\n\r\n        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell]*hidden_layer_num,state_is_tuple=True)\r\n\r\n\r\n\r\n        self._initial_state = cell.zero_state(self.batch_size,dtype=tf.float32)\r\n\r\n\r\n\r\n        #embedding layer\r\n\r\n        with tf.device(\"/cpu:0\"),tf.name_scope(\"embedding_layer\"):\r\n\r\n            embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\r\n\r\n            inputs=tf.nn.embedding_lookup(embedding,self.input_data)\r\n\r\n\r\n\r\n        if self.keep_prob<1:\r\n\r\n            inputs = tf.nn.dropout(inputs,self.keep_prob)\r\n\r\n\r\n\r\n        out_put=[]\r\n\r\n        state=self._initial_state\r\n\r\n        with tf.variable_scope(\"LSTM_layer\"):\r\n\r\n            for time_step in range(num_step):\r\n\r\n                if time_step>0: tf.get_variable_scope().reuse_variables()\r\n\r\n                (cell_output,state)=cell(inputs[:,time_step,:],state)\r\n\r\n                out_put.append(cell_output)\r\n\r\n\r\n\r\n        out_put=out_put*self.mask_x[:,:,None]\r\n\r\n\r\n\r\n        with tf.name_scope(\"mean_pooling_layer\"):\r\n\r\n\r\n\r\n            out_put=tf.reduce_sum(out_put,0)/(tf.reduce_sum(self.mask_x,0)[:,None])\r\n\r\n\r\n\r\n        with tf.name_scope(\"Softmax_layer_and_output\"):\r\n\r\n            softmax_w = tf.get_variable(\"softmax_w\",[hidden_neural_size,class_num],dtype=tf.float32)\r\n\r\n            softmax_b = tf.get_variable(\"softmax_b\",[class_num],dtype=tf.float32)\r\n\r\n            self.logits = tf.matmul(out_put,softmax_w)+softmax_b\r\n\r\n\r\n\r\n        with tf.name_scope(\"loss\"):\r\n\r\n            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target,logits=self.logits+1e-10,)\r\n\r\n            self.cost = tf.reduce_mean(self.loss)\r\n\r\n\r\n\r\n        with tf.name_scope(\"accuracy\"):\r\n\r\n            self.prediction = tf.argmax(self.logits,1)\r\n\r\n            correct_prediction = tf.equal(self.prediction,self.target)\r\n\r\n            self.correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\r\n\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\r\n\r\n\r\n\r\n        #add summary\r\n\r\n        loss_summary = tf.contrib.deprecated.scalar_summary(\"loss\",self.cost)\r\n\r\n        #add summary\r\n\r\n        accuracy_summary=tf.contrib.deprecated.scalar_summary(\"accuracy_summary\",self.accuracy)\r\n\r\n\r\n\r\n        if not is_training:\r\n\r\n            return\r\n\r\n\r\n\r\n        self.globle_step = tf.Variable(0,name=\"globle_step\",trainable=False)\r\n\r\n        self.lr = tf.Variable(0.0,trainable=False)\r\n\r\n\r\n\r\n        tvars = tf.trainable_variables()\r\n\r\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\r\n\r\n                                      config.max_grad_norm)\r\n\r\n\r\n\r\n\r\n\r\n        # Keep track of gradient values and sparsity (optional)\r\n\r\n        grad_summaries = []\r\n\r\n        for g, v in zip(grads, tvars):\r\n\r\n            if g is not None:\r\n\r\n                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\r\n\r\n                sparsity_summary = tf.contrib.deprecated.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\r\n\r\n                grad_summaries.append(grad_hist_summary)\r\n\r\n                grad_summaries.append(sparsity_summary)\r\n\r\n        self.grad_summaries_merged = tf.summary.merge(grad_summaries)\r\n\r\n\r\n\r\n        self.summary =tf.summary.merge([loss_summary,accuracy_summary,self.grad_summaries_merged])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n        optimizer = tf.train.GradientDescentOptimizer(self.lr)\r\n\r\n        optimizer.apply_gradients(zip(grads, tvars))\r\n\r\n        self.train_op=optimizer.apply_gradients(zip(grads, tvars))\r\n\r\n\r\n\r\n        self.new_lr = tf.placeholder(tf.float32,shape=[],name=\"new_learning_rate\")\r\n\r\n        self._lr_update = tf.assign(self.lr,self.new_lr)\r\n\r\n\r\n\r\n    def assign_new_lr(self,session,lr_value):\r\n\r\n        session.run(self._lr_update,feed_dict={self.new_lr:lr_value})\r\n\r\n    def assign_new_batch_size(self,session,batch_size_value):\r\n\r\n        session.run(self._batch_size_update,feed_dict={self.new_batch_size:batch_size_value})`\r\n```\r\n  data handle code  data_helper.py:\r\n\r\n```\r\n\r\nimport numpy as np\r\n\r\nimport cPickle as pkl\r\n\r\n\r\n\r\n#file path\r\n\r\ndataset_path='/home/hadoop/lstm/subj0.pkl'\r\n\r\n\r\n\r\ndef set_dataset_path(path):\r\n\r\n    dataset_path=path\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef load_data(max_len,batch_size,n_words=20000,valid_portion=0.1,sort_by_len=True):\r\n\r\n    f=open(dataset_path,'rb')\r\n\r\n    print ('load data from %s',dataset_path)\r\n\r\n    train_set = np.array(pkl.load(f))\r\n\r\n    test_set = np.array(pkl.load(f))\r\n\r\n    f.close()\r\n\r\n\r\n\r\n    train_set_x,train_set_y = train_set\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    #train_set length\r\n\r\n    n_samples= len(train_set_x)\r\n\r\n    #shuffle and generate train and valid dataset\r\n\r\n    sidx = np.random.permutation(n_samples)\r\n\r\n    n_train = int(np.round(n_samples * (1. - valid_portion)))\r\n\r\n    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\r\n\r\n    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\r\n\r\n    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\r\n\r\n    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\r\n\r\n\r\n\r\n\r\n\r\n    train_set = (train_set_x, train_set_y)\r\n\r\n    valid_set = (valid_set_x, valid_set_y)\r\n\r\n\r\n\r\n\r\n\r\n    #remove unknow words\r\n\r\n    def remove_unk(x):\r\n\r\n        return [[1 if w >= n_words else w for w in sen] for sen in x]\r\n\r\n\r\n\r\n    test_set_x, test_set_y = test_set\r\n\r\n    valid_set_x, valid_set_y = valid_set\r\n\r\n    train_set_x, train_set_y = train_set\r\n\r\n\r\n\r\n    train_set_x = remove_unk(train_set_x)\r\n\r\n    valid_set_x = remove_unk(valid_set_x)\r\n\r\n    test_set_x = remove_unk(test_set_x)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    def len_argsort(seq):\r\n\r\n        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\r\n\r\n\r\n\r\n    if sort_by_len:\r\n\r\n        sorted_index = len_argsort(test_set_x)\r\n\r\n        test_set_x = [test_set_x[i] for i in sorted_index]\r\n\r\n        test_set_y = [test_set_y[i] for i in sorted_index]\r\n\r\n\r\n\r\n        sorted_index = len_argsort(valid_set_x)\r\n\r\n        valid_set_x = [valid_set_x[i] for i in sorted_index]\r\n\r\n        valid_set_y = [valid_set_y[i] for i in sorted_index]\r\n\r\n\r\n\r\n\r\n\r\n        sorted_index = len_argsort(train_set_x)\r\n\r\n        train_set_x = [train_set_x[i] for i in sorted_index]\r\n\r\n        train_set_y = [train_set_y[i] for i in sorted_index]\r\n\r\n\r\n\r\n    train_set=(train_set_x,train_set_y)\r\n\r\n    valid_set=(valid_set_x,valid_set_y)\r\n\r\n    test_set=(test_set_x,test_set_y)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    new_train_set_x=np.zeros([len(train_set[0]),max_len])\r\n\r\n    new_train_set_y=np.zeros(len(train_set[0]))\r\n\r\n\r\n\r\n    new_valid_set_x=np.zeros([len(valid_set[0]),max_len])\r\n\r\n    new_valid_set_y=np.zeros(len(valid_set[0]))\r\n\r\n\r\n\r\n    new_test_set_x=np.zeros([len(test_set[0]),max_len])\r\n\r\n    new_test_set_y=np.zeros(len(test_set[0]))\r\n\r\n\r\n\r\n    mask_train_x=np.zeros([max_len,len(train_set[0])])\r\n\r\n    mask_test_x=np.zeros([max_len,len(test_set[0])])\r\n\r\n    mask_valid_x=np.zeros([max_len,len(valid_set[0])])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    def padding_and_generate_mask(x,y,new_x,new_y,new_mask_x):\r\n\r\n\r\n\r\n        for i,(x,y) in enumerate(zip(x,y)):\r\n\r\n            #whether to remove sentences with length larger than maxlen\r\n\r\n            if len(x)<=max_len:\r\n\r\n                new_x[i,0:len(x)]=x\r\n\r\n                new_mask_x[0:len(x),i]=1\r\n\r\n                new_y[i]=y\r\n\r\n            else:\r\n\r\n                new_x[i]=(x[0:max_len])\r\n\r\n                new_mask_x[:,i]=1\r\n\r\n                new_y[i]=y\r\n\r\n        new_set =(new_x,new_y,new_mask_x)\r\n\r\n        del new_x,new_y\r\n\r\n        return new_set\r\n\r\n\r\n\r\n    train_set=padding_and_generate_mask(train_set[0],train_set[1],new_train_set_x,new_train_set_y,mask_train_x)\r\n\r\n    test_set=padding_and_generate_mask(test_set[0],test_set[1],new_test_set_x,new_test_set_y,mask_test_x)\r\n\r\n    valid_set=padding_and_generate_mask(valid_set[0],valid_set[1],new_valid_set_x,new_valid_set_y,mask_valid_x)\r\n\r\n\r\n\r\n    return train_set,valid_set,test_set\r\n\r\n\r\n\r\n\r\n\r\n#return batch dataset\r\n\r\ndef batch_iter(data,batch_size):\r\n\r\n\r\n\r\n    #get dataset and label\r\n\r\n    x,y,mask_x=data\r\n\r\n    x=np.array(x)\r\n\r\n    y=np.array(y)\r\n\r\n    data_size=len(x)\r\n\r\n    num_batches_per_epoch=int((data_size-1)/batch_size)\r\n\r\n    for batch_index in range(num_batches_per_epoch):\r\n\r\n        start_index=batch_index*batch_size\r\n\r\n        end_index=min((batch_index+1)*batch_size,data_size)\r\n\r\n        return_x = x[start_index:end_index]\r\n\r\n        return_y = y[start_index:end_index]\r\n\r\n        return_mask_x = mask_x[:,start_index:end_index]\r\n\r\n        # if(len(return_x)<batch_size):\r\n\r\n        #     print(len(return_x))\r\n\r\n        #     print return_x\r\n\r\n        #     print return_y\r\n\r\n        #     print return_mask_x\r\n\r\n        #     import sys\r\n\r\n        #     sys.exit(0)\r\n\r\n        yield (return_x,return_y,return_mask_x)\r\n```\r\n\r\n\r\nWhen I open a terminal and run\r\n`  python train_rnn_classify.py`\r\nthen  has error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_rnn_classify.py\", line 176, in <module>\r\n    tf.app.run()\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train_rnn_classify.py\", line 172, in main\r\n    train_step()\r\n  File \"train_rnn_classify.py\", line 128, in train_step\r\n    valid_model = RNN_Model(config=eval_config,is_training=False)\r\n  File \"/home/hadoop/lstm/rnn_model.py\", line 51, in __init__\r\n    (cell_output,state)=cell(inputs[:,time_step,:],state)\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 953, in __call__\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\r\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 93, in _checked_scope\r\n    \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\r\nValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'model/LSTM_layer/multi_rnn_cell/cell_0/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.\r\n```\r\n\r\nWhy can't I run this example?How to solve this  problem?\r\nThank you all for your kind help!!!"}