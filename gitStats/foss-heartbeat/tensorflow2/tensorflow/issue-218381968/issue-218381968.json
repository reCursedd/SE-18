{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8857", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8857/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8857/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8857/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8857", "id": 218381968, "node_id": "MDU6SXNzdWUyMTgzODE5Njg=", "number": 8857, "title": "Utility for repeatedly running tensors on queued input and accumulating the results", "user": {"login": "shoyer", "id": 1217238, "node_id": "MDQ6VXNlcjEyMTcyMzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1217238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shoyer", "html_url": "https://github.com/shoyer", "followers_url": "https://api.github.com/users/shoyer/followers", "following_url": "https://api.github.com/users/shoyer/following{/other_user}", "gists_url": "https://api.github.com/users/shoyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/shoyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shoyer/subscriptions", "organizations_url": "https://api.github.com/users/shoyer/orgs", "repos_url": "https://api.github.com/users/shoyer/repos", "events_url": "https://api.github.com/users/shoyer/events{/privacy}", "received_events_url": "https://api.github.com/users/shoyer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-03-31T02:38:13Z", "updated_at": "2017-07-21T18:25:52Z", "closed_at": "2017-07-21T18:25:51Z", "author_association": "MEMBER", "body_html": "<p>I've written a utility function in TensorFlow that I've found quite helpful for loading Tensors with queue based inputs into memory, e.g., for looking at input data stored in the form of TF-records files or for looking at inference results. I've found it especially useful for interactively exploring the results of saved models from IPython notebooks, i.e., doing inference on small datasets or small portions of big datasets.</p>\n<p>Here's what the API looks like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Mapping, Optional, Dict\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run_repeatedly</span>(\n    <span class=\"pl-smi\">batched_tensors</span>: Mapping[<span class=\"pl-c1\">object</span>, tf.Tensor],\n    <span class=\"pl-smi\">checkpoint_dir</span>: Optional[<span class=\"pl-c1\">str</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>,\n    <span class=\"pl-smi\">max_num_batches</span>: Optional[<span class=\"pl-c1\">int</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>) -&gt; Dict[<span class=\"pl-c1\">object</span>, np.ndarray]:\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Repeatedly run tensors until they are exhausted.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    batched_tensors: dict of tensors to evaluate, each of which should have a</span>\n<span class=\"pl-s\">      first axis corresponding to a batch of examples.</span>\n<span class=\"pl-s\">    checkpoint_dir: optional path to checkpoint to load.</span>\n<span class=\"pl-s\">    max_num_batches: optional maximum number of batches to run.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    A dict of numpy.ndarray objects containing the result of evaluating</span>\n<span class=\"pl-s\">    batched_tensors and concatenated across batches along the first axis.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Raises:</span>\n<span class=\"pl-s\">    ValueError: if checkpoint_dir has no valid checkpoint</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> the implementation makes use of tf.contrib.metrics.streaming_concat</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> and a tf.train.Supervisor: it handles all the boilerplate around starting</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> up a session.</span></pre></div>\n<p>And a few usage example:</p>\n<ul>\n<li>Previewing features loaded from files:</li>\n</ul>\n<pre><code>tensors = tf.contrib.learn.read_batch_features(....)\narrays = run_repeatedly(tensors, max_num_batches=10)\n</code></pre>\n<ul>\n<li>For running inference on a full dataset while also accumulating input tensors (useful for debugging):</li>\n</ul>\n<pre><code>tensors = tf.contrib.learn.read_batch_features(....)\npredictions = make_predictions(tensor_inputs, ...)\ntensors.update(predictions)\narrays = run_repeatedly(tensors, checkpoint_dir=path_to_saved_model)\n</code></pre>\n<p><strong>Does something like this belong somewhere in core TensorFlow, or maybe one of the contrib libraries?</strong></p>\n<p>This is somewhat similar to <code>Estimator.predict</code> from <code>tf.contrib.learn</code>, but with a few key differences:</p>\n<ul>\n<li>It's more flexible, not expecting inputs in the form of a <code>tf.learn</code> model.</li>\n<li>It automatically concatenates across batches. In practice, I find this highly useful, because I can often store the results of a model in memory even though I don't have enough memory to run inference on everything at once.</li>\n</ul>", "body_text": "I've written a utility function in TensorFlow that I've found quite helpful for loading Tensors with queue based inputs into memory, e.g., for looking at input data stored in the form of TF-records files or for looking at inference results. I've found it especially useful for interactively exploring the results of saved models from IPython notebooks, i.e., doing inference on small datasets or small portions of big datasets.\nHere's what the API looks like:\nfrom typing import Mapping, Optional, Dict\n\ndef run_repeatedly(\n    batched_tensors: Mapping[object, tf.Tensor],\n    checkpoint_dir: Optional[str] = None,\n    max_num_batches: Optional[int] = None) -> Dict[object, np.ndarray]:\n  \"\"\"Repeatedly run tensors until they are exhausted.\n\n  Args:\n    batched_tensors: dict of tensors to evaluate, each of which should have a\n      first axis corresponding to a batch of examples.\n    checkpoint_dir: optional path to checkpoint to load.\n    max_num_batches: optional maximum number of batches to run.\n\n  Returns:\n    A dict of numpy.ndarray objects containing the result of evaluating\n    batched_tensors and concatenated across batches along the first axis.\n\n  Raises:\n    ValueError: if checkpoint_dir has no valid checkpoint\n  \"\"\"\n  # the implementation makes use of tf.contrib.metrics.streaming_concat\n  # and a tf.train.Supervisor: it handles all the boilerplate around starting\n  # up a session.\nAnd a few usage example:\n\nPreviewing features loaded from files:\n\ntensors = tf.contrib.learn.read_batch_features(....)\narrays = run_repeatedly(tensors, max_num_batches=10)\n\n\nFor running inference on a full dataset while also accumulating input tensors (useful for debugging):\n\ntensors = tf.contrib.learn.read_batch_features(....)\npredictions = make_predictions(tensor_inputs, ...)\ntensors.update(predictions)\narrays = run_repeatedly(tensors, checkpoint_dir=path_to_saved_model)\n\nDoes something like this belong somewhere in core TensorFlow, or maybe one of the contrib libraries?\nThis is somewhat similar to Estimator.predict from tf.contrib.learn, but with a few key differences:\n\nIt's more flexible, not expecting inputs in the form of a tf.learn model.\nIt automatically concatenates across batches. In practice, I find this highly useful, because I can often store the results of a model in memory even though I don't have enough memory to run inference on everything at once.", "body": "I've written a utility function in TensorFlow that I've found quite helpful for loading Tensors with queue based inputs into memory, e.g., for looking at input data stored in the form of TF-records files or for looking at inference results. I've found it especially useful for interactively exploring the results of saved models from IPython notebooks, i.e., doing inference on small datasets or small portions of big datasets.\r\n\r\nHere's what the API looks like:\r\n```python\r\nfrom typing import Mapping, Optional, Dict\r\n\r\ndef run_repeatedly(\r\n    batched_tensors: Mapping[object, tf.Tensor],\r\n    checkpoint_dir: Optional[str] = None,\r\n    max_num_batches: Optional[int] = None) -> Dict[object, np.ndarray]:\r\n  \"\"\"Repeatedly run tensors until they are exhausted.\r\n\r\n  Args:\r\n    batched_tensors: dict of tensors to evaluate, each of which should have a\r\n      first axis corresponding to a batch of examples.\r\n    checkpoint_dir: optional path to checkpoint to load.\r\n    max_num_batches: optional maximum number of batches to run.\r\n\r\n  Returns:\r\n    A dict of numpy.ndarray objects containing the result of evaluating\r\n    batched_tensors and concatenated across batches along the first axis.\r\n\r\n  Raises:\r\n    ValueError: if checkpoint_dir has no valid checkpoint\r\n  \"\"\"\r\n  # the implementation makes use of tf.contrib.metrics.streaming_concat\r\n  # and a tf.train.Supervisor: it handles all the boilerplate around starting\r\n  # up a session.\r\n```\r\n\r\nAnd a few usage example:\r\n\r\n- Previewing features loaded from files:\r\n```\r\ntensors = tf.contrib.learn.read_batch_features(....)\r\narrays = run_repeatedly(tensors, max_num_batches=10)\r\n```\r\n- For running inference on a full dataset while also accumulating input tensors (useful for debugging):\r\n```\r\ntensors = tf.contrib.learn.read_batch_features(....)\r\npredictions = make_predictions(tensor_inputs, ...)\r\ntensors.update(predictions)\r\narrays = run_repeatedly(tensors, checkpoint_dir=path_to_saved_model)\r\n```\r\n\r\n**Does something like this belong somewhere in core TensorFlow, or maybe one of the contrib libraries?**\r\n\r\nThis is somewhat similar to `Estimator.predict` from `tf.contrib.learn`, but with a few key differences:\r\n- It's more flexible, not expecting inputs in the form of a `tf.learn` model.\r\n- It automatically concatenates across batches. In practice, I find this highly useful, because I can often store the results of a model in memory even though I don't have enough memory to run inference on everything at once."}