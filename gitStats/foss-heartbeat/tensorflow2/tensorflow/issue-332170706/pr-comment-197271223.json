{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/197271223", "pull_request_review_id": 130985665, "id": 197271223, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NzI3MTIyMw==", "diff_hunk": "@@ -1273,6 +1274,316 @@ def _create_tpu_estimator_spec(\n         train_op=train_op)\n \n \n+def _multi_label_head_with_sigmoid_cross_entropy_loss(\n+    n_labels,\n+    weight_column=None,\n+    thresholds=None,\n+    label_vocabulary=None,\n+    label_sep=',',\n+    loss_reduction=losses.Reduction.SUM,\n+    loss_fn=None,\n+    name=None):\n+  \"\"\"Creates a '_Head' for multi label classification.\n+\n+  The head expects `logits` with shape `[D0, D1, ... DN, n_labels]`.\n+  In many applications, the shape is `[batch_size, n_labels]`.\n+\n+  `labels` must be a dense `Tensor` with shape matching `logits`, namely\n+  `[D0, D1, ... DN, 1]`. If `label_vocabulary` given, `labels` must be a string,\n+  using `label_sep` concat multi labels.\n+  `Tensor` with values from the vocabulary. If `label_vocabulary` is not given,\n+  `labels` must be an integer `Tensor` with values specifying the label index.\n+\n+  If `weight_column` is specified, weights must be of shape\n+  `[D0, D1, ... DN]`, or `[D0, D1, ... DN, 1]`.\n+\n+  The loss is the weighted sum over the input dimensions. Namely, if the input\n+  labels have shape `[batch_size, n_labels]`, the loss is the weighted sum over\n+  `batch_size`.\n+\n+  Also supports custom `loss_fn`. `loss_fn` takes `(labels, logits)` or\n+  `(labels, logits, features)` as arguments and returns unreduced loss with\n+  shape `[D0, D1, ... DN, 1]`. `loss_fn` must support integer `labels` with\n+  shape `[D0, D1, ... DN, 1]`. Namely, the head applies `label_vocabulary` to\n+  the input labels before passing them to `loss_fn`.\n+\n+  Args:\n+    n_labels: Number of labels, must be greater than 2 (for 2 classes, use\n+      `_BinaryLogisticHeadWithSigmoidCrossEntropyLoss`).\n+    weight_column: A string or a `_NumericColumn` created by\n+      `tf.feature_column.numeric_column` defining feature column representing\n+      weights. It is used to down weight or boost examples during training. It\n+      will be multiplied by the loss of the example.\n+    thresholds: Iterable of floats in the range `(0, 1)`. For binary\n+      classification metrics such as precision and recall, an eval metric is\n+      generated for each threshold value. This threshold is applied to the\n+      logistic values to determine the binary classification of each label (i.e.,\n+      above the threshold is `true`, below is `false`.\n+    label_vocabulary: A list or tuple of strings representing possible label\n+      values. If it is not given, that means labels are already encoded as an\n+      integer within [0, n_labels). If given, labels must be of string type and\n+      have any value in `label_vocabulary`. Note that errors will be raised if\n+      `label_vocabulary` is not provided but labels are strings.\n+    label_sep: A string that concat multiple label in one string. Note that errors\n+      will be raised if `label_vocabulary` is provided but not `label_sep`.\n+    loss_reduction: One of `tf.losses.Reduction` except `NONE`. Describes how to\n+      reduce training loss over batch. Defaults to `SUM`.\n+    loss_fn: Optional loss function.\n+    name: name of the head. If provided, summary and metrics keys will be\n+      suffixed by `\"/\" + name`. Also used as `name_scope` when creating ops.\n+\n+  Returns:\n+    An instance of `_Head` for multi label classification.\n+\n+  Raises:\n+    ValueError: If `n_labels`, `label_vocabulary`, `label_sep` or `loss_reduction`\n+    is invalid.\n+  \"\"\"\n+  thresholds = tuple(thresholds) if thresholds else tuple()\n+  if label_vocabulary is not None:\n+    if not isinstance(label_sep, basestring):\n+      raise TypeError(\n+        'label_sep should be a str. Given type: {}'.format(\n+            type(label_sep)))\n+    if not isinstance(label_vocabulary, (list, tuple)):\n+      raise TypeError(\n+        'label_vocabulary should be a list or tuple. Given type: {}'.format(\n+            type(label_vocabulary)))\n+\n+  for threshold in thresholds:\n+    if (threshold <= 0.0) or (threshold >= 1.0):\n+      raise ValueError('thresholds not in (0, 1): {}.'.format((thresholds,)))\n+  if (loss_reduction not in losses.Reduction.all() or\n+      loss_reduction == losses.Reduction.NONE):\n+    raise ValueError('Invalid loss_reduction: {}'.format(loss_reduction))\n+  if loss_fn:\n+    _validate_loss_fn_args(loss_fn)\n+  return _MultiLabelHeadWithSigmoidCrossEntropyLoss(\n+      n_labels=n_labels,\n+      weight_column=weight_column,\n+      thresholds=thresholds,\n+      label_vocabulary=label_vocabulary,\n+      label_sep=label_sep,\n+      loss_reduction=loss_reduction,\n+      loss_fn=loss_fn,\n+      name=name)\n+\n+\n+class _MultiLabelHeadWithSigmoidCrossEntropyLoss(_BinaryLogisticHeadWithSigmoidCrossEntropyLoss):\n+  \"\"\"See `_multi_label_head_with_sigmoid_cross_entropy_loss`.\"\"\"\n+\n+  def __init__(self,\n+                 n_labels,\n+                 weight_column=None,\n+                 thresholds=None,\n+                 label_vocabulary=None,\n+                 label_sep=None,\n+                 loss_reduction=losses.Reduction.SUM,\n+                 loss_fn=None,\n+                 name=None):\n+    self._n_labels = n_labels\n+    self._weight_column = weight_column\n+    self._thresholds = thresholds\n+    self._label_vocabulary = label_vocabulary\n+    self._label_sep = label_sep\n+    self._loss_reduction = loss_reduction\n+    self._loss_fn = loss_fn\n+    self._name = name\n+\n+  @property\n+  def logits_dimension(self):\n+    return self._n_labels\n+\n+  def _eval_metric_opss(self,\n+                        labels,\n+                        logits,\n+                        class_ids,\n+                        weights,\n+                        unreduced_loss,\n+                        regularization_loss):\n+\n+    with ops.name_scope(None, 'metrics',\n+                        (labels, logits, class_ids, weights,\n+                         unreduced_loss, regularization_loss)):\n+      keys = metric_keys.MetricKeys\n+      labels_mean = _indicator_labels_mean(\n+          labels=labels, weights=weights, name=keys.LABEL_MEAN)\n+\n+      metric_ops = {\n+          # Estimator already adds a metric for loss.\n+          _summary_key(self._name, keys.LOSS_MEAN):\n+              metrics_lib.mean(\n+                  values=unreduced_loss,\n+                  weights=weights,\n+                  name=keys.LOSS_MEAN),\n+          _summary_key(self._name, keys.ACCURACY):\n+              metrics_lib.accuracy(\n+                  labels=labels,\n+                  predictions=class_ids,\n+                  weights=weights,\n+                  name=keys.ACCURACY),\n+          _summary_key(self._name, keys.PRECISION):\n+              metrics_lib.precision(\n+                  labels=labels,\n+                  predictions=class_ids,\n+                  weights=weights,\n+                  name=keys.PRECISION),\n+          _summary_key(self._name, keys.RECALL):\n+              metrics_lib.recall(\n+                  labels=labels,\n+                  predictions=class_ids,\n+                  weights=weights,\n+                  name=keys.RECALL),\n+          _summary_key(self._name, keys.LABEL_MEAN):\n+              labels_mean,\n+          _summary_key(self._name, keys.ACCURACY_BASELINE):\n+              _accuracy_baseline(labels_mean)\n+      }\n+      if regularization_loss is not None:\n+        metric_ops[_summary_key(self._name, keys.LOSS_REGULARIZATION)] = (\n+            metrics_lib.mean(\n+                values=regularization_loss,\n+                name=keys.LOSS_REGULARIZATION))\n+\n+      return metric_ops\n+\n+  def create_loss(self, features, mode, logits, labels):\n+    \"\"\"See `Head`.\"\"\"\n+    del mode  # Unused for this head.\n+    logits = ops.convert_to_tensor(logits)\n+    labels = _check_dense_labels_match_logits_and_reshape(\n+        labels=labels, logits=logits, expected_labels_dimension=self.logits_dimension)\n+    if self._label_vocabulary is not None:\n+      sparse_labels = string_ops.string_split(labels, '.')", "path": "tensorflow/python/estimator/canned/head.py", "position": 192, "original_position": 192, "commit_id": "71d24d5d3951c83c3a34756be1c54d8b0a5b2f3b", "original_commit_id": "71d24d5d3951c83c3a34756be1c54d8b0a5b2f3b", "user": {"login": "yupbank", "id": 741544, "node_id": "MDQ6VXNlcjc0MTU0NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/741544?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yupbank", "html_url": "https://github.com/yupbank", "followers_url": "https://api.github.com/users/yupbank/followers", "following_url": "https://api.github.com/users/yupbank/following{/other_user}", "gists_url": "https://api.github.com/users/yupbank/gists{/gist_id}", "starred_url": "https://api.github.com/users/yupbank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yupbank/subscriptions", "organizations_url": "https://api.github.com/users/yupbank/orgs", "repos_url": "https://api.github.com/users/yupbank/repos", "events_url": "https://api.github.com/users/yupbank/events{/privacy}", "received_events_url": "https://api.github.com/users/yupbank/received_events", "type": "User", "site_admin": false}, "body": "`s/'.'/self._label_sep/`", "created_at": "2018-06-21T20:44:43Z", "updated_at": "2018-06-21T20:44:44Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20001#discussion_r197271223", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20001", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/197271223"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20001#discussion_r197271223"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20001"}}, "body_html": "<p><code>s/'.'/self._label_sep/</code></p>", "body_text": "s/'.'/self._label_sep/"}