{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4124", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4124/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4124/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4124/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4124", "id": 174271743, "node_id": "MDU6SXNzdWUxNzQyNzE3NDM=", "number": 4124, "title": "\"ValueError: No gradients provided for any variable\" when using bidirectional_dynamic_rnn", "user": {"login": "xuanchien", "id": 1203702, "node_id": "MDQ6VXNlcjEyMDM3MDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/1203702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuanchien", "html_url": "https://github.com/xuanchien", "followers_url": "https://api.github.com/users/xuanchien/followers", "following_url": "https://api.github.com/users/xuanchien/following{/other_user}", "gists_url": "https://api.github.com/users/xuanchien/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuanchien/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuanchien/subscriptions", "organizations_url": "https://api.github.com/users/xuanchien/orgs", "repos_url": "https://api.github.com/users/xuanchien/repos", "events_url": "https://api.github.com/users/xuanchien/events{/privacy}", "received_events_url": "https://api.github.com/users/xuanchien/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-08-31T13:22:39Z", "updated_at": "2016-10-15T09:07:10Z", "closed_at": "2016-10-15T09:07:10Z", "author_association": "NONE", "body_html": "<p>I got the following error when using <code>tf.nn.bidirectional_dynamic_rnn</code>.</p>\n<pre><code>Traceback (most recent call last):\n  File \"main_cnn.py\", line 63, in &lt;module&gt;\n    main()\n  File \"main_cnn.py\", line 50, in main\n    model.run(num_epoch, learning_rate=learning_rate)\n  File \"/home/s1510032/research/programs/textsum-cnn/model.py\", line 266, in run\n    self.optim = self.opt.apply_gradients(zip(grads, params), global_step=self.global_step)\n  File \"/home/s1510032/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 298, in apply_gradients\n    (grads_and_vars,))\nValueError: No gradients provided for any variable: ((None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1e11c570d0&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1dfdd58e10&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1dfdd63690&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1dfdd7c5d0&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1e11c0f1d0&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1e11c22890&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1e11c35850&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1f103da890&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1f103dac50&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1f103dad10&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1f103e13d0&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1f1038cf10&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1f103aabd0&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1e119ecf90&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1e1197f150&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3d50&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3dd0&gt;))\n\n</code></pre>\n<p>If I keep everything and use <code>tf.nn.bidirectional_rnn</code> instead, this problem goes away. Here is the working code and non-working code. Loss is calculated based on outputs and that part is the same for both cases.</p>\n<p>Working code:</p>\n<pre><code>outputs, fw_states, _ = tf.nn.bidirectional_rnn(self.lstm_fw_cell,\n                         self.lstm_bw_cell,\n                         self.sent_cnn_outputs,\n                         dtype=tf.float32)\n\n # calculate loss using outputs\n</code></pre>\n<p>Non-working code:</p>\n<pre><code>outputs, output_states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, \n                    self.lstm_bw_cell,\n                    self.sent_cnn_outputs,\n                    time_major=True,\n                    sequence_length=self.sequence_length,\n                    dtype=tf.float32)\n\noutputs = tf.concat(2, outputs)\noutputs = tf.unpack(outputs, 0)\n\n# calculate loss using outputs\n</code></pre>\n<p>I suspect the issue here could because of <code>tf.unpack</code> function, which I use to make <code>outputs</code> become a list (for using in <code>zip</code> function in python with other list. Do you have any suggestion to resolve this?</p>", "body_text": "I got the following error when using tf.nn.bidirectional_dynamic_rnn.\nTraceback (most recent call last):\n  File \"main_cnn.py\", line 63, in <module>\n    main()\n  File \"main_cnn.py\", line 50, in main\n    model.run(num_epoch, learning_rate=learning_rate)\n  File \"/home/s1510032/research/programs/textsum-cnn/model.py\", line 266, in run\n    self.optim = self.opt.apply_gradients(zip(grads, params), global_step=self.global_step)\n  File \"/home/s1510032/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 298, in apply_gradients\n    (grads_and_vars,))\nValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c570d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd58e10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd63690>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd7c5d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c0f1d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c22890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c35850>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103da890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dac50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dad10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103e13d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f1038cf10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103aabd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e119ecf90>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e1197f150>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3d50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3dd0>))\n\n\nIf I keep everything and use tf.nn.bidirectional_rnn instead, this problem goes away. Here is the working code and non-working code. Loss is calculated based on outputs and that part is the same for both cases.\nWorking code:\noutputs, fw_states, _ = tf.nn.bidirectional_rnn(self.lstm_fw_cell,\n                         self.lstm_bw_cell,\n                         self.sent_cnn_outputs,\n                         dtype=tf.float32)\n\n # calculate loss using outputs\n\nNon-working code:\noutputs, output_states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, \n                    self.lstm_bw_cell,\n                    self.sent_cnn_outputs,\n                    time_major=True,\n                    sequence_length=self.sequence_length,\n                    dtype=tf.float32)\n\noutputs = tf.concat(2, outputs)\noutputs = tf.unpack(outputs, 0)\n\n# calculate loss using outputs\n\nI suspect the issue here could because of tf.unpack function, which I use to make outputs become a list (for using in zip function in python with other list. Do you have any suggestion to resolve this?", "body": "I got the following error when using `tf.nn.bidirectional_dynamic_rnn`. \n\n```\nTraceback (most recent call last):\n  File \"main_cnn.py\", line 63, in <module>\n    main()\n  File \"main_cnn.py\", line 50, in main\n    model.run(num_epoch, learning_rate=learning_rate)\n  File \"/home/s1510032/research/programs/textsum-cnn/model.py\", line 266, in run\n    self.optim = self.opt.apply_gradients(zip(grads, params), global_step=self.global_step)\n  File \"/home/s1510032/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 298, in apply_gradients\n    (grads_and_vars,))\nValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c570d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd58e10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd63690>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1dfdd7c5d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c0f1d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c22890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e11c35850>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103da890>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dac50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103dad10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103e13d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f1038cf10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1f103aabd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e119ecf90>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1e1197f150>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3d50>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f1d9f2c3dd0>))\n\n```\n\nIf I keep everything and use `tf.nn.bidirectional_rnn` instead, this problem goes away. Here is the working code and non-working code. Loss is calculated based on outputs and that part is the same for both cases.\n\nWorking code:\n\n```\noutputs, fw_states, _ = tf.nn.bidirectional_rnn(self.lstm_fw_cell,\n                         self.lstm_bw_cell,\n                         self.sent_cnn_outputs,\n                         dtype=tf.float32)\n\n # calculate loss using outputs\n```\n\nNon-working code:\n\n```\noutputs, output_states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, \n                    self.lstm_bw_cell,\n                    self.sent_cnn_outputs,\n                    time_major=True,\n                    sequence_length=self.sequence_length,\n                    dtype=tf.float32)\n\noutputs = tf.concat(2, outputs)\noutputs = tf.unpack(outputs, 0)\n\n# calculate loss using outputs\n```\n\nI suspect the issue here could because of `tf.unpack` function, which I use to make `outputs` become a list (for using in `zip` function in python with other list. Do you have any suggestion to resolve this?\n"}