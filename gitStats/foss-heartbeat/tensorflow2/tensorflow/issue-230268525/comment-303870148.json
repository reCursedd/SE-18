{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303870148", "html_url": "https://github.com/tensorflow/tensorflow/issues/10089#issuecomment-303870148", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10089", "id": 303870148, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzg3MDE0OA==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-24T22:29:39Z", "updated_at": "2017-05-24T22:29:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=88808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skye\">@skye</a> How about the following proposal?</p>\n<p>I have not really looked much at how functions (i.e., subgraphs) are defined in graphs, but intuitively I think the following solution makes sense.</p>\n<p>Currently, there exist gradient registration functions in the Python API and in the C++ API, as well as in my Scala API. Furthermore, the gradient back-propagation code is also defined in all three places. If we change the gradient registration functions to instead return a TF function (i.e., a sub-graph) that has the appropriate inputs and outputs, then that would make these sub-graphs \"transferrable\" between other language APIs and the C API.</p>\n<p>This should allow for the back-propagation code to only exist in the C++ and then we could have C API functions for registering op gradients, which would only need the op type name and the gradient function (i.e., sub-graph in this case) as arguments. This will allow registering gradient functions for yet unsupported gradients in the C++ side, while also avoiding replication of the gradient back-propagation code.</p>\n<p>I hope my brief description makes sense.</p>", "body_text": "@skye How about the following proposal?\nI have not really looked much at how functions (i.e., subgraphs) are defined in graphs, but intuitively I think the following solution makes sense.\nCurrently, there exist gradient registration functions in the Python API and in the C++ API, as well as in my Scala API. Furthermore, the gradient back-propagation code is also defined in all three places. If we change the gradient registration functions to instead return a TF function (i.e., a sub-graph) that has the appropriate inputs and outputs, then that would make these sub-graphs \"transferrable\" between other language APIs and the C API.\nThis should allow for the back-propagation code to only exist in the C++ and then we could have C API functions for registering op gradients, which would only need the op type name and the gradient function (i.e., sub-graph in this case) as arguments. This will allow registering gradient functions for yet unsupported gradients in the C++ side, while also avoiding replication of the gradient back-propagation code.\nI hope my brief description makes sense.", "body": "@skye How about the following proposal?\r\n\r\nI have not really looked much at how functions (i.e., subgraphs) are defined in graphs, but intuitively I think the following solution makes sense.\r\n\r\nCurrently, there exist gradient registration functions in the Python API and in the C++ API, as well as in my Scala API. Furthermore, the gradient back-propagation code is also defined in all three places. If we change the gradient registration functions to instead return a TF function (i.e., a sub-graph) that has the appropriate inputs and outputs, then that would make these sub-graphs \"transferrable\" between other language APIs and the C API.\r\n\r\nThis should allow for the back-propagation code to only exist in the C++ and then we could have C API functions for registering op gradients, which would only need the op type name and the gradient function (i.e., sub-graph in this case) as arguments. This will allow registering gradient functions for yet unsupported gradients in the C++ side, while also avoiding replication of the gradient back-propagation code.\r\n\r\nI hope my brief description makes sense."}