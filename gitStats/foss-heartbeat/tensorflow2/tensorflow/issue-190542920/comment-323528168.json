{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323528168", "html_url": "https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323528168", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5729", "id": 323528168, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzUyODE2OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-19T14:59:37Z", "updated_at": "2017-08-19T14:59:37Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">However we should definitely look into optimizing performance as well.  I'm\nassuming the pip package was built with -c opt?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Aug 19, 2017 7:58 AM, \"Eugene Brevdo\" ***@***.***&gt; wrote:\n Have you tried running the benchmark_model binary with your model?  The\n python bindings are good for prototyping but the best performance check for\n inference time is benchmark_model.\n\n On Aug 19, 2017 12:07 AM, \"Matt Feury\" ***@***.***&gt; wrote:\n\n hey all,\n\n first off thanks for all the work here. I've been struggling to get TF\n setup on my Pi A+ (armv6), but with the recent work in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"249454413\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12190\" href=\"https://github.com/tensorflow/tensorflow/pull/12190\">#12190</a>\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"249454413\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12190\" href=\"https://github.com/tensorflow/tensorflow/pull/12190\">#12190</a>&gt;, i was able to\n build, install, and run TF. sweet!\n\n i was curious what type of benchmarks others were seeing though. I am\n running (through Keras) a very basic predict on a somewhat small CNN, and I\n am seeing anywhere from 10-40s predict times (not including loading the\n model, imports, etc). This seems extremely high to me consider switching\n the Keras backend to Theano causes predictions to stick around 5s.\n\n Is there perhaps someway to optimize the installation? I've tried\n overclocking and shrinking/expanding gpu memory, but everything is still\n 10+. As far as I know, it can't leverage the GPU but my next step is to try\n building the wheel with gpu support.\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"190542920\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5729\" href=\"https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323505961\">#5729 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim7odRybLFXkJbW-3MSuEaGfHedepks5sZom3gaJpZM4K3eBk\">https://github.com/notifications/unsubscribe-auth/ABtim7odRybLFXkJbW-3MSuEaGfHedepks5sZom3gaJpZM4K3eBk</a>&gt;\n .\n\n\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "However we should definitely look into optimizing performance as well.  I'm\nassuming the pip package was built with -c opt?\n\u2026\nOn Aug 19, 2017 7:58 AM, \"Eugene Brevdo\" ***@***.***> wrote:\n Have you tried running the benchmark_model binary with your model?  The\n python bindings are good for prototyping but the best performance check for\n inference time is benchmark_model.\n\n On Aug 19, 2017 12:07 AM, \"Matt Feury\" ***@***.***> wrote:\n\n hey all,\n\n first off thanks for all the work here. I've been struggling to get TF\n setup on my Pi A+ (armv6), but with the recent work in #12190\n <#12190>, i was able to\n build, install, and run TF. sweet!\n\n i was curious what type of benchmarks others were seeing though. I am\n running (through Keras) a very basic predict on a somewhat small CNN, and I\n am seeing anywhere from 10-40s predict times (not including loading the\n model, imports, etc). This seems extremely high to me consider switching\n the Keras backend to Theano causes predictions to stick around 5s.\n\n Is there perhaps someway to optimize the installation? I've tried\n overclocking and shrinking/expanding gpu memory, but everything is still\n 10+. As far as I know, it can't leverage the GPU but my next step is to try\n building the wheel with gpu support.\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n <#5729 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim7odRybLFXkJbW-3MSuEaGfHedepks5sZom3gaJpZM4K3eBk>\n .", "body": "However we should definitely look into optimizing performance as well.  I'm\nassuming the pip package was built with -c opt?\n\nOn Aug 19, 2017 7:58 AM, \"Eugene Brevdo\" <ebrevdo@google.com> wrote:\n\n> Have you tried running the benchmark_model binary with your model?  The\n> python bindings are good for prototyping but the best performance check for\n> inference time is benchmark_model.\n>\n> On Aug 19, 2017 12:07 AM, \"Matt Feury\" <notifications@github.com> wrote:\n>\n> hey all,\n>\n> first off thanks for all the work here. I've been struggling to get TF\n> setup on my Pi A+ (armv6), but with the recent work in #12190\n> <https://github.com/tensorflow/tensorflow/pull/12190>, i was able to\n> build, install, and run TF. sweet!\n>\n> i was curious what type of benchmarks others were seeing though. I am\n> running (through Keras) a very basic predict on a somewhat small CNN, and I\n> am seeing anywhere from 10-40s predict times (not including loading the\n> model, imports, etc). This seems extremely high to me consider switching\n> the Keras backend to Theano causes predictions to stick around 5s.\n>\n> Is there perhaps someway to optimize the installation? I've tried\n> overclocking and shrinking/expanding gpu memory, but everything is still\n> 10+. As far as I know, it can't leverage the GPU but my next step is to try\n> building the wheel with gpu support.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323505961>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7odRybLFXkJbW-3MSuEaGfHedepks5sZom3gaJpZM4K3eBk>\n> .\n>\n>\n>\n"}