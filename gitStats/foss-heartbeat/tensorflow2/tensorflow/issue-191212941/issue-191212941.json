{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5803", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5803/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5803/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5803/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5803", "id": 191212941, "node_id": "MDU6SXNzdWUxOTEyMTI5NDE=", "number": 5803, "title": "The parameters are not updated using multi-gpu training. ", "user": {"login": "zhuangbohan", "id": 15856321, "node_id": "MDQ6VXNlcjE1ODU2MzIx", "avatar_url": "https://avatars3.githubusercontent.com/u/15856321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuangbohan", "html_url": "https://github.com/zhuangbohan", "followers_url": "https://api.github.com/users/zhuangbohan/followers", "following_url": "https://api.github.com/users/zhuangbohan/following{/other_user}", "gists_url": "https://api.github.com/users/zhuangbohan/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuangbohan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuangbohan/subscriptions", "organizations_url": "https://api.github.com/users/zhuangbohan/orgs", "repos_url": "https://api.github.com/users/zhuangbohan/repos", "events_url": "https://api.github.com/users/zhuangbohan/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuangbohan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-11-23T08:37:27Z", "updated_at": "2016-11-23T18:06:47Z", "closed_at": "2016-11-23T18:06:47Z", "author_association": "NONE", "body_html": "<p>from <strong>future</strong> import print_function</p>\n<p>import tensorflow as tf<br>\nimport numpy as np</p>\n<p>##------ Import MNIST data<br>\nfrom tensorflow.examples.tutorials.mnist import input_data<br>\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)</p>\n<p>##------- Parameters<br>\nlearning_rate = 0.001<br>\ntraining_iters = 100000<br>\nbatch_size = 32<br>\ndisplay_step = 10<br>\nnum_gpus = 2</p>\n<p>##--------- Network Parameters<br>\nn_input = 784 # MNIST data input (img shape: 28*28)<br>\nn_classes = 10 # MNIST total classes (0-9 digits)<br>\ndropout = 0.75 # Dropout, probability to keep units</p>\n<p>##------- Create some wrappers for simplicity</p>\n<p>def conv2d(x, W, b, strides=1):<br>\n# Conv2D wrapper, with bias and relu activation<br>\nx = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')<br>\nx = tf.nn.bias_add(x, b)<br>\nreturn tf.nn.relu(x)</p>\n<p>def maxpool2d(x, k=2):<br>\n# MaxPool2D wrapper<br>\nreturn tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],<br>\npadding='SAME')</p>\n<p>def average_gradients(tower_grads):</p>\n<pre><code>average_grads = []\nfor grad_and_vars in zip(*tower_grads):\n# Note that each grad_and_vars looks like the following:\n#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n  # Add 0 dimension to the gradients to represent the tower.\n        expanded_g = tf.expand_dims(g, 0)\n\n  # Append on a 'tower' dimension which we will average over below.\n        grads.append(expanded_g)\n\n# Average over the 'tower' dimension.\n        grad = tf.concat(0, grads)\n        grad = tf.reduce_mean(grad, 0)\n\n# Keep in mind that the Variables are redundant because they are shared\n# across towers. So .. we will just return the first tower's pointer to\n# the Variable.\nv = grad_and_vars[0][1]\ngrad_and_var = (grad, v)\naverage_grads.append(grad_and_var)\nreturn average_grads\n</code></pre>\n<p>##--------   Create model<br>\ndef conv_net(images, dropout):<br>\n# Reshape input picture<br>\nimages = tf.reshape(images, shape=[-1, 28, 28, 1])</p>\n<pre><code># Convolution Layer\nwith tf.variable_scope('conv1'):\n    W = tf.get_variable('weights', [5, 5, 1, 32], initializer=tf.random_normal_initializer())\n    b = tf.get_variable(\"biases\", [32], initializer=tf.random_normal_initializer())\n    conv1 = conv2d(images, W, b)\n    # Max Pooling (down-sampling)\n    conv1 = maxpool2d(conv1, k=2)\n\n# Convolution Layer\nwith tf.variable_scope('conv2'):\n    W = tf.get_variable('weights', [5, 5, 32, 64], initializer=tf.random_normal_initializer())\n    b = tf.get_variable(\"biases\", [64], initializer=tf.random_normal_initializer())\n# Max Pooling (down-sampling)\n    conv2 = conv2d(conv1, W, b)\n    conv2 = maxpool2d(conv2, k=2)\n\n# Fully connected layer\n# Reshape conv2 output to fit fully connected layer input\nfc1 = tf.reshape(conv2, [-1, 7*7*64])\n\nwith tf.variable_scope('fully'):\n    weights = tf.get_variable('weights', [7*7*64, 1024], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable('bias', [1024], initializer=tf.random_normal_initializer())\n    fc1 = tf.add(tf.matmul(fc1, weights), bias)\n    fc1 = tf.nn.relu(fc1)\n# Apply Dropout\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n# Output, class prediction\n\nwith tf.variable_scope('softmax'):\n    weights = tf.get_variable('weights', [1024, 10], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable('bias', [10], initializer=tf.random_normal_initializer())     \n    out = tf.add(tf.matmul(fc1, weights), bias)\n\nreturn out\n</code></pre>\n<p>##----------    multi-GPU</p>\n<p>def train():</p>\n<pre><code>with tf.Graph().as_default():\n\n    x = tf.placeholder(tf.float32, [batch_size*num_gpus, 784])\n    y = tf.placeholder(tf.float32, [batch_size*num_gpus, n_classes])\n    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n\n    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    tower_grads = []\n    for i in xrange(num_gpus):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('MNIST', i)) as scope:\n      # Calculate the loss for one tower of the CIFAR model. This function\n      # constructs the entire CIFAR model but shares the variables across\n      # all towers\n      # Reuse variables for the next tower.\n\n                next_batch = x[i*batch_size:(i+1)*batch_size, :]\n                next_label_batch = y[i*batch_size:(i+1)*batch_size, :]\n                pred = conv_net(next_batch, keep_prob)\n                cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, next_label_batch))\n                correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(next_label_batch, 1))\n                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n                tf.get_variable_scope().reuse_variables()\n\n      # Calculate the gradients for the batch of data on this CIFAR tower.\n                grads = optimizer.compute_gradients(cost)\n      # Keep track of the gradients across all towers.\n                tower_grads.append(grads)\n\n\n\n    grads = average_gradients(tower_grads)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n</code></pre>\n<p>##--------------------start training----------------------</p>\n<p>##---------- Initializing the variables<br>\ninit = tf.initialize_all_variables()</p>\n<p>##----------- Launch the graph<br>\nwith tf.Session() as sess:</p>\n<pre><code>        sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=True))\n        sess.run(init)\n        saver = tf.train.Saver()\n        var = [v for v in tf.trainable_variables()][0]\n        aa = sess.run(var)\n        np.save('initial.npy', aa)\n\n        step = 1\n# Keep training until reach max iterations\n        while step * (num_gpus*batch_size) &lt; training_iters:\n\n            large_input = np.zeros((num_gpus*batch_size, n_input))\n            large_labels = np.zeros((num_gpus*batch_size, n_classes))\n    \n            for index in xrange(num_gpus):\n                batch_x, batch_y = mnist.train.next_batch(batch_size)\n                large_input[index*batch_size:(index+1)*batch_size, :] = batch_x\n                large_labels[index*batch_size:(index+1)*batch_size, :] = batch_y\n\n    # Run optimization op (backprop)\n            sess.run(apply_gradient_op, feed_dict={x: large_input, y: large_labels,\n                                   keep_prob: dropout})\n            if step % display_step == 0:\n        # Calculate batch loss and accuracy\n                loss, acc = sess.run([cost, accuracy], feed_dict={x: large_input,\n                                                          y: large_labels,\n                                                          keep_prob: 1.})\n                print(\"Iter \" + str(step*num_gpus*batch_size) + \", Minibatch Loss= \" + \\\n              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n              \"{:.5f}\".format(acc))\n            step += 1\n        print(\"Optimization Finished!\")\n        var = [v for v in tf.trainable_variables()][0]\n        aa = sess.run(var)\n        np.save('final.npy', aa)\n</code></pre>\n<p>##-----------main---<br>\nif <strong>name</strong> == '<strong>main</strong>':<br>\ntrain()</p>\n<p>I'm a beginner of Tensorflow and wrote a piece of code to test on the MNIST dataset. I find that during training my parameters doesn't change at all and cannot find the problem, so SOS!!!</p>", "body_text": "from future import print_function\nimport tensorflow as tf\nimport numpy as np\n##------ Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n##------- Parameters\nlearning_rate = 0.001\ntraining_iters = 100000\nbatch_size = 32\ndisplay_step = 10\nnum_gpus = 2\n##--------- Network Parameters\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\ndropout = 0.75 # Dropout, probability to keep units\n##------- Create some wrappers for simplicity\ndef conv2d(x, W, b, strides=1):\n# Conv2D wrapper, with bias and relu activation\nx = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\nx = tf.nn.bias_add(x, b)\nreturn tf.nn.relu(x)\ndef maxpool2d(x, k=2):\n# MaxPool2D wrapper\nreturn tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\npadding='SAME')\ndef average_gradients(tower_grads):\naverage_grads = []\nfor grad_and_vars in zip(*tower_grads):\n# Note that each grad_and_vars looks like the following:\n#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n  # Add 0 dimension to the gradients to represent the tower.\n        expanded_g = tf.expand_dims(g, 0)\n\n  # Append on a 'tower' dimension which we will average over below.\n        grads.append(expanded_g)\n\n# Average over the 'tower' dimension.\n        grad = tf.concat(0, grads)\n        grad = tf.reduce_mean(grad, 0)\n\n# Keep in mind that the Variables are redundant because they are shared\n# across towers. So .. we will just return the first tower's pointer to\n# the Variable.\nv = grad_and_vars[0][1]\ngrad_and_var = (grad, v)\naverage_grads.append(grad_and_var)\nreturn average_grads\n\n##--------   Create model\ndef conv_net(images, dropout):\n# Reshape input picture\nimages = tf.reshape(images, shape=[-1, 28, 28, 1])\n# Convolution Layer\nwith tf.variable_scope('conv1'):\n    W = tf.get_variable('weights', [5, 5, 1, 32], initializer=tf.random_normal_initializer())\n    b = tf.get_variable(\"biases\", [32], initializer=tf.random_normal_initializer())\n    conv1 = conv2d(images, W, b)\n    # Max Pooling (down-sampling)\n    conv1 = maxpool2d(conv1, k=2)\n\n# Convolution Layer\nwith tf.variable_scope('conv2'):\n    W = tf.get_variable('weights', [5, 5, 32, 64], initializer=tf.random_normal_initializer())\n    b = tf.get_variable(\"biases\", [64], initializer=tf.random_normal_initializer())\n# Max Pooling (down-sampling)\n    conv2 = conv2d(conv1, W, b)\n    conv2 = maxpool2d(conv2, k=2)\n\n# Fully connected layer\n# Reshape conv2 output to fit fully connected layer input\nfc1 = tf.reshape(conv2, [-1, 7*7*64])\n\nwith tf.variable_scope('fully'):\n    weights = tf.get_variable('weights', [7*7*64, 1024], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable('bias', [1024], initializer=tf.random_normal_initializer())\n    fc1 = tf.add(tf.matmul(fc1, weights), bias)\n    fc1 = tf.nn.relu(fc1)\n# Apply Dropout\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n# Output, class prediction\n\nwith tf.variable_scope('softmax'):\n    weights = tf.get_variable('weights', [1024, 10], initializer=tf.random_normal_initializer())\n    bias = tf.get_variable('bias', [10], initializer=tf.random_normal_initializer())     \n    out = tf.add(tf.matmul(fc1, weights), bias)\n\nreturn out\n\n##----------    multi-GPU\ndef train():\nwith tf.Graph().as_default():\n\n    x = tf.placeholder(tf.float32, [batch_size*num_gpus, 784])\n    y = tf.placeholder(tf.float32, [batch_size*num_gpus, n_classes])\n    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n\n    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    tower_grads = []\n    for i in xrange(num_gpus):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('MNIST', i)) as scope:\n      # Calculate the loss for one tower of the CIFAR model. This function\n      # constructs the entire CIFAR model but shares the variables across\n      # all towers\n      # Reuse variables for the next tower.\n\n                next_batch = x[i*batch_size:(i+1)*batch_size, :]\n                next_label_batch = y[i*batch_size:(i+1)*batch_size, :]\n                pred = conv_net(next_batch, keep_prob)\n                cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, next_label_batch))\n                correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(next_label_batch, 1))\n                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n                tf.get_variable_scope().reuse_variables()\n\n      # Calculate the gradients for the batch of data on this CIFAR tower.\n                grads = optimizer.compute_gradients(cost)\n      # Keep track of the gradients across all towers.\n                tower_grads.append(grads)\n\n\n\n    grads = average_gradients(tower_grads)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n##--------------------start training----------------------\n##---------- Initializing the variables\ninit = tf.initialize_all_variables()\n##----------- Launch the graph\nwith tf.Session() as sess:\n        sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=True))\n        sess.run(init)\n        saver = tf.train.Saver()\n        var = [v for v in tf.trainable_variables()][0]\n        aa = sess.run(var)\n        np.save('initial.npy', aa)\n\n        step = 1\n# Keep training until reach max iterations\n        while step * (num_gpus*batch_size) < training_iters:\n\n            large_input = np.zeros((num_gpus*batch_size, n_input))\n            large_labels = np.zeros((num_gpus*batch_size, n_classes))\n    \n            for index in xrange(num_gpus):\n                batch_x, batch_y = mnist.train.next_batch(batch_size)\n                large_input[index*batch_size:(index+1)*batch_size, :] = batch_x\n                large_labels[index*batch_size:(index+1)*batch_size, :] = batch_y\n\n    # Run optimization op (backprop)\n            sess.run(apply_gradient_op, feed_dict={x: large_input, y: large_labels,\n                                   keep_prob: dropout})\n            if step % display_step == 0:\n        # Calculate batch loss and accuracy\n                loss, acc = sess.run([cost, accuracy], feed_dict={x: large_input,\n                                                          y: large_labels,\n                                                          keep_prob: 1.})\n                print(\"Iter \" + str(step*num_gpus*batch_size) + \", Minibatch Loss= \" + \\\n              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n              \"{:.5f}\".format(acc))\n            step += 1\n        print(\"Optimization Finished!\")\n        var = [v for v in tf.trainable_variables()][0]\n        aa = sess.run(var)\n        np.save('final.npy', aa)\n\n##-----------main---\nif name == 'main':\ntrain()\nI'm a beginner of Tensorflow and wrote a piece of code to test on the MNIST dataset. I find that during training my parameters doesn't change at all and cannot find the problem, so SOS!!!", "body": "from __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n##------ Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n\r\n##------- Parameters\r\nlearning_rate = 0.001\r\ntraining_iters = 100000\r\nbatch_size = 32\r\ndisplay_step = 10\r\nnum_gpus = 2\r\n\r\n##--------- Network Parameters\r\nn_input = 784 # MNIST data input (img shape: 28*28)\r\nn_classes = 10 # MNIST total classes (0-9 digits)\r\ndropout = 0.75 # Dropout, probability to keep units\r\n\r\n\r\n##------- Create some wrappers for simplicity\r\n\r\ndef conv2d(x, W, b, strides=1):\r\n    # Conv2D wrapper, with bias and relu activation\r\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\r\n    x = tf.nn.bias_add(x, b)\r\n    return tf.nn.relu(x)\r\n\r\n\r\ndef maxpool2d(x, k=2):\r\n    # MaxPool2D wrapper\r\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\r\n                          padding='SAME')\r\n\r\n\r\ndef average_gradients(tower_grads):\r\n\r\n    average_grads = []\r\n    for grad_and_vars in zip(*tower_grads):\r\n    # Note that each grad_and_vars looks like the following:\r\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n        grads = []\r\n        for g, _ in grad_and_vars:\r\n      # Add 0 dimension to the gradients to represent the tower.\r\n            expanded_g = tf.expand_dims(g, 0)\r\n\r\n      # Append on a 'tower' dimension which we will average over below.\r\n            grads.append(expanded_g)\r\n\r\n    # Average over the 'tower' dimension.\r\n            grad = tf.concat(0, grads)\r\n            grad = tf.reduce_mean(grad, 0)\r\n\r\n    # Keep in mind that the Variables are redundant because they are shared\r\n    # across towers. So .. we will just return the first tower's pointer to\r\n    # the Variable.\r\n    v = grad_and_vars[0][1]\r\n    grad_and_var = (grad, v)\r\n    average_grads.append(grad_and_var)\r\n    return average_grads\r\n\r\n\r\n\r\n##--------   Create model\r\ndef conv_net(images, dropout):\r\n    # Reshape input picture\r\n    images = tf.reshape(images, shape=[-1, 28, 28, 1])\r\n\r\n    # Convolution Layer\r\n    with tf.variable_scope('conv1'):\r\n        W = tf.get_variable('weights', [5, 5, 1, 32], initializer=tf.random_normal_initializer())\r\n        b = tf.get_variable(\"biases\", [32], initializer=tf.random_normal_initializer())\r\n        conv1 = conv2d(images, W, b)\r\n        # Max Pooling (down-sampling)\r\n        conv1 = maxpool2d(conv1, k=2)\r\n\r\n    # Convolution Layer\r\n    with tf.variable_scope('conv2'):\r\n        W = tf.get_variable('weights', [5, 5, 32, 64], initializer=tf.random_normal_initializer())\r\n        b = tf.get_variable(\"biases\", [64], initializer=tf.random_normal_initializer())\r\n    # Max Pooling (down-sampling)\r\n        conv2 = conv2d(conv1, W, b)\r\n        conv2 = maxpool2d(conv2, k=2)\r\n\r\n    # Fully connected layer\r\n    # Reshape conv2 output to fit fully connected layer input\r\n    fc1 = tf.reshape(conv2, [-1, 7*7*64])\r\n\r\n    with tf.variable_scope('fully'):\r\n        weights = tf.get_variable('weights', [7*7*64, 1024], initializer=tf.random_normal_initializer())\r\n        bias = tf.get_variable('bias', [1024], initializer=tf.random_normal_initializer())\r\n        fc1 = tf.add(tf.matmul(fc1, weights), bias)\r\n        fc1 = tf.nn.relu(fc1)\r\n    # Apply Dropout\r\n        fc1 = tf.nn.dropout(fc1, dropout)\r\n\r\n    # Output, class prediction\r\n\r\n    with tf.variable_scope('softmax'):\r\n        weights = tf.get_variable('weights', [1024, 10], initializer=tf.random_normal_initializer())\r\n        bias = tf.get_variable('bias', [10], initializer=tf.random_normal_initializer())     \r\n        out = tf.add(tf.matmul(fc1, weights), bias)\r\n\r\n    return out\r\n\r\n##----------    multi-GPU\r\n\r\ndef train():\r\n\r\n    with tf.Graph().as_default():\r\n\r\n        x = tf.placeholder(tf.float32, [batch_size*num_gpus, 784])\r\n        y = tf.placeholder(tf.float32, [batch_size*num_gpus, n_classes])\r\n        keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\r\n\r\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\r\n\r\n\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n        tower_grads = []\r\n        for i in xrange(num_gpus):\r\n            with tf.device('/gpu:%d' % i):\r\n                with tf.name_scope('%s_%d' % ('MNIST', i)) as scope:\r\n          # Calculate the loss for one tower of the CIFAR model. This function\r\n          # constructs the entire CIFAR model but shares the variables across\r\n          # all towers\r\n          # Reuse variables for the next tower.\r\n\r\n                    next_batch = x[i*batch_size:(i+1)*batch_size, :]\r\n                    next_label_batch = y[i*batch_size:(i+1)*batch_size, :]\r\n                    pred = conv_net(next_batch, keep_prob)\r\n                    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, next_label_batch))\r\n                    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(next_label_batch, 1))\r\n                    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n                    tf.get_variable_scope().reuse_variables()\r\n\r\n          # Calculate the gradients for the batch of data on this CIFAR tower.\r\n                    grads = optimizer.compute_gradients(cost)\r\n          # Keep track of the gradients across all towers.\r\n                    tower_grads.append(grads)\r\n\r\n\r\n\r\n        grads = average_gradients(tower_grads)\r\n        apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\r\n\r\n\r\n##--------------------start training----------------------\r\n\r\n##---------- Initializing the variables\r\n        init = tf.initialize_all_variables()\r\n\r\n##----------- Launch the graph\r\n        with tf.Session() as sess:\r\n\r\n            sess = tf.Session(config=tf.ConfigProto(\r\n            allow_soft_placement=True,\r\n            log_device_placement=True))\r\n            sess.run(init)\r\n            saver = tf.train.Saver()\r\n            var = [v for v in tf.trainable_variables()][0]\r\n            aa = sess.run(var)\r\n            np.save('initial.npy', aa)\r\n\r\n            step = 1\r\n    # Keep training until reach max iterations\r\n            while step * (num_gpus*batch_size) < training_iters:\r\n\r\n                large_input = np.zeros((num_gpus*batch_size, n_input))\r\n                large_labels = np.zeros((num_gpus*batch_size, n_classes))\r\n        \r\n                for index in xrange(num_gpus):\r\n                    batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n                    large_input[index*batch_size:(index+1)*batch_size, :] = batch_x\r\n                    large_labels[index*batch_size:(index+1)*batch_size, :] = batch_y\r\n\r\n        # Run optimization op (backprop)\r\n                sess.run(apply_gradient_op, feed_dict={x: large_input, y: large_labels,\r\n                                       keep_prob: dropout})\r\n                if step % display_step == 0:\r\n            # Calculate batch loss and accuracy\r\n                    loss, acc = sess.run([cost, accuracy], feed_dict={x: large_input,\r\n                                                              y: large_labels,\r\n                                                              keep_prob: 1.})\r\n                    print(\"Iter \" + str(step*num_gpus*batch_size) + \", Minibatch Loss= \" + \\\r\n                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\r\n                  \"{:.5f}\".format(acc))\r\n                step += 1\r\n            print(\"Optimization Finished!\")\r\n            var = [v for v in tf.trainable_variables()][0]\r\n            aa = sess.run(var)\r\n            np.save('final.npy', aa)\r\n\r\n##-----------main---\r\nif __name__ == '__main__':\r\n    train()\r\n\r\n\r\nI'm a beginner of Tensorflow and wrote a piece of code to test on the MNIST dataset. I find that during training my parameters doesn't change at all and cannot find the problem, so SOS!!!\r\n"}