{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22059", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22059/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22059/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22059/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22059", "id": 356911243, "node_id": "MDU6SXNzdWUzNTY5MTEyNDM=", "number": 22059, "title": "Optimizing slice of variable not possible", "user": {"login": "Hoeze", "id": 1200058, "node_id": "MDQ6VXNlcjEyMDAwNTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1200058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hoeze", "html_url": "https://github.com/Hoeze", "followers_url": "https://api.github.com/users/Hoeze/followers", "following_url": "https://api.github.com/users/Hoeze/following{/other_user}", "gists_url": "https://api.github.com/users/Hoeze/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hoeze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hoeze/subscriptions", "organizations_url": "https://api.github.com/users/Hoeze/orgs", "repos_url": "https://api.github.com/users/Hoeze/repos", "events_url": "https://api.github.com/users/Hoeze/events{/privacy}", "received_events_url": "https://api.github.com/users/Hoeze/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-04T17:44:33Z", "updated_at": "2018-11-11T18:41:26Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Applying the gradient of a variable slice currently results in a <code>NotImplemented</code> error of tf.train.Optimizer.</p>\n<p><strong>The following two examples are working:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>## WORKING ###</span>\nX <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\ny <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\nloss <span class=\"pl-k\">=</span> y <span class=\"pl-k\">-</span> (X<span class=\"pl-k\">*</span>X)\n\nvariables<span class=\"pl-k\">=</span>[X]\ngradient <span class=\"pl-k\">=</span> tf.gradients(loss, variables)\ngradient <span class=\"pl-k\">=</span> [(g, v) <span class=\"pl-k\">for</span> g, v <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(gradient, variables)]\ntrain_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer().apply_gradients(gradient)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>## WORKING ###</span>\nbig_X <span class=\"pl-k\">=</span> tf.Variable([<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nX <span class=\"pl-k\">=</span> big_X[<span class=\"pl-c1\">0</span>]\ny <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\nloss <span class=\"pl-k\">=</span> y <span class=\"pl-k\">-</span> (X<span class=\"pl-k\">*</span>X)\n\ntrain_op <span class=\"pl-k\">=</span> train_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer().minimize(loss)</pre></div>\n<p><strong>The following example throws an error:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>## NOT WORKING ###</span>\nbig_X <span class=\"pl-k\">=</span> tf.Variable([<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nX <span class=\"pl-k\">=</span> big_X[<span class=\"pl-c1\">0</span>]\ny <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\nloss <span class=\"pl-k\">=</span> y <span class=\"pl-k\">-</span> (X<span class=\"pl-k\">*</span>X)\n\nvariables<span class=\"pl-k\">=</span>[X]\ngradient <span class=\"pl-k\">=</span> tf.gradients(loss, variables)\ngradient <span class=\"pl-k\">=</span> [(g, v) <span class=\"pl-k\">for</span> g, v <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(gradient, variables)]\ntrain_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer().apply_gradients(gradient)</pre></div>\n<p>The error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-22-10282dee2005&gt;\", line 10, in &lt;module&gt;\n    train_op = tf.train.AdamOptimizer().apply_gradients(gradient)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 605, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 189, in update_op\n    raise NotImplementedError(\"Trying to update a Tensor \", self._v)\nNotImplementedError: ('Trying to update a Tensor ', &lt;tf.Tensor 'strided_slice_9:0' shape=() dtype=float32&gt;)\n</code></pre>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 18.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: NA</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.10.1-0-g4dcfddc5d1 1.10.1</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>: NA</li>\n</ul>", "body_text": "Applying the gradient of a variable slice currently results in a NotImplemented error of tf.train.Optimizer.\nThe following two examples are working:\n### WORKING ###\nX = tf.Variable(2, dtype=tf.float32)\ny = tf.constant(10, dtype=\"float32\")\nloss = y - (X*X)\n\nvariables=[X]\ngradient = tf.gradients(loss, variables)\ngradient = [(g, v) for g, v in zip(gradient, variables)]\ntrain_op = tf.train.AdamOptimizer().apply_gradients(gradient)\n### WORKING ###\nbig_X = tf.Variable([2,3,4], dtype=tf.float32)\nX = big_X[0]\ny = tf.constant(10, dtype=\"float32\")\nloss = y - (X*X)\n\ntrain_op = train_op = tf.train.AdamOptimizer().minimize(loss)\nThe following example throws an error:\n### NOT WORKING ###\nbig_X = tf.Variable([2,3,4], dtype=tf.float32)\nX = big_X[0]\ny = tf.constant(10, dtype=\"float32\")\nloss = y - (X*X)\n\nvariables=[X]\ngradient = tf.gradients(loss, variables)\ngradient = [(g, v) for g, v in zip(gradient, variables)]\ntrain_op = tf.train.AdamOptimizer().apply_gradients(gradient)\nThe error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-10282dee2005>\", line 10, in <module>\n    train_op = tf.train.AdamOptimizer().apply_gradients(gradient)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 605, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 189, in update_op\n    raise NotImplementedError(\"Trying to update a Tensor \", self._v)\nNotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'strided_slice_9:0' shape=() dtype=float32>)\n\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.10.1-0-g4dcfddc5d1 1.10.1\nPython version: 3.6.5\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: NA", "body": "Applying the gradient of a variable slice currently results in a `NotImplemented` error of tf.train.Optimizer.\r\n\r\n**The following two examples are working:**\r\n```python\r\n### WORKING ###\r\nX = tf.Variable(2, dtype=tf.float32)\r\ny = tf.constant(10, dtype=\"float32\")\r\nloss = y - (X*X)\r\n\r\nvariables=[X]\r\ngradient = tf.gradients(loss, variables)\r\ngradient = [(g, v) for g, v in zip(gradient, variables)]\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(gradient)\r\n```\r\n\r\n```python\r\n### WORKING ###\r\nbig_X = tf.Variable([2,3,4], dtype=tf.float32)\r\nX = big_X[0]\r\ny = tf.constant(10, dtype=\"float32\")\r\nloss = y - (X*X)\r\n\r\ntrain_op = train_op = tf.train.AdamOptimizer().minimize(loss)\r\n```\r\n\r\n**The following example throws an error:**\r\n```python\r\n### NOT WORKING ###\r\nbig_X = tf.Variable([2,3,4], dtype=tf.float32)\r\nX = big_X[0]\r\ny = tf.constant(10, dtype=\"float32\")\r\nloss = y - (X*X)\r\n\r\nvariables=[X]\r\ngradient = tf.gradients(loss, variables)\r\ngradient = [(g, v) for g, v in zip(gradient, variables)]\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(gradient)\r\n```\r\nThe error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-22-10282dee2005>\", line 10, in <module>\r\n    train_op = tf.train.AdamOptimizer().apply_gradients(gradient)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 605, in apply_gradients\r\n    update_ops.append(processor.update_op(self, grad))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 189, in update_op\r\n    raise NotImplementedError(\"Trying to update a Tensor \", self._v)\r\nNotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'strided_slice_9:0' shape=() dtype=float32>)\r\n```\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA"}