{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/168237741", "html_url": "https://github.com/tensorflow/tensorflow/issues/654#issuecomment-168237741", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/654", "id": 168237741, "node_id": "MDEyOklzc3VlQ29tbWVudDE2ODIzNzc0MQ==", "user": {"login": "PrajitR", "id": 4674442, "node_id": "MDQ6VXNlcjQ2NzQ0NDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4674442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PrajitR", "html_url": "https://github.com/PrajitR", "followers_url": "https://api.github.com/users/PrajitR/followers", "following_url": "https://api.github.com/users/PrajitR/following{/other_user}", "gists_url": "https://api.github.com/users/PrajitR/gists{/gist_id}", "starred_url": "https://api.github.com/users/PrajitR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PrajitR/subscriptions", "organizations_url": "https://api.github.com/users/PrajitR/orgs", "repos_url": "https://api.github.com/users/PrajitR/repos", "events_url": "https://api.github.com/users/PrajitR/events{/privacy}", "received_events_url": "https://api.github.com/users/PrajitR/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-31T19:34:38Z", "updated_at": "2015-12-31T19:34:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a><br>\nHere's a self contained example demonstrating a possible beam search implementation:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    beam_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Number of hypotheses in beam.</span>\n    num_symbols <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Output vocabulary size.</span>\n    embedding_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    num_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\n    embedding <span class=\"pl-k\">=</span> tf.zeros([num_symbols, embedding_size])\n    output_projection <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> log_beam_probs: list of [beam_size, 1] Tensors</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  Ordered log probabilities of the `beam_size` best hypotheses</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  found in each beam step (highest probability first).</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> beam_symbols: list of [beam_size] Tensors </span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  The ordered `beam_size` words / symbols extracted by the beam</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  step, which will be appended to their corresponding hypotheses</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  (corresponding hypotheses found in `beam_path`).</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> beam_path: list of [beam_size] Tensor</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  The ordered `beam_size` parent indices. Their values range</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  from [0, `beam_size`), and they denote which previous</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  hypothesis each word should be appended to.</span>\n    log_beam_probs, beam_symbols, beam_path  <span class=\"pl-k\">=</span> [], [], []\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">beam_search</span>(<span class=\"pl-smi\">prev</span>, <span class=\"pl-smi\">i</span>):\n        <span class=\"pl-k\">if</span> output_projection <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n            prev <span class=\"pl-k\">=</span> tf.nn.xw_plus_b(\n                prev, output_projection[<span class=\"pl-c1\">0</span>], output_projection[<span class=\"pl-c1\">1</span>])\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute </span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>  log P(next_word, hypothesis) = </span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>  log P(next_word | hypothesis)*P(hypothesis) =</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>  log P(next_word | hypothesis) + log P(hypothesis)</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> for each hypothesis separately, then join them together </span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> on the same tensor dimension to form the example's </span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> beam probability distribution:</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> [P(word1, hypothesis1), P(word2, hypothesis1), ...,</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>  P(word1, hypothesis2), P(word2, hypothesis2), ...]</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> If TF had a log_sum_exp operator, then it would be </span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> more numerically stable to use: </span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>   probs = prev - tf.log_sum_exp(prev, reduction_dims=[1])</span>\n        probs <span class=\"pl-k\">=</span> tf.log(tf.nn.softmax(prev))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> i == 1 corresponds to the input being \"&lt;GO&gt;\", with</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> uniform prior probability and only the empty hypothesis</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> (each row is a separate example).</span>\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span>:\n            probs <span class=\"pl-k\">=</span> tf.reshape(probs <span class=\"pl-k\">+</span> log_beam_probs[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], \n                               [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, beam_size <span class=\"pl-k\">*</span> num_symbols])\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get the top `beam_size` candidates and reshape them such</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> that the number of rows = batch_size * beam_size, which</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> allows us to process each hypothesis independently.</span>\n        best_probs, indices <span class=\"pl-k\">=</span> tf.nn.top_k(probs, beam_size)\n        indices <span class=\"pl-k\">=</span> tf.stop_gradient(tf.squeeze(tf.reshape(indices, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>])))\n        best_probs <span class=\"pl-k\">=</span> tf.stop_gradient(tf.reshape(best_probs, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]))\n\n        symbols <span class=\"pl-k\">=</span> indices <span class=\"pl-k\">%</span> num_symbols <span class=\"pl-c\"><span class=\"pl-c\">#</span> Which word in vocabulary.</span>\n        beam_parent <span class=\"pl-k\">=</span> indices <span class=\"pl-k\">//</span> num_symbols <span class=\"pl-c\"><span class=\"pl-c\">#</span> Which hypothesis it came from.</span>\n\n        beam_symbols.append(symbols)\n        beam_path.append(beam_parent)\n        log_beam_probs.append(best_probs)\n        <span class=\"pl-k\">return</span> tf.nn.embedding_lookup(embedding, symbols)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Setting up graph.</span>\n    inputs <span class=\"pl-k\">=</span> [tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, num_symbols])\n              <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_steps)]\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_steps):\n        beam_search(inputs[i], i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Running the graph.</span>\n    input_vals <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]\n    l <span class=\"pl-k\">=</span> np.log\n    eps <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">10</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> exp(-10) ~= 0</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> These values mimic the distribution of vocabulary words</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> from each hypothesis independently (in log scale since</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> they will be put through exp() in softmax).</span>\n    input_vals[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> np.array([[<span class=\"pl-c1\">0</span>, eps, l(<span class=\"pl-c1\">2</span>), eps, l(<span class=\"pl-c1\">3</span>)]])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Step 1 beam hypotheses =</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (1) Path: [4], prob = log(1 / 2)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (2) Path: [2], prob = log(1 / 3)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (3) Path: [0], prob = log(1 / 6)</span>\n\n    input_vals[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> np.array([[l(<span class=\"pl-c1\">1.2</span>), <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, l(<span class=\"pl-c1\">1.1</span>), <span class=\"pl-c1\">0</span>], <span class=\"pl-c\"><span class=\"pl-c\">#</span> Path [4] </span>\n                              [<span class=\"pl-c1\">0</span>,   eps, eps, eps, eps], <span class=\"pl-c\"><span class=\"pl-c\">#</span> Path [2]</span>\n                              [<span class=\"pl-c1\">0</span>,  <span class=\"pl-c1\">0</span>,   <span class=\"pl-c1\">0</span>,   <span class=\"pl-c1\">0</span>,   <span class=\"pl-c1\">0</span>]])   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Path [0]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Step 2 beam hypotheses =</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (1) Path: [2, 0], prob = log(1 / 3) + log(1)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (2) Path: [4, 0], prob = log(1 / 2) + log(1.2 / 5.3)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (3) Path: [4, 3], prob = log(1 / 2) + log(1.1 / 5.3)</span>\n\n    input_vals[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">=</span> np.array([[<span class=\"pl-c1\">0</span>,  l(<span class=\"pl-c1\">1.1</span>), <span class=\"pl-c1\">0</span>,   <span class=\"pl-c1\">0</span>,   <span class=\"pl-c1\">0</span>], <span class=\"pl-c\"><span class=\"pl-c\">#</span> Path [2, 0]</span>\n                              [eps, <span class=\"pl-c1\">0</span>,   eps, eps, eps], <span class=\"pl-c\"><span class=\"pl-c\">#</span> Path [4, 0]</span>\n                              [eps, eps, eps, eps, <span class=\"pl-c1\">0</span>]])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Path [4, 3]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Step 3 beam hypotheses =</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (1) Path: [4, 0, 1], prob = log(1 / 2) + log(1.2 / 5.3) + log(1)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (2) Path: [4, 3, 4], prob = log(1 / 2) + log(1.1 / 5.3) + log(1)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (3) Path: [2, 0, 1], prob = log(1 / 3) + log(1) + log(1.1 / 5.1)</span>\n\n    input_feed <span class=\"pl-k\">=</span> {inputs[i]: input_vals[i][:beam_size, :] \n                  <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(num_steps)} \n    output_feed <span class=\"pl-k\">=</span> beam_symbols <span class=\"pl-k\">+</span> beam_path <span class=\"pl-k\">+</span> log_beam_probs\n    session <span class=\"pl-k\">=</span> tf.InteractiveSession()\n    outputs <span class=\"pl-k\">=</span> session.run(output_feed, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>input_feed)\n\n    expected_beam_symbols <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>],\n                             [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>],\n                             [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>]]\n    expected_beam_path <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>],\n                          [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>],\n                          [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>]]\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>predicted beam_symbols vs. expected beam_symbols<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">for</span> ind, predicted <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(outputs[:num_steps]):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">list</span>(predicted), expected_beam_symbols[ind])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span>predicted beam_path vs. expected beam_path<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">for</span> ind, predicted <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(outputs[num_steps:num_steps <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>]):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">list</span>(predicted), expected_beam_path[ind])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span>log beam probs<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">for</span> log_probs <span class=\"pl-k\">in</span> outputs[<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> num_steps:]:\n        <span class=\"pl-c1\">print</span>(log_probs)</pre></div>\n<p>Output:</p>\n<pre><code>\npredicted beam_symbols vs. expected beam_symbols\n([4, 2, 0], [4, 2, 0])\n([0, 0, 3], [0, 0, 3])\n([1, 4, 1], [1, 4, 1])\n\npredicted beam_path vs. expected beam_path\n([0, 0, 0], [0, 0, 0])\n([1, 0, 0], [1, 0, 0])\n([1, 2, 0], [1, 2, 0])\n\nlog beam probs\n[[-0.6931622 ]\n [-1.09862733]\n [-1.79177451]]\n[[-1.098809  ]\n [-2.17854738]\n [-2.26555872]]\n[[-2.17872906]\n [-2.26574016]\n [-2.63273931]]\n</code></pre>\n<p>A simple function very similar to the decoding step of Viterbi is needed to extract the best hypothesis. Furthermore, there will have to be logic outside the function that extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch * beam_size.</p>\n<p>One downside to this implementation is that it does not pull off hypotheses that have reached the  token from the beam. It might be possible to do that, though I think it would require significantly more complicated logic. I'm interested in hearing your thoughts!</p>", "body_text": "@lukaszkaiser\nHere's a self contained example demonstrating a possible beam search implementation:\nfrom __future__ import division\nimport tensorflow as tf\n\nwith tf.Graph().as_default():\n    beam_size = 3 # Number of hypotheses in beam.\n    num_symbols = 5 # Output vocabulary size.\n    embedding_size = 10\n    num_steps = 3\n    embedding = tf.zeros([num_symbols, embedding_size])\n    output_projection = None\n\n    # log_beam_probs: list of [beam_size, 1] Tensors\n    #  Ordered log probabilities of the `beam_size` best hypotheses\n    #  found in each beam step (highest probability first).\n    # beam_symbols: list of [beam_size] Tensors \n    #  The ordered `beam_size` words / symbols extracted by the beam\n    #  step, which will be appended to their corresponding hypotheses\n    #  (corresponding hypotheses found in `beam_path`).\n    # beam_path: list of [beam_size] Tensor\n    #  The ordered `beam_size` parent indices. Their values range\n    #  from [0, `beam_size`), and they denote which previous\n    #  hypothesis each word should be appended to.\n    log_beam_probs, beam_symbols, beam_path  = [], [], []\n    def beam_search(prev, i):\n        if output_projection is not None:\n            prev = tf.nn.xw_plus_b(\n                prev, output_projection[0], output_projection[1])\n\n        # Compute \n        #  log P(next_word, hypothesis) = \n        #  log P(next_word | hypothesis)*P(hypothesis) =\n        #  log P(next_word | hypothesis) + log P(hypothesis)\n        # for each hypothesis separately, then join them together \n        # on the same tensor dimension to form the example's \n        # beam probability distribution:\n        # [P(word1, hypothesis1), P(word2, hypothesis1), ...,\n        #  P(word1, hypothesis2), P(word2, hypothesis2), ...]\n\n        # If TF had a log_sum_exp operator, then it would be \n        # more numerically stable to use: \n        #   probs = prev - tf.log_sum_exp(prev, reduction_dims=[1])\n        probs = tf.log(tf.nn.softmax(prev))\n        # i == 1 corresponds to the input being \"<GO>\", with\n        # uniform prior probability and only the empty hypothesis\n        # (each row is a separate example).\n        if i > 1:\n            probs = tf.reshape(probs + log_beam_probs[-1], \n                               [-1, beam_size * num_symbols])\n\n        # Get the top `beam_size` candidates and reshape them such\n        # that the number of rows = batch_size * beam_size, which\n        # allows us to process each hypothesis independently.\n        best_probs, indices = tf.nn.top_k(probs, beam_size)\n        indices = tf.stop_gradient(tf.squeeze(tf.reshape(indices, [-1, 1])))\n        best_probs = tf.stop_gradient(tf.reshape(best_probs, [-1, 1]))\n\n        symbols = indices % num_symbols # Which word in vocabulary.\n        beam_parent = indices // num_symbols # Which hypothesis it came from.\n\n        beam_symbols.append(symbols)\n        beam_path.append(beam_parent)\n        log_beam_probs.append(best_probs)\n        return tf.nn.embedding_lookup(embedding, symbols)\n\n    # Setting up graph.\n    inputs = [tf.placeholder(tf.float32, shape=[None, num_symbols])\n              for i in range(num_steps)]\n    for i in range(num_steps):\n        beam_search(inputs[i], i + 1)\n\n    # Running the graph.\n    input_vals = [0, 0, 0]\n    l = np.log\n    eps = -10 # exp(-10) ~= 0\n\n    # These values mimic the distribution of vocabulary words\n    # from each hypothesis independently (in log scale since\n    # they will be put through exp() in softmax).\n    input_vals[0] = np.array([[0, eps, l(2), eps, l(3)]])\n    # Step 1 beam hypotheses =\n    # (1) Path: [4], prob = log(1 / 2)\n    # (2) Path: [2], prob = log(1 / 3)\n    # (3) Path: [0], prob = log(1 / 6)\n\n    input_vals[1] = np.array([[l(1.2), 0, 0, l(1.1), 0], # Path [4] \n                              [0,   eps, eps, eps, eps], # Path [2]\n                              [0,  0,   0,   0,   0]])   # Path [0]\n    # Step 2 beam hypotheses =\n    # (1) Path: [2, 0], prob = log(1 / 3) + log(1)\n    # (2) Path: [4, 0], prob = log(1 / 2) + log(1.2 / 5.3)\n    # (3) Path: [4, 3], prob = log(1 / 2) + log(1.1 / 5.3)\n\n    input_vals[2] = np.array([[0,  l(1.1), 0,   0,   0], # Path [2, 0]\n                              [eps, 0,   eps, eps, eps], # Path [4, 0]\n                              [eps, eps, eps, eps, 0]])  # Path [4, 3]\n    # Step 3 beam hypotheses =\n    # (1) Path: [4, 0, 1], prob = log(1 / 2) + log(1.2 / 5.3) + log(1)\n    # (2) Path: [4, 3, 4], prob = log(1 / 2) + log(1.1 / 5.3) + log(1)\n    # (3) Path: [2, 0, 1], prob = log(1 / 3) + log(1) + log(1.1 / 5.1)\n\n    input_feed = {inputs[i]: input_vals[i][:beam_size, :] \n                  for i in xrange(num_steps)} \n    output_feed = beam_symbols + beam_path + log_beam_probs\n    session = tf.InteractiveSession()\n    outputs = session.run(output_feed, feed_dict=input_feed)\n\n    expected_beam_symbols = [[4, 2, 0],\n                             [0, 0, 3],\n                             [1, 4, 1]]\n    expected_beam_path = [[0, 0, 0],\n                          [1, 0, 0],\n                          [1, 2, 0]]\n\n    print(\"predicted beam_symbols vs. expected beam_symbols\")\n    for ind, predicted in enumerate(outputs[:num_steps]):\n        print(list(predicted), expected_beam_symbols[ind])\n    print(\"\\npredicted beam_path vs. expected beam_path\")\n    for ind, predicted in enumerate(outputs[num_steps:num_steps * 2]):\n        print(list(predicted), expected_beam_path[ind])\n    print(\"\\nlog beam probs\")\n    for log_probs in outputs[2 * num_steps:]:\n        print(log_probs)\nOutput:\n\npredicted beam_symbols vs. expected beam_symbols\n([4, 2, 0], [4, 2, 0])\n([0, 0, 3], [0, 0, 3])\n([1, 4, 1], [1, 4, 1])\n\npredicted beam_path vs. expected beam_path\n([0, 0, 0], [0, 0, 0])\n([1, 0, 0], [1, 0, 0])\n([1, 2, 0], [1, 2, 0])\n\nlog beam probs\n[[-0.6931622 ]\n [-1.09862733]\n [-1.79177451]]\n[[-1.098809  ]\n [-2.17854738]\n [-2.26555872]]\n[[-2.17872906]\n [-2.26574016]\n [-2.63273931]]\n\nA simple function very similar to the decoding step of Viterbi is needed to extract the best hypothesis. Furthermore, there will have to be logic outside the function that extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch * beam_size.\nOne downside to this implementation is that it does not pull off hypotheses that have reached the  token from the beam. It might be possible to do that, though I think it would require significantly more complicated logic. I'm interested in hearing your thoughts!", "body": "@lukaszkaiser \nHere's a self contained example demonstrating a possible beam search implementation:\n\n``` python\nfrom __future__ import division\nimport tensorflow as tf\n\nwith tf.Graph().as_default():\n    beam_size = 3 # Number of hypotheses in beam.\n    num_symbols = 5 # Output vocabulary size.\n    embedding_size = 10\n    num_steps = 3\n    embedding = tf.zeros([num_symbols, embedding_size])\n    output_projection = None\n\n    # log_beam_probs: list of [beam_size, 1] Tensors\n    #  Ordered log probabilities of the `beam_size` best hypotheses\n    #  found in each beam step (highest probability first).\n    # beam_symbols: list of [beam_size] Tensors \n    #  The ordered `beam_size` words / symbols extracted by the beam\n    #  step, which will be appended to their corresponding hypotheses\n    #  (corresponding hypotheses found in `beam_path`).\n    # beam_path: list of [beam_size] Tensor\n    #  The ordered `beam_size` parent indices. Their values range\n    #  from [0, `beam_size`), and they denote which previous\n    #  hypothesis each word should be appended to.\n    log_beam_probs, beam_symbols, beam_path  = [], [], []\n    def beam_search(prev, i):\n        if output_projection is not None:\n            prev = tf.nn.xw_plus_b(\n                prev, output_projection[0], output_projection[1])\n\n        # Compute \n        #  log P(next_word, hypothesis) = \n        #  log P(next_word | hypothesis)*P(hypothesis) =\n        #  log P(next_word | hypothesis) + log P(hypothesis)\n        # for each hypothesis separately, then join them together \n        # on the same tensor dimension to form the example's \n        # beam probability distribution:\n        # [P(word1, hypothesis1), P(word2, hypothesis1), ...,\n        #  P(word1, hypothesis2), P(word2, hypothesis2), ...]\n\n        # If TF had a log_sum_exp operator, then it would be \n        # more numerically stable to use: \n        #   probs = prev - tf.log_sum_exp(prev, reduction_dims=[1])\n        probs = tf.log(tf.nn.softmax(prev))\n        # i == 1 corresponds to the input being \"<GO>\", with\n        # uniform prior probability and only the empty hypothesis\n        # (each row is a separate example).\n        if i > 1:\n            probs = tf.reshape(probs + log_beam_probs[-1], \n                               [-1, beam_size * num_symbols])\n\n        # Get the top `beam_size` candidates and reshape them such\n        # that the number of rows = batch_size * beam_size, which\n        # allows us to process each hypothesis independently.\n        best_probs, indices = tf.nn.top_k(probs, beam_size)\n        indices = tf.stop_gradient(tf.squeeze(tf.reshape(indices, [-1, 1])))\n        best_probs = tf.stop_gradient(tf.reshape(best_probs, [-1, 1]))\n\n        symbols = indices % num_symbols # Which word in vocabulary.\n        beam_parent = indices // num_symbols # Which hypothesis it came from.\n\n        beam_symbols.append(symbols)\n        beam_path.append(beam_parent)\n        log_beam_probs.append(best_probs)\n        return tf.nn.embedding_lookup(embedding, symbols)\n\n    # Setting up graph.\n    inputs = [tf.placeholder(tf.float32, shape=[None, num_symbols])\n              for i in range(num_steps)]\n    for i in range(num_steps):\n        beam_search(inputs[i], i + 1)\n\n    # Running the graph.\n    input_vals = [0, 0, 0]\n    l = np.log\n    eps = -10 # exp(-10) ~= 0\n\n    # These values mimic the distribution of vocabulary words\n    # from each hypothesis independently (in log scale since\n    # they will be put through exp() in softmax).\n    input_vals[0] = np.array([[0, eps, l(2), eps, l(3)]])\n    # Step 1 beam hypotheses =\n    # (1) Path: [4], prob = log(1 / 2)\n    # (2) Path: [2], prob = log(1 / 3)\n    # (3) Path: [0], prob = log(1 / 6)\n\n    input_vals[1] = np.array([[l(1.2), 0, 0, l(1.1), 0], # Path [4] \n                              [0,   eps, eps, eps, eps], # Path [2]\n                              [0,  0,   0,   0,   0]])   # Path [0]\n    # Step 2 beam hypotheses =\n    # (1) Path: [2, 0], prob = log(1 / 3) + log(1)\n    # (2) Path: [4, 0], prob = log(1 / 2) + log(1.2 / 5.3)\n    # (3) Path: [4, 3], prob = log(1 / 2) + log(1.1 / 5.3)\n\n    input_vals[2] = np.array([[0,  l(1.1), 0,   0,   0], # Path [2, 0]\n                              [eps, 0,   eps, eps, eps], # Path [4, 0]\n                              [eps, eps, eps, eps, 0]])  # Path [4, 3]\n    # Step 3 beam hypotheses =\n    # (1) Path: [4, 0, 1], prob = log(1 / 2) + log(1.2 / 5.3) + log(1)\n    # (2) Path: [4, 3, 4], prob = log(1 / 2) + log(1.1 / 5.3) + log(1)\n    # (3) Path: [2, 0, 1], prob = log(1 / 3) + log(1) + log(1.1 / 5.1)\n\n    input_feed = {inputs[i]: input_vals[i][:beam_size, :] \n                  for i in xrange(num_steps)} \n    output_feed = beam_symbols + beam_path + log_beam_probs\n    session = tf.InteractiveSession()\n    outputs = session.run(output_feed, feed_dict=input_feed)\n\n    expected_beam_symbols = [[4, 2, 0],\n                             [0, 0, 3],\n                             [1, 4, 1]]\n    expected_beam_path = [[0, 0, 0],\n                          [1, 0, 0],\n                          [1, 2, 0]]\n\n    print(\"predicted beam_symbols vs. expected beam_symbols\")\n    for ind, predicted in enumerate(outputs[:num_steps]):\n        print(list(predicted), expected_beam_symbols[ind])\n    print(\"\\npredicted beam_path vs. expected beam_path\")\n    for ind, predicted in enumerate(outputs[num_steps:num_steps * 2]):\n        print(list(predicted), expected_beam_path[ind])\n    print(\"\\nlog beam probs\")\n    for log_probs in outputs[2 * num_steps:]:\n        print(log_probs)\n```\n\nOutput:\n\n```\n\npredicted beam_symbols vs. expected beam_symbols\n([4, 2, 0], [4, 2, 0])\n([0, 0, 3], [0, 0, 3])\n([1, 4, 1], [1, 4, 1])\n\npredicted beam_path vs. expected beam_path\n([0, 0, 0], [0, 0, 0])\n([1, 0, 0], [1, 0, 0])\n([1, 2, 0], [1, 2, 0])\n\nlog beam probs\n[[-0.6931622 ]\n [-1.09862733]\n [-1.79177451]]\n[[-1.098809  ]\n [-2.17854738]\n [-2.26555872]]\n[[-2.17872906]\n [-2.26574016]\n [-2.63273931]]\n```\n\nA simple function very similar to the decoding step of Viterbi is needed to extract the best hypothesis. Furthermore, there will have to be logic outside the function that extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch \\* beam_size.\n\nOne downside to this implementation is that it does not pull off hypotheses that have reached the <EOS> token from the beam. It might be possible to do that, though I think it would require significantly more complicated logic. I'm interested in hearing your thoughts!\n"}