{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/169079029", "html_url": "https://github.com/tensorflow/tensorflow/issues/654#issuecomment-169079029", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/654", "id": 169079029, "node_id": "MDEyOklzc3VlQ29tbWVudDE2OTA3OTAyOQ==", "user": {"login": "PrajitR", "id": 4674442, "node_id": "MDQ6VXNlcjQ2NzQ0NDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4674442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PrajitR", "html_url": "https://github.com/PrajitR", "followers_url": "https://api.github.com/users/PrajitR/followers", "following_url": "https://api.github.com/users/PrajitR/following{/other_user}", "gists_url": "https://api.github.com/users/PrajitR/gists{/gist_id}", "starred_url": "https://api.github.com/users/PrajitR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PrajitR/subscriptions", "organizations_url": "https://api.github.com/users/PrajitR/orgs", "repos_url": "https://api.github.com/users/PrajitR/repos", "events_url": "https://api.github.com/users/PrajitR/events{/privacy}", "received_events_url": "https://api.github.com/users/PrajitR/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-05T17:50:30Z", "updated_at": "2016-01-05T17:50:45Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> In the Seq2Seq paper they say, \"As soon as the <code>&lt;EOS&gt;</code><br>\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete<br>\nhypotheses\". The problem with the implementation I did above is that once an EOS token is appended, the hypothesis remains on the beam. This means that the effective beam size is reduced by one. The easy way to avoid this is to extract 2k hypothesis, which guarantees that we will follow the Seq2Seq approach. Of course, it has the downside of doubling computation.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1135354\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mfederico\">@mfederico</a> In TF, Python is just a front end language -- no operations are actually run in Python. Whether an operation runs on GPU depends on whether the operation has a GPU kernel (e.g. <code>.cu</code> file) and other scheduling heuristics. Computing hypotheses in parallel is equivalent to decreasing the batch size for each model replica (actually faster because data transfer between GPUs doesn't have to happen).</p>\n<p>I like the idea of computing top k on each hypothesis, then computing top k on the combination of remaining words. I think this would be faster if the top k operation is computed in parallel for each row (<code>O(n + k^2) vs. O(nk)</code>, where <code>n = vocab size, k = beam size, k &lt;&lt; n</code>). <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> would this be faster?</p>", "body_text": "@lukaszkaiser In the Seq2Seq paper they say, \"As soon as the <EOS>\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses\". The problem with the implementation I did above is that once an EOS token is appended, the hypothesis remains on the beam. This means that the effective beam size is reduced by one. The easy way to avoid this is to extract 2k hypothesis, which guarantees that we will follow the Seq2Seq approach. Of course, it has the downside of doubling computation.\n@mfederico In TF, Python is just a front end language -- no operations are actually run in Python. Whether an operation runs on GPU depends on whether the operation has a GPU kernel (e.g. .cu file) and other scheduling heuristics. Computing hypotheses in parallel is equivalent to decreasing the batch size for each model replica (actually faster because data transfer between GPUs doesn't have to happen).\nI like the idea of computing top k on each hypothesis, then computing top k on the combination of remaining words. I think this would be faster if the top k operation is computed in parallel for each row (O(n + k^2) vs. O(nk), where n = vocab size, k = beam size, k << n). @lukaszkaiser would this be faster?", "body": "@lukaszkaiser In the Seq2Seq paper they say, \"As soon as the `<EOS>`\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses\". The problem with the implementation I did above is that once an EOS token is appended, the hypothesis remains on the beam. This means that the effective beam size is reduced by one. The easy way to avoid this is to extract 2k hypothesis, which guarantees that we will follow the Seq2Seq approach. Of course, it has the downside of doubling computation.\n\n@mfederico In TF, Python is just a front end language -- no operations are actually run in Python. Whether an operation runs on GPU depends on whether the operation has a GPU kernel (e.g. `.cu` file) and other scheduling heuristics. Computing hypotheses in parallel is equivalent to decreasing the batch size for each model replica (actually faster because data transfer between GPUs doesn't have to happen).\n\nI like the idea of computing top k on each hypothesis, then computing top k on the combination of remaining words. I think this would be faster if the top k operation is computed in parallel for each row (`O(n + k^2) vs. O(nk)`, where `n = vocab size, k = beam size, k << n`). @lukaszkaiser would this be faster?\n"}