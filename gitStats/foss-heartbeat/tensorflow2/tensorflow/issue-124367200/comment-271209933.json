{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271209933", "html_url": "https://github.com/tensorflow/tensorflow/issues/654#issuecomment-271209933", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/654", "id": 271209933, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTIwOTkzMw==", "user": {"login": "nikitakit", "id": 252225, "node_id": "MDQ6VXNlcjI1MjIyNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/252225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikitakit", "html_url": "https://github.com/nikitakit", "followers_url": "https://api.github.com/users/nikitakit/followers", "following_url": "https://api.github.com/users/nikitakit/following{/other_user}", "gists_url": "https://api.github.com/users/nikitakit/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikitakit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikitakit/subscriptions", "organizations_url": "https://api.github.com/users/nikitakit/orgs", "repos_url": "https://api.github.com/users/nikitakit/repos", "events_url": "https://api.github.com/users/nikitakit/events{/privacy}", "received_events_url": "https://api.github.com/users/nikitakit/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-09T04:32:49Z", "updated_at": "2017-01-09T04:33:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17009658\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pbhatia243\">@pbhatia243</a> Could you clarify what you meant by your last comment?</p>\n<p>To be clear, my implementation is self-contained and independent of what's in your repo, so surely you were referring to something else? (Feel free to use my code if you need it, though)</p>\n<p>One thing I want to point out, having taken a look at your Neural Conversion Models repo, is that it uses a loop function approach. I therefore suspect that the implementation is actually subtly broken -- I believe it contains the same mistake that I made when I first tried to implement in-graph beam search. (It's really hard to notice without directly looking for it or having good tests.) If you look at the code <a href=\"https://github.com/pbhatia243/Neural_Conversation_Models/blob/master/my_seq2seq.py#L215\">here</a>, you'll see that the cell produces both outputs and hidden states. The outputs are eventually <a href=\"https://github.com/pbhatia243/Neural_Conversation_Models/blob/master/my_seq2seq.py#L103\">reordered</a> in a top-k step. But when the outputs are re-ordered, the states need to be re-ordered as well! Yet, I have not found a re-ordering step anywhere (i.e. states are never passed to a function like tf.gather). This is the reason why I had to abandon the original loop_function based approach and waited for hidden state reordering to be added to <code>raw_rnn</code> before I could take advantage of it.</p>", "body_text": "@pbhatia243 Could you clarify what you meant by your last comment?\nTo be clear, my implementation is self-contained and independent of what's in your repo, so surely you were referring to something else? (Feel free to use my code if you need it, though)\nOne thing I want to point out, having taken a look at your Neural Conversion Models repo, is that it uses a loop function approach. I therefore suspect that the implementation is actually subtly broken -- I believe it contains the same mistake that I made when I first tried to implement in-graph beam search. (It's really hard to notice without directly looking for it or having good tests.) If you look at the code here, you'll see that the cell produces both outputs and hidden states. The outputs are eventually reordered in a top-k step. But when the outputs are re-ordered, the states need to be re-ordered as well! Yet, I have not found a re-ordering step anywhere (i.e. states are never passed to a function like tf.gather). This is the reason why I had to abandon the original loop_function based approach and waited for hidden state reordering to be added to raw_rnn before I could take advantage of it.", "body": "@pbhatia243 Could you clarify what you meant by your last comment?\r\n\r\nTo be clear, my implementation is self-contained and independent of what's in your repo, so surely you were referring to something else? (Feel free to use my code if you need it, though)\r\n\r\nOne thing I want to point out, having taken a look at your Neural Conversion Models repo, is that it uses a loop function approach. I therefore suspect that the implementation is actually subtly broken -- I believe it contains the same mistake that I made when I first tried to implement in-graph beam search. (It's really hard to notice without directly looking for it or having good tests.) If you look at the code [here](https://github.com/pbhatia243/Neural_Conversation_Models/blob/master/my_seq2seq.py#L215), you'll see that the cell produces both outputs and hidden states. The outputs are eventually [reordered](https://github.com/pbhatia243/Neural_Conversation_Models/blob/master/my_seq2seq.py#L103) in a top-k step. But when the outputs are re-ordered, the states need to be re-ordered as well! Yet, I have not found a re-ordering step anywhere (i.e. states are never passed to a function like tf.gather). This is the reason why I had to abandon the original loop_function based approach and waited for hidden state reordering to be added to `raw_rnn` before I could take advantage of it."}