{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271136347", "html_url": "https://github.com/tensorflow/tensorflow/issues/654#issuecomment-271136347", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/654", "id": 271136347, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTEzNjM0Nw==", "user": {"login": "nikitakit", "id": 252225, "node_id": "MDQ6VXNlcjI1MjIyNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/252225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikitakit", "html_url": "https://github.com/nikitakit", "followers_url": "https://api.github.com/users/nikitakit/followers", "following_url": "https://api.github.com/users/nikitakit/following{/other_user}", "gists_url": "https://api.github.com/users/nikitakit/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikitakit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikitakit/subscriptions", "organizations_url": "https://api.github.com/users/nikitakit/orgs", "repos_url": "https://api.github.com/users/nikitakit/repos", "events_url": "https://api.github.com/users/nikitakit/events{/privacy}", "received_events_url": "https://api.github.com/users/nikitakit/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-08T07:54:32Z", "updated_at": "2017-01-08T07:54:32Z", "author_association": "NONE", "body_html": "<p>Now that my research has once again touched on beam search, I've taken another look at the latest comments here. It seems that my previous remarks about <code>raw_rnn</code> were greeted with some confusion, though I thought the meaning was sufficiently clear if you understand the math behind beam search. I hope that in the meantime everyone found a working beam search solution among those posted in this thread.</p>\n<p>I've now also updated my <a href=\"https://gist.github.com/nikitakit/6ab61a73b86c50ad88d409bac3c3d09f\">gist</a> to take advantage of the latest changes to <code>raw_rnn</code>. The API it provides has changed slightly, and the code passes all of the test cases I have (though fair warning: I have yet to apply it to my own full-scale models).</p>\n<p>This new version is more efficient in that it can stop early if the highest-scoring solution has already been found. I've also now figured out how to transparently handle all cells (including those with attention) without requiring a custom wrapper class, and have updated the API to take advantage of that. The new API is just a single <code>beam_decoder</code> function.</p>\n<p>It should be straightforward to slot in anywhere greedy search is currently used. (If anyone has code that can adapt it to the seq2seq primitives, I think that may be helpful for others. I myself tend to use the base rnn functions instead of anything higher-level).</p>\n<p>Last time my beam search implementation was suggesting for merging into <code>tf.contrib</code>, it was determined that the old API was not ideal. I'd be happy to try again with this new API if others are interested.</p>\n<p>To conclude, a note on back-propagation: a few people have asked me whether it's possible to use my code during training, and back-prop through the beam search. I've come to conclude that it is fine to just run the beam search to find the top-scoring sequences, and then do a second forced decode under those sequences (where the second decode is back-propagated through). Beam search has to consider a very large number of possible candidates, so duplicating this work shouldn't be too much of an issue. On the other hand, the way the beam search couples multiple candidate sequence means that any naive application of automatic differentiation may instantiate backwards-pass tensors for all of the candidates, rather than just the sequence you care about. That would be an even bigger performance hit than duplicating a small fraction of the forwards pass computations.</p>", "body_text": "Now that my research has once again touched on beam search, I've taken another look at the latest comments here. It seems that my previous remarks about raw_rnn were greeted with some confusion, though I thought the meaning was sufficiently clear if you understand the math behind beam search. I hope that in the meantime everyone found a working beam search solution among those posted in this thread.\nI've now also updated my gist to take advantage of the latest changes to raw_rnn. The API it provides has changed slightly, and the code passes all of the test cases I have (though fair warning: I have yet to apply it to my own full-scale models).\nThis new version is more efficient in that it can stop early if the highest-scoring solution has already been found. I've also now figured out how to transparently handle all cells (including those with attention) without requiring a custom wrapper class, and have updated the API to take advantage of that. The new API is just a single beam_decoder function.\nIt should be straightforward to slot in anywhere greedy search is currently used. (If anyone has code that can adapt it to the seq2seq primitives, I think that may be helpful for others. I myself tend to use the base rnn functions instead of anything higher-level).\nLast time my beam search implementation was suggesting for merging into tf.contrib, it was determined that the old API was not ideal. I'd be happy to try again with this new API if others are interested.\nTo conclude, a note on back-propagation: a few people have asked me whether it's possible to use my code during training, and back-prop through the beam search. I've come to conclude that it is fine to just run the beam search to find the top-scoring sequences, and then do a second forced decode under those sequences (where the second decode is back-propagated through). Beam search has to consider a very large number of possible candidates, so duplicating this work shouldn't be too much of an issue. On the other hand, the way the beam search couples multiple candidate sequence means that any naive application of automatic differentiation may instantiate backwards-pass tensors for all of the candidates, rather than just the sequence you care about. That would be an even bigger performance hit than duplicating a small fraction of the forwards pass computations.", "body": "Now that my research has once again touched on beam search, I've taken another look at the latest comments here. It seems that my previous remarks about `raw_rnn` were greeted with some confusion, though I thought the meaning was sufficiently clear if you understand the math behind beam search. I hope that in the meantime everyone found a working beam search solution among those posted in this thread.\r\n\r\nI've now also updated my [gist](https://gist.github.com/nikitakit/6ab61a73b86c50ad88d409bac3c3d09f) to take advantage of the latest changes to `raw_rnn`. The API it provides has changed slightly, and the code passes all of the test cases I have (though fair warning: I have yet to apply it to my own full-scale models).\r\n\r\nThis new version is more efficient in that it can stop early if the highest-scoring solution has already been found. I've also now figured out how to transparently handle all cells (including those with attention) without requiring a custom wrapper class, and have updated the API to take advantage of that. The new API is just a single `beam_decoder` function.\r\n\r\nIt should be straightforward to slot in anywhere greedy search is currently used. (If anyone has code that can adapt it to the seq2seq primitives, I think that may be helpful for others. I myself tend to use the base rnn functions instead of anything higher-level).\r\n\r\nLast time my beam search implementation was suggesting for merging into `tf.contrib`, it was determined that the old API was not ideal. I'd be happy to try again with this new API if others are interested.\r\n\r\nTo conclude, a note on back-propagation: a few people have asked me whether it's possible to use my code during training, and back-prop through the beam search. I've come to conclude that it is fine to just run the beam search to find the top-scoring sequences, and then do a second forced decode under those sequences (where the second decode is back-propagated through). Beam search has to consider a very large number of possible candidates, so duplicating this work shouldn't be too much of an issue. On the other hand, the way the beam search couples multiple candidate sequence means that any naive application of automatic differentiation may instantiate backwards-pass tensors for all of the candidates, rather than just the sequence you care about. That would be an even bigger performance hit than duplicating a small fraction of the forwards pass computations."}