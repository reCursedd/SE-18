{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/169009989", "html_url": "https://github.com/tensorflow/tensorflow/issues/654#issuecomment-169009989", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/654", "id": 169009989, "node_id": "MDEyOklzc3VlQ29tbWVudDE2OTAwOTk4OQ==", "user": {"login": "mfederico", "id": 1135354, "node_id": "MDQ6VXNlcjExMzUzNTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1135354?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfederico", "html_url": "https://github.com/mfederico", "followers_url": "https://api.github.com/users/mfederico/followers", "following_url": "https://api.github.com/users/mfederico/following{/other_user}", "gists_url": "https://api.github.com/users/mfederico/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfederico/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfederico/subscriptions", "organizations_url": "https://api.github.com/users/mfederico/orgs", "repos_url": "https://api.github.com/users/mfederico/repos", "events_url": "https://api.github.com/users/mfederico/events{/privacy}", "received_events_url": "https://api.github.com/users/mfederico/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-05T14:07:02Z", "updated_at": "2016-01-05T14:12:18Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nlet me first say that my knowledge of tensorflow is still quite limited, hence forgive me if I'll write something wrong. From what I read I understood that (1) it is not good getting back and forth between the computations on the graph (on GPU) and computations in python (on CPU), and (2) it is advisable<br>\nto also exploit as much as possibly parallel computations on the graph (on GPU), which means in our case computing expansions of alternative hypotheses in parallel.</p>\n<p>I have put down the following figure to explain how the beam search could work.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/1135354/12116947/9e9898d2-b3bd-11e5-9f23-b2c8ba03ab52.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/1135354/12116947/9e9898d2-b3bd-11e5-9f23-b2c8ba03ab52.jpg\" alt=\"nmt-search\" style=\"max-width:100%;\"></a></p>\n<p>The figure focuses on the decoding step and shows on the top outputs of decoding steps that can be computed in parallel, and the best K=2 output words for each step. As no recombination is possible with RNN, because each output word depends on its whole history, we have to select the output words (with possible repetitions) that result in the top B=3 cumulative scores.  (Notice that K should ideally be equal to B to have beams search with beam B ) These B words become the input for the following step. To allow backtracking of the best translation we only need to store the provenance of each input with respect to the input of the previous step (see dotted arrows). A numeric index corresponding to a position index should work, too. Once we have finished with all computations, we can start backtracking the<br>\ninput trellis (on the bottom) starting from the input word &lt;/s&gt; with the best global score. This could be performed with a linear pass over all columns, from right to left.</p>", "body_text": "Hi,\nlet me first say that my knowledge of tensorflow is still quite limited, hence forgive me if I'll write something wrong. From what I read I understood that (1) it is not good getting back and forth between the computations on the graph (on GPU) and computations in python (on CPU), and (2) it is advisable\nto also exploit as much as possibly parallel computations on the graph (on GPU), which means in our case computing expansions of alternative hypotheses in parallel.\nI have put down the following figure to explain how the beam search could work.\n\nThe figure focuses on the decoding step and shows on the top outputs of decoding steps that can be computed in parallel, and the best K=2 output words for each step. As no recombination is possible with RNN, because each output word depends on its whole history, we have to select the output words (with possible repetitions) that result in the top B=3 cumulative scores.  (Notice that K should ideally be equal to B to have beams search with beam B ) These B words become the input for the following step. To allow backtracking of the best translation we only need to store the provenance of each input with respect to the input of the previous step (see dotted arrows). A numeric index corresponding to a position index should work, too. Once we have finished with all computations, we can start backtracking the\ninput trellis (on the bottom) starting from the input word </s> with the best global score. This could be performed with a linear pass over all columns, from right to left.", "body": "Hi, \nlet me first say that my knowledge of tensorflow is still quite limited, hence forgive me if I'll write something wrong. From what I read I understood that (1) it is not good getting back and forth between the computations on the graph (on GPU) and computations in python (on CPU), and (2) it is advisable \nto also exploit as much as possibly parallel computations on the graph (on GPU), which means in our case computing expansions of alternative hypotheses in parallel.  \n\nI have put down the following figure to explain how the beam search could work. \n\n![nmt-search](https://cloud.githubusercontent.com/assets/1135354/12116947/9e9898d2-b3bd-11e5-9f23-b2c8ba03ab52.jpg)\n\nThe figure focuses on the decoding step and shows on the top outputs of decoding steps that can be computed in parallel, and the best K=2 output words for each step. As no recombination is possible with RNN, because each output word depends on its whole history, we have to select the output words (with possible repetitions) that result in the top B=3 cumulative scores.  (Notice that K should ideally be equal to B to have beams search with beam B ) These B words become the input for the following step. To allow backtracking of the best translation we only need to store the provenance of each input with respect to the input of the previous step (see dotted arrows). A numeric index corresponding to a position index should work, too. Once we have finished with all computations, we can start backtracking the \ninput trellis (on the bottom) starting from the input word <\\/s> with the best global score. This could be performed with a linear pass over all columns, from right to left. \n"}