{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/185322066", "html_url": "https://github.com/tensorflow/tensorflow/issues/654#issuecomment-185322066", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/654", "id": 185322066, "node_id": "MDEyOklzc3VlQ29tbWVudDE4NTMyMjA2Ng==", "user": {"login": "giancds", "id": 3305726, "node_id": "MDQ6VXNlcjMzMDU3MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/3305726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/giancds", "html_url": "https://github.com/giancds", "followers_url": "https://api.github.com/users/giancds/followers", "following_url": "https://api.github.com/users/giancds/following{/other_user}", "gists_url": "https://api.github.com/users/giancds/gists{/gist_id}", "starred_url": "https://api.github.com/users/giancds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/giancds/subscriptions", "organizations_url": "https://api.github.com/users/giancds/orgs", "repos_url": "https://api.github.com/users/giancds/repos", "events_url": "https://api.github.com/users/giancds/events{/privacy}", "received_events_url": "https://api.github.com/users/giancds/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-17T17:46:13Z", "updated_at": "2016-02-17T17:46:13Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4674442\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PrajitR\">@PrajitR</a>, I'm trying to implement your suggestion into my experiments, using the Seq2seq interfaces. I think your code is very clever and make lots of sense. Nevertheless, I'm still confused by one thing you mentioned:</p>\n<p>\"(...) extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch * beam_size.\"</p>\n<p>If I'm using a batch of 1 (i.e., one sentence at the time - am I right with this assertion?), wouldn't  just the lookup of prev_state suffice? If not, I think I didn't get that.</p>\n<p>In addition, I think that if we combine the predicted symbols with their parents right after producing them, we could keep a list of complete hypotheses to add those which reach the EOS symbol and we could try to remove them from the beam_path. This would not stop the graph earlier but I think would solve the problem of reducing the effectiveness of the beam size.</p>", "body_text": "@PrajitR, I'm trying to implement your suggestion into my experiments, using the Seq2seq interfaces. I think your code is very clever and make lots of sense. Nevertheless, I'm still confused by one thing you mentioned:\n\"(...) extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch * beam_size.\"\nIf I'm using a batch of 1 (i.e., one sentence at the time - am I right with this assertion?), wouldn't  just the lookup of prev_state suffice? If not, I think I didn't get that.\nIn addition, I think that if we combine the predicted symbols with their parents right after producing them, we could keep a list of complete hypotheses to add those which reach the EOS symbol and we could try to remove them from the beam_path. This would not stop the graph earlier but I think would solve the problem of reducing the effectiveness of the beam size.", "body": "@PrajitR, I'm trying to implement your suggestion into my experiments, using the Seq2seq interfaces. I think your code is very clever and make lots of sense. Nevertheless, I'm still confused by one thing you mentioned:\n\n\"(...) extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch \\* beam_size.\"\n\nIf I'm using a batch of 1 (i.e., one sentence at the time - am I right with this assertion?), wouldn't  just the lookup of prev_state suffice? If not, I think I didn't get that.\n\nIn addition, I think that if we combine the predicted symbols with their parents right after producing them, we could keep a list of complete hypotheses to add those which reach the EOS symbol and we could try to remove them from the beam_path. This would not stop the graph earlier but I think would solve the problem of reducing the effectiveness of the beam size.\n"}