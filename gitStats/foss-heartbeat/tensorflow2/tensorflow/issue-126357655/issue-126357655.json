{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/758", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/758/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/758/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/758/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/758", "id": 126357655, "node_id": "MDU6SXNzdWUxMjYzNTc2NTU=", "number": 758, "title": "[RBM implemention] Problem with GraphDef cannot be larger than 2GB ?", "user": {"login": "btpeter", "id": 9015364, "node_id": "MDQ6VXNlcjkwMTUzNjQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/9015364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/btpeter", "html_url": "https://github.com/btpeter", "followers_url": "https://api.github.com/users/btpeter/followers", "following_url": "https://api.github.com/users/btpeter/following{/other_user}", "gists_url": "https://api.github.com/users/btpeter/gists{/gist_id}", "starred_url": "https://api.github.com/users/btpeter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/btpeter/subscriptions", "organizations_url": "https://api.github.com/users/btpeter/orgs", "repos_url": "https://api.github.com/users/btpeter/repos", "events_url": "https://api.github.com/users/btpeter/events{/privacy}", "received_events_url": "https://api.github.com/users/btpeter/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-01-13T07:12:50Z", "updated_at": "2016-01-14T00:04:52Z", "closed_at": "2016-01-14T00:04:52Z", "author_association": "NONE", "body_html": "<p>Hi, everyone<br>\nI'm trying to implement RBM with tensorflow, here is the code:</p>\n<p>rbm.py</p>\n<p>\"\"\" An rbm implementation for TensorFlow, based closely on the one in Theano \"\"\"</p>\n<p>import tensorflow as tf<br>\nimport math</p>\n<p>def sample_prob(probs):<br>\n\"\"\"Takes a tensor of probabilities (as from a sigmoidal activation)<br>\nand samples from all the distributions\"\"\"<br>\nreturn tf.nn.relu(<br>\ntf.sign(<br>\nprobs - tf.random_uniform(probs.get_shape())))</p>\n<p>class RBM(object):<br>\n\"\"\" represents a sigmoidal rbm \"\"\"</p>\n<pre><code>def __init__(self, name, input_size, output_size):\n    with tf.name_scope(\"rbm_\" + name):\n        self.weights = tf.Variable(\n            tf.truncated_normal([input_size, output_size],\n                stddev=1.0 / math.sqrt(float(input_size))), name=\"weights\")\n        self.v_bias = tf.Variable(tf.zeros([input_size]), name=\"v_bias\")\n        self.h_bias = tf.Variable(tf.zeros([output_size]), name=\"h_bias\")\n\ndef propup(self, visible):\n    \"\"\" P(h|v) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(visible, self.weights) + self.h_bias)\n\ndef propdown(self, hidden):\n    \"\"\" P(v|h) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(self.weights)) + self.v_bias)\n\ndef sample_h_given_v(self, v_sample):\n    \"\"\" Generate a sample from the hidden layer \"\"\"\n    return sample_prob(self.propup(v_sample))\n\ndef sample_v_given_h(self, h_sample):\n    \"\"\" Generate a sample from the visible layer \"\"\"\n    return sample_prob(self.propdown(h_sample))\n\ndef gibbs_hvh(self, h0_sample):\n    \"\"\" A gibbs step starting from the hidden layer \"\"\"\n    v_sample = self.sample_v_given_h(h0_sample)\n    h_sample = self.sample_h_given_v(v_sample)\n    return [v_sample, h_sample]\n\ndef gibbs_vhv(self, v0_sample):\n    \"\"\" A gibbs step starting from the visible layer \"\"\"\n    h_sample = self.sample_h_given_v(v0_sample)\n    v_sample = self.sample_v_given_h(h_sample)\n    return  [h_sample, v_sample]\n\ndef cd1(self, visibles, learning_rate=0.1):\n    \" One step of contrastive divergence, with Rao-Blackwellization \"\n    h_start = self.propup(visibles)\n    v_end = self.propdown(h_start)\n    h_end = self.propup(v_end)\n    w_positive_grad = tf.matmul(tf.transpose(visibles), h_start)\n    w_negative_grad = tf.matmul(tf.transpose(v_end), h_end)\n\n    update_w = self.weights.assign_add(learning_rate * (w_positive_grad - w_negative_grad))\n\n    update_vb = self.v_bias.assign_add(learning_rate * tf.reduce_mean(visibles - v_end, 0))\n\n    update_hb = self.h_bias.assign_add(learning_rate * tf.reduce_mean(h_start - h_end, 0))\n\n    return [update_w, update_vb, update_hb]\n\ndef reconstruction_error(self, dataset):\n    \"\"\" The reconstruction cost for the whole dataset \"\"\"\n    err = tf.stop_gradient(dataset - self.gibbs_vhv(dataset)[1])\n    return tf.reduce_sum(err * err)\n</code></pre>\n<h6></h6>\n<p>rbm_MNIST_test.py</p>\n<p>import tensorflow as tf<br>\nimport numpy as np<br>\nimport rbm<br>\nimport input_data</p>\n<p>def build_model(X, w1, b1, wo, bo):<br>\nh1 = tf.nn.sigmoid(tf.matmul(X, w1)+b1)<br>\nmodel = tf.nn.sigmoid(tf.matmul(h1, wo)+bo)<br>\nreturn model</p>\n<p>def init_weight(shape):<br>\nreturn tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))</p>\n<p>def init_bias(dim):<br>\nreturn tf.Variable(tf.zeros([dim]))</p>\n<p>mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)<br>\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels</p>\n<p>X = tf.placeholder(\"float\", [None, 784])<br>\nY = tf.placeholder(\"float\", [None, 10])</p>\n<p>rbm_layer = rbm.RBM(\"mnist\", 784, 500)</p>\n<p>for i in range(10):<br>\nprint \"RBM CD: \", i<br>\nrbm_layer.cd1(trX)</p>\n<p>rbm_w, rbm_vb, rbm_hb = rbm_layer.cd1(trX)</p>\n<p>wo = init_weight([500,10])<br>\nbo = init_bias(10)<br>\npy_x = build_model(X, rbm_w, rbm_hb, wo, bo)</p>\n<p>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))<br>\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)<br>\npredict_op = tf.argmax(py_x, 1)</p>\n<p>sess = tf.Session()<br>\ninit = tf.initialize_all_variables()<br>\nsess.run(init)</p>\n<p>for i in range(10):<br>\nfor start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):<br>\nsess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})<br>\nprint i, np.mean(np.argmax(teY, axis=1) ==<br>\nsess.run(predict_op, feed_dict={X: teX, Y: teY}))</p>\n<h6></h6>\n<p>but here comes the error:</p>\n<p>File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1626, in as_graph_def<br>\nraise ValueError(\"GraphDef cannot be larger than 2GB.\")<br>\nValueError: GraphDef cannot be larger than 2GB.</p>\n<p>Can someone help me to solve?</p>", "body_text": "Hi, everyone\nI'm trying to implement RBM with tensorflow, here is the code:\nrbm.py\n\"\"\" An rbm implementation for TensorFlow, based closely on the one in Theano \"\"\"\nimport tensorflow as tf\nimport math\ndef sample_prob(probs):\n\"\"\"Takes a tensor of probabilities (as from a sigmoidal activation)\nand samples from all the distributions\"\"\"\nreturn tf.nn.relu(\ntf.sign(\nprobs - tf.random_uniform(probs.get_shape())))\nclass RBM(object):\n\"\"\" represents a sigmoidal rbm \"\"\"\ndef __init__(self, name, input_size, output_size):\n    with tf.name_scope(\"rbm_\" + name):\n        self.weights = tf.Variable(\n            tf.truncated_normal([input_size, output_size],\n                stddev=1.0 / math.sqrt(float(input_size))), name=\"weights\")\n        self.v_bias = tf.Variable(tf.zeros([input_size]), name=\"v_bias\")\n        self.h_bias = tf.Variable(tf.zeros([output_size]), name=\"h_bias\")\n\ndef propup(self, visible):\n    \"\"\" P(h|v) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(visible, self.weights) + self.h_bias)\n\ndef propdown(self, hidden):\n    \"\"\" P(v|h) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(self.weights)) + self.v_bias)\n\ndef sample_h_given_v(self, v_sample):\n    \"\"\" Generate a sample from the hidden layer \"\"\"\n    return sample_prob(self.propup(v_sample))\n\ndef sample_v_given_h(self, h_sample):\n    \"\"\" Generate a sample from the visible layer \"\"\"\n    return sample_prob(self.propdown(h_sample))\n\ndef gibbs_hvh(self, h0_sample):\n    \"\"\" A gibbs step starting from the hidden layer \"\"\"\n    v_sample = self.sample_v_given_h(h0_sample)\n    h_sample = self.sample_h_given_v(v_sample)\n    return [v_sample, h_sample]\n\ndef gibbs_vhv(self, v0_sample):\n    \"\"\" A gibbs step starting from the visible layer \"\"\"\n    h_sample = self.sample_h_given_v(v0_sample)\n    v_sample = self.sample_v_given_h(h_sample)\n    return  [h_sample, v_sample]\n\ndef cd1(self, visibles, learning_rate=0.1):\n    \" One step of contrastive divergence, with Rao-Blackwellization \"\n    h_start = self.propup(visibles)\n    v_end = self.propdown(h_start)\n    h_end = self.propup(v_end)\n    w_positive_grad = tf.matmul(tf.transpose(visibles), h_start)\n    w_negative_grad = tf.matmul(tf.transpose(v_end), h_end)\n\n    update_w = self.weights.assign_add(learning_rate * (w_positive_grad - w_negative_grad))\n\n    update_vb = self.v_bias.assign_add(learning_rate * tf.reduce_mean(visibles - v_end, 0))\n\n    update_hb = self.h_bias.assign_add(learning_rate * tf.reduce_mean(h_start - h_end, 0))\n\n    return [update_w, update_vb, update_hb]\n\ndef reconstruction_error(self, dataset):\n    \"\"\" The reconstruction cost for the whole dataset \"\"\"\n    err = tf.stop_gradient(dataset - self.gibbs_vhv(dataset)[1])\n    return tf.reduce_sum(err * err)\n\n\nrbm_MNIST_test.py\nimport tensorflow as tf\nimport numpy as np\nimport rbm\nimport input_data\ndef build_model(X, w1, b1, wo, bo):\nh1 = tf.nn.sigmoid(tf.matmul(X, w1)+b1)\nmodel = tf.nn.sigmoid(tf.matmul(h1, wo)+bo)\nreturn model\ndef init_weight(shape):\nreturn tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))\ndef init_bias(dim):\nreturn tf.Variable(tf.zeros([dim]))\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\nX = tf.placeholder(\"float\", [None, 784])\nY = tf.placeholder(\"float\", [None, 10])\nrbm_layer = rbm.RBM(\"mnist\", 784, 500)\nfor i in range(10):\nprint \"RBM CD: \", i\nrbm_layer.cd1(trX)\nrbm_w, rbm_vb, rbm_hb = rbm_layer.cd1(trX)\nwo = init_weight([500,10])\nbo = init_bias(10)\npy_x = build_model(X, rbm_w, rbm_hb, wo, bo)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\npredict_op = tf.argmax(py_x, 1)\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\nfor i in range(10):\nfor start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\nsess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\nprint i, np.mean(np.argmax(teY, axis=1) ==\nsess.run(predict_op, feed_dict={X: teX, Y: teY}))\n\nbut here comes the error:\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1626, in as_graph_def\nraise ValueError(\"GraphDef cannot be larger than 2GB.\")\nValueError: GraphDef cannot be larger than 2GB.\nCan someone help me to solve?", "body": "Hi, everyone\nI'm trying to implement RBM with tensorflow, here is the code:\n\nrbm.py\n\n\"\"\" An rbm implementation for TensorFlow, based closely on the one in Theano \"\"\"\n\nimport tensorflow as tf\nimport math\n\ndef sample_prob(probs):\n    \"\"\"Takes a tensor of probabilities (as from a sigmoidal activation)\n       and samples from all the distributions\"\"\"\n    return tf.nn.relu(\n        tf.sign(\n            probs - tf.random_uniform(probs.get_shape())))\n\nclass RBM(object):\n    \"\"\" represents a sigmoidal rbm \"\"\"\n\n```\ndef __init__(self, name, input_size, output_size):\n    with tf.name_scope(\"rbm_\" + name):\n        self.weights = tf.Variable(\n            tf.truncated_normal([input_size, output_size],\n                stddev=1.0 / math.sqrt(float(input_size))), name=\"weights\")\n        self.v_bias = tf.Variable(tf.zeros([input_size]), name=\"v_bias\")\n        self.h_bias = tf.Variable(tf.zeros([output_size]), name=\"h_bias\")\n\ndef propup(self, visible):\n    \"\"\" P(h|v) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(visible, self.weights) + self.h_bias)\n\ndef propdown(self, hidden):\n    \"\"\" P(v|h) \"\"\"\n    return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(self.weights)) + self.v_bias)\n\ndef sample_h_given_v(self, v_sample):\n    \"\"\" Generate a sample from the hidden layer \"\"\"\n    return sample_prob(self.propup(v_sample))\n\ndef sample_v_given_h(self, h_sample):\n    \"\"\" Generate a sample from the visible layer \"\"\"\n    return sample_prob(self.propdown(h_sample))\n\ndef gibbs_hvh(self, h0_sample):\n    \"\"\" A gibbs step starting from the hidden layer \"\"\"\n    v_sample = self.sample_v_given_h(h0_sample)\n    h_sample = self.sample_h_given_v(v_sample)\n    return [v_sample, h_sample]\n\ndef gibbs_vhv(self, v0_sample):\n    \"\"\" A gibbs step starting from the visible layer \"\"\"\n    h_sample = self.sample_h_given_v(v0_sample)\n    v_sample = self.sample_v_given_h(h_sample)\n    return  [h_sample, v_sample]\n\ndef cd1(self, visibles, learning_rate=0.1):\n    \" One step of contrastive divergence, with Rao-Blackwellization \"\n    h_start = self.propup(visibles)\n    v_end = self.propdown(h_start)\n    h_end = self.propup(v_end)\n    w_positive_grad = tf.matmul(tf.transpose(visibles), h_start)\n    w_negative_grad = tf.matmul(tf.transpose(v_end), h_end)\n\n    update_w = self.weights.assign_add(learning_rate * (w_positive_grad - w_negative_grad))\n\n    update_vb = self.v_bias.assign_add(learning_rate * tf.reduce_mean(visibles - v_end, 0))\n\n    update_hb = self.h_bias.assign_add(learning_rate * tf.reduce_mean(h_start - h_end, 0))\n\n    return [update_w, update_vb, update_hb]\n\ndef reconstruction_error(self, dataset):\n    \"\"\" The reconstruction cost for the whole dataset \"\"\"\n    err = tf.stop_gradient(dataset - self.gibbs_vhv(dataset)[1])\n    return tf.reduce_sum(err * err)\n```\n###### \n\nrbm_MNIST_test.py\n\nimport tensorflow as tf\nimport numpy as np\nimport rbm\nimport input_data\n\ndef build_model(X, w1, b1, wo, bo):\n    h1 = tf.nn.sigmoid(tf.matmul(X, w1)+b1)\n    model = tf.nn.sigmoid(tf.matmul(h1, wo)+bo)\n    return model\n\ndef init_weight(shape):\n    return tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01))\n\ndef init_bias(dim):\n    return tf.Variable(tf.zeros([dim]))\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n\nX = tf.placeholder(\"float\", [None, 784])\nY = tf.placeholder(\"float\", [None, 10])\n\nrbm_layer = rbm.RBM(\"mnist\", 784, 500)\n\nfor i in range(10):\n    print \"RBM CD: \", i\n    rbm_layer.cd1(trX)\n\nrbm_w, rbm_vb, rbm_hb = rbm_layer.cd1(trX)\n\nwo = init_weight([500,10])\nbo = init_bias(10)\npy_x = build_model(X, rbm_w, rbm_hb, wo, bo)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\npredict_op = tf.argmax(py_x, 1)\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor i in range(10):\n    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n        sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n    print i, np.mean(np.argmax(teY, axis=1) ==\n                     sess.run(predict_op, feed_dict={X: teX, Y: teY}))\n###### \n\nbut here comes the error:\n\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1626, in as_graph_def\n    raise ValueError(\"GraphDef cannot be larger than 2GB.\")\nValueError: GraphDef cannot be larger than 2GB.\n\nCan someone help me to solve?\n"}