{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5683", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5683/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5683/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5683/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5683", "id": 190218239, "node_id": "MDU6SXNzdWUxOTAyMTgyMzk=", "number": 5683, "title": "Synchronous distributed training is too slow", "user": {"login": "indhub", "id": 7954672, "node_id": "MDQ6VXNlcjc5NTQ2NzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/7954672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/indhub", "html_url": "https://github.com/indhub", "followers_url": "https://api.github.com/users/indhub/followers", "following_url": "https://api.github.com/users/indhub/following{/other_user}", "gists_url": "https://api.github.com/users/indhub/gists{/gist_id}", "starred_url": "https://api.github.com/users/indhub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/indhub/subscriptions", "organizations_url": "https://api.github.com/users/indhub/orgs", "repos_url": "https://api.github.com/users/indhub/repos", "events_url": "https://api.github.com/users/indhub/events{/privacy}", "received_events_url": "https://api.github.com/users/indhub/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-11-18T02:10:15Z", "updated_at": "2017-06-16T17:28:44Z", "closed_at": "2017-06-16T17:28:44Z", "author_association": "NONE", "body_html": "<p>While trying to train Imagenet using Inceptionv3, I noticed the training was proceeding too slow. I then tested with Alexnet and Resnet and I saw the same slow training speed.</p>\n<p>Here is the images/sec I'm able to achieve on various models:</p>\n<pre><code>Inceptionv3:\n# GPU, Images/sec\n1,        17.525\n2,        34.568\n4,        62.580\n8,        94.832\n16,       99.072\n32,       140.704\n64,       183.488\n128,      322.816\n\nAlexnet:\n1,        92.464\n2,        189.736\n4,        344.336\n8,        532.40\n16,       710.224\n32,       878.944\n64,       848.128\n128,      874.624\n\nResnet 152:\n1,        10.567\n2,        20.224\n4,        32.332\n8,        37.952\n16,       42.416\n32,       66.016\n64,       95.168\n128,      152.192\n\n</code></pre>\n<p>Is it possible there is some bug causing this slow performance?</p>\n<p>I'm using EC2 P2 instances (p2.16xlarge). Each instance has 16 GPUs. Each GPU is a Tesla K80. I'm running one worker per GPU and one PS per host. Here is more info on P2 instances: <a href=\"https://aws.amazon.com/ec2/instance-types/p2/\" rel=\"nofollow\">https://aws.amazon.com/ec2/instance-types/p2/</a></p>\n<p>I use randomly generated synthetic data for images and label. I've written some scripts to reproduce this issue in case it helps: <a href=\"https://github.com/indhub/tfperftest/tree/master/perftest\">https://github.com/indhub/tfperftest/tree/master/perftest</a></p>\n<ul>\n<li>Inception model is based on <a href=\"https://github.com/tensorflow/models/tree/master/inception\">https://github.com/tensorflow/models/tree/master/inception</a>. I've done some changes to use randomly generated synthetic data for images and labels. Modified code here: <a href=\"https://github.com/indhub/tfperftest/tree/master/inception\">https://github.com/indhub/tfperftest/tree/master/inception</a></li>\n<li>Resnet model is based on <a href=\"https://github.com/tensorflow/models/tree/master/resnet\">https://github.com/tensorflow/models/tree/master/resnet</a>. I've done some changes to build Resnet 152. Modified code here: <a href=\"https://github.com/indhub/tfperftest/tree/master/resnet\">https://github.com/indhub/tfperftest/tree/master/resnet</a></li>\n<li>Alexnet model is based on <a href=\"https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py\">https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py</a>. I've added the fully connected layers at the end. Modified code here: <a href=\"https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py\">https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py</a></li>\n</ul>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p><a href=\"http://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow\" rel=\"nofollow\">http://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow</a></p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>ubuntu@ip-172-31-52-161:~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0\nlrwxrwxrwx 1 root root     19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44\n-rw-r--r-- 1 root root 415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root 775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a\n\n</code></pre>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed:<br>\n<a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl</a></li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.<br>\nubuntu@ip-172-31-52-161:~$ python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\"</li>\n</ol>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0rc2\n\n</code></pre>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>Just running <a href=\"https://github.com/tensorflow/models/tree/master/inception\">https://github.com/tensorflow/models/tree/master/inception</a> reproduces the problem.</p>", "body_text": "While trying to train Imagenet using Inceptionv3, I noticed the training was proceeding too slow. I then tested with Alexnet and Resnet and I saw the same slow training speed.\nHere is the images/sec I'm able to achieve on various models:\nInceptionv3:\n# GPU, Images/sec\n1,        17.525\n2,        34.568\n4,        62.580\n8,        94.832\n16,       99.072\n32,       140.704\n64,       183.488\n128,      322.816\n\nAlexnet:\n1,        92.464\n2,        189.736\n4,        344.336\n8,        532.40\n16,       710.224\n32,       878.944\n64,       848.128\n128,      874.624\n\nResnet 152:\n1,        10.567\n2,        20.224\n4,        32.332\n8,        37.952\n16,       42.416\n32,       66.016\n64,       95.168\n128,      152.192\n\n\nIs it possible there is some bug causing this slow performance?\nI'm using EC2 P2 instances (p2.16xlarge). Each instance has 16 GPUs. Each GPU is a Tesla K80. I'm running one worker per GPU and one PS per host. Here is more info on P2 instances: https://aws.amazon.com/ec2/instance-types/p2/\nI use randomly generated synthetic data for images and label. I've written some scripts to reproduce this issue in case it helps: https://github.com/indhub/tfperftest/tree/master/perftest\n\nInception model is based on https://github.com/tensorflow/models/tree/master/inception. I've done some changes to use randomly generated synthetic data for images and labels. Modified code here: https://github.com/indhub/tfperftest/tree/master/inception\nResnet model is based on https://github.com/tensorflow/models/tree/master/resnet. I've done some changes to build Resnet 152. Modified code here: https://github.com/indhub/tfperftest/tree/master/resnet\nAlexnet model is based on https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py. I've added the fully connected layers at the end. Modified code here: https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py\n\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nhttp://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow\nEnvironment info\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nubuntu@ip-172-31-52-161:~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root     19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rw-r--r-- 1 root root 415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root 775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a\n\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\nubuntu@ip-172-31-52-161:~$ python -c \"import tensorflow; print(tensorflow.version)\"\n\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0rc2\n\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nJust running https://github.com/tensorflow/models/tree/master/inception reproduces the problem.", "body": "While trying to train Imagenet using Inceptionv3, I noticed the training was proceeding too slow. I then tested with Alexnet and Resnet and I saw the same slow training speed. \r\n\r\nHere is the images/sec I'm able to achieve on various models:\r\n\r\n```\r\nInceptionv3:\r\n# GPU, Images/sec\r\n1,        17.525\r\n2,        34.568\r\n4,        62.580\r\n8,        94.832\r\n16,       99.072\r\n32,       140.704\r\n64,       183.488\r\n128,      322.816\r\n\r\nAlexnet:\r\n1,        92.464\r\n2,        189.736\r\n4,        344.336\r\n8,        532.40\r\n16,       710.224\r\n32,       878.944\r\n64,       848.128\r\n128,      874.624\r\n\r\nResnet 152:\r\n1,        10.567\r\n2,        20.224\r\n4,        32.332\r\n8,        37.952\r\n16,       42.416\r\n32,       66.016\r\n64,       95.168\r\n128,      152.192\r\n\r\n```\r\nIs it possible there is some bug causing this slow performance?\r\n\r\nI'm using EC2 P2 instances (p2.16xlarge). Each instance has 16 GPUs. Each GPU is a Tesla K80. I'm running one worker per GPU and one PS per host. Here is more info on P2 instances: https://aws.amazon.com/ec2/instance-types/p2/\r\n\r\nI use randomly generated synthetic data for images and label. I've written some scripts to reproduce this issue in case it helps: https://github.com/indhub/tfperftest/tree/master/perftest\r\n\r\n- Inception model is based on https://github.com/tensorflow/models/tree/master/inception. I've done some changes to use randomly generated synthetic data for images and labels. Modified code here: https://github.com/indhub/tfperftest/tree/master/inception\r\n- Resnet model is based on https://github.com/tensorflow/models/tree/master/resnet. I've done some changes to build Resnet 152. Modified code here: https://github.com/indhub/tfperftest/tree/master/resnet\r\n- Alexnet model is based on https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py. I've added the fully connected layers at the end. Modified code here: https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nubuntu@ip-172-31-52-161:~$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root 558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a\r\n\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nubuntu@ip-172-31-52-161:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0rc2\r\n\r\n```\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nJust running https://github.com/tensorflow/models/tree/master/inception reproduces the problem. \r\n"}