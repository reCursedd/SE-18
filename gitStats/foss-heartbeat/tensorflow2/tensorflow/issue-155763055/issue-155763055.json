{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2431", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2431/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2431/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2431/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2431", "id": 155763055, "node_id": "MDU6SXNzdWUxNTU3NjMwNTU=", "number": 2431, "title": "Better support for breaking up too-large operations", "user": {"login": "orome", "id": 757327, "node_id": "MDQ6VXNlcjc1NzMyNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/757327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orome", "html_url": "https://github.com/orome", "followers_url": "https://api.github.com/users/orome/followers", "following_url": "https://api.github.com/users/orome/following{/other_user}", "gists_url": "https://api.github.com/users/orome/gists{/gist_id}", "starred_url": "https://api.github.com/users/orome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orome/subscriptions", "organizations_url": "https://api.github.com/users/orome/orgs", "repos_url": "https://api.github.com/users/orome/repos", "events_url": "https://api.github.com/users/orome/events{/privacy}", "received_events_url": "https://api.github.com/users/orome/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2016-05-19T15:17:12Z", "updated_at": "2018-06-20T23:10:54Z", "closed_at": "2017-06-16T17:46:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Unless I'm missing something, the ability to automatically break up calculations that don't fit on a device does not seem to be part of the TensorFlow API. This surprises me since one of the very first things users (at least naive ones like me) encounter in readily accessible GPU machines (e.g. AWS EC2 instances) are memory crashes on GPUs.</p>\n<p>To avoid these errors users have to either give up on GPUs for large parts of their calculations (again, unless I'm missing something) or hand code some form of batching to avoid the crashes (if they can figure out where they're happening, which isn't always easy with such errors).</p>\n<p>Shouldn't TensorFlow do this automatically \"under the hood\", breaking up too-large calculations and merging them, to avoid code like that below?</p>\n<hr>\n<p><em>From a <a href=\"http://stackoverflow.com/q/37327312/656912\" rel=\"nofollow\">related question</a> asked on SO:</em></p>\n<p>I'm puzzled about how to efficiently assign my TensorFlow operations and variables to devices. It's clear that, at least for my implementation of a basic convolutional neural network, placing as many operations as possible on a GPU is desirable. But the GPU I currently have access to has limited memory and results in many warnings of the form</p>\n<blockquote>\n<p><code>Ran out of memory trying to allocate 2.60GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.</code></p>\n</blockquote>\n<p>and occasional crashes for certain specific operations, like</p>\n<blockquote>\n<pre><code>Ran out of memory trying to allocate 83.74MiB.  See logs for memory state.\nResource exhausted: OOM when allocating tensor with shape[28000,1,28,28]\n</code></pre>\n</blockquote>\n<p>These can be avoided by placing variables on CPUs, but in my implementation, this results in training epochs taking 10 times as long to compute.</p>\n<p>Clearly the ideal policy is to identify specific chunks of code that generate errors, and attempt to place only those on CPUs. But it is unclear to me how to do this, because those calculations can't be isolated from others that require GPU placement to achieve efficiencies.</p>\n<p>For example, simply generating predictions on a test set with something like</p>\n<pre><code>evals = sess.run(tf.argmax(y, 1), feed_dict={x: use_x_all})\n</code></pre>\n<p>where <code>x</code> is a <code>tf.placeholder</code> of inputs to my model, and <code>y</code> are the output activations of my network, produces the above error when <code>use_x_all</code> is a large array (here with <code>28000</code> examples). Attempting to put this calculation alone on a CPU fails, presumably because the network evaluation producing <code>y</code> is on the GPU.</p>\n<p>Because of this I (seem to) need to resort to approaches like</p>\n<pre><code>use_x_all, _ = data_loader.stack_data(use_data, as_cols=False)\nuse_x_split = np.split(use_x_all, splits)\nfor use_x in use_x_split:\n    # ...\n    evals_part = sess.run(tf.argmax(y, 1), feed_dict={x: use_x})\n    # accumulate evals\n</code></pre>\n<p>which clearly doesn't scale.</p>\n<p>Is there a better way? Specifically:</p>\n<ul>\n<li>Is there a way to place calculations like the one above on a CPU and still have those calculations for the same graph (e.g. training) run on a GPU?</li>\n</ul>\n<p>or, alternatively</p>\n<ul>\n<li>Is there an idiom (like batching) that can be more easily applied in such situations to reduce the memory demands of such calculations?</li>\n</ul>", "body_text": "Unless I'm missing something, the ability to automatically break up calculations that don't fit on a device does not seem to be part of the TensorFlow API. This surprises me since one of the very first things users (at least naive ones like me) encounter in readily accessible GPU machines (e.g. AWS EC2 instances) are memory crashes on GPUs.\nTo avoid these errors users have to either give up on GPUs for large parts of their calculations (again, unless I'm missing something) or hand code some form of batching to avoid the crashes (if they can figure out where they're happening, which isn't always easy with such errors).\nShouldn't TensorFlow do this automatically \"under the hood\", breaking up too-large calculations and merging them, to avoid code like that below?\n\nFrom a related question asked on SO:\nI'm puzzled about how to efficiently assign my TensorFlow operations and variables to devices. It's clear that, at least for my implementation of a basic convolutional neural network, placing as many operations as possible on a GPU is desirable. But the GPU I currently have access to has limited memory and results in many warnings of the form\n\nRan out of memory trying to allocate 2.60GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n\nand occasional crashes for certain specific operations, like\n\nRan out of memory trying to allocate 83.74MiB.  See logs for memory state.\nResource exhausted: OOM when allocating tensor with shape[28000,1,28,28]\n\n\nThese can be avoided by placing variables on CPUs, but in my implementation, this results in training epochs taking 10 times as long to compute.\nClearly the ideal policy is to identify specific chunks of code that generate errors, and attempt to place only those on CPUs. But it is unclear to me how to do this, because those calculations can't be isolated from others that require GPU placement to achieve efficiencies.\nFor example, simply generating predictions on a test set with something like\nevals = sess.run(tf.argmax(y, 1), feed_dict={x: use_x_all})\n\nwhere x is a tf.placeholder of inputs to my model, and y are the output activations of my network, produces the above error when use_x_all is a large array (here with 28000 examples). Attempting to put this calculation alone on a CPU fails, presumably because the network evaluation producing y is on the GPU.\nBecause of this I (seem to) need to resort to approaches like\nuse_x_all, _ = data_loader.stack_data(use_data, as_cols=False)\nuse_x_split = np.split(use_x_all, splits)\nfor use_x in use_x_split:\n    # ...\n    evals_part = sess.run(tf.argmax(y, 1), feed_dict={x: use_x})\n    # accumulate evals\n\nwhich clearly doesn't scale.\nIs there a better way? Specifically:\n\nIs there a way to place calculations like the one above on a CPU and still have those calculations for the same graph (e.g. training) run on a GPU?\n\nor, alternatively\n\nIs there an idiom (like batching) that can be more easily applied in such situations to reduce the memory demands of such calculations?", "body": "Unless I'm missing something, the ability to automatically break up calculations that don't fit on a device does not seem to be part of the TensorFlow API. This surprises me since one of the very first things users (at least naive ones like me) encounter in readily accessible GPU machines (e.g. AWS EC2 instances) are memory crashes on GPUs. \n\nTo avoid these errors users have to either give up on GPUs for large parts of their calculations (again, unless I'm missing something) or hand code some form of batching to avoid the crashes (if they can figure out where they're happening, which isn't always easy with such errors).\n\nShouldn't TensorFlow do this automatically \"under the hood\", breaking up too-large calculations and merging them, to avoid code like that below?\n\n---\n\n_From a [related question](http://stackoverflow.com/q/37327312/656912) asked on SO:_\n\nI'm puzzled about how to efficiently assign my TensorFlow operations and variables to devices. It's clear that, at least for my implementation of a basic convolutional neural network, placing as many operations as possible on a GPU is desirable. But the GPU I currently have access to has limited memory and results in many warnings of the form\n\n> `Ran out of memory trying to allocate 2.60GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.`\n\nand occasional crashes for certain specific operations, like\n\n> ```\n> Ran out of memory trying to allocate 83.74MiB.  See logs for memory state.\n> Resource exhausted: OOM when allocating tensor with shape[28000,1,28,28]\n> ```\n\nThese can be avoided by placing variables on CPUs, but in my implementation, this results in training epochs taking 10 times as long to compute. \n\nClearly the ideal policy is to identify specific chunks of code that generate errors, and attempt to place only those on CPUs. But it is unclear to me how to do this, because those calculations can't be isolated from others that require GPU placement to achieve efficiencies.\n\nFor example, simply generating predictions on a test set with something like\n\n```\nevals = sess.run(tf.argmax(y, 1), feed_dict={x: use_x_all})\n```\n\nwhere `x` is a `tf.placeholder` of inputs to my model, and `y` are the output activations of my network, produces the above error when `use_x_all` is a large array (here with `28000` examples). Attempting to put this calculation alone on a CPU fails, presumably because the network evaluation producing `y` is on the GPU.\n\nBecause of this I (seem to) need to resort to approaches like\n\n```\nuse_x_all, _ = data_loader.stack_data(use_data, as_cols=False)\nuse_x_split = np.split(use_x_all, splits)\nfor use_x in use_x_split:\n    # ...\n    evals_part = sess.run(tf.argmax(y, 1), feed_dict={x: use_x})\n    # accumulate evals\n```\n\nwhich clearly doesn't scale.\n\nIs there a better way? Specifically:\n- Is there a way to place calculations like the one above on a CPU and still have those calculations for the same graph (e.g. training) run on a GPU?\n\nor, alternatively\n- Is there an idiom (like batching) that can be more easily applied in such situations to reduce the memory demands of such calculations?\n"}