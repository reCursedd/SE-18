{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/220403857", "html_url": "https://github.com/tensorflow/tensorflow/issues/2431#issuecomment-220403857", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2431", "id": 220403857, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMDQwMzg1Nw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-19T17:59:24Z", "updated_at": "2016-05-19T17:59:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Why does your <code>np.split(...)</code> approach not scale? This is an easy way to proceed if your dataset fits in host memory. For larger datasets, you can use the standard reader pipeline to read batches of input at a time: e.g. see the <code>batch_inputs()</code> function in the <a href=\"https://github.com/tensorflow/models/blob/dc7791d01c9a6b1fcc40e9e2c1ca107cbd982027/inception/inception/image_processing.py#L407\">Inception model</a>. You could also try <a href=\"https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline\" rel=\"nofollow\"><code>tf.train.batch()</code> and related functions</a> to control how the inputs are batched.</p>\n<p>As to why TensorFlow doesn't do this automatically, there are clearly <a href=\"https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline\" rel=\"nofollow\">lots of different ways to do batching</a>, and TensorFlow can't reliably infer the user's intent. Therefore, we provide higher level libraries to allow users to build the appropriate input pipelines.</p>", "body_text": "Why does your np.split(...) approach not scale? This is an easy way to proceed if your dataset fits in host memory. For larger datasets, you can use the standard reader pipeline to read batches of input at a time: e.g. see the batch_inputs() function in the Inception model. You could also try tf.train.batch() and related functions to control how the inputs are batched.\nAs to why TensorFlow doesn't do this automatically, there are clearly lots of different ways to do batching, and TensorFlow can't reliably infer the user's intent. Therefore, we provide higher level libraries to allow users to build the appropriate input pipelines.", "body": "Why does your `np.split(...)` approach not scale? This is an easy way to proceed if your dataset fits in host memory. For larger datasets, you can use the standard reader pipeline to read batches of input at a time: e.g. see the `batch_inputs()` function in the [Inception model](https://github.com/tensorflow/models/blob/dc7791d01c9a6b1fcc40e9e2c1ca107cbd982027/inception/inception/image_processing.py#L407). You could also try [`tf.train.batch()` and related functions](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline) to control how the inputs are batched.\n\nAs to why TensorFlow doesn't do this automatically, there are clearly [lots of different ways to do batching](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline), and TensorFlow can't reliably infer the user's intent. Therefore, we provide higher level libraries to allow users to build the appropriate input pipelines. \n"}