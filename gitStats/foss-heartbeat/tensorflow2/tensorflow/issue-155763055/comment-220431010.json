{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/220431010", "html_url": "https://github.com/tensorflow/tensorflow/issues/2431#issuecomment-220431010", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2431", "id": 220431010, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMDQzMTAxMA==", "user": {"login": "orome", "id": 757327, "node_id": "MDQ6VXNlcjc1NzMyNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/757327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orome", "html_url": "https://github.com/orome", "followers_url": "https://api.github.com/users/orome/followers", "following_url": "https://api.github.com/users/orome/following{/other_user}", "gists_url": "https://api.github.com/users/orome/gists{/gist_id}", "starred_url": "https://api.github.com/users/orome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orome/subscriptions", "organizations_url": "https://api.github.com/users/orome/orgs", "repos_url": "https://api.github.com/users/orome/repos", "events_url": "https://api.github.com/users/orome/events{/privacy}", "received_events_url": "https://api.github.com/users/orome/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-19T19:42:31Z", "updated_at": "2016-05-19T19:42:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> \u2014 At least in this context, it's not an issue of the many ways to do batching, but a simple matter of  sequencing: doing something in parts rather than all at once (as in the hand-coded version). What doesn't scale is that I need to ferret out every place where this happens, and write code like I did above.</p>\n<p>But I see the problem: its not easily generalized, as you say. The data might need shuffling, there could be various ways to combine the results, and not all of it may fit in memory at once, etc.</p>\n<p>So I guess as long as I'm \"doing it right\", I'm not worried; and the question boils down to that: Is it idiomatic to be manually breaking up and reassembling data fed to TF operations as needed when they result in calculations that are too big for the hardware?</p>", "body_text": "@mrry \u2014 At least in this context, it's not an issue of the many ways to do batching, but a simple matter of  sequencing: doing something in parts rather than all at once (as in the hand-coded version). What doesn't scale is that I need to ferret out every place where this happens, and write code like I did above.\nBut I see the problem: its not easily generalized, as you say. The data might need shuffling, there could be various ways to combine the results, and not all of it may fit in memory at once, etc.\nSo I guess as long as I'm \"doing it right\", I'm not worried; and the question boils down to that: Is it idiomatic to be manually breaking up and reassembling data fed to TF operations as needed when they result in calculations that are too big for the hardware?", "body": "@mrry \u2014 At least in this context, it's not an issue of the many ways to do batching, but a simple matter of  sequencing: doing something in parts rather than all at once (as in the hand-coded version). What doesn't scale is that I need to ferret out every place where this happens, and write code like I did above.\n\nBut I see the problem: its not easily generalized, as you say. The data might need shuffling, there could be various ways to combine the results, and not all of it may fit in memory at once, etc.\n\nSo I guess as long as I'm \"doing it right\", I'm not worried; and the question boils down to that: Is it idiomatic to be manually breaking up and reassembling data fed to TF operations as needed when they result in calculations that are too big for the hardware?\n"}