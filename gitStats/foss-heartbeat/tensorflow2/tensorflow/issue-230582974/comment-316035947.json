{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316035947", "html_url": "https://github.com/tensorflow/tensorflow/issues/10125#issuecomment-316035947", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10125", "id": 316035947, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjAzNTk0Nw==", "user": {"login": "discoveredcheck", "id": 25035016, "node_id": "MDQ6VXNlcjI1MDM1MDE2", "avatar_url": "https://avatars1.githubusercontent.com/u/25035016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/discoveredcheck", "html_url": "https://github.com/discoveredcheck", "followers_url": "https://api.github.com/users/discoveredcheck/followers", "following_url": "https://api.github.com/users/discoveredcheck/following{/other_user}", "gists_url": "https://api.github.com/users/discoveredcheck/gists{/gist_id}", "starred_url": "https://api.github.com/users/discoveredcheck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/discoveredcheck/subscriptions", "organizations_url": "https://api.github.com/users/discoveredcheck/orgs", "repos_url": "https://api.github.com/users/discoveredcheck/repos", "events_url": "https://api.github.com/users/discoveredcheck/events{/privacy}", "received_events_url": "https://api.github.com/users/discoveredcheck/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-18T11:28:51Z", "updated_at": "2017-07-18T11:28:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi<br>\nSeconding <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16302833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/appierys\">@appierys</a> 's request, we have also been experimenting with weight-normalisation in RNNs with great results. It seems to be significantly helpful in stabilising training and reducing its dependence on hyperparameters. Seeing as this has become a default RNN configuration for us, we are quite keen on integrating it with the tensorflow rnn machinery.</p>\n<p>Our implementation is based on<br>\n<a href=\"https://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py\">https://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py</a></p>\n<p>which requires a few lines of additional code in tensorflow core. This is because in the current setup (dynamic_rnn etc.) RNN weights are created via the cell's call() function which is called from within tf.while(). Based on the above link, weight-normalised cells require weight creation and normalisation before entering the while loop.</p>\n<p>This may not be the best way to implement it, but for the moment we have sent in a pull request:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"243684111\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11573\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/11573/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/11573\">#11573</a></p>\n<p>We'd be keen to hear your thoughts and/or to engage in further testing/discussing a different implementation structure.</p>", "body_text": "Hi\nSeconding @appierys 's request, we have also been experimenting with weight-normalisation in RNNs with great results. It seems to be significantly helpful in stabilising training and reducing its dependence on hyperparameters. Seeing as this has become a default RNN configuration for us, we are quite keen on integrating it with the tensorflow rnn machinery.\nOur implementation is based on\nhttps://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py\nwhich requires a few lines of additional code in tensorflow core. This is because in the current setup (dynamic_rnn etc.) RNN weights are created via the cell's call() function which is called from within tf.while(). Based on the above link, weight-normalised cells require weight creation and normalisation before entering the while loop.\nThis may not be the best way to implement it, but for the moment we have sent in a pull request:\n#11573\nWe'd be keen to hear your thoughts and/or to engage in further testing/discussing a different implementation structure.", "body": "Hi\r\nSeconding @appierys 's request, we have also been experimenting with weight-normalisation in RNNs with great results. It seems to be significantly helpful in stabilising training and reducing its dependence on hyperparameters. Seeing as this has become a default RNN configuration for us, we are quite keen on integrating it with the tensorflow rnn machinery. \r\n\r\nOur implementation is based on \r\nhttps://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py\r\n\r\nwhich requires a few lines of additional code in tensorflow core. This is because in the current setup (dynamic_rnn etc.) RNN weights are created via the cell's call() function which is called from within tf.while(). Based on the above link, weight-normalised cells require weight creation and normalisation before entering the while loop. \r\n\r\nThis may not be the best way to implement it, but for the moment we have sent in a pull request:\r\nhttps://github.com/tensorflow/tensorflow/pull/11573\r\n\r\nWe'd be keen to hear your thoughts and/or to engage in further testing/discussing a different implementation structure.\r\n"}