{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293622506", "html_url": "https://github.com/tensorflow/tensorflow/issues/2373#issuecomment-293622506", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373", "id": 293622506, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzYyMjUwNg==", "user": {"login": "memeda", "id": 5585818, "node_id": "MDQ6VXNlcjU1ODU4MTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5585818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/memeda", "html_url": "https://github.com/memeda", "followers_url": "https://api.github.com/users/memeda/followers", "following_url": "https://api.github.com/users/memeda/following{/other_user}", "gists_url": "https://api.github.com/users/memeda/gists{/gist_id}", "starred_url": "https://api.github.com/users/memeda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/memeda/subscriptions", "organizations_url": "https://api.github.com/users/memeda/orgs", "repos_url": "https://api.github.com/users/memeda/repos", "events_url": "https://api.github.com/users/memeda/events{/privacy}", "received_events_url": "https://api.github.com/users/memeda/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-12T15:50:26Z", "updated_at": "2017-04-13T14:32:45Z", "author_association": "NONE", "body_html": "<p>Hi, I come across this problem and get into this issue by searching. I'm so glad that this question is proposed properly, which indicating a SEEK-THE-PRECISE attitude!</p>\n<p>I'm not very familiar with TF, so <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10116557\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/LittleYUYU\">@LittleYUYU</a> 's methods description still un-clearly until I come across the code at <a href=\"http://stackoverflow.com/questions/37255038/embedding-lookup-table-doesnt-mask-padding-value\" rel=\"nofollow\">Embedding lookup table doesn't mask padding value</a> (the same question in stackoverflow), I rewrite it to support arbitrary PADDING ID value instead only 0.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> omit some CONSTANT</span>\nraw_mask_array <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">1</span>.]] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">PADDING_ID</span> <span class=\"pl-k\">+</span> [[<span class=\"pl-c1\">0</span>.]] <span class=\"pl-k\">+</span> [[<span class=\"pl-c1\">1</span>.]] <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">WORDS_NUM</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">PADDING_ID</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Embedding<span class=\"pl-pds\">\"</span></span>):\n    lookup_table <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lookup_table<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">WORDS_NUM</span>, <span class=\"pl-c1\">EMBEDDING_DIM</span>],\n                                   <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_uniform_initializer(\n                                               <span class=\"pl-v\">minval</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span>R, <span class=\"pl-v\">maxval</span><span class=\"pl-k\">=</span>R),\n                                   <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DTYPE</span>,\n                                   <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    mask_padding_lookup_table <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mask_padding_lookup_table<span class=\"pl-pds\">\"</span></span>,\n                                                <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>raw_mask_array,\n                                                <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DTYPE</span>,\n                                                <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nembedding_input <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(lookup_table, id_input)\nmask_padding_input <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(mask_padding_lookup_table, id_input)\nembedding_input <span class=\"pl-k\">=</span> tf.multiply(embedding_input, mask_padding_input) <span class=\"pl-c\"><span class=\"pl-c\">#</span> broadcast</span></pre></div>\n<hr>\n<p>as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8296469\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/youngia\">@youngia</a> said, <strong>following method can't work</strong>. keep it only for record the following response. I'm so sorry to make a wrong method.</p>\n<p><del>what's more, I make a more simple solution(which has been write down in that stackoverflow page):</del></p>\n<div class=\"highlight highlight-source-python\"><pre>mask_padding_zero_op <span class=\"pl-k\">=</span> tf.scatter_update(lookup_table, \n                                         <span class=\"pl-c1\">PADDING_ID</span>, \n                                         tf.zeros([<span class=\"pl-c1\">EMBEDDING_DIM</span>,], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DTYPE</span>))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> just for explicity</span>\nlookup_table <span class=\"pl-k\">=</span> mask_padding_zero_op</pre></div>\n<p><del>just to reset PADDING ID's embedding value to zeros and replace it. see the stackoverflow page for details.</del></p>\n<p><del>this code may be naive or not property, writing here for some  people like me searching this. Any errors please point out.</del></p>", "body_text": "Hi, I come across this problem and get into this issue by searching. I'm so glad that this question is proposed properly, which indicating a SEEK-THE-PRECISE attitude!\nI'm not very familiar with TF, so @LittleYUYU 's methods description still un-clearly until I come across the code at Embedding lookup table doesn't mask padding value (the same question in stackoverflow), I rewrite it to support arbitrary PADDING ID value instead only 0.\n# omit some CONSTANT\nraw_mask_array = [[1.]] * PADDING_ID + [[0.]] + [[1.]] * (WORDS_NUM - PADDING_ID - 1)\nwith tf.variable_scope(\"Embedding\"):\n    lookup_table = tf.get_variable(\"lookup_table\", shape=[WORDS_NUM, EMBEDDING_DIM],\n                                   initializer=tf.random_uniform_initializer(\n                                               minval=-R, maxval=R),\n                                   dtype=DTYPE,\n                                   trainable=True)\n    mask_padding_lookup_table = tf.get_variable(\"mask_padding_lookup_table\",\n                                                initializer=raw_mask_array,\n                                                dtype=DTYPE,\n                                                trainable=False)\nembedding_input = tf.nn.embedding_lookup(lookup_table, id_input)\nmask_padding_input = tf.nn.embedding_lookup(mask_padding_lookup_table, id_input)\nembedding_input = tf.multiply(embedding_input, mask_padding_input) # broadcast\n\nas @youngia said, following method can't work. keep it only for record the following response. I'm so sorry to make a wrong method.\nwhat's more, I make a more simple solution(which has been write down in that stackoverflow page):\nmask_padding_zero_op = tf.scatter_update(lookup_table, \n                                         PADDING_ID, \n                                         tf.zeros([EMBEDDING_DIM,], dtype=DTYPE))\n# just for explicity\nlookup_table = mask_padding_zero_op\njust to reset PADDING ID's embedding value to zeros and replace it. see the stackoverflow page for details.\nthis code may be naive or not property, writing here for some  people like me searching this. Any errors please point out.", "body": "Hi, I come across this problem and get into this issue by searching. I'm so glad that this question is proposed properly, which indicating a SEEK-THE-PRECISE attitude!\r\n\r\nI'm not very familiar with TF, so @LittleYUYU 's methods description still un-clearly until I come across the code at [Embedding lookup table doesn't mask padding value](http://stackoverflow.com/questions/37255038/embedding-lookup-table-doesnt-mask-padding-value) (the same question in stackoverflow), I rewrite it to support arbitrary PADDING ID value instead only 0.\r\n\r\n```Python\r\n# omit some CONSTANT\r\nraw_mask_array = [[1.]] * PADDING_ID + [[0.]] + [[1.]] * (WORDS_NUM - PADDING_ID - 1)\r\nwith tf.variable_scope(\"Embedding\"):\r\n    lookup_table = tf.get_variable(\"lookup_table\", shape=[WORDS_NUM, EMBEDDING_DIM],\r\n                                   initializer=tf.random_uniform_initializer(\r\n                                               minval=-R, maxval=R),\r\n                                   dtype=DTYPE,\r\n                                   trainable=True)\r\n    mask_padding_lookup_table = tf.get_variable(\"mask_padding_lookup_table\",\r\n                                                initializer=raw_mask_array,\r\n                                                dtype=DTYPE,\r\n                                                trainable=False)\r\nembedding_input = tf.nn.embedding_lookup(lookup_table, id_input)\r\nmask_padding_input = tf.nn.embedding_lookup(mask_padding_lookup_table, id_input)\r\nembedding_input = tf.multiply(embedding_input, mask_padding_input) # broadcast\r\n``` \r\n\r\n-----\r\n\r\nas @youngia said, **following method can't work**. keep it only for record the following response. I'm so sorry to make a wrong method.\r\n\r\n<del>what's more, I make a more simple solution(which has been write down in that stackoverflow page):</del>\r\n\r\n```Python\r\nmask_padding_zero_op = tf.scatter_update(lookup_table, \r\n                                         PADDING_ID, \r\n                                         tf.zeros([EMBEDDING_DIM,], dtype=DTYPE))\r\n# just for explicity\r\nlookup_table = mask_padding_zero_op\r\n```\r\n\r\n<del>just to reset PADDING ID's embedding value to zeros and replace it. see the stackoverflow page for details.</del>\r\n\r\n<del>this code may be naive or not property, writing here for some  people like me searching this. Any errors please point out.</del>"}