{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243146150", "html_url": "https://github.com/tensorflow/tensorflow/issues/2373#issuecomment-243146150", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373", "id": 243146150, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzE0NjE1MA==", "user": {"login": "shawnjhenry", "id": 9464836, "node_id": "MDQ6VXNlcjk0NjQ4MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9464836?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnjhenry", "html_url": "https://github.com/shawnjhenry", "followers_url": "https://api.github.com/users/shawnjhenry/followers", "following_url": "https://api.github.com/users/shawnjhenry/following{/other_user}", "gists_url": "https://api.github.com/users/shawnjhenry/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnjhenry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnjhenry/subscriptions", "organizations_url": "https://api.github.com/users/shawnjhenry/orgs", "repos_url": "https://api.github.com/users/shawnjhenry/repos", "events_url": "https://api.github.com/users/shawnjhenry/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnjhenry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-29T14:47:15Z", "updated_at": "2016-08-29T14:47:15Z", "author_association": "NONE", "body_html": "<p>This can actually cause a fairly substantial fluctuation in performance in some networks.  Suppose that instead of a convnet, we feed the embeddings to a deep averaging network.  Then the varying number of nonzero pad vectors (according to which training batch the example is assigned in SGD) will very much affect the value of the average embedding.  I've seen this cause a variation of up to 3% accuracy on text classification tasks.  One possible remedy is to set the value of the embedding for the pad index to zero explicitly between each backprop step during training.  This is computationally kind of wasteful (and also requires explicitly feeding the number of nonzero embeddings in each sample to the network), but it does the trick.  I would like to see a feature like Torch's LookupTableMaskZero as well.</p>", "body_text": "This can actually cause a fairly substantial fluctuation in performance in some networks.  Suppose that instead of a convnet, we feed the embeddings to a deep averaging network.  Then the varying number of nonzero pad vectors (according to which training batch the example is assigned in SGD) will very much affect the value of the average embedding.  I've seen this cause a variation of up to 3% accuracy on text classification tasks.  One possible remedy is to set the value of the embedding for the pad index to zero explicitly between each backprop step during training.  This is computationally kind of wasteful (and also requires explicitly feeding the number of nonzero embeddings in each sample to the network), but it does the trick.  I would like to see a feature like Torch's LookupTableMaskZero as well.", "body": "This can actually cause a fairly substantial fluctuation in performance in some networks.  Suppose that instead of a convnet, we feed the embeddings to a deep averaging network.  Then the varying number of nonzero pad vectors (according to which training batch the example is assigned in SGD) will very much affect the value of the average embedding.  I've seen this cause a variation of up to 3% accuracy on text classification tasks.  One possible remedy is to set the value of the embedding for the pad index to zero explicitly between each backprop step during training.  This is computationally kind of wasteful (and also requires explicitly feeding the number of nonzero embeddings in each sample to the network), but it does the trick.  I would like to see a feature like Torch's LookupTableMaskZero as well.   \n"}