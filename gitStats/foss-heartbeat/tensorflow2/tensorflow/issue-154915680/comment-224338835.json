{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/224338835", "html_url": "https://github.com/tensorflow/tensorflow/issues/2373#issuecomment-224338835", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373", "id": 224338835, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDMzODgzNQ==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-07T16:37:01Z", "updated_at": "2016-06-07T16:37:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think the sample code doesn't consider the issue because it doesn't affect neural network performance.  If you're seeing significant differences between zero padding an new-token padding, I would use a mask as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10116557\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/LittleYUYU\">@LittleYUYU</a> suggests.  <code>embedding_lookup</code> doesn't handle this by itself since we try to keep the semantics of each op simple and focused.</p>", "body_text": "I think the sample code doesn't consider the issue because it doesn't affect neural network performance.  If you're seeing significant differences between zero padding an new-token padding, I would use a mask as @LittleYUYU suggests.  embedding_lookup doesn't handle this by itself since we try to keep the semantics of each op simple and focused.", "body": "I think the sample code doesn't consider the issue because it doesn't affect neural network performance.  If you're seeing significant differences between zero padding an new-token padding, I would use a mask as @LittleYUYU suggests.  `embedding_lookup` doesn't handle this by itself since we try to keep the semantics of each op simple and focused.\n"}