{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/220718781", "html_url": "https://github.com/tensorflow/tensorflow/issues/2373#issuecomment-220718781", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373", "id": 220718781, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMDcxODc4MQ==", "user": {"login": "LittleYUYU", "id": 10116557, "node_id": "MDQ6VXNlcjEwMTE2NTU3", "avatar_url": "https://avatars1.githubusercontent.com/u/10116557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LittleYUYU", "html_url": "https://github.com/LittleYUYU", "followers_url": "https://api.github.com/users/LittleYUYU/followers", "following_url": "https://api.github.com/users/LittleYUYU/following{/other_user}", "gists_url": "https://api.github.com/users/LittleYUYU/gists{/gist_id}", "starred_url": "https://api.github.com/users/LittleYUYU/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LittleYUYU/subscriptions", "organizations_url": "https://api.github.com/users/LittleYUYU/orgs", "repos_url": "https://api.github.com/users/LittleYUYU/repos", "events_url": "https://api.github.com/users/LittleYUYU/events{/privacy}", "received_events_url": "https://api.github.com/users/LittleYUYU/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-20T21:07:30Z", "updated_at": "2016-05-20T21:07:30Z", "author_association": "NONE", "body_html": "<p>I guess I met the same problem, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1479733\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aleSuglia\">@aleSuglia</a> .</p>\n<p>In the released \"tranlsate\" codes of a seq2seq model, buckets are used for defining different lengths of encoders and decoders. Every time we get a batch of data, we pad the original input with \"PAD\" to satisfy the required length of a bucket, e.g., [1,2,3] --&gt; [1,2,3,PAD,PAD] for the bucket with length=5. However, we take the PAD as a normal token in embedding. That's strange.</p>\n<p>In training a seq2seq model, we usually reverse the input sequence to be [PAD, PAD, 3, 2, 1]. In this case, the embedding of PAD can change the final decoding result. In my experiments, when testing new data without knowing its corresponding bucket, the decoding result would be different with different buckets.</p>\n<p>So I guess that's a bug? Thanks much!</p>\n<p>Ziyu</p>", "body_text": "I guess I met the same problem, @aleSuglia .\nIn the released \"tranlsate\" codes of a seq2seq model, buckets are used for defining different lengths of encoders and decoders. Every time we get a batch of data, we pad the original input with \"PAD\" to satisfy the required length of a bucket, e.g., [1,2,3] --> [1,2,3,PAD,PAD] for the bucket with length=5. However, we take the PAD as a normal token in embedding. That's strange.\nIn training a seq2seq model, we usually reverse the input sequence to be [PAD, PAD, 3, 2, 1]. In this case, the embedding of PAD can change the final decoding result. In my experiments, when testing new data without knowing its corresponding bucket, the decoding result would be different with different buckets.\nSo I guess that's a bug? Thanks much!\nZiyu", "body": "I guess I met the same problem, @aleSuglia . \n\nIn the released \"tranlsate\" codes of a seq2seq model, buckets are used for defining different lengths of encoders and decoders. Every time we get a batch of data, we pad the original input with \"PAD\" to satisfy the required length of a bucket, e.g., [1,2,3] --> [1,2,3,PAD,PAD] for the bucket with length=5. However, we take the PAD as a normal token in embedding. That's strange.\n\nIn training a seq2seq model, we usually reverse the input sequence to be [PAD, PAD, 3, 2, 1]. In this case, the embedding of PAD can change the final decoding result. In my experiments, when testing new data without knowing its corresponding bucket, the decoding result would be different with different buckets.\n\nSo I guess that's a bug? Thanks much!\n\nZiyu\n"}