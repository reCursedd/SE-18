{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/220775997", "html_url": "https://github.com/tensorflow/tensorflow/issues/2373#issuecomment-220775997", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373", "id": 220775997, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMDc3NTk5Nw==", "user": {"login": "LittleYUYU", "id": 10116557, "node_id": "MDQ6VXNlcjEwMTE2NTU3", "avatar_url": "https://avatars1.githubusercontent.com/u/10116557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LittleYUYU", "html_url": "https://github.com/LittleYUYU", "followers_url": "https://api.github.com/users/LittleYUYU/followers", "following_url": "https://api.github.com/users/LittleYUYU/following{/other_user}", "gists_url": "https://api.github.com/users/LittleYUYU/gists{/gist_id}", "starred_url": "https://api.github.com/users/LittleYUYU/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LittleYUYU/subscriptions", "organizations_url": "https://api.github.com/users/LittleYUYU/orgs", "repos_url": "https://api.github.com/users/LittleYUYU/repos", "events_url": "https://api.github.com/users/LittleYUYU/events{/privacy}", "received_events_url": "https://api.github.com/users/LittleYUYU/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-21T12:46:00Z", "updated_at": "2017-04-13T13:38:53Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1479733\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aleSuglia\">@aleSuglia</a> Right! Currently I use a mask which is a matrix with the same dimension as the embedding matrix but with the padding value set to be zero to solve this problem. I just feel confused: why the released sample code doesn't consider this issue? Thanks!</p>\n<p>UPDATE (04/13/17):<br>\nI posted this suggestion nearly one year ago and I'd like to update my solution to the padding problem for now. <strong>If the original answer looks confusing, then just skip it.</strong></p>\n<p>If you are using RNN, you can certainly pass a \"sequence_length\" parameter, such that the reverse of [1, 2, 3, PAD, PAD] will be [3, 2, 1, PAD, PAD] and the valid RNN steps will be [1, 2, 3], so now the PAD won't affect the RNN anymore.</p>\n<p>If you need to do any operation on the top of the RNN output, you can design a \"mask\". For example, if your sequence input (in a batch) looks like:<br>\n[[1,2,3,PAD,PAD]<br>\n[4,5,PAD,PAD,PAD]<br>\n[6,7,8,9,PAD]]<br>\nthen the mask can be<br>\n[[1,1,1,0,0]<br>\n[1,1,0,0,0]<br>\n[1,1,1,1,0]]<br>\nwhere all PAD will be marked as zero otherwise one. Then simply multiply the RNN output by the mask to eliminate the effect of PAD before your further operations (e.g., you may need the sum of each row for some purposes).</p>\n<p>If you are using CNN, you may design a similar mask before pooling.</p>", "body_text": "@aleSuglia Right! Currently I use a mask which is a matrix with the same dimension as the embedding matrix but with the padding value set to be zero to solve this problem. I just feel confused: why the released sample code doesn't consider this issue? Thanks!\nUPDATE (04/13/17):\nI posted this suggestion nearly one year ago and I'd like to update my solution to the padding problem for now. If the original answer looks confusing, then just skip it.\nIf you are using RNN, you can certainly pass a \"sequence_length\" parameter, such that the reverse of [1, 2, 3, PAD, PAD] will be [3, 2, 1, PAD, PAD] and the valid RNN steps will be [1, 2, 3], so now the PAD won't affect the RNN anymore.\nIf you need to do any operation on the top of the RNN output, you can design a \"mask\". For example, if your sequence input (in a batch) looks like:\n[[1,2,3,PAD,PAD]\n[4,5,PAD,PAD,PAD]\n[6,7,8,9,PAD]]\nthen the mask can be\n[[1,1,1,0,0]\n[1,1,0,0,0]\n[1,1,1,1,0]]\nwhere all PAD will be marked as zero otherwise one. Then simply multiply the RNN output by the mask to eliminate the effect of PAD before your further operations (e.g., you may need the sum of each row for some purposes).\nIf you are using CNN, you may design a similar mask before pooling.", "body": "@aleSuglia Right! Currently I use a mask which is a matrix with the same dimension as the embedding matrix but with the padding value set to be zero to solve this problem. I just feel confused: why the released sample code doesn't consider this issue? Thanks!\r\n\r\nUPDATE (04/13/17):\r\nI posted this suggestion nearly one year ago and I'd like to update my solution to the padding problem for now. **If the original answer looks confusing, then just skip it.**\r\n\r\nIf you are using RNN, you can certainly pass a \"sequence_length\" parameter, such that the reverse of [1, 2, 3, PAD, PAD] will be [3, 2, 1, PAD, PAD] and the valid RNN steps will be [1, 2, 3], so now the PAD won't affect the RNN anymore. \r\n\r\nIf you need to do any operation on the top of the RNN output, you can design a \"mask\". For example, if your sequence input (in a batch) looks like:\r\n[[1,2,3,PAD,PAD]\r\n[4,5,PAD,PAD,PAD]\r\n[6,7,8,9,PAD]]\r\nthen the mask can be \r\n[[1,1,1,0,0]\r\n[1,1,0,0,0]\r\n[1,1,1,1,0]]\r\nwhere all PAD will be marked as zero otherwise one. Then simply multiply the RNN output by the mask to eliminate the effect of PAD before your further operations (e.g., you may need the sum of each row for some purposes).\r\n\r\nIf you are using CNN, you may design a similar mask before pooling.\r\n\r\n"}