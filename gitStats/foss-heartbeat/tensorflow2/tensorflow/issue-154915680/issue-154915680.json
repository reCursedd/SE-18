{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2373/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2373", "id": 154915680, "node_id": "MDU6SXNzdWUxNTQ5MTU2ODA=", "number": 2373, "title": "Embedding lookup table doesn't mask padding value", "user": {"login": "aleSuglia", "id": 1479733, "node_id": "MDQ6VXNlcjE0Nzk3MzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1479733?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aleSuglia", "html_url": "https://github.com/aleSuglia", "followers_url": "https://api.github.com/users/aleSuglia/followers", "following_url": "https://api.github.com/users/aleSuglia/following{/other_user}", "gists_url": "https://api.github.com/users/aleSuglia/gists{/gist_id}", "starred_url": "https://api.github.com/users/aleSuglia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aleSuglia/subscriptions", "organizations_url": "https://api.github.com/users/aleSuglia/orgs", "repos_url": "https://api.github.com/users/aleSuglia/repos", "events_url": "https://api.github.com/users/aleSuglia/events{/privacy}", "received_events_url": "https://api.github.com/users/aleSuglia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2016-05-15T16:00:27Z", "updated_at": "2018-04-24T09:14:25Z", "closed_at": "2016-06-07T16:37:01Z", "author_association": "NONE", "body_html": "<p>Hi there,</p>\n<p>I'm using an <em>embedding_lookup</em> operation in order to generate dense vector representations for each token in my document which are feed to a convolutional neural network (the network architecture is similar to the one in a <a href=\"http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\" rel=\"nofollow\">WildML article</a>).</p>\n<p>Everything works correctly but when I pad my document inserting a padding value in it, the embedding lookup generates a vector for this token too. I think that this approach could alterate the results in the classification task. What I want to achieve is something similar to what Torch <a href=\"https://github.com/Element-Research/rnn#rnn.LookupTableMaskZerol\">LookupTableMaskZero</a> does.</p>\n<ol>\n<li>Is correct what I want to do?</li>\n<li>Is already implemented something like this?</li>\n<li>If not, how can I mask the padding value in order to prevent the generation of the corresponding vector for it?</li>\n</ol>\n<p>Thank you in advance,<br>\nAlessandro</p>", "body_text": "Hi there,\nI'm using an embedding_lookup operation in order to generate dense vector representations for each token in my document which are feed to a convolutional neural network (the network architecture is similar to the one in a WildML article).\nEverything works correctly but when I pad my document inserting a padding value in it, the embedding lookup generates a vector for this token too. I think that this approach could alterate the results in the classification task. What I want to achieve is something similar to what Torch LookupTableMaskZero does.\n\nIs correct what I want to do?\nIs already implemented something like this?\nIf not, how can I mask the padding value in order to prevent the generation of the corresponding vector for it?\n\nThank you in advance,\nAlessandro", "body": "Hi there,\n\nI'm using an _embedding_lookup_ operation in order to generate dense vector representations for each token in my document which are feed to a convolutional neural network (the network architecture is similar to the one in a [WildML article](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)). \n\nEverything works correctly but when I pad my document inserting a padding value in it, the embedding lookup generates a vector for this token too. I think that this approach could alterate the results in the classification task. What I want to achieve is something similar to what Torch [LookupTableMaskZero](https://github.com/Element-Research/rnn#rnn.LookupTableMaskZerol) does. \n\n1) Is correct what I want to do?\n2) Is already implemented something like this? \n3) If not, how can I mask the padding value in order to prevent the generation of the corresponding vector for it? \n\nThank you in advance, \nAlessandro\n"}