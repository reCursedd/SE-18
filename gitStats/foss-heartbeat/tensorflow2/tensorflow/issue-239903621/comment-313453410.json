{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313453410", "html_url": "https://github.com/tensorflow/tensorflow/issues/11196#issuecomment-313453410", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11196", "id": 313453410, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzQ1MzQxMA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-06T16:47:35Z", "updated_at": "2017-07-06T16:47:35Z", "author_association": "MEMBER", "body_html": "<p>The analysis of the situation by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13041074\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jongsae\">@jongsae</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2613663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/byronyi\">@byronyi</a> seems largely correct.  Inter-process communication is less performant than intra-process, so the later is to be preferred when using multiple GPUs on a single machine.   Separate processes cannot access each other's address space, so TF will always try to use an RPC when communicating between processes, even if they're on the same machine.  Although DMA will be used in moving data on/off GPUs, there will be lots of extra useless work to execute the RPC whose data packets don't actually exit the local machine.</p>\n<p>My first thought would be to restructure the program construction so that the elements which are now executed as independent processes could be packaged either as subgraphs assigned to a single worker, or subgraphs assigned to different workers, depending on whether real (cross-process) distribution is actually required.  I understand this may be difficult when trying to reuse software not designed for that purpose.  Perhaps <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1284535\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/saeta\">@saeta</a>  has some useful observations on the general problem of deploying data parallel programs with one versus multiple processes.</p>", "body_text": "The analysis of the situation by @jongsae and @byronyi seems largely correct.  Inter-process communication is less performant than intra-process, so the later is to be preferred when using multiple GPUs on a single machine.   Separate processes cannot access each other's address space, so TF will always try to use an RPC when communicating between processes, even if they're on the same machine.  Although DMA will be used in moving data on/off GPUs, there will be lots of extra useless work to execute the RPC whose data packets don't actually exit the local machine.\nMy first thought would be to restructure the program construction so that the elements which are now executed as independent processes could be packaged either as subgraphs assigned to a single worker, or subgraphs assigned to different workers, depending on whether real (cross-process) distribution is actually required.  I understand this may be difficult when trying to reuse software not designed for that purpose.  Perhaps @saeta  has some useful observations on the general problem of deploying data parallel programs with one versus multiple processes.", "body": "The analysis of the situation by @jongsae and @byronyi seems largely correct.  Inter-process communication is less performant than intra-process, so the later is to be preferred when using multiple GPUs on a single machine.   Separate processes cannot access each other's address space, so TF will always try to use an RPC when communicating between processes, even if they're on the same machine.  Although DMA will be used in moving data on/off GPUs, there will be lots of extra useless work to execute the RPC whose data packets don't actually exit the local machine.\r\n\r\nMy first thought would be to restructure the program construction so that the elements which are now executed as independent processes could be packaged either as subgraphs assigned to a single worker, or subgraphs assigned to different workers, depending on whether real (cross-process) distribution is actually required.  I understand this may be difficult when trying to reuse software not designed for that purpose.  Perhaps @saeta  has some useful observations on the general problem of deploying data parallel programs with one versus multiple processes.\r\n"}