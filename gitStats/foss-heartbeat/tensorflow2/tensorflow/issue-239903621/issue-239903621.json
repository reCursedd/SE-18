{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11196", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11196/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11196/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11196/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11196", "id": 239903621, "node_id": "MDU6SXNzdWUyMzk5MDM2MjE=", "number": 11196, "title": "[Feature Request] Distributed Tensorflow with Data Parallelism and DMA", "user": {"login": "jongsae", "id": 13041074, "node_id": "MDQ6VXNlcjEzMDQxMDc0", "avatar_url": "https://avatars0.githubusercontent.com/u/13041074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jongsae", "html_url": "https://github.com/jongsae", "followers_url": "https://api.github.com/users/jongsae/followers", "following_url": "https://api.github.com/users/jongsae/following{/other_user}", "gists_url": "https://api.github.com/users/jongsae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jongsae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jongsae/subscriptions", "organizations_url": "https://api.github.com/users/jongsae/orgs", "repos_url": "https://api.github.com/users/jongsae/repos", "events_url": "https://api.github.com/users/jongsae/events{/privacy}", "received_events_url": "https://api.github.com/users/jongsae/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-06-30T22:35:42Z", "updated_at": "2018-01-23T23:09:39Z", "closed_at": "2018-01-22T17:33:11Z", "author_association": "NONE", "body_html": "<p>Hi, I have a machine with multiple GPUs and deployed a distributed Tensorflow exploiting data parallelism. I use ClusterConfig to configure the cluster's topology and pass this to Experiment to run the distributed training. I wanted to make one of the GPUs as the parameter server and use the rest for the workers, each of which uses one GPU. I did this through launching multiple processes for parameter server and workers while setting up the env variable (CUDA_VISIBLE_DEVICES) to one of the GPUs.</p>\n<p>In this case, I noticed that the GPUs don't communicate to each other via DMA and I guess this is because the DMA is only available between visible devices to Tensorflow.</p>\n<p>I also tried \"device_count={\"GPU\": 1}\" to make Tensorflow see all the GPUs while using only one GPU, but this still seems to occupy the whole resource of all visible GPUs, preventing another Tensorflow process to be launched over the idle GPU.</p>\n<p>It would be great if there is a way to use only one GPU for a Tensorflow process but still enable DMA with the other GPUs sitting in the system. Am I missing such feature even though it exists?</p>\n<p>Thank you!</p>", "body_text": "Hi, I have a machine with multiple GPUs and deployed a distributed Tensorflow exploiting data parallelism. I use ClusterConfig to configure the cluster's topology and pass this to Experiment to run the distributed training. I wanted to make one of the GPUs as the parameter server and use the rest for the workers, each of which uses one GPU. I did this through launching multiple processes for parameter server and workers while setting up the env variable (CUDA_VISIBLE_DEVICES) to one of the GPUs.\nIn this case, I noticed that the GPUs don't communicate to each other via DMA and I guess this is because the DMA is only available between visible devices to Tensorflow.\nI also tried \"device_count={\"GPU\": 1}\" to make Tensorflow see all the GPUs while using only one GPU, but this still seems to occupy the whole resource of all visible GPUs, preventing another Tensorflow process to be launched over the idle GPU.\nIt would be great if there is a way to use only one GPU for a Tensorflow process but still enable DMA with the other GPUs sitting in the system. Am I missing such feature even though it exists?\nThank you!", "body": "Hi, I have a machine with multiple GPUs and deployed a distributed Tensorflow exploiting data parallelism. I use ClusterConfig to configure the cluster's topology and pass this to Experiment to run the distributed training. I wanted to make one of the GPUs as the parameter server and use the rest for the workers, each of which uses one GPU. I did this through launching multiple processes for parameter server and workers while setting up the env variable (CUDA_VISIBLE_DEVICES) to one of the GPUs. \r\n\r\nIn this case, I noticed that the GPUs don't communicate to each other via DMA and I guess this is because the DMA is only available between visible devices to Tensorflow.  \r\n\r\nI also tried \"device_count={\"GPU\": 1}\" to make Tensorflow see all the GPUs while using only one GPU, but this still seems to occupy the whole resource of all visible GPUs, preventing another Tensorflow process to be launched over the idle GPU. \r\n\r\nIt would be great if there is a way to use only one GPU for a Tensorflow process but still enable DMA with the other GPUs sitting in the system. Am I missing such feature even though it exists? \r\n\r\nThank you!"}