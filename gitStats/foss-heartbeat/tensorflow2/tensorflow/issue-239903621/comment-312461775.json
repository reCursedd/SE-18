{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312461775", "html_url": "https://github.com/tensorflow/tensorflow/issues/11196#issuecomment-312461775", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11196", "id": 312461775, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjQ2MTc3NQ==", "user": {"login": "jongsae", "id": 13041074, "node_id": "MDQ6VXNlcjEzMDQxMDc0", "avatar_url": "https://avatars0.githubusercontent.com/u/13041074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jongsae", "html_url": "https://github.com/jongsae", "followers_url": "https://api.github.com/users/jongsae/followers", "following_url": "https://api.github.com/users/jongsae/following{/other_user}", "gists_url": "https://api.github.com/users/jongsae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jongsae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jongsae/subscriptions", "organizations_url": "https://api.github.com/users/jongsae/orgs", "repos_url": "https://api.github.com/users/jongsae/repos", "events_url": "https://api.github.com/users/jongsae/events{/privacy}", "received_events_url": "https://api.github.com/users/jongsae/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-01T23:50:24Z", "updated_at": "2017-07-01T23:50:24Z", "author_association": "NONE", "body_html": "<p>Hi Bairen, first, thanks for your response!</p>\n<p>The reason why I wanted to deploy a distributed TF on a single node is that my application is not a very good fit for exploiting model parallelism. However, I have multi GPUs available in a machine so I would like to exploit data parallelism.</p>\n<p>I agree with you that having an intra process parameter server could be the solution for my case. However, the problem here is the fact that I would like to have a distributed version of the pre-existing implementation that uses Tensorflow's <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment\" rel=\"nofollow\">Experiment</a>. As far as I understand, the only way to parallelize the Experiment-based application for data parallelism is to use ClusterConfig, which requires to launch multiple processes.</p>\n<p>Re-developing the application from scratch to enable the intra process parameter server imposes a lot of engineering effort on me and I believe other developers could have similar use-cases like this if they don't develop TF application from scratch.</p>\n<p>Then your question could be why I need DMA. My application has a large size of model parameters that need to be communicated and the CPU parameter server requires the communication to go through PCIe, which is not very performant. Therefore, I would like to deploy my parameter server on one of the GPUs and launch the workers on the other GPUs and have them to communicate via DMA for performant communication. (I would be even happier if Tensorflow allows me to place a parameter server with a worker on a GPU since then I could minimize the waste of compute resource. I am not quite sure if this is possible in the current implementation.)</p>\n<p>Although I have found people who already use a single machine for distributed TF, I think my case will be more common as the number of GPUs per machine increases because it would be unlikely for people to always assign all GPUs available in a machine to a single model replica. Moreover, as the learning model gets larger (e.g., RNNs), the PCIe communication of CPU parameter server will eventually be impractical.</p>\n<p>I think the best way of handling this issue is that TF launches a single process for parameter servers and workers sitting in the same machine and only communicate through gRPC when there is a need for machine-to-machine communication. The current way of having distributed TF through multi processes with CUDA_VISIBLE_DEVICES effectively disables DMA, which limits the use case like mine.</p>\n<p>Please feel free to correct me if any of my understanding is wrong. Thank you!</p>", "body_text": "Hi Bairen, first, thanks for your response!\nThe reason why I wanted to deploy a distributed TF on a single node is that my application is not a very good fit for exploiting model parallelism. However, I have multi GPUs available in a machine so I would like to exploit data parallelism.\nI agree with you that having an intra process parameter server could be the solution for my case. However, the problem here is the fact that I would like to have a distributed version of the pre-existing implementation that uses Tensorflow's Experiment. As far as I understand, the only way to parallelize the Experiment-based application for data parallelism is to use ClusterConfig, which requires to launch multiple processes.\nRe-developing the application from scratch to enable the intra process parameter server imposes a lot of engineering effort on me and I believe other developers could have similar use-cases like this if they don't develop TF application from scratch.\nThen your question could be why I need DMA. My application has a large size of model parameters that need to be communicated and the CPU parameter server requires the communication to go through PCIe, which is not very performant. Therefore, I would like to deploy my parameter server on one of the GPUs and launch the workers on the other GPUs and have them to communicate via DMA for performant communication. (I would be even happier if Tensorflow allows me to place a parameter server with a worker on a GPU since then I could minimize the waste of compute resource. I am not quite sure if this is possible in the current implementation.)\nAlthough I have found people who already use a single machine for distributed TF, I think my case will be more common as the number of GPUs per machine increases because it would be unlikely for people to always assign all GPUs available in a machine to a single model replica. Moreover, as the learning model gets larger (e.g., RNNs), the PCIe communication of CPU parameter server will eventually be impractical.\nI think the best way of handling this issue is that TF launches a single process for parameter servers and workers sitting in the same machine and only communicate through gRPC when there is a need for machine-to-machine communication. The current way of having distributed TF through multi processes with CUDA_VISIBLE_DEVICES effectively disables DMA, which limits the use case like mine.\nPlease feel free to correct me if any of my understanding is wrong. Thank you!", "body": "Hi Bairen, first, thanks for your response! \r\n\r\nThe reason why I wanted to deploy a distributed TF on a single node is that my application is not a very good fit for exploiting model parallelism. However, I have multi GPUs available in a machine so I would like to exploit data parallelism. \r\n\r\nI agree with you that having an intra process parameter server could be the solution for my case. However, the problem here is the fact that I would like to have a distributed version of the pre-existing implementation that uses Tensorflow's [Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment). As far as I understand, the only way to parallelize the Experiment-based application for data parallelism is to use ClusterConfig, which requires to launch multiple processes. \r\n\r\nRe-developing the application from scratch to enable the intra process parameter server imposes a lot of engineering effort on me and I believe other developers could have similar use-cases like this if they don't develop TF application from scratch. \r\n\r\nThen your question could be why I need DMA. My application has a large size of model parameters that need to be communicated and the CPU parameter server requires the communication to go through PCIe, which is not very performant. Therefore, I would like to deploy my parameter server on one of the GPUs and launch the workers on the other GPUs and have them to communicate via DMA for performant communication. (I would be even happier if Tensorflow allows me to place a parameter server with a worker on a GPU since then I could minimize the waste of compute resource. I am not quite sure if this is possible in the current implementation.)\r\n\r\nAlthough I have found people who already use a single machine for distributed TF, I think my case will be more common as the number of GPUs per machine increases because it would be unlikely for people to always assign all GPUs available in a machine to a single model replica. Moreover, as the learning model gets larger (e.g., RNNs), the PCIe communication of CPU parameter server will eventually be impractical. \r\n\r\nI think the best way of handling this issue is that TF launches a single process for parameter servers and workers sitting in the same machine and only communicate through gRPC when there is a need for machine-to-machine communication. The current way of having distributed TF through multi processes with CUDA_VISIBLE_DEVICES effectively disables DMA, which limits the use case like mine.\r\n\r\nPlease feel free to correct me if any of my understanding is wrong. Thank you!"}