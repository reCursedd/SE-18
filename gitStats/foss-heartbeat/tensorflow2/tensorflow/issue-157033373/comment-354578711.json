{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354578711", "html_url": "https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-354578711", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2514", "id": 354578711, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDU3ODcxMQ==", "user": {"login": "markpwoodward", "id": 6820773, "node_id": "MDQ6VXNlcjY4MjA3NzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6820773?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markpwoodward", "html_url": "https://github.com/markpwoodward", "followers_url": "https://api.github.com/users/markpwoodward/followers", "following_url": "https://api.github.com/users/markpwoodward/following{/other_user}", "gists_url": "https://api.github.com/users/markpwoodward/gists{/gist_id}", "starred_url": "https://api.github.com/users/markpwoodward/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markpwoodward/subscriptions", "organizations_url": "https://api.github.com/users/markpwoodward/orgs", "repos_url": "https://api.github.com/users/markpwoodward/repos", "events_url": "https://api.github.com/users/markpwoodward/events{/privacy}", "received_events_url": "https://api.github.com/users/markpwoodward/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-31T01:05:45Z", "updated_at": "2017-12-31T01:05:45Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9762186\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rsethur\">@rsethur</a> Saving only the best checkpoint would be efficient, but I am not sure how to do that with this setup. Also, evaluation loss may not be the thing you want to optimize, you might want to review a number of evaluation metrics and pick the checkpoint that looks best in a general sense. I just keep all checkpoints, and visually inspect if I need to train more.</p>\n<p>As for setting the frequency that estimator.train() saves checkpoints, there are probably other ways to do this, but I do it in a RunConfig object passed to Estimator's constructor.</p>\n<pre><code>estimator = tf.estimator.Estimator(\n  model_fn=...,\n  config=tf.estimator.RunConfig().replace( # I'm not sure why I use replace here and not the constructor\n    save_checkpoints_steps=1000, # or whatever you want\n    keep_checkpoint_max=None, # defaults to last 5, but I keep all\n  ),\n  ...\n)\n</code></pre>\n<p>Also, take a look at the new Estimator.train_and_evaluate(), you may prefer it. I still prefer my proposed approach as I actually run evaluation on a fixed subset of my training data in addition to running evaluation on my validation set. I haven't been able to get train_and_evaluate() to support this (e.g. multiple EvalSpec's)</p>", "body_text": "@rsethur Saving only the best checkpoint would be efficient, but I am not sure how to do that with this setup. Also, evaluation loss may not be the thing you want to optimize, you might want to review a number of evaluation metrics and pick the checkpoint that looks best in a general sense. I just keep all checkpoints, and visually inspect if I need to train more.\nAs for setting the frequency that estimator.train() saves checkpoints, there are probably other ways to do this, but I do it in a RunConfig object passed to Estimator's constructor.\nestimator = tf.estimator.Estimator(\n  model_fn=...,\n  config=tf.estimator.RunConfig().replace( # I'm not sure why I use replace here and not the constructor\n    save_checkpoints_steps=1000, # or whatever you want\n    keep_checkpoint_max=None, # defaults to last 5, but I keep all\n  ),\n  ...\n)\n\nAlso, take a look at the new Estimator.train_and_evaluate(), you may prefer it. I still prefer my proposed approach as I actually run evaluation on a fixed subset of my training data in addition to running evaluation on my validation set. I haven't been able to get train_and_evaluate() to support this (e.g. multiple EvalSpec's)", "body": "@rsethur Saving only the best checkpoint would be efficient, but I am not sure how to do that with this setup. Also, evaluation loss may not be the thing you want to optimize, you might want to review a number of evaluation metrics and pick the checkpoint that looks best in a general sense. I just keep all checkpoints, and visually inspect if I need to train more.\r\n\r\nAs for setting the frequency that estimator.train() saves checkpoints, there are probably other ways to do this, but I do it in a RunConfig object passed to Estimator's constructor.\r\n\r\n```\r\nestimator = tf.estimator.Estimator(\r\n  model_fn=...,\r\n  config=tf.estimator.RunConfig().replace( # I'm not sure why I use replace here and not the constructor\r\n    save_checkpoints_steps=1000, # or whatever you want\r\n    keep_checkpoint_max=None, # defaults to last 5, but I keep all\r\n  ),\r\n  ...\r\n)\r\n```\r\n\r\nAlso, take a look at the new Estimator.train_and_evaluate(), you may prefer it. I still prefer my proposed approach as I actually run evaluation on a fixed subset of my training data in addition to running evaluation on my validation set. I haven't been able to get train_and_evaluate() to support this (e.g. multiple EvalSpec's)  "}