{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345012796", "html_url": "https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-345012796", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2514", "id": 345012796, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTAxMjc5Ng==", "user": {"login": "markpwoodward", "id": 6820773, "node_id": "MDQ6VXNlcjY4MjA3NzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6820773?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markpwoodward", "html_url": "https://github.com/markpwoodward", "followers_url": "https://api.github.com/users/markpwoodward/followers", "following_url": "https://api.github.com/users/markpwoodward/following{/other_user}", "gists_url": "https://api.github.com/users/markpwoodward/gists{/gist_id}", "starred_url": "https://api.github.com/users/markpwoodward/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markpwoodward/subscriptions", "organizations_url": "https://api.github.com/users/markpwoodward/orgs", "repos_url": "https://api.github.com/users/markpwoodward/repos", "events_url": "https://api.github.com/users/markpwoodward/events{/privacy}", "received_events_url": "https://api.github.com/users/markpwoodward/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-16T18:22:45Z", "updated_at": "2017-11-16T18:22:45Z", "author_association": "NONE", "body_html": "<p>I just closed this. Feel free to re-open it. In my opinion, <code>tf.estimator</code> and <code>tf.dataset</code> are the way to go, and solve my original issue.</p>\n<p>If you need to do things at the end of each epoch then just run <code>estimator.train()</code> with a <code>Dataset</code> that does not repeat. Alternate this with <code>estimator.evaluate()</code> or whatever you need.</p>\n<p>Often times the dataset is too big to wait for epoch boundaries, so use a <code>Dataset</code> that repeats and pass a listener to <code>estimator.train()</code> that runs <code>estimator.eval()</code> whenever a checkpoint is saved. See the link in my last comment above.</p>\n<p>I initially had a problem with this approach because of the perceived overhead of creating the graph on each call to <code>estimator.train()</code> or <code>estimator.evaluate()</code>. But, for me, the overhead has been negligible, and  having checkpoints be the transfer of information between <code>train()</code> and <code>evaluate()</code> feels like the right approach.</p>", "body_text": "I just closed this. Feel free to re-open it. In my opinion, tf.estimator and tf.dataset are the way to go, and solve my original issue.\nIf you need to do things at the end of each epoch then just run estimator.train() with a Dataset that does not repeat. Alternate this with estimator.evaluate() or whatever you need.\nOften times the dataset is too big to wait for epoch boundaries, so use a Dataset that repeats and pass a listener to estimator.train() that runs estimator.eval() whenever a checkpoint is saved. See the link in my last comment above.\nI initially had a problem with this approach because of the perceived overhead of creating the graph on each call to estimator.train() or estimator.evaluate(). But, for me, the overhead has been negligible, and  having checkpoints be the transfer of information between train() and evaluate() feels like the right approach.", "body": "I just closed this. Feel free to re-open it. In my opinion, `tf.estimator` and `tf.dataset` are the way to go, and solve my original issue.\r\n\r\nIf you need to do things at the end of each epoch then just run `estimator.train()` with a `Dataset` that does not repeat. Alternate this with `estimator.evaluate()` or whatever you need.\r\n\r\nOften times the dataset is too big to wait for epoch boundaries, so use a `Dataset` that repeats and pass a listener to `estimator.train()` that runs `estimator.eval()` whenever a checkpoint is saved. See the link in my last comment above.\r\n\r\nI initially had a problem with this approach because of the perceived overhead of creating the graph on each call to `estimator.train()` or `estimator.evaluate()`. But, for me, the overhead has been negligible, and  having checkpoints be the transfer of information between `train()` and `evaluate()` feels like the right approach."}