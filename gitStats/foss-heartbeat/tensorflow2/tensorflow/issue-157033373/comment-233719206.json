{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233719206", "html_url": "https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233719206", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2514", "id": 233719206, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzcxOTIwNg==", "user": {"login": "osdf", "id": 193341, "node_id": "MDQ6VXNlcjE5MzM0MQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/193341?v=4", "gravatar_id": "", "url": "https://api.github.com/users/osdf", "html_url": "https://github.com/osdf", "followers_url": "https://api.github.com/users/osdf/followers", "following_url": "https://api.github.com/users/osdf/following{/other_user}", "gists_url": "https://api.github.com/users/osdf/gists{/gist_id}", "starred_url": "https://api.github.com/users/osdf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/osdf/subscriptions", "organizations_url": "https://api.github.com/users/osdf/orgs", "repos_url": "https://api.github.com/users/osdf/repos", "events_url": "https://api.github.com/users/osdf/events{/privacy}", "received_events_url": "https://api.github.com/users/osdf/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-19T18:14:27Z", "updated_at": "2016-07-19T18:14:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6820773\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/markpwoodward\">@markpwoodward</a> hmm, i don't know what an <em>operation path</em> is. Say you call it twice, with two different input tensors (e.g. coming from a deque op on your training/validation set respectively). What is happening is that in the first call, the graph with the parameters is constructed. In the second call, this graph is simply reused. If you open up tensorboard, you see exactly this view, sort of a bottlenecked picture (if your shared graph is followed by some additional, loss-connected ops, which you need to have duplicated, that's true): At the bottom the two queues and their nodes, both feeding into the network expression that is made up of shared parameters, then going splitting up again to two loss-connected paths. With respect to messiness, I thought it the least messy solution, as I only had to add a for loop over the queues and kept the rest the same, make_template taking care of the rest. But messiness is probably quite subjective :-).</p>", "body_text": "@markpwoodward hmm, i don't know what an operation path is. Say you call it twice, with two different input tensors (e.g. coming from a deque op on your training/validation set respectively). What is happening is that in the first call, the graph with the parameters is constructed. In the second call, this graph is simply reused. If you open up tensorboard, you see exactly this view, sort of a bottlenecked picture (if your shared graph is followed by some additional, loss-connected ops, which you need to have duplicated, that's true): At the bottom the two queues and their nodes, both feeding into the network expression that is made up of shared parameters, then going splitting up again to two loss-connected paths. With respect to messiness, I thought it the least messy solution, as I only had to add a for loop over the queues and kept the rest the same, make_template taking care of the rest. But messiness is probably quite subjective :-).", "body": "@markpwoodward hmm, i don't know what an _operation path_ is. Say you call it twice, with two different input tensors (e.g. coming from a deque op on your training/validation set respectively). What is happening is that in the first call, the graph with the parameters is constructed. In the second call, this graph is simply reused. If you open up tensorboard, you see exactly this view, sort of a bottlenecked picture (if your shared graph is followed by some additional, loss-connected ops, which you need to have duplicated, that's true): At the bottom the two queues and their nodes, both feeding into the network expression that is made up of shared parameters, then going splitting up again to two loss-connected paths. With respect to messiness, I thought it the least messy solution, as I only had to add a for loop over the queues and kept the rest the same, make_template taking care of the rest. But messiness is probably quite subjective :-).\n"}