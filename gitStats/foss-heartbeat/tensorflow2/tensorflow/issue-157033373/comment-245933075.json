{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/245933075", "html_url": "https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-245933075", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2514", "id": 245933075, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NTkzMzA3NQ==", "user": {"login": "cancan101", "id": 51059, "node_id": "MDQ6VXNlcjUxMDU5", "avatar_url": "https://avatars1.githubusercontent.com/u/51059?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cancan101", "html_url": "https://github.com/cancan101", "followers_url": "https://api.github.com/users/cancan101/followers", "following_url": "https://api.github.com/users/cancan101/following{/other_user}", "gists_url": "https://api.github.com/users/cancan101/gists{/gist_id}", "starred_url": "https://api.github.com/users/cancan101/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cancan101/subscriptions", "organizations_url": "https://api.github.com/users/cancan101/orgs", "repos_url": "https://api.github.com/users/cancan101/repos", "events_url": "https://api.github.com/users/cancan101/events{/privacy}", "received_events_url": "https://api.github.com/users/cancan101/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-09T14:40:04Z", "updated_at": "2016-09-09T14:40:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The issues that I have with the two original proposals are:<br>\nThe two solutions that I have thought of using the existing code are:</p>\n<blockquote>\n<ol>\n<li>enqueue() some sentinel at the end of an epoch in the data producer and then tf.Assert() against the sentinel and catch the tf.errors.InvalidArgumentError in the data consumer.</li>\n</ol>\n</blockquote>\n<p>What to do with batches? ie the single rows are handled by a buffering batch creator</p>\n<blockquote>\n<ol start=\"2\">\n<li>know the number of enqueue's for the epoch and only dequeue that number.<br>\nBoth seem a little hacky.</li>\n</ol>\n</blockquote>\n<p>Again with batching, If the epoch size is not a multiple of the batch size, then part of the next epoch ends up on the last batch.</p>\n<p>Perhaps an alternative solution to the OP would be a queues that raises <code>OutOfRangeError</code> but stays open. That way the local variable for <code>epochs</code> can just be reset.</p>", "body_text": "The issues that I have with the two original proposals are:\nThe two solutions that I have thought of using the existing code are:\n\n\nenqueue() some sentinel at the end of an epoch in the data producer and then tf.Assert() against the sentinel and catch the tf.errors.InvalidArgumentError in the data consumer.\n\n\nWhat to do with batches? ie the single rows are handled by a buffering batch creator\n\n\nknow the number of enqueue's for the epoch and only dequeue that number.\nBoth seem a little hacky.\n\n\nAgain with batching, If the epoch size is not a multiple of the batch size, then part of the next epoch ends up on the last batch.\nPerhaps an alternative solution to the OP would be a queues that raises OutOfRangeError but stays open. That way the local variable for epochs can just be reset.", "body": "The issues that I have with the two original proposals are:\nThe two solutions that I have thought of using the existing code are:\n\n> 1) enqueue() some sentinel at the end of an epoch in the data producer and then tf.Assert() against the sentinel and catch the tf.errors.InvalidArgumentError in the data consumer.\n\nWhat to do with batches? ie the single rows are handled by a buffering batch creator\n\n> 2) know the number of enqueue's for the epoch and only dequeue that number.\n> Both seem a little hacky.\n\nAgain with batching, If the epoch size is not a multiple of the batch size, then part of the next epoch ends up on the last batch.\n\nPerhaps an alternative solution to the OP would be a queues that raises `OutOfRangeError` but stays open. That way the local variable for `epochs` can just be reset.\n"}