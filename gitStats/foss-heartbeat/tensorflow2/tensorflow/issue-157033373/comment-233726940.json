{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233726940", "html_url": "https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233726940", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2514", "id": 233726940, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzcyNjk0MA==", "user": {"login": "markpwoodward", "id": 6820773, "node_id": "MDQ6VXNlcjY4MjA3NzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6820773?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markpwoodward", "html_url": "https://github.com/markpwoodward", "followers_url": "https://api.github.com/users/markpwoodward/followers", "following_url": "https://api.github.com/users/markpwoodward/following{/other_user}", "gists_url": "https://api.github.com/users/markpwoodward/gists{/gist_id}", "starred_url": "https://api.github.com/users/markpwoodward/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markpwoodward/subscriptions", "organizations_url": "https://api.github.com/users/markpwoodward/orgs", "repos_url": "https://api.github.com/users/markpwoodward/repos", "events_url": "https://api.github.com/users/markpwoodward/events{/privacy}", "received_events_url": "https://api.github.com/users/markpwoodward/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-19T18:40:54Z", "updated_at": "2016-07-19T18:40:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=193341\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/osdf\">@osdf</a>, thank you for the response. Would you mind include the picture from<br>\ntensorboard? I certainly may be missing something about how tensorflow<br>\nexecution happens. Also, maybe brief pseudo code for your usage of<br>\nmake_template(); where the goal of the pseudo code is just inference (no<br>\nloss or training), but on two different queues.</p>\n<p>On Tue, Jul 19, 2016 at 11:16 AM, Christian <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6820773\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/markpwoodward\">@markpwoodward</a> <a href=\"https://github.com/markpwoodward\">https://github.com/markpwoodward</a> hmm, i don't know what<br>\nan <em>operation path</em> is. Say you call it twice, with two different input<br>\ntensors (e.g. coming from a deque op on your training/validation set<br>\nrespectively). What is happening is that in the first call, the graph with<br>\nthe parameters is constructed. In the second call, this graph is simply<br>\nreused. If you open up tensorboard, you see exactly this view, sort of a<br>\nbottlenecked picture (if your shared graph is followed by some additional,<br>\nloss-connected ops, which you need to have duplicated, that's true): At the<br>\nbottom the two queues and their nodes, both feeding into the network<br>\nexpression that is made up of shared parameters, then going splitting up<br>\nagain to two loss-connected paths. With respect to messiness, I thought it<br>\nthe least messy solution, as I only had to add a for loop over the queues<br>\nand kept the rest the same, make_template taking care of the rest. But<br>\nmessiness is probably quite subjective :-).</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"157033373\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2514\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2514/hovercard?comment_id=233719206&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233719206\">#2514 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/AGgTpRXQZW9sXwpodVkpt3Joe28JgAnrks5qXRRygaJpZM4Inwby\">https://github.com/notifications/unsubscribe-auth/AGgTpRXQZW9sXwpodVkpt3Joe28JgAnrks5qXRRygaJpZM4Inwby</a><br>\n.</p>\n</blockquote>", "body_text": "@osdf, thank you for the response. Would you mind include the picture from\ntensorboard? I certainly may be missing something about how tensorflow\nexecution happens. Also, maybe brief pseudo code for your usage of\nmake_template(); where the goal of the pseudo code is just inference (no\nloss or training), but on two different queues.\nOn Tue, Jul 19, 2016 at 11:16 AM, Christian notifications@github.com\nwrote:\n\n@markpwoodward https://github.com/markpwoodward hmm, i don't know what\nan operation path is. Say you call it twice, with two different input\ntensors (e.g. coming from a deque op on your training/validation set\nrespectively). What is happening is that in the first call, the graph with\nthe parameters is constructed. In the second call, this graph is simply\nreused. If you open up tensorboard, you see exactly this view, sort of a\nbottlenecked picture (if your shared graph is followed by some additional,\nloss-connected ops, which you need to have duplicated, that's true): At the\nbottom the two queues and their nodes, both feeding into the network\nexpression that is made up of shared parameters, then going splitting up\nagain to two loss-connected paths. With respect to messiness, I thought it\nthe least messy solution, as I only had to add a for loop over the queues\nand kept the rest the same, make_template taking care of the rest. But\nmessiness is probably quite subjective :-).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#2514 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGgTpRXQZW9sXwpodVkpt3Joe28JgAnrks5qXRRygaJpZM4Inwby\n.", "body": "@osdf, thank you for the response. Would you mind include the picture from\ntensorboard? I certainly may be missing something about how tensorflow\nexecution happens. Also, maybe brief pseudo code for your usage of\nmake_template(); where the goal of the pseudo code is just inference (no\nloss or training), but on two different queues.\n\nOn Tue, Jul 19, 2016 at 11:16 AM, Christian notifications@github.com\nwrote:\n\n> @markpwoodward https://github.com/markpwoodward hmm, i don't know what\n> an _operation path_ is. Say you call it twice, with two different input\n> tensors (e.g. coming from a deque op on your training/validation set\n> respectively). What is happening is that in the first call, the graph with\n> the parameters is constructed. In the second call, this graph is simply\n> reused. If you open up tensorboard, you see exactly this view, sort of a\n> bottlenecked picture (if your shared graph is followed by some additional,\n> loss-connected ops, which you need to have duplicated, that's true): At the\n> bottom the two queues and their nodes, both feeding into the network\n> expression that is made up of shared parameters, then going splitting up\n> again to two loss-connected paths. With respect to messiness, I thought it\n> the least messy solution, as I only had to add a for loop over the queues\n> and kept the rest the same, make_template taking care of the rest. But\n> messiness is probably quite subjective :-).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233719206,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGgTpRXQZW9sXwpodVkpt3Joe28JgAnrks5qXRRygaJpZM4Inwby\n> .\n"}