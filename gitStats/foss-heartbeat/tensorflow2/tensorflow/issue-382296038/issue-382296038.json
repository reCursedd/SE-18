{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23856", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23856/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23856/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23856/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23856", "id": 382296038, "node_id": "MDU6SXNzdWUzODIyOTYwMzg=", "number": 23856, "title": "GPU there still training on CPU", "user": {"login": "yuvrajkhanna", "id": 41501579, "node_id": "MDQ6VXNlcjQxNTAxNTc5", "avatar_url": "https://avatars3.githubusercontent.com/u/41501579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuvrajkhanna", "html_url": "https://github.com/yuvrajkhanna", "followers_url": "https://api.github.com/users/yuvrajkhanna/followers", "following_url": "https://api.github.com/users/yuvrajkhanna/following{/other_user}", "gists_url": "https://api.github.com/users/yuvrajkhanna/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuvrajkhanna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuvrajkhanna/subscriptions", "organizations_url": "https://api.github.com/users/yuvrajkhanna/orgs", "repos_url": "https://api.github.com/users/yuvrajkhanna/repos", "events_url": "https://api.github.com/users/yuvrajkhanna/events{/privacy}", "received_events_url": "https://api.github.com/users/yuvrajkhanna/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-19T16:31:59Z", "updated_at": "2018-11-19T16:31:59Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>./deviceQuery Starting...</p>\n<p>CUDA Device Query (Runtime API) version (CUDART static linking)</p>\n<p>Detected 1 CUDA Capable device(s)</p>\n<p>Device 0: \"GeForce 940MX\"<br>\nCUDA Driver Version / Runtime Version          9.0 / 9.0<br>\nCUDA Capability Major/Minor version number:    5.0<br>\nTotal amount of global memory:                 2003 MBytes (2100232192 bytes)<br>\n( 3) Multiprocessors, (128) CUDA Cores/MP:     384 CUDA Cores<br>\nGPU Max Clock rate:                            1242 MHz (1.24 GHz)<br>\nMemory Clock rate:                             1001 Mhz<br>\nMemory Bus Width:                              64-bit<br>\nL2 Cache Size:                                 1048576 bytes<br>\nMaximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)<br>\nMaximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers<br>\nMaximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers<br>\nTotal amount of constant memory:               65536 bytes<br>\nTotal amount of shared memory per block:       49152 bytes<br>\nTotal number of registers available per block: 65536<br>\nWarp size:                                     32<br>\nMaximum number of threads per multiprocessor:  2048<br>\nMaximum number of threads per block:           1024<br>\nMax dimension size of a thread block (x,y,z): (1024, 1024, 64)<br>\nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)<br>\nMaximum memory pitch:                          2147483647 bytes<br>\nTexture alignment:                             512 bytes<br>\nConcurrent copy and kernel execution:          Yes with 1 copy engine(s)<br>\nRun time limit on kernels:                     Yes<br>\nIntegrated GPU sharing Host Memory:            No<br>\nSupport host page-locked memory mapping:       Yes<br>\nAlignment requirement for Surfaces:            Yes<br>\nDevice has ECC support:                        Disabled<br>\nDevice supports Unified Addressing (UVA):      Yes<br>\nSupports Cooperative Kernel Launch:            No<br>\nSupports MultiDevice Co-op Kernel Launch:      No<br>\nDevice PCI Domain ID / Bus ID / location ID:   0 / 1 / 0<br>\nCompute Mode:<br>\n&lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</p>\n<p>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1<br>\nResult = PASS</p>\n<p><strong>this shows my cuda is ok</strong></p>\n<p>cudnnGetVersion() : 7401 , CUDNN_VERSION from cudnn.h : 7401 (7.4.1)<br>\nHost compiler version : GCC 5.4.0<br>\nThere are 1 CUDA capable devices on your machine :<br>\ndevice 0 : sms  3  Capabilities 5.0, SmClock 1241.5 Mhz, MemSize (Mb) 2002, MemClock 1001.0 Mhz, Ecc=0, boardGroupID=0<br>\nUsing device 0</p>\n<p>Testing single precision<br>\nLoading image data/one_28x28.pgm<br>\nPerforming forward propagation ...<br>\nTesting cudnnGetConvolutionForwardAlgorithm ...<br>\nFastest algorithm is Algo 1<br>\nTesting cudnnFindConvolutionForwardAlgorithm ...<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028768 time requiring 0 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.044288 time requiring 3464 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054624 time requiring 57600 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.197760 time requiring 207360 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.501696 time requiring 2057744 memory<br>\nResulting weights from Softmax:<br>\n0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000<br>\nLoading image data/three_28x28.pgm<br>\nPerforming forward propagation ...<br>\nResulting weights from Softmax:<br>\n0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000<br>\nLoading image data/five_28x28.pgm<br>\nPerforming forward propagation ...<br>\nResulting weights from Softmax:<br>\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006</p>\n<p>Result of classification: 1 3 5</p>\n<p>Test passed!</p>\n<p>Testing half precision (math in single precision)<br>\nLoading image data/one_28x28.pgm<br>\nPerforming forward propagation ...<br>\nTesting cudnnGetConvolutionForwardAlgorithm ...<br>\nFastest algorithm is Algo 1<br>\nTesting cudnnFindConvolutionForwardAlgorithm ...<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028288 time requiring 0 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.038432 time requiring 3464 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.057536 time requiring 28800 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.199968 time requiring 207360 memory<br>\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.498144 time requiring 2057744 memory<br>\nResulting weights from Softmax:<br>\n0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001<br>\nLoading image data/three_28x28.pgm<br>\nPerforming forward propagation ...<br>\nResulting weights from Softmax:<br>\n0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000<br>\nLoading image data/five_28x28.pgm<br>\nPerforming forward propagation ...<br>\nResulting weights from Softmax:<br>\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006</p>\n<p>Result of classification: 1 3 5</p>\n<p>Test passed!<br>\n<strong>this shows my cudnn is ok</strong></p>\n<p>[0] GeForce 940MX    | 51'C,  92 % |  1904 /  2002 MB | yuvraj(1699M) root(94M) yuvraj(55M) yuvraj(43M)<br>\nthis is command gpustat during training</p>\n<p>nvidia-smi<br>\nMon Nov 19 21:54:09 2018<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  GeForce 940MX       Off  | 00000000:01:00.0 Off |                  N/A |<br>\n| N/A   61C    P0    N/A /  N/A |   1927MiB /  2002MiB |     91%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID   Type   Process name                             Usage      |<br>\n|=============================================================================|<br>\n|    0       942      G   /usr/lib/xorg/Xorg                           103MiB |<br>\n|    0      1749      G   compiz                                        57MiB |<br>\n|    0      2278      G   ...uest-channel-token=13477521418036853922    45MiB |<br>\n|    0      3726      C   /home/yuvraj/anaconda3/envs/tf/bin/python   1699MiB |<br>\n|    0      3873      C   /usr/lib/libreoffice/program/soffice.bin      17MiB |<br>\n+-----------------------------------------------------------------------------+</p>\n<p>this is nvidia-smi during training as we can see the load is 90% but training is on CPU as CPU load is also 90% and the training time was similar to that of cpu like before i installed GPU version i had CPU version and the training time was similar i was doing it on mnist dataset with batch size 32 and it was taking 40-45sec per epoch so idk where i have done wrong please help !!!</p>", "body_text": "./deviceQuery Starting...\nCUDA Device Query (Runtime API) version (CUDART static linking)\nDetected 1 CUDA Capable device(s)\nDevice 0: \"GeForce 940MX\"\nCUDA Driver Version / Runtime Version          9.0 / 9.0\nCUDA Capability Major/Minor version number:    5.0\nTotal amount of global memory:                 2003 MBytes (2100232192 bytes)\n( 3) Multiprocessors, (128) CUDA Cores/MP:     384 CUDA Cores\nGPU Max Clock rate:                            1242 MHz (1.24 GHz)\nMemory Clock rate:                             1001 Mhz\nMemory Bus Width:                              64-bit\nL2 Cache Size:                                 1048576 bytes\nMaximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\nMaximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\nMaximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\nTotal amount of constant memory:               65536 bytes\nTotal amount of shared memory per block:       49152 bytes\nTotal number of registers available per block: 65536\nWarp size:                                     32\nMaximum number of threads per multiprocessor:  2048\nMaximum number of threads per block:           1024\nMax dimension size of a thread block (x,y,z): (1024, 1024, 64)\nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\nMaximum memory pitch:                          2147483647 bytes\nTexture alignment:                             512 bytes\nConcurrent copy and kernel execution:          Yes with 1 copy engine(s)\nRun time limit on kernels:                     Yes\nIntegrated GPU sharing Host Memory:            No\nSupport host page-locked memory mapping:       Yes\nAlignment requirement for Surfaces:            Yes\nDevice has ECC support:                        Disabled\nDevice supports Unified Addressing (UVA):      Yes\nSupports Cooperative Kernel Launch:            No\nSupports MultiDevice Co-op Kernel Launch:      No\nDevice PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\nCompute Mode:\n< Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1\nResult = PASS\nthis shows my cuda is ok\ncudnnGetVersion() : 7401 , CUDNN_VERSION from cudnn.h : 7401 (7.4.1)\nHost compiler version : GCC 5.4.0\nThere are 1 CUDA capable devices on your machine :\ndevice 0 : sms  3  Capabilities 5.0, SmClock 1241.5 Mhz, MemSize (Mb) 2002, MemClock 1001.0 Mhz, Ecc=0, boardGroupID=0\nUsing device 0\nTesting single precision\nLoading image data/one_28x28.pgm\nPerforming forward propagation ...\nTesting cudnnGetConvolutionForwardAlgorithm ...\nFastest algorithm is Algo 1\nTesting cudnnFindConvolutionForwardAlgorithm ...\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028768 time requiring 0 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.044288 time requiring 3464 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054624 time requiring 57600 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.197760 time requiring 207360 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.501696 time requiring 2057744 memory\nResulting weights from Softmax:\n0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000\nLoading image data/three_28x28.pgm\nPerforming forward propagation ...\nResulting weights from Softmax:\n0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000\nLoading image data/five_28x28.pgm\nPerforming forward propagation ...\nResulting weights from Softmax:\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006\nResult of classification: 1 3 5\nTest passed!\nTesting half precision (math in single precision)\nLoading image data/one_28x28.pgm\nPerforming forward propagation ...\nTesting cudnnGetConvolutionForwardAlgorithm ...\nFastest algorithm is Algo 1\nTesting cudnnFindConvolutionForwardAlgorithm ...\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028288 time requiring 0 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.038432 time requiring 3464 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.057536 time requiring 28800 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.199968 time requiring 207360 memory\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.498144 time requiring 2057744 memory\nResulting weights from Softmax:\n0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001\nLoading image data/three_28x28.pgm\nPerforming forward propagation ...\nResulting weights from Softmax:\n0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000\nLoading image data/five_28x28.pgm\nPerforming forward propagation ...\nResulting weights from Softmax:\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006\nResult of classification: 1 3 5\nTest passed!\nthis shows my cudnn is ok\n[0] GeForce 940MX    | 51'C,  92 % |  1904 /  2002 MB | yuvraj(1699M) root(94M) yuvraj(55M) yuvraj(43M)\nthis is command gpustat during training\nnvidia-smi\nMon Nov 19 21:54:09 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce 940MX       Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   61C    P0    N/A /  N/A |   1927MiB /  2002MiB |     91%      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0       942      G   /usr/lib/xorg/Xorg                           103MiB |\n|    0      1749      G   compiz                                        57MiB |\n|    0      2278      G   ...uest-channel-token=13477521418036853922    45MiB |\n|    0      3726      C   /home/yuvraj/anaconda3/envs/tf/bin/python   1699MiB |\n|    0      3873      C   /usr/lib/libreoffice/program/soffice.bin      17MiB |\n+-----------------------------------------------------------------------------+\nthis is nvidia-smi during training as we can see the load is 90% but training is on CPU as CPU load is also 90% and the training time was similar to that of cpu like before i installed GPU version i had CPU version and the training time was similar i was doing it on mnist dataset with batch size 32 and it was taking 40-45sec per epoch so idk where i have done wrong please help !!!", "body": "./deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce 940MX\"\r\n  CUDA Driver Version / Runtime Version          9.0 / 9.0\r\n  CUDA Capability Major/Minor version number:    5.0\r\n  Total amount of global memory:                 2003 MBytes (2100232192 bytes)\r\n  ( 3) Multiprocessors, (128) CUDA Cores/MP:     384 CUDA Cores\r\n  GPU Max Clock rate:                            1242 MHz (1.24 GHz)\r\n  Memory Clock rate:                             1001 Mhz\r\n  Memory Bus Width:                              64-bit\r\n  L2 Cache Size:                                 1048576 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Supports Cooperative Kernel Launch:            No\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1\r\nResult = PASS\r\n\r\n**this shows my cuda is ok**\r\n\r\ncudnnGetVersion() : 7401 , CUDNN_VERSION from cudnn.h : 7401 (7.4.1)\r\nHost compiler version : GCC 5.4.0\r\nThere are 1 CUDA capable devices on your machine :\r\ndevice 0 : sms  3  Capabilities 5.0, SmClock 1241.5 Mhz, MemSize (Mb) 2002, MemClock 1001.0 Mhz, Ecc=0, boardGroupID=0\r\nUsing device 0\r\n\r\nTesting single precision\r\nLoading image data/one_28x28.pgm\r\nPerforming forward propagation ...\r\nTesting cudnnGetConvolutionForwardAlgorithm ...\r\nFastest algorithm is Algo 1\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028768 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.044288 time requiring 3464 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054624 time requiring 57600 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.197760 time requiring 207360 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.501696 time requiring 2057744 memory\r\nResulting weights from Softmax:\r\n0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 \r\nLoading image data/three_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 \r\nLoading image data/five_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 \r\n\r\nResult of classification: 1 3 5\r\n\r\nTest passed!\r\n\r\nTesting half precision (math in single precision)\r\nLoading image data/one_28x28.pgm\r\nPerforming forward propagation ...\r\nTesting cudnnGetConvolutionForwardAlgorithm ...\r\nFastest algorithm is Algo 1\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.028288 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.038432 time requiring 3464 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.057536 time requiring 28800 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.199968 time requiring 207360 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.498144 time requiring 2057744 memory\r\nResulting weights from Softmax:\r\n0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001 \r\nLoading image data/three_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000 \r\nLoading image data/five_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 \r\n\r\nResult of classification: 1 3 5\r\n\r\nTest passed!\r\n **this shows my cudnn is ok**\r\n\r\n\r\n[0] GeForce 940MX    | 51'C,  92 % |  1904 /  2002 MB | yuvraj(1699M) root(94M) yuvraj(55M) yuvraj(43M)\r\n this is command gpustat during training \r\n\r\n nvidia-smi\r\nMon Nov 19 21:54:09 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce 940MX       Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   61C    P0    N/A /  N/A |   1927MiB /  2002MiB |     91%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       942      G   /usr/lib/xorg/Xorg                           103MiB |\r\n|    0      1749      G   compiz                                        57MiB |\r\n|    0      2278      G   ...uest-channel-token=13477521418036853922    45MiB |\r\n|    0      3726      C   /home/yuvraj/anaconda3/envs/tf/bin/python   1699MiB |\r\n|    0      3873      C   /usr/lib/libreoffice/program/soffice.bin      17MiB |\r\n+-----------------------------------------------------------------------------+ \r\n\r\nthis is nvidia-smi during training as we can see the load is 90% but training is on CPU as CPU load is also 90% and the training time was similar to that of cpu like before i installed GPU version i had CPU version and the training time was similar i was doing it on mnist dataset with batch size 32 and it was taking 40-45sec per epoch so idk where i have done wrong please help !!!\r\n\r\n\r\n\r\n\r\n\r\n"}