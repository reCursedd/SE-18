{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15685", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15685/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15685/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15685/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15685", "id": 284868116, "node_id": "MDU6SXNzdWUyODQ4NjgxMTY=", "number": 15685, "title": "fake_quant_with_min_max_vars doesn't change min, max vars ", "user": {"login": "agoncharenko1992", "id": 34908246, "node_id": "MDQ6VXNlcjM0OTA4MjQ2", "avatar_url": "https://avatars2.githubusercontent.com/u/34908246?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agoncharenko1992", "html_url": "https://github.com/agoncharenko1992", "followers_url": "https://api.github.com/users/agoncharenko1992/followers", "following_url": "https://api.github.com/users/agoncharenko1992/following{/other_user}", "gists_url": "https://api.github.com/users/agoncharenko1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/agoncharenko1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agoncharenko1992/subscriptions", "organizations_url": "https://api.github.com/users/agoncharenko1992/orgs", "repos_url": "https://api.github.com/users/agoncharenko1992/repos", "events_url": "https://api.github.com/users/agoncharenko1992/events{/privacy}", "received_events_url": "https://api.github.com/users/agoncharenko1992/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-28T08:39:27Z", "updated_at": "2018-01-24T04:46:01Z", "closed_at": "2018-01-24T04:46:01Z", "author_association": "NONE", "body_html": "<h1>System Information</h1>\n<ul>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04</li>\n<li>TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow-gpu</li>\n<li>TensorFlow version (use command below): v1.4.0-19-ga52c8d9, 1.4.1</li>\n<li>Python version:3.5.2</li>\n<li>Bazel version (if compiling from source): 0.7.0</li>\n<li>GCC/Compiler version (if compiling from source): GCC 5.4.0</li>\n<li>CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61, cuDNN : 6.0.21</li>\n<li>GPU model and memory: Two GeForce GTX 1080 Ti devices.</li>\n<li>Exact command to reproduce: N/A</li>\n</ul>\n<h1>Problem description</h1>\n<p>I've got a problem with tf-lite conversion tool. There is learned graph which should be converted in tflite format and quantinized.<br>\nI have read the answer <a href=\"https://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef\" rel=\"nofollow\">https://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef</a> where authors recommend to create new network with fake_quant operations and retrain it. However variables passed in tf.fake_quant_with_min_max_vars op did not change their values during training. Here is modeling code which shows the problem.</p>\n<h1>Code to reproduce</h1>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nkhe_init = tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(2.0 / 1000))\n\n\ndef quant_conv_op(inpt, num_filters, filter_size=[3, 3], strides=[1, 1, 1, 1], padding=\"VALID\", layer_name=\"layer1\", use_acivation=True):\n    num_input_map = inpt.get_shape().as_list()[-1]\n    kernel_shape = filter_size + [num_input_map, num_filters]    \n    with tf.variable_scope(layer_name):\n        W = tf.get_variable(\"weights\", shape=kernel_shape, initializer=khe_init)\n        max_w = tf.get_variable(\"max_quant_weights\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\n        min_w = tf.get_variable(\"min_quant_weights\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)\n        b = tf.get_variable(\"bias\", shape=[num_filters, ], initializer=tf.zeros_initializer)\n \n        q_W = tf.fake_quant_with_min_max_vars(W, min_w, max_w)\n        out = tf.nn.conv2d(inpt, q_W, strides=strides, padding=padding, name=\"2d_convolution_operation\")\n        out = tf.nn.bias_add(out, b)\n        \n        if use_acivation: \n            out = tf.nn.relu6(out, name=layer_name+\"out\")\n            out = tf.fake_quant_with_min_max_args(out, 0, 6)\n        else:\n            max_out = tf.get_variable(\"max_quant_output\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\n            min_out = tf.get_variable(\"min_quant_output\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)  \n            out = tf.fake_quant_with_min_max_vars(out, min_out, max_out, name=\"fake_quant_with_min_max_out_quantinization\")\n    \n    return out, max_w, min_w, W\n\n\ndef loss(logits, batch):\n    return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.ones(shape=[batch, 1]))\n\n\ndef build_net(ph_input):\n    inp = tf.fake_quant_with_min_max_args(ph_input, -1, 1)\n    out, max_w1, min_w1, W1 = quant_conv_op(inp, num_filters=64, filter_size=[3, 3], layer_name=\"layer1\")    \n    out, max_w2, min_w2, W2 = quant_conv_op(out, num_filters=128, filter_size=[3, 3], layer_name=\"layer2\")\n    out, max_w3, min_w3, W3 = quant_conv_op(out, num_filters=256, filter_size=[3, 3], layer_name=\"layer3\")\n    out = tf.reduce_mean(out, axis=[1, 2], keep_dims=True, name=\"avg_pool\")\n    out = tf.fake_quant_with_min_max_args(out, 0, 6, name=\"fake_quant_with_min_max_avgpool_quantinization\")\n    logits, max_w4, min_w4, W4 = quant_conv_op(out, num_filters=1, filter_size=[1, 1], layer_name=\"layer4\", use_acivation=False)\n    \n    max_list = [max_w1, max_w2, max_w3, max_w4]\n    min_list = [min_w1, min_w2, min_w3, min_w4]\n    W_list = [W1, W2, W3, W4]\n    logits = tf.reshape(logits, [-1, 1])\n    sig_loss = tf.reduce_mean(loss(logits, batch=64))\n    adam_op = tf.train.AdamOptimizer(10**-0)\n    train_op = adam_op.minimize(sig_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) \n    return train_op, max_list, min_list, sig_loss, W_list\n\n\ndef main():\n    sess = tf.InteractiveSession()\n    ph_input = tf.placeholder(tf.float32, [None, 28, 28, 3], name=\"network_input\") \n    train_op, max_list, min_list, sig_loss, W_list = build_net(ph_input)\n    sess.run(tf.global_variables_initializer())\n    tf.summary.FileWriter('.', graph=tf.get_default_graph())\n    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    for t in trainable_vars:\n        if len(t.shape) == 0:\n           print(t.name, sess.run(t))\n        if len(t.shape) == 4:\n           v = sess.run(t)\n           print(t.name, np.max(v), np.min(v))\n\n    for i in range(10):\n        res = sess.run([train_op, sig_loss, W_list[0]]+max_list, feed_dict={ph_input:np.random.uniform(low=-1, high=1, size=[64, 28, 28, 3])})\n    \n    print(\"=========\")\n    for t in trainable_vars:\n        if len(t.shape) == 0:\n           print(t.name, sess.run(t))\n        if len(t.shape) == 4:\n           v = sess.run(t)\n           print(t.name, np.max(v), np.min(v))\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>", "body_text": "System Information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): pip3 install --upgrade tensorflow-gpu\nTensorFlow version (use command below): v1.4.0-19-ga52c8d9, 1.4.1\nPython version:3.5.2\nBazel version (if compiling from source): 0.7.0\nGCC/Compiler version (if compiling from source): GCC 5.4.0\nCUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61, cuDNN : 6.0.21\nGPU model and memory: Two GeForce GTX 1080 Ti devices.\nExact command to reproduce: N/A\n\nProblem description\nI've got a problem with tf-lite conversion tool. There is learned graph which should be converted in tflite format and quantinized.\nI have read the answer https://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef where authors recommend to create new network with fake_quant operations and retrain it. However variables passed in tf.fake_quant_with_min_max_vars op did not change their values during training. Here is modeling code which shows the problem.\nCode to reproduce\nimport tensorflow as tf\nimport numpy as np\n\nkhe_init = tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(2.0 / 1000))\n\n\ndef quant_conv_op(inpt, num_filters, filter_size=[3, 3], strides=[1, 1, 1, 1], padding=\"VALID\", layer_name=\"layer1\", use_acivation=True):\n    num_input_map = inpt.get_shape().as_list()[-1]\n    kernel_shape = filter_size + [num_input_map, num_filters]    \n    with tf.variable_scope(layer_name):\n        W = tf.get_variable(\"weights\", shape=kernel_shape, initializer=khe_init)\n        max_w = tf.get_variable(\"max_quant_weights\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\n        min_w = tf.get_variable(\"min_quant_weights\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)\n        b = tf.get_variable(\"bias\", shape=[num_filters, ], initializer=tf.zeros_initializer)\n \n        q_W = tf.fake_quant_with_min_max_vars(W, min_w, max_w)\n        out = tf.nn.conv2d(inpt, q_W, strides=strides, padding=padding, name=\"2d_convolution_operation\")\n        out = tf.nn.bias_add(out, b)\n        \n        if use_acivation: \n            out = tf.nn.relu6(out, name=layer_name+\"out\")\n            out = tf.fake_quant_with_min_max_args(out, 0, 6)\n        else:\n            max_out = tf.get_variable(\"max_quant_output\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\n            min_out = tf.get_variable(\"min_quant_output\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)  \n            out = tf.fake_quant_with_min_max_vars(out, min_out, max_out, name=\"fake_quant_with_min_max_out_quantinization\")\n    \n    return out, max_w, min_w, W\n\n\ndef loss(logits, batch):\n    return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.ones(shape=[batch, 1]))\n\n\ndef build_net(ph_input):\n    inp = tf.fake_quant_with_min_max_args(ph_input, -1, 1)\n    out, max_w1, min_w1, W1 = quant_conv_op(inp, num_filters=64, filter_size=[3, 3], layer_name=\"layer1\")    \n    out, max_w2, min_w2, W2 = quant_conv_op(out, num_filters=128, filter_size=[3, 3], layer_name=\"layer2\")\n    out, max_w3, min_w3, W3 = quant_conv_op(out, num_filters=256, filter_size=[3, 3], layer_name=\"layer3\")\n    out = tf.reduce_mean(out, axis=[1, 2], keep_dims=True, name=\"avg_pool\")\n    out = tf.fake_quant_with_min_max_args(out, 0, 6, name=\"fake_quant_with_min_max_avgpool_quantinization\")\n    logits, max_w4, min_w4, W4 = quant_conv_op(out, num_filters=1, filter_size=[1, 1], layer_name=\"layer4\", use_acivation=False)\n    \n    max_list = [max_w1, max_w2, max_w3, max_w4]\n    min_list = [min_w1, min_w2, min_w3, min_w4]\n    W_list = [W1, W2, W3, W4]\n    logits = tf.reshape(logits, [-1, 1])\n    sig_loss = tf.reduce_mean(loss(logits, batch=64))\n    adam_op = tf.train.AdamOptimizer(10**-0)\n    train_op = adam_op.minimize(sig_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) \n    return train_op, max_list, min_list, sig_loss, W_list\n\n\ndef main():\n    sess = tf.InteractiveSession()\n    ph_input = tf.placeholder(tf.float32, [None, 28, 28, 3], name=\"network_input\") \n    train_op, max_list, min_list, sig_loss, W_list = build_net(ph_input)\n    sess.run(tf.global_variables_initializer())\n    tf.summary.FileWriter('.', graph=tf.get_default_graph())\n    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    for t in trainable_vars:\n        if len(t.shape) == 0:\n           print(t.name, sess.run(t))\n        if len(t.shape) == 4:\n           v = sess.run(t)\n           print(t.name, np.max(v), np.min(v))\n\n    for i in range(10):\n        res = sess.run([train_op, sig_loss, W_list[0]]+max_list, feed_dict={ph_input:np.random.uniform(low=-1, high=1, size=[64, 28, 28, 3])})\n    \n    print(\"=========\")\n    for t in trainable_vars:\n        if len(t.shape) == 0:\n           print(t.name, sess.run(t))\n        if len(t.shape) == 4:\n           v = sess.run(t)\n           print(t.name, np.max(v), np.min(v))\n\nif __name__ == \"__main__\":\n    main()", "body": "# System Information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow-gpu\r\n- TensorFlow version (use command below): v1.4.0-19-ga52c8d9, 1.4.1\r\n- Python version:3.5.2 \r\n- Bazel version (if compiling from source): 0.7.0\r\n- GCC/Compiler version (if compiling from source): GCC 5.4.0\r\n- CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61, cuDNN : 6.0.21\r\n- GPU model and memory: Two GeForce GTX 1080 Ti devices.\r\n- Exact command to reproduce: N/A\r\n# Problem description\r\nI've got a problem with tf-lite conversion tool. There is learned graph which should be converted in tflite format and quantinized.\r\nI have read the answer https://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef where authors recommend to create new network with fake_quant operations and retrain it. However variables passed in tf.fake_quant_with_min_max_vars op did not change their values during training. Here is modeling code which shows the problem.\r\n# Code to reproduce\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nkhe_init = tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(2.0 / 1000))\r\n\r\n\r\ndef quant_conv_op(inpt, num_filters, filter_size=[3, 3], strides=[1, 1, 1, 1], padding=\"VALID\", layer_name=\"layer1\", use_acivation=True):\r\n    num_input_map = inpt.get_shape().as_list()[-1]\r\n    kernel_shape = filter_size + [num_input_map, num_filters]    \r\n    with tf.variable_scope(layer_name):\r\n        W = tf.get_variable(\"weights\", shape=kernel_shape, initializer=khe_init)\r\n        max_w = tf.get_variable(\"max_quant_weights\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\r\n        min_w = tf.get_variable(\"min_quant_weights\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)\r\n        b = tf.get_variable(\"bias\", shape=[num_filters, ], initializer=tf.zeros_initializer)\r\n \r\n        q_W = tf.fake_quant_with_min_max_vars(W, min_w, max_w)\r\n        out = tf.nn.conv2d(inpt, q_W, strides=strides, padding=padding, name=\"2d_convolution_operation\")\r\n        out = tf.nn.bias_add(out, b)\r\n        \r\n        if use_acivation: \r\n            out = tf.nn.relu6(out, name=layer_name+\"out\")\r\n            out = tf.fake_quant_with_min_max_args(out, 0, 6)\r\n        else:\r\n            max_out = tf.get_variable(\"max_quant_output\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\r\n            min_out = tf.get_variable(\"min_quant_output\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)  \r\n            out = tf.fake_quant_with_min_max_vars(out, min_out, max_out, name=\"fake_quant_with_min_max_out_quantinization\")\r\n    \r\n    return out, max_w, min_w, W\r\n\r\n\r\ndef loss(logits, batch):\r\n    return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.ones(shape=[batch, 1]))\r\n\r\n\r\ndef build_net(ph_input):\r\n    inp = tf.fake_quant_with_min_max_args(ph_input, -1, 1)\r\n    out, max_w1, min_w1, W1 = quant_conv_op(inp, num_filters=64, filter_size=[3, 3], layer_name=\"layer1\")    \r\n    out, max_w2, min_w2, W2 = quant_conv_op(out, num_filters=128, filter_size=[3, 3], layer_name=\"layer2\")\r\n    out, max_w3, min_w3, W3 = quant_conv_op(out, num_filters=256, filter_size=[3, 3], layer_name=\"layer3\")\r\n    out = tf.reduce_mean(out, axis=[1, 2], keep_dims=True, name=\"avg_pool\")\r\n    out = tf.fake_quant_with_min_max_args(out, 0, 6, name=\"fake_quant_with_min_max_avgpool_quantinization\")\r\n    logits, max_w4, min_w4, W4 = quant_conv_op(out, num_filters=1, filter_size=[1, 1], layer_name=\"layer4\", use_acivation=False)\r\n    \r\n    max_list = [max_w1, max_w2, max_w3, max_w4]\r\n    min_list = [min_w1, min_w2, min_w3, min_w4]\r\n    W_list = [W1, W2, W3, W4]\r\n    logits = tf.reshape(logits, [-1, 1])\r\n    sig_loss = tf.reduce_mean(loss(logits, batch=64))\r\n    adam_op = tf.train.AdamOptimizer(10**-0)\r\n    train_op = adam_op.minimize(sig_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) \r\n    return train_op, max_list, min_list, sig_loss, W_list\r\n\r\n\r\ndef main():\r\n    sess = tf.InteractiveSession()\r\n    ph_input = tf.placeholder(tf.float32, [None, 28, 28, 3], name=\"network_input\") \r\n    train_op, max_list, min_list, sig_loss, W_list = build_net(ph_input)\r\n    sess.run(tf.global_variables_initializer())\r\n    tf.summary.FileWriter('.', graph=tf.get_default_graph())\r\n    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n    for t in trainable_vars:\r\n        if len(t.shape) == 0:\r\n           print(t.name, sess.run(t))\r\n        if len(t.shape) == 4:\r\n           v = sess.run(t)\r\n           print(t.name, np.max(v), np.min(v))\r\n\r\n    for i in range(10):\r\n        res = sess.run([train_op, sig_loss, W_list[0]]+max_list, feed_dict={ph_input:np.random.uniform(low=-1, high=1, size=[64, 28, 28, 3])})\r\n    \r\n    print(\"=========\")\r\n    for t in trainable_vars:\r\n        if len(t.shape) == 0:\r\n           print(t.name, sess.run(t))\r\n        if len(t.shape) == 4:\r\n           v = sess.run(t)\r\n           print(t.name, np.max(v), np.min(v))\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n  \r\n  "}