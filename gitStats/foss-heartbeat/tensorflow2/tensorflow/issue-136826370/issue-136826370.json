{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1310", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1310/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1310/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1310/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1310", "id": 136826370, "node_id": "MDU6SXNzdWUxMzY4MjYzNzA=", "number": 1310, "title": "Could not specify explicit device specification ''", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-02-26T21:59:00Z", "updated_at": "2016-06-06T20:49:05Z", "closed_at": "2016-06-06T20:48:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Several users have reported an issue that occurs with the following steps (e.g. Issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"136698202\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1297\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1297/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1297\">#1297</a>):</p>\n<ol>\n<li>Start TensorFlow in a setting with a GPU.</li>\n<li>Define some variables with no explicit <code>tf.device()</code> set. This most often happens with embedding variables (used as arguments to <code>tf.gather()</code> or <code>tf.embedding_lookup()</code>).</li>\n<li>Initialize them. (They will be placed on the GPU, because it is the \"best available device\".)</li>\n<li>On running the first training step, the following error (or similar) is raised:</li>\n</ol>\n<pre><code>InvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n</code></pre>\n<p>The <code>SparseApplyAdagrad</code> op (or in general most <code>SparseApplyFoo</code> ops) is only defined on CPU. The variable has already been placed on GPU, so the graph is not placeable. Attempting to run the same program with <code>use_soft_placement=True</code> also fails, although with a stranger error.</p>\n<p><strong>TL;DR:</strong> If this affects you, create your variables in a <code>with tf.device(\"/cpu:0\"):</code> block, until this issue is resolved.</p>\n<p>The issue arises because (i) TensorFlow places variables on the first device where they run, (ii) it always prefers GPU over CPU when it is availabe, (iii) initialization ops are available on GPU, and (iv) it applies the placement algorith to the <em>pruned subgraph</em> (not the entire client graph).</p>\n<p>One workaround would be to apply the placement algorithm to the entire client graph. (This is the approach used in the separate <code>master_session.cc</code>/<code>simple_graph_execution_state.cc</code> codepath, used in the distributed runtime.) However, this has the effect of leaving the session in a broken state as soon as an unplaceable node is encountered. Switching to this behavior might cause issues for people doing exploratory graph construction in a REPL (IPython etc.) because the only remedy would be to recreate the entire graph on seeing such an error. Therefore, while failing fast in a non-interactive setting would be fine, a different solution for interactive use might be required.</p>", "body_text": "Several users have reported an issue that occurs with the following steps (e.g. Issue #1297):\n\nStart TensorFlow in a setting with a GPU.\nDefine some variables with no explicit tf.device() set. This most often happens with embedding variables (used as arguments to tf.gather() or tf.embedding_lookup()).\nInitialize them. (They will be placed on the GPU, because it is the \"best available device\".)\nOn running the first training step, the following error (or similar) is raised:\n\nInvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n\nThe SparseApplyAdagrad op (or in general most SparseApplyFoo ops) is only defined on CPU. The variable has already been placed on GPU, so the graph is not placeable. Attempting to run the same program with use_soft_placement=True also fails, although with a stranger error.\nTL;DR: If this affects you, create your variables in a with tf.device(\"/cpu:0\"): block, until this issue is resolved.\nThe issue arises because (i) TensorFlow places variables on the first device where they run, (ii) it always prefers GPU over CPU when it is availabe, (iii) initialization ops are available on GPU, and (iv) it applies the placement algorith to the pruned subgraph (not the entire client graph).\nOne workaround would be to apply the placement algorithm to the entire client graph. (This is the approach used in the separate master_session.cc/simple_graph_execution_state.cc codepath, used in the distributed runtime.) However, this has the effect of leaving the session in a broken state as soon as an unplaceable node is encountered. Switching to this behavior might cause issues for people doing exploratory graph construction in a REPL (IPython etc.) because the only remedy would be to recreate the entire graph on seeing such an error. Therefore, while failing fast in a non-interactive setting would be fine, a different solution for interactive use might be required.", "body": "Several users have reported an issue that occurs with the following steps (e.g. Issue #1297):\n1. Start TensorFlow in a setting with a GPU.\n2. Define some variables with no explicit `tf.device()` set. This most often happens with embedding variables (used as arguments to `tf.gather()` or `tf.embedding_lookup()`).\n3. Initialize them. (They will be placed on the GPU, because it is the \"best available device\".)\n4. On running the first training step, the following error (or similar) is raised:\n\n```\nInvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n```\n\nThe `SparseApplyAdagrad` op (or in general most `SparseApplyFoo` ops) is only defined on CPU. The variable has already been placed on GPU, so the graph is not placeable. Attempting to run the same program with `use_soft_placement=True` also fails, although with a stranger error.\n\n**TL;DR:** If this affects you, create your variables in a `with tf.device(\"/cpu:0\"):` block, until this issue is resolved.\n\nThe issue arises because (i) TensorFlow places variables on the first device where they run, (ii) it always prefers GPU over CPU when it is availabe, (iii) initialization ops are available on GPU, and (iv) it applies the placement algorith to the _pruned subgraph_ (not the entire client graph).\n\nOne workaround would be to apply the placement algorithm to the entire client graph. (This is the approach used in the separate `master_session.cc`/`simple_graph_execution_state.cc` codepath, used in the distributed runtime.) However, this has the effect of leaving the session in a broken state as soon as an unplaceable node is encountered. Switching to this behavior might cause issues for people doing exploratory graph construction in a REPL (IPython etc.) because the only remedy would be to recreate the entire graph on seeing such an error. Therefore, while failing fast in a non-interactive setting would be fine, a different solution for interactive use might be required.\n"}