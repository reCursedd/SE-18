{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18548", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18548/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18548/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18548/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18548", "id": 314546386, "node_id": "MDU6SXNzdWUzMTQ1NDYzODY=", "number": 18548, "title": "[r1.7][TensorRT] The passing parameter \"max_batch_size\" within function \"trt.create_inference_graph()\" is fixed to 1", "user": {"login": "oscarriddle", "id": 13745902, "node_id": "MDQ6VXNlcjEzNzQ1OTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/13745902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oscarriddle", "html_url": "https://github.com/oscarriddle", "followers_url": "https://api.github.com/users/oscarriddle/followers", "following_url": "https://api.github.com/users/oscarriddle/following{/other_user}", "gists_url": "https://api.github.com/users/oscarriddle/gists{/gist_id}", "starred_url": "https://api.github.com/users/oscarriddle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oscarriddle/subscriptions", "organizations_url": "https://api.github.com/users/oscarriddle/orgs", "repos_url": "https://api.github.com/users/oscarriddle/repos", "events_url": "https://api.github.com/users/oscarriddle/events{/privacy}", "received_events_url": "https://api.github.com/users/oscarriddle/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-04-16T08:23:50Z", "updated_at": "2018-05-01T18:11:19Z", "closed_at": "2018-04-18T10:10:47Z", "author_association": "NONE", "body_html": "<p>What do I intended to do:<br>\nI've already trained a model by TensorFlow, and then try to use the integrated TensorRT to import the frozen model and optimize it, then output the optimized graph_def and session run it by TensorFlow.</p>\n<p>Script:</p>\n<pre><code>      with tf.Graph().as_default():\n         with tf.device(\"/device:GPU:0\"):                                                                                 \n            output_graph_def = tf.GraphDef()\n            with open(\"./\"+frozen_model_name, \"rb\") as f:\n               output_graph_def.ParseFromString(f.read())\n               print (len(output_graph_def.node))\n               _ = tf.import_graph_def(output_graph_def, name=\"\")\n                                                                                   \n        f32_graph = trt.create_inference_graph(\n            input_graph_def=output_graph_def,                                                                                       \n            outputs=['InceptionV3/Logits/SpatialSqueeze'],\n            max_batch_size = 16,  #&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;HERE\n            max_workspace_size_bytes=1 &lt;&lt; 20,\n            precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"                                        \n            minimum_segment_size=2  # minimum number of nodes in an engine                                                \n        )\n</code></pre>\n<p>Log:</p>\n<pre><code>host/replica:0/task:0/device:GPU:0 with 4757 MB memory) -&gt; physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:08.0, compute capability: 6.1)\nTraceback (most recent call last):\n  File \"extract_model_tf_trt_frozen.py\", line 118, in &lt;module&gt;\n    _main()\n  File \"extract_model_tf_trt_frozen.py\", line 91, in _main\n    val = sess.run(out, {inp: batch_input})\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1116, in _run\n    str(subfeed_t.get_shape())))\nValueError: Cannot feed value of shape (16, 299, 299, 3) for Tensor u'import/Placeholder:0', which has shape '(1, 299, 299, 3)'\n</code></pre>\n<p>Observation:<br>\nSeems like even though I set the passing parameter \"max_batch_size\" to 16, the generated new graph still only has batch_size == 1. I can't locate the source file of trt_convert(), which is relevant actually, probably it's a binary .o file so I can only post a new issue here for help.</p>\n<p>Has anyone encountered the same issue?<br>\nAny idea will be welcome.</p>", "body_text": "What do I intended to do:\nI've already trained a model by TensorFlow, and then try to use the integrated TensorRT to import the frozen model and optimize it, then output the optimized graph_def and session run it by TensorFlow.\nScript:\n      with tf.Graph().as_default():\n         with tf.device(\"/device:GPU:0\"):                                                                                 \n            output_graph_def = tf.GraphDef()\n            with open(\"./\"+frozen_model_name, \"rb\") as f:\n               output_graph_def.ParseFromString(f.read())\n               print (len(output_graph_def.node))\n               _ = tf.import_graph_def(output_graph_def, name=\"\")\n                                                                                   \n        f32_graph = trt.create_inference_graph(\n            input_graph_def=output_graph_def,                                                                                       \n            outputs=['InceptionV3/Logits/SpatialSqueeze'],\n            max_batch_size = 16,  #<<<<<<<<<<<<<<<HERE\n            max_workspace_size_bytes=1 << 20,\n            precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"                                        \n            minimum_segment_size=2  # minimum number of nodes in an engine                                                \n        )\n\nLog:\nhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:08.0, compute capability: 6.1)\nTraceback (most recent call last):\n  File \"extract_model_tf_trt_frozen.py\", line 118, in <module>\n    _main()\n  File \"extract_model_tf_trt_frozen.py\", line 91, in _main\n    val = sess.run(out, {inp: batch_input})\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1116, in _run\n    str(subfeed_t.get_shape())))\nValueError: Cannot feed value of shape (16, 299, 299, 3) for Tensor u'import/Placeholder:0', which has shape '(1, 299, 299, 3)'\n\nObservation:\nSeems like even though I set the passing parameter \"max_batch_size\" to 16, the generated new graph still only has batch_size == 1. I can't locate the source file of trt_convert(), which is relevant actually, probably it's a binary .o file so I can only post a new issue here for help.\nHas anyone encountered the same issue?\nAny idea will be welcome.", "body": "What do I intended to do:\r\nI've already trained a model by TensorFlow, and then try to use the integrated TensorRT to import the frozen model and optimize it, then output the optimized graph_def and session run it by TensorFlow.\r\n\r\nScript:                                                 \r\n```\r\n      with tf.Graph().as_default():\r\n         with tf.device(\"/device:GPU:0\"):                                                                                 \r\n            output_graph_def = tf.GraphDef()\r\n            with open(\"./\"+frozen_model_name, \"rb\") as f:\r\n               output_graph_def.ParseFromString(f.read())\r\n               print (len(output_graph_def.node))\r\n               _ = tf.import_graph_def(output_graph_def, name=\"\")\r\n                                                                                   \r\n        f32_graph = trt.create_inference_graph(\r\n            input_graph_def=output_graph_def,                                                                                       \r\n            outputs=['InceptionV3/Logits/SpatialSqueeze'],\r\n            max_batch_size = 16,  #<<<<<<<<<<<<<<<HERE\r\n            max_workspace_size_bytes=1 << 20,\r\n            precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"                                        \r\n            minimum_segment_size=2  # minimum number of nodes in an engine                                                \r\n        )\r\n```\r\nLog:\r\n```\r\nhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:08.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"extract_model_tf_trt_frozen.py\", line 118, in <module>\r\n    _main()\r\n  File \"extract_model_tf_trt_frozen.py\", line 91, in _main\r\n    val = sess.run(out, {inp: batch_input})\r\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1116, in _run\r\n    str(subfeed_t.get_shape())))\r\nValueError: Cannot feed value of shape (16, 299, 299, 3) for Tensor u'import/Placeholder:0', which has shape '(1, 299, 299, 3)'\r\n```\r\n\r\nObservation:\r\nSeems like even though I set the passing parameter \"max_batch_size\" to 16, the generated new graph still only has batch_size == 1. I can't locate the source file of trt_convert(), which is relevant actually, probably it's a binary .o file so I can only post a new issue here for help.\r\n\r\nHas anyone encountered the same issue?\r\nAny idea will be welcome."}