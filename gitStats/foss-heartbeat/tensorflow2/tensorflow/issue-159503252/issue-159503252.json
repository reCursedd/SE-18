{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2764", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2764/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2764/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2764/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2764", "id": 159503252, "node_id": "MDU6SXNzdWUxNTk1MDMyNTI=", "number": 2764, "title": "Run train op multiple times", "user": {"login": "vladfi1", "id": 691536, "node_id": "MDQ6VXNlcjY5MTUzNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/691536?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vladfi1", "html_url": "https://github.com/vladfi1", "followers_url": "https://api.github.com/users/vladfi1/followers", "following_url": "https://api.github.com/users/vladfi1/following{/other_user}", "gists_url": "https://api.github.com/users/vladfi1/gists{/gist_id}", "starred_url": "https://api.github.com/users/vladfi1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vladfi1/subscriptions", "organizations_url": "https://api.github.com/users/vladfi1/orgs", "repos_url": "https://api.github.com/users/vladfi1/repos", "events_url": "https://api.github.com/users/vladfi1/events{/privacy}", "received_events_url": "https://api.github.com/users/vladfi1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-06-09T20:42:32Z", "updated_at": "2016-06-10T02:41:56Z", "closed_at": "2016-06-10T01:52:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have some fairly large batch sizes on which I'd like to take multiple gradient steps. While I could easily do this with a python for loop, I imagine that there might be a more efficient method that doesn't involve transferring the data to gpu on each iteration.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> has confirmed that putting the train op in the fetch list multiple times doesn't work, and suggested that I could save my input tensors as variables with <code>tf.assign</code>. Unfortunately, this doesn't work for me because my input tensors have variable sizes, and tensorflow variables must have a fixed size.</p>", "body_text": "I have some fairly large batch sizes on which I'd like to take multiple gradient steps. While I could easily do this with a python for loop, I imagine that there might be a more efficient method that doesn't involve transferring the data to gpu on each iteration.\n@yaroslavvb has confirmed that putting the train op in the fetch list multiple times doesn't work, and suggested that I could save my input tensors as variables with tf.assign. Unfortunately, this doesn't work for me because my input tensors have variable sizes, and tensorflow variables must have a fixed size.", "body": "I have some fairly large batch sizes on which I'd like to take multiple gradient steps. While I could easily do this with a python for loop, I imagine that there might be a more efficient method that doesn't involve transferring the data to gpu on each iteration.\n\n@yaroslavvb has confirmed that putting the train op in the fetch list multiple times doesn't work, and suggested that I could save my input tensors as variables with `tf.assign`. Unfortunately, this doesn't work for me because my input tensors have variable sizes, and tensorflow variables must have a fixed size.\n"}