{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/329935174", "html_url": "https://github.com/tensorflow/tensorflow/issues/13075#issuecomment-329935174", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13075", "id": 329935174, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTkzNTE3NA==", "user": {"login": "RuofanKong", "id": 7396554, "node_id": "MDQ6VXNlcjczOTY1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7396554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RuofanKong", "html_url": "https://github.com/RuofanKong", "followers_url": "https://api.github.com/users/RuofanKong/followers", "following_url": "https://api.github.com/users/RuofanKong/following{/other_user}", "gists_url": "https://api.github.com/users/RuofanKong/gists{/gist_id}", "starred_url": "https://api.github.com/users/RuofanKong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RuofanKong/subscriptions", "organizations_url": "https://api.github.com/users/RuofanKong/orgs", "repos_url": "https://api.github.com/users/RuofanKong/repos", "events_url": "https://api.github.com/users/RuofanKong/events{/privacy}", "received_events_url": "https://api.github.com/users/RuofanKong/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-16T01:13:12Z", "updated_at": "2017-09-16T01:26:33Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> Thanks for the reply! That makes sense. But I still don't understand why all tensorflow ops go to the parameter server every time if we do <code>worker_device=\"/gpu:0\"</code>, is it intentional?</p>\n<p>The other issue is I compared the performance for using <code>worker_device=\"/job:worker/task:0\"</code> vs. <code>worker_device=\"/gpu:0\"</code>, and I noticed that when <code>worker_device=\"/gpu:0\"</code> where all ops go to parameter server, the training performance is much better than using <code>worker_device=\"/job:worker/task:0\"</code> (I compared them in different code which real training is involved, and the training speed for using <code>worker_device=\"/gpu:0\"</code> is much much faster than using <code>worker_device=\"/job:worker/task:0\"</code>). Is it also what we should expect? Why does it happen?</p>", "body_text": "@yaroslavvb @mrry Thanks for the reply! That makes sense. But I still don't understand why all tensorflow ops go to the parameter server every time if we do worker_device=\"/gpu:0\", is it intentional?\nThe other issue is I compared the performance for using worker_device=\"/job:worker/task:0\" vs. worker_device=\"/gpu:0\", and I noticed that when worker_device=\"/gpu:0\" where all ops go to parameter server, the training performance is much better than using worker_device=\"/job:worker/task:0\" (I compared them in different code which real training is involved, and the training speed for using worker_device=\"/gpu:0\" is much much faster than using worker_device=\"/job:worker/task:0\"). Is it also what we should expect? Why does it happen?", "body": "@yaroslavvb @mrry Thanks for the reply! That makes sense. But I still don't understand why all tensorflow ops go to the parameter server every time if we do `worker_device=\"/gpu:0\"`, is it intentional?\r\n\r\nThe other issue is I compared the performance for using `worker_device=\"/job:worker/task:0\"` vs. `worker_device=\"/gpu:0\"`, and I noticed that when `worker_device=\"/gpu:0\"` where all ops go to parameter server, the training performance is much better than using `worker_device=\"/job:worker/task:0\"` (I compared them in different code which real training is involved, and the training speed for using `worker_device=\"/gpu:0\"` is much much faster than using `worker_device=\"/job:worker/task:0\"`). Is it also what we should expect? Why does it happen?"}