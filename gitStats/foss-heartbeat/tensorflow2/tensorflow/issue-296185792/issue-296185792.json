{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16932", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16932/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16932/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16932/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16932", "id": 296185792, "node_id": "MDU6SXNzdWUyOTYxODU3OTI=", "number": 16932, "title": "Clarify RNNCell documentation on state_size and zero_state", "user": {"login": "Joshuaalbert", "id": 14807032, "node_id": "MDQ6VXNlcjE0ODA3MDMy", "avatar_url": "https://avatars2.githubusercontent.com/u/14807032?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Joshuaalbert", "html_url": "https://github.com/Joshuaalbert", "followers_url": "https://api.github.com/users/Joshuaalbert/followers", "following_url": "https://api.github.com/users/Joshuaalbert/following{/other_user}", "gists_url": "https://api.github.com/users/Joshuaalbert/gists{/gist_id}", "starred_url": "https://api.github.com/users/Joshuaalbert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Joshuaalbert/subscriptions", "organizations_url": "https://api.github.com/users/Joshuaalbert/orgs", "repos_url": "https://api.github.com/users/Joshuaalbert/repos", "events_url": "https://api.github.com/users/Joshuaalbert/events{/privacy}", "received_events_url": "https://api.github.com/users/Joshuaalbert/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-02-11T12:17:54Z", "updated_at": "2018-04-17T13:18:37Z", "closed_at": "2018-04-17T13:18:37Z", "author_association": "NONE", "body_html": "<h2>TF Version</h2>\n<p>Documentation of current head r1.5</p>\n<h3>Describe the problem</h3>\n<p>In the <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell#zero_state\" rel=\"nofollow\">RNNCell documentation</a> for <code>zero_state</code> it is unclear if the state returned from an RNNCell should always be a 1-D Tensor or if it can be an N-D Tensor. The second is actually the case.</p>\n<p>The current documentation says</p>\n<blockquote>\n<p>If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size, state_size] filled with zeros.</p>\n</blockquote>\n<p>This is mostly true, however if Rank(TensorShape) is &gt; 1 then it should say <code>[batch_size] + state_size.shape.as_list()</code> or some variant, to properly inform the user that arbitrary dimensioned Tensors can be passed through.</p>\n<p>This is contradicted in the next sentence with:</p>\n<blockquote>\n<p>If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size.</p>\n</blockquote>\n<p>Here 2-D tensors should be N-D tensors and <code>[batch_size, s]</code>  should be <code>[batch_size] + s.shape.as_list()</code>.</p>\n<p>Failure to understand that N-D states can be passed back might force some users to flatten and then reshape states, which will result in performance penalties. An example where such N-D states are required might be for RNNCell's with external memory devices (like the DNC from Deepmind).<br>\nThe documentation for <code>state_size</code> is adequate but could be made to explicitly mention N-D states are allowed.</p>\n<h3>Proof</h3>\n<p>Example RNNCell with structure state tuple containing N-D state</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn_cell_impl\n\nclass NDState(rnn_cell_impl.RNNCell):\n    def __init__(self,\n            output_size):\n        self._output_size=output_size\n\n    @property\n    def state_size(self):\n       return (1, (tf.TensorShape([4,4]), 2) )\n    @property\n    def output_size(self):\n        return self._output_size\n\n\n    \n    def __call__(self, inputs, state):\n        \"\"\"\n        inputs : batch_size x input_size\n        state : tuple of 2D [batch_size x s]\n        \"\"\"\n        print(\"State:\",state)\n\n        return tf.layers.dense(inputs,self.output_size), state\n        \n\n\ndef test():\n    test_kwargs = { 'output_size':1}\n\n    inputs = tf.random_normal(shape=(100,10,2),dtype=tf.float32)\n    cell = NDState(**test_kwargs)\n    zero_state = cell.zero_state(10, tf.float32)\n    print(\"Zero_state:\",zero_state)\n    output,state = tf.nn.dynamic_rnn(cell,inputs, time_major=True,dtype=tf.float32)\n    \n\n    \nif __name__ == \"__main__\":\n    test()\n</code></pre>\n<p>Prints:</p>\n<pre><code>Zero_state: (&lt;tf.Tensor 'NDStateZeroState/zeros:0' shape=(10, 1) dtype=float32&gt;, (&lt;tf.Tensor 'NDStateZeroState/zeros_1:0' shape=(10, 4, 4) dtype=float32&gt;, &lt;tf.Tensor 'NDStateZeroState/zeros_2:0' shape=(10, 2) dtype=float32&gt;))\nState: (&lt;tf.Tensor 'rnn/while/Identity_3:0' shape=(10, 1) dtype=float32&gt;, (&lt;tf.Tensor 'rnn/while/Identity_4:0' shape=(10, 4, 4) dtype=float32&gt;, &lt;tf.Tensor 'rnn/while/Identity_5:0' shape=(10, 2) dtype=float32&gt;))\n</code></pre>\n<h2>Template</h2>\n<p>Have I written custom code: N/A<br>\nOS Platform and Distribution: N/A<br>\nTensorFlow installed from: git master<br>\nTensorFlow version: r1.5 and previous<br>\nBazel version: N/A<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: N/A<br>\nExact command to reproduce: N/A</p>", "body_text": "TF Version\nDocumentation of current head r1.5\nDescribe the problem\nIn the RNNCell documentation for zero_state it is unclear if the state returned from an RNNCell should always be a 1-D Tensor or if it can be an N-D Tensor. The second is actually the case.\nThe current documentation says\n\nIf state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size, state_size] filled with zeros.\n\nThis is mostly true, however if Rank(TensorShape) is > 1 then it should say [batch_size] + state_size.shape.as_list() or some variant, to properly inform the user that arbitrary dimensioned Tensors can be passed through.\nThis is contradicted in the next sentence with:\n\nIf state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size.\n\nHere 2-D tensors should be N-D tensors and [batch_size, s]  should be [batch_size] + s.shape.as_list().\nFailure to understand that N-D states can be passed back might force some users to flatten and then reshape states, which will result in performance penalties. An example where such N-D states are required might be for RNNCell's with external memory devices (like the DNC from Deepmind).\nThe documentation for state_size is adequate but could be made to explicitly mention N-D states are allowed.\nProof\nExample RNNCell with structure state tuple containing N-D state\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn_cell_impl\n\nclass NDState(rnn_cell_impl.RNNCell):\n    def __init__(self,\n            output_size):\n        self._output_size=output_size\n\n    @property\n    def state_size(self):\n       return (1, (tf.TensorShape([4,4]), 2) )\n    @property\n    def output_size(self):\n        return self._output_size\n\n\n    \n    def __call__(self, inputs, state):\n        \"\"\"\n        inputs : batch_size x input_size\n        state : tuple of 2D [batch_size x s]\n        \"\"\"\n        print(\"State:\",state)\n\n        return tf.layers.dense(inputs,self.output_size), state\n        \n\n\ndef test():\n    test_kwargs = { 'output_size':1}\n\n    inputs = tf.random_normal(shape=(100,10,2),dtype=tf.float32)\n    cell = NDState(**test_kwargs)\n    zero_state = cell.zero_state(10, tf.float32)\n    print(\"Zero_state:\",zero_state)\n    output,state = tf.nn.dynamic_rnn(cell,inputs, time_major=True,dtype=tf.float32)\n    \n\n    \nif __name__ == \"__main__\":\n    test()\n\nPrints:\nZero_state: (<tf.Tensor 'NDStateZeroState/zeros:0' shape=(10, 1) dtype=float32>, (<tf.Tensor 'NDStateZeroState/zeros_1:0' shape=(10, 4, 4) dtype=float32>, <tf.Tensor 'NDStateZeroState/zeros_2:0' shape=(10, 2) dtype=float32>))\nState: (<tf.Tensor 'rnn/while/Identity_3:0' shape=(10, 1) dtype=float32>, (<tf.Tensor 'rnn/while/Identity_4:0' shape=(10, 4, 4) dtype=float32>, <tf.Tensor 'rnn/while/Identity_5:0' shape=(10, 2) dtype=float32>))\n\nTemplate\nHave I written custom code: N/A\nOS Platform and Distribution: N/A\nTensorFlow installed from: git master\nTensorFlow version: r1.5 and previous\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A", "body": "## TF Version\r\nDocumentation of current head r1.5\r\n\r\n### Describe the problem\r\nIn the [RNNCell documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell#zero_state) for `zero_state` it is unclear if the state returned from an RNNCell should always be a 1-D Tensor or if it can be an N-D Tensor. The second is actually the case.\r\n\r\nThe current documentation says\r\n> If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size, state_size] filled with zeros.\r\n\r\nThis is mostly true, however if Rank(TensorShape) is > 1 then it should say `[batch_size] + state_size.shape.as_list()` or some variant, to properly inform the user that arbitrary dimensioned Tensors can be passed through.\r\n\r\nThis is contradicted in the next sentence with:\r\n> If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size, s] for each s in state_size.\r\n\r\nHere 2-D tensors should be N-D tensors and `[batch_size, s]`  should be `[batch_size] + s.shape.as_list()`.\r\n\r\nFailure to understand that N-D states can be passed back might force some users to flatten and then reshape states, which will result in performance penalties. An example where such N-D states are required might be for RNNCell's with external memory devices (like the DNC from Deepmind).\r\nThe documentation for `state_size` is adequate but could be made to explicitly mention N-D states are allowed.\r\n\r\n### Proof\r\nExample RNNCell with structure state tuple containing N-D state\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.ops import rnn_cell_impl\r\n\r\nclass NDState(rnn_cell_impl.RNNCell):\r\n    def __init__(self,\r\n            output_size):\r\n        self._output_size=output_size\r\n\r\n    @property\r\n    def state_size(self):\r\n       return (1, (tf.TensorShape([4,4]), 2) )\r\n    @property\r\n    def output_size(self):\r\n        return self._output_size\r\n\r\n\r\n    \r\n    def __call__(self, inputs, state):\r\n        \"\"\"\r\n        inputs : batch_size x input_size\r\n        state : tuple of 2D [batch_size x s]\r\n        \"\"\"\r\n        print(\"State:\",state)\r\n\r\n        return tf.layers.dense(inputs,self.output_size), state\r\n        \r\n\r\n\r\ndef test():\r\n    test_kwargs = { 'output_size':1}\r\n\r\n    inputs = tf.random_normal(shape=(100,10,2),dtype=tf.float32)\r\n    cell = NDState(**test_kwargs)\r\n    zero_state = cell.zero_state(10, tf.float32)\r\n    print(\"Zero_state:\",zero_state)\r\n    output,state = tf.nn.dynamic_rnn(cell,inputs, time_major=True,dtype=tf.float32)\r\n    \r\n\r\n    \r\nif __name__ == \"__main__\":\r\n    test()\r\n```\r\n\r\nPrints:\r\n```\r\nZero_state: (<tf.Tensor 'NDStateZeroState/zeros:0' shape=(10, 1) dtype=float32>, (<tf.Tensor 'NDStateZeroState/zeros_1:0' shape=(10, 4, 4) dtype=float32>, <tf.Tensor 'NDStateZeroState/zeros_2:0' shape=(10, 2) dtype=float32>))\r\nState: (<tf.Tensor 'rnn/while/Identity_3:0' shape=(10, 1) dtype=float32>, (<tf.Tensor 'rnn/while/Identity_4:0' shape=(10, 4, 4) dtype=float32>, <tf.Tensor 'rnn/while/Identity_5:0' shape=(10, 2) dtype=float32>))\r\n```\r\n\r\n## Template\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: git master\r\nTensorFlow version: r1.5 and previous\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A"}