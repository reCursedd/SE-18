{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/75197605", "pull_request_review_id": null, "id": 75197605, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc1MTk3NjA1", "diff_hunk": "@@ -0,0 +1,612 @@\n+/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// This file contains a set of different implementations of the two-dimensional\n+// convolution operation. The standard TensorFlow Conv2d kernel uses EigenTensor\n+// to implement the computation, but here there are a variety of different ways\n+// of producing the same result. These methods are designed to be easier to\n+// understand and connect to other libraries, so that we can take advantage of\n+// platforms that have specialized implementations of GEMM for example.\n+//\n+// The basic interface is a Conv functor object that's templated by the types\n+// of the data it will be operating on, and is passed in the arguments needed to\n+// calculate the convolution. The simplest implementation of this functor is\n+// ReferenceConvFunctor, which is a readable but slow reference version.\n+//\n+// A faster version uses the approach of packing image patches into a matrix\n+// before calling a matrix multiply, the Im2ColConvFunctor. In turn, this can\n+// use a variety of different methods to calculate the matrix multiplication,\n+// or GEMM. The simplest but slowest is the ReferenceGemmFunctor, but the\n+// FastGemmFunctor will use whatever optimized libraries are available. By\n+// default it uses Eigen, but on Apple platforms it will take advantage of the\n+// system's Accelerate BLAS library to get better performance than the standard\n+// TensorFlow convolution kernel.\n+//\n+// The version actually used is defined at the bottom of this file using the\n+// REGISTER_KERNEL_BUILDER() macro. To try out different implementations (for\n+// example to switch to a reference one for easier debugging) you can swap out\n+// the default functors in that call.\n+//\n+// The registration itself is guarded with the USE_GEMM_FOR_CONV macro. The iOS\n+// makefile build defines this, but if you want to enable this implementation\n+// and disable the standard EigenTensor one in other build setups, you'll need\n+// to define it there too.\n+\n+#include <string.h>\n+#include <map>\n+#include <vector>\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/numeric_op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/resource_mgr.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_slice.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n+#include \"tensorflow/core/util/padding.h\"\n+#include \"tensorflow/core/util/tensor_format.h\"\n+\n+#if defined(__APPLE__)\n+#include <Accelerate/Accelerate.h>\n+#define USE_ACCELERATE_GEMM\n+#endif  // __APPLE__\n+\n+namespace tensorflow {\n+\n+namespace {\n+// This function implements the convolution operation in as simple a form as\n+// possible. It won't give great performance, but it is very useful for\n+// stepping through and instrumenting for debugging, creating minimal benchmarks\n+// to prototype with, and sharing with teams that want to run this outside of\n+// our environment.\n+// With that in mind, I've avoided using anything except pretty standard C++\n+// types. This is especially noticeable in the data access through raw array\n+// indexing. It's deliberate in this case though, since it makes the underlying\n+// memory order very explicit, which is important for both inspecting memory\n+// contents during debugging and for specifying what we expect to others.\n+// The memory layout of the data is, from biggest stride to smallest:\n+// input_data = [input_batches, input_height, input_width, input_depth]\n+// filter_data = [filter_height, filter_width, input_depth, filter_count]\n+// output_data = [input_batches, output_height, output_width, filter_count]\n+template <class T1, class T2, class T3>\n+class ReferenceConvFunctor {\n+ public:\n+  void operator()(OpKernelContext* context, const T1* input_data,", "path": "tensorflow/core/kernels/conv_ops_using_gemm.cc", "position": 87, "original_position": 87, "commit_id": "38c644f32971c2a17fd0ca6ad9e09fdfb36d1b19", "original_commit_id": "1de99caa7c5967f768f5fb4cd0cab287ecccde41", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "body": "Good point. We do actually make sure that the int64 input parameters fit into int's as part of the common Compute() code below.\n", "created_at": "2016-08-17T20:14:16Z", "updated_at": "2016-08-23T01:49:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/3778#discussion_r75197605", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3778", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/75197605"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/3778#discussion_r75197605"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3778"}}, "body_html": "<p>Good point. We do actually make sure that the int64 input parameters fit into int's as part of the common Compute() code below.</p>", "body_text": "Good point. We do actually make sure that the int64 input parameters fit into int's as part of the common Compute() code below."}