{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3208", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3208/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3208/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3208/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3208", "id": 164134830, "node_id": "MDU6SXNzdWUxNjQxMzQ4MzA=", "number": 3208, "title": "minimize raises \"ValueError: None values not supported.\" error when tf.while_loop used inside fn() of tf.cond()", "user": {"login": "liusiqi43", "id": 1165468, "node_id": "MDQ6VXNlcjExNjU0Njg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1165468?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liusiqi43", "html_url": "https://github.com/liusiqi43", "followers_url": "https://api.github.com/users/liusiqi43/followers", "following_url": "https://api.github.com/users/liusiqi43/following{/other_user}", "gists_url": "https://api.github.com/users/liusiqi43/gists{/gist_id}", "starred_url": "https://api.github.com/users/liusiqi43/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liusiqi43/subscriptions", "organizations_url": "https://api.github.com/users/liusiqi43/orgs", "repos_url": "https://api.github.com/users/liusiqi43/repos", "events_url": "https://api.github.com/users/liusiqi43/events{/privacy}", "received_events_url": "https://api.github.com/users/liusiqi43/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2016-07-06T17:48:54Z", "updated_at": "2017-04-05T01:36:10Z", "closed_at": "2017-04-05T01:36:10Z", "author_association": "NONE", "body_html": "<p>On TF 0.9.0, The following code with tf.while_loop inside tf.cond raises ValueError when minimized.</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.ops import tensor_array_ops\n\nwith tf.Session() as sess:\n    a = tf.constant([[1., 2.], [3.,4.]])\n    w = tf.get_variable('w', [2, 1], tf.float32)\n    c = tf.Variable(1)\n\n    def loss_a(a, w):\n        a_ta = tensor_array_ops.TensorArray(dtype=tf.float32, size=2,\n                                            tensor_array_name=\"a_ta\")\n        a_ta = a_ta.unpack(a)\n\n        b_ta = tensor_array_ops.TensorArray(dtype=tf.float32, size=2,\n                                            tensor_array_name=\"b_ta\")\n\n\n        time = tf.constant(0, dtype=tf.int32, name=\"time\")\n\n        def _time_step(time, a_ta_t, b_ta_t):\n          a = a_ta_t.read(time)\n          b_ta_t = b_ta_t.write(time, a * w)\n          return (time+1, a_ta_t, b_ta_t)\n\n        (_, _, final_b) = tf.while_loop(\n            cond=lambda time, _1, _2: time &lt; 2,\n            body=_time_step,\n            loop_vars=(time, a_ta, b_ta),\n            parallel_iterations=32,\n            swap_memory=True)\n        b = tf.reduce_sum(final_b.pack(), 0)\n        return b\n\n    def loss_b(a, w):\n        return 2 * a * w\n\n    loss = tf.cond(tf.equal(c, 0), lambda: loss_a(a, w), lambda: loss_b(a,w))\n    train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n\n    sess.run(train_op)\n</code></pre>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"test.py\", line 39, in <br>\ntrain_op = tf.train.AdamOptimizer(1e-4).minimize(loss)<br>\nFile \"/.../tensorflow/python/training/optimizer.py\", line 193, in minimize<br>\ngrad_loss=grad_loss)<br>\nFile \"/.../tensorflow/python/training/optimizer.py\", line 250, in compute_gradients<br>\ncolocate_gradients_with_ops=colocate_gradients_with_ops)<br>\nFile \"/.../tensorflow/python/ops/gradients.py\", line 481, in gradients<br>\nin_grads = _AsList(grad_fn(op, *out_grads))<br>\nFile \"/.../tensorflow/python/ops/tensor_array_grad.py\", line 115, in _TensorArrayWriteGrad<br>\ngrad = g.read(index)<br>\nFile \"/.../tensorflow/python/ops/tensor_array_ops.py\", line 191, in read<br>\ndtype=self._dtype, name=name)<br>\nFile \"/.../tensorflow/python/ops/gen_data_flow_ops.py\", line 905, in _tensor_array_read<br>\nflow_in=flow_in, dtype=dtype, name=name)<br>\nFile \"/.../tensorflow/python/ops/op_def_library.py\", line 704, in apply_op<br>\nop_def=op_def)<br>\nFile \"/.../tensorflow/python/framework/ops.py\", line 2260, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/.../tensorflow/python/framework/ops.py\", line 1234, in <strong>init</strong><br>\nself._control_flow_context.AddOp(self)<br>\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1479, in AddOp<br>\nself._AddOpInternal(op)<br>\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1499, in _AddOpInternal<br>\nself.AddValue(x)<br>\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1438, in AddValue<br>\nreal_val = grad_ctxt.grad_state.GetRealValue(val)<br>\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 781, in GetRealValue<br>\nreal_value = self.AddBackPropAccumulatedValue(h_value, value)<br>\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 731, in AddBackPropAccumulatedValue<br>\nhistory_value = _SwitchRefOrTensor(history_value, pred)[branch]<br>\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 324, in _SwitchRefOrTensor<br>\nreturn ref_switch(data, pred, name=name)<br>\nFile \"/.../tensorflow/python/ops/gen_control_flow_ops.py\", line 341, in ref_switch<br>\nresult = _op_def_lib.apply_op(\"RefSwitch\", data=data, pred=pred, name=name)<br>\nFile \"/.../tensorflow/python/ops/op_def_library.py\", line 459, in apply_op<br>\nas_ref=input_arg.is_ref).dtype.name<br>\nFile \"/.../tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor<br>\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)<br>\nFile \"/.../tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function<br>\nreturn constant(v, dtype=dtype, name=name)<br>\nFile \"/.../tensorflow/python/ops/constant_op.py\", line 162, in constant<br>\ntensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))<br>\nFile \"/.../tensorflow/python/framework/tensor_util.py\", line 346, in make_tensor_proto<br>\nraise ValueError(\"None values not supported.\")<br>\nValueError: None values not supported.</p>\n</blockquote>\n<p>It works fine if I add la, lb as follows, although this would force execution of loss_a and loss_b regardless of the pred in tf.cond().</p>\n<pre><code>    la = loss_a(a, w)\n    lb = loss_b(a, w)\n\n    loss = tf.cond(tf.equal(c, 0), lambda: la, lambda: lb)\n    train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n\n    sess.run(train_op)\n</code></pre>\n<p>Is this a known issue or did I miss something in how to use tf.while_loop within tf.cond?</p>", "body_text": "On TF 0.9.0, The following code with tf.while_loop inside tf.cond raises ValueError when minimized.\nimport tensorflow as tf\nfrom tensorflow.python.ops import tensor_array_ops\n\nwith tf.Session() as sess:\n    a = tf.constant([[1., 2.], [3.,4.]])\n    w = tf.get_variable('w', [2, 1], tf.float32)\n    c = tf.Variable(1)\n\n    def loss_a(a, w):\n        a_ta = tensor_array_ops.TensorArray(dtype=tf.float32, size=2,\n                                            tensor_array_name=\"a_ta\")\n        a_ta = a_ta.unpack(a)\n\n        b_ta = tensor_array_ops.TensorArray(dtype=tf.float32, size=2,\n                                            tensor_array_name=\"b_ta\")\n\n\n        time = tf.constant(0, dtype=tf.int32, name=\"time\")\n\n        def _time_step(time, a_ta_t, b_ta_t):\n          a = a_ta_t.read(time)\n          b_ta_t = b_ta_t.write(time, a * w)\n          return (time+1, a_ta_t, b_ta_t)\n\n        (_, _, final_b) = tf.while_loop(\n            cond=lambda time, _1, _2: time < 2,\n            body=_time_step,\n            loop_vars=(time, a_ta, b_ta),\n            parallel_iterations=32,\n            swap_memory=True)\n        b = tf.reduce_sum(final_b.pack(), 0)\n        return b\n\n    def loss_b(a, w):\n        return 2 * a * w\n\n    loss = tf.cond(tf.equal(c, 0), lambda: loss_a(a, w), lambda: loss_b(a,w))\n    train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n\n    sess.run(train_op)\n\n\nTraceback (most recent call last):\nFile \"test.py\", line 39, in \ntrain_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\nFile \"/.../tensorflow/python/training/optimizer.py\", line 193, in minimize\ngrad_loss=grad_loss)\nFile \"/.../tensorflow/python/training/optimizer.py\", line 250, in compute_gradients\ncolocate_gradients_with_ops=colocate_gradients_with_ops)\nFile \"/.../tensorflow/python/ops/gradients.py\", line 481, in gradients\nin_grads = _AsList(grad_fn(op, *out_grads))\nFile \"/.../tensorflow/python/ops/tensor_array_grad.py\", line 115, in _TensorArrayWriteGrad\ngrad = g.read(index)\nFile \"/.../tensorflow/python/ops/tensor_array_ops.py\", line 191, in read\ndtype=self._dtype, name=name)\nFile \"/.../tensorflow/python/ops/gen_data_flow_ops.py\", line 905, in _tensor_array_read\nflow_in=flow_in, dtype=dtype, name=name)\nFile \"/.../tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\nop_def=op_def)\nFile \"/.../tensorflow/python/framework/ops.py\", line 2260, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/.../tensorflow/python/framework/ops.py\", line 1234, in init\nself._control_flow_context.AddOp(self)\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1479, in AddOp\nself._AddOpInternal(op)\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1499, in _AddOpInternal\nself.AddValue(x)\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1438, in AddValue\nreal_val = grad_ctxt.grad_state.GetRealValue(val)\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 781, in GetRealValue\nreal_value = self.AddBackPropAccumulatedValue(h_value, value)\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 731, in AddBackPropAccumulatedValue\nhistory_value = _SwitchRefOrTensor(history_value, pred)[branch]\nFile \"/.../tensorflow/python/ops/control_flow_ops.py\", line 324, in _SwitchRefOrTensor\nreturn ref_switch(data, pred, name=name)\nFile \"/.../tensorflow/python/ops/gen_control_flow_ops.py\", line 341, in ref_switch\nresult = _op_def_lib.apply_op(\"RefSwitch\", data=data, pred=pred, name=name)\nFile \"/.../tensorflow/python/ops/op_def_library.py\", line 459, in apply_op\nas_ref=input_arg.is_ref).dtype.name\nFile \"/.../tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\nFile \"/.../tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\nreturn constant(v, dtype=dtype, name=name)\nFile \"/.../tensorflow/python/ops/constant_op.py\", line 162, in constant\ntensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\nFile \"/.../tensorflow/python/framework/tensor_util.py\", line 346, in make_tensor_proto\nraise ValueError(\"None values not supported.\")\nValueError: None values not supported.\n\nIt works fine if I add la, lb as follows, although this would force execution of loss_a and loss_b regardless of the pred in tf.cond().\n    la = loss_a(a, w)\n    lb = loss_b(a, w)\n\n    loss = tf.cond(tf.equal(c, 0), lambda: la, lambda: lb)\n    train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n\n    sess.run(train_op)\n\nIs this a known issue or did I miss something in how to use tf.while_loop within tf.cond?", "body": "On TF 0.9.0, The following code with tf.while_loop inside tf.cond raises ValueError when minimized. \n\n```\nimport tensorflow as tf\nfrom tensorflow.python.ops import tensor_array_ops\n\nwith tf.Session() as sess:\n    a = tf.constant([[1., 2.], [3.,4.]])\n    w = tf.get_variable('w', [2, 1], tf.float32)\n    c = tf.Variable(1)\n\n    def loss_a(a, w):\n        a_ta = tensor_array_ops.TensorArray(dtype=tf.float32, size=2,\n                                            tensor_array_name=\"a_ta\")\n        a_ta = a_ta.unpack(a)\n\n        b_ta = tensor_array_ops.TensorArray(dtype=tf.float32, size=2,\n                                            tensor_array_name=\"b_ta\")\n\n\n        time = tf.constant(0, dtype=tf.int32, name=\"time\")\n\n        def _time_step(time, a_ta_t, b_ta_t):\n          a = a_ta_t.read(time)\n          b_ta_t = b_ta_t.write(time, a * w)\n          return (time+1, a_ta_t, b_ta_t)\n\n        (_, _, final_b) = tf.while_loop(\n            cond=lambda time, _1, _2: time < 2,\n            body=_time_step,\n            loop_vars=(time, a_ta, b_ta),\n            parallel_iterations=32,\n            swap_memory=True)\n        b = tf.reduce_sum(final_b.pack(), 0)\n        return b\n\n    def loss_b(a, w):\n        return 2 * a * w\n\n    loss = tf.cond(tf.equal(c, 0), lambda: loss_a(a, w), lambda: loss_b(a,w))\n    train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n\n    sess.run(train_op)\n```\n\n> Traceback (most recent call last):\n>   File \"test.py\", line 39, in <module>\n>     train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n>   File \"/.../tensorflow/python/training/optimizer.py\", line 193, in minimize\n>     grad_loss=grad_loss)\n>   File \"/.../tensorflow/python/training/optimizer.py\", line 250, in compute_gradients\n>     colocate_gradients_with_ops=colocate_gradients_with_ops)\n>   File \"/.../tensorflow/python/ops/gradients.py\", line 481, in gradients\n>     in_grads = _AsList(grad_fn(op, *out_grads))\n>   File \"/.../tensorflow/python/ops/tensor_array_grad.py\", line 115, in _TensorArrayWriteGrad\n>     grad = g.read(index)\n>   File \"/.../tensorflow/python/ops/tensor_array_ops.py\", line 191, in read\n>     dtype=self._dtype, name=name)\n>   File \"/.../tensorflow/python/ops/gen_data_flow_ops.py\", line 905, in _tensor_array_read\n>     flow_in=flow_in, dtype=dtype, name=name)\n>   File \"/.../tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n>     op_def=op_def)\n>   File \"/.../tensorflow/python/framework/ops.py\", line 2260, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/.../tensorflow/python/framework/ops.py\", line 1234, in __init__\n>     self._control_flow_context.AddOp(self)\n>   File \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1479, in AddOp\n>     self._AddOpInternal(op)\n>   File \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1499, in _AddOpInternal\n>     self.AddValue(x)\n>   File \"/.../tensorflow/python/ops/control_flow_ops.py\", line 1438, in AddValue\n>     real_val = grad_ctxt.grad_state.GetRealValue(val)\n>   File \"/.../tensorflow/python/ops/control_flow_ops.py\", line 781, in GetRealValue\n>     real_value = self.AddBackPropAccumulatedValue(h_value, value)\n>   File \"/.../tensorflow/python/ops/control_flow_ops.py\", line 731, in AddBackPropAccumulatedValue\n>     history_value = _SwitchRefOrTensor(history_value, pred)[branch]\n>   File \"/.../tensorflow/python/ops/control_flow_ops.py\", line 324, in _SwitchRefOrTensor\n>     return ref_switch(data, pred, name=name)\n>   File \"/.../tensorflow/python/ops/gen_control_flow_ops.py\", line 341, in ref_switch\n>     result = _op_def_lib.apply_op(\"RefSwitch\", data=data, pred=pred, name=name)\n>   File \"/.../tensorflow/python/ops/op_def_library.py\", line 459, in apply_op\n>     as_ref=input_arg.is_ref).dtype.name\n>   File \"/.../tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n>   File \"/.../tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n>     return constant(v, dtype=dtype, name=name)\n>   File \"/.../tensorflow/python/ops/constant_op.py\", line 162, in constant\n>     tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n>   File \"/.../tensorflow/python/framework/tensor_util.py\", line 346, in make_tensor_proto\n>     raise ValueError(\"None values not supported.\")\n> ValueError: None values not supported.\n\nIt works fine if I add la, lb as follows, although this would force execution of loss_a and loss_b regardless of the pred in tf.cond(). \n\n```\n    la = loss_a(a, w)\n    lb = loss_b(a, w)\n\n    loss = tf.cond(tf.equal(c, 0), lambda: la, lambda: lb)\n    train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n\n    sess.run(train_op)\n```\n\nIs this a known issue or did I miss something in how to use tf.while_loop within tf.cond? \n"}