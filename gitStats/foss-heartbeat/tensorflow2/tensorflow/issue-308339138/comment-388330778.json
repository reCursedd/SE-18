{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/388330778", "html_url": "https://github.com/tensorflow/tensorflow/issues/17984#issuecomment-388330778", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17984", "id": 388330778, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODMzMDc3OA==", "user": {"login": "guohengkai", "id": 5835678, "node_id": "MDQ6VXNlcjU4MzU2Nzg=", "avatar_url": "https://avatars3.githubusercontent.com/u/5835678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guohengkai", "html_url": "https://github.com/guohengkai", "followers_url": "https://api.github.com/users/guohengkai/followers", "following_url": "https://api.github.com/users/guohengkai/following{/other_user}", "gists_url": "https://api.github.com/users/guohengkai/gists{/gist_id}", "starred_url": "https://api.github.com/users/guohengkai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guohengkai/subscriptions", "organizations_url": "https://api.github.com/users/guohengkai/orgs", "repos_url": "https://api.github.com/users/guohengkai/repos", "events_url": "https://api.github.com/users/guohengkai/events{/privacy}", "received_events_url": "https://api.github.com/users/guohengkai/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-11T10:52:21Z", "updated_at": "2018-05-11T10:53:34Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> Sorry for the late response. The following codes will save the model into \"test.pbtxt\", which will cause this problem.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> graph:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Input</span>\n    input_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>))\n    input_split_1, input_split_2 <span class=\"pl-k\">=</span> tf.split(input_placeholder, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Stream 1</span>\n    conv1 <span class=\"pl-k\">=</span> tf.layers.conv2d(input_split_1, <span class=\"pl-c1\">8</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>)\n    bn1 <span class=\"pl-k\">=</span> tf.layers.batch_normalization(conv1, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.997</span>,\n                                        <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>, <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                        <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn1<span class=\"pl-pds\">'</span></span>)\n    relu1 <span class=\"pl-k\">=</span> tf.nn.relu(bn1)\n    conv2 <span class=\"pl-k\">=</span> tf.layers.conv2d(relu1, <span class=\"pl-c1\">8</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2<span class=\"pl-pds\">'</span></span>)\n    bn2 <span class=\"pl-k\">=</span> tf.layers.batch_normalization(conv2, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.997</span>,\n                                        <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>, <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                        <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn2<span class=\"pl-pds\">'</span></span>)\n    relu2 <span class=\"pl-k\">=</span> tf.nn.relu(bn2)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Stream 2</span>\n    conv1_2 <span class=\"pl-k\">=</span> tf.layers.conv2d(input_split_2, <span class=\"pl-c1\">8</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>,\n                            <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    bn1_2 <span class=\"pl-k\">=</span> tf.layers.batch_normalization(conv1_2, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.997</span>,\n                                        <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>, <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                        <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn1<span class=\"pl-pds\">'</span></span>,\n                                        <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    relu1_2 <span class=\"pl-k\">=</span> tf.nn.relu(bn1_2)\n    conv2_2 <span class=\"pl-k\">=</span> tf.layers.conv2d(relu1_2, <span class=\"pl-c1\">8</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2<span class=\"pl-pds\">'</span></span>,\n                            <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    bn2_2 <span class=\"pl-k\">=</span> tf.layers.batch_normalization(conv2_2, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.997</span>,\n                                        <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>, <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                        <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn2<span class=\"pl-pds\">'</span></span>,\n                                        <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    relu2_2 <span class=\"pl-k\">=</span> tf.nn.relu(bn2_2)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Final</span>\n    concat <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">values</span><span class=\"pl-k\">=</span>[relu2, relu2_2])\n    flatten <span class=\"pl-k\">=</span> tf.layers.flatten(concat)\n    fc <span class=\"pl-k\">=</span> tf.layers.dense(flatten, <span class=\"pl-c1\">200</span>)\n\n    tf.train.write_graph(graph.as_graph_def(), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>.<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test.pbtxt<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "@aselle Sorry for the late response. The following codes will save the model into \"test.pbtxt\", which will cause this problem.\nimport tensorflow as tf\n\nwith tf.Graph().as_default() as graph:\n    # Input\n    input_placeholder = tf.placeholder(tf.float32, shape=(1, 20, 20, 2))\n    input_split_1, input_split_2 = tf.split(input_placeholder, 2, axis=3)\n\n    # Stream 1\n    conv1 = tf.layers.conv2d(input_split_1, 8, kernel_size=3, strides=1,\n                            padding='SAME', activation=None, name='conv1')\n    bn1 = tf.layers.batch_normalization(conv1, axis=3, momentum=0.997,\n                                        epsilon=1e-5, center=True, scale=True,\n                                        training=True, fused=True, name='bn1')\n    relu1 = tf.nn.relu(bn1)\n    conv2 = tf.layers.conv2d(relu1, 8, kernel_size=3, strides=1,\n                            padding='SAME', activation=None, name='conv2')\n    bn2 = tf.layers.batch_normalization(conv2, axis=3, momentum=0.997,\n                                        epsilon=1e-5, center=True, scale=True,\n                                        training=True, fused=True, name='bn2')\n    relu2 = tf.nn.relu(bn2)\n\n    # Stream 2\n    conv1_2 = tf.layers.conv2d(input_split_2, 8, kernel_size=3, strides=1,\n                            padding='SAME', activation=None, name='conv1',\n                            reuse=True)\n    bn1_2 = tf.layers.batch_normalization(conv1_2, axis=3, momentum=0.997,\n                                        epsilon=1e-5, center=True, scale=True,\n                                        training=True, fused=True, name='bn1',\n                                        reuse=True)\n    relu1_2 = tf.nn.relu(bn1_2)\n    conv2_2 = tf.layers.conv2d(relu1_2, 8, kernel_size=3, strides=1,\n                            padding='SAME', activation=None, name='conv2',\n                            reuse=True)\n    bn2_2 = tf.layers.batch_normalization(conv2_2, axis=3, momentum=0.997,\n                                        epsilon=1e-5, center=True, scale=True,\n                                        training=True, fused=True, name='bn2',\n                                        reuse=True)\n    relu2_2 = tf.nn.relu(bn2_2)\n\n    # Final\n    concat = tf.concat(axis=3, values=[relu2, relu2_2])\n    flatten = tf.layers.flatten(concat)\n    fc = tf.layers.dense(flatten, 200)\n\n    tf.train.write_graph(graph.as_graph_def(), '.', 'test.pbtxt')", "body": "@aselle Sorry for the late response. The following codes will save the model into \"test.pbtxt\", which will cause this problem.\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    # Input\r\n    input_placeholder = tf.placeholder(tf.float32, shape=(1, 20, 20, 2))\r\n    input_split_1, input_split_2 = tf.split(input_placeholder, 2, axis=3)\r\n\r\n    # Stream 1\r\n    conv1 = tf.layers.conv2d(input_split_1, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv1')\r\n    bn1 = tf.layers.batch_normalization(conv1, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn1')\r\n    relu1 = tf.nn.relu(bn1)\r\n    conv2 = tf.layers.conv2d(relu1, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv2')\r\n    bn2 = tf.layers.batch_normalization(conv2, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn2')\r\n    relu2 = tf.nn.relu(bn2)\r\n\r\n    # Stream 2\r\n    conv1_2 = tf.layers.conv2d(input_split_2, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv1',\r\n                            reuse=True)\r\n    bn1_2 = tf.layers.batch_normalization(conv1_2, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn1',\r\n                                        reuse=True)\r\n    relu1_2 = tf.nn.relu(bn1_2)\r\n    conv2_2 = tf.layers.conv2d(relu1_2, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv2',\r\n                            reuse=True)\r\n    bn2_2 = tf.layers.batch_normalization(conv2_2, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn2',\r\n                                        reuse=True)\r\n    relu2_2 = tf.nn.relu(bn2_2)\r\n\r\n    # Final\r\n    concat = tf.concat(axis=3, values=[relu2, relu2_2])\r\n    flatten = tf.layers.flatten(concat)\r\n    fc = tf.layers.dense(flatten, 200)\r\n\r\n    tf.train.write_graph(graph.as_graph_def(), '.', 'test.pbtxt')\r\n```"}