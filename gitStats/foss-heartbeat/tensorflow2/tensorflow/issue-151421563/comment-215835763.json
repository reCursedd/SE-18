{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215835763", "html_url": "https://github.com/tensorflow/tensorflow/issues/2131#issuecomment-215835763", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2131", "id": 215835763, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTgzNTc2Mw==", "user": {"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-29T18:14:40Z", "updated_at": "2016-04-29T18:14:40Z", "author_association": "CONTRIBUTOR", "body_html": "<ol>\n<li>By doing the following:<br>\nwith tf.control_dependencies([apply_gradient_op]):<br>\ntrain_op = variable_averages.apply(tf.trainable_variables())</li>\n</ol>\n<p>The parameters are updated before the variable_averages.</p>\n<ol>\n<li>This is multi-gpu run. We are trying to return a snapshot of total_loss in case it's being updated by the other thread.</li>\n<li>I don't fully understand your question. Are you asking whether the implementation you quoted is correct, or you are trying to do something else different, and wonder how you should do it?</li>\n</ol>\n<p>Thanks,<br>\nSherry</p>", "body_text": "By doing the following:\nwith tf.control_dependencies([apply_gradient_op]):\ntrain_op = variable_averages.apply(tf.trainable_variables())\n\nThe parameters are updated before the variable_averages.\n\nThis is multi-gpu run. We are trying to return a snapshot of total_loss in case it's being updated by the other thread.\nI don't fully understand your question. Are you asking whether the implementation you quoted is correct, or you are trying to do something else different, and wonder how you should do it?\n\nThanks,\nSherry", "body": "1. By doing the following:\n   with tf.control_dependencies([apply_gradient_op]):\n     train_op = variable_averages.apply(tf.trainable_variables())\n\nThe parameters are updated before the variable_averages.\n1. This is multi-gpu run. We are trying to return a snapshot of total_loss in case it's being updated by the other thread.\n2. I don't fully understand your question. Are you asking whether the implementation you quoted is correct, or you are trying to do something else different, and wonder how you should do it?\n\nThanks,\nSherry \n"}