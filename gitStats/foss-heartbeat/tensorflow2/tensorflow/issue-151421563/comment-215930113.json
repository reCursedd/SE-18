{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215930113", "html_url": "https://github.com/tensorflow/tensorflow/issues/2131#issuecomment-215930113", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2131", "id": 215930113, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTkzMDExMw==", "user": {"login": "c149028", "id": 18388769, "node_id": "MDQ6VXNlcjE4Mzg4NzY5", "avatar_url": "https://avatars2.githubusercontent.com/u/18388769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c149028", "html_url": "https://github.com/c149028", "followers_url": "https://api.github.com/users/c149028/followers", "following_url": "https://api.github.com/users/c149028/following{/other_user}", "gists_url": "https://api.github.com/users/c149028/gists{/gist_id}", "starred_url": "https://api.github.com/users/c149028/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c149028/subscriptions", "organizations_url": "https://api.github.com/users/c149028/orgs", "repos_url": "https://api.github.com/users/c149028/repos", "events_url": "https://api.github.com/users/c149028/events{/privacy}", "received_events_url": "https://api.github.com/users/c149028/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-30T03:34:09Z", "updated_at": "2016-04-30T03:34:09Z", "author_association": "NONE", "body_html": "<p>Hi Sherry,</p>\n<p>Suppose there is a parameter <code>p</code> initialized by 0. During training time the value of <code>p</code> are<br>\n0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4<br>\nIs it possible that the result of <code>ExponentialMovingAverage</code> become the moving average of 0, 1, 1, 3, 4 when we didn't specify the dependencies between <code>variable_averages.apply</code> and <code>apply_gradient_op</code>?</p>\n<hr>\n<p>I cannot figure out under what circumstances will <code>total_loss</code> be modified by the other thread within a single <code>sess.run</code>.<br>\nWhy we need</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.control_dependencies([loss_averages_op]):\n    total_loss <span class=\"pl-k\">=</span> tf.identity(total_loss)</pre></div>\n<p>to ensure <code>loss_averages_op</code> be executed before using <code>total_loss</code>?</p>", "body_text": "Hi Sherry,\nSuppose there is a parameter p initialized by 0. During training time the value of p are\n0 -> 1 -> 2 -> 3 -> 4\nIs it possible that the result of ExponentialMovingAverage become the moving average of 0, 1, 1, 3, 4 when we didn't specify the dependencies between variable_averages.apply and apply_gradient_op?\n\nI cannot figure out under what circumstances will total_loss be modified by the other thread within a single sess.run.\nWhy we need\nwith tf.control_dependencies([loss_averages_op]):\n    total_loss = tf.identity(total_loss)\nto ensure loss_averages_op be executed before using total_loss?", "body": "Hi Sherry,\n\nSuppose there is a parameter `p` initialized by 0. During training time the value of `p` are  \n0 -> 1 -> 2 -> 3 -> 4  \nIs it possible that the result of `ExponentialMovingAverage` become the moving average of 0, 1, 1, 3, 4 when we didn't specify the dependencies between `variable_averages.apply` and `apply_gradient_op`? \n\n---\n\nI cannot figure out under what circumstances will `total_loss` be modified by the other thread within a single `sess.run`.\nWhy we need\n\n``` python\nwith tf.control_dependencies([loss_averages_op]):\n    total_loss = tf.identity(total_loss)\n```\n\nto ensure `loss_averages_op` be executed before using `total_loss`?\n"}