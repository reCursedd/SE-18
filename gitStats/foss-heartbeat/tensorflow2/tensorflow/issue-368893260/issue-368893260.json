{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22878", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22878/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22878/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22878/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22878", "id": 368893260, "node_id": "MDU6SXNzdWUzNjg4OTMyNjA=", "number": 22878, "title": "[Bug] Strange FIFOQueue behavior in distributed tensorflow. Is this a bug?", "user": {"login": "DevarakondaV", "id": 24705015, "node_id": "MDQ6VXNlcjI0NzA1MDE1", "avatar_url": "https://avatars1.githubusercontent.com/u/24705015?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DevarakondaV", "html_url": "https://github.com/DevarakondaV", "followers_url": "https://api.github.com/users/DevarakondaV/followers", "following_url": "https://api.github.com/users/DevarakondaV/following{/other_user}", "gists_url": "https://api.github.com/users/DevarakondaV/gists{/gist_id}", "starred_url": "https://api.github.com/users/DevarakondaV/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DevarakondaV/subscriptions", "organizations_url": "https://api.github.com/users/DevarakondaV/orgs", "repos_url": "https://api.github.com/users/DevarakondaV/repos", "events_url": "https://api.github.com/users/DevarakondaV/events{/privacy}", "received_events_url": "https://api.github.com/users/DevarakondaV/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 996845227, "node_id": "MDU6TGFiZWw5OTY4NDUyMjc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:dist-strat", "name": "comp:dist-strat", "color": "0052cc", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-10-10T23:24:30Z", "updated_at": "2018-11-22T16:43:49Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install tensorflow</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: No CUDA</li>\n<li><strong>GPU model and memory</strong>: No GPU</li>\n<li><strong>Exact command to reproduce</strong>: Check code below</li>\n</ul>\n<hr>\n<h3>Describe the problem</h3>\n<p>There seems to be very little documentation on distributed tensorflow, so I am unsure of whether or not this is a bug or a logical error on my part. But the behavior is still a little strange.  Essentially, the issue seems to arise due to tf.train.SummarySaverHook(), it's interaction with tf.train.MoniteredTrainingSession() and the chief worker. If you look at my code below, you will see that there are two workers in this model. The chief worker is associated with \"task:0\". I've set up the model such that the queue and it operations live on \"task:1\"(not the chief worker). There is a simple sum operation on \"task:0\". Once I launch the 3 nodes(ps, task0, task1), task0 is able to complete one simple sum operation and then all tasks hang indefinitely.</p>\n<p>For the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:0). By doing this, there don't seem to be any issues and the code works fine. I'm unsure of why this bypasses the issue. If chief workers handle summary write operations, shouldn't it still hang?</p>\n<p>The strange behavior that I was talking about earlier occurs when you pass both the summary hook to the chief worker (Doing this will cause all workers to hang). You can get past the issue by simple removing the tf.FIFOqueue().dequeue() operation. It seems that this operation is getting called by \"task:0\" despite the fact that I've specified its location as \"task:1\" using tf.device. It is also being called despite not being an input requirement for any part of the graph that lives on \"task:0\".</p>\n<p>My question is essentially: Is this a bug or a logical error on my part?</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf \nimport sys\n\ns_name = str(sys.argv[1])\nt_num = int(sys.argv[2])\n\n\n#Cluster details\ncl_spec = tf.train.ClusterSpec({\n    \"worker\": [\n        \"localhost:2223\",\n        \"localhost:2224\"\n    ],\n    \"ps\": [\n        \"localhost:2222\"\n    ]\n})\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n\n#Decides whether parameter server or worker\nif s_name == \"ps\":\n    server = tf.train.Server(cl_spec,job_name=\"ps\",task_index=0,config=config)\n    server.join()\nelse:\n    server = tf.train.Server(cl_spec,job_name=\"worker\",task_index=t_num,config=config)\n\n    with tf.device(\"/job:worker/replica:0/task:0\"):\n        a = tf.Variable(tf.constant(5))\n        b = tf.Variable(tf.constant(6))\n        z = tf.add(a,b)\n        z_p = tf.Print([z],[z],\"sum: \")\n        tf.summary.histogram(\"t0sum\",z_p)\n\n    with tf.device(\"/job:ps/replica:0/task:0\"):    \n        with tf.name_scope(\"train_place_holder\"):\n            x = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_img1\")\n            y = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_a\")    \n        \n        global_step = tf.train.create_global_step()\n    \n    with tf.device(\"/job:worker/replica:0/task:1\"):\n        q = tf.FIFOQueue(capacity=25,\n                         dtypes= (tf.uint8,tf.uint8),\n                         shapes= (tf.TensorShape([1,1]),tf.TensorShape([1,1])),\n                         name=\"tq\",shared_name=\"train_queue\")\n        \n        enqueue_op = q.enqueue((x,y),name=\"enqueue\")\n        xx,yy = q.dequeue(name=\"dq\")\n        zz = tf.add(xx,yy)\n        tf.summary.histogram(\"t1sum: \",zz)\n\n    summ = tf.summary.merge_all()\n\n    lap_dir = r'C:\\Users\\Vishnu\\Documents\\EngProj\\bug_tf\\log'\n\n    summary_hook = tf.train.SummarySaverHook(   save_steps=1,save_secs=None,\n                                                    output_dir=lap_dir,summary_writer=None,\n                                                    scaffold=None,summary_op=summ)\n\n    if (t_num == 0):\n        # THis is chief worker\n        saver_hook = tf.train.CheckpointSaverHook(  checkpoint_dir=lap_dir,\n                                                    save_secs=3600,save_steps=None,\n                                                    saver=tf.train.Saver(),checkpoint_basename='model.ckpt',\n                                                    scaffold=None)\n\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=True,\n                                                hooks=[saver_hook,summary_hook], config=config) as sess:   \n           while True:\n               sess.run([z,z_p])\n\n    else:\n\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=False,\n                                                save_summaries_steps=1,config=config) as sess:\n            while not sess.should_stop():\n                sess.run([zz])\n\n    \n</code></pre>\n<p>You can run the above file and start 3 workers using the following command line commands.</p>\n<pre><code>python [file_name] worker 1\n\npython [file_name] worker 0\n\npython [file_name] ps 0\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): pip install tensorflow\nTensorFlow version (use command below): 1.8\nPython version: 3.6.5\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: No CUDA\nGPU model and memory: No GPU\nExact command to reproduce: Check code below\n\n\nDescribe the problem\nThere seems to be very little documentation on distributed tensorflow, so I am unsure of whether or not this is a bug or a logical error on my part. But the behavior is still a little strange.  Essentially, the issue seems to arise due to tf.train.SummarySaverHook(), it's interaction with tf.train.MoniteredTrainingSession() and the chief worker. If you look at my code below, you will see that there are two workers in this model. The chief worker is associated with \"task:0\". I've set up the model such that the queue and it operations live on \"task:1\"(not the chief worker). There is a simple sum operation on \"task:0\". Once I launch the 3 nodes(ps, task0, task1), task0 is able to complete one simple sum operation and then all tasks hang indefinitely.\nFor the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:0). By doing this, there don't seem to be any issues and the code works fine. I'm unsure of why this bypasses the issue. If chief workers handle summary write operations, shouldn't it still hang?\nThe strange behavior that I was talking about earlier occurs when you pass both the summary hook to the chief worker (Doing this will cause all workers to hang). You can get past the issue by simple removing the tf.FIFOqueue().dequeue() operation. It seems that this operation is getting called by \"task:0\" despite the fact that I've specified its location as \"task:1\" using tf.device. It is also being called despite not being an input requirement for any part of the graph that lives on \"task:0\".\nMy question is essentially: Is this a bug or a logical error on my part?\nSource code / logs\nimport tensorflow as tf \nimport sys\n\ns_name = str(sys.argv[1])\nt_num = int(sys.argv[2])\n\n\n#Cluster details\ncl_spec = tf.train.ClusterSpec({\n    \"worker\": [\n        \"localhost:2223\",\n        \"localhost:2224\"\n    ],\n    \"ps\": [\n        \"localhost:2222\"\n    ]\n})\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n\n#Decides whether parameter server or worker\nif s_name == \"ps\":\n    server = tf.train.Server(cl_spec,job_name=\"ps\",task_index=0,config=config)\n    server.join()\nelse:\n    server = tf.train.Server(cl_spec,job_name=\"worker\",task_index=t_num,config=config)\n\n    with tf.device(\"/job:worker/replica:0/task:0\"):\n        a = tf.Variable(tf.constant(5))\n        b = tf.Variable(tf.constant(6))\n        z = tf.add(a,b)\n        z_p = tf.Print([z],[z],\"sum: \")\n        tf.summary.histogram(\"t0sum\",z_p)\n\n    with tf.device(\"/job:ps/replica:0/task:0\"):    \n        with tf.name_scope(\"train_place_holder\"):\n            x = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_img1\")\n            y = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_a\")    \n        \n        global_step = tf.train.create_global_step()\n    \n    with tf.device(\"/job:worker/replica:0/task:1\"):\n        q = tf.FIFOQueue(capacity=25,\n                         dtypes= (tf.uint8,tf.uint8),\n                         shapes= (tf.TensorShape([1,1]),tf.TensorShape([1,1])),\n                         name=\"tq\",shared_name=\"train_queue\")\n        \n        enqueue_op = q.enqueue((x,y),name=\"enqueue\")\n        xx,yy = q.dequeue(name=\"dq\")\n        zz = tf.add(xx,yy)\n        tf.summary.histogram(\"t1sum: \",zz)\n\n    summ = tf.summary.merge_all()\n\n    lap_dir = r'C:\\Users\\Vishnu\\Documents\\EngProj\\bug_tf\\log'\n\n    summary_hook = tf.train.SummarySaverHook(   save_steps=1,save_secs=None,\n                                                    output_dir=lap_dir,summary_writer=None,\n                                                    scaffold=None,summary_op=summ)\n\n    if (t_num == 0):\n        # THis is chief worker\n        saver_hook = tf.train.CheckpointSaverHook(  checkpoint_dir=lap_dir,\n                                                    save_secs=3600,save_steps=None,\n                                                    saver=tf.train.Saver(),checkpoint_basename='model.ckpt',\n                                                    scaffold=None)\n\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=True,\n                                                hooks=[saver_hook,summary_hook], config=config) as sess:   \n           while True:\n               sess.run([z,z_p])\n\n    else:\n\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=False,\n                                                save_summaries_steps=1,config=config) as sess:\n            while not sess.should_stop():\n                sess.run([zz])\n\n    \n\nYou can run the above file and start 3 workers using the following command line commands.\npython [file_name] worker 1\n\npython [file_name] worker 0\n\npython [file_name] ps 0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow \r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: No CUDA\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: Check code below\r\n\r\n\r\n------------------------\r\n### Describe the problem\r\nThere seems to be very little documentation on distributed tensorflow, so I am unsure of whether or not this is a bug or a logical error on my part. But the behavior is still a little strange.  Essentially, the issue seems to arise due to tf.train.SummarySaverHook(), it's interaction with tf.train.MoniteredTrainingSession() and the chief worker. If you look at my code below, you will see that there are two workers in this model. The chief worker is associated with \"task:0\". I've set up the model such that the queue and it operations live on \"task:1\"(not the chief worker). There is a simple sum operation on \"task:0\". Once I launch the 3 nodes(ps, task0, task1), task0 is able to complete one simple sum operation and then all tasks hang indefinitely.  \r\n\r\nFor the moment, I've bypassed this issue by passing the summary hook object to the non-chief worker(task:0). By doing this, there don't seem to be any issues and the code works fine. I'm unsure of why this bypasses the issue. If chief workers handle summary write operations, shouldn't it still hang?\r\n\r\nThe strange behavior that I was talking about earlier occurs when you pass both the summary hook to the chief worker (Doing this will cause all workers to hang). You can get past the issue by simple removing the tf.FIFOqueue().dequeue() operation. It seems that this operation is getting called by \"task:0\" despite the fact that I've specified its location as \"task:1\" using tf.device. It is also being called despite not being an input requirement for any part of the graph that lives on \"task:0\". \r\n\r\nMy question is essentially: Is this a bug or a logical error on my part?\r\n\r\n### Source code / logs\r\n\r\n\r\n```\r\nimport tensorflow as tf \r\nimport sys\r\n\r\ns_name = str(sys.argv[1])\r\nt_num = int(sys.argv[2])\r\n\r\n\r\n#Cluster details\r\ncl_spec = tf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"localhost:2223\",\r\n        \"localhost:2224\"\r\n    ],\r\n    \"ps\": [\r\n        \"localhost:2222\"\r\n    ]\r\n})\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\n\r\n#Decides whether parameter server or worker\r\nif s_name == \"ps\":\r\n    server = tf.train.Server(cl_spec,job_name=\"ps\",task_index=0,config=config)\r\n    server.join()\r\nelse:\r\n    server = tf.train.Server(cl_spec,job_name=\"worker\",task_index=t_num,config=config)\r\n\r\n    with tf.device(\"/job:worker/replica:0/task:0\"):\r\n        a = tf.Variable(tf.constant(5))\r\n        b = tf.Variable(tf.constant(6))\r\n        z = tf.add(a,b)\r\n        z_p = tf.Print([z],[z],\"sum: \")\r\n        tf.summary.histogram(\"t0sum\",z_p)\r\n\r\n    with tf.device(\"/job:ps/replica:0/task:0\"):    \r\n        with tf.name_scope(\"train_place_holder\"):\r\n            x = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_img1\")\r\n            y = tf.placeholder(tf.uint8,shape=[1,1],name=\"s_a\")    \r\n        \r\n        global_step = tf.train.create_global_step()\r\n    \r\n    with tf.device(\"/job:worker/replica:0/task:1\"):\r\n        q = tf.FIFOQueue(capacity=25,\r\n                         dtypes= (tf.uint8,tf.uint8),\r\n                         shapes= (tf.TensorShape([1,1]),tf.TensorShape([1,1])),\r\n                         name=\"tq\",shared_name=\"train_queue\")\r\n        \r\n        enqueue_op = q.enqueue((x,y),name=\"enqueue\")\r\n        xx,yy = q.dequeue(name=\"dq\")\r\n        zz = tf.add(xx,yy)\r\n        tf.summary.histogram(\"t1sum: \",zz)\r\n\r\n    summ = tf.summary.merge_all()\r\n\r\n    lap_dir = r'C:\\Users\\Vishnu\\Documents\\EngProj\\bug_tf\\log'\r\n\r\n    summary_hook = tf.train.SummarySaverHook(   save_steps=1,save_secs=None,\r\n                                                    output_dir=lap_dir,summary_writer=None,\r\n                                                    scaffold=None,summary_op=summ)\r\n\r\n    if (t_num == 0):\r\n        # THis is chief worker\r\n        saver_hook = tf.train.CheckpointSaverHook(  checkpoint_dir=lap_dir,\r\n                                                    save_secs=3600,save_steps=None,\r\n                                                    saver=tf.train.Saver(),checkpoint_basename='model.ckpt',\r\n                                                    scaffold=None)\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=True,\r\n                                                hooks=[saver_hook,summary_hook], config=config) as sess:   \r\n           while True:\r\n               sess.run([z,z_p])\r\n\r\n    else:\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,is_chief=False,\r\n                                                save_summaries_steps=1,config=config) as sess:\r\n            while not sess.should_stop():\r\n                sess.run([zz])\r\n\r\n    \r\n```\r\n\r\n\r\nYou can run the above file and start 3 workers using the following command line commands.\r\n\r\n```\r\npython [file_name] worker 1\r\n\r\npython [file_name] worker 0\r\n\r\npython [file_name] ps 0\r\n```"}