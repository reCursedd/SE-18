{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/363011944", "html_url": "https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-363011944", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16768", "id": 363011944, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MzAxMTk0NA==", "user": {"login": "unwritten", "id": 1272812, "node_id": "MDQ6VXNlcjEyNzI4MTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1272812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/unwritten", "html_url": "https://github.com/unwritten", "followers_url": "https://api.github.com/users/unwritten/followers", "following_url": "https://api.github.com/users/unwritten/following{/other_user}", "gists_url": "https://api.github.com/users/unwritten/gists{/gist_id}", "starred_url": "https://api.github.com/users/unwritten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/unwritten/subscriptions", "organizations_url": "https://api.github.com/users/unwritten/orgs", "repos_url": "https://api.github.com/users/unwritten/repos", "events_url": "https://api.github.com/users/unwritten/events{/privacy}", "received_events_url": "https://api.github.com/users/unwritten/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-05T08:22:30Z", "updated_at": "2018-02-06T03:39:44Z", "author_association": "NONE", "body_html": "<p>yes, I manually placed the training set, here on my machine n_gpu is 2, tower_encoder_inputs and tower_decoder_inputs is splitted according to n_gpu</p>\n<pre><code>    for i in range(n_gpu):\n        with tf.variable_scope(variable_scope, reuse=True if i &gt; 0 else reuse):\n            with tf.device(\"/gpu:{}\".format(i)):\n                with tf.name_scope(\"trainig_gpu_{}\".format(i)):\n                    print(\"building train graph on GPU {}\".format(i))\n\n                    #outputs, attentions, stop_tokens, n_predicted = self.seq2seq(\n                    outputs, attentions= seq2seq(\n                        self.tower_encoder_inputs[i],\n                        self.tower_decoder_inputs[i])\n</code></pre>\n<p>thx</p>", "body_text": "yes, I manually placed the training set, here on my machine n_gpu is 2, tower_encoder_inputs and tower_decoder_inputs is splitted according to n_gpu\n    for i in range(n_gpu):\n        with tf.variable_scope(variable_scope, reuse=True if i > 0 else reuse):\n            with tf.device(\"/gpu:{}\".format(i)):\n                with tf.name_scope(\"trainig_gpu_{}\".format(i)):\n                    print(\"building train graph on GPU {}\".format(i))\n\n                    #outputs, attentions, stop_tokens, n_predicted = self.seq2seq(\n                    outputs, attentions= seq2seq(\n                        self.tower_encoder_inputs[i],\n                        self.tower_decoder_inputs[i])\n\nthx", "body": "yes, I manually placed the training set, here on my machine n_gpu is 2, tower_encoder_inputs and tower_decoder_inputs is splitted according to n_gpu\r\n\r\n        for i in range(n_gpu):\r\n            with tf.variable_scope(variable_scope, reuse=True if i > 0 else reuse):\r\n                with tf.device(\"/gpu:{}\".format(i)):\r\n                    with tf.name_scope(\"trainig_gpu_{}\".format(i)):\r\n                        print(\"building train graph on GPU {}\".format(i))\r\n\r\n                        #outputs, attentions, stop_tokens, n_predicted = self.seq2seq(\r\n                        outputs, attentions= seq2seq(\r\n                            self.tower_encoder_inputs[i],\r\n                            self.tower_decoder_inputs[i])\r\n\r\n\r\n\r\nthx\r\n"}