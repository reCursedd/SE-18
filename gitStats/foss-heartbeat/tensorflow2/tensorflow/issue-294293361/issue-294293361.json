{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16768", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16768/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16768/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16768/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16768", "id": 294293361, "node_id": "MDU6SXNzdWUyOTQyOTMzNjE=", "number": 16768, "title": "OOM when allocating tensor with shape", "user": {"login": "unwritten", "id": 1272812, "node_id": "MDQ6VXNlcjEyNzI4MTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1272812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/unwritten", "html_url": "https://github.com/unwritten", "followers_url": "https://api.github.com/users/unwritten/followers", "following_url": "https://api.github.com/users/unwritten/following{/other_user}", "gists_url": "https://api.github.com/users/unwritten/gists{/gist_id}", "starred_url": "https://api.github.com/users/unwritten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/unwritten/subscriptions", "organizations_url": "https://api.github.com/users/unwritten/orgs", "repos_url": "https://api.github.com/users/unwritten/repos", "events_url": "https://api.github.com/users/unwritten/events{/privacy}", "received_events_url": "https://api.github.com/users/unwritten/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2018-02-05T07:10:39Z", "updated_at": "2018-09-13T00:10:51Z", "closed_at": "2018-02-06T08:30:56Z", "author_association": "NONE", "body_html": "<p>I'm using tensorflow 1.3, tested on a linux machine with 2 NVIDIA Tesla K80 cards,<br>\nhowever, I keep getting OOM error on GPU, but it does not happen when using cpu for training:</p>\n<hr>\n<h2>---log below</h2>\n<p>Exception happened during training, message: OOM when allocating tensor with shape[1792,4096]<br>\n[[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]<br>\n[[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]</p>\n<p>Caused by op 'projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1', defined at:<br>\nFile \"sync_train.py\", line 383, in <br>\nmain()<br>\nFile \"sync_train.py\", line 380, in main<br>\ntrain(config)<br>\nFile \"sync_train.py\", line 64, in train<br>\ntrain_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)<br>\nFile \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 291, in <strong>init</strong><br>\ngrads_and_vars = self.optimizer.compute_gradients(loss)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients<br>\ncolocate_gradients_with_ops=colocate_gradients_with_ops)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients<br>\ngrad_scope, op, func_call, lambda: grad_fn(op, *out_grads))<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile<br>\nreturn grad_fn()  # Exit early<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <br>\ngrad_scope, op, func_call, lambda: grad_fn(op, *out_grads))<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py\", line 922, in _MatMulGrad<br>\ngrad_b = math_ops.matmul(a, grad, transpose_a=True)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul<br>\na, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul<br>\nname=name)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper<br>\nop_def=op_def)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op<br>\nop_def=op_def)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in <strong>init</strong><br>\nself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access</p>\n<p>...which was originally created as op 'projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36', defined at:<br>\nFile \"sync_train.py\", line 383, in <br>\nmain()<br>\n[elided 1 identical lines from previous traceback]<br>\nFile \"sync_train.py\", line 64, in train<br>\ntrain_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)<br>\nFile \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 189, in <strong>init</strong><br>\nfeed_previous=feed_previous)<br>\nFile \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 427, in seq2seq<br>\npre_alignments)<br>\nFile \"/kaldi/exp/tacotron/exp_2/decoder.py\", line 99, in <strong>call</strong><br>\nattention_rnn_outputs, new_attention_rnn_state, context, alignments = self._attention_rnn_cell(prenet_output, state, pre_alignments)<br>\nFile \"/kaldi/exp/tacotron/exp_2/attention.py\", line 427, in <strong>call</strong><br>\nlstm_output, next_lstm_state = cell(lstm_inputs, states[i + 1])<br>\nFile \"/kaldi/exp/tacotron/exp_2/zoneout_lstm.py\", line 48, in <strong>call</strong><br>\noutput, new_state = self._cell(inputs, state, scope)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in <strong>call</strong><br>\nreturn super(RNNCell, self).<strong>call</strong>(inputs, state)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in <strong>call</strong><br>\noutputs = self.call(inputs, *args, **kwargs)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 611, in call<br>\nlstm_matrix = self._linear1([inputs, m_prev])<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in <strong>call</strong><br>\nres = math_ops.matmul(array_ops.concat(args, 1), self._weights)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul<br>\na, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul<br>\nname=name)</p>\n<p>ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1792,4096]<br>\n[[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]<br>\n[[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]</p>", "body_text": "I'm using tensorflow 1.3, tested on a linux machine with 2 NVIDIA Tesla K80 cards,\nhowever, I keep getting OOM error on GPU, but it does not happen when using cpu for training:\n\n---log below\nException happened during training, message: OOM when allocating tensor with shape[1792,4096]\n[[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]\n[[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\nCaused by op 'projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1', defined at:\nFile \"sync_train.py\", line 383, in \nmain()\nFile \"sync_train.py\", line 380, in main\ntrain(config)\nFile \"sync_train.py\", line 64, in train\ntrain_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)\nFile \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 291, in init\ngrads_and_vars = self.optimizer.compute_gradients(loss)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\ncolocate_gradients_with_ops=colocate_gradients_with_ops)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\ngrad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\nreturn grad_fn()  # Exit early\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in \ngrad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py\", line 922, in _MatMulGrad\ngrad_b = math_ops.matmul(a, grad, transpose_a=True)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\na, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\nname=name)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\nop_def=op_def)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\nop_def=op_def)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in init\nself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n...which was originally created as op 'projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36', defined at:\nFile \"sync_train.py\", line 383, in \nmain()\n[elided 1 identical lines from previous traceback]\nFile \"sync_train.py\", line 64, in train\ntrain_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)\nFile \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 189, in init\nfeed_previous=feed_previous)\nFile \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 427, in seq2seq\npre_alignments)\nFile \"/kaldi/exp/tacotron/exp_2/decoder.py\", line 99, in call\nattention_rnn_outputs, new_attention_rnn_state, context, alignments = self._attention_rnn_cell(prenet_output, state, pre_alignments)\nFile \"/kaldi/exp/tacotron/exp_2/attention.py\", line 427, in call\nlstm_output, next_lstm_state = cell(lstm_inputs, states[i + 1])\nFile \"/kaldi/exp/tacotron/exp_2/zoneout_lstm.py\", line 48, in call\noutput, new_state = self._cell(inputs, state, scope)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in call\nreturn super(RNNCell, self).call(inputs, state)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in call\noutputs = self.call(inputs, *args, **kwargs)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 611, in call\nlstm_matrix = self._linear1([inputs, m_prev])\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in call\nres = math_ops.matmul(array_ops.concat(args, 1), self._weights)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\na, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\nname=name)\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1792,4096]\n[[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]\n[[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]", "body": "I'm using tensorflow 1.3, tested on a linux machine with 2 NVIDIA Tesla K80 cards,\r\nhowever, I keep getting OOM error on GPU, but it does not happen when using cpu for training:\r\n\r\n-------------------------------------\r\n---log below\r\n-------------------------------------\r\n\r\nException happened during training, message: OOM when allocating tensor with shape[1792,4096]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1', defined at:\r\n  File \"sync_train.py\", line 383, in <module>\r\n    main()\r\n  File \"sync_train.py\", line 380, in main\r\n    train(config)\r\n  File \"sync_train.py\", line 64, in train\r\n    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)\r\n  File \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 291, in __init__\r\n    grads_and_vars = self.optimizer.compute_gradients(loss)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py\", line 922, in _MatMulGrad\r\n    grad_b = math_ops.matmul(a, grad, transpose_a=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36', defined at:\r\n  File \"sync_train.py\", line 383, in <module>\r\n    main()\r\n[elided 1 identical lines from previous traceback]\r\n  File \"sync_train.py\", line 64, in train\r\n    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)\r\n  File \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 189, in __init__\r\n    feed_previous=feed_previous)\r\n  File \"/kaldi/exp/tacotron/exp_2/projectx.py\", line 427, in seq2seq\r\n    pre_alignments)\r\n  File \"/kaldi/exp/tacotron/exp_2/decoder.py\", line 99, in __call__\r\n    attention_rnn_outputs, new_attention_rnn_state, context, alignments = self._attention_rnn_cell(prenet_output, state, pre_alignments)\r\n  File \"/kaldi/exp/tacotron/exp_2/attention.py\", line 427, in __call__\r\n    lstm_output, next_lstm_state = cell(lstm_inputs, states[i + 1])\r\n  File \"/kaldi/exp/tacotron/exp_2/zoneout_lstm.py\", line 48, in __call__\r\n    output, new_state = self._cell(inputs, state, scope)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 611, in call\r\n    lstm_matrix = self._linear1([inputs, m_prev])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in __call__\r\n    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\r\n    name=name)\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1792,4096]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]\r\n\t [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_646323_projectx/trainig_gpu_0/gradients/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"}