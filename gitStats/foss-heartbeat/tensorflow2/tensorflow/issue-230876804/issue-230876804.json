{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10147", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10147/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10147/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10147/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10147", "id": 230876804, "node_id": "MDU6SXNzdWUyMzA4NzY4MDQ=", "number": 10147, "title": "Strange Behavior During Grid Search", "user": {"login": "todpole3", "id": 4227871, "node_id": "MDQ6VXNlcjQyMjc4NzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4227871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/todpole3", "html_url": "https://github.com/todpole3", "followers_url": "https://api.github.com/users/todpole3/followers", "following_url": "https://api.github.com/users/todpole3/following{/other_user}", "gists_url": "https://api.github.com/users/todpole3/gists{/gist_id}", "starred_url": "https://api.github.com/users/todpole3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/todpole3/subscriptions", "organizations_url": "https://api.github.com/users/todpole3/orgs", "repos_url": "https://api.github.com/users/todpole3/repos", "events_url": "https://api.github.com/users/todpole3/events{/privacy}", "received_events_url": "https://api.github.com/users/todpole3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-05-23T23:20:28Z", "updated_at": "2017-05-23T23:29:24Z", "closed_at": "2017-05-23T23:29:24Z", "author_association": "NONE", "body_html": "<p>I implemented a customized grid search wrapper for a sequence-to-sequence model. The implementation is extremely simple:</p>\n<pre><code>def single_round_model_eval(train_fun, decode_fun, eval_fun,\n                            train_set, dev_set, metrics):\n    # Train the model with a certain set of hyperparameters and evaluate on the\n    # development set.\n\n    # :param train_fun: Function to train the model.\n    # :param decode_fun: Function to decode from the trained model.\n    # :param eval_fun: Function to evaluate the decoding results.\n    # :param train_set: Training dataset.\n    # :param dev_set: Development dataset.\n    # :param metrics: Name of the evaluation metrics to be tuned.\n\n    # :return: The metrics being tuned.\n\n    tf.reset_default_graph()\n    train_fun(train_set, dev_set)\n\n    tf.reset_default_graph()\n    decode_sig = decode_fun(dev_set, verbose=False)\n\n    M = eval_fun(dev_set, decode_sig, verbose=False)\n\n    return M[metrics]\n</code></pre>\n<p>The algorithm basically calls the above function every time it moves to a new point in the grid (a new set of hyperparameters).</p>\n<p>The complete implementation can be found here:<br>\n<a href=\"https://github.com/TellinaTool/awesome_nmt/blob/master/encoder_decoder/grid_search.py\">https://github.com/TellinaTool/awesome_nmt/blob/master/encoder_decoder/grid_search.py</a></p>\n<p>The <code>single_round_model_eval</code> creates a training graph, trains the model; then creates a \"forward only\" decoding graph, and decode the predictions on the dev set; finally evaluates the newly decoded predictions (no graph operations is used in the evaluation step).</p>\n<p>Yet I encountered a strange behavior that my subsequent runs always achieve much worse numbers compared to the first run. I didn't find problems in hyperparameter settings, and think this may be caused by that some variables carries residue values from the previous run. I tried to use <code>tf.reset_default_graph()</code> to force reset but that also doesn't help. I'm also wondering if it is caused by missing resets of the optimizer I used (tf.train.AdamOptimizer).</p>\n<p>Could anyone offer some help? Thanks!</p>", "body_text": "I implemented a customized grid search wrapper for a sequence-to-sequence model. The implementation is extremely simple:\ndef single_round_model_eval(train_fun, decode_fun, eval_fun,\n                            train_set, dev_set, metrics):\n    # Train the model with a certain set of hyperparameters and evaluate on the\n    # development set.\n\n    # :param train_fun: Function to train the model.\n    # :param decode_fun: Function to decode from the trained model.\n    # :param eval_fun: Function to evaluate the decoding results.\n    # :param train_set: Training dataset.\n    # :param dev_set: Development dataset.\n    # :param metrics: Name of the evaluation metrics to be tuned.\n\n    # :return: The metrics being tuned.\n\n    tf.reset_default_graph()\n    train_fun(train_set, dev_set)\n\n    tf.reset_default_graph()\n    decode_sig = decode_fun(dev_set, verbose=False)\n\n    M = eval_fun(dev_set, decode_sig, verbose=False)\n\n    return M[metrics]\n\nThe algorithm basically calls the above function every time it moves to a new point in the grid (a new set of hyperparameters).\nThe complete implementation can be found here:\nhttps://github.com/TellinaTool/awesome_nmt/blob/master/encoder_decoder/grid_search.py\nThe single_round_model_eval creates a training graph, trains the model; then creates a \"forward only\" decoding graph, and decode the predictions on the dev set; finally evaluates the newly decoded predictions (no graph operations is used in the evaluation step).\nYet I encountered a strange behavior that my subsequent runs always achieve much worse numbers compared to the first run. I didn't find problems in hyperparameter settings, and think this may be caused by that some variables carries residue values from the previous run. I tried to use tf.reset_default_graph() to force reset but that also doesn't help. I'm also wondering if it is caused by missing resets of the optimizer I used (tf.train.AdamOptimizer).\nCould anyone offer some help? Thanks!", "body": "I implemented a customized grid search wrapper for a sequence-to-sequence model. The implementation is extremely simple:\r\n\r\n```\r\ndef single_round_model_eval(train_fun, decode_fun, eval_fun,\r\n                            train_set, dev_set, metrics):\r\n    # Train the model with a certain set of hyperparameters and evaluate on the\r\n    # development set.\r\n\r\n    # :param train_fun: Function to train the model.\r\n    # :param decode_fun: Function to decode from the trained model.\r\n    # :param eval_fun: Function to evaluate the decoding results.\r\n    # :param train_set: Training dataset.\r\n    # :param dev_set: Development dataset.\r\n    # :param metrics: Name of the evaluation metrics to be tuned.\r\n\r\n    # :return: The metrics being tuned.\r\n\r\n    tf.reset_default_graph()\r\n    train_fun(train_set, dev_set)\r\n\r\n    tf.reset_default_graph()\r\n    decode_sig = decode_fun(dev_set, verbose=False)\r\n\r\n    M = eval_fun(dev_set, decode_sig, verbose=False)\r\n\r\n    return M[metrics]\r\n```\r\n\r\nThe algorithm basically calls the above function every time it moves to a new point in the grid (a new set of hyperparameters).\r\n\r\nThe complete implementation can be found here:\r\nhttps://github.com/TellinaTool/awesome_nmt/blob/master/encoder_decoder/grid_search.py\r\n\r\nThe `single_round_model_eval` creates a training graph, trains the model; then creates a \"forward only\" decoding graph, and decode the predictions on the dev set; finally evaluates the newly decoded predictions (no graph operations is used in the evaluation step). \r\n\r\nYet I encountered a strange behavior that my subsequent runs always achieve much worse numbers compared to the first run. I didn't find problems in hyperparameter settings, and think this may be caused by that some variables carries residue values from the previous run. I tried to use `tf.reset_default_graph()` to force reset but that also doesn't help. I'm also wondering if it is caused by missing resets of the optimizer I used (tf.train.AdamOptimizer).\r\n\r\nCould anyone offer some help? Thanks!"}