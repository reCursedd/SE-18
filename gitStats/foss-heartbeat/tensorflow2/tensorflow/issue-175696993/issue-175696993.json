{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4272", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4272/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4272/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4272/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4272", "id": 175696993, "node_id": "MDU6SXNzdWUxNzU2OTY5OTM=", "number": 4272, "title": "Training time for CIFAR-10 is higher on multi GPU cards compared to single", "user": {"login": "dashrathc", "id": 7776101, "node_id": "MDQ6VXNlcjc3NzYxMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/7776101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashrathc", "html_url": "https://github.com/dashrathc", "followers_url": "https://api.github.com/users/dashrathc/followers", "following_url": "https://api.github.com/users/dashrathc/following{/other_user}", "gists_url": "https://api.github.com/users/dashrathc/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashrathc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashrathc/subscriptions", "organizations_url": "https://api.github.com/users/dashrathc/orgs", "repos_url": "https://api.github.com/users/dashrathc/repos", "events_url": "https://api.github.com/users/dashrathc/events{/privacy}", "received_events_url": "https://api.github.com/users/dashrathc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-09-08T08:42:26Z", "updated_at": "2017-08-30T22:10:46Z", "closed_at": "2016-11-01T17:55:08Z", "author_association": "NONE", "body_html": "<p>When running CIFAR-10 example on two GPU cards in a single machine, the training time is higher compared to a single GPU card.</p>\n<p>It is utilizing both cards but overall execution time is higher compared to single GPU.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/7776101/18342617/48dfeb66-75cd-11e6-83cb-473be9746f3a.png\"><img src=\"https://cloud.githubusercontent.com/assets/7776101/18342617/48dfeb66-75cd-11e6-83cb-473be9746f3a.png\" alt=\"multi-gpu-cards\" style=\"max-width:100%;\"></a></p>\n<p>One more thing I noticed is that the queue is filled twice. I got following messages in log.<br>\n<code>Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.</code><br>\n<code>Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.</code></p>\n<p>Ideally, It should be filled once and distribute across two GPUs but that is not happening.<br>\nCould anyone please help me out to resolve this?</p>", "body_text": "When running CIFAR-10 example on two GPU cards in a single machine, the training time is higher compared to a single GPU card.\nIt is utilizing both cards but overall execution time is higher compared to single GPU.\n\nOne more thing I noticed is that the queue is filled twice. I got following messages in log.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nIdeally, It should be filled once and distribute across two GPUs but that is not happening.\nCould anyone please help me out to resolve this?", "body": "When running CIFAR-10 example on two GPU cards in a single machine, the training time is higher compared to a single GPU card.\n\nIt is utilizing both cards but overall execution time is higher compared to single GPU.\n\n![multi-gpu-cards](https://cloud.githubusercontent.com/assets/7776101/18342617/48dfeb66-75cd-11e6-83cb-473be9746f3a.png)\n\nOne more thing I noticed is that the queue is filled twice. I got following messages in log.\n`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.`\n`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.`\n\nIdeally, It should be filled once and distribute across two GPUs but that is not happening. \nCould anyone please help me out to resolve this?\n"}