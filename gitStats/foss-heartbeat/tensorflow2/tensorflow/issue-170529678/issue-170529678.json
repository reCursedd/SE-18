{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3739", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3739/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3739/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3739/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3739", "id": 170529678, "node_id": "MDU6SXNzdWUxNzA1Mjk2Nzg=", "number": 3739, "title": "Cross entropy should give targets out of range error as given by tf.nn.in_top_k", "user": {"login": "inderpreetsganda", "id": 10248819, "node_id": "MDQ6VXNlcjEwMjQ4ODE5", "avatar_url": "https://avatars0.githubusercontent.com/u/10248819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/inderpreetsganda", "html_url": "https://github.com/inderpreetsganda", "followers_url": "https://api.github.com/users/inderpreetsganda/followers", "following_url": "https://api.github.com/users/inderpreetsganda/following{/other_user}", "gists_url": "https://api.github.com/users/inderpreetsganda/gists{/gist_id}", "starred_url": "https://api.github.com/users/inderpreetsganda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/inderpreetsganda/subscriptions", "organizations_url": "https://api.github.com/users/inderpreetsganda/orgs", "repos_url": "https://api.github.com/users/inderpreetsganda/repos", "events_url": "https://api.github.com/users/inderpreetsganda/events{/privacy}", "received_events_url": "https://api.github.com/users/inderpreetsganda/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-08-10T22:02:55Z", "updated_at": "2018-01-19T15:24:28Z", "closed_at": "2016-08-11T18:47:53Z", "author_association": "NONE", "body_html": "<p>I ran the tensorflow code(given below) and it gave me error(error stack is below code) targets out of range.</p>\n<p>I have figured out what was causing this error, it was due to mismatch between labels and outputs, like I'm doing 8 class sentiment classification and my labels are <code>(1,2,3,4,7,8,9,10)</code> so it was unable to match predictions<code>(1,2,3,4,5,6,7,8)</code> with my labels, so that's why it was giving out of range error. My question is, why it didn't gave me error in this line <code>c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)</code> , how it's matching labels with predictions in this case as opposed to in in_top_k? I think c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y) should give me error because predictions and labels are not same. Why I'm not getting targets out of range error in cross entropy function?</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nfrom nltk.tokenize import TweetTokenizer\nbatch = 500\nstart = 0\nend = batch - 1\nlearning_rate = 0.2\nnum_classes = 8\npath = \"/home/indy/Downloads/aclImdb/train/pos\"\ntime_steps = 250\nembedding = 50\n\ndef get_embedding():\n    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")\n    f = open(gfile_path,'r')\n    embeddings = {}\n    for line in f:\n        sp_value = line.split()\n        word = sp_value[0]\n        embedding = [float(value) for value in sp_value[1:]]\n        embeddings[word] = embedding\n    return embeddings\n\nebd = get_embedding()\n\ndef get_y(file_name):\n    y_value = file_name.split('_')\n    y_value = y_value[1].split('.')\n    return y_value[0] \n\ndef get_x(path,file_name):\n    file_path = os.path.join(path,file_name)\n    x_value = open(file_path,'r')\n    for line in x_value:\n        x_value = line.replace(\"&lt;br /&gt;&lt;br /&gt;\",\"\") \n        x_value = x_value.lower()\n    tokeniz = TweetTokenizer()\n    x_value = tokeniz.tokenize(x_value)\n    padding = 250 - len(x_value)\n    if padding &gt; 0:\n       p_value = ['pad' for i in range(padding)]\n       x_value = np.concatenate((x_value,p_value))\n    x_value = [ebd['value'] for value in x_value]\n\n    return x_value\n\ndef  batch_f(path):\n     directory = os.listdir(path)\n     y = [get_y(directory[i]) for i in range(len(directory))]\n     x = [get_x(path,directory[i]) for i in range(len(directory))]    \n     return x,y\n\n\nX = tf.placeholder(tf.float32, [batch,time_steps,embedding])\nY = tf.placeholder(tf.int32, [batch])\n\ndef build_nlp_model(x, _units, lstm_layers,num_classes):\n\n     x = tf.transpose(x, [1, 0, 2])\n     x = tf.reshape(x, [-1, embedding])\n     x = tf.split(0, time_steps, x)\n\n\n     lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units, state_is_tuple = True)\n\n     multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\n     outputs , state = tf.nn.rnn(multi_lstm,x, dtype = tf.float32)     \n\n     weights = tf.Variable(tf.random_normal([_units,num_classes]))\n     biases  = tf.Variable(tf.random_normal([num_classes]))\n\n     logits = tf.matmul(outputs[-1], weights) + biases\n     return logits\n\nlogits = build_nlp_model(X,400,4,num_classes)\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\n\ndecayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(decayed_learning_rate)\nminimize_loss = optimizer.minimize(loss)\n\n\n\ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(25):\n         x, y = batch_f(path)\n         sess.run(minimize_loss,feed_dict={X : x, Y : y})\n         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n         cost = sess.run(loss,feed_dict = {X: x,Y: y})\n         start = end \n         end = (start + batch)\n         print (\"Minibatch Loss = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n</code></pre>\n<p>This is the error stack that is caused by tf.nn.in_top_k.</p>\n<pre><code>(500, 250, 50)\n(500,)\nTraceback (most recent call last):\n  File \"nlp.py\", line 115, in &lt;module&gt;\n    accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: targets[0] is out of range\n     [[Node: InTopK = InTopK[T=DT_INT32, k=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add, _recv_Placeholder_1_0)]]\nCaused by op u'InTopK', defined at:\n  File \"nlp.py\", line 102, in &lt;module&gt;\n    correct_predict = tf.nn.in_top_k(logits, Y, 1)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 890, in in_top_k\n    targets=targets, k=k, name=name)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n</code></pre>\n<p>And I think this type of error(targets out of range) should be given by cross entropy also, when labels don't match with predictions.</p>", "body_text": "I ran the tensorflow code(given below) and it gave me error(error stack is below code) targets out of range.\nI have figured out what was causing this error, it was due to mismatch between labels and outputs, like I'm doing 8 class sentiment classification and my labels are (1,2,3,4,7,8,9,10) so it was unable to match predictions(1,2,3,4,5,6,7,8) with my labels, so that's why it was giving out of range error. My question is, why it didn't gave me error in this line c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y) , how it's matching labels with predictions in this case as opposed to in in_top_k? I think c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y) should give me error because predictions and labels are not same. Why I'm not getting targets out of range error in cross entropy function?\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nfrom nltk.tokenize import TweetTokenizer\nbatch = 500\nstart = 0\nend = batch - 1\nlearning_rate = 0.2\nnum_classes = 8\npath = \"/home/indy/Downloads/aclImdb/train/pos\"\ntime_steps = 250\nembedding = 50\n\ndef get_embedding():\n    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")\n    f = open(gfile_path,'r')\n    embeddings = {}\n    for line in f:\n        sp_value = line.split()\n        word = sp_value[0]\n        embedding = [float(value) for value in sp_value[1:]]\n        embeddings[word] = embedding\n    return embeddings\n\nebd = get_embedding()\n\ndef get_y(file_name):\n    y_value = file_name.split('_')\n    y_value = y_value[1].split('.')\n    return y_value[0] \n\ndef get_x(path,file_name):\n    file_path = os.path.join(path,file_name)\n    x_value = open(file_path,'r')\n    for line in x_value:\n        x_value = line.replace(\"<br /><br />\",\"\") \n        x_value = x_value.lower()\n    tokeniz = TweetTokenizer()\n    x_value = tokeniz.tokenize(x_value)\n    padding = 250 - len(x_value)\n    if padding > 0:\n       p_value = ['pad' for i in range(padding)]\n       x_value = np.concatenate((x_value,p_value))\n    x_value = [ebd['value'] for value in x_value]\n\n    return x_value\n\ndef  batch_f(path):\n     directory = os.listdir(path)\n     y = [get_y(directory[i]) for i in range(len(directory))]\n     x = [get_x(path,directory[i]) for i in range(len(directory))]    \n     return x,y\n\n\nX = tf.placeholder(tf.float32, [batch,time_steps,embedding])\nY = tf.placeholder(tf.int32, [batch])\n\ndef build_nlp_model(x, _units, lstm_layers,num_classes):\n\n     x = tf.transpose(x, [1, 0, 2])\n     x = tf.reshape(x, [-1, embedding])\n     x = tf.split(0, time_steps, x)\n\n\n     lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units, state_is_tuple = True)\n\n     multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\n     outputs , state = tf.nn.rnn(multi_lstm,x, dtype = tf.float32)     \n\n     weights = tf.Variable(tf.random_normal([_units,num_classes]))\n     biases  = tf.Variable(tf.random_normal([num_classes]))\n\n     logits = tf.matmul(outputs[-1], weights) + biases\n     return logits\n\nlogits = build_nlp_model(X,400,4,num_classes)\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\n\ndecayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(decayed_learning_rate)\nminimize_loss = optimizer.minimize(loss)\n\n\n\ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(25):\n         x, y = batch_f(path)\n         sess.run(minimize_loss,feed_dict={X : x, Y : y})\n         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n         cost = sess.run(loss,feed_dict = {X: x,Y: y})\n         start = end \n         end = (start + batch)\n         print (\"Minibatch Loss = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n\nThis is the error stack that is caused by tf.nn.in_top_k.\n(500, 250, 50)\n(500,)\nTraceback (most recent call last):\n  File \"nlp.py\", line 115, in <module>\n    accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: targets[0] is out of range\n     [[Node: InTopK = InTopK[T=DT_INT32, k=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add, _recv_Placeholder_1_0)]]\nCaused by op u'InTopK', defined at:\n  File \"nlp.py\", line 102, in <module>\n    correct_predict = tf.nn.in_top_k(logits, Y, 1)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 890, in in_top_k\n    targets=targets, k=k, name=name)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n\nAnd I think this type of error(targets out of range) should be given by cross entropy also, when labels don't match with predictions.", "body": "I ran the tensorflow code(given below) and it gave me error(error stack is below code) targets out of range.\n\nI have figured out what was causing this error, it was due to mismatch between labels and outputs, like I'm doing 8 class sentiment classification and my labels are `(1,2,3,4,7,8,9,10)` so it was unable to match predictions`(1,2,3,4,5,6,7,8)` with my labels, so that's why it was giving out of range error. My question is, why it didn't gave me error in this line `c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)` , how it's matching labels with predictions in this case as opposed to in in_top_k? I think c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y) should give me error because predictions and labels are not same. Why I'm not getting targets out of range error in cross entropy function? \n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nfrom nltk.tokenize import TweetTokenizer\nbatch = 500\nstart = 0\nend = batch - 1\nlearning_rate = 0.2\nnum_classes = 8\npath = \"/home/indy/Downloads/aclImdb/train/pos\"\ntime_steps = 250\nembedding = 50\n\ndef get_embedding():\n    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")\n    f = open(gfile_path,'r')\n    embeddings = {}\n    for line in f:\n        sp_value = line.split()\n        word = sp_value[0]\n        embedding = [float(value) for value in sp_value[1:]]\n        embeddings[word] = embedding\n    return embeddings\n\nebd = get_embedding()\n\ndef get_y(file_name):\n    y_value = file_name.split('_')\n    y_value = y_value[1].split('.')\n    return y_value[0] \n\ndef get_x(path,file_name):\n    file_path = os.path.join(path,file_name)\n    x_value = open(file_path,'r')\n    for line in x_value:\n        x_value = line.replace(\"<br /><br />\",\"\") \n        x_value = x_value.lower()\n    tokeniz = TweetTokenizer()\n    x_value = tokeniz.tokenize(x_value)\n    padding = 250 - len(x_value)\n    if padding > 0:\n       p_value = ['pad' for i in range(padding)]\n       x_value = np.concatenate((x_value,p_value))\n    x_value = [ebd['value'] for value in x_value]\n\n    return x_value\n\ndef  batch_f(path):\n     directory = os.listdir(path)\n     y = [get_y(directory[i]) for i in range(len(directory))]\n     x = [get_x(path,directory[i]) for i in range(len(directory))]    \n     return x,y\n\n\nX = tf.placeholder(tf.float32, [batch,time_steps,embedding])\nY = tf.placeholder(tf.int32, [batch])\n\ndef build_nlp_model(x, _units, lstm_layers,num_classes):\n\n     x = tf.transpose(x, [1, 0, 2])\n     x = tf.reshape(x, [-1, embedding])\n     x = tf.split(0, time_steps, x)\n\n\n     lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units, state_is_tuple = True)\n\n     multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\n     outputs , state = tf.nn.rnn(multi_lstm,x, dtype = tf.float32)     \n\n     weights = tf.Variable(tf.random_normal([_units,num_classes]))\n     biases  = tf.Variable(tf.random_normal([num_classes]))\n\n     logits = tf.matmul(outputs[-1], weights) + biases\n     return logits\n\nlogits = build_nlp_model(X,400,4,num_classes)\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\n\ndecayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(decayed_learning_rate)\nminimize_loss = optimizer.minimize(loss)\n\n\n\ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(25):\n         x, y = batch_f(path)\n         sess.run(minimize_loss,feed_dict={X : x, Y : y})\n         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n         cost = sess.run(loss,feed_dict = {X: x,Y: y})\n         start = end \n         end = (start + batch)\n         print (\"Minibatch Loss = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n```\n\nThis is the error stack that is caused by tf.nn.in_top_k.\n\n```\n(500, 250, 50)\n(500,)\nTraceback (most recent call last):\n  File \"nlp.py\", line 115, in <module>\n    accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: targets[0] is out of range\n     [[Node: InTopK = InTopK[T=DT_INT32, k=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add, _recv_Placeholder_1_0)]]\nCaused by op u'InTopK', defined at:\n  File \"nlp.py\", line 102, in <module>\n    correct_predict = tf.nn.in_top_k(logits, Y, 1)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 890, in in_top_k\n    targets=targets, k=k, name=name)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n```\n\nAnd I think this type of error(targets out of range) should be given by cross entropy also, when labels don't match with predictions.\n"}