{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/272335691", "html_url": "https://github.com/tensorflow/tensorflow/pull/6818#issuecomment-272335691", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6818", "id": 272335691, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MjMzNTY5MQ==", "user": {"login": "mkolod", "id": 476135, "node_id": "MDQ6VXNlcjQ3NjEzNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/476135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mkolod", "html_url": "https://github.com/mkolod", "followers_url": "https://api.github.com/users/mkolod/followers", "following_url": "https://api.github.com/users/mkolod/following{/other_user}", "gists_url": "https://api.github.com/users/mkolod/gists{/gist_id}", "starred_url": "https://api.github.com/users/mkolod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mkolod/subscriptions", "organizations_url": "https://api.github.com/users/mkolod/orgs", "repos_url": "https://api.github.com/users/mkolod/repos", "events_url": "https://api.github.com/users/mkolod/events{/privacy}", "received_events_url": "https://api.github.com/users/mkolod/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-13T01:25:05Z", "updated_at": "2017-01-13T18:23:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>BTW, by the <a href=\"https://gist.github.com/mkolod/71d899687e21f91e1e870832d4de5569\">following test</a>, run on a machine with 32 GB RAM, a 6-core Intel Core i7-5930K CPU @ 3.50GHz and an NVIDIA GTX 1080 GPU produced a GPU-based speedup factor of 17.3, given both the use of a fused CPU and GPU kernel. This was based on a test with a 4D random image tensor with a batch size of 64 and \"image\" size of 256x256x3. The GTX 1080 throughput 31,950 images per second, while the Core i7-5930K throughput was 1,845 images per second. Longer running times, e.g. running tf.while_loop 100k times, can bring the speedup factor to about 30x on this reference hardware, especially for real images where pixels' hues are spatially correlated (which reduces GPU warp divergence in the RGB-to-HSV part of the kernel). Both 3D and 4D tensors work, but of course 4D tensors perform better due to batching. Note that for uint8 inputs (e.g. from tf.image_jpeg_decode), the results of the CPU and GPU kernel are identical, pixel for pixel. For inputs of int32, with an input numeric range of [0, 256], a small fraction of pixels differ by up to 0.3% in intensity (1/255), which is caused by the fact that the Python wrapper for adjust_hue() does scaling, and scaling a range of [0, 255] by 1E31 causes precision problems. That's a very small error for that unusual case. Numbers in range of [0, 255] represented as floats, or as uint8 (scaled by 255 for float conversion by convert_image_dtype() inside tf.image_adjust_hue()) return exactly the same results on the CPU and GPU.</p>", "body_text": "BTW, by the following test, run on a machine with 32 GB RAM, a 6-core Intel Core i7-5930K CPU @ 3.50GHz and an NVIDIA GTX 1080 GPU produced a GPU-based speedup factor of 17.3, given both the use of a fused CPU and GPU kernel. This was based on a test with a 4D random image tensor with a batch size of 64 and \"image\" size of 256x256x3. The GTX 1080 throughput 31,950 images per second, while the Core i7-5930K throughput was 1,845 images per second. Longer running times, e.g. running tf.while_loop 100k times, can bring the speedup factor to about 30x on this reference hardware, especially for real images where pixels' hues are spatially correlated (which reduces GPU warp divergence in the RGB-to-HSV part of the kernel). Both 3D and 4D tensors work, but of course 4D tensors perform better due to batching. Note that for uint8 inputs (e.g. from tf.image_jpeg_decode), the results of the CPU and GPU kernel are identical, pixel for pixel. For inputs of int32, with an input numeric range of [0, 256], a small fraction of pixels differ by up to 0.3% in intensity (1/255), which is caused by the fact that the Python wrapper for adjust_hue() does scaling, and scaling a range of [0, 255] by 1E31 causes precision problems. That's a very small error for that unusual case. Numbers in range of [0, 255] represented as floats, or as uint8 (scaled by 255 for float conversion by convert_image_dtype() inside tf.image_adjust_hue()) return exactly the same results on the CPU and GPU.", "body": "BTW, by the [following test](https://gist.github.com/mkolod/71d899687e21f91e1e870832d4de5569), run on a machine with 32 GB RAM, a 6-core Intel Core i7-5930K CPU @ 3.50GHz and an NVIDIA GTX 1080 GPU produced a GPU-based speedup factor of 17.3, given both the use of a fused CPU and GPU kernel. This was based on a test with a 4D random image tensor with a batch size of 64 and \"image\" size of 256x256x3. The GTX 1080 throughput 31,950 images per second, while the Core i7-5930K throughput was 1,845 images per second. Longer running times, e.g. running tf.while_loop 100k times, can bring the speedup factor to about 30x on this reference hardware, especially for real images where pixels' hues are spatially correlated (which reduces GPU warp divergence in the RGB-to-HSV part of the kernel). Both 3D and 4D tensors work, but of course 4D tensors perform better due to batching. Note that for uint8 inputs (e.g. from tf.image_jpeg_decode), the results of the CPU and GPU kernel are identical, pixel for pixel. For inputs of int32, with an input numeric range of [0, 256], a small fraction of pixels differ by up to 0.3% in intensity (1/255), which is caused by the fact that the Python wrapper for adjust_hue() does scaling, and scaling a range of [0, 255] by 1E31 causes precision problems. That's a very small error for that unusual case. Numbers in range of [0, 255] represented as floats, or as uint8 (scaled by 255 for float conversion by convert_image_dtype() inside tf.image_adjust_hue()) return exactly the same results on the CPU and GPU. "}