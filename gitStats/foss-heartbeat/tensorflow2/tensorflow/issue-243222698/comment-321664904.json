{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321664904", "html_url": "https://github.com/tensorflow/tensorflow/pull/11530#issuecomment-321664904", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11530", "id": 321664904, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTY2NDkwNA==", "user": {"login": "DimanNe", "id": 279056, "node_id": "MDQ6VXNlcjI3OTA1Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/279056?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DimanNe", "html_url": "https://github.com/DimanNe", "followers_url": "https://api.github.com/users/DimanNe/followers", "following_url": "https://api.github.com/users/DimanNe/following{/other_user}", "gists_url": "https://api.github.com/users/DimanNe/gists{/gist_id}", "starred_url": "https://api.github.com/users/DimanNe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DimanNe/subscriptions", "organizations_url": "https://api.github.com/users/DimanNe/orgs", "repos_url": "https://api.github.com/users/DimanNe/repos", "events_url": "https://api.github.com/users/DimanNe/events{/privacy}", "received_events_url": "https://api.github.com/users/DimanNe/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-10T20:31:20Z", "updated_at": "2017-08-10T20:31:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16907534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rmlarsen\">@rmlarsen</a> Sorry for bothering, but I realized that I had misunderstood something. Can you, please, clarify where I should add gradient operation for the new gradient operation(s) <code>+-2(x-y)</code>?</p>\n<p>For example, in order to make gradient op for the <strong>named</strong> operation <code>\"SquaredDifference\"</code>, I registered a C++ function using macro <code>REGISTER_GRADIENT_OP</code>, but how am I supposed to register function computing gradient for two <strong>unnamed</strong> functions <code>+-2(x-y)</code>? Or perhaps there is any example of doing it?</p>\n<p>If it is not difficult, could you please take a step backward and elucidate why do we need at all gradient op for gradient? What is the rationale behind having hand-written second-order derivatives/gradients? Would not it be enough to have only first-order gradients and do everything else with automatic differentiation?</p>", "body_text": "@rmlarsen Sorry for bothering, but I realized that I had misunderstood something. Can you, please, clarify where I should add gradient operation for the new gradient operation(s) +-2(x-y)?\nFor example, in order to make gradient op for the named operation \"SquaredDifference\", I registered a C++ function using macro REGISTER_GRADIENT_OP, but how am I supposed to register function computing gradient for two unnamed functions +-2(x-y)? Or perhaps there is any example of doing it?\nIf it is not difficult, could you please take a step backward and elucidate why do we need at all gradient op for gradient? What is the rationale behind having hand-written second-order derivatives/gradients? Would not it be enough to have only first-order gradients and do everything else with automatic differentiation?", "body": "@rmlarsen Sorry for bothering, but I realized that I had misunderstood something. Can you, please, clarify where I should add gradient operation for the new gradient operation(s) `+-2(x-y)`?\r\n\r\nFor example, in order to make gradient op for the **named** operation `\"SquaredDifference\"`, I registered a C++ function using macro `REGISTER_GRADIENT_OP`, but how am I supposed to register function computing gradient for two **unnamed** functions `+-2(x-y)`? Or perhaps there is any example of doing it?\r\n\r\nIf it is not difficult, could you please take a step backward and elucidate why do we need at all gradient op for gradient? What is the rationale behind having hand-written second-order derivatives/gradients? Would not it be enough to have only first-order gradients and do everything else with automatic differentiation?"}