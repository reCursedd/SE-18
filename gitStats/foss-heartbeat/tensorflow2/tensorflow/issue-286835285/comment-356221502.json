{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356221502", "html_url": "https://github.com/tensorflow/tensorflow/issues/15954#issuecomment-356221502", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15954", "id": 356221502, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjIyMTUwMg==", "user": {"login": "Nassimmd", "id": 35070038, "node_id": "MDQ6VXNlcjM1MDcwMDM4", "avatar_url": "https://avatars0.githubusercontent.com/u/35070038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nassimmd", "html_url": "https://github.com/Nassimmd", "followers_url": "https://api.github.com/users/Nassimmd/followers", "following_url": "https://api.github.com/users/Nassimmd/following{/other_user}", "gists_url": "https://api.github.com/users/Nassimmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nassimmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nassimmd/subscriptions", "organizations_url": "https://api.github.com/users/Nassimmd/orgs", "repos_url": "https://api.github.com/users/Nassimmd/repos", "events_url": "https://api.github.com/users/Nassimmd/events{/privacy}", "received_events_url": "https://api.github.com/users/Nassimmd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-09T09:01:16Z", "updated_at": "2018-01-09T09:09:25Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Windows 7</li>\n<li>TensorFlow 1.4 installed with pip and then upgraded to 1.5.0-rc0</li>\n<li>TensorFlow version 1.5.0-rc0</li>\n<li>Python version 3.5.2</li>\n</ul>\n<h3>Problem</h3>\n<p>I am using the code for social LSTM (long short-term memory) which you can find here: <a href=\"https://github.com/vvanirudh/social-lstm-tf\">https://github.com/vvanirudh/social-lstm-tf</a></p>\n<p>when I want to train my model, no matter whether it is a normal or social LSTM I get the following error.</p>\n<h3>Error</h3>\n<p>Traceback (most recent call last):<br>\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 510, in _apply_op_helper<br>\npreferred_dtype=default_dtype)<br>\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1013, in internal_convert_to_tensor<br>\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)<br>\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 857, in _TensorTensorConversionFunction<br>\n(dtype.name, t.dtype.name, str(t)))<br>\nValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"Placeholder:0\", shape=(?, 8, 2), dtype=float32)'</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>Traceback (most recent call last):<br>\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 125, in <br>\nmain()<br>\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 60, in main<br>\ntrain(args)<br>\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 77, in train<br>\nmodel = Model(args)<br>\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\model.py\", line 80, in <strong>init</strong><br>\ninputs = tf.split(1, args.seq_length, self.input_data)<br>\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1330, in split<br>\naxis=axis, num_split=num_or_size_splits, value=value, name=name)<br>\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6922, in _split<br>\n\"Split\", split_dim=axis, value=value, num_split=num_split, name=name)<br>\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 533, in _apply_op_helper<br>\n(prefix, dtypes.as_dtype(input_arg.type).name))<br>\nTypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.</p>\n<p>Here you can find the code for Training the model:</p>\n<h3>Source code / logs</h3>\n<p>import numpy as np<br>\nimport tensorflow as tf<br>\nimport argparse<br>\nimport os<br>\nimport time<br>\nimport pickle<br>\nfrom model import Model<br>\nfrom utils import DataLoader</p>\n<p>def main():<br>\nparser = argparse.ArgumentParser()<br>\n# RNN size parameter (dimension of the output/hidden state)<br>\nparser.add_argument('--rnn_size', type=int, default=128,<br>\nhelp='size of RNN hidden state')<br>\n# Number of layers parameter<br>\n# TODO: (improve) Number of layers not used. Only a single layer implemented<br>\nparser.add_argument('--num_layers', type=int, default=1,<br>\nhelp='number of layers in the RNN')<br>\n# Type of recurrent unit parameter<br>\n# Model currently not used. Only LSTM implemented<br>\nparser.add_argument('--model', type=str, default='lstm',<br>\nhelp='rnn, gru, or lstm')<br>\n# Size of each batch parameter<br>\nparser.add_argument('--batch_size', type=int, default=50,<br>\nhelp='minibatch size')<br>\n# Length of sequence to be considered parameter<br>\nparser.add_argument('--seq_length', type=int, default=8,<br>\nhelp='RNN sequence length')<br>\n# Number of epochs parameter<br>\nparser.add_argument('--num_epochs', type=int, default=100,<br>\nhelp='number of epochs')<br>\n# Frequency at which the model should be saved parameter<br>\nparser.add_argument('--save_every', type=int, default=400,<br>\nhelp='save frequency')<br>\n# Gradient value at which it should be clipped<br>\n# TODO: (resolve) Clipping gradients for now. No idea whether we should<br>\nparser.add_argument('--grad_clip', type=float, default=10.,<br>\nhelp='clip gradients at this value')<br>\n# Learning rate parameter<br>\nparser.add_argument('--learning_rate', type=float, default=0.003,<br>\nhelp='learning rate')<br>\n# Decay rate for the learning rate parameter<br>\nparser.add_argument('--decay_rate', type=float, default=0.95,<br>\nhelp='decay rate for rmsprop')<br>\n# Dropout probability parameter<br>\n# Dropout not implemented.<br>\nparser.add_argument('--keep_prob', type=float, default=0.8,<br>\nhelp='dropout keep probability')<br>\n# Dimension of the embeddings parameter<br>\nparser.add_argument('--embedding_size', type=int, default=128,<br>\nhelp='Embedding dimension for the spatial coordinates')<br>\nparser.add_argument('--leaveDataset', type=int, default=3,<br>\nhelp='The dataset index to be left out in training')<br>\n# Lambda regularization parameter (L2)<br>\nparser.add_argument('--lambda_param', type=float, default=0.05,<br>\nhelp='L2 regularization parameter')<br>\nargs = parser.parse_args()<br>\ntrain(args)</p>\n<p>def train(args):<br>\ndatasets = list(range(4))<br>\n# Remove the leaveDataset from datasets<br>\ndatasets.remove(args.leaveDataset)</p>\n<pre><code># Create the data loader object. This object would preprocess the data in terms of\n# batches each of size args.batch_size, of length args.seq_length\ndata_loader = DataLoader(args.batch_size, args.seq_length, datasets, forcePreProcess=True)\n\n# Save the arguments int the config file\nwith open(os.path.join('save_lstm', 'config.pkl'), 'wb') as f:\n    pickle.dump(args, f)\n\n# Create a Vanilla LSTM model with the arguments\nmodel = Model(args)\n\n# Initialize a TensorFlow session\nwith tf.Session() as sess:\n    # Initialize all the variables in the graph\n    sess.run(tf.initialize_all_variables())\n    # Add all the variables to the list of variables to be saved\n    saver = tf.train.Saver(tf.all_variables())\n\n    # For each epoch\n    for e in range(args.num_epochs):\n        # Assign the learning rate (decayed acc. to the epoch number)\n        sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n        # Reset the pointers in the data loader object\n        data_loader.reset_batch_pointer()\n        # Get the initial cell state of the LSTM\n        state = sess.run(model.initial_state)\n\n        # For each batch in this epoch\n        for b in range(data_loader.num_batches):\n            # Tic\n            start = time.time()\n            # Get the source and target data of the current batch\n            # x has the source data, y has the target data\n            x, y = data_loader.next_batch()\n\n            # Feed the source, target data and the initial LSTM state to the model\n            feed = {model.input_data: x, model.target_data: y, model.initial_state: state}\n            # Fetch the loss of the model on this batch, the final LSTM state from the session\n            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n            # Toc\n            end = time.time()\n            # Print epoch, batch, loss and time taken\n            print(\n                \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n                .format(\n                    e * data_loader.num_batches + b,\n                    args.num_epochs * data_loader.num_batches,\n                    e,\n                    train_loss, end - start))\n\n            # Save the model if the current epoch and batch number match the frequency\n            if (e * data_loader.num_batches + b) % args.save_every == 0 and ((e * data_loader.num_batches + b) &gt; 0):\n                checkpoint_path = os.path.join('save_lstm', 'model.ckpt')\n                saver.save(sess, checkpoint_path, global_step=e * data_loader.num_batches + b)\n                print(\"model saved to {}\".format(checkpoint_path))\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nmain()</p>\n<h3></h3>\n<p>Thank you in advance,<br>\nNassim</p>", "body_text": "System information\n\nWindows 7\nTensorFlow 1.4 installed with pip and then upgraded to 1.5.0-rc0\nTensorFlow version 1.5.0-rc0\nPython version 3.5.2\n\nProblem\nI am using the code for social LSTM (long short-term memory) which you can find here: https://github.com/vvanirudh/social-lstm-tf\nwhen I want to train my model, no matter whether it is a normal or social LSTM I get the following error.\nError\nTraceback (most recent call last):\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 510, in _apply_op_helper\npreferred_dtype=default_dtype)\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1013, in internal_convert_to_tensor\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 857, in _TensorTensorConversionFunction\n(dtype.name, t.dtype.name, str(t)))\nValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"Placeholder:0\", shape=(?, 8, 2), dtype=float32)'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 125, in \nmain()\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 60, in main\ntrain(args)\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 77, in train\nmodel = Model(args)\nFile \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\model.py\", line 80, in init\ninputs = tf.split(1, args.seq_length, self.input_data)\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1330, in split\naxis=axis, num_split=num_or_size_splits, value=value, name=name)\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6922, in _split\n\"Split\", split_dim=axis, value=value, num_split=num_split, name=name)\nFile \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 533, in _apply_op_helper\n(prefix, dtypes.as_dtype(input_arg.type).name))\nTypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.\nHere you can find the code for Training the model:\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nimport argparse\nimport os\nimport time\nimport pickle\nfrom model import Model\nfrom utils import DataLoader\ndef main():\nparser = argparse.ArgumentParser()\n# RNN size parameter (dimension of the output/hidden state)\nparser.add_argument('--rnn_size', type=int, default=128,\nhelp='size of RNN hidden state')\n# Number of layers parameter\n# TODO: (improve) Number of layers not used. Only a single layer implemented\nparser.add_argument('--num_layers', type=int, default=1,\nhelp='number of layers in the RNN')\n# Type of recurrent unit parameter\n# Model currently not used. Only LSTM implemented\nparser.add_argument('--model', type=str, default='lstm',\nhelp='rnn, gru, or lstm')\n# Size of each batch parameter\nparser.add_argument('--batch_size', type=int, default=50,\nhelp='minibatch size')\n# Length of sequence to be considered parameter\nparser.add_argument('--seq_length', type=int, default=8,\nhelp='RNN sequence length')\n# Number of epochs parameter\nparser.add_argument('--num_epochs', type=int, default=100,\nhelp='number of epochs')\n# Frequency at which the model should be saved parameter\nparser.add_argument('--save_every', type=int, default=400,\nhelp='save frequency')\n# Gradient value at which it should be clipped\n# TODO: (resolve) Clipping gradients for now. No idea whether we should\nparser.add_argument('--grad_clip', type=float, default=10.,\nhelp='clip gradients at this value')\n# Learning rate parameter\nparser.add_argument('--learning_rate', type=float, default=0.003,\nhelp='learning rate')\n# Decay rate for the learning rate parameter\nparser.add_argument('--decay_rate', type=float, default=0.95,\nhelp='decay rate for rmsprop')\n# Dropout probability parameter\n# Dropout not implemented.\nparser.add_argument('--keep_prob', type=float, default=0.8,\nhelp='dropout keep probability')\n# Dimension of the embeddings parameter\nparser.add_argument('--embedding_size', type=int, default=128,\nhelp='Embedding dimension for the spatial coordinates')\nparser.add_argument('--leaveDataset', type=int, default=3,\nhelp='The dataset index to be left out in training')\n# Lambda regularization parameter (L2)\nparser.add_argument('--lambda_param', type=float, default=0.05,\nhelp='L2 regularization parameter')\nargs = parser.parse_args()\ntrain(args)\ndef train(args):\ndatasets = list(range(4))\n# Remove the leaveDataset from datasets\ndatasets.remove(args.leaveDataset)\n# Create the data loader object. This object would preprocess the data in terms of\n# batches each of size args.batch_size, of length args.seq_length\ndata_loader = DataLoader(args.batch_size, args.seq_length, datasets, forcePreProcess=True)\n\n# Save the arguments int the config file\nwith open(os.path.join('save_lstm', 'config.pkl'), 'wb') as f:\n    pickle.dump(args, f)\n\n# Create a Vanilla LSTM model with the arguments\nmodel = Model(args)\n\n# Initialize a TensorFlow session\nwith tf.Session() as sess:\n    # Initialize all the variables in the graph\n    sess.run(tf.initialize_all_variables())\n    # Add all the variables to the list of variables to be saved\n    saver = tf.train.Saver(tf.all_variables())\n\n    # For each epoch\n    for e in range(args.num_epochs):\n        # Assign the learning rate (decayed acc. to the epoch number)\n        sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n        # Reset the pointers in the data loader object\n        data_loader.reset_batch_pointer()\n        # Get the initial cell state of the LSTM\n        state = sess.run(model.initial_state)\n\n        # For each batch in this epoch\n        for b in range(data_loader.num_batches):\n            # Tic\n            start = time.time()\n            # Get the source and target data of the current batch\n            # x has the source data, y has the target data\n            x, y = data_loader.next_batch()\n\n            # Feed the source, target data and the initial LSTM state to the model\n            feed = {model.input_data: x, model.target_data: y, model.initial_state: state}\n            # Fetch the loss of the model on this batch, the final LSTM state from the session\n            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n            # Toc\n            end = time.time()\n            # Print epoch, batch, loss and time taken\n            print(\n                \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n                .format(\n                    e * data_loader.num_batches + b,\n                    args.num_epochs * data_loader.num_batches,\n                    e,\n                    train_loss, end - start))\n\n            # Save the model if the current epoch and batch number match the frequency\n            if (e * data_loader.num_batches + b) % args.save_every == 0 and ((e * data_loader.num_batches + b) > 0):\n                checkpoint_path = os.path.join('save_lstm', 'model.ckpt')\n                saver.save(sess, checkpoint_path, global_step=e * data_loader.num_batches + b)\n                print(\"model saved to {}\".format(checkpoint_path))\n\nif name == 'main':\nmain()\n\nThank you in advance,\nNassim", "body": "### System information\r\n- Windows 7\r\n- TensorFlow 1.4 installed with pip and then upgraded to 1.5.0-rc0 \r\n- TensorFlow version 1.5.0-rc0\r\n- Python version 3.5.2\r\n\r\n### Problem\r\nI am using the code for social LSTM (long short-term memory) which you can find here: https://github.com/vvanirudh/social-lstm-tf\r\n\r\nwhen I want to train my model, no matter whether it is a normal or social LSTM I get the following error. \r\n### Error\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1013, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 857, in _TensorTensorConversionFunction\r\n    (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"Placeholder:0\", shape=(?, 8, 2), dtype=float32)'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 125, in <module>\r\n    main()\r\n  File \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 60, in main\r\n    train(args)\r\n  File \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\train.py\", line 77, in train\r\n    model = Model(args)\r\n  File \"C:\\Users\\ga67zod\\Desktop\\social-lstm-tf\\lstm\\model.py\", line 80, in __init__\r\n    inputs = tf.split(1, args.seq_length, self.input_data)\r\n  File \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1330, in split\r\n    axis=axis, num_split=num_or_size_splits, value=value, name=name)\r\n  File \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6922, in _split\r\n    \"Split\", split_dim=axis, value=value, num_split=num_split, name=name)\r\n  File \"C:\\Users\\ga67zod\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 533, in _apply_op_helper\r\n    (prefix, dtypes.as_dtype(input_arg.type).name))\r\nTypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.\r\n\r\nHere you can find the code for Training the model: \r\n### Source code / logs\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport argparse\r\nimport os\r\nimport time\r\nimport pickle\r\nfrom model import Model\r\nfrom utils import DataLoader\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    # RNN size parameter (dimension of the output/hidden state)\r\n    parser.add_argument('--rnn_size', type=int, default=128,\r\n                        help='size of RNN hidden state')\r\n    # Number of layers parameter\r\n    # TODO: (improve) Number of layers not used. Only a single layer implemented\r\n    parser.add_argument('--num_layers', type=int, default=1,\r\n                        help='number of layers in the RNN')\r\n    # Type of recurrent unit parameter\r\n    # Model currently not used. Only LSTM implemented\r\n    parser.add_argument('--model', type=str, default='lstm',\r\n                        help='rnn, gru, or lstm')\r\n    # Size of each batch parameter\r\n    parser.add_argument('--batch_size', type=int, default=50,\r\n                        help='minibatch size')\r\n    # Length of sequence to be considered parameter\r\n    parser.add_argument('--seq_length', type=int, default=8,\r\n                        help='RNN sequence length')\r\n    # Number of epochs parameter\r\n    parser.add_argument('--num_epochs', type=int, default=100,\r\n                        help='number of epochs')\r\n    # Frequency at which the model should be saved parameter\r\n    parser.add_argument('--save_every', type=int, default=400,\r\n                        help='save frequency')\r\n    # Gradient value at which it should be clipped\r\n    # TODO: (resolve) Clipping gradients for now. No idea whether we should\r\n    parser.add_argument('--grad_clip', type=float, default=10.,\r\n                        help='clip gradients at this value')\r\n    # Learning rate parameter\r\n    parser.add_argument('--learning_rate', type=float, default=0.003,\r\n                        help='learning rate')\r\n    # Decay rate for the learning rate parameter\r\n    parser.add_argument('--decay_rate', type=float, default=0.95,\r\n                        help='decay rate for rmsprop')\r\n    # Dropout probability parameter\r\n    # Dropout not implemented.\r\n    parser.add_argument('--keep_prob', type=float, default=0.8,\r\n                        help='dropout keep probability')\r\n    # Dimension of the embeddings parameter\r\n    parser.add_argument('--embedding_size', type=int, default=128,\r\n                        help='Embedding dimension for the spatial coordinates')\r\n    parser.add_argument('--leaveDataset', type=int, default=3,\r\n                        help='The dataset index to be left out in training')\r\n    # Lambda regularization parameter (L2)\r\n    parser.add_argument('--lambda_param', type=float, default=0.05,\r\n                        help='L2 regularization parameter')\r\n    args = parser.parse_args()\r\n    train(args)\r\n\r\n\r\ndef train(args):\r\n    datasets = list(range(4))\r\n    # Remove the leaveDataset from datasets\r\n    datasets.remove(args.leaveDataset)\r\n\r\n    # Create the data loader object. This object would preprocess the data in terms of\r\n    # batches each of size args.batch_size, of length args.seq_length\r\n    data_loader = DataLoader(args.batch_size, args.seq_length, datasets, forcePreProcess=True)\r\n\r\n    # Save the arguments int the config file\r\n    with open(os.path.join('save_lstm', 'config.pkl'), 'wb') as f:\r\n        pickle.dump(args, f)\r\n\r\n    # Create a Vanilla LSTM model with the arguments\r\n    model = Model(args)\r\n\r\n    # Initialize a TensorFlow session\r\n    with tf.Session() as sess:\r\n        # Initialize all the variables in the graph\r\n        sess.run(tf.initialize_all_variables())\r\n        # Add all the variables to the list of variables to be saved\r\n        saver = tf.train.Saver(tf.all_variables())\r\n\r\n        # For each epoch\r\n        for e in range(args.num_epochs):\r\n            # Assign the learning rate (decayed acc. to the epoch number)\r\n            sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\r\n            # Reset the pointers in the data loader object\r\n            data_loader.reset_batch_pointer()\r\n            # Get the initial cell state of the LSTM\r\n            state = sess.run(model.initial_state)\r\n\r\n            # For each batch in this epoch\r\n            for b in range(data_loader.num_batches):\r\n                # Tic\r\n                start = time.time()\r\n                # Get the source and target data of the current batch\r\n                # x has the source data, y has the target data\r\n                x, y = data_loader.next_batch()\r\n\r\n                # Feed the source, target data and the initial LSTM state to the model\r\n                feed = {model.input_data: x, model.target_data: y, model.initial_state: state}\r\n                # Fetch the loss of the model on this batch, the final LSTM state from the session\r\n                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\r\n                # Toc\r\n                end = time.time()\r\n                # Print epoch, batch, loss and time taken\r\n                print(\r\n                    \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\r\n                    .format(\r\n                        e * data_loader.num_batches + b,\r\n                        args.num_epochs * data_loader.num_batches,\r\n                        e,\r\n                        train_loss, end - start))\r\n\r\n                # Save the model if the current epoch and batch number match the frequency\r\n                if (e * data_loader.num_batches + b) % args.save_every == 0 and ((e * data_loader.num_batches + b) > 0):\r\n                    checkpoint_path = os.path.join('save_lstm', 'model.ckpt')\r\n                    saver.save(sess, checkpoint_path, global_step=e * data_loader.num_batches + b)\r\n                    print(\"model saved to {}\".format(checkpoint_path))\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n###\r\nThank you in advance,\r\nNassim"}