{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11725", "id": 245192488, "node_id": "MDU6SXNzdWUyNDUxOTI0ODg=", "number": 11725, "title": "tf_cnn_benchmarks.py stuck when running with multiple GPUs and ImageNet data with protocol grpc+verbs", "user": {"login": "shamoya", "id": 22274255, "node_id": "MDQ6VXNlcjIyMjc0MjU1", "avatar_url": "https://avatars2.githubusercontent.com/u/22274255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shamoya", "html_url": "https://github.com/shamoya", "followers_url": "https://api.github.com/users/shamoya/followers", "following_url": "https://api.github.com/users/shamoya/following{/other_user}", "gists_url": "https://api.github.com/users/shamoya/gists{/gist_id}", "starred_url": "https://api.github.com/users/shamoya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shamoya/subscriptions", "organizations_url": "https://api.github.com/users/shamoya/orgs", "repos_url": "https://api.github.com/users/shamoya/repos", "events_url": "https://api.github.com/users/shamoya/events{/privacy}", "received_events_url": "https://api.github.com/users/shamoya/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 56, "created_at": "2017-07-24T20:01:58Z", "updated_at": "2018-10-21T04:38:19Z", "closed_at": "2017-08-24T05:21:42Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No, running tf_cnn_benchmarks.py from benchmarks repo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04.2 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Unmodified source with RDMA Verbs enabled</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0-rc0</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.1</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/6</li>\n<li><strong>GPU model and memory</strong>: NVIDIA Tesla P100 PCIe 16GB (8 per node)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>PS: CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=ps --task_index=0 --server_protocol grpc+verbs</p>\n<p>Worker0: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=0 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs</p>\n<p>Worker1: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=1 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs</p>\n<ul>\n<li><strong>RDMA driver version</strong>: MLNX_OFED_LINUX-4.1-1.0.2.0</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When running the above commands (Inception V3 synchronized data parallelism training with 2 workers and 1 external ps), the tf_cnn_benchmarks application hangs forever after some iterations (usually in warm up).</p>\n<p>It happens only when real data is involved (ImageNet), and with &gt;4 GPUs. (More GPUs, less iterations before it hangs). Doesn't happen with grpc protocol, or when running with \"synthetic\" data.</p>\n<p>The master_service in the workers is stuck <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608\">here</a>, which I guess means some operations in the computation have not been completed.</p>\n<p>The RDMA protocol looks valid and clean, all messages corresponds to the protocol (see below logs).<br>\nThere some tensors requested by the workers which they don't receive, but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way, and for some reason the higher level worker service doesn't trigger the Send kernel on those tensors.</p>\n<p>Any help is much appreciated!<br>\nIf there are some debug mechanisms I can use to understand which tensors/operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now, without much success, and I feel I don't have enough information there to understand what's missing.<br>\nAlso I feel we don't have enough knowledge on how the step_id acts (diving into this in the code now, but there's some higher level documentation it can greatly help).</p>\n<p>My initial guess was an occurrence of a racy condition when loading the data, since it creates a gap in execution time (worker0 starts the first training step 30-60 seconds after worker1, since it does the preprocessing of the data twice for a reason I couldn't understand yet), but after the first iteration (which usually passes successfully) the time is synchronized between workers.</p>\n<h3>Source code / logs</h3>\n<p>Those are the logs of the runtime after moving the logging in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc\">rdma.cc</a> to VLOG(0) (also adding Tensor name and step id for all cases, in some cases the step_id doesn't mean anything like BUFFER_REQUEST/RESPONSE for example), and also some VLOG in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc\">master_session.cc</a></p>\n<p><a href=\"https://gist.github.com/shamoya/15a42f421e088473b8f02bf00c16d0fc\">worker0</a><br>\n<a href=\"https://gist.github.com/shamoya/dd3126c02c73990a6e28b534d9a9ddf6\">worker1</a><br>\n<a href=\"https://gist.github.com/shamoya/0c856365802ae4d42b38baf988149574\">ps</a></p>\n<p>Unfortunately they are fairly large, but it's better then to cut the log files IMO.<br>\nExample for analysis I did in the verbs layer, comparing the Sent Tensor requests to the actual received tensors writes in both workers:</p>\n<p>worker 0:</p>\n<ul>\n<li>/job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:0/gpu:0;edge_116943_group_deps_2/NoOp_1;0:0 80661058974090965</li>\n<li>/job:worker/replica:0/task:1/cpu:0;1a50d5c51cd9c5d1;/job:worker/replica:0/task:0/gpu:0;edge_116947_group_deps_3/NoOp_1;0:0 80661058974090965</li>\n<li>/job:worker/replica:0/task:1/gpu:2;7f00fadabfe781f5;/job:worker/replica:0/task:0/gpu:0;edge_111078_group_deps_1/NoOp_2;0:0 80661058974090965</li>\n<li>/job:worker/replica:0/task:1/gpu:4;b07185dd19f62088;/job:worker/replica:0/task:0/gpu:0;edge_111080_group_deps_1/NoOp_4;0:0 80661058974090965</li>\n</ul>\n<p>worker 1:</p>\n<ul>\n<li>/job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:1/cpu:0;edge_155113_AssignAdd;0:0 80661058974090965</li>\n<li>/job:worker/replica:0/task:0/gpu:0;f3df8abf03739fe8;/job:worker/replica:0/task:1/cpu:0;edge_116948_group_deps_3;0:0 80661058974090965</li>\n</ul>\n<p>The tensors requests received well by the other side and passed to RecvLocalAsync, but are not called later.</p>\n<p>Thanks a lot.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No, running tf_cnn_benchmarks.py from benchmarks repo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.2 LTS\nTensorFlow installed from (source or binary): Unmodified source with RDMA Verbs enabled\nTensorFlow version (use command below): 1.3.0-rc0\nPython version: 2.7.12\nBazel version (if compiling from source): 0.5.1\nCUDA/cuDNN version: 8.0/6\nGPU model and memory: NVIDIA Tesla P100 PCIe 16GB (8 per node)\nExact command to reproduce:\n\nPS: CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=ps --task_index=0 --server_protocol grpc+verbs\nWorker0: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=0 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs\nWorker1: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=1 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs\n\nRDMA driver version: MLNX_OFED_LINUX-4.1-1.0.2.0\n\nDescribe the problem\nWhen running the above commands (Inception V3 synchronized data parallelism training with 2 workers and 1 external ps), the tf_cnn_benchmarks application hangs forever after some iterations (usually in warm up).\nIt happens only when real data is involved (ImageNet), and with >4 GPUs. (More GPUs, less iterations before it hangs). Doesn't happen with grpc protocol, or when running with \"synthetic\" data.\nThe master_service in the workers is stuck here, which I guess means some operations in the computation have not been completed.\nThe RDMA protocol looks valid and clean, all messages corresponds to the protocol (see below logs).\nThere some tensors requested by the workers which they don't receive, but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way, and for some reason the higher level worker service doesn't trigger the Send kernel on those tensors.\nAny help is much appreciated!\nIf there are some debug mechanisms I can use to understand which tensors/operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now, without much success, and I feel I don't have enough information there to understand what's missing.\nAlso I feel we don't have enough knowledge on how the step_id acts (diving into this in the code now, but there's some higher level documentation it can greatly help).\nMy initial guess was an occurrence of a racy condition when loading the data, since it creates a gap in execution time (worker0 starts the first training step 30-60 seconds after worker1, since it does the preprocessing of the data twice for a reason I couldn't understand yet), but after the first iteration (which usually passes successfully) the time is synchronized between workers.\nSource code / logs\nThose are the logs of the runtime after moving the logging in rdma.cc to VLOG(0) (also adding Tensor name and step id for all cases, in some cases the step_id doesn't mean anything like BUFFER_REQUEST/RESPONSE for example), and also some VLOG in master_session.cc\nworker0\nworker1\nps\nUnfortunately they are fairly large, but it's better then to cut the log files IMO.\nExample for analysis I did in the verbs layer, comparing the Sent Tensor requests to the actual received tensors writes in both workers:\nworker 0:\n\n/job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:0/gpu:0;edge_116943_group_deps_2/NoOp_1;0:0 80661058974090965\n/job:worker/replica:0/task:1/cpu:0;1a50d5c51cd9c5d1;/job:worker/replica:0/task:0/gpu:0;edge_116947_group_deps_3/NoOp_1;0:0 80661058974090965\n/job:worker/replica:0/task:1/gpu:2;7f00fadabfe781f5;/job:worker/replica:0/task:0/gpu:0;edge_111078_group_deps_1/NoOp_2;0:0 80661058974090965\n/job:worker/replica:0/task:1/gpu:4;b07185dd19f62088;/job:worker/replica:0/task:0/gpu:0;edge_111080_group_deps_1/NoOp_4;0:0 80661058974090965\n\nworker 1:\n\n/job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:1/cpu:0;edge_155113_AssignAdd;0:0 80661058974090965\n/job:worker/replica:0/task:0/gpu:0;f3df8abf03739fe8;/job:worker/replica:0/task:1/cpu:0;edge_116948_group_deps_3;0:0 80661058974090965\n\nThe tensors requests received well by the other side and passed to RecvLocalAsync, but are not called later.\nThanks a lot.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, running tf_cnn_benchmarks.py from benchmarks repo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: Unmodified source with RDMA Verbs enabled\r\n- **TensorFlow version (use command below)**: 1.3.0-rc0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: NVIDIA Tesla P100 PCIe 16GB (8 per node)\r\n- **Exact command to reproduce**: \r\n\r\nPS: CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=ps --task_index=0 --server_protocol grpc+verbs\r\n\r\nWorker0: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=0 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs\r\n\r\nWorker1: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=1 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs\r\n\r\n- **RDMA driver version**: MLNX_OFED_LINUX-4.1-1.0.2.0\r\n\r\n### Describe the problem\r\nWhen running the above commands (Inception V3 synchronized data parallelism training with 2 workers and 1 external ps), the tf_cnn_benchmarks application hangs forever after some iterations (usually in warm up).\r\n\r\nIt happens only when real data is involved (ImageNet), and with >4 GPUs. (More GPUs, less iterations before it hangs). Doesn't happen with grpc protocol, or when running with \"synthetic\" data.\r\n\r\nThe master_service in the workers is stuck [here](\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608), which I guess means some operations in the computation have not been completed.\r\n\r\nThe RDMA protocol looks valid and clean, all messages corresponds to the protocol (see below logs). \r\nThere some tensors requested by the workers which they don't receive, but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way, and for some reason the higher level worker service doesn't trigger the Send kernel on those tensors.\r\n\r\nAny help is much appreciated!\r\nIf there are some debug mechanisms I can use to understand which tensors/operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now, without much success, and I feel I don't have enough information there to understand what's missing.\r\nAlso I feel we don't have enough knowledge on how the step_id acts (diving into this in the code now, but there's some higher level documentation it can greatly help).\r\n\r\nMy initial guess was an occurrence of a racy condition when loading the data, since it creates a gap in execution time (worker0 starts the first training step 30-60 seconds after worker1, since it does the preprocessing of the data twice for a reason I couldn't understand yet), but after the first iteration (which usually passes successfully) the time is synchronized between workers.\r\n\r\n### Source code / logs\r\nThose are the logs of the runtime after moving the logging in [rdma.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc) to VLOG(0) (also adding Tensor name and step id for all cases, in some cases the step_id doesn't mean anything like BUFFER_REQUEST/RESPONSE for example), and also some VLOG in [master_session.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc)\r\n\r\n[worker0](https://gist.github.com/shamoya/15a42f421e088473b8f02bf00c16d0fc)\r\n[worker1](https://gist.github.com/shamoya/dd3126c02c73990a6e28b534d9a9ddf6)\r\n[ps](https://gist.github.com/shamoya/0c856365802ae4d42b38baf988149574)\r\n\r\nUnfortunately they are fairly large, but it's better then to cut the log files IMO.\r\nExample for analysis I did in the verbs layer, comparing the Sent Tensor requests to the actual received tensors writes in both workers:\r\n\r\nworker 0:\r\n -  /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:0/gpu:0;edge_116943_group_deps_2/NoOp_1;0:0 80661058974090965\r\n-  /job:worker/replica:0/task:1/cpu:0;1a50d5c51cd9c5d1;/job:worker/replica:0/task:0/gpu:0;edge_116947_group_deps_3/NoOp_1;0:0 80661058974090965\r\n- /job:worker/replica:0/task:1/gpu:2;7f00fadabfe781f5;/job:worker/replica:0/task:0/gpu:0;edge_111078_group_deps_1/NoOp_2;0:0 80661058974090965\r\n- /job:worker/replica:0/task:1/gpu:4;b07185dd19f62088;/job:worker/replica:0/task:0/gpu:0;edge_111080_group_deps_1/NoOp_4;0:0 80661058974090965\r\n\r\nworker 1:\r\n- /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:1/cpu:0;edge_155113_AssignAdd;0:0 80661058974090965\r\n- /job:worker/replica:0/task:0/gpu:0;f3df8abf03739fe8;/job:worker/replica:0/task:1/cpu:0;edge_116948_group_deps_3;0:0 80661058974090965\r\n\r\nThe tensors requests received well by the other side and passed to RecvLocalAsync, but are not called later.\r\n\r\nThanks a lot."}