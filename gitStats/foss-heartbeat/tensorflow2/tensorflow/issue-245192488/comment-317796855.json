{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317796855", "html_url": "https://github.com/tensorflow/tensorflow/issues/11725#issuecomment-317796855", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725", "id": 317796855, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzc5Njg1NQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-25T16:41:12Z", "updated_at": "2017-07-25T16:41:12Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">When I'm trying to debug a hung concurrent program the first step is\nusually to try to find a small case that exhibits the problem, i.e. try to\nfind the least number of workers and GPUs that still hangs.  It's much\neasier to debug a small case than a large one.  Then, if it's a really\nsmall case you might be able to set the logging level high and read all the\nlog files.  Usually though, I form some hypotheses about where the problem\nmight be (e.g. a missing lock, a race that might result in deadlock, logic\nerror that always deadlocks) and start putting in LOG(INFO) statements\naround the suspicious points to confirm or refute each hypothesis.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Tue, Jul 25, 2017 at 4:53 AM, Ziming Dong ***@***.***&gt; wrote:\n I think global_step should be protected by a lock.I raised an issue\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"245373413\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/benchmarks/issues/38\" href=\"https://github.com/tensorflow/benchmarks/issues/38\">tensorflow/benchmarks#38</a>&gt;. I'm not sure if\n tensorflow has some special code for global_step.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"245192488\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11725\" href=\"https://github.com/tensorflow/tensorflow/issues/11725#issuecomment-317713980\">#11725 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AO818aHj9Ak6IS6emES-wdAhL-n1M4oUks5sRdcmgaJpZM4OhqyQ\">https://github.com/notifications/unsubscribe-auth/AO818aHj9Ak6IS6emES-wdAhL-n1M4oUks5sRdcmgaJpZM4OhqyQ</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "When I'm trying to debug a hung concurrent program the first step is\nusually to try to find a small case that exhibits the problem, i.e. try to\nfind the least number of workers and GPUs that still hangs.  It's much\neasier to debug a small case than a large one.  Then, if it's a really\nsmall case you might be able to set the logging level high and read all the\nlog files.  Usually though, I form some hypotheses about where the problem\nmight be (e.g. a missing lock, a race that might result in deadlock, logic\nerror that always deadlocks) and start putting in LOG(INFO) statements\naround the suspicious points to confirm or refute each hypothesis.\n\u2026\nOn Tue, Jul 25, 2017 at 4:53 AM, Ziming Dong ***@***.***> wrote:\n I think global_step should be protected by a lock.I raised an issue\n <tensorflow/benchmarks#38>. I'm not sure if\n tensorflow has some special code for global_step.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#11725 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AO818aHj9Ak6IS6emES-wdAhL-n1M4oUks5sRdcmgaJpZM4OhqyQ>\n .", "body": "When I'm trying to debug a hung concurrent program the first step is\nusually to try to find a small case that exhibits the problem, i.e. try to\nfind the least number of workers and GPUs that still hangs.  It's much\neasier to debug a small case than a large one.  Then, if it's a really\nsmall case you might be able to set the logging level high and read all the\nlog files.  Usually though, I form some hypotheses about where the problem\nmight be (e.g. a missing lock, a race that might result in deadlock, logic\nerror that always deadlocks) and start putting in LOG(INFO) statements\naround the suspicious points to confirm or refute each hypothesis.\n\nOn Tue, Jul 25, 2017 at 4:53 AM, Ziming Dong <notifications@github.com>\nwrote:\n\n> I think global_step should be protected by a lock.I raised an issue\n> <https://github.com/tensorflow/benchmarks/issues/38>. I'm not sure if\n> tensorflow has some special code for global_step.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11725#issuecomment-317713980>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO818aHj9Ak6IS6emES-wdAhL-n1M4oUks5sRdcmgaJpZM4OhqyQ>\n> .\n>\n"}