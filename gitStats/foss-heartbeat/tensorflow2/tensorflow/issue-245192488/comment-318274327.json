{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318274327", "html_url": "https://github.com/tensorflow/tensorflow/issues/11725#issuecomment-318274327", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725", "id": 318274327, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODI3NDMyNw==", "user": {"login": "junshi15", "id": 12075848, "node_id": "MDQ6VXNlcjEyMDc1ODQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/12075848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junshi15", "html_url": "https://github.com/junshi15", "followers_url": "https://api.github.com/users/junshi15/followers", "following_url": "https://api.github.com/users/junshi15/following{/other_user}", "gists_url": "https://api.github.com/users/junshi15/gists{/gist_id}", "starred_url": "https://api.github.com/users/junshi15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junshi15/subscriptions", "organizations_url": "https://api.github.com/users/junshi15/orgs", "repos_url": "https://api.github.com/users/junshi15/repos", "events_url": "https://api.github.com/users/junshi15/events{/privacy}", "received_events_url": "https://api.github.com/users/junshi15/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T06:47:04Z", "updated_at": "2017-07-27T06:47:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p>My observation agrees with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22274255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shamoya\">@shamoya</a> 's.</p>\n<ol>\n<li>the program got stuck with the trivial model, but it needed 8 GPUs per worker. Even 7 GPUs do not result in hang (after running 1000 iterations).</li>\n<li>the program got stuck either at warm-up or early iterations (&lt; 100).</li>\n<li>locking global_step does not help.</li>\n<li>At the very beginning, some tensors changed size as shown below. This may not be an issue, but it will be good to find out why.</li>\n</ol>\n<pre><code>tensor and buffer size do not agree! buffer_size = 653 requested tensor size = 593 Tensor&lt;type: int64 shape: [0,1] values: &gt; key = /job:ps/replica:0/task:0/cpu:0;ccc0db7aa974ba53;/job:worker/replica:0/task:0/gpu:0;edge_37_report_uninitialized_variables/boolean_mask/Where;0:0\n\ntensor and buffer size do not agree! buffer_size = 649 requested tensor size = 589 Tensor&lt;type: int64 shape: [0] values: &gt; key = /job:worker/replica:0/task:0/gpu:0;62e354fc58da5afa;/job:ps/replica:0/task:0/cpu:0;edge_39_report_uninitialized_variables/boolean_mask/Squeeze;0:0\n</code></pre>", "body_text": "My observation agrees with @shamoya 's.\n\nthe program got stuck with the trivial model, but it needed 8 GPUs per worker. Even 7 GPUs do not result in hang (after running 1000 iterations).\nthe program got stuck either at warm-up or early iterations (< 100).\nlocking global_step does not help.\nAt the very beginning, some tensors changed size as shown below. This may not be an issue, but it will be good to find out why.\n\ntensor and buffer size do not agree! buffer_size = 653 requested tensor size = 593 Tensor<type: int64 shape: [0,1] values: > key = /job:ps/replica:0/task:0/cpu:0;ccc0db7aa974ba53;/job:worker/replica:0/task:0/gpu:0;edge_37_report_uninitialized_variables/boolean_mask/Where;0:0\n\ntensor and buffer size do not agree! buffer_size = 649 requested tensor size = 589 Tensor<type: int64 shape: [0] values: > key = /job:worker/replica:0/task:0/gpu:0;62e354fc58da5afa;/job:ps/replica:0/task:0/cpu:0;edge_39_report_uninitialized_variables/boolean_mask/Squeeze;0:0", "body": "My observation agrees with @shamoya 's.\r\n\r\n1) the program got stuck with the trivial model, but it needed 8 GPUs per worker. Even 7 GPUs do not result in hang (after running 1000 iterations).\r\n2) the program got stuck either at warm-up or early iterations (< 100).\r\n3) locking global_step does not help.\r\n4) At the very beginning, some tensors changed size as shown below. This may not be an issue, but it will be good to find out why.\r\n\r\n```\r\ntensor and buffer size do not agree! buffer_size = 653 requested tensor size = 593 Tensor<type: int64 shape: [0,1] values: > key = /job:ps/replica:0/task:0/cpu:0;ccc0db7aa974ba53;/job:worker/replica:0/task:0/gpu:0;edge_37_report_uninitialized_variables/boolean_mask/Where;0:0\r\n\r\ntensor and buffer size do not agree! buffer_size = 649 requested tensor size = 589 Tensor<type: int64 shape: [0] values: > key = /job:worker/replica:0/task:0/gpu:0;62e354fc58da5afa;/job:ps/replica:0/task:0/cpu:0;edge_39_report_uninitialized_variables/boolean_mask/Squeeze;0:0\r\n```"}