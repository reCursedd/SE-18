{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318470372", "html_url": "https://github.com/tensorflow/tensorflow/issues/11725#issuecomment-318470372", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725", "id": 318470372, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODQ3MDM3Mg==", "user": {"login": "junshi15", "id": 12075848, "node_id": "MDQ6VXNlcjEyMDc1ODQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/12075848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junshi15", "html_url": "https://github.com/junshi15", "followers_url": "https://api.github.com/users/junshi15/followers", "following_url": "https://api.github.com/users/junshi15/following{/other_user}", "gists_url": "https://api.github.com/users/junshi15/gists{/gist_id}", "starred_url": "https://api.github.com/users/junshi15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junshi15/subscriptions", "organizations_url": "https://api.github.com/users/junshi15/orgs", "repos_url": "https://api.github.com/users/junshi15/repos", "events_url": "https://api.github.com/users/junshi15/events{/privacy}", "received_events_url": "https://api.github.com/users/junshi15/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T20:01:26Z", "updated_at": "2017-07-27T20:01:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22274255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shamoya\">@shamoya</a> intriguing discovery. Local tensor transfer is handled by BaseRemoteRendezvous. There is one difference between gRPC and RDMA: tolerate_dup_recv is set to <code>false</code> in <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.cc#L42\">the former</a> and <code>true</code> in <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L34\">the latter</a>. The reason: gRPC only receives a tensor once, with verbs we have multiple receive attempts, since the tx/rx buffer may not be ready at early attempts. I ran some experiments by changing that to <code>true</code> in gPRC model, no hang was observed. However <a href=\"https://github.com/tensorflow/tensorflow/commit/cbfd50ff0f01e1825922230a8bc6e5766da98dd7#diff-b9ae16e68ba80801fe243bb5e19bac51\">this patch</a> totally broke the verbs code. I had to raise an issue <a href=\"https://github.com/tensorflow/tensorflow/issues/11825\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11825/hovercard\">here</a>. Something to worry about after this debug.</p>\n<p>A correction to what I said early, I finally saw hang with 7 GPUs per worker, after 2000 steps.</p>", "body_text": "@shamoya intriguing discovery. Local tensor transfer is handled by BaseRemoteRendezvous. There is one difference between gRPC and RDMA: tolerate_dup_recv is set to false in the former and true in the latter. The reason: gRPC only receives a tensor once, with verbs we have multiple receive attempts, since the tx/rx buffer may not be ready at early attempts. I ran some experiments by changing that to true in gPRC model, no hang was observed. However this patch totally broke the verbs code. I had to raise an issue here. Something to worry about after this debug.\nA correction to what I said early, I finally saw hang with 7 GPUs per worker, after 2000 steps.", "body": "@shamoya intriguing discovery. Local tensor transfer is handled by BaseRemoteRendezvous. There is one difference between gRPC and RDMA: tolerate_dup_recv is set to `false` in [the former](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.cc#L42) and `true` in [the latter](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L34). The reason: gRPC only receives a tensor once, with verbs we have multiple receive attempts, since the tx/rx buffer may not be ready at early attempts. I ran some experiments by changing that to `true` in gPRC model, no hang was observed. However [this patch](https://github.com/tensorflow/tensorflow/commit/cbfd50ff0f01e1825922230a8bc6e5766da98dd7#diff-b9ae16e68ba80801fe243bb5e19bac51) totally broke the verbs code. I had to raise an issue [here](https://github.com/tensorflow/tensorflow/issues/11825). Something to worry about after this debug.\r\n\r\nA correction to what I said early, I finally saw hang with 7 GPUs per worker, after 2000 steps.\r\n\r\n\r\n"}