{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318385213", "html_url": "https://github.com/tensorflow/tensorflow/issues/11725#issuecomment-318385213", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11725", "id": 318385213, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODM4NTIxMw==", "user": {"login": "shamoya", "id": 22274255, "node_id": "MDQ6VXNlcjIyMjc0MjU1", "avatar_url": "https://avatars2.githubusercontent.com/u/22274255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shamoya", "html_url": "https://github.com/shamoya", "followers_url": "https://api.github.com/users/shamoya/followers", "following_url": "https://api.github.com/users/shamoya/following{/other_user}", "gists_url": "https://api.github.com/users/shamoya/gists{/gist_id}", "starred_url": "https://api.github.com/users/shamoya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shamoya/subscriptions", "organizations_url": "https://api.github.com/users/shamoya/orgs", "repos_url": "https://api.github.com/users/shamoya/repos", "events_url": "https://api.github.com/users/shamoya/events{/privacy}", "received_events_url": "https://api.github.com/users/shamoya/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T14:49:16Z", "updated_at": "2017-07-27T14:53:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6030236\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/katyakats\">@katyakats</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22419555\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bkovalev\">@bkovalev</a><br>\nOk, after reviewing the full logs, this is what we think is the root cause:<br>\nA single GPU (in worker1) doesn't complete loading of the model parameters from the CPU.<br>\nFor this GPU we don't see \"Async kernel done\" for the SEND/RECV operation (CPU:0 -&gt; GPU:x locally).</p>\n<p>The reason why it happens only with RDMA (and not with gRPC) is not known yet.<br>\nThought about possible interrupts issue due to excessive interrupts (8 GPUs + NIC + NVMe drive which holds the ImageNet data, all on the same PCIe bus). however polling mode of Process_CQ (no interrupts from the NIC at all) didn't resolve the issue.</p>\n<p><a href=\"https://gist.github.com/shamoya/824d452be527d95902f20b59f868b391\">This</a> is the problematic GPU relevant logs.</p>\n<p>This is what I get from grepping one of the model parameters tensors  in the problematic GPU log (as above) VS one of the other valid GPUs:</p>\n<p>idos@MTR-IDOS $cat gpu2.log | grep \"affine1/biases\"<br>\n2017-07-27 16:04:48.740960: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G109 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:2\"</a> is dead: 0<br>\n2017-07-27 16:04:48.748463: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35506 step 286 v/affine1/biases/read_G108 = _Send<a href=\"v/affine1/biases/read\">T=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\"</a> is dead: 0</p>\n<p>idos@MTR-IDOS $cat gpu4.log | grep \"affine1/biases\"<br>\n2017-07-27 16:04:48.740332: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G105 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"</a> is dead: 0<br>\n2017-07-27 16:04:48.748482: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35508 step 286 v/affine1/biases/read_G104 = _Send<a href=\"v/affine1/biases/read\">T=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\"</a> is dead: 0<br>\n2017-07-27 16:04:48.764311: I tensorflow/core/common_runtime/executor.cc:1612] 0x7f1fc4e67ab0 Async kernel done: v/affine1/biases/read_G105 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"</a><br>\n2017-07-27 16:04:48.765221: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 69 step 286 v_4/tower_4/gradients/v_4/tower_4/L2Loss_3_grad/mul = Mul[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v/affine1/biase<br>\n/read_G105, v_4/tower_4/gradients/v_4/tower_4/mul_1_grad/Reshape_1) is dead: 0<br>\n2017-07-27 16:04:48.765199: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 9 step 286 v_4/tower_4/L2Loss_3 = L2Loss<a href=\"v/affine1/biases/read_G105\">T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"</a> is dead: 0<br>\n2017-07-27 16:04:48.765338: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 38 step 286 v_4/tower_4/affine1/add = Add[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v_4/tower_4/affine1/MatMul, v/affine1/biase<br>\n/read_G105) is dead: 0</p>", "body_text": "@katyakats @bkovalev\nOk, after reviewing the full logs, this is what we think is the root cause:\nA single GPU (in worker1) doesn't complete loading of the model parameters from the CPU.\nFor this GPU we don't see \"Async kernel done\" for the SEND/RECV operation (CPU:0 -> GPU:x locally).\nThe reason why it happens only with RDMA (and not with gRPC) is not known yet.\nThought about possible interrupts issue due to excessive interrupts (8 GPUs + NIC + NVMe drive which holds the ImageNet data, all on the same PCIe bus). however polling mode of Process_CQ (no interrupts from the NIC at all) didn't resolve the issue.\nThis is the problematic GPU relevant logs.\nThis is what I get from grepping one of the model parameters tensors  in the problematic GPU log (as above) VS one of the other valid GPUs:\nidos@MTR-IDOS $cat gpu2.log | grep \"affine1/biases\"\n2017-07-27 16:04:48.740960: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G109 = _Recvclient_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:2\" is dead: 0\n2017-07-27 16:04:48.748463: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35506 step 286 v/affine1/biases/read_G108 = _SendT=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\" is dead: 0\nidos@MTR-IDOS $cat gpu4.log | grep \"affine1/biases\"\n2017-07-27 16:04:48.740332: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G105 = _Recvclient_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\" is dead: 0\n2017-07-27 16:04:48.748482: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35508 step 286 v/affine1/biases/read_G104 = _SendT=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\" is dead: 0\n2017-07-27 16:04:48.764311: I tensorflow/core/common_runtime/executor.cc:1612] 0x7f1fc4e67ab0 Async kernel done: v/affine1/biases/read_G105 = _Recvclient_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"\n2017-07-27 16:04:48.765221: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 69 step 286 v_4/tower_4/gradients/v_4/tower_4/L2Loss_3_grad/mul = Mul[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v/affine1/biase\n/read_G105, v_4/tower_4/gradients/v_4/tower_4/mul_1_grad/Reshape_1) is dead: 0\n2017-07-27 16:04:48.765199: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 9 step 286 v_4/tower_4/L2Loss_3 = L2LossT=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\" is dead: 0\n2017-07-27 16:04:48.765338: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 38 step 286 v_4/tower_4/affine1/add = Add[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v_4/tower_4/affine1/MatMul, v/affine1/biase\n/read_G105) is dead: 0", "body": "@katyakats @bkovalev\r\nOk, after reviewing the full logs, this is what we think is the root cause: \r\nA single GPU (in worker1) doesn't complete loading of the model parameters from the CPU.\r\nFor this GPU we don't see \"Async kernel done\" for the SEND/RECV operation (CPU:0 -> GPU:x locally).\r\n\r\nThe reason why it happens only with RDMA (and not with gRPC) is not known yet.\r\nThought about possible interrupts issue due to excessive interrupts (8 GPUs + NIC + NVMe drive which holds the ImageNet data, all on the same PCIe bus). however polling mode of Process_CQ (no interrupts from the NIC at all) didn't resolve the issue.\r\n\r\n[This](https://gist.github.com/shamoya/824d452be527d95902f20b59f868b391) is the problematic GPU relevant logs.\r\n\r\nThis is what I get from grepping one of the model parameters tensors  in the problematic GPU log (as above) VS one of the other valid GPUs:\r\n\r\nidos@MTR-IDOS $cat gpu2.log | grep \"affine1/biases\"\r\n2017-07-27 16:04:48.740960: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G109 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:2\"]() is dead: 0\r\n2017-07-27 16:04:48.748463: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35506 step 286 v/affine1/biases/read_G108 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\"](v/affine1/biases/read) is dead: 0\r\n\r\nidos@MTR-IDOS $cat gpu4.log | grep \"affine1/biases\"\r\n2017-07-27 16:04:48.740332: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G105 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"]() is dead: 0\r\n2017-07-27 16:04:48.748482: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35508 step 286 v/affine1/biases/read_G104 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\"](v/affine1/biases/read) is dead: 0\r\n2017-07-27 16:04:48.764311: I tensorflow/core/common_runtime/executor.cc:1612] 0x7f1fc4e67ab0 Async kernel done: v/affine1/biases/read_G105 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"]()\r\n2017-07-27 16:04:48.765221: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 69 step 286 v_4/tower_4/gradients/v_4/tower_4/L2Loss_3_grad/mul = Mul[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v/affine1/biase\r\n/read_G105, v_4/tower_4/gradients/v_4/tower_4/mul_1_grad/Reshape_1) is dead: 0\r\n2017-07-27 16:04:48.765199: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 9 step 286 v_4/tower_4/L2Loss_3 = L2Loss[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v/affine1/biases/read_G105) is dead: 0\r\n2017-07-27 16:04:48.765338: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 38 step 286 v_4/tower_4/affine1/add = Add[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v_4/tower_4/affine1/MatMul, v/affine1/biase\r\n/read_G105) is dead: 0"}