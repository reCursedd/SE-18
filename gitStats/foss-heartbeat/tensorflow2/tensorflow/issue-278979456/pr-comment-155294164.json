{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/155294164", "pull_request_review_id": 81581750, "id": 155294164, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NTI5NDE2NA==", "diff_hunk": "@@ -0,0 +1,312 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <cstdarg>\n+#include <cstdio>\n+#include <cstdlib>\n+#include <fstream>\n+#include <iostream>\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <unordered_set>\n+#include <vector>\n+\n+#include <fcntl.h>\n+#include <getopt.h>\n+#include <sys/time.h>\n+#include <sys/types.h>\n+#include <sys/uio.h>\n+#include <unistd.h>\n+\n+#include \"tensorflow/contrib/lite/kernels/register.h\"\n+#include \"tensorflow/contrib/lite/model.h\"\n+#include \"tensorflow/contrib/lite/optional_debug_tools.h\"\n+#include \"tensorflow/contrib/lite/string_util.h\"\n+\n+#include \"label_image.h\"\n+\n+#define LOG(x) std::cerr\n+#define CHECK(x)                  \\\n+  if (!(x)) {                     \\\n+    LOG(ERROR) << #x << \"failed\"; \\\n+    exit(1);                      \\\n+  }\n+\n+namespace tflite {\n+namespace label_image {\n+\n+using std::string;\n+\n+bool verbose = false;\n+bool accel = true;\n+bool input_floating = false;\n+int loop_count = 1;\n+\n+float input_mean = 127.5f;\n+float input_std = 127.5f;\n+\n+double get_us(struct timeval t) { return (t.tv_sec * 1000000 + t.tv_usec); }\n+\n+// Takes a file name, and loads a list of labels from it, one per line, and\n+// returns a vector of the strings. It pads with empty strings so the length\n+// of the result is a multiple of 16, because our model expects that.\n+TfLiteStatus ReadLabelsFile(const string& file_name,\n+                            std::vector<string>* result,\n+                            size_t* found_label_count) {\n+  std::ifstream file(file_name);\n+  if (!file) {\n+    LOG(FATAL) << \"Labels file \" << file_name << \" not found\\n\";\n+    return kTfLiteError;\n+  }\n+  result->clear();\n+  string line;\n+  while (std::getline(file, line)) {\n+    result->push_back(line);\n+  }\n+  *found_label_count = result->size();\n+  const int padding = 16;\n+  while (result->size() % padding) {\n+    result->emplace_back();\n+  }\n+  return kTfLiteOk;\n+}\n+\n+void RunInference(const std::string& graph, const std::string& input_layer_type,\n+                  int num_threads, const std::string& input_bmp_name,\n+                  const std::string& labels_file_name) {\n+  CHECK(graph.c_str());\n+\n+  std::unique_ptr<tflite::FlatBufferModel> model;\n+  std::unique_ptr<tflite::Interpreter> interpreter;\n+\n+  model = tflite::FlatBufferModel::BuildFromFile(graph.c_str());\n+  if (!model) {\n+    LOG(FATAL) << \"\\nFailed to mmap model \" << graph << \"\\n\";\n+    exit(-1);\n+  }\n+  LOG(INFO) << \"Loaded model \" << graph << \"\\n\";\n+  model->error_reporter();\n+  LOG(INFO) << \"resolved reporter\\n\";\n+\n+  tflite::ops::builtin::BuiltinOpResolver resolver;\n+\n+  tflite::InterpreterBuilder(*model, resolver)(&interpreter);\n+  if (!interpreter) {\n+    LOG(FATAL) << \"Failed to construct interpreter\\n\";\n+    exit(-1);\n+  }\n+\n+  if (!accel) interpreter->UseNNAPI(accel);\n+\n+  if (verbose) {\n+    LOG(INFO) << \"tensors size: \" << interpreter->tensors_size() << \"\\n\";\n+    LOG(INFO) << \"nodes size: \" << interpreter->nodes_size() << \"\\n\";\n+    LOG(INFO) << \"inputs: \" << interpreter->inputs().size() << \"\\n\";\n+    LOG(INFO) << \"input(0) name: \" << interpreter->GetInputName(0) << \"\\n\";\n+\n+    int t_size = interpreter->tensors_size();\n+    for (int i = 0; i < t_size; i++) {\n+      if (interpreter->tensor(i)->name)\n+        LOG(INFO) << i << \": \" << interpreter->tensor(i)->name << \", \"\n+                  << interpreter->tensor(i)->bytes << \", \"\n+                  << interpreter->tensor(i)->type << \", \"\n+                  << interpreter->tensor(i)->params.scale << \", \"\n+                  << interpreter->tensor(i)->params.zero_point << \"\\n\";\n+    }\n+  }\n+\n+  if (num_threads != -1) {\n+    interpreter->SetNumThreads(num_threads);\n+  }\n+\n+  int image_width = 224;\n+  int image_height = 224;\n+  int image_channels = 3;\n+  uint8_t* in =\n+      read_bmp(input_bmp_name, image_width, image_height, image_channels);\n+\n+  int input = interpreter->inputs()[0];\n+  if (verbose) LOG(INFO) << \"input: \" << input << \"\\n\";\n+\n+  const std::vector<int> inputs = interpreter->inputs();\n+  const std::vector<int> outputs = interpreter->outputs();\n+\n+  if (verbose) {\n+    LOG(INFO) << \"number of inputs: \" << inputs.size() << \"\\n\";\n+    LOG(INFO) << \"number of outputs: \" << outputs.size() << \"\\n\";\n+  }\n+\n+  if (interpreter->AllocateTensors() != kTfLiteOk) {\n+    LOG(FATAL) << \"Failed to allocate tensors!\";\n+  }\n+\n+  if (verbose) PrintInterpreterState(interpreter.get());\n+\n+  // get input dimension from the input tensor metadata\n+  // assuming one input only\n+  TfLiteIntArray* dims = interpreter->tensor(input)->dims;\n+  int wanted_height = dims->data[1];\n+  int wanted_width = dims->data[2];\n+  int wanted_channels = dims->data[3];\n+\n+  if (input_floating)\n+    downsize<float>(interpreter->typed_tensor<float>(input), in, image_height,\n+                    image_width, image_channels, wanted_height, wanted_width,\n+                    wanted_channels);\n+  else", "path": "tensorflow/contrib/lite/examples/label_image/label_image.cc", "position": null, "original_position": 169, "commit_id": "8b7ad1194bbcb6e3f147791381c7502edf8b0ba8", "original_commit_id": "9d6093d09ebeb547fedf08aa05a8174d720dc651", "user": {"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}, "body": "Except for very short if-statements, please use curly braces.", "created_at": "2017-12-06T16:51:46Z", "updated_at": "2017-12-14T07:45:55Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/15095#discussion_r155294164", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15095", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/155294164"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/15095#discussion_r155294164"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15095"}}, "body_html": "<p>Except for very short if-statements, please use curly braces.</p>", "body_text": "Except for very short if-statements, please use curly braces."}