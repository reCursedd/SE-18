{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267025615", "html_url": "https://github.com/tensorflow/tensorflow/issues/6209#issuecomment-267025615", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6209", "id": 267025615, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzAyNTYxNQ==", "user": {"login": "rdipietro", "id": 5150559, "node_id": "MDQ6VXNlcjUxNTA1NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5150559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rdipietro", "html_url": "https://github.com/rdipietro", "followers_url": "https://api.github.com/users/rdipietro/followers", "following_url": "https://api.github.com/users/rdipietro/following{/other_user}", "gists_url": "https://api.github.com/users/rdipietro/gists{/gist_id}", "starred_url": "https://api.github.com/users/rdipietro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rdipietro/subscriptions", "organizations_url": "https://api.github.com/users/rdipietro/orgs", "repos_url": "https://api.github.com/users/rdipietro/repos", "events_url": "https://api.github.com/users/rdipietro/events{/privacy}", "received_events_url": "https://api.github.com/users/rdipietro/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-14T12:48:55Z", "updated_at": "2016-12-14T12:48:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't see why the gradient with respect to time step 5 (for example) should depend in any way on the information at time step 10. Is this actually happening?</p>\n<p>The logs I include above suggest that this isn't happening: it works if there is only one NaN after the valid data.</p>\n<p>Your suggested workaround to the bug works for this simplified case but doesn't work in the general case. In a real RNN setting, the batch is made up of multiple sequences, each with NaNs at different places, so flattening along batch/time with <code>tf.boolean_mask</code> does not work. (We could possibly post process segments independently, but this would a) lead to messy code and b) make us lose the efficiency gains from batch processing.)</p>", "body_text": "I don't see why the gradient with respect to time step 5 (for example) should depend in any way on the information at time step 10. Is this actually happening?\nThe logs I include above suggest that this isn't happening: it works if there is only one NaN after the valid data.\nYour suggested workaround to the bug works for this simplified case but doesn't work in the general case. In a real RNN setting, the batch is made up of multiple sequences, each with NaNs at different places, so flattening along batch/time with tf.boolean_mask does not work. (We could possibly post process segments independently, but this would a) lead to messy code and b) make us lose the efficiency gains from batch processing.)", "body": "I don't see why the gradient with respect to time step 5 (for example) should depend in any way on the information at time step 10. Is this actually happening?\r\n\r\nThe logs I include above suggest that this isn't happening: it works if there is only one NaN after the valid data.\r\n\r\nYour suggested workaround to the bug works for this simplified case but doesn't work in the general case. In a real RNN setting, the batch is made up of multiple sequences, each with NaNs at different places, so flattening along batch/time with `tf.boolean_mask` does not work. (We could possibly post process segments independently, but this would a) lead to messy code and b) make us lose the efficiency gains from batch processing.)"}