{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/250363094", "html_url": "https://github.com/tensorflow/tensorflow/issues/4600#issuecomment-250363094", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4600", "id": 250363094, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDM2MzA5NA==", "user": {"login": "Karl-Krauth", "id": 4045620, "node_id": "MDQ6VXNlcjQwNDU2MjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/4045620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Karl-Krauth", "html_url": "https://github.com/Karl-Krauth", "followers_url": "https://api.github.com/users/Karl-Krauth/followers", "following_url": "https://api.github.com/users/Karl-Krauth/following{/other_user}", "gists_url": "https://api.github.com/users/Karl-Krauth/gists{/gist_id}", "starred_url": "https://api.github.com/users/Karl-Krauth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Karl-Krauth/subscriptions", "organizations_url": "https://api.github.com/users/Karl-Krauth/orgs", "repos_url": "https://api.github.com/users/Karl-Krauth/repos", "events_url": "https://api.github.com/users/Karl-Krauth/events{/privacy}", "received_events_url": "https://api.github.com/users/Karl-Krauth/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-29T03:41:53Z", "updated_at": "2016-09-29T03:41:53Z", "author_association": "NONE", "body_html": "<p>Thanks for the reply <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=592670\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/concretevitamin\">@concretevitamin</a> :)</p>\n<p>I'm not entirely sure a complex scheduling policy would be needed in this case. Wouldn't it be enough to just compute the gradient from each iteration individually and then incrementally build up the overall gradient of <code>foldl</code>? This would be similar to the way I imagine the actual value of <code>foldl</code> is currently computed.</p>\n<p>Also do you happen to know of any way I could work around this memory issue given the current implementation of tensorflow?</p>", "body_text": "Thanks for the reply @concretevitamin :)\nI'm not entirely sure a complex scheduling policy would be needed in this case. Wouldn't it be enough to just compute the gradient from each iteration individually and then incrementally build up the overall gradient of foldl? This would be similar to the way I imagine the actual value of foldl is currently computed.\nAlso do you happen to know of any way I could work around this memory issue given the current implementation of tensorflow?", "body": "Thanks for the reply @concretevitamin :)\n\nI'm not entirely sure a complex scheduling policy would be needed in this case. Wouldn't it be enough to just compute the gradient from each iteration individually and then incrementally build up the overall gradient of `foldl`? This would be similar to the way I imagine the actual value of `foldl` is currently computed.\n\nAlso do you happen to know of any way I could work around this memory issue given the current implementation of tensorflow?\n"}