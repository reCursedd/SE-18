{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4600", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4600/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4600/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4600/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4600", "id": 179545592, "node_id": "MDU6SXNzdWUxNzk1NDU1OTI=", "number": 4600, "title": "fold/scan gradient high memory usage", "user": {"login": "Karl-Krauth", "id": 4045620, "node_id": "MDQ6VXNlcjQwNDU2MjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/4045620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Karl-Krauth", "html_url": "https://github.com/Karl-Krauth", "followers_url": "https://api.github.com/users/Karl-Krauth/followers", "following_url": "https://api.github.com/users/Karl-Krauth/following{/other_user}", "gists_url": "https://api.github.com/users/Karl-Krauth/gists{/gist_id}", "starred_url": "https://api.github.com/users/Karl-Krauth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Karl-Krauth/subscriptions", "organizations_url": "https://api.github.com/users/Karl-Krauth/orgs", "repos_url": "https://api.github.com/users/Karl-Krauth/repos", "events_url": "https://api.github.com/users/Karl-Krauth/events{/privacy}", "received_events_url": "https://api.github.com/users/Karl-Krauth/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2016-09-27T16:34:09Z", "updated_at": "2016-12-11T00:18:49Z", "closed_at": "2016-12-11T00:18:49Z", "author_association": "NONE", "body_html": "<p>It seems that the gradients for higher order functions (foldl/foldr/scan) require a very large amount of memory.</p>\n<p>The following code block will easily compute <code>res</code> (even with <code>swap_memory</code> disabled) but will fail when computing <code>grad</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>v <span class=\"pl-k\">=</span> tf.Variable([<span class=\"pl-c1\">1.0</span>])\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">foo</span>(<span class=\"pl-smi\">aggregate</span>, <span class=\"pl-smi\">arr</span>):\n    arr <span class=\"pl-k\">=</span> v <span class=\"pl-k\">*</span> tf.tile(arr, [<span class=\"pl-c1\">10000</span>])\n    <span class=\"pl-k\">return</span> aggregate <span class=\"pl-k\">+</span> arr[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]\n\narr <span class=\"pl-k\">=</span> np.empty([<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">1000</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\nres <span class=\"pl-k\">=</span> tf.foldl(foo, arr, <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">swap_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nopt <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.001</span>)\ngrad <span class=\"pl-k\">=</span> opt.compute_gradients(res)\n\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(tf.initialize_all_variables())\nsess.run(res) <span class=\"pl-c\"><span class=\"pl-c\">#</span>This will run</span>\nsess.run(grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span>This will cause an OOM error</span></pre></div>\n<p>This was tested on a machine with a GTX1070 and 24GB RAM.</p>\n<p>I'm guessing that what's happening is that tensorflow is only keeping track of data necessary for one iteration of <code>foldl</code> at a time when computing <code>res</code> (~40MB) but has to keep track of all the memory in all iterations of <code>foldl</code> when computing <code>grad</code> (~400GB).</p>\n<p>In fact, reducing <code>arr</code> to be of size 1000x1000, still causes an OOM error, but reducing the size of <code>arr</code> to be 500x1000 ends up with the code block succeeding. This makes sense given that setting <code>arr</code> to these two sizes requires ~40GB and ~20GB respectively, the latter of which just fits on my machine's memory.</p>\n<p>From what I understand of automatic differentiation, tensorflow shouldn't need to simultaneously keep track of the data needed across all iterations of <code>foldl</code> when computing gradients. Or am I missing something?</p>", "body_text": "It seems that the gradients for higher order functions (foldl/foldr/scan) require a very large amount of memory.\nThe following code block will easily compute res (even with swap_memory disabled) but will fail when computing grad:\nv = tf.Variable([1.0])\ndef foo(aggregate, arr):\n    arr = v * tf.tile(arr, [10000])\n    return aggregate + arr[0, 0]\n\narr = np.empty([10000, 1000], dtype=np.float32)\nres = tf.foldl(foo, arr, 0.0, 1, swap_memory=True)\nopt = tf.train.GradientDescentOptimizer(0.001)\ngrad = opt.compute_gradients(res)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nsess.run(res) #This will run\nsess.run(grad) #This will cause an OOM error\nThis was tested on a machine with a GTX1070 and 24GB RAM.\nI'm guessing that what's happening is that tensorflow is only keeping track of data necessary for one iteration of foldl at a time when computing res (~40MB) but has to keep track of all the memory in all iterations of foldl when computing grad (~400GB).\nIn fact, reducing arr to be of size 1000x1000, still causes an OOM error, but reducing the size of arr to be 500x1000 ends up with the code block succeeding. This makes sense given that setting arr to these two sizes requires ~40GB and ~20GB respectively, the latter of which just fits on my machine's memory.\nFrom what I understand of automatic differentiation, tensorflow shouldn't need to simultaneously keep track of the data needed across all iterations of foldl when computing gradients. Or am I missing something?", "body": "It seems that the gradients for higher order functions (foldl/foldr/scan) require a very large amount of memory.\n\nThe following code block will easily compute `res` (even with `swap_memory` disabled) but will fail when computing `grad`:\n\n``` python\nv = tf.Variable([1.0])\ndef foo(aggregate, arr):\n    arr = v * tf.tile(arr, [10000])\n    return aggregate + arr[0, 0]\n\narr = np.empty([10000, 1000], dtype=np.float32)\nres = tf.foldl(foo, arr, 0.0, 1, swap_memory=True)\nopt = tf.train.GradientDescentOptimizer(0.001)\ngrad = opt.compute_gradients(res)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nsess.run(res) #This will run\nsess.run(grad) #This will cause an OOM error\n```\n\nThis was tested on a machine with a GTX1070 and 24GB RAM. \n\nI'm guessing that what's happening is that tensorflow is only keeping track of data necessary for one iteration of `foldl` at a time when computing `res` (~40MB) but has to keep track of all the memory in all iterations of `foldl` when computing `grad` (~400GB).\n\nIn fact, reducing `arr` to be of size 1000x1000, still causes an OOM error, but reducing the size of `arr` to be 500x1000 ends up with the code block succeeding. This makes sense given that setting `arr` to these two sizes requires ~40GB and ~20GB respectively, the latter of which just fits on my machine's memory.\n\nFrom what I understand of automatic differentiation, tensorflow shouldn't need to simultaneously keep track of the data needed across all iterations of `foldl` when computing gradients. Or am I missing something?\n"}