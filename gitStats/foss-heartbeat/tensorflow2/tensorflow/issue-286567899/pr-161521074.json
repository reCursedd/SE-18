{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927", "id": 161521074, "node_id": "MDExOlB1bGxSZXF1ZXN0MTYxNTIxMDc0", "html_url": "https://github.com/tensorflow/tensorflow/pull/15927", "diff_url": "https://github.com/tensorflow/tensorflow/pull/15927.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/15927.patch", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15927", "number": 15927, "state": "closed", "locked": false, "title": "R1.5/verbs w 0 copies", "user": {"login": "eladweiss", "id": 31474666, "node_id": "MDQ6VXNlcjMxNDc0NjY2", "avatar_url": "https://avatars2.githubusercontent.com/u/31474666?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eladweiss", "html_url": "https://github.com/eladweiss", "followers_url": "https://api.github.com/users/eladweiss/followers", "following_url": "https://api.github.com/users/eladweiss/following{/other_user}", "gists_url": "https://api.github.com/users/eladweiss/gists{/gist_id}", "starred_url": "https://api.github.com/users/eladweiss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eladweiss/subscriptions", "organizations_url": "https://api.github.com/users/eladweiss/orgs", "repos_url": "https://api.github.com/users/eladweiss/repos", "events_url": "https://api.github.com/users/eladweiss/events{/privacy}", "received_events_url": "https://api.github.com/users/eladweiss/received_events", "type": "User", "site_admin": false}, "body": "## Verbs implementation to use direct tensor writes (0 copies)\r\n\r\n### Motivation:\r\n\r\nFollowing HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.\r\n\r\n### Performance:\r\n\r\nCompared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.\r\n\r\n### Implementation requirements:\r\n\r\n1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). \r\n2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.\r\n3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.\r\n\r\n### Implementation constrains:\r\n\r\nFor best stability and proof of correctness, we will divide the implementation to two stages:\r\n1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. \r\n2. At second stage, we will re-iterate over the code and remove irrelevant code parts.\r\nThe design of the solution aims that we will achieve both stages with relative ease. \r\n\r\n### Design guidelines:\r\n\r\n1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.\r\n2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.\r\n3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). \r\n4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple \"Response\" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.\r\n5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single \"Request\" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.\r\n\r\n### New types/classes:\r\n\r\n* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.\r\n* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.\r\n* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.\r\n* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.\r\n* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).\r\n* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.\r\n* **class RdmaTensorRequest**  - Holds and manages information for a single tensor request throughout the entire receive cycle. API:\r\n\t* Start() - Start the request.\r\n\t* RecvTensorMetaData() - Receive meta-data from the remote side.\r\n\t* RecvTensorContent() - Receive tensor content from the remote side and invoke the done() callback. \r\n* **class RdmaTensorResponse** - Holds information for a single tensor response, such as destination address and rkey.\r\n\r\n### Protocol changes:\r\n\r\nThe protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the \"buffer_size\" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the \"tensor_bytes\" field. Instead, we use that field to pass the \"request_index\".\r\n\r\n### Message structure:\r\n\r\n| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |\r\n|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|\r\n|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |\r\n\r\n* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. \r\n\t* type - The message type.\r\n\t* name (name_size) - Name of the requested tensor.\r\n\t* step_id - Step ID.\r\n\t* request_index - Request index.\r\n\t* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.\r\n* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).\r\n\t* type - The message type.\r\n\t* request_index - Request index.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.\r\n* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.\r\n\t* type - The message type.\r\n\t* name (name_size) - Name of the requested tensor.\r\n\t* step_id - Step ID.\r\n\t* request_index - Request index.\r\n\t* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.\r\n* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.\r\n* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.\r\n\r\n![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg \"Phase 1 message protocol\")\r\n\r\n### Second stage optimizations:\r\n1. Remove unused code leftovers.\r\n2. Remove the ACK buffer completely, since we can rely completely on its immediate value.\r\n\r\n### Future optimizations:\r\n1. Map the tensor names to indexes, to significantly reduce the request message size.\r\n2. Understand the purpose of empty tensors and if we can skip remote fetching for them.\r\n3. Consider concatenating multiple requests and/or using multiple message buffers.\r\n4. Consider a no-request architecture.\r\n  ", "created_at": "2018-01-07T13:09:15Z", "updated_at": "2018-01-10T12:18:58Z", "closed_at": "2018-01-10T12:18:57Z", "merged_at": null, "merge_commit_sha": "d35f3909a8b2aeac7e2c3625ce0fe6e233068dc1", "assignee": null, "assignees": [], "requested_reviewers": [], "requested_teams": [], "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "milestone": null, "commits_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927/commits", "review_comments_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927/comments", "review_comment_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15927/comments", "statuses_url": "https://api.github.com/repos/tensorflow/tensorflow/statuses/0a7ada105770febc75794b4003b7aa6584e2e753", "head": {"label": "Mellanox:r1.5/verbs_w_0_copies", "ref": "r1.5/verbs_w_0_copies", "sha": "0a7ada105770febc75794b4003b7aa6584e2e753", "user": {"login": "Mellanox", "id": 5813145, "node_id": "MDEyOk9yZ2FuaXphdGlvbjU4MTMxNDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5813145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mellanox", "html_url": "https://github.com/Mellanox", "followers_url": "https://api.github.com/users/Mellanox/followers", "following_url": "https://api.github.com/users/Mellanox/following{/other_user}", "gists_url": "https://api.github.com/users/Mellanox/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mellanox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mellanox/subscriptions", "organizations_url": "https://api.github.com/users/Mellanox/orgs", "repos_url": "https://api.github.com/users/Mellanox/repos", "events_url": "https://api.github.com/users/Mellanox/events{/privacy}", "received_events_url": "https://api.github.com/users/Mellanox/received_events", "type": "Organization", "site_admin": false}, "repo": {"id": 103814912, "node_id": "MDEwOlJlcG9zaXRvcnkxMDM4MTQ5MTI=", "name": "tensorflow", "full_name": "Mellanox/tensorflow", "private": false, "owner": {"login": "Mellanox", "id": 5813145, "node_id": "MDEyOk9yZ2FuaXphdGlvbjU4MTMxNDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5813145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mellanox", "html_url": "https://github.com/Mellanox", "followers_url": "https://api.github.com/users/Mellanox/followers", "following_url": "https://api.github.com/users/Mellanox/following{/other_user}", "gists_url": "https://api.github.com/users/Mellanox/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mellanox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mellanox/subscriptions", "organizations_url": "https://api.github.com/users/Mellanox/orgs", "repos_url": "https://api.github.com/users/Mellanox/repos", "events_url": "https://api.github.com/users/Mellanox/events{/privacy}", "received_events_url": "https://api.github.com/users/Mellanox/received_events", "type": "Organization", "site_admin": false}, "html_url": "https://github.com/Mellanox/tensorflow", "description": "Computation using data flow graphs for scalable machine learning", "fork": true, "url": "https://api.github.com/repos/Mellanox/tensorflow", "forks_url": "https://api.github.com/repos/Mellanox/tensorflow/forks", "keys_url": "https://api.github.com/repos/Mellanox/tensorflow/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/Mellanox/tensorflow/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/Mellanox/tensorflow/teams", "hooks_url": "https://api.github.com/repos/Mellanox/tensorflow/hooks", "issue_events_url": "https://api.github.com/repos/Mellanox/tensorflow/issues/events{/number}", "events_url": "https://api.github.com/repos/Mellanox/tensorflow/events", "assignees_url": "https://api.github.com/repos/Mellanox/tensorflow/assignees{/user}", "branches_url": "https://api.github.com/repos/Mellanox/tensorflow/branches{/branch}", "tags_url": "https://api.github.com/repos/Mellanox/tensorflow/tags", "blobs_url": "https://api.github.com/repos/Mellanox/tensorflow/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/Mellanox/tensorflow/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/Mellanox/tensorflow/git/refs{/sha}", "trees_url": "https://api.github.com/repos/Mellanox/tensorflow/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/Mellanox/tensorflow/statuses/{sha}", "languages_url": "https://api.github.com/repos/Mellanox/tensorflow/languages", "stargazers_url": "https://api.github.com/repos/Mellanox/tensorflow/stargazers", "contributors_url": "https://api.github.com/repos/Mellanox/tensorflow/contributors", "subscribers_url": "https://api.github.com/repos/Mellanox/tensorflow/subscribers", "subscription_url": "https://api.github.com/repos/Mellanox/tensorflow/subscription", "commits_url": "https://api.github.com/repos/Mellanox/tensorflow/commits{/sha}", "git_commits_url": "https://api.github.com/repos/Mellanox/tensorflow/git/commits{/sha}", "comments_url": "https://api.github.com/repos/Mellanox/tensorflow/comments{/number}", "issue_comment_url": "https://api.github.com/repos/Mellanox/tensorflow/issues/comments{/number}", "contents_url": "https://api.github.com/repos/Mellanox/tensorflow/contents/{+path}", "compare_url": "https://api.github.com/repos/Mellanox/tensorflow/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/Mellanox/tensorflow/merges", "archive_url": "https://api.github.com/repos/Mellanox/tensorflow/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/Mellanox/tensorflow/downloads", "issues_url": "https://api.github.com/repos/Mellanox/tensorflow/issues{/number}", "pulls_url": "https://api.github.com/repos/Mellanox/tensorflow/pulls{/number}", "milestones_url": "https://api.github.com/repos/Mellanox/tensorflow/milestones{/number}", "notifications_url": "https://api.github.com/repos/Mellanox/tensorflow/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/Mellanox/tensorflow/labels{/name}", "releases_url": "https://api.github.com/repos/Mellanox/tensorflow/releases{/id}", "deployments_url": "https://api.github.com/repos/Mellanox/tensorflow/deployments", "created_at": "2017-09-17T08:43:10Z", "updated_at": "2018-06-11T11:59:05Z", "pushed_at": "2018-06-06T13:31:15Z", "git_url": "git://github.com/Mellanox/tensorflow.git", "ssh_url": "git@github.com:Mellanox/tensorflow.git", "clone_url": "https://github.com/Mellanox/tensorflow.git", "svn_url": "https://github.com/Mellanox/tensorflow", "homepage": "http://tensorflow.org", "size": 173334, "stargazers_count": 1, "watchers_count": 1, "language": "C++", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "forks_count": 0, "mirror_url": null, "archived": false, "open_issues_count": 1, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "forks": 0, "open_issues": 1, "watchers": 1, "default_branch": "master"}}, "base": {"label": "tensorflow:r1.5", "ref": "r1.5", "sha": "793280a0a378e3cbaf558a7d8ef320f227287d11", "user": {"login": "tensorflow", "id": 15658638, "node_id": "MDEyOk9yZ2FuaXphdGlvbjE1NjU4NjM4", "avatar_url": "https://avatars1.githubusercontent.com/u/15658638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tensorflow", "html_url": "https://github.com/tensorflow", "followers_url": "https://api.github.com/users/tensorflow/followers", "following_url": "https://api.github.com/users/tensorflow/following{/other_user}", "gists_url": "https://api.github.com/users/tensorflow/gists{/gist_id}", "starred_url": "https://api.github.com/users/tensorflow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tensorflow/subscriptions", "organizations_url": "https://api.github.com/users/tensorflow/orgs", "repos_url": "https://api.github.com/users/tensorflow/repos", "events_url": "https://api.github.com/users/tensorflow/events{/privacy}", "received_events_url": "https://api.github.com/users/tensorflow/received_events", "type": "Organization", "site_admin": false}, "repo": {"id": 45717250, "node_id": "MDEwOlJlcG9zaXRvcnk0NTcxNzI1MA==", "name": "tensorflow", "full_name": "tensorflow/tensorflow", "private": false, "owner": {"login": "tensorflow", "id": 15658638, "node_id": "MDEyOk9yZ2FuaXphdGlvbjE1NjU4NjM4", "avatar_url": "https://avatars1.githubusercontent.com/u/15658638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tensorflow", "html_url": "https://github.com/tensorflow", "followers_url": "https://api.github.com/users/tensorflow/followers", "following_url": "https://api.github.com/users/tensorflow/following{/other_user}", "gists_url": "https://api.github.com/users/tensorflow/gists{/gist_id}", "starred_url": "https://api.github.com/users/tensorflow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tensorflow/subscriptions", "organizations_url": "https://api.github.com/users/tensorflow/orgs", "repos_url": "https://api.github.com/users/tensorflow/repos", "events_url": "https://api.github.com/users/tensorflow/events{/privacy}", "received_events_url": "https://api.github.com/users/tensorflow/received_events", "type": "Organization", "site_admin": false}, "html_url": "https://github.com/tensorflow/tensorflow", "description": "An Open Source Machine Learning Framework for Everyone", "fork": false, "url": "https://api.github.com/repos/tensorflow/tensorflow", "forks_url": "https://api.github.com/repos/tensorflow/tensorflow/forks", "keys_url": "https://api.github.com/repos/tensorflow/tensorflow/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/tensorflow/tensorflow/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/tensorflow/tensorflow/teams", "hooks_url": "https://api.github.com/repos/tensorflow/tensorflow/hooks", "issue_events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/events{/number}", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/events", "assignees_url": "https://api.github.com/repos/tensorflow/tensorflow/assignees{/user}", "branches_url": "https://api.github.com/repos/tensorflow/tensorflow/branches{/branch}", "tags_url": "https://api.github.com/repos/tensorflow/tensorflow/tags", "blobs_url": "https://api.github.com/repos/tensorflow/tensorflow/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/tensorflow/tensorflow/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/tensorflow/tensorflow/git/refs{/sha}", "trees_url": "https://api.github.com/repos/tensorflow/tensorflow/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/tensorflow/tensorflow/statuses/{sha}", "languages_url": "https://api.github.com/repos/tensorflow/tensorflow/languages", "stargazers_url": "https://api.github.com/repos/tensorflow/tensorflow/stargazers", "contributors_url": "https://api.github.com/repos/tensorflow/tensorflow/contributors", "subscribers_url": "https://api.github.com/repos/tensorflow/tensorflow/subscribers", "subscription_url": "https://api.github.com/repos/tensorflow/tensorflow/subscription", "commits_url": "https://api.github.com/repos/tensorflow/tensorflow/commits{/sha}", "git_commits_url": "https://api.github.com/repos/tensorflow/tensorflow/git/commits{/sha}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/comments{/number}", "issue_comment_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments{/number}", "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/{+path}", "compare_url": "https://api.github.com/repos/tensorflow/tensorflow/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/tensorflow/tensorflow/merges", "archive_url": "https://api.github.com/repos/tensorflow/tensorflow/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/tensorflow/tensorflow/downloads", "issues_url": "https://api.github.com/repos/tensorflow/tensorflow/issues{/number}", "pulls_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls{/number}", "milestones_url": "https://api.github.com/repos/tensorflow/tensorflow/milestones{/number}", "notifications_url": "https://api.github.com/repos/tensorflow/tensorflow/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/labels{/name}", "releases_url": "https://api.github.com/repos/tensorflow/tensorflow/releases{/id}", "deployments_url": "https://api.github.com/repos/tensorflow/tensorflow/deployments", "created_at": "2015-11-07T01:19:20Z", "updated_at": "2018-11-24T22:13:04Z", "pushed_at": "2018-11-24T18:40:19Z", "git_url": "git://github.com/tensorflow/tensorflow.git", "ssh_url": "git@github.com:tensorflow/tensorflow.git", "clone_url": "https://github.com/tensorflow/tensorflow.git", "svn_url": "https://github.com/tensorflow/tensorflow", "homepage": "https://tensorflow.org", "size": 284546, "stargazers_count": 115176, "watchers_count": 115176, "language": "C++", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "forks_count": 69946, "mirror_url": null, "archived": false, "open_issues_count": 1760, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "forks": 69946, "open_issues": 1760, "watchers": 115176, "default_branch": "master"}}, "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/15927"}, "issue": {"href": "https://api.github.com/repos/tensorflow/tensorflow/issues/15927"}, "comments": {"href": "https://api.github.com/repos/tensorflow/tensorflow/issues/15927/comments"}, "review_comments": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927/comments"}, "review_comment": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments{/number}"}, "commits": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927/commits"}, "statuses": {"href": "https://api.github.com/repos/tensorflow/tensorflow/statuses/0a7ada105770febc75794b4003b7aa6584e2e753"}}, "author_association": "CONTRIBUTOR", "body_html": "<h2>Verbs implementation to use direct tensor writes (0 copies)</h2>\n<h3>Motivation:</h3>\n<p>Following HKUST research on the use of GPU direct, and their <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md\">GDR implementation</a>, we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.</p>\n<h3>Performance:</h3>\n<p>Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.</p>\n<h3>Implementation requirements:</h3>\n<ol>\n<li>Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct).</li>\n<li>Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.</li>\n<li>Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.</li>\n</ol>\n<h3>Implementation constrains:</h3>\n<p>For best stability and proof of correctness, we will divide the implementation to two stages:</p>\n<ol>\n<li>At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance.</li>\n<li>At second stage, we will re-iterate over the code and remove irrelevant code parts.<br>\nThe design of the solution aims that we will achieve both stages with relative ease.</li>\n</ol>\n<h3>Design guidelines:</h3>\n<ol>\n<li>Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.</li>\n<li>The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.</li>\n<li>When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string).</li>\n<li>Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple \"Response\" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.</li>\n<li>With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single \"Request\" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.</li>\n</ol>\n<h3>New types/classes:</h3>\n<ul>\n<li><strong>enum RdmaImmDataType</strong> - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.</li>\n<li><strong>enum  RdmaWriteIDType</strong>    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.</li>\n<li><strong>class RdmaWriteID</strong>        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.</li>\n<li><strong>class RemoteAddressContext</strong> - Remote address information (address + mr). Will be passed as write context for tensor proto writes.</li>\n<li><strong>class RdmaTensorMetaData</strong> - Meta-data for a tensor (type, shape, is_dead, proto_size).</li>\n<li><strong>class RdmaMemoryMgr</strong>      - Manages the meta-data cache, and the registered memory regions.</li>\n<li><strong>class RdmaTensorRequest</strong>  - Holds and manages information for a single tensor request throughout the entire receive cycle. API:\n<ul>\n<li>Start() - Start the request.</li>\n<li>RecvTensorMetaData() - Receive meta-data from the remote side.</li>\n<li>RecvTensorContent() - Receive tensor content from the remote side and invoke the done() callback.</li>\n</ul>\n</li>\n<li><strong>class RdmaTensorResponse</strong> - Holds information for a single tensor response, such as destination address and rkey.</li>\n</ul>\n<h3>Protocol changes:</h3>\n<p>The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the \"buffer_size\" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the \"tensor_bytes\" field. Instead, we use that field to pass the \"request_index\".</p>\n<h3>Message structure:</h3>\n<table>\n<thead>\n<tr>\n<th>type</th>\n<th>name_size</th>\n<th>name</th>\n<th>step_id</th>\n<th>request_index</th>\n<th>remote_addr</th>\n<th>rkey</th>\n<th>is_dead</th>\n<th>data_type</th>\n<th>tensor_shape</th>\n<th>tensor_bytes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1B</td>\n<td>2B</td>\n<td>512</td>\n<td>8B</td>\n<td>8B</td>\n<td>8B</td>\n<td>4B</td>\n<td>1B</td>\n<td>XB</td>\n<td>XB</td>\n<td>8B</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><strong>RDMA_MESSAGE_TENSOR_REQUEST</strong>  - (receiver ==&gt; sender) The original tensor request.\n<ul>\n<li>type - The message type.</li>\n<li>name (name_size) - Name of the requested tensor.</li>\n<li>step_id - Step ID.</li>\n<li>request_index - Request index.</li>\n<li>remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.</li>\n<li>is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.</li>\n</ul>\n</li>\n<li><strong>RDMA_MESSAGE_BUFFER_REQUEST</strong>  - (sender ==&gt; receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).\n<ul>\n<li>type - The message type.</li>\n<li>request_index - Request index.</li>\n<li>is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.</li>\n</ul>\n</li>\n<li><strong>RDMA_MESSAGE_BUFFER_RESPONSE</strong> - (receiver ==&gt; sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.\n<ul>\n<li>type - The message type.</li>\n<li>name (name_size) - Name of the requested tensor.</li>\n<li>step_id - Step ID.</li>\n<li>request_index - Request index.</li>\n<li>remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.</li>\n<li>is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.</li>\n</ul>\n</li>\n<li><strong>RDMA_MESSAGE_TENSOR_WRITE</strong>    - (sender ==&gt; receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.</li>\n<li><strong>RDMA_MESSAGE_TENSOR_IDLE</strong>     - (receiver ==&gt; sender) No longer sent.</li>\n</ul>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg\"><img src=\"https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg\" alt=\"alt text\" title=\"Phase 1 message protocol\" style=\"max-width:100%;\"></a></p>\n<h3>Second stage optimizations:</h3>\n<ol>\n<li>Remove unused code leftovers.</li>\n<li>Remove the ACK buffer completely, since we can rely completely on its immediate value.</li>\n</ol>\n<h3>Future optimizations:</h3>\n<ol>\n<li>Map the tensor names to indexes, to significantly reduce the request message size.</li>\n<li>Understand the purpose of empty tensors and if we can skip remote fetching for them.</li>\n<li>Consider concatenating multiple requests and/or using multiple message buffers.</li>\n<li>Consider a no-request architecture.</li>\n</ol>", "body_text": "Verbs implementation to use direct tensor writes (0 copies)\nMotivation:\nFollowing HKUST research on the use of GPU direct, and their GDR implementation, we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.\nPerformance:\nCompared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.\nImplementation requirements:\n\nTensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct).\nNon DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.\nTensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.\n\nImplementation constrains:\nFor best stability and proof of correctness, we will divide the implementation to two stages:\n\nAt first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance.\nAt second stage, we will re-iterate over the code and remove irrelevant code parts.\nThe design of the solution aims that we will achieve both stages with relative ease.\n\nDesign guidelines:\n\nSince we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.\nThe address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.\nWhen the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string).\nSince the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple \"Response\" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.\nWith the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single \"Request\" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.\n\nNew types/classes:\n\nenum RdmaImmDataType - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.\nenum  RdmaWriteIDType    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.\nclass RdmaWriteID        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.\nclass RemoteAddressContext - Remote address information (address + mr). Will be passed as write context for tensor proto writes.\nclass RdmaTensorMetaData - Meta-data for a tensor (type, shape, is_dead, proto_size).\nclass RdmaMemoryMgr      - Manages the meta-data cache, and the registered memory regions.\nclass RdmaTensorRequest  - Holds and manages information for a single tensor request throughout the entire receive cycle. API:\n\nStart() - Start the request.\nRecvTensorMetaData() - Receive meta-data from the remote side.\nRecvTensorContent() - Receive tensor content from the remote side and invoke the done() callback.\n\n\nclass RdmaTensorResponse - Holds information for a single tensor response, such as destination address and rkey.\n\nProtocol changes:\nThe protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the \"buffer_size\" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the \"tensor_bytes\" field. Instead, we use that field to pass the \"request_index\".\nMessage structure:\n\n\n\ntype\nname_size\nname\nstep_id\nrequest_index\nremote_addr\nrkey\nis_dead\ndata_type\ntensor_shape\ntensor_bytes\n\n\n\n\n1B\n2B\n512\n8B\n8B\n8B\n4B\n1B\nXB\nXB\n8B\n\n\n\n\nRDMA_MESSAGE_TENSOR_REQUEST  - (receiver ==> sender) The original tensor request.\n\ntype - The message type.\nname (name_size) - Name of the requested tensor.\nstep_id - Step ID.\nrequest_index - Request index.\nremote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.\nis_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.\n\n\nRDMA_MESSAGE_BUFFER_REQUEST  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).\n\ntype - The message type.\nrequest_index - Request index.\nis_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.\n\n\nRDMA_MESSAGE_BUFFER_RESPONSE - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.\n\ntype - The message type.\nname (name_size) - Name of the requested tensor.\nstep_id - Step ID.\nrequest_index - Request index.\nremote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.\nis_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.\n\n\nRDMA_MESSAGE_TENSOR_WRITE    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.\nRDMA_MESSAGE_TENSOR_IDLE     - (receiver ==> sender) No longer sent.\n\n\nSecond stage optimizations:\n\nRemove unused code leftovers.\nRemove the ACK buffer completely, since we can rely completely on its immediate value.\n\nFuture optimizations:\n\nMap the tensor names to indexes, to significantly reduce the request message size.\nUnderstand the purpose of empty tensors and if we can skip remote fetching for them.\nConsider concatenating multiple requests and/or using multiple message buffers.\nConsider a no-request architecture.", "merged": false, "mergeable": null, "rebaseable": null, "mergeable_state": "unknown", "merged_by": null, "comments": 9, "review_comments": 2, "maintainer_can_modify": false, "commits": 12, "additions": 1378, "deletions": 831, "changed_files": 14}