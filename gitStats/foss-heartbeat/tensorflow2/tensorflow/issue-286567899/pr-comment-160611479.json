{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/160611479", "pull_request_review_id": 87757138, "id": 160611479, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MDYxMTQ3OQ==", "diff_hunk": "@@ -716,114 +662,45 @@ void RdmaChannel::Recv() {\n   CHECK(!ibv_post_recv(qp_, &wr, &bad_wr)) << \"Failed to post recv\";\n }\n \n-// Lookup 32-bit buffer index from buffer name\n-// Args:\n-//   buffer_name: name of the buffer\n-// Returns:\n-//   32-bit index\n-uint32_t RdmaChannel::LookupBufferIndex(const string& buffer_name) {\n-  mutex_lock lock{bt_mu_};\n-  BufferNameIndexTable::iterator iter =\n-      buffer_name_index_table_.find(buffer_name);\n-  CHECK(iter != buffer_name_index_table_.end());\n-  return iter->second;\n-}\n-\n-// Find a buffer by its 32-bit index\n-// Args:\n-//   index: 32-bit hash code of the tensor buffer name\n-// Returns:\n-//   name of the tensor buffer\n-RdmaBuffer* RdmaChannel::FindBuffer(const uint32_t index) {\n-  mutex_lock lock{bt_mu_};\n-  BufferTable::iterator iter = buffer_table_.find(index);\n-  CHECK(iter != buffer_table_.end());\n-  return iter->second;\n-}\n-\n-// Find a buffer by its name\n-// Args:\n-//   name: name of the buffer\n-// Returns:\n-//   the named rdma buffer\n-RdmaBuffer* RdmaChannel::FindBuffer(const string& name) {\n-  uint32_t index = LookupBufferIndex(name);\n-  return FindBuffer(index);\n-}\n-\n-// Find a buffer if it exists, otherwise create one.\n-// The memory inside the created buffer is not allocated.\n-// Args:\n-//   name: the name of the buffer\n-//   buffer_type: TENSOR, MESSAGE or ACK.\n-// Returns:\n-//   the named buffer\n-RdmaBuffer* RdmaChannel::FindOrCreateBuffer(const string& name,\n-                                            BufferType buffer_type) {\n-  mutex_lock lock{bt_mu_};\n-  RdmaBuffer* rb;\n-  // find index\n-  BufferNameIndexTable::iterator iter = buffer_name_index_table_.find(name);\n-  if (iter != buffer_name_index_table_.end()) {\n-    uint32_t index = iter->second;\n-    // find buffer\n-    BufferTable::iterator iter = buffer_table_.find(index);\n-    CHECK(iter != buffer_table_.end());\n-    rb = iter->second;\n-  } else {\n-    uint32_t index = NameHash(name);\n-    if (buffer_type == TENSOR) {\n-      rb = new RdmaTensorBuffer(this, name);\n-    } else if (buffer_type == MESSAGE) {\n-      rb = new RdmaMessageBuffer(this, name);\n-    } else if (buffer_type == ACK) {\n-      rb = new RdmaAckBuffer(this, name);\n-    }\n-    buffer_name_index_table_.insert({name, index});\n-    buffer_index_name_table_.insert({index, name});\n-    buffer_table_.insert({index, rb});\n-  }\n-  CHECK(rb);\n-  return rb;\n-}\n-\n // Insert callback to the callback_table.\n // The callback is activated when the corresponding tensor is received.\n // Arg:\n //   key: the name of the tensor\n //   recv_done: the callback associated with the tensor.\n // Returns:\n //   None\n-void RdmaChannel::InsertRecvCallback(const string& key,\n-                                     std::function<void()> recv_done) {\n+RdmaTensorRequest* RdmaChannel::InsertTensorRequest(\n+    const string& key, int64 step_id, Device* dst_dev,\n+    const Rendezvous::Args recv_args,\n+    const RdmaTensorRequest::RecvDoneCallback& done) {\n   mutex_lock lock{ct_mu_};\n-  callback_table_.insert({key, recv_done});\n+  uint32_t request_index = request_serial_++ & 0x7FFFFFFF;", "path": "tensorflow/contrib/verbs/rdma.cc", "position": 345, "original_position": 345, "commit_id": "0a7ada105770febc75794b4003b7aa6584e2e753", "original_commit_id": "0a7ada105770febc75794b4003b7aa6584e2e753", "user": {"login": "eladweiss", "id": 31474666, "node_id": "MDQ6VXNlcjMxNDc0NjY2", "avatar_url": "https://avatars2.githubusercontent.com/u/31474666?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eladweiss", "html_url": "https://github.com/eladweiss", "followers_url": "https://api.github.com/users/eladweiss/followers", "following_url": "https://api.github.com/users/eladweiss/following{/other_user}", "gists_url": "https://api.github.com/users/eladweiss/gists{/gist_id}", "starred_url": "https://api.github.com/users/eladweiss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eladweiss/subscriptions", "organizations_url": "https://api.github.com/users/eladweiss/orgs", "repos_url": "https://api.github.com/users/eladweiss/repos", "events_url": "https://api.github.com/users/eladweiss/events{/privacy}", "received_events_url": "https://api.github.com/users/eladweiss/received_events", "type": "User", "site_admin": false}, "body": "@shamoya You are right. I can change the roll to be at 4G - K (k is the number of special IMMs). It will be slightly less efficient in computation, but far from being a bottleneck. Note that if the requests roll at X, it allows us up to X **concurrent** requests per channel. Not total requests.\r\n  ", "created_at": "2018-01-10T08:10:31Z", "updated_at": "2018-01-10T08:11:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/15927#discussion_r160611479", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/160611479"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/15927#discussion_r160611479"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15927"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22274255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shamoya\">@shamoya</a> You are right. I can change the roll to be at 4G - K (k is the number of special IMMs). It will be slightly less efficient in computation, but far from being a bottleneck. Note that if the requests roll at X, it allows us up to X <strong>concurrent</strong> requests per channel. Not total requests.</p>", "body_text": "@shamoya You are right. I can change the roll to be at 4G - K (k is the number of special IMMs). It will be slightly less efficient in computation, but far from being a bottleneck. Note that if the requests roll at X, it allows us up to X concurrent requests per channel. Not total requests.", "in_reply_to_id": 160438550}