{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8053", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8053/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8053/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8053/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8053", "id": 211682292, "node_id": "MDU6SXNzdWUyMTE2ODIyOTI=", "number": 8053, "title": "per_process_gpu_memory_fraction causes memory error", "user": {"login": "DavidNemeskey", "id": 690386, "node_id": "MDQ6VXNlcjY5MDM4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/690386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNemeskey", "html_url": "https://github.com/DavidNemeskey", "followers_url": "https://api.github.com/users/DavidNemeskey/followers", "following_url": "https://api.github.com/users/DavidNemeskey/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNemeskey/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNemeskey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNemeskey/subscriptions", "organizations_url": "https://api.github.com/users/DavidNemeskey/orgs", "repos_url": "https://api.github.com/users/DavidNemeskey/repos", "events_url": "https://api.github.com/users/DavidNemeskey/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNemeskey/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-03T13:02:43Z", "updated_at": "2017-06-16T20:43:01Z", "closed_at": "2017-06-16T20:40:46Z", "author_association": "NONE", "body_html": "<p>I wrote an LSTM model in tensorflow and use a <code>Saver</code> to create a checkpoint after each epoch. So far so good. However, when I add the option <code>per_process_gpu_memory_fraction=0.9</code> to the session, it runs out of memory when I save the model right after the first epoch. I have a GeForce Titan X with 12GB of memory; tf uses a bit more than half of it (see below).</p>\n<p>Interestingly, if I do not save the model, it does not crash (at least not after the first epoch). Also,</p>\n<table>\n<thead>\n<tr>\n<th>per_process_gpu_memory_fraction</th>\n<th>memory used</th>\n<th>crashes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.9</td>\n<td>7377MiB</td>\n<td>yes</td>\n</tr>\n<tr>\n<td>0.8</td>\n<td>7288MiB</td>\n<td>yes</td>\n</tr>\n<tr>\n<td>0.5</td>\n<td>6272MiB</td>\n<td>no</td>\n</tr>\n<tr>\n<td>not set</td>\n<td>6955MiB</td>\n<td>no</td>\n</tr>\n</tbody>\n</table>\n<p>I forgot with what parameters, but I also saw the program survive the first epoch, only to crash after the fifth.</p>\n<p>I have modified the <a href=\"https://github.com/DavidNemeskey/models/blob/saver_memory_error/tutorials/rnn/ptb/ptb_word_lm.py\">ptf_word_lm.py</a> in tensorflow/models, so that it can be used to reproduce the error. It does not crash with <code>per_process_gpu_memory_fraction=0.8</code>, as my model is a bit different, but it does at <code>0.9</code>. Please call the script with <code>--model medium</code>.</p>\n<p>On the surface, I only see a <code>Dst tensor is not initialized</code> error, but as described in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"202683778\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7025\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7025/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7025\">#7025</a>, it is a sign of a memory error. Also, in tf 0.11, it returned with a proper out of memory error, but I have updated to 1.0 since, as I hoped this bug had been fixed there. Apparently not.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I couldn't find anything similar.</p>\n<h3>Environment info</h3>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/816786/environment.txt\">environment.txt</a></p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>See <code>ptb_word_lm.py</code> in <a href=\"https://github.com/DavidNemeskey/models/tree/saver_memory_error\">this branch</a></p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>--</p>\n<h3>Logs or other output that would be helpful</h3>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/816796/log_err.txt\">log_err.txt</a></p>", "body_text": "I wrote an LSTM model in tensorflow and use a Saver to create a checkpoint after each epoch. So far so good. However, when I add the option per_process_gpu_memory_fraction=0.9 to the session, it runs out of memory when I save the model right after the first epoch. I have a GeForce Titan X with 12GB of memory; tf uses a bit more than half of it (see below).\nInterestingly, if I do not save the model, it does not crash (at least not after the first epoch). Also,\n\n\n\nper_process_gpu_memory_fraction\nmemory used\ncrashes\n\n\n\n\n0.9\n7377MiB\nyes\n\n\n0.8\n7288MiB\nyes\n\n\n0.5\n6272MiB\nno\n\n\nnot set\n6955MiB\nno\n\n\n\nI forgot with what parameters, but I also saw the program survive the first epoch, only to crash after the fifth.\nI have modified the ptf_word_lm.py in tensorflow/models, so that it can be used to reproduce the error. It does not crash with per_process_gpu_memory_fraction=0.8, as my model is a bit different, but it does at 0.9. Please call the script with --model medium.\nOn the surface, I only see a Dst tensor is not initialized error, but as described in #7025, it is a sign of a memory error. Also, in tf 0.11, it returned with a proper out of memory error, but I have updated to 1.0 since, as I hoped this bug had been fixed there. Apparently not.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI couldn't find anything similar.\nEnvironment info\nenvironment.txt\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nSee ptb_word_lm.py in this branch\nWhat other attempted solutions have you tried?\n--\nLogs or other output that would be helpful\nlog_err.txt", "body": "I wrote an LSTM model in tensorflow and use a `Saver` to create a checkpoint after each epoch. So far so good. However, when I add the option `per_process_gpu_memory_fraction=0.9` to the session, it runs out of memory when I save the model right after the first epoch. I have a GeForce Titan X with 12GB of memory; tf uses a bit more than half of it (see below).\r\n\r\nInterestingly, if I do not save the model, it does not crash (at least not after the first epoch). Also,\r\n\r\n| per_process_gpu_memory_fraction | memory used | crashes |\r\n| ------------------------------------------- | ----------------- | --------- |\r\n| 0.9 | 7377MiB | yes |\r\n| 0.8 | 7288MiB | yes |\r\n| 0.5 | 6272MiB | no |\r\n| not set | 6955MiB | no |\r\n\r\nI forgot with what parameters, but I also saw the program survive the first epoch, only to crash after the fifth.\r\n\r\nI have modified the [ptf_word_lm.py](https://github.com/DavidNemeskey/models/blob/saver_memory_error/tutorials/rnn/ptb/ptb_word_lm.py) in tensorflow/models, so that it can be used to reproduce the error. It does not crash with `per_process_gpu_memory_fraction=0.8`, as my model is a bit different, but it does at `0.9`. Please call the script with `--model medium`.\r\n\r\nOn the surface, I only see a `Dst tensor is not initialized` error, but as described in #7025, it is a sign of a memory error. Also, in tf 0.11, it returned with a proper out of memory error, but I have updated to 1.0 since, as I hoped this bug had been fixed there. Apparently not.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI couldn't find anything similar.\r\n\r\n### Environment info\r\n[environment.txt](https://github.com/tensorflow/tensorflow/files/816786/environment.txt)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nSee `ptb_word_lm.py` in [this branch](https://github.com/DavidNemeskey/models/tree/saver_memory_error)\r\n\r\n### What other attempted solutions have you tried?\r\n--\r\n\r\n### Logs or other output that would be helpful\r\n[log_err.txt](https://github.com/tensorflow/tensorflow/files/816796/log_err.txt)\r\n"}