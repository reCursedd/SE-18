{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/230285605", "html_url": "https://github.com/tensorflow/tensorflow/issues/3180#issuecomment-230285605", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3180", "id": 230285605, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMDI4NTYwNQ==", "user": {"login": "TimZaman", "id": 7721540, "node_id": "MDQ6VXNlcjc3MjE1NDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7721540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TimZaman", "html_url": "https://github.com/TimZaman", "followers_url": "https://api.github.com/users/TimZaman/followers", "following_url": "https://api.github.com/users/TimZaman/following{/other_user}", "gists_url": "https://api.github.com/users/TimZaman/gists{/gist_id}", "starred_url": "https://api.github.com/users/TimZaman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TimZaman/subscriptions", "organizations_url": "https://api.github.com/users/TimZaman/orgs", "repos_url": "https://api.github.com/users/TimZaman/repos", "events_url": "https://api.github.com/users/TimZaman/events{/privacy}", "received_events_url": "https://api.github.com/users/TimZaman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-04T13:02:51Z", "updated_at": "2016-07-04T13:02:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Reversing the order 'optimizing' auto<strong>magically</strong> is a bad idea imo, especially (relatively to other DNN frameworks) tensorflow being relatively low level. Moreover, computationally, it might be inconclusive because the ReLU is a relatively cheap operation compared to the heavy convolutions, it would be a very minor optimization in terms of computational cost.<br>\nIn any case, I think you're right in thinking that MaxPoolin(ReLU(x)) = ReLU(MaxPoolin(x)). I think this isn't the standard because one super-layer can be seen as {conv, ReLu} while the MaxPooling is an intermediate step to prepare for the next super-layer.</p>\n<p>Tensorboard is a no-brainer to set-up initially, just run the <code>tensorboard --logdir=/foo/bar</code> and make sure you log your summary writer in <code>/foo/bar</code>, see the docs for this. You will be amaze: <a href=\"https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html</a></p>", "body_text": "Reversing the order 'optimizing' automagically is a bad idea imo, especially (relatively to other DNN frameworks) tensorflow being relatively low level. Moreover, computationally, it might be inconclusive because the ReLU is a relatively cheap operation compared to the heavy convolutions, it would be a very minor optimization in terms of computational cost.\nIn any case, I think you're right in thinking that MaxPoolin(ReLU(x)) = ReLU(MaxPoolin(x)). I think this isn't the standard because one super-layer can be seen as {conv, ReLu} while the MaxPooling is an intermediate step to prepare for the next super-layer.\nTensorboard is a no-brainer to set-up initially, just run the tensorboard --logdir=/foo/bar and make sure you log your summary writer in /foo/bar, see the docs for this. You will be amaze: https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html", "body": "Reversing the order 'optimizing' auto**magically** is a bad idea imo, especially (relatively to other DNN frameworks) tensorflow being relatively low level. Moreover, computationally, it might be inconclusive because the ReLU is a relatively cheap operation compared to the heavy convolutions, it would be a very minor optimization in terms of computational cost.\nIn any case, I think you're right in thinking that MaxPoolin(ReLU(x)) = ReLU(MaxPoolin(x)). I think this isn't the standard because one super-layer can be seen as {conv, ReLu} while the MaxPooling is an intermediate step to prepare for the next super-layer.\n\nTensorboard is a no-brainer to set-up initially, just run the `tensorboard --logdir=/foo/bar` and make sure you log your summary writer in `/foo/bar`, see the docs for this. You will be amaze: https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html\n"}