{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/230395109", "html_url": "https://github.com/tensorflow/tensorflow/issues/3180#issuecomment-230395109", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3180", "id": 230395109, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMDM5NTEwOQ==", "user": {"login": "Hvass-Labs", "id": 13588114, "node_id": "MDQ6VXNlcjEzNTg4MTE0", "avatar_url": "https://avatars2.githubusercontent.com/u/13588114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hvass-Labs", "html_url": "https://github.com/Hvass-Labs", "followers_url": "https://api.github.com/users/Hvass-Labs/followers", "following_url": "https://api.github.com/users/Hvass-Labs/following{/other_user}", "gists_url": "https://api.github.com/users/Hvass-Labs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hvass-Labs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hvass-Labs/subscriptions", "organizations_url": "https://api.github.com/users/Hvass-Labs/orgs", "repos_url": "https://api.github.com/users/Hvass-Labs/repos", "events_url": "https://api.github.com/users/Hvass-Labs/events{/privacy}", "received_events_url": "https://api.github.com/users/Hvass-Labs/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-05T06:13:31Z", "updated_at": "2016-07-05T06:13:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Such code optimizations are common in e.g. C / C++ compilers. It has probably been 15 years since I've implemented a compiler, but as I recall, these kinds of optimizations are called Peephole Optimizations. TensorFlow is a kind of compiler which uses a computational graph, so it would make sense for it to do automatic code optimizations like this.</p>\n<p>Regarding the potential time savings, I think more investigation is needed before we rule it out. Let's do a quick back-of-the-envelope calculation to begin with. This may seem a little confusing and I hope I got the numbers right as I'm still new to these things.</p>\n<p>The convolution performs approximately O(input_width * input_height * input_channels * filter_width * filter_height * num_filters) operations for each input image. This results in a tensor having approximately input_width * input_height * num_filters elements, depending on the padding settings. Dividing the two numbers shows that there's O(filter_width * filter_height * input_channels) arithmetic operations being performed to calculate one element of the output tensor. If we assume the ReLU operations take the same time to execute as each of those convolutional operations, then we should expect to save approximately 0.75 * (1 / (filter_width * filter_height * input_channels)) of the overall computational costs, because the cost of one single ReLU operation is approximately (1 / (filter_width * filter_height * input_channels)) of the total computational cost of the convolutional layer, and we can save 75% of those operations simply by switching the order of the ReLU operation and the 2x2 Max-Pooling.</p>\n<p>For example, having filter_width == filter_height == 5 and input_channels == 1 we would get 0.75 * 0.04 = 0.03, that is, approximately 3% of the overall computational cost of the convolutional layer would be saved from this simple reversal of ReLU and Max-Pooling. That's actually a quite nice saving for such a simple code optimization! However, if input_channels == 64 then the saving is only 0.75 * (1 / 1600) which is about 0.0005 or about 0.05% which is clearly insignificant.</p>\n<p>But if more expensive functions are used instead of ReLU, e.g. something with floating-point division which is computationally expensive, then perhaps it would still make sense to do this optimization even if the number of input channels is high.</p>\n<p>Another thing to consider is the number of layers in the network, because the number of input channels to a convolutional layer is low in the first layer (e.g. 1-channel for gray-scale images and 3-channels for RGB colours), and the number of channels becomes higher in later layers because of the higher number of filter-channels. So the first layer might actually provide a small but tangible saving to the overall computational cost of the network, simply by switching the order of ReLU and Max-Pooling, while the deeper layers may only provide a tiny and insignificant computational saving. But if this optimization was done transparently by the TensorFlow compiler, then any potential time-saving would be gratis to the user of TensorFlow.</p>\n<p>Nevertheless, I find it curious that people in Deep Learning continue to have the ReLU -&gt; Max-Pooling ordering and apparently not realizing that it wastes operations. It suggests that people in the field might have a slightly rigid way of thinking about these things, and perhaps there are more substantial improvements waiting to be discovered.</p>", "body_text": "Such code optimizations are common in e.g. C / C++ compilers. It has probably been 15 years since I've implemented a compiler, but as I recall, these kinds of optimizations are called Peephole Optimizations. TensorFlow is a kind of compiler which uses a computational graph, so it would make sense for it to do automatic code optimizations like this.\nRegarding the potential time savings, I think more investigation is needed before we rule it out. Let's do a quick back-of-the-envelope calculation to begin with. This may seem a little confusing and I hope I got the numbers right as I'm still new to these things.\nThe convolution performs approximately O(input_width * input_height * input_channels * filter_width * filter_height * num_filters) operations for each input image. This results in a tensor having approximately input_width * input_height * num_filters elements, depending on the padding settings. Dividing the two numbers shows that there's O(filter_width * filter_height * input_channels) arithmetic operations being performed to calculate one element of the output tensor. If we assume the ReLU operations take the same time to execute as each of those convolutional operations, then we should expect to save approximately 0.75 * (1 / (filter_width * filter_height * input_channels)) of the overall computational costs, because the cost of one single ReLU operation is approximately (1 / (filter_width * filter_height * input_channels)) of the total computational cost of the convolutional layer, and we can save 75% of those operations simply by switching the order of the ReLU operation and the 2x2 Max-Pooling.\nFor example, having filter_width == filter_height == 5 and input_channels == 1 we would get 0.75 * 0.04 = 0.03, that is, approximately 3% of the overall computational cost of the convolutional layer would be saved from this simple reversal of ReLU and Max-Pooling. That's actually a quite nice saving for such a simple code optimization! However, if input_channels == 64 then the saving is only 0.75 * (1 / 1600) which is about 0.0005 or about 0.05% which is clearly insignificant.\nBut if more expensive functions are used instead of ReLU, e.g. something with floating-point division which is computationally expensive, then perhaps it would still make sense to do this optimization even if the number of input channels is high.\nAnother thing to consider is the number of layers in the network, because the number of input channels to a convolutional layer is low in the first layer (e.g. 1-channel for gray-scale images and 3-channels for RGB colours), and the number of channels becomes higher in later layers because of the higher number of filter-channels. So the first layer might actually provide a small but tangible saving to the overall computational cost of the network, simply by switching the order of ReLU and Max-Pooling, while the deeper layers may only provide a tiny and insignificant computational saving. But if this optimization was done transparently by the TensorFlow compiler, then any potential time-saving would be gratis to the user of TensorFlow.\nNevertheless, I find it curious that people in Deep Learning continue to have the ReLU -> Max-Pooling ordering and apparently not realizing that it wastes operations. It suggests that people in the field might have a slightly rigid way of thinking about these things, and perhaps there are more substantial improvements waiting to be discovered.", "body": "Such code optimizations are common in e.g. C / C++ compilers. It has probably been 15 years since I've implemented a compiler, but as I recall, these kinds of optimizations are called Peephole Optimizations. TensorFlow is a kind of compiler which uses a computational graph, so it would make sense for it to do automatic code optimizations like this.\n\nRegarding the potential time savings, I think more investigation is needed before we rule it out. Let's do a quick back-of-the-envelope calculation to begin with. This may seem a little confusing and I hope I got the numbers right as I'm still new to these things.\n\nThe convolution performs approximately O(input_width \\* input_height \\* input_channels \\* filter_width \\* filter_height \\* num_filters) operations for each input image. This results in a tensor having approximately input_width \\* input_height \\* num_filters elements, depending on the padding settings. Dividing the two numbers shows that there's O(filter_width \\* filter_height \\* input_channels) arithmetic operations being performed to calculate one element of the output tensor. If we assume the ReLU operations take the same time to execute as each of those convolutional operations, then we should expect to save approximately 0.75 \\* (1 / (filter_width \\* filter_height \\* input_channels)) of the overall computational costs, because the cost of one single ReLU operation is approximately (1 / (filter_width \\* filter_height \\* input_channels)) of the total computational cost of the convolutional layer, and we can save 75% of those operations simply by switching the order of the ReLU operation and the 2x2 Max-Pooling.\n\nFor example, having filter_width == filter_height == 5 and input_channels == 1 we would get 0.75 \\* 0.04 = 0.03, that is, approximately 3% of the overall computational cost of the convolutional layer would be saved from this simple reversal of ReLU and Max-Pooling. That's actually a quite nice saving for such a simple code optimization! However, if input_channels == 64 then the saving is only 0.75 \\* (1 / 1600) which is about 0.0005 or about 0.05% which is clearly insignificant.\n\nBut if more expensive functions are used instead of ReLU, e.g. something with floating-point division which is computationally expensive, then perhaps it would still make sense to do this optimization even if the number of input channels is high.\n\nAnother thing to consider is the number of layers in the network, because the number of input channels to a convolutional layer is low in the first layer (e.g. 1-channel for gray-scale images and 3-channels for RGB colours), and the number of channels becomes higher in later layers because of the higher number of filter-channels. So the first layer might actually provide a small but tangible saving to the overall computational cost of the network, simply by switching the order of ReLU and Max-Pooling, while the deeper layers may only provide a tiny and insignificant computational saving. But if this optimization was done transparently by the TensorFlow compiler, then any potential time-saving would be gratis to the user of TensorFlow.\n\nNevertheless, I find it curious that people in Deep Learning continue to have the ReLU -> Max-Pooling ordering and apparently not realizing that it wastes operations. It suggests that people in the field might have a slightly rigid way of thinking about these things, and perhaps there are more substantial improvements waiting to be discovered.\n"}