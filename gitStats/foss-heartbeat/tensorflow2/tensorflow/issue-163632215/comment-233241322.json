{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233241322", "html_url": "https://github.com/tensorflow/tensorflow/issues/3180#issuecomment-233241322", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3180", "id": 233241322, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzI0MTMyMg==", "user": {"login": "Hvass-Labs", "id": 13588114, "node_id": "MDQ6VXNlcjEzNTg4MTE0", "avatar_url": "https://avatars2.githubusercontent.com/u/13588114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hvass-Labs", "html_url": "https://github.com/Hvass-Labs", "followers_url": "https://api.github.com/users/Hvass-Labs/followers", "following_url": "https://api.github.com/users/Hvass-Labs/following{/other_user}", "gists_url": "https://api.github.com/users/Hvass-Labs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hvass-Labs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hvass-Labs/subscriptions", "organizations_url": "https://api.github.com/users/Hvass-Labs/orgs", "repos_url": "https://api.github.com/users/Hvass-Labs/repos", "events_url": "https://api.github.com/users/Hvass-Labs/events{/privacy}", "received_events_url": "https://api.github.com/users/Hvass-Labs/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-18T06:20:15Z", "updated_at": "2016-07-18T06:20:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a> Thanks for commenting on the dev-status on Peephole Optimizations, I was curious if it was already included or not in TensorFlow.</p>\n<p>I've now tried PrettyTensor which automatically adds the activation function to the layer, e.g. like this for adding ReLU to each layer:</p>\n<pre><code>with pt.defaults_scope(activation_fn=tf.nn.relu):\n    y_pred, loss = x_pretty.\\\n        conv2d(kernel=5, depth=16, name='layer_conv1').\\\n        max_pool(kernel=2, stride=2).\\\n        conv2d(kernel=5, depth=36, name='layer_conv2').\\\n        max_pool(kernel=2, stride=2).\\\n        flatten().\\\n        fully_connected(size=128, name='layer_fc1').\\\n        softmax_classifier(class_count=10, labels=y_true)\n</code></pre>\n<p>I think it would make most sense to do Peephole Optimizations inside the TensorFlow compiler, rather than have people doing them manually in e.g. PrettyTensor, TFLearn, Keras, user-code, etc. As discussed above, there might not be a huge gain by reversing the activation function and max-pooling, but perhaps you will start to discover more possible optimizations once you see the code from this angle. I would be curious to hear of any discoveries - perhaps you can post in this thread if you find something significant?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7721540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/TimZaman\">@TimZaman</a> Regarding max-pooling vs. stride in the convolutional layer, I did notice this redundancy when I was watching online lectures, reading tutorials and papers, etc. It is not a mathematical equivalency as in the case of execution order for relu/max-pool, so the TensorFlow compiler should not replace one for the other. But it would make sense that max-pooling and stride were quite similar, because nearby pixels in images are usually quite similar. If you have some references for papers that study max-pooling vs. stride then I'd like to see them. Thanks.</p>", "body_text": "@vrv Thanks for commenting on the dev-status on Peephole Optimizations, I was curious if it was already included or not in TensorFlow.\nI've now tried PrettyTensor which automatically adds the activation function to the layer, e.g. like this for adding ReLU to each layer:\nwith pt.defaults_scope(activation_fn=tf.nn.relu):\n    y_pred, loss = x_pretty.\\\n        conv2d(kernel=5, depth=16, name='layer_conv1').\\\n        max_pool(kernel=2, stride=2).\\\n        conv2d(kernel=5, depth=36, name='layer_conv2').\\\n        max_pool(kernel=2, stride=2).\\\n        flatten().\\\n        fully_connected(size=128, name='layer_fc1').\\\n        softmax_classifier(class_count=10, labels=y_true)\n\nI think it would make most sense to do Peephole Optimizations inside the TensorFlow compiler, rather than have people doing them manually in e.g. PrettyTensor, TFLearn, Keras, user-code, etc. As discussed above, there might not be a huge gain by reversing the activation function and max-pooling, but perhaps you will start to discover more possible optimizations once you see the code from this angle. I would be curious to hear of any discoveries - perhaps you can post in this thread if you find something significant?\n@TimZaman Regarding max-pooling vs. stride in the convolutional layer, I did notice this redundancy when I was watching online lectures, reading tutorials and papers, etc. It is not a mathematical equivalency as in the case of execution order for relu/max-pool, so the TensorFlow compiler should not replace one for the other. But it would make sense that max-pooling and stride were quite similar, because nearby pixels in images are usually quite similar. If you have some references for papers that study max-pooling vs. stride then I'd like to see them. Thanks.", "body": "@vrv Thanks for commenting on the dev-status on Peephole Optimizations, I was curious if it was already included or not in TensorFlow.\n\nI've now tried PrettyTensor which automatically adds the activation function to the layer, e.g. like this for adding ReLU to each layer:\n\n```\nwith pt.defaults_scope(activation_fn=tf.nn.relu):\n    y_pred, loss = x_pretty.\\\n        conv2d(kernel=5, depth=16, name='layer_conv1').\\\n        max_pool(kernel=2, stride=2).\\\n        conv2d(kernel=5, depth=36, name='layer_conv2').\\\n        max_pool(kernel=2, stride=2).\\\n        flatten().\\\n        fully_connected(size=128, name='layer_fc1').\\\n        softmax_classifier(class_count=10, labels=y_true)\n```\n\nI think it would make most sense to do Peephole Optimizations inside the TensorFlow compiler, rather than have people doing them manually in e.g. PrettyTensor, TFLearn, Keras, user-code, etc. As discussed above, there might not be a huge gain by reversing the activation function and max-pooling, but perhaps you will start to discover more possible optimizations once you see the code from this angle. I would be curious to hear of any discoveries - perhaps you can post in this thread if you find something significant?\n\n@TimZaman Regarding max-pooling vs. stride in the convolutional layer, I did notice this redundancy when I was watching online lectures, reading tutorials and papers, etc. It is not a mathematical equivalency as in the case of execution order for relu/max-pool, so the TensorFlow compiler should not replace one for the other. But it would make sense that max-pooling and stride were quite similar, because nearby pixels in images are usually quite similar. If you have some references for papers that study max-pooling vs. stride then I'd like to see them. Thanks.\n"}