{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/288389772", "html_url": "https://github.com/tensorflow/tensorflow/issues/3180#issuecomment-288389772", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3180", "id": 288389772, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODM4OTc3Mg==", "user": {"login": "scanyameres", "id": 14943398, "node_id": "MDQ6VXNlcjE0OTQzMzk4", "avatar_url": "https://avatars0.githubusercontent.com/u/14943398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scanyameres", "html_url": "https://github.com/scanyameres", "followers_url": "https://api.github.com/users/scanyameres/followers", "following_url": "https://api.github.com/users/scanyameres/following{/other_user}", "gists_url": "https://api.github.com/users/scanyameres/gists{/gist_id}", "starred_url": "https://api.github.com/users/scanyameres/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scanyameres/subscriptions", "organizations_url": "https://api.github.com/users/scanyameres/orgs", "repos_url": "https://api.github.com/users/scanyameres/repos", "events_url": "https://api.github.com/users/scanyameres/events{/privacy}", "received_events_url": "https://api.github.com/users/scanyameres/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-22T12:56:13Z", "updated_at": "2017-03-22T12:56:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13588114\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Hvass-Labs\">@Hvass-Labs</a> Your own explanation about the insignificant cost performance is very nice and correct, yet of course it is interesting to wonder if it would make sense to reverse the layer order anyways. IMHO I would expect industry or embedded-level networks to be optimized in such a way when possible, as any \"free\" computation saving is beneficial.</p>\n<p>However, I wanted to point out that some networks <strong>do</strong> need the ReLU to be performed right after the convolution. In semantic segmentation, for example, recurrent CNNs use the \"convolution output\" (including the ReLU) to feed again later stages of the architecture where an upsampling is required, and the original activations are necessary in the original size, to recover the exact neuron which fired the activation. If the MaxPool was done before the ReLU, this detailed local information would be lost, and therefore it wouldnt be possible to recover an output as big (or almost) as the original image.</p>\n<p>I know it is not a big deal, and of course this only involves some small subset of CNNs, but hopefully I satisfied a bit of your curiosity! :)</p>", "body_text": "@Hvass-Labs Your own explanation about the insignificant cost performance is very nice and correct, yet of course it is interesting to wonder if it would make sense to reverse the layer order anyways. IMHO I would expect industry or embedded-level networks to be optimized in such a way when possible, as any \"free\" computation saving is beneficial.\nHowever, I wanted to point out that some networks do need the ReLU to be performed right after the convolution. In semantic segmentation, for example, recurrent CNNs use the \"convolution output\" (including the ReLU) to feed again later stages of the architecture where an upsampling is required, and the original activations are necessary in the original size, to recover the exact neuron which fired the activation. If the MaxPool was done before the ReLU, this detailed local information would be lost, and therefore it wouldnt be possible to recover an output as big (or almost) as the original image.\nI know it is not a big deal, and of course this only involves some small subset of CNNs, but hopefully I satisfied a bit of your curiosity! :)", "body": "@Hvass-Labs Your own explanation about the insignificant cost performance is very nice and correct, yet of course it is interesting to wonder if it would make sense to reverse the layer order anyways. IMHO I would expect industry or embedded-level networks to be optimized in such a way when possible, as any \"free\" computation saving is beneficial.\r\n\r\nHowever, I wanted to point out that some networks **do** need the ReLU to be performed right after the convolution. In semantic segmentation, for example, recurrent CNNs use the \"convolution output\" (including the ReLU) to feed again later stages of the architecture where an upsampling is required, and the original activations are necessary in the original size, to recover the exact neuron which fired the activation. If the MaxPool was done before the ReLU, this detailed local information would be lost, and therefore it wouldnt be possible to recover an output as big (or almost) as the original image.\r\n\r\nI know it is not a big deal, and of course this only involves some small subset of CNNs, but hopefully I satisfied a bit of your curiosity! :)"}