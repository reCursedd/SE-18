{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285962895", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-285962895", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 285962895, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTk2Mjg5NQ==", "user": {"login": "langmore", "id": 178152, "node_id": "MDQ6VXNlcjE3ODE1Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/178152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/langmore", "html_url": "https://github.com/langmore", "followers_url": "https://api.github.com/users/langmore/followers", "following_url": "https://api.github.com/users/langmore/following{/other_user}", "gists_url": "https://api.github.com/users/langmore/gists{/gist_id}", "starred_url": "https://api.github.com/users/langmore/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/langmore/subscriptions", "organizations_url": "https://api.github.com/users/langmore/orgs", "repos_url": "https://api.github.com/users/langmore/repos", "events_url": "https://api.github.com/users/langmore/events{/privacy}", "received_events_url": "https://api.github.com/users/langmore/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-12T18:14:04Z", "updated_at": "2017-03-12T18:14:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Question:  Why did you feed <code>stdevs ** 2</code> to <code>log_prob</code> rather than <code>stdevs</code>?</p>\n<p>I see this as a good features request.  We have to figure out if there is a solution that will work across different distributions.  Building upon your previous suggestion, what about <code>log_prob_pre_X</code>, where <code>X</code> is <code>softmax</code>, <code>softplus</code>, <code>exp</code>, etc...  ?</p>\n<p>I don't think this will be resolved immediately.  So for the moment I recommend writing helper functions.  This will actually be better since you can see what works and give us feedback.  If you want, you could add a helper function to the same file that contains the class and submit a pull request.  E.g. add <code>log_prob_pre_softmax(dist, logits)</code> to <code>dirichlet.py</code>, along with some tests.  For the moment, keep it hidden, i.e. don't add it to the <code>__all__</code> list at the top of the file.  If this helper function works well, we can either un-hide it, or add it as a method.</p>\n<p>Also, and I realize every use case is different, let me tell you how I've been able to solve this issue.  So far, I've been able to avoid the collapse to zero (with 32 bit tensors) by using <code>AdamOptimizer</code> with a small step size of around 0.01 and making sure that the prior's log-prob was included in my loss (you're already doing this I assume, but just in case...).  So in your case above I make sure my loss is <code>-1 * log_likelihood - 1 * stdevs_prior</code>.  In theory, the penalty from the prior should prevent collapse to zero, but in practice of course many crazy things can happen, especially when you have hidden layers.</p>", "body_text": "Question:  Why did you feed stdevs ** 2 to log_prob rather than stdevs?\nI see this as a good features request.  We have to figure out if there is a solution that will work across different distributions.  Building upon your previous suggestion, what about log_prob_pre_X, where X is softmax, softplus, exp, etc...  ?\nI don't think this will be resolved immediately.  So for the moment I recommend writing helper functions.  This will actually be better since you can see what works and give us feedback.  If you want, you could add a helper function to the same file that contains the class and submit a pull request.  E.g. add log_prob_pre_softmax(dist, logits) to dirichlet.py, along with some tests.  For the moment, keep it hidden, i.e. don't add it to the __all__ list at the top of the file.  If this helper function works well, we can either un-hide it, or add it as a method.\nAlso, and I realize every use case is different, let me tell you how I've been able to solve this issue.  So far, I've been able to avoid the collapse to zero (with 32 bit tensors) by using AdamOptimizer with a small step size of around 0.01 and making sure that the prior's log-prob was included in my loss (you're already doing this I assume, but just in case...).  So in your case above I make sure my loss is -1 * log_likelihood - 1 * stdevs_prior.  In theory, the penalty from the prior should prevent collapse to zero, but in practice of course many crazy things can happen, especially when you have hidden layers.", "body": "Question:  Why did you feed `stdevs ** 2` to `log_prob` rather than `stdevs`?\r\n\r\nI see this as a good features request.  We have to figure out if there is a solution that will work across different distributions.  Building upon your previous suggestion, what about `log_prob_pre_X`, where `X` is `softmax`, `softplus`, `exp`, etc...  ?\r\n\r\nI don't think this will be resolved immediately.  So for the moment I recommend writing helper functions.  This will actually be better since you can see what works and give us feedback.  If you want, you could add a helper function to the same file that contains the class and submit a pull request.  E.g. add `log_prob_pre_softmax(dist, logits)` to `dirichlet.py`, along with some tests.  For the moment, keep it hidden, i.e. don't add it to the `__all__` list at the top of the file.  If this helper function works well, we can either un-hide it, or add it as a method.\r\n\r\nAlso, and I realize every use case is different, let me tell you how I've been able to solve this issue.  So far, I've been able to avoid the collapse to zero (with 32 bit tensors) by using `AdamOptimizer` with a small step size of around 0.01 and making sure that the prior's log-prob was included in my loss (you're already doing this I assume, but just in case...).  So in your case above I make sure my loss is `-1 * log_likelihood - 1 * stdevs_prior`.  In theory, the penalty from the prior should prevent collapse to zero, but in practice of course many crazy things can happen, especially when you have hidden layers."}