{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284690116", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-284690116", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 284690116, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDY5MDExNg==", "user": {"login": "akuz", "id": 2409854, "node_id": "MDQ6VXNlcjI0MDk4NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2409854?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akuz", "html_url": "https://github.com/akuz", "followers_url": "https://api.github.com/users/akuz/followers", "following_url": "https://api.github.com/users/akuz/following{/other_user}", "gists_url": "https://api.github.com/users/akuz/gists{/gist_id}", "starred_url": "https://api.github.com/users/akuz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akuz/subscriptions", "organizations_url": "https://api.github.com/users/akuz/orgs", "repos_url": "https://api.github.com/users/akuz/repos", "events_url": "https://api.github.com/users/akuz/events{/privacy}", "received_events_url": "https://api.github.com/users/akuz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-07T11:00:06Z", "updated_at": "2017-03-07T11:01:22Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=178152\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/langmore\">@langmore</a> I was looking more into prior distributions defined on R+, such as Gamma, InverseGamma, etc. I think these would benefit from something like log_prob_with_softplus(). I've made a simple model of clustering using expectation maximisation. In it, the latent variables (cluster assignments) and the parameters of the clusters (cluster probs, means, stdevs) are estimated using a single optimizer loop (all together). If I put the stdevs directly as the variables to optimize, this leads to problems, because optimizers don't allow specifying the bounds. So what I ended up doing is:</p>\n<p>stdevs_raw = tf.Variable(...)</p>\n<p>stdevs = tf.nn.softplus(stdevs_raw) - this makes the stdevs always positive</p>\n<p>stdevs_prior = tf.constrib.distributions.InverseGamma(100, 10).log_prob(tf.pow(stdevs, 2))</p>\n<p>However, this still has a problem if one of the stdevs collapses to zero (which is possible if stdevs_raw becomes a large negative number).</p>\n<p>Therefore, it would be useful to have something like InverseGamma.log_prob_with_softplus(x) which would compute the log probability for softplus(x) without actually computing the softplus.</p>\n<p>Maybe this can be done with some variable transformations? I am not sure</p>", "body_text": "@langmore I was looking more into prior distributions defined on R+, such as Gamma, InverseGamma, etc. I think these would benefit from something like log_prob_with_softplus(). I've made a simple model of clustering using expectation maximisation. In it, the latent variables (cluster assignments) and the parameters of the clusters (cluster probs, means, stdevs) are estimated using a single optimizer loop (all together). If I put the stdevs directly as the variables to optimize, this leads to problems, because optimizers don't allow specifying the bounds. So what I ended up doing is:\nstdevs_raw = tf.Variable(...)\nstdevs = tf.nn.softplus(stdevs_raw) - this makes the stdevs always positive\nstdevs_prior = tf.constrib.distributions.InverseGamma(100, 10).log_prob(tf.pow(stdevs, 2))\nHowever, this still has a problem if one of the stdevs collapses to zero (which is possible if stdevs_raw becomes a large negative number).\nTherefore, it would be useful to have something like InverseGamma.log_prob_with_softplus(x) which would compute the log probability for softplus(x) without actually computing the softplus.\nMaybe this can be done with some variable transformations? I am not sure", "body": "@langmore I was looking more into prior distributions defined on R+, such as Gamma, InverseGamma, etc. I think these would benefit from something like log_prob_with_softplus(). I've made a simple model of clustering using expectation maximisation. In it, the latent variables (cluster assignments) and the parameters of the clusters (cluster probs, means, stdevs) are estimated using a single optimizer loop (all together). If I put the stdevs directly as the variables to optimize, this leads to problems, because optimizers don't allow specifying the bounds. So what I ended up doing is:\r\n\r\nstdevs_raw = tf.Variable(...)\r\n\r\nstdevs = tf.nn.softplus(stdevs_raw) - this makes the stdevs always positive\r\n\r\nstdevs_prior = tf.constrib.distributions.InverseGamma(100, 10).log_prob(tf.pow(stdevs, 2))\r\n\r\nHowever, this still has a problem if one of the stdevs collapses to zero (which is possible if stdevs_raw becomes a large negative number).\r\n\r\nTherefore, it would be useful to have something like InverseGamma.log_prob_with_softplus(x) which would compute the log probability for softplus(x) without actually computing the softplus.\r\n\r\nMaybe this can be done with some variable transformations? I am not sure\r\n"}