{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356496974", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-356496974", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 356496974, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjQ5Njk3NA==", "user": {"login": "langmore", "id": 178152, "node_id": "MDQ6VXNlcjE3ODE1Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/178152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/langmore", "html_url": "https://github.com/langmore", "followers_url": "https://api.github.com/users/langmore/followers", "following_url": "https://api.github.com/users/langmore/following{/other_user}", "gists_url": "https://api.github.com/users/langmore/gists{/gist_id}", "starred_url": "https://api.github.com/users/langmore/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/langmore/subscriptions", "organizations_url": "https://api.github.com/users/langmore/orgs", "repos_url": "https://api.github.com/users/langmore/repos", "events_url": "https://api.github.com/users/langmore/events{/privacy}", "received_events_url": "https://api.github.com/users/langmore/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-10T04:31:59Z", "updated_at": "2018-01-10T04:31:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Note that I'm not suggesting we use transformed distribution (since what I wrote above would be no more stable than <code>Dirichlet</code>.  I'm suggesting we pre-bake an equivalent distribution.</p>\n<p>I don't understand the normalization issue.  Perhaps tell me what the args are above to <code>log_prob</code> and <code>log_prob_pre_softmax</code>?</p>", "body_text": "Note that I'm not suggesting we use transformed distribution (since what I wrote above would be no more stable than Dirichlet.  I'm suggesting we pre-bake an equivalent distribution.\nI don't understand the normalization issue.  Perhaps tell me what the args are above to log_prob and log_prob_pre_softmax?", "body": "Note that I'm not suggesting we use transformed distribution (since what I wrote above would be no more stable than `Dirichlet`.  I'm suggesting we pre-bake an equivalent distribution.\r\n\r\nI don't understand the normalization issue.  Perhaps tell me what the args are above to `log_prob` and `log_prob_pre_softmax`?"}