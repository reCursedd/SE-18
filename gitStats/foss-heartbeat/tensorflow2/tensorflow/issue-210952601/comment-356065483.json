{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356065483", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-356065483", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 356065483, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjA2NTQ4Mw==", "user": {"login": "dustinvtran", "id": 2569867, "node_id": "MDQ6VXNlcjI1Njk4Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2569867?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dustinvtran", "html_url": "https://github.com/dustinvtran", "followers_url": "https://api.github.com/users/dustinvtran/followers", "following_url": "https://api.github.com/users/dustinvtran/following{/other_user}", "gists_url": "https://api.github.com/users/dustinvtran/gists{/gist_id}", "starred_url": "https://api.github.com/users/dustinvtran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dustinvtran/subscriptions", "organizations_url": "https://api.github.com/users/dustinvtran/orgs", "repos_url": "https://api.github.com/users/dustinvtran/repos", "events_url": "https://api.github.com/users/dustinvtran/events{/privacy}", "received_events_url": "https://api.github.com/users/dustinvtran/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-08T19:12:17Z", "updated_at": "2018-01-08T19:12:17Z", "author_association": "MEMBER", "body_html": "<p>I like your solution <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=178152\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/langmore\">@langmore</a> to use transformed distribution. Among other things, a canned implementation could be useful for Dirichlet priors on Categorical/Multinomial logits.</p>\n<p>How do you propose handling the fact that the desired function doesn't normalize? If I understand the discussion correctly, <code>dist_uc.log_prob</code> does <em>not</em> return the same as \"<code>log_prob_pre_softmax</code>\"?</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2409854\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/akuz\">@akuz</a>: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> - just saw the new paper that came out \"Deep Probabilistic Programming\" - how do you deal with the problem described in this ticket within the Edward lib?</p>\n</blockquote>\n<p>This is actually a problem in Edward's mixture models. We just work in the <code>probs</code> space and it can be unstable.</p>\n<p>Examples:</p>\n<ul>\n<li><a href=\"https://github.com/blei-lab/edward/blob/93b6e31dce17c2ad3dca46b80a825819f02da5fe/examples/stochastic_block_model.py\"><code>examples/stochastic_block_model.py</code></a></li>\n<li><a href=\"https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_gibbs.py\"><code>examples/mixture_gaussian_gibbs.py</code></a></li>\n</ul>", "body_text": "I like your solution @langmore to use transformed distribution. Among other things, a canned implementation could be useful for Dirichlet priors on Categorical/Multinomial logits.\nHow do you propose handling the fact that the desired function doesn't normalize? If I understand the discussion correctly, dist_uc.log_prob does not return the same as \"log_prob_pre_softmax\"?\n\n@akuz: @ebrevdo - just saw the new paper that came out \"Deep Probabilistic Programming\" - how do you deal with the problem described in this ticket within the Edward lib?\n\nThis is actually a problem in Edward's mixture models. We just work in the probs space and it can be unstable.\nExamples:\n\nexamples/stochastic_block_model.py\nexamples/mixture_gaussian_gibbs.py", "body": "I like your solution @langmore to use transformed distribution. Among other things, a canned implementation could be useful for Dirichlet priors on Categorical/Multinomial logits.  \r\n\r\nHow do you propose handling the fact that the desired function doesn't normalize? If I understand the discussion correctly, `dist_uc.log_prob` does _not_ return the same as \"`log_prob_pre_softmax`\"?\r\n\r\n> @akuz: @ebrevdo - just saw the new paper that came out \"Deep Probabilistic Programming\" - how do you deal with the problem described in this ticket within the Edward lib?\r\n\r\nThis is actually a problem in Edward's mixture models. We just work in the `probs` space and it can be unstable.\r\n\r\nExamples:\r\n+ [`examples/stochastic_block_model.py`](https://github.com/blei-lab/edward/blob/93b6e31dce17c2ad3dca46b80a825819f02da5fe/examples/stochastic_block_model.py)\r\n+ [`examples/mixture_gaussian_gibbs.py`](https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_gibbs.py)"}