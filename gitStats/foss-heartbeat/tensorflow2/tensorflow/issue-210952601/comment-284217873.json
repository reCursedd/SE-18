{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284217873", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-284217873", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 284217873, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDIxNzg3Mw==", "user": {"login": "akuz", "id": 2409854, "node_id": "MDQ6VXNlcjI0MDk4NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2409854?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akuz", "html_url": "https://github.com/akuz", "followers_url": "https://api.github.com/users/akuz/followers", "following_url": "https://api.github.com/users/akuz/following{/other_user}", "gists_url": "https://api.github.com/users/akuz/gists{/gist_id}", "starred_url": "https://api.github.com/users/akuz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akuz/subscriptions", "organizations_url": "https://api.github.com/users/akuz/orgs", "repos_url": "https://api.github.com/users/akuz/repos", "events_url": "https://api.github.com/users/akuz/events{/privacy}", "received_events_url": "https://api.github.com/users/akuz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-05T10:06:34Z", "updated_at": "2017-03-05T10:15:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=178152\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/langmore\">@langmore</a> you're right that the above formula doesn't define a proper density on the k-D space of logits, due to the reasons you mentioned, so it would not make sense to call it log_prob(x)... The transformed distribution trick you suggested looks great, but I just looked through the documentation, and it seems SoftmaxCentered only supports low dimensionality inputs. What I have is a 4D tensor, the last dimension of which would be the logits, so I'm afraid the transformed distribution might not work in this case? Also would that be efficient?</p>\n<p>On the other side, if you ask yourself the following question: \"Does the above formula calculate log probability of softmax(x) under the specified Dirichlet prior?\" - the answer is yes. So, it does not provide the density in the space of logits, but rather in the space of discrete distributions resulting from softmax(x). So, all I'm trying to do is: 1) y = softmax(x), 2) Dirichlet(concentrations).log_prob(y). But because softmax has a tendency to collapse to [0, 0, ..., 1, 0] due to rounding, the Dirichlet log_prob fails to work properly. The above formula would provide a shortcut to compute steps 1) and 2) in one go, without actually computing softmax. Yes, it would not define a valid density on x, and therefore maybe should not be implemented as a Distribution. Maybe it would be better if implemented as some sort of function on tensors? dirichlet_log_prior_on_softmax(concentrations, logits)? Well, maybe a better name would be needed :)</p>\n<p>So, it short, I'm not looking to define any new types of Distributions, but an efficient way to compute steps 1) and 2) above in one go, without actually computing the softmax in between. Similar concept to cross_entropy_with_logits().</p>", "body_text": "@langmore you're right that the above formula doesn't define a proper density on the k-D space of logits, due to the reasons you mentioned, so it would not make sense to call it log_prob(x)... The transformed distribution trick you suggested looks great, but I just looked through the documentation, and it seems SoftmaxCentered only supports low dimensionality inputs. What I have is a 4D tensor, the last dimension of which would be the logits, so I'm afraid the transformed distribution might not work in this case? Also would that be efficient?\nOn the other side, if you ask yourself the following question: \"Does the above formula calculate log probability of softmax(x) under the specified Dirichlet prior?\" - the answer is yes. So, it does not provide the density in the space of logits, but rather in the space of discrete distributions resulting from softmax(x). So, all I'm trying to do is: 1) y = softmax(x), 2) Dirichlet(concentrations).log_prob(y). But because softmax has a tendency to collapse to [0, 0, ..., 1, 0] due to rounding, the Dirichlet log_prob fails to work properly. The above formula would provide a shortcut to compute steps 1) and 2) in one go, without actually computing softmax. Yes, it would not define a valid density on x, and therefore maybe should not be implemented as a Distribution. Maybe it would be better if implemented as some sort of function on tensors? dirichlet_log_prior_on_softmax(concentrations, logits)? Well, maybe a better name would be needed :)\nSo, it short, I'm not looking to define any new types of Distributions, but an efficient way to compute steps 1) and 2) above in one go, without actually computing the softmax in between. Similar concept to cross_entropy_with_logits().", "body": "@langmore you're right that the above formula doesn't define a proper density on the k-D space of logits, due to the reasons you mentioned, so it would not make sense to call it log_prob(x)... The transformed distribution trick you suggested looks great, but I just looked through the documentation, and it seems SoftmaxCentered only supports low dimensionality inputs. What I have is a 4D tensor, the last dimension of which would be the logits, so I'm afraid the transformed distribution might not work in this case? Also would that be efficient?\r\n\r\nOn the other side, if you ask yourself the following question: \"Does the above formula calculate log probability of softmax(x) under the specified Dirichlet prior?\" - the answer is yes. So, it does not provide the density in the space of logits, but rather in the space of discrete distributions resulting from softmax(x). So, all I'm trying to do is: 1) y = softmax(x), 2) Dirichlet(concentrations).log_prob(y). But because softmax has a tendency to collapse to [0, 0, ..., 1, 0] due to rounding, the Dirichlet log_prob fails to work properly. The above formula would provide a shortcut to compute steps 1) and 2) in one go, without actually computing softmax. Yes, it would not define a valid density on x, and therefore maybe should not be implemented as a Distribution. Maybe it would be better if implemented as some sort of function on tensors? dirichlet_log_prior_on_softmax(concentrations, logits)? Well, maybe a better name would be needed :)\r\n\r\nSo, it short, I'm not looking to define any new types of Distributions, but an efficient way to compute steps 1) and 2) above in one go, without actually computing the softmax in between. Similar concept to cross_entropy_with_logits()."}