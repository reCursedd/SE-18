{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284218878", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-284218878", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 284218878, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDIxODg3OA==", "user": {"login": "akuz", "id": 2409854, "node_id": "MDQ6VXNlcjI0MDk4NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2409854?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akuz", "html_url": "https://github.com/akuz", "followers_url": "https://api.github.com/users/akuz/followers", "following_url": "https://api.github.com/users/akuz/following{/other_user}", "gists_url": "https://api.github.com/users/akuz/gists{/gist_id}", "starred_url": "https://api.github.com/users/akuz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akuz/subscriptions", "organizations_url": "https://api.github.com/users/akuz/orgs", "repos_url": "https://api.github.com/users/akuz/repos", "events_url": "https://api.github.com/users/akuz/events{/privacy}", "received_events_url": "https://api.github.com/users/akuz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-05T10:29:09Z", "updated_at": "2017-03-05T10:36:33Z", "author_association": "NONE", "body_html": "<p>In fact, if we ignore the normalisation constant for now, the above formula can be implemented using the convolution (where concentrations would be used as weights for the last dimension) for the first part, and reduce_logsumexp() for the second part (you would also need to compute and multiply it by the negative of the sum of concentrations). But this implementation has two drawbacks 1) you need to configure the convolution in a special way, and it is only implemented for some specific shapes of tensors, whereas here we always need to convolve along the last dimension only, and 2) it produces two intermediate tensors, which are the summed up. Therefore, a specific implementation would be much more efficient.</p>\n<p>Also, the dirichlet_prior_on_softmax() could just sum up the numbers along all dimensions above the last one, because all that is usually needed is the total log probability (user may need to specify the axis for the computation, and whether to reduce all the others axes). In case when everything is reduced to one number, a sum of log probabilities along all remaining dimensions, this function would not produce any intermediate tensors at all, and would just output a scalar.</p>", "body_text": "In fact, if we ignore the normalisation constant for now, the above formula can be implemented using the convolution (where concentrations would be used as weights for the last dimension) for the first part, and reduce_logsumexp() for the second part (you would also need to compute and multiply it by the negative of the sum of concentrations). But this implementation has two drawbacks 1) you need to configure the convolution in a special way, and it is only implemented for some specific shapes of tensors, whereas here we always need to convolve along the last dimension only, and 2) it produces two intermediate tensors, which are the summed up. Therefore, a specific implementation would be much more efficient.\nAlso, the dirichlet_prior_on_softmax() could just sum up the numbers along all dimensions above the last one, because all that is usually needed is the total log probability (user may need to specify the axis for the computation, and whether to reduce all the others axes). In case when everything is reduced to one number, a sum of log probabilities along all remaining dimensions, this function would not produce any intermediate tensors at all, and would just output a scalar.", "body": "In fact, if we ignore the normalisation constant for now, the above formula can be implemented using the convolution (where concentrations would be used as weights for the last dimension) for the first part, and reduce_logsumexp() for the second part (you would also need to compute and multiply it by the negative of the sum of concentrations). But this implementation has two drawbacks 1) you need to configure the convolution in a special way, and it is only implemented for some specific shapes of tensors, whereas here we always need to convolve along the last dimension only, and 2) it produces two intermediate tensors, which are the summed up. Therefore, a specific implementation would be much more efficient. \r\n\r\nAlso, the dirichlet_prior_on_softmax() could just sum up the numbers along all dimensions above the last one, because all that is usually needed is the total log probability (user may need to specify the axis for the computation, and whether to reduce all the others axes). In case when everything is reduced to one number, a sum of log probabilities along all remaining dimensions, this function would not produce any intermediate tensors at all, and would just output a scalar."}