{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284138193", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-284138193", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 284138193, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDEzODE5Mw==", "user": {"login": "akuz", "id": 2409854, "node_id": "MDQ6VXNlcjI0MDk4NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2409854?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akuz", "html_url": "https://github.com/akuz", "followers_url": "https://api.github.com/users/akuz/followers", "following_url": "https://api.github.com/users/akuz/following{/other_user}", "gists_url": "https://api.github.com/users/akuz/gists{/gist_id}", "starred_url": "https://api.github.com/users/akuz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akuz/subscriptions", "organizations_url": "https://api.github.com/users/akuz/orgs", "repos_url": "https://api.github.com/users/akuz/repos", "events_url": "https://api.github.com/users/akuz/events{/privacy}", "received_events_url": "https://api.github.com/users/akuz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-04T08:54:24Z", "updated_at": "2017-03-04T08:54:24Z", "author_association": "NONE", "body_html": "<p>Hi Ian, I guess this could also accept log_concentration in parameter (although I don't have a use case for that). This still doesn't solve the problem I was talking about. The Dirichlet distribution is defined <em>over</em> the categorical distributions. These are the concrete distributions that can be passed as x to log_prob(x). They have to be valid distributions, I.e. sum up to one, and all numbers must be &gt; 0 and &lt; 1. The Dirichlet doesn't allow any of the components to be exactly 0 or 1, and will output NAN in such case. Now imagine that the input to log_prob(x) is such that x = softmax(y). This can easily result in the output like x = [0, 0, 1, ..., 0, 0], for which the Dirichlet log_prob(x) will output NAN. So what I am suggesting is to pass logits directly to log_prob() without applying softmax first. We can do it like Dirichlet.log_prob_with_logits(), which would be similar to cross_entropy_with_logits(). With cross entropy they don't apply softmax before passing to the function for exactly the same reason of potential rounding of the output of softmax to x = [0, 0, 1, ..., 0, 0]. (Aternatively, instead of making a new method, we could add a new parameter to log_prob(x, logits) such that only one of x or logits can be specified. What do you think?</p>", "body_text": "Hi Ian, I guess this could also accept log_concentration in parameter (although I don't have a use case for that). This still doesn't solve the problem I was talking about. The Dirichlet distribution is defined over the categorical distributions. These are the concrete distributions that can be passed as x to log_prob(x). They have to be valid distributions, I.e. sum up to one, and all numbers must be > 0 and < 1. The Dirichlet doesn't allow any of the components to be exactly 0 or 1, and will output NAN in such case. Now imagine that the input to log_prob(x) is such that x = softmax(y). This can easily result in the output like x = [0, 0, 1, ..., 0, 0], for which the Dirichlet log_prob(x) will output NAN. So what I am suggesting is to pass logits directly to log_prob() without applying softmax first. We can do it like Dirichlet.log_prob_with_logits(), which would be similar to cross_entropy_with_logits(). With cross entropy they don't apply softmax before passing to the function for exactly the same reason of potential rounding of the output of softmax to x = [0, 0, 1, ..., 0, 0]. (Aternatively, instead of making a new method, we could add a new parameter to log_prob(x, logits) such that only one of x or logits can be specified. What do you think?", "body": "Hi Ian, I guess this could also accept log_concentration in parameter (although I don't have a use case for that). This still doesn't solve the problem I was talking about. The Dirichlet distribution is defined *over* the categorical distributions. These are the concrete distributions that can be passed as x to log_prob(x). They have to be valid distributions, I.e. sum up to one, and all numbers must be > 0 and < 1. The Dirichlet doesn't allow any of the components to be exactly 0 or 1, and will output NAN in such case. Now imagine that the input to log_prob(x) is such that x = softmax(y). This can easily result in the output like x = [0, 0, 1, ..., 0, 0], for which the Dirichlet log_prob(x) will output NAN. So what I am suggesting is to pass logits directly to log_prob() without applying softmax first. We can do it like Dirichlet.log_prob_with_logits(), which would be similar to cross_entropy_with_logits(). With cross entropy they don't apply softmax before passing to the function for exactly the same reason of potential rounding of the output of softmax to x = [0, 0, 1, ..., 0, 0]. (Aternatively, instead of making a new method, we could add a new parameter to log_prob(x, logits) such that only one of x or logits can be specified. What do you think?"}