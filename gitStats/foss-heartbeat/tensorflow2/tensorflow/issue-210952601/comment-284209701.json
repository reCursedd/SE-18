{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284209701", "html_url": "https://github.com/tensorflow/tensorflow/issues/7956#issuecomment-284209701", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7956", "id": 284209701, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDIwOTcwMQ==", "user": {"login": "langmore", "id": 178152, "node_id": "MDQ6VXNlcjE3ODE1Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/178152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/langmore", "html_url": "https://github.com/langmore", "followers_url": "https://api.github.com/users/langmore/followers", "following_url": "https://api.github.com/users/langmore/following{/other_user}", "gists_url": "https://api.github.com/users/langmore/gists{/gist_id}", "starred_url": "https://api.github.com/users/langmore/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/langmore/subscriptions", "organizations_url": "https://api.github.com/users/langmore/orgs", "repos_url": "https://api.github.com/users/langmore/repos", "events_url": "https://api.github.com/users/langmore/events{/privacy}", "received_events_url": "https://api.github.com/users/langmore/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-05T06:49:16Z", "updated_at": "2017-03-05T06:49:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> , If we simply use the formula above then we do not have the log of a probability density over <code>R^k</code>, so we can't call it <code>log_prob(x)</code>.  To see that the above formula is not a <code>log_prob</code>, note that it doesn't go to negative infinity as <code>|x| \\to \\inf</code>, so the corresponding <code>prob(x) := exp(log_prob(x))</code> will not go to zero.  You can see also that the above formula is constant if all the <code>x_i</code> are equal.  The reason being that whenever all the <code>x_i</code> are equal, <code>Softmax(x) = [1/k,...., 1/k]</code>.  So the entire line <code>{x : x_1 = x_2 = ... = x_k}</code> is mapped by Softmax to a single point.</p>\n<p>I believe <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2409854\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/akuz\">@akuz</a> wants the standard Dirichlet distribution over <code>k-simplex</code> of probabilities, but wants to evaluate it on an intermediate state of data (before the probabilities are probabilities).</p>\n<p>Note <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2409854\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/akuz\">@akuz</a> that since this doesn't give you a valid prior on logits, it may be better to create the distribution over logits that corresponds to a specified Dirichlet distribution over probabilities.  After all, your network produces logits.  This can be done as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre>ds <span class=\"pl-k\">=</span> tf.contrib.distributions\nbijectors <span class=\"pl-k\">=</span> tf.contrib.distributions.bijectors  <span class=\"pl-c\"><span class=\"pl-c\">#</span> you'll have to search for this...depending on version.</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The dirichlet is a distribution over the k-1 dimensional k-simplex</span>\ndirichlet <span class=\"pl-k\">=</span> ds.Dirichlet(<span class=\"pl-v\">concentration</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">3</span>.])\nsoftmax_centered <span class=\"pl-k\">=</span> bijectors.SoftmaxCentered(<span class=\"pl-v\">event_ndims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> dist_over_logits is a distribution over R^{k-1}</span>\ndist_over_logits <span class=\"pl-k\">=</span> ds.TransformedDistribution(\n    dirichlet,\n    <span class=\"pl-v\">bijector</span><span class=\"pl-k\">=</span>bijectors.Invert(softmax_centered))\n\nlogits <span class=\"pl-k\">=</span> [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.]\nprobs <span class=\"pl-k\">=</span> softmax_centered.forward(logits)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> These two are equal</span>\nprobs\n<span class=\"pl-k\">==&gt;</span> [ <span class=\"pl-c1\">0.09003057</span>  <span class=\"pl-c1\">0.66524088</span>  <span class=\"pl-c1\">0.24472845</span>]\ntf.nn.softmax(logits <span class=\"pl-k\">+</span> [<span class=\"pl-c1\">0</span>.])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Note implicit appending of [0]</span>\n<span class=\"pl-k\">==&gt;</span> [ <span class=\"pl-c1\">0.09003057</span>  <span class=\"pl-c1\">0.66524088</span>  <span class=\"pl-c1\">0.24472845</span>]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> These two are not equal</span>\ndirichlet.log_prob(probs)\n<span class=\"pl-k\">==&gt;</span> <span class=\"pl-c1\">0.871526</span>\ndist_over_logits.log_prob(logits)\n<span class=\"pl-k\">==&gt;</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">3.35129</span></pre></div>\n<p>The last two lines above are not equal because the Dirichlet is a distribution over the (k-1 dimensional) k-simplex, which is very different (in particular, it is much smaller) than R^{k-1} in which logits live.  The probability density over R^{k-1} must be much smaller at the point <code>logits</code> that corresponds to <code>probs</code></p>", "body_text": "@ebrevdo , If we simply use the formula above then we do not have the log of a probability density over R^k, so we can't call it log_prob(x).  To see that the above formula is not a log_prob, note that it doesn't go to negative infinity as |x| \\to \\inf, so the corresponding prob(x) := exp(log_prob(x)) will not go to zero.  You can see also that the above formula is constant if all the x_i are equal.  The reason being that whenever all the x_i are equal, Softmax(x) = [1/k,...., 1/k].  So the entire line {x : x_1 = x_2 = ... = x_k} is mapped by Softmax to a single point.\nI believe @akuz wants the standard Dirichlet distribution over k-simplex of probabilities, but wants to evaluate it on an intermediate state of data (before the probabilities are probabilities).\nNote @akuz that since this doesn't give you a valid prior on logits, it may be better to create the distribution over logits that corresponds to a specified Dirichlet distribution over probabilities.  After all, your network produces logits.  This can be done as follows:\nds = tf.contrib.distributions\nbijectors = tf.contrib.distributions.bijectors  # you'll have to search for this...depending on version.\n\n# The dirichlet is a distribution over the k-1 dimensional k-simplex\ndirichlet = ds.Dirichlet(concentration=[1., 2., 3.])\nsoftmax_centered = bijectors.SoftmaxCentered(event_ndims=1)\n\n# dist_over_logits is a distribution over R^{k-1}\ndist_over_logits = ds.TransformedDistribution(\n    dirichlet,\n    bijector=bijectors.Invert(softmax_centered))\n\nlogits = [-1., 1.]\nprobs = softmax_centered.forward(logits)\n\n# These two are equal\nprobs\n==> [ 0.09003057  0.66524088  0.24472845]\ntf.nn.softmax(logits + [0.])  # Note implicit appending of [0]\n==> [ 0.09003057  0.66524088  0.24472845]\n\n# These two are not equal\ndirichlet.log_prob(probs)\n==> 0.871526\ndist_over_logits.log_prob(logits)\n==> -3.35129\nThe last two lines above are not equal because the Dirichlet is a distribution over the (k-1 dimensional) k-simplex, which is very different (in particular, it is much smaller) than R^{k-1} in which logits live.  The probability density over R^{k-1} must be much smaller at the point logits that corresponds to probs", "body": "@ebrevdo , If we simply use the formula above then we do not have the log of a probability density over `R^k`, so we can't call it `log_prob(x)`.  To see that the above formula is not a `log_prob`, note that it doesn't go to negative infinity as `|x| \\to \\inf`, so the corresponding `prob(x) := exp(log_prob(x))` will not go to zero.  You can see also that the above formula is constant if all the `x_i` are equal.  The reason being that whenever all the `x_i` are equal, `Softmax(x) = [1/k,...., 1/k]`.  So the entire line `{x : x_1 = x_2 = ... = x_k}` is mapped by Softmax to a single point.\r\n\r\nI believe @akuz wants the standard Dirichlet distribution over `k-simplex` of probabilities, but wants to evaluate it on an intermediate state of data (before the probabilities are probabilities).\r\n\r\nNote @akuz that since this doesn't give you a valid prior on logits, it may be better to create the distribution over logits that corresponds to a specified Dirichlet distribution over probabilities.  After all, your network produces logits.  This can be done as follows:\r\n\r\n```python\r\nds = tf.contrib.distributions\r\nbijectors = tf.contrib.distributions.bijectors  # you'll have to search for this...depending on version.\r\n\r\n# The dirichlet is a distribution over the k-1 dimensional k-simplex\r\ndirichlet = ds.Dirichlet(concentration=[1., 2., 3.])\r\nsoftmax_centered = bijectors.SoftmaxCentered(event_ndims=1)\r\n\r\n# dist_over_logits is a distribution over R^{k-1}\r\ndist_over_logits = ds.TransformedDistribution(\r\n    dirichlet,\r\n    bijector=bijectors.Invert(softmax_centered))\r\n\r\nlogits = [-1., 1.]\r\nprobs = softmax_centered.forward(logits)\r\n\r\n# These two are equal\r\nprobs\r\n==> [ 0.09003057  0.66524088  0.24472845]\r\ntf.nn.softmax(logits + [0.])  # Note implicit appending of [0]\r\n==> [ 0.09003057  0.66524088  0.24472845]\r\n\r\n# These two are not equal\r\ndirichlet.log_prob(probs)\r\n==> 0.871526\r\ndist_over_logits.log_prob(logits)\r\n==> -3.35129\r\n```\r\n\r\nThe last two lines above are not equal because the Dirichlet is a distribution over the (k-1 dimensional) k-simplex, which is very different (in particular, it is much smaller) than R^{k-1} in which logits live.  The probability density over R^{k-1} must be much smaller at the point `logits` that corresponds to `probs`\r\n"}