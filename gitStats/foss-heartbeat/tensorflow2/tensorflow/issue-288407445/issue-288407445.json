{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16112", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16112/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16112/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16112/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16112", "id": 288407445, "node_id": "MDU6SXNzdWUyODg0MDc0NDU=", "number": 16112, "title": "Define gradient for tf.linspace and make it work with higher-rank tensors, not just scalars.", "user": {"login": "konstunn", "id": 17176546, "node_id": "MDQ6VXNlcjE3MTc2NTQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/17176546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/konstunn", "html_url": "https://github.com/konstunn", "followers_url": "https://api.github.com/users/konstunn/followers", "following_url": "https://api.github.com/users/konstunn/following{/other_user}", "gists_url": "https://api.github.com/users/konstunn/gists{/gist_id}", "starred_url": "https://api.github.com/users/konstunn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/konstunn/subscriptions", "organizations_url": "https://api.github.com/users/konstunn/orgs", "repos_url": "https://api.github.com/users/konstunn/repos", "events_url": "https://api.github.com/users/konstunn/events{/privacy}", "received_events_url": "https://api.github.com/users/konstunn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-01-14T13:10:47Z", "updated_at": "2018-02-07T22:52:41Z", "closed_at": "2018-02-07T22:52:40Z", "author_association": "NONE", "body_html": "<p>I needed to feed-forward through <code>tf.linspace</code>, but it seems that it does not have gradient defined.</p>\n<p>I don't know how to define gradient for existing op, but I've implemented my own version of <code>tf.linspace</code> in python using tensorflow with so that automatically defined gradient.</p>\n<div class=\"highlight highlight-source-python\"><pre>            <span class=\"pl-k\">def</span> <span class=\"pl-en\">linspace</span>(<span class=\"pl-smi\">start</span>, <span class=\"pl-smi\">end</span>, <span class=\"pl-smi\">num</span>):\n                <span class=\"pl-c1\">range</span> <span class=\"pl-k\">=</span> end <span class=\"pl-k\">-</span> start\n                num_steps <span class=\"pl-k\">=</span> num <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n                h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">range</span> <span class=\"pl-k\">/</span> num_steps\n\n                <span class=\"pl-k\">def</span> <span class=\"pl-en\">cond</span>(<span class=\"pl-smi\">ta</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">k</span>):\n                    <span class=\"pl-k\">return</span> tf.less(x, end)\n\n                <span class=\"pl-k\">def</span> <span class=\"pl-en\">body</span>(<span class=\"pl-smi\">ta</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">k</span>):\n                    x <span class=\"pl-k\">=</span> x <span class=\"pl-k\">+</span> h\n                    ta <span class=\"pl-k\">=</span> ta.write(k, x)\n                    <span class=\"pl-k\">return</span> ta, x, k<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>\n\n                k <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>)\n                ta <span class=\"pl-k\">=</span> tf.TensorArray(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>num)\n                ta <span class=\"pl-k\">=</span> ta.write(k, start)\n                ta <span class=\"pl-k\">=</span> tf.while_loop(cond, body, [ta, start, k<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>])[<span class=\"pl-c1\">0</span>]\n                <span class=\"pl-k\">return</span> ta.stack()</pre></div>\n<p>One more feature I can suggest adding is improve to <code>linspace</code> so it would work with higher-rank tensors.<br>\nThe function I wrote is also very short and simple, but is interesting as a generalization of <code>linspace</code>.</p>\n<div class=\"highlight highlight-source-python\"><pre>            <span class=\"pl-k\">def</span> <span class=\"pl-en\">linspace_vectors</span>(<span class=\"pl-smi\">start</span>, <span class=\"pl-smi\">end</span>, <span class=\"pl-smi\">num</span>):\n                cnct <span class=\"pl-k\">=</span> tf.concat([start, end], <span class=\"pl-c1\">1</span>)\n                seq <span class=\"pl-k\">=</span> tf.map_fn(\n                    <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">row_i</span>: linspace(row_i[<span class=\"pl-c1\">0</span>], row_i[<span class=\"pl-c1\">1</span>], num), cnct)\n                splits <span class=\"pl-k\">=</span> tf.split(seq, num, <span class=\"pl-c1\">1</span>)\n                <span class=\"pl-k\">return</span> tf.stack(splits)</pre></div>\n<p>The function is taken from my project and returns 3-rank tensor with shapes [num, r, 1]. Inputs are 2-rank tensors with shapes [r, 1].<br>\nSo that I linspaced vectors-columns, not just scalars as <code>tf.linspace</code> do.<br>\nWhat do you think? Is it worth adding?</p>\n<p>Have I written custom code: Yes<br>\nOS Platform and Distribution: Ubuntu 16.04<br>\nTensorFlow installed from: pip nightly build<br>\nTensorFlow version: 1.4.1<br>\nBazel version: N/A<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: N/A<br>\nExact command to reproduce: N/A</p>", "body_text": "I needed to feed-forward through tf.linspace, but it seems that it does not have gradient defined.\nI don't know how to define gradient for existing op, but I've implemented my own version of tf.linspace in python using tensorflow with so that automatically defined gradient.\n            def linspace(start, end, num):\n                range = end - start\n                num_steps = num - 1\n                h = range / num_steps\n\n                def cond(ta, x, k):\n                    return tf.less(x, end)\n\n                def body(ta, x, k):\n                    x = x + h\n                    ta = ta.write(k, x)\n                    return ta, x, k+1\n\n                k = tf.constant(0)\n                ta = tf.TensorArray(dtype=tf.float32, size=num)\n                ta = ta.write(k, start)\n                ta = tf.while_loop(cond, body, [ta, start, k+1])[0]\n                return ta.stack()\nOne more feature I can suggest adding is improve to linspace so it would work with higher-rank tensors.\nThe function I wrote is also very short and simple, but is interesting as a generalization of linspace.\n            def linspace_vectors(start, end, num):\n                cnct = tf.concat([start, end], 1)\n                seq = tf.map_fn(\n                    lambda row_i: linspace(row_i[0], row_i[1], num), cnct)\n                splits = tf.split(seq, num, 1)\n                return tf.stack(splits)\nThe function is taken from my project and returns 3-rank tensor with shapes [num, r, 1]. Inputs are 2-rank tensors with shapes [r, 1].\nSo that I linspaced vectors-columns, not just scalars as tf.linspace do.\nWhat do you think? Is it worth adding?\nHave I written custom code: Yes\nOS Platform and Distribution: Ubuntu 16.04\nTensorFlow installed from: pip nightly build\nTensorFlow version: 1.4.1\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A", "body": "I needed to feed-forward through `tf.linspace`, but it seems that it does not have gradient defined.\r\n\r\nI don't know how to define gradient for existing op, but I've implemented my own version of `tf.linspace` in python using tensorflow with so that automatically defined gradient.\r\n```python\r\n            def linspace(start, end, num):\r\n                range = end - start\r\n                num_steps = num - 1\r\n                h = range / num_steps\r\n\r\n                def cond(ta, x, k):\r\n                    return tf.less(x, end)\r\n\r\n                def body(ta, x, k):\r\n                    x = x + h\r\n                    ta = ta.write(k, x)\r\n                    return ta, x, k+1\r\n\r\n                k = tf.constant(0)\r\n                ta = tf.TensorArray(dtype=tf.float32, size=num)\r\n                ta = ta.write(k, start)\r\n                ta = tf.while_loop(cond, body, [ta, start, k+1])[0]\r\n                return ta.stack()\r\n```\r\n\r\nOne more feature I can suggest adding is improve to `linspace` so it would work with higher-rank tensors.\r\nThe function I wrote is also very short and simple, but is interesting as a generalization of `linspace`.\r\n```python\r\n            def linspace_vectors(start, end, num):\r\n                cnct = tf.concat([start, end], 1)\r\n                seq = tf.map_fn(\r\n                    lambda row_i: linspace(row_i[0], row_i[1], num), cnct)\r\n                splits = tf.split(seq, num, 1)\r\n                return tf.stack(splits)\r\n```\r\nThe function is taken from my project and returns 3-rank tensor with shapes [num, r, 1]. Inputs are 2-rank tensors with shapes [r, 1].\r\nSo that I linspaced vectors-columns, not just scalars as `tf.linspace` do.\r\nWhat do you think? Is it worth adding?\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: pip nightly build\r\nTensorFlow version: 1.4.1\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A"}