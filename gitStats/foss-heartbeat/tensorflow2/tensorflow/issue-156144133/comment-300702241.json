{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300702241", "html_url": "https://github.com/tensorflow/tensorflow/issues/2462#issuecomment-300702241", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2462", "id": 300702241, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDcwMjI0MQ==", "user": {"login": "petrux", "id": 401919, "node_id": "MDQ6VXNlcjQwMTkxOQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/401919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrux", "html_url": "https://github.com/petrux", "followers_url": "https://api.github.com/users/petrux/followers", "following_url": "https://api.github.com/users/petrux/following{/other_user}", "gists_url": "https://api.github.com/users/petrux/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrux/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrux/subscriptions", "organizations_url": "https://api.github.com/users/petrux/orgs", "repos_url": "https://api.github.com/users/petrux/repos", "events_url": "https://api.github.com/users/petrux/events{/privacy}", "received_events_url": "https://api.github.com/users/petrux/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-11T07:08:23Z", "updated_at": "2017-05-11T08:47:28Z", "author_association": "NONE", "body_html": "<p>I found this implementation from the <a href=\"https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2717\">Keras code</a>. As far as I could get, they just clip the <code>output</code> (i.e. the softmax) between <code>EPSILON</code> and <code>1-EPSILON</code>, perform a <code>log</code> operation on it and us the result as the logits. Could make sense?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sparse_categorical_crossentropy</span>(<span class=\"pl-smi\">output</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">from_logits</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Categorical crossentropy with integer targets.</span>\n<span class=\"pl-s\">    # Arguments</span>\n<span class=\"pl-s\">        output: A tensor resulting from a softmax</span>\n<span class=\"pl-s\">            (unless `from_logits` is True, in which</span>\n<span class=\"pl-s\">            case `output` is expected to be the logits).</span>\n<span class=\"pl-s\">        target: An integer tensor.</span>\n<span class=\"pl-s\">        from_logits: Boolean, whether `output` is the</span>\n<span class=\"pl-s\">            result of a softmax, or is a tensor of logits.</span>\n<span class=\"pl-s\">    # Returns</span>\n<span class=\"pl-s\">        Output tensor.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Note: tf.nn.softmax_cross_entropy_with_logits</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> expects logits, Keras expects probabilities.</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> from_logits:\n        epsilon <span class=\"pl-k\">=</span> _to_tensor(<span class=\"pl-c1\">_EPSILON</span>, output.dtype.base_dtype)\n        output <span class=\"pl-k\">=</span> tf.clip_by_value(output, epsilon, <span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> epsilon)\n        output <span class=\"pl-k\">=</span> tf.log(output)\n\n    output_shape <span class=\"pl-k\">=</span> output.get_shape()\n    targets <span class=\"pl-k\">=</span> cast(flatten(target), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>int64<span class=\"pl-pds\">'</span></span>)\n    logits <span class=\"pl-k\">=</span> tf.reshape(output, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">int</span>(output_shape[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])])\n    res <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(\n        <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>targets,\n        <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(output_shape) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">3</span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> if our output includes timesteps we need to reshape</span>\n        <span class=\"pl-k\">return</span> tf.reshape(res, tf.shape(output)[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> res</pre></div>\n<p><em>EDIT</em>: of course it makes sense, my question should be read as: could this solution make sense from the point of view of the numerical stability and efficiency?</p>", "body_text": "I found this implementation from the Keras code. As far as I could get, they just clip the output (i.e. the softmax) between EPSILON and 1-EPSILON, perform a log operation on it and us the result as the logits. Could make sense?\ndef sparse_categorical_crossentropy(output, target, from_logits=False):\n    \"\"\"Categorical crossentropy with integer targets.\n    # Arguments\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        target: An integer tensor.\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n    # Returns\n        Output tensor.\n    \"\"\"\n    # Note: tf.nn.softmax_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n        output = tf.log(output)\n\n    output_shape = output.get_shape()\n    targets = cast(flatten(target), 'int64')\n    logits = tf.reshape(output, [-1, int(output_shape[-1])])\n    res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=targets,\n        logits=logits)\n    if len(output_shape) == 3:\n        # if our output includes timesteps we need to reshape\n        return tf.reshape(res, tf.shape(output)[:-1])\n    else:\n        return res\nEDIT: of course it makes sense, my question should be read as: could this solution make sense from the point of view of the numerical stability and efficiency?", "body": "I found this implementation from the [Keras code](https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2717). As far as I could get, they just clip the `output` (i.e. the softmax) between `EPSILON` and `1-EPSILON`, perform a `log` operation on it and us the result as the logits. Could make sense?  \r\n  \r\n```python\r\ndef sparse_categorical_crossentropy(output, target, from_logits=False):\r\n    \"\"\"Categorical crossentropy with integer targets.\r\n    # Arguments\r\n        output: A tensor resulting from a softmax\r\n            (unless `from_logits` is True, in which\r\n            case `output` is expected to be the logits).\r\n        target: An integer tensor.\r\n        from_logits: Boolean, whether `output` is the\r\n            result of a softmax, or is a tensor of logits.\r\n    # Returns\r\n        Output tensor.\r\n    \"\"\"\r\n    # Note: tf.nn.softmax_cross_entropy_with_logits\r\n    # expects logits, Keras expects probabilities.\r\n    if not from_logits:\r\n        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\r\n        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\r\n        output = tf.log(output)\r\n\r\n    output_shape = output.get_shape()\r\n    targets = cast(flatten(target), 'int64')\r\n    logits = tf.reshape(output, [-1, int(output_shape[-1])])\r\n    res = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n        labels=targets,\r\n        logits=logits)\r\n    if len(output_shape) == 3:\r\n        # if our output includes timesteps we need to reshape\r\n        return tf.reshape(res, tf.shape(output)[:-1])\r\n    else:\r\n        return res\r\n```\r\n  \r\n*EDIT*: of course it makes sense, my question should be read as: could this solution make sense from the point of view of the numerical stability and efficiency?\r\n"}