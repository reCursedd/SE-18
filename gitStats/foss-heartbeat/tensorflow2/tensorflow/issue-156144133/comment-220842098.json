{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/220842098", "html_url": "https://github.com/tensorflow/tensorflow/issues/2462#issuecomment-220842098", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2462", "id": 220842098, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMDg0MjA5OA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-22T16:37:16Z", "updated_at": "2016-05-22T16:37:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We provide optimized cross-entropy implementations that are fused with the softmax/sigmoid implementations because their performance and numerical stability are critical to efficient training.</p>\n<p>If however you are just interested in the cross entropy itself, you can compute it directly using code from the <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/mnist/beginners/index.html\" rel=\"nofollow\">beginners tutorial</a>:</p>\n<div class=\"highlight highlight-source-python\"><pre>cross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(<span class=\"pl-k\">-</span>tf.reduce_sum(y_ <span class=\"pl-k\">*</span> tf.log(y), <span class=\"pl-v\">reduction_indices</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>]))</pre></div>\n<p><strong>N.B.</strong> DO NOT use this code for training. Use <a href=\"https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#softmax_cross_entropy_with_logits\" rel=\"nofollow\"><code>tf.nn.softmax_cross_entropy_with_logits()</code></a> instead.</p>", "body_text": "We provide optimized cross-entropy implementations that are fused with the softmax/sigmoid implementations because their performance and numerical stability are critical to efficient training.\nIf however you are just interested in the cross entropy itself, you can compute it directly using code from the beginners tutorial:\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\nN.B. DO NOT use this code for training. Use tf.nn.softmax_cross_entropy_with_logits() instead.", "body": "We provide optimized cross-entropy implementations that are fused with the softmax/sigmoid implementations because their performance and numerical stability are critical to efficient training.\n\nIf however you are just interested in the cross entropy itself, you can compute it directly using code from the [beginners tutorial](https://www.tensorflow.org/versions/r0.8/tutorials/mnist/beginners/index.html):\n\n``` python\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n```\n\n**N.B.** DO NOT use this code for training. Use [`tf.nn.softmax_cross_entropy_with_logits()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#softmax_cross_entropy_with_logits) instead.\n"}