{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269404387", "html_url": "https://github.com/tensorflow/tensorflow/issues/6384#issuecomment-269404387", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6384", "id": 269404387, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTQwNDM4Nw==", "user": {"login": "asrivat1", "id": 3476822, "node_id": "MDQ6VXNlcjM0NzY4MjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3476822?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asrivat1", "html_url": "https://github.com/asrivat1", "followers_url": "https://api.github.com/users/asrivat1/followers", "following_url": "https://api.github.com/users/asrivat1/following{/other_user}", "gists_url": "https://api.github.com/users/asrivat1/gists{/gist_id}", "starred_url": "https://api.github.com/users/asrivat1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asrivat1/subscriptions", "organizations_url": "https://api.github.com/users/asrivat1/orgs", "repos_url": "https://api.github.com/users/asrivat1/repos", "events_url": "https://api.github.com/users/asrivat1/events{/privacy}", "received_events_url": "https://api.github.com/users/asrivat1/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-28T00:48:37Z", "updated_at": "2016-12-28T00:48:37Z", "author_association": "NONE", "body_html": "<p>I'm in 2.7. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> are you on 0.12.0-rc1? I found two workarounds. The first is to pad out the shape of the tensors to some fixed size using 0s via <code>tf.pad</code>, and this fixes the <code>tf.einsum</code> bug. The other solution is what @j88k suggested, and it's about 80% faster for my program. Also worth noting that for joining on multiple dimensions, <code>tf.batch_matmul</code> works wonders. The only disadvantage is that according to <code>Timeline</code> I'm now spending as much time transposing and reshaping my tensors as I am actually multiplying them.</p>", "body_text": "I'm in 2.7. @aselle are you on 0.12.0-rc1? I found two workarounds. The first is to pad out the shape of the tensors to some fixed size using 0s via tf.pad, and this fixes the tf.einsum bug. The other solution is what @j88k suggested, and it's about 80% faster for my program. Also worth noting that for joining on multiple dimensions, tf.batch_matmul works wonders. The only disadvantage is that according to Timeline I'm now spending as much time transposing and reshaping my tensors as I am actually multiplying them.", "body": "I'm in 2.7. @aselle are you on 0.12.0-rc1? I found two workarounds. The first is to pad out the shape of the tensors to some fixed size using 0s via `tf.pad`, and this fixes the `tf.einsum` bug. The other solution is what @j88k suggested, and it's about 80% faster for my program. Also worth noting that for joining on multiple dimensions, `tf.batch_matmul` works wonders. The only disadvantage is that according to `Timeline` I'm now spending as much time transposing and reshaping my tensors as I am actually multiplying them."}