{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3037", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3037/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3037/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3037/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3037", "id": 162262899, "node_id": "MDU6SXNzdWUxNjIyNjI4OTk=", "number": 3037, "title": "Stuck at prepare_or_wait_for_session for workers when running on kubernetes", "user": {"login": "perhapszzy", "id": 7953637, "node_id": "MDQ6VXNlcjc5NTM2Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7953637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/perhapszzy", "html_url": "https://github.com/perhapszzy", "followers_url": "https://api.github.com/users/perhapszzy/followers", "following_url": "https://api.github.com/users/perhapszzy/following{/other_user}", "gists_url": "https://api.github.com/users/perhapszzy/gists{/gist_id}", "starred_url": "https://api.github.com/users/perhapszzy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/perhapszzy/subscriptions", "organizations_url": "https://api.github.com/users/perhapszzy/orgs", "repos_url": "https://api.github.com/users/perhapszzy/repos", "events_url": "https://api.github.com/users/perhapszzy/events{/privacy}", "received_events_url": "https://api.github.com/users/perhapszzy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-06-25T04:21:28Z", "updated_at": "2016-06-25T08:30:42Z", "closed_at": "2016-06-25T08:30:42Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Kubernetes/Ubuntu 14.04</p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>start tensorflow server on kubernetes using yaml generated by <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/scripts/k8s_tensorflow.py\">the python code</a></li>\n<li>Start tensorflow code:</li>\n</ol>\n<pre><code>python mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30001 --worker_index=0 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n</code></pre>\n<pre><code>python mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30002 --worker_index=1 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n</code></pre>\n<h3>What have you tried?</h3>\n<ol>\n<li>The worker with index 0 (chief) can execute normally.</li>\n<li>It was able to execute well (using the same yaml and code)</li>\n<li>I tried to restart the servers, but it didn't work.</li>\n<li>All other workers stuck at <code>prepare_or_wait_for_session</code>. However, it seems that logs suggest other workers are actually executing.</li>\n</ol>\n<p>Log is <a href=\"https://github.com/tensorflow/tensorflow/files/332895/log.txt\">here</a> and the code is here:</p>\n<pre><code>import sys\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport datetime\n\nflags = tf.app.flags\nflags.DEFINE_string(\"data_dir\", \"/tmp/data\",\n                    \"Directory for storing mnist data\")\n\nflags.DEFINE_boolean(\"download_only\", False,\n                     \"Only perform downloading of data; Do not proceed to \"\n                     \"session preparation, model definition or training\")\n\nflags.DEFINE_integer(\"worker_index\", 0,\n                     \"Worker task index, should be &gt;= 0. worker_index=0 is \"\n                     \"the master worker task the performs the variable \"\n                     \"initialization \")\n\nflags.DEFINE_string(\"workers\", None,\n                    \"The worker url list, separated by comma (e.g. tf-worker1:2222,1.2.3.4:2222)\")\n\nflags.DEFINE_string(\"parameter_servers\", None,\n                    \"The ps url list, separated by comma (e.g. tf-ps2:2222,1.2.3.5:2222)\")\n\nflags.DEFINE_integer(\"grpc_port\", 2222,\n                     \"TensorFlow GRPC port\")\n\nflags.DEFINE_integer(\"train_steps\", 200000,\n                     \"Number of (global) training steps to perform\")\n\nflags.DEFINE_string(\"worker_grpc_url\", None,\n                    \"Worker GRPC URL (e.g., grpc://1.2.3.4:2222, or \"\n                    \"grpc://tf-worker0:2222)\")\nFLAGS = flags.FLAGS\n\ncur_time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n\ndef nn_layer(input_tensor, input_dim, output_dim, act=tf.nn.relu):\n    with tf.name_scope(cur_time):\n        weights = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n        biases = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n    activations = act(tf.matmul(input_tensor, weights) + biases)\n    return activations\n\ndef model(x, y_, global_step):\n    hidden_nodes = 500\n    hidden1 = nn_layer(x, 784, hidden_nodes)\n    y = nn_layer(hidden1, hidden_nodes, 10, act=tf.nn.softmax)\n\n    cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy, global_step=global_step)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    return train_step, accuracy\n\nprint(\"Loading data from worker index = %d\" % FLAGS.worker_index)\n\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\nprint(\"Testing set size: %d\" % len(mnist.test.images))\nprint(\"Training set size: %d\" % len(mnist.train.images))\nif FLAGS.download_only: sys.exit(0)\n\nprint(\"Worker GRPC URL: %s\" % FLAGS.worker_grpc_url)\nprint(\"Workers = %s\" % FLAGS.workers)\nprint(\"Using time = %s\" % cur_time)\n\nis_chief = (FLAGS.worker_index == 0)\ncluster = tf.train.ClusterSpec({\"ps\": FLAGS.parameter_servers.split(\",\"), \"worker\": FLAGS.workers.split(\",\")})\n# Construct device setter object\ndevice_setter = tf.train.replica_device_setter(cluster=cluster)\n\n# The device setter will automatically place Variables ops on separate\n# parameter servers (ps). The non-Variable ops will be placed on the workers.\nwith tf.device(device_setter):\n    with tf.name_scope(cur_time):\n        global_step = tf.Variable(0, trainable=False)\n\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n    val_feed = {x: mnist.test.images, y_: mnist.test.labels}\n\n    train_step, accuracy = model(x, y_, global_step)\n\n    sv = tf.train.Supervisor(is_chief=is_chief,\n                             logdir=\"/tmp/dist-mnist-log/train\",\n                             saver=tf.train.Saver(),\n                             init_op=tf.initialize_all_variables(),\n                             recovery_wait_secs=1,\n                             global_step=global_step)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True,\n                                 device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.worker_index])\n\n    # The chief worker (worker_index==0) session will prepare the session,\n    # while the remaining workers will wait for the preparation to complete.\n    if is_chief:\n        print(\"Worker %d: Initializing session...\" % FLAGS.worker_index)\n    else:\n        print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.worker_index)\n\n    with sv.prepare_or_wait_for_session(FLAGS.worker_grpc_url, config=sess_config) as sess:\n        print(\"Worker %d: Session initialization complete.\" % FLAGS.worker_index)\n\n        # Perform training\n        time_begin = time.time()\n        print(\"Training begins @ %f\" % time_begin)\n\n        local_step = 0\n        while True:\n            # Training feed\n            batch_xs, batch_ys = mnist.train.next_batch(100)\n            train_feed = {x: batch_xs, y_: batch_ys}\n\n            _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n            local_step += 1\n            if local_step % 100 == 0:\n                print(\"Worker %d: training step %d done (global step: %d); Accuracy: %g\" % \n                      (FLAGS.worker_index, local_step, step, sess.run(accuracy, feed_dict=val_feed)))\n            if step &gt;= FLAGS.train_steps: break\n\n        time_end = time.time()\n        print(\"Training ends @ %f\" % time_end)\n        training_time = time_end - time_begin\n        print(\"Training elapsed time: %f s\" % training_time)\n\n        # Accuracy on test data\n        print(\"Final test accuracy = %g\" % (sess.run(accuracy, feed_dict=val_feed)))\n</code></pre>", "body_text": "Environment info\nOperating System: Kubernetes/Ubuntu 14.04\nSteps to reproduce\n\nstart tensorflow server on kubernetes using yaml generated by the python code\nStart tensorflow code:\n\npython mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30001 --worker_index=0 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n\npython mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30002 --worker_index=1 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n\nWhat have you tried?\n\nThe worker with index 0 (chief) can execute normally.\nIt was able to execute well (using the same yaml and code)\nI tried to restart the servers, but it didn't work.\nAll other workers stuck at prepare_or_wait_for_session. However, it seems that logs suggest other workers are actually executing.\n\nLog is here and the code is here:\nimport sys\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport datetime\n\nflags = tf.app.flags\nflags.DEFINE_string(\"data_dir\", \"/tmp/data\",\n                    \"Directory for storing mnist data\")\n\nflags.DEFINE_boolean(\"download_only\", False,\n                     \"Only perform downloading of data; Do not proceed to \"\n                     \"session preparation, model definition or training\")\n\nflags.DEFINE_integer(\"worker_index\", 0,\n                     \"Worker task index, should be >= 0. worker_index=0 is \"\n                     \"the master worker task the performs the variable \"\n                     \"initialization \")\n\nflags.DEFINE_string(\"workers\", None,\n                    \"The worker url list, separated by comma (e.g. tf-worker1:2222,1.2.3.4:2222)\")\n\nflags.DEFINE_string(\"parameter_servers\", None,\n                    \"The ps url list, separated by comma (e.g. tf-ps2:2222,1.2.3.5:2222)\")\n\nflags.DEFINE_integer(\"grpc_port\", 2222,\n                     \"TensorFlow GRPC port\")\n\nflags.DEFINE_integer(\"train_steps\", 200000,\n                     \"Number of (global) training steps to perform\")\n\nflags.DEFINE_string(\"worker_grpc_url\", None,\n                    \"Worker GRPC URL (e.g., grpc://1.2.3.4:2222, or \"\n                    \"grpc://tf-worker0:2222)\")\nFLAGS = flags.FLAGS\n\ncur_time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n\ndef nn_layer(input_tensor, input_dim, output_dim, act=tf.nn.relu):\n    with tf.name_scope(cur_time):\n        weights = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n        biases = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n    activations = act(tf.matmul(input_tensor, weights) + biases)\n    return activations\n\ndef model(x, y_, global_step):\n    hidden_nodes = 500\n    hidden1 = nn_layer(x, 784, hidden_nodes)\n    y = nn_layer(hidden1, hidden_nodes, 10, act=tf.nn.softmax)\n\n    cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy, global_step=global_step)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    return train_step, accuracy\n\nprint(\"Loading data from worker index = %d\" % FLAGS.worker_index)\n\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\nprint(\"Testing set size: %d\" % len(mnist.test.images))\nprint(\"Training set size: %d\" % len(mnist.train.images))\nif FLAGS.download_only: sys.exit(0)\n\nprint(\"Worker GRPC URL: %s\" % FLAGS.worker_grpc_url)\nprint(\"Workers = %s\" % FLAGS.workers)\nprint(\"Using time = %s\" % cur_time)\n\nis_chief = (FLAGS.worker_index == 0)\ncluster = tf.train.ClusterSpec({\"ps\": FLAGS.parameter_servers.split(\",\"), \"worker\": FLAGS.workers.split(\",\")})\n# Construct device setter object\ndevice_setter = tf.train.replica_device_setter(cluster=cluster)\n\n# The device setter will automatically place Variables ops on separate\n# parameter servers (ps). The non-Variable ops will be placed on the workers.\nwith tf.device(device_setter):\n    with tf.name_scope(cur_time):\n        global_step = tf.Variable(0, trainable=False)\n\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n    val_feed = {x: mnist.test.images, y_: mnist.test.labels}\n\n    train_step, accuracy = model(x, y_, global_step)\n\n    sv = tf.train.Supervisor(is_chief=is_chief,\n                             logdir=\"/tmp/dist-mnist-log/train\",\n                             saver=tf.train.Saver(),\n                             init_op=tf.initialize_all_variables(),\n                             recovery_wait_secs=1,\n                             global_step=global_step)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True,\n                                 device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.worker_index])\n\n    # The chief worker (worker_index==0) session will prepare the session,\n    # while the remaining workers will wait for the preparation to complete.\n    if is_chief:\n        print(\"Worker %d: Initializing session...\" % FLAGS.worker_index)\n    else:\n        print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.worker_index)\n\n    with sv.prepare_or_wait_for_session(FLAGS.worker_grpc_url, config=sess_config) as sess:\n        print(\"Worker %d: Session initialization complete.\" % FLAGS.worker_index)\n\n        # Perform training\n        time_begin = time.time()\n        print(\"Training begins @ %f\" % time_begin)\n\n        local_step = 0\n        while True:\n            # Training feed\n            batch_xs, batch_ys = mnist.train.next_batch(100)\n            train_feed = {x: batch_xs, y_: batch_ys}\n\n            _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n            local_step += 1\n            if local_step % 100 == 0:\n                print(\"Worker %d: training step %d done (global step: %d); Accuracy: %g\" % \n                      (FLAGS.worker_index, local_step, step, sess.run(accuracy, feed_dict=val_feed)))\n            if step >= FLAGS.train_steps: break\n\n        time_end = time.time()\n        print(\"Training ends @ %f\" % time_end)\n        training_time = time_end - time_begin\n        print(\"Training elapsed time: %f s\" % training_time)\n\n        # Accuracy on test data\n        print(\"Final test accuracy = %g\" % (sess.run(accuracy, feed_dict=val_feed)))", "body": "### Environment info\n\nOperating System: Kubernetes/Ubuntu 14.04\n### Steps to reproduce\n1. start tensorflow server on kubernetes using yaml generated by [the python code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/scripts/k8s_tensorflow.py)\n2. Start tensorflow code:\n\n```\npython mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30001 --worker_index=0 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n```\n\n```\npython mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30002 --worker_index=1 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n```\n### What have you tried?\n1. The worker with index 0 (chief) can execute normally.\n2. It was able to execute well (using the same yaml and code)\n3. I tried to restart the servers, but it didn't work.\n4. All other workers stuck at `prepare_or_wait_for_session`. However, it seems that logs suggest other workers are actually executing. \n\nLog is [here](https://github.com/tensorflow/tensorflow/files/332895/log.txt) and the code is here:\n\n```\nimport sys\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport datetime\n\nflags = tf.app.flags\nflags.DEFINE_string(\"data_dir\", \"/tmp/data\",\n                    \"Directory for storing mnist data\")\n\nflags.DEFINE_boolean(\"download_only\", False,\n                     \"Only perform downloading of data; Do not proceed to \"\n                     \"session preparation, model definition or training\")\n\nflags.DEFINE_integer(\"worker_index\", 0,\n                     \"Worker task index, should be >= 0. worker_index=0 is \"\n                     \"the master worker task the performs the variable \"\n                     \"initialization \")\n\nflags.DEFINE_string(\"workers\", None,\n                    \"The worker url list, separated by comma (e.g. tf-worker1:2222,1.2.3.4:2222)\")\n\nflags.DEFINE_string(\"parameter_servers\", None,\n                    \"The ps url list, separated by comma (e.g. tf-ps2:2222,1.2.3.5:2222)\")\n\nflags.DEFINE_integer(\"grpc_port\", 2222,\n                     \"TensorFlow GRPC port\")\n\nflags.DEFINE_integer(\"train_steps\", 200000,\n                     \"Number of (global) training steps to perform\")\n\nflags.DEFINE_string(\"worker_grpc_url\", None,\n                    \"Worker GRPC URL (e.g., grpc://1.2.3.4:2222, or \"\n                    \"grpc://tf-worker0:2222)\")\nFLAGS = flags.FLAGS\n\ncur_time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n\ndef nn_layer(input_tensor, input_dim, output_dim, act=tf.nn.relu):\n    with tf.name_scope(cur_time):\n        weights = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n        biases = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n    activations = act(tf.matmul(input_tensor, weights) + biases)\n    return activations\n\ndef model(x, y_, global_step):\n    hidden_nodes = 500\n    hidden1 = nn_layer(x, 784, hidden_nodes)\n    y = nn_layer(hidden1, hidden_nodes, 10, act=tf.nn.softmax)\n\n    cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy, global_step=global_step)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    return train_step, accuracy\n\nprint(\"Loading data from worker index = %d\" % FLAGS.worker_index)\n\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\nprint(\"Testing set size: %d\" % len(mnist.test.images))\nprint(\"Training set size: %d\" % len(mnist.train.images))\nif FLAGS.download_only: sys.exit(0)\n\nprint(\"Worker GRPC URL: %s\" % FLAGS.worker_grpc_url)\nprint(\"Workers = %s\" % FLAGS.workers)\nprint(\"Using time = %s\" % cur_time)\n\nis_chief = (FLAGS.worker_index == 0)\ncluster = tf.train.ClusterSpec({\"ps\": FLAGS.parameter_servers.split(\",\"), \"worker\": FLAGS.workers.split(\",\")})\n# Construct device setter object\ndevice_setter = tf.train.replica_device_setter(cluster=cluster)\n\n# The device setter will automatically place Variables ops on separate\n# parameter servers (ps). The non-Variable ops will be placed on the workers.\nwith tf.device(device_setter):\n    with tf.name_scope(cur_time):\n        global_step = tf.Variable(0, trainable=False)\n\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n    val_feed = {x: mnist.test.images, y_: mnist.test.labels}\n\n    train_step, accuracy = model(x, y_, global_step)\n\n    sv = tf.train.Supervisor(is_chief=is_chief,\n                             logdir=\"/tmp/dist-mnist-log/train\",\n                             saver=tf.train.Saver(),\n                             init_op=tf.initialize_all_variables(),\n                             recovery_wait_secs=1,\n                             global_step=global_step)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True,\n                                 device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.worker_index])\n\n    # The chief worker (worker_index==0) session will prepare the session,\n    # while the remaining workers will wait for the preparation to complete.\n    if is_chief:\n        print(\"Worker %d: Initializing session...\" % FLAGS.worker_index)\n    else:\n        print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.worker_index)\n\n    with sv.prepare_or_wait_for_session(FLAGS.worker_grpc_url, config=sess_config) as sess:\n        print(\"Worker %d: Session initialization complete.\" % FLAGS.worker_index)\n\n        # Perform training\n        time_begin = time.time()\n        print(\"Training begins @ %f\" % time_begin)\n\n        local_step = 0\n        while True:\n            # Training feed\n            batch_xs, batch_ys = mnist.train.next_batch(100)\n            train_feed = {x: batch_xs, y_: batch_ys}\n\n            _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n            local_step += 1\n            if local_step % 100 == 0:\n                print(\"Worker %d: training step %d done (global step: %d); Accuracy: %g\" % \n                      (FLAGS.worker_index, local_step, step, sess.run(accuracy, feed_dict=val_feed)))\n            if step >= FLAGS.train_steps: break\n\n        time_end = time.time()\n        print(\"Training ends @ %f\" % time_end)\n        training_time = time_end - time_begin\n        print(\"Training elapsed time: %f s\" % training_time)\n\n        # Accuracy on test data\n        print(\"Final test accuracy = %g\" % (sess.run(accuracy, feed_dict=val_feed)))\n```\n"}