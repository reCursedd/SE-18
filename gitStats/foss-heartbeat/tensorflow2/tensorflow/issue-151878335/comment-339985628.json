{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/339985628", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-339985628", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 339985628, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTk4NTYyOA==", "user": {"login": "Pepslee", "id": 13853798, "node_id": "MDQ6VXNlcjEzODUzNzk4", "avatar_url": "https://avatars1.githubusercontent.com/u/13853798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pepslee", "html_url": "https://github.com/Pepslee", "followers_url": "https://api.github.com/users/Pepslee/followers", "following_url": "https://api.github.com/users/Pepslee/following{/other_user}", "gists_url": "https://api.github.com/users/Pepslee/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pepslee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pepslee/subscriptions", "organizations_url": "https://api.github.com/users/Pepslee/orgs", "repos_url": "https://api.github.com/users/Pepslee/repos", "events_url": "https://api.github.com/users/Pepslee/events{/privacy}", "received_events_url": "https://api.github.com/users/Pepslee/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-27T14:22:11Z", "updated_at": "2017-10-27T14:25:11Z", "author_association": "NONE", "body_html": "<p>new implementation,   <code>tf.one_hot </code> has GPU implementation, but i check only forward computation, and i`m not sure, that  backward gradient  is implemented for this operation</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">unpool</span>(<span class=\"pl-smi\">pool</span>, <span class=\"pl-smi\">ind</span>, <span class=\"pl-smi\">ksize</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>unpool<span class=\"pl-pds\">'</span></span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">       Unpooling layer after max_pool_with_argmax.</span>\n<span class=\"pl-s\">       Args:</span>\n<span class=\"pl-s\">           pool:   max pooled output tensor</span>\n<span class=\"pl-s\">           ind:      argmax indices (produced by tf.nn.max_pool_with_argmax)</span>\n<span class=\"pl-s\">           ksize:     ksize is the same as for the pool</span>\n<span class=\"pl-s\">       Return:</span>\n<span class=\"pl-s\">           unpooled:    unpooling tensor</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n        pooled_shape <span class=\"pl-k\">=</span> pool.get_shape().as_list()\n\n        flatten_ind <span class=\"pl-k\">=</span> tf.reshape(ind, (pooled_shape[<span class=\"pl-c1\">0</span>], pooled_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> pooled_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> pooled_shape[<span class=\"pl-c1\">3</span>]))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> sparse indices to dense ones_like matrics</span>\n        one_hot_ind <span class=\"pl-k\">=</span> tf.one_hot(flatten_ind,  pooled_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> pooled_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> pooled_shape[<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">on_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>., <span class=\"pl-v\">off_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>., <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        one_hot_ind <span class=\"pl-k\">=</span> tf.reduce_sum(one_hot_ind, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        one_like_mask <span class=\"pl-k\">=</span> tf.reshape(one_hot_ind, (pooled_shape[<span class=\"pl-c1\">0</span>], pooled_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">1</span>], pooled_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">2</span>], pooled_shape[<span class=\"pl-c1\">3</span>]))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> resize input array to the output size by nearest neighbor</span>\n        img <span class=\"pl-k\">=</span> tf.image.resize_nearest_neighbor(pool, [pooled_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">1</span>], pooled_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">2</span>]])\n        unpooled <span class=\"pl-k\">=</span> tf.multiply(img, tf.cast(one_like_mask, img.dtype))\n        <span class=\"pl-k\">return</span> unpooled</pre></div>", "body_text": "new implementation,   tf.one_hot  has GPU implementation, but i check only forward computation, and i`m not sure, that  backward gradient  is implemented for this operation\ndef unpool(pool, ind, ksize=(1, 2, 2, 1), scope='unpool'):\n    \"\"\"\n       Unpooling layer after max_pool_with_argmax.\n       Args:\n           pool:   max pooled output tensor\n           ind:      argmax indices (produced by tf.nn.max_pool_with_argmax)\n           ksize:     ksize is the same as for the pool\n       Return:\n           unpooled:    unpooling tensor\n    \"\"\"\n    with tf.variable_scope(scope):\n        pooled_shape = pool.get_shape().as_list()\n\n        flatten_ind = tf.reshape(ind, (pooled_shape[0], pooled_shape[1] * pooled_shape[2] * pooled_shape[3]))\n        # sparse indices to dense ones_like matrics\n        one_hot_ind = tf.one_hot(flatten_ind,  pooled_shape[1] * ksize[1] * pooled_shape[2] * ksize[2] * pooled_shape[3], on_value=1., off_value=0., axis=-1)\n        one_hot_ind = tf.reduce_sum(one_hot_ind, axis=1)\n        one_like_mask = tf.reshape(one_hot_ind, (pooled_shape[0], pooled_shape[1] * ksize[1], pooled_shape[2] * ksize[2], pooled_shape[3]))\n        # resize input array to the output size by nearest neighbor\n        img = tf.image.resize_nearest_neighbor(pool, [pooled_shape[1] * ksize[1], pooled_shape[2] * ksize[2]])\n        unpooled = tf.multiply(img, tf.cast(one_like_mask, img.dtype))\n        return unpooled", "body": "new implementation,   ```tf.one_hot ``` has GPU implementation, but i check only forward computation, and i`m not sure, that  backward gradient  is implemented for this operation\r\n```python \r\ndef unpool(pool, ind, ksize=(1, 2, 2, 1), scope='unpool'):\r\n    \"\"\"\r\n       Unpooling layer after max_pool_with_argmax.\r\n       Args:\r\n           pool:   max pooled output tensor\r\n           ind:      argmax indices (produced by tf.nn.max_pool_with_argmax)\r\n           ksize:     ksize is the same as for the pool\r\n       Return:\r\n           unpooled:    unpooling tensor\r\n    \"\"\"\r\n    with tf.variable_scope(scope):\r\n        pooled_shape = pool.get_shape().as_list()\r\n\r\n        flatten_ind = tf.reshape(ind, (pooled_shape[0], pooled_shape[1] * pooled_shape[2] * pooled_shape[3]))\r\n        # sparse indices to dense ones_like matrics\r\n        one_hot_ind = tf.one_hot(flatten_ind,  pooled_shape[1] * ksize[1] * pooled_shape[2] * ksize[2] * pooled_shape[3], on_value=1., off_value=0., axis=-1)\r\n        one_hot_ind = tf.reduce_sum(one_hot_ind, axis=1)\r\n        one_like_mask = tf.reshape(one_hot_ind, (pooled_shape[0], pooled_shape[1] * ksize[1], pooled_shape[2] * ksize[2], pooled_shape[3]))\r\n        # resize input array to the output size by nearest neighbor\r\n        img = tf.image.resize_nearest_neighbor(pool, [pooled_shape[1] * ksize[1], pooled_shape[2] * ksize[2]])\r\n        unpooled = tf.multiply(img, tf.cast(one_like_mask, img.dtype))\r\n        return unpooled\r\n```"}