{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/338931264", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-338931264", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 338931264, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODkzMTI2NA==", "user": {"login": "rayanelleuch", "id": 10159876, "node_id": "MDQ6VXNlcjEwMTU5ODc2", "avatar_url": "https://avatars2.githubusercontent.com/u/10159876?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rayanelleuch", "html_url": "https://github.com/rayanelleuch", "followers_url": "https://api.github.com/users/rayanelleuch/followers", "following_url": "https://api.github.com/users/rayanelleuch/following{/other_user}", "gists_url": "https://api.github.com/users/rayanelleuch/gists{/gist_id}", "starred_url": "https://api.github.com/users/rayanelleuch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rayanelleuch/subscriptions", "organizations_url": "https://api.github.com/users/rayanelleuch/orgs", "repos_url": "https://api.github.com/users/rayanelleuch/repos", "events_url": "https://api.github.com/users/rayanelleuch/events{/privacy}", "received_events_url": "https://api.github.com/users/rayanelleuch/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-24T09:30:20Z", "updated_at": "2017-10-25T16:09:20Z", "author_association": "NONE", "body_html": "<p>Small improvement of ThomasWollmann code to add the known shape to the output tensor (also removed the tf.stack that were not needed).<br>\nUseful when using tf.contrib.layers.conv2d.</p>\n<pre><code>def unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\n    \"\"\"\n       Unpooling layer after max_pool_with_argmax.\n       Args:\n           pool:   max pooled output tensor\n           ind:      argmax indices\n           ksize:     ksize is the same as for the pool\n       Return:\n           unpool:    unpooling tensor\n    \"\"\"\n    with tf.variable_scope(scope):\n        input_shape = tf.shape(pool)\n        output_shape = [input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]]\n\n        flat_input_size = tf.reduce_prod(input_shape)\n        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n\n        pool_ = tf.reshape(pool, [flat_input_size])\n        batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype), \n                                          shape=[input_shape[0], 1, 1, 1])\n        b = tf.ones_like(ind) * batch_range\n        b1 = tf.reshape(b, [flat_input_size, 1])\n        ind_ = tf.reshape(ind, [flat_input_size, 1])\n        ind_ = tf.concat([b1, ind_], 1)\n\n        ret = tf.scatter_nd(ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\n        ret = tf.reshape(ret, output_shape)\n\n        set_input_shape = pool.get_shape()\n        set_output_shape = [set_input_shape[0], set_input_shape[1] * ksize[1], set_input_shape[2] * ksize[2], set_input_shape[3]]\n        ret.set_shape(set_output_shape)\n        return ret\n</code></pre>", "body_text": "Small improvement of ThomasWollmann code to add the known shape to the output tensor (also removed the tf.stack that were not needed).\nUseful when using tf.contrib.layers.conv2d.\ndef unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\n    \"\"\"\n       Unpooling layer after max_pool_with_argmax.\n       Args:\n           pool:   max pooled output tensor\n           ind:      argmax indices\n           ksize:     ksize is the same as for the pool\n       Return:\n           unpool:    unpooling tensor\n    \"\"\"\n    with tf.variable_scope(scope):\n        input_shape = tf.shape(pool)\n        output_shape = [input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]]\n\n        flat_input_size = tf.reduce_prod(input_shape)\n        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n\n        pool_ = tf.reshape(pool, [flat_input_size])\n        batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype), \n                                          shape=[input_shape[0], 1, 1, 1])\n        b = tf.ones_like(ind) * batch_range\n        b1 = tf.reshape(b, [flat_input_size, 1])\n        ind_ = tf.reshape(ind, [flat_input_size, 1])\n        ind_ = tf.concat([b1, ind_], 1)\n\n        ret = tf.scatter_nd(ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\n        ret = tf.reshape(ret, output_shape)\n\n        set_input_shape = pool.get_shape()\n        set_output_shape = [set_input_shape[0], set_input_shape[1] * ksize[1], set_input_shape[2] * ksize[2], set_input_shape[3]]\n        ret.set_shape(set_output_shape)\n        return ret", "body": "Small improvement of ThomasWollmann code to add the known shape to the output tensor (also removed the tf.stack that were not needed).\r\nUseful when using tf.contrib.layers.conv2d.\r\n\r\n\r\n```\r\ndef unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\r\n    \"\"\"\r\n       Unpooling layer after max_pool_with_argmax.\r\n       Args:\r\n           pool:   max pooled output tensor\r\n           ind:      argmax indices\r\n           ksize:     ksize is the same as for the pool\r\n       Return:\r\n           unpool:    unpooling tensor\r\n    \"\"\"\r\n    with tf.variable_scope(scope):\r\n        input_shape = tf.shape(pool)\r\n        output_shape = [input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]]\r\n\r\n        flat_input_size = tf.reduce_prod(input_shape)\r\n        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\r\n\r\n        pool_ = tf.reshape(pool, [flat_input_size])\r\n        batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype), \r\n                                          shape=[input_shape[0], 1, 1, 1])\r\n        b = tf.ones_like(ind) * batch_range\r\n        b1 = tf.reshape(b, [flat_input_size, 1])\r\n        ind_ = tf.reshape(ind, [flat_input_size, 1])\r\n        ind_ = tf.concat([b1, ind_], 1)\r\n\r\n        ret = tf.scatter_nd(ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\r\n        ret = tf.reshape(ret, output_shape)\r\n\r\n        set_input_shape = pool.get_shape()\r\n        set_output_shape = [set_input_shape[0], set_input_shape[1] * ksize[1], set_input_shape[2] * ksize[2], set_input_shape[3]]\r\n        ret.set_shape(set_output_shape)\r\n        return ret\r\n```\r\n"}