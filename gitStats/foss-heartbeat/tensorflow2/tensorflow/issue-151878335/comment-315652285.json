{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/315652285", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-315652285", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 315652285, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTY1MjI4NQ==", "user": {"login": "chahld", "id": 8064384, "node_id": "MDQ6VXNlcjgwNjQzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/8064384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chahld", "html_url": "https://github.com/chahld", "followers_url": "https://api.github.com/users/chahld/followers", "following_url": "https://api.github.com/users/chahld/following{/other_user}", "gists_url": "https://api.github.com/users/chahld/gists{/gist_id}", "starred_url": "https://api.github.com/users/chahld/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chahld/subscriptions", "organizations_url": "https://api.github.com/users/chahld/orgs", "repos_url": "https://api.github.com/users/chahld/repos", "events_url": "https://api.github.com/users/chahld/events{/privacy}", "received_events_url": "https://api.github.com/users/chahld/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-17T01:17:42Z", "updated_at": "2017-07-17T01:17:42Z", "author_association": "NONE", "body_html": "<p>I've adapted Pepslee's version to use tf.scatter_nd, instead of tf.scatter_nd_update to avoid creation of a Variable. The Variable was causing problems to checkpoint files because of fixed batch size, so if you run with a different batch size it wasn't able to read the checkpoint.</p>\n<pre><code>def unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\n    \"\"\"\n       Unpooling layer after max_pool_with_argmax.\n       Args:\n           pool:   max pooled output tensor\n           ind:      argmax indices\n           ksize:     ksize is the same as for the pool\n       Return:\n           unpool:    unpooling tensor\n    \"\"\"\n    with tf.variable_scope(scope):\n        input_shape = pool.get_shape().as_list()\n        output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n        flat_input_size = np.prod(input_shape)\n        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n\n        pool_ = tf.reshape(pool, [flat_input_size])\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\n        b = tf.ones_like(ind) * batch_range\n        b = tf.reshape(b, [flat_input_size, 1])\n        ind_ = tf.reshape(ind, [flat_input_size, 1])\n        ind_ = tf.concat([b, ind_], 1)\n\n        ret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\n        ret = tf.reshape(ret, output_shape)\n        return ret\n</code></pre>", "body_text": "I've adapted Pepslee's version to use tf.scatter_nd, instead of tf.scatter_nd_update to avoid creation of a Variable. The Variable was causing problems to checkpoint files because of fixed batch size, so if you run with a different batch size it wasn't able to read the checkpoint.\ndef unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\n    \"\"\"\n       Unpooling layer after max_pool_with_argmax.\n       Args:\n           pool:   max pooled output tensor\n           ind:      argmax indices\n           ksize:     ksize is the same as for the pool\n       Return:\n           unpool:    unpooling tensor\n    \"\"\"\n    with tf.variable_scope(scope):\n        input_shape = pool.get_shape().as_list()\n        output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n        flat_input_size = np.prod(input_shape)\n        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n\n        pool_ = tf.reshape(pool, [flat_input_size])\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\n        b = tf.ones_like(ind) * batch_range\n        b = tf.reshape(b, [flat_input_size, 1])\n        ind_ = tf.reshape(ind, [flat_input_size, 1])\n        ind_ = tf.concat([b, ind_], 1)\n\n        ret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\n        ret = tf.reshape(ret, output_shape)\n        return ret", "body": "I've adapted Pepslee's version to use tf.scatter_nd, instead of tf.scatter_nd_update to avoid creation of a Variable. The Variable was causing problems to checkpoint files because of fixed batch size, so if you run with a different batch size it wasn't able to read the checkpoint. \r\n\r\n```\r\ndef unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\r\n    \"\"\"\r\n       Unpooling layer after max_pool_with_argmax.\r\n       Args:\r\n           pool:   max pooled output tensor\r\n           ind:      argmax indices\r\n           ksize:     ksize is the same as for the pool\r\n       Return:\r\n           unpool:    unpooling tensor\r\n    \"\"\"\r\n    with tf.variable_scope(scope):\r\n        input_shape = pool.get_shape().as_list()\r\n        output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\r\n\r\n        flat_input_size = np.prod(input_shape)\r\n        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\r\n\r\n        pool_ = tf.reshape(pool, [flat_input_size])\r\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\r\n        b = tf.ones_like(ind) * batch_range\r\n        b = tf.reshape(b, [flat_input_size, 1])\r\n        ind_ = tf.reshape(ind, [flat_input_size, 1])\r\n        ind_ = tf.concat([b, ind_], 1)\r\n\r\n        ret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\r\n        ret = tf.reshape(ret, output_shape)\r\n        return ret\r\n```"}