{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235743155", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-235743155", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 235743155, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTc0MzE1NQ==", "user": {"login": "syed-ahmed", "id": 8906225, "node_id": "MDQ6VXNlcjg5MDYyMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8906225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syed-ahmed", "html_url": "https://github.com/syed-ahmed", "followers_url": "https://api.github.com/users/syed-ahmed/followers", "following_url": "https://api.github.com/users/syed-ahmed/following{/other_user}", "gists_url": "https://api.github.com/users/syed-ahmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/syed-ahmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syed-ahmed/subscriptions", "organizations_url": "https://api.github.com/users/syed-ahmed/orgs", "repos_url": "https://api.github.com/users/syed-ahmed/repos", "events_url": "https://api.github.com/users/syed-ahmed/events{/privacy}", "received_events_url": "https://api.github.com/users/syed-ahmed/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-27T22:37:23Z", "updated_at": "2016-07-28T00:03:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a>. Sorry for not posting the full code. I didn't want to lengthen this issue by posting all the code. You can review the changes in this <a href=\"https://github.com/syed-ahmed/tensorflow/commit/95c145b19ce0537b80368472d7daf2bbe7d03788\">link</a>.</p>\n<p>I am calling the max unpool like this:</p>\n<pre><code> return gen_nn_ops._max_unpool(array_ops.shape(origin_input_tensor), grad,\n                                     argmax_tensor,\n                                     ksize=[1, 2, 2, 1], strides=[1,1,1,1],\n                                     padding=\"VALID\", name=name)\n</code></pre>\n<p>I am not sure if the origin_input_tensor and argmax_tensor objects are in CPU or GPU. The cuda-gdb output of MaxUnpoolForward suggests that \"This occurs when any thread within a warp accesses an address that is outside the valid range of local or shared memory regions.\" <a href=\"http://docs.nvidia.com/cuda/cuda-gdb/#gpu-error-reporting\" rel=\"nofollow\">gpu error reporting</a></p>", "body_text": "Hi @girving. Sorry for not posting the full code. I didn't want to lengthen this issue by posting all the code. You can review the changes in this link.\nI am calling the max unpool like this:\n return gen_nn_ops._max_unpool(array_ops.shape(origin_input_tensor), grad,\n                                     argmax_tensor,\n                                     ksize=[1, 2, 2, 1], strides=[1,1,1,1],\n                                     padding=\"VALID\", name=name)\n\nI am not sure if the origin_input_tensor and argmax_tensor objects are in CPU or GPU. The cuda-gdb output of MaxUnpoolForward suggests that \"This occurs when any thread within a warp accesses an address that is outside the valid range of local or shared memory regions.\" gpu error reporting", "body": "Hi @girving. Sorry for not posting the full code. I didn't want to lengthen this issue by posting all the code. You can review the changes in this [link](https://github.com/syed-ahmed/tensorflow/commit/95c145b19ce0537b80368472d7daf2bbe7d03788). \n\nI am calling the max unpool like this:\n\n```\n return gen_nn_ops._max_unpool(array_ops.shape(origin_input_tensor), grad,\n                                     argmax_tensor,\n                                     ksize=[1, 2, 2, 1], strides=[1,1,1,1],\n                                     padding=\"VALID\", name=name)\n```\n\nI am not sure if the origin_input_tensor and argmax_tensor objects are in CPU or GPU. The cuda-gdb output of MaxUnpoolForward suggests that \"This occurs when any thread within a warp accesses an address that is outside the valid range of local or shared memory regions.\" [gpu error reporting](http://docs.nvidia.com/cuda/cuda-gdb/#gpu-error-reporting)\n"}