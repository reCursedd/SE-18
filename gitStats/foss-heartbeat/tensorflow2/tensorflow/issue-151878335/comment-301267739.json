{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301267739", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-301267739", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 301267739, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTI2NzczOQ==", "user": {"login": "Panaetius", "id": 664486, "node_id": "MDQ6VXNlcjY2NDQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/664486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Panaetius", "html_url": "https://github.com/Panaetius", "followers_url": "https://api.github.com/users/Panaetius/followers", "following_url": "https://api.github.com/users/Panaetius/following{/other_user}", "gists_url": "https://api.github.com/users/Panaetius/gists{/gist_id}", "starred_url": "https://api.github.com/users/Panaetius/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Panaetius/subscriptions", "organizations_url": "https://api.github.com/users/Panaetius/orgs", "repos_url": "https://api.github.com/users/Panaetius/repos", "events_url": "https://api.github.com/users/Panaetius/events{/privacy}", "received_events_url": "https://api.github.com/users/Panaetius/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-13T18:56:59Z", "updated_at": "2017-05-13T18:58:38Z", "author_association": "NONE", "body_html": "<p>I think I found bug/issue with tf.nn.max_pool_with_argmax and the unpool workaround as presented here.</p>\n<p>tf.nn.max_pool_with_argmax indices are calculated as (y * w + x) * channels + c, but the \"w\" the width of the input tensor, not the width of the input tensor + padding, if any padding (padding='SAME' and width of tensor being odd) is applied.</p>\n<p>Using the unpool method, the width is calculated by dividing/modulo  that output with input_shape[2] * ksize[2], with padding this will be 1 pixel bigger than the width that tf.nn.max_pool_with_argmax uses for its argmax output. So if a padding is applied, every row of the output image of the unpool() op will be  slightly offset, leading to the image being slightly tilted.</p>\n<p>I'm currently implementing SegNet, which has several unpool operations one after the other, each making the tilting worse if there was any padding for it, which is really noticeable when looking at the final output.</p>\n<p>My workaround was to change the proposed unpool operation by simply adding an input-argument for the output shape as follows:</p>\n<pre><code>def unpool(updates, mask, ksize=[1, 2, 2, 1], output_shape=None, name=''):\n    with tf.variable_scope(name):\n        mask = tf.cast(mask, tf.int32)\n        input_shape = tf.shape(updates, out_type=tf.int32)\n        #  calculation new shape\n        if output_shape is None:\n            output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n        # calculation indices for batch, height, width and feature maps\n        one_like_mask = tf.ones_like(mask, dtype=tf.int32)\n        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\n        b = one_like_mask * batch_range\n        y = mask // (output_shape[2] * output_shape[3])\n        x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n        feature_range = tf.range(output_shape[3], dtype=tf.int32)\n        f = one_like_mask * feature_range\n        # transpose indices &amp; reshape update values to one dimension\n        updates_size = tf.size(updates)\n        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n        values = tf.reshape(updates, [updates_size])\n        ret = tf.scatter_nd(indices, values, output_shape)\n        return ret\n</code></pre>\n<p>then when calling the op, I supply the shape of the convolution in the encoder part of segnet as output_shape, so the code will use the correct (well, incorrect...) width when transforming the tf.nn.max_pool_with_argmax indices.</p>\n<p>Arguably, this is a bug with tf.nn.max_pool_with_argmax, since it should calculate the argmax indices by taking potential padding into account</p>", "body_text": "I think I found bug/issue with tf.nn.max_pool_with_argmax and the unpool workaround as presented here.\ntf.nn.max_pool_with_argmax indices are calculated as (y * w + x) * channels + c, but the \"w\" the width of the input tensor, not the width of the input tensor + padding, if any padding (padding='SAME' and width of tensor being odd) is applied.\nUsing the unpool method, the width is calculated by dividing/modulo  that output with input_shape[2] * ksize[2], with padding this will be 1 pixel bigger than the width that tf.nn.max_pool_with_argmax uses for its argmax output. So if a padding is applied, every row of the output image of the unpool() op will be  slightly offset, leading to the image being slightly tilted.\nI'm currently implementing SegNet, which has several unpool operations one after the other, each making the tilting worse if there was any padding for it, which is really noticeable when looking at the final output.\nMy workaround was to change the proposed unpool operation by simply adding an input-argument for the output shape as follows:\ndef unpool(updates, mask, ksize=[1, 2, 2, 1], output_shape=None, name=''):\n    with tf.variable_scope(name):\n        mask = tf.cast(mask, tf.int32)\n        input_shape = tf.shape(updates, out_type=tf.int32)\n        #  calculation new shape\n        if output_shape is None:\n            output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n        # calculation indices for batch, height, width and feature maps\n        one_like_mask = tf.ones_like(mask, dtype=tf.int32)\n        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\n        b = one_like_mask * batch_range\n        y = mask // (output_shape[2] * output_shape[3])\n        x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n        feature_range = tf.range(output_shape[3], dtype=tf.int32)\n        f = one_like_mask * feature_range\n        # transpose indices & reshape update values to one dimension\n        updates_size = tf.size(updates)\n        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n        values = tf.reshape(updates, [updates_size])\n        ret = tf.scatter_nd(indices, values, output_shape)\n        return ret\n\nthen when calling the op, I supply the shape of the convolution in the encoder part of segnet as output_shape, so the code will use the correct (well, incorrect...) width when transforming the tf.nn.max_pool_with_argmax indices.\nArguably, this is a bug with tf.nn.max_pool_with_argmax, since it should calculate the argmax indices by taking potential padding into account", "body": "I think I found bug/issue with tf.nn.max_pool_with_argmax and the unpool workaround as presented here.\r\n\r\ntf.nn.max_pool_with_argmax indices are calculated as (y * w + x) * channels + c, but the \"w\" the width of the input tensor, not the width of the input tensor + padding, if any padding (padding='SAME' and width of tensor being odd) is applied. \r\n\r\nUsing the unpool method, the width is calculated by dividing/modulo  that output with input_shape[2] * ksize[2], with padding this will be 1 pixel bigger than the width that tf.nn.max_pool_with_argmax uses for its argmax output. So if a padding is applied, every row of the output image of the unpool() op will be  slightly offset, leading to the image being slightly tilted.\r\n\r\nI'm currently implementing SegNet, which has several unpool operations one after the other, each making the tilting worse if there was any padding for it, which is really noticeable when looking at the final output.\r\n\r\nMy workaround was to change the proposed unpool operation by simply adding an input-argument for the output shape as follows:\r\n\r\n    def unpool(updates, mask, ksize=[1, 2, 2, 1], output_shape=None, name=''):\r\n        with tf.variable_scope(name):\r\n            mask = tf.cast(mask, tf.int32)\r\n            input_shape = tf.shape(updates, out_type=tf.int32)\r\n            #  calculation new shape\r\n            if output_shape is None:\r\n                output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\r\n\r\n            # calculation indices for batch, height, width and feature maps\r\n            one_like_mask = tf.ones_like(mask, dtype=tf.int32)\r\n            batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\r\n            batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\r\n            b = one_like_mask * batch_range\r\n            y = mask // (output_shape[2] * output_shape[3])\r\n            x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]\r\n            feature_range = tf.range(output_shape[3], dtype=tf.int32)\r\n            f = one_like_mask * feature_range\r\n            # transpose indices & reshape update values to one dimension\r\n            updates_size = tf.size(updates)\r\n            indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\r\n            values = tf.reshape(updates, [updates_size])\r\n            ret = tf.scatter_nd(indices, values, output_shape)\r\n            return ret\r\n\r\nthen when calling the op, I supply the shape of the convolution in the encoder part of segnet as output_shape, so the code will use the correct (well, incorrect...) width when transforming the tf.nn.max_pool_with_argmax indices.\r\n\r\nArguably, this is a bug with tf.nn.max_pool_with_argmax, since it should calculate the argmax indices by taking potential padding into account"}