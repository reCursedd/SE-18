{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251808361", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-251808361", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 251808361, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTgwODM2MQ==", "user": {"login": "hermitman", "id": 5199377, "node_id": "MDQ6VXNlcjUxOTkzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5199377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hermitman", "html_url": "https://github.com/hermitman", "followers_url": "https://api.github.com/users/hermitman/followers", "following_url": "https://api.github.com/users/hermitman/following{/other_user}", "gists_url": "https://api.github.com/users/hermitman/gists{/gist_id}", "starred_url": "https://api.github.com/users/hermitman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hermitman/subscriptions", "organizations_url": "https://api.github.com/users/hermitman/orgs", "repos_url": "https://api.github.com/users/hermitman/repos", "events_url": "https://api.github.com/users/hermitman/events{/privacy}", "received_events_url": "https://api.github.com/users/hermitman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-05T21:43:07Z", "updated_at": "2016-10-05T21:57:59Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1525818\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fabianbormann\">@fabianbormann</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a>  Can your unpooling operation backpropagate gradients? I manage to get one version work on the forward pass, but tensorflow could not backpropagate the gradients. My code currently works like this:</p>\n<pre><code>def unpool_layer2x2(inputs, argmax, name):\n\n    with tf.variable_scope(name) as scope:\n\n        x_shape = tf.shape(inputs)\n        batches = x_shape[0]\n        height = x_shape[1]\n        width = x_shape[2]\n        channels = x_shape[3]\n\n        height_ori = height * 2\n        width_ori = width*2\n\n        argmax_offset = tf.range(batches)\n        argmax_offset = tf.reshape(argmax_offset, [-1, 1, 1, 1])\n        with tf.device('/cpu:0'):\n          argmax_offset = tf.tile(argmax_offset, [1, height, width, channels]) * height_ori * width_ori * channels\n        argmax = argmax + tf.to_int64(argmax_offset)\n\n        list_x = tf.reshape(inputs, [batches*height*width*channels, 1])\n        list_argmax = tf.reshape(argmax, [batches*height*width*channels, 1])\n        list_indices_batches = list_argmax//tf.to_int64(height_ori*width_ori*channels)\n        with tf.device('/cpu:0'):\n            list_indices_height = list_argmax%tf.to_int64(height_ori*width_ori*channels) // tf.to_int64(width_ori*channels)\n            list_indices_width = list_argmax%tf.to_int64(width_ori*channels) // tf.to_int64(channels)\n            list_indices_channels = list_argmax%tf.to_int64(channels)\n            list_indices = tf.concat(1, [list_indices_batches, list_indices_height, list_indices_width, list_indices_channels])\n        output = tf.SparseTensor(list_indices, tf.squeeze(list_x), tf.to_int64([batches, height_ori, width_ori, channels]))\n        with tf.device('/cpu:0'):\n          return tf.sparse_tensor_to_dense(tf.sparse_reorder(output))\n</code></pre>\n<p>I am not familiar with how TF determine if a op is differentiable, so I do not know what I did was affecting the backprop. Could you direct me to some related readings?</p>", "body_text": "@fabianbormann @girving  Can your unpooling operation backpropagate gradients? I manage to get one version work on the forward pass, but tensorflow could not backpropagate the gradients. My code currently works like this:\ndef unpool_layer2x2(inputs, argmax, name):\n\n    with tf.variable_scope(name) as scope:\n\n        x_shape = tf.shape(inputs)\n        batches = x_shape[0]\n        height = x_shape[1]\n        width = x_shape[2]\n        channels = x_shape[3]\n\n        height_ori = height * 2\n        width_ori = width*2\n\n        argmax_offset = tf.range(batches)\n        argmax_offset = tf.reshape(argmax_offset, [-1, 1, 1, 1])\n        with tf.device('/cpu:0'):\n          argmax_offset = tf.tile(argmax_offset, [1, height, width, channels]) * height_ori * width_ori * channels\n        argmax = argmax + tf.to_int64(argmax_offset)\n\n        list_x = tf.reshape(inputs, [batches*height*width*channels, 1])\n        list_argmax = tf.reshape(argmax, [batches*height*width*channels, 1])\n        list_indices_batches = list_argmax//tf.to_int64(height_ori*width_ori*channels)\n        with tf.device('/cpu:0'):\n            list_indices_height = list_argmax%tf.to_int64(height_ori*width_ori*channels) // tf.to_int64(width_ori*channels)\n            list_indices_width = list_argmax%tf.to_int64(width_ori*channels) // tf.to_int64(channels)\n            list_indices_channels = list_argmax%tf.to_int64(channels)\n            list_indices = tf.concat(1, [list_indices_batches, list_indices_height, list_indices_width, list_indices_channels])\n        output = tf.SparseTensor(list_indices, tf.squeeze(list_x), tf.to_int64([batches, height_ori, width_ori, channels]))\n        with tf.device('/cpu:0'):\n          return tf.sparse_tensor_to_dense(tf.sparse_reorder(output))\n\nI am not familiar with how TF determine if a op is differentiable, so I do not know what I did was affecting the backprop. Could you direct me to some related readings?", "body": "@fabianbormann @girving  Can your unpooling operation backpropagate gradients? I manage to get one version work on the forward pass, but tensorflow could not backpropagate the gradients. My code currently works like this:\n\n```\ndef unpool_layer2x2(inputs, argmax, name):\n\n    with tf.variable_scope(name) as scope:\n\n        x_shape = tf.shape(inputs)\n        batches = x_shape[0]\n        height = x_shape[1]\n        width = x_shape[2]\n        channels = x_shape[3]\n\n        height_ori = height * 2\n        width_ori = width*2\n\n        argmax_offset = tf.range(batches)\n        argmax_offset = tf.reshape(argmax_offset, [-1, 1, 1, 1])\n        with tf.device('/cpu:0'):\n          argmax_offset = tf.tile(argmax_offset, [1, height, width, channels]) * height_ori * width_ori * channels\n        argmax = argmax + tf.to_int64(argmax_offset)\n\n        list_x = tf.reshape(inputs, [batches*height*width*channels, 1])\n        list_argmax = tf.reshape(argmax, [batches*height*width*channels, 1])\n        list_indices_batches = list_argmax//tf.to_int64(height_ori*width_ori*channels)\n        with tf.device('/cpu:0'):\n            list_indices_height = list_argmax%tf.to_int64(height_ori*width_ori*channels) // tf.to_int64(width_ori*channels)\n            list_indices_width = list_argmax%tf.to_int64(width_ori*channels) // tf.to_int64(channels)\n            list_indices_channels = list_argmax%tf.to_int64(channels)\n            list_indices = tf.concat(1, [list_indices_batches, list_indices_height, list_indices_width, list_indices_channels])\n        output = tf.SparseTensor(list_indices, tf.squeeze(list_x), tf.to_int64([batches, height_ori, width_ori, channels]))\n        with tf.device('/cpu:0'):\n          return tf.sparse_tensor_to_dense(tf.sparse_reorder(output))\n```\n\nI am not familiar with how TF determine if a op is differentiable, so I do not know what I did was affecting the backprop. Could you direct me to some related readings?\n"}