{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253110753", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-253110753", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 253110753, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzExMDc1Mw==", "user": {"login": "EmmaBYPeng", "id": 7233743, "node_id": "MDQ6VXNlcjcyMzM3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/7233743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EmmaBYPeng", "html_url": "https://github.com/EmmaBYPeng", "followers_url": "https://api.github.com/users/EmmaBYPeng/followers", "following_url": "https://api.github.com/users/EmmaBYPeng/following{/other_user}", "gists_url": "https://api.github.com/users/EmmaBYPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/EmmaBYPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EmmaBYPeng/subscriptions", "organizations_url": "https://api.github.com/users/EmmaBYPeng/orgs", "repos_url": "https://api.github.com/users/EmmaBYPeng/repos", "events_url": "https://api.github.com/users/EmmaBYPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/EmmaBYPeng/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-12T03:37:57Z", "updated_at": "2016-10-12T20:44:33Z", "author_association": "NONE", "body_html": "<p>Hi, I implemented the batch version (i.e. batch_size &gt;= 1) of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1525818\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fabianbormann\">@fabianbormann</a> 's unpool layer and it's been working well for me:</p>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-k\">def</span> <span class=\"pl-en\">unravel_argmax</span>(<span class=\"pl-smi\">argmax</span>, <span class=\"pl-smi\">shape</span>):\n    output_list <span class=\"pl-k\">=</span> [argmax <span class=\"pl-k\">//</span> (shape[<span class=\"pl-c1\">2</span>]<span class=\"pl-k\">*</span>shape[<span class=\"pl-c1\">3</span>]),\n                   argmax <span class=\"pl-k\">%</span> (shape[<span class=\"pl-c1\">2</span>]<span class=\"pl-k\">*</span>shape[<span class=\"pl-c1\">3</span>]) <span class=\"pl-k\">//</span> shape[<span class=\"pl-c1\">3</span>]]\n    <span class=\"pl-k\">return</span> tf.pack(output_list)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">unpool_layer2x2_batch</span>(<span class=\"pl-smi\">bottom</span>, <span class=\"pl-smi\">argmax</span>):\n    bottom_shape <span class=\"pl-k\">=</span> tf.shape(bottom)\n    top_shape <span class=\"pl-k\">=</span> [bottom_shape[<span class=\"pl-c1\">0</span>], bottom_shape[<span class=\"pl-c1\">1</span>]<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>, bottom_shape[<span class=\"pl-c1\">2</span>]<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>, bottom_shape[<span class=\"pl-c1\">3</span>]]\n\n    batch_size <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">0</span>]\n    height <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">1</span>]\n    width <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">2</span>]\n    channels <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">3</span>]\n\n    argmax_shape <span class=\"pl-k\">=</span> tf.to_int64([batch_size, height, width, channels])\n    argmax <span class=\"pl-k\">=</span> unravel_argmax(argmax, argmax_shape)\n\n    t1 <span class=\"pl-k\">=</span> tf.to_int64(tf.range(channels))\n    t1 <span class=\"pl-k\">=</span> tf.tile(t1, [batch_size<span class=\"pl-k\">*</span>(width<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)<span class=\"pl-k\">*</span>(height<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)])\n    t1 <span class=\"pl-k\">=</span> tf.reshape(t1, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, channels])\n    t1 <span class=\"pl-k\">=</span> tf.transpose(t1, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>])\n    t1 <span class=\"pl-k\">=</span> tf.reshape(t1, [channels, batch_size, height<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>, width<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\n    t1 <span class=\"pl-k\">=</span> tf.transpose(t1, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n\n    t2 <span class=\"pl-k\">=</span> tf.to_int64(tf.range(batch_size))\n    t2 <span class=\"pl-k\">=</span> tf.tile(t2, [channels<span class=\"pl-k\">*</span>(width<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)<span class=\"pl-k\">*</span>(height<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)])\n    t2 <span class=\"pl-k\">=</span> tf.reshape(t2, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, batch_size])\n    t2 <span class=\"pl-k\">=</span> tf.transpose(t2, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>])\n    t2 <span class=\"pl-k\">=</span> tf.reshape(t2, [batch_size, channels, height<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>, width<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\n\n    t3 <span class=\"pl-k\">=</span> tf.transpose(argmax, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">0</span>])\n\n    t <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">4</span>, [t2, t3, t1])\n    indices <span class=\"pl-k\">=</span> tf.reshape(t, [(height<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)<span class=\"pl-k\">*</span>(width<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)<span class=\"pl-k\">*</span>channels<span class=\"pl-k\">*</span>batch_size, <span class=\"pl-c1\">4</span>])\n\n    x1 <span class=\"pl-k\">=</span> tf.transpose(bottom, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n    values <span class=\"pl-k\">=</span> tf.reshape(x1, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n\n    delta <span class=\"pl-k\">=</span> tf.SparseTensor(indices, values, tf.to_int64(top_shape))\n    <span class=\"pl-k\">return</span> tf.sparse_tensor_to_dense(tf.sparse_reorder(delta))</pre></div>", "body_text": "Hi, I implemented the batch version (i.e. batch_size >= 1) of @fabianbormann 's unpool layer and it's been working well for me:\n  def unravel_argmax(argmax, shape):\n    output_list = [argmax // (shape[2]*shape[3]),\n                   argmax % (shape[2]*shape[3]) // shape[3]]\n    return tf.pack(output_list)\n\n  def unpool_layer2x2_batch(bottom, argmax):\n    bottom_shape = tf.shape(bottom)\n    top_shape = [bottom_shape[0], bottom_shape[1]*2, bottom_shape[2]*2, bottom_shape[3]]\n\n    batch_size = top_shape[0]\n    height = top_shape[1]\n    width = top_shape[2]\n    channels = top_shape[3]\n\n    argmax_shape = tf.to_int64([batch_size, height, width, channels])\n    argmax = unravel_argmax(argmax, argmax_shape)\n\n    t1 = tf.to_int64(tf.range(channels))\n    t1 = tf.tile(t1, [batch_size*(width//2)*(height//2)])\n    t1 = tf.reshape(t1, [-1, channels])\n    t1 = tf.transpose(t1, perm=[1, 0])\n    t1 = tf.reshape(t1, [channels, batch_size, height//2, width//2, 1])\n    t1 = tf.transpose(t1, perm=[1, 0, 2, 3, 4])\n\n    t2 = tf.to_int64(tf.range(batch_size))\n    t2 = tf.tile(t2, [channels*(width//2)*(height//2)])\n    t2 = tf.reshape(t2, [-1, batch_size])\n    t2 = tf.transpose(t2, perm=[1, 0])\n    t2 = tf.reshape(t2, [batch_size, channels, height//2, width//2, 1])\n\n    t3 = tf.transpose(argmax, perm=[1, 4, 2, 3, 0])\n\n    t = tf.concat(4, [t2, t3, t1])\n    indices = tf.reshape(t, [(height//2)*(width//2)*channels*batch_size, 4])\n\n    x1 = tf.transpose(bottom, perm=[0, 3, 1, 2])\n    values = tf.reshape(x1, [-1])\n\n    delta = tf.SparseTensor(indices, values, tf.to_int64(top_shape))\n    return tf.sparse_tensor_to_dense(tf.sparse_reorder(delta))", "body": "Hi, I implemented the batch version (i.e. batch_size >= 1) of @fabianbormann 's unpool layer and it's been working well for me:\n\n``` python\n\n  def unravel_argmax(argmax, shape):\n    output_list = [argmax // (shape[2]*shape[3]),\n                   argmax % (shape[2]*shape[3]) // shape[3]]\n    return tf.pack(output_list)\n\n  def unpool_layer2x2_batch(bottom, argmax):\n    bottom_shape = tf.shape(bottom)\n    top_shape = [bottom_shape[0], bottom_shape[1]*2, bottom_shape[2]*2, bottom_shape[3]]\n\n    batch_size = top_shape[0]\n    height = top_shape[1]\n    width = top_shape[2]\n    channels = top_shape[3]\n\n    argmax_shape = tf.to_int64([batch_size, height, width, channels])\n    argmax = unravel_argmax(argmax, argmax_shape)\n\n    t1 = tf.to_int64(tf.range(channels))\n    t1 = tf.tile(t1, [batch_size*(width//2)*(height//2)])\n    t1 = tf.reshape(t1, [-1, channels])\n    t1 = tf.transpose(t1, perm=[1, 0])\n    t1 = tf.reshape(t1, [channels, batch_size, height//2, width//2, 1])\n    t1 = tf.transpose(t1, perm=[1, 0, 2, 3, 4])\n\n    t2 = tf.to_int64(tf.range(batch_size))\n    t2 = tf.tile(t2, [channels*(width//2)*(height//2)])\n    t2 = tf.reshape(t2, [-1, batch_size])\n    t2 = tf.transpose(t2, perm=[1, 0])\n    t2 = tf.reshape(t2, [batch_size, channels, height//2, width//2, 1])\n\n    t3 = tf.transpose(argmax, perm=[1, 4, 2, 3, 0])\n\n    t = tf.concat(4, [t2, t3, t1])\n    indices = tf.reshape(t, [(height//2)*(width//2)*channels*batch_size, 4])\n\n    x1 = tf.transpose(bottom, perm=[0, 3, 1, 2])\n    values = tf.reshape(x1, [-1])\n\n    delta = tf.SparseTensor(indices, values, tf.to_int64(top_shape))\n    return tf.sparse_tensor_to_dense(tf.sparse_reorder(delta))\n```\n"}