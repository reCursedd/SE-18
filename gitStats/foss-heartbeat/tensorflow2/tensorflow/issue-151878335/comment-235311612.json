{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235311612", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-235311612", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 235311612, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTMxMTYxMg==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-26T15:48:35Z", "updated_at": "2016-07-26T15:48:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8906225\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/syed-ahmed\">@syed-ahmed</a> It's not an actual error unless you run out of memory.  The issue is that if the gradient takes the original input tensor rather than the shape, the original input must be stored for the remainder of the forward pass and the backward pass up to that point.  If only the shape is needed, that's a long time to hold onto otherwise unneeded memory.</p>", "body_text": "@syed-ahmed It's not an actual error unless you run out of memory.  The issue is that if the gradient takes the original input tensor rather than the shape, the original input must be stored for the remainder of the forward pass and the backward pass up to that point.  If only the shape is needed, that's a long time to hold onto otherwise unneeded memory.", "body": "@syed-ahmed It's not an actual error unless you run out of memory.  The issue is that if the gradient takes the original input tensor rather than the shape, the original input must be stored for the remainder of the forward pass and the backward pass up to that point.  If only the shape is needed, that's a long time to hold onto otherwise unneeded memory.\n"}