{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273784413", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-273784413", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 273784413, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Mzc4NDQxMw==", "user": {"login": "Pepslee", "id": 13853798, "node_id": "MDQ6VXNlcjEzODUzNzk4", "avatar_url": "https://avatars1.githubusercontent.com/u/13853798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pepslee", "html_url": "https://github.com/Pepslee", "followers_url": "https://api.github.com/users/Pepslee/followers", "following_url": "https://api.github.com/users/Pepslee/following{/other_user}", "gists_url": "https://api.github.com/users/Pepslee/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pepslee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pepslee/subscriptions", "organizations_url": "https://api.github.com/users/Pepslee/orgs", "repos_url": "https://api.github.com/users/Pepslee/repos", "events_url": "https://api.github.com/users/Pepslee/events{/privacy}", "received_events_url": "https://api.github.com/users/Pepslee/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-19T14:06:17Z", "updated_at": "2017-01-19T14:17:07Z", "author_association": "NONE", "body_html": "<p>much faster</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">unpool_layer2x2_batch</span>(<span class=\"pl-smi\">bottom</span>, <span class=\"pl-smi\">argmax</span>):\n    bottom_shape <span class=\"pl-k\">=</span> tf.shape(bottom)\n    top_shape <span class=\"pl-k\">=</span> [bottom_shape[<span class=\"pl-c1\">0</span>], bottom_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, bottom_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, bottom_shape[<span class=\"pl-c1\">3</span>]]\n\n    batch_size <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">0</span>]\n    height <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">1</span>]\n    width <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">2</span>]\n    channels <span class=\"pl-k\">=</span> top_shape[<span class=\"pl-c1\">3</span>]\n\n    argmax_shape <span class=\"pl-k\">=</span> tf.to_int64([batch_size, height, width, channels])\n    argmax <span class=\"pl-k\">=</span> unravel_argmax(argmax, argmax_shape)\n\n    t1 <span class=\"pl-k\">=</span> tf.to_int64(tf.range(channels))\n    t1 <span class=\"pl-k\">=</span> tf.tile(t1, [batch_size <span class=\"pl-k\">*</span> (width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">*</span> (height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>)])\n    t1 <span class=\"pl-k\">=</span> tf.reshape(t1, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, channels])\n    t1 <span class=\"pl-k\">=</span> tf.transpose(t1, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>])\n    t1 <span class=\"pl-k\">=</span> tf.reshape(t1, [channels, batch_size, height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>, width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\n    t1 <span class=\"pl-k\">=</span> tf.transpose(t1, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n\n    t2 <span class=\"pl-k\">=</span> tf.to_int64(tf.range(batch_size))\n    t2 <span class=\"pl-k\">=</span> tf.tile(t2, [channels <span class=\"pl-k\">*</span> (width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">*</span> (height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>)])\n    t2 <span class=\"pl-k\">=</span> tf.reshape(t2, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, batch_size])\n    t2 <span class=\"pl-k\">=</span> tf.transpose(t2, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>])\n    t2 <span class=\"pl-k\">=</span> tf.reshape(t2, [batch_size, channels, height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>, width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\n\n    t3 <span class=\"pl-k\">=</span> tf.transpose(argmax, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">0</span>])\n\n    t <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">4</span>, [t2, t3, t1])\n    indices <span class=\"pl-k\">=</span> tf.reshape(t, [(height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">*</span> (width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">*</span> channels <span class=\"pl-k\">*</span> batch_size, <span class=\"pl-c1\">4</span>])\n\n    x1 <span class=\"pl-k\">=</span> tf.transpose(bottom, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n    values <span class=\"pl-k\">=</span> tf.reshape(x1, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n    <span class=\"pl-k\">return</span> tf.scatter_nd(indices, values, tf.to_int64(top_shape))</pre></div>", "body_text": "much faster\ndef unpool_layer2x2_batch(bottom, argmax):\n    bottom_shape = tf.shape(bottom)\n    top_shape = [bottom_shape[0], bottom_shape[1] * 2, bottom_shape[2] * 2, bottom_shape[3]]\n\n    batch_size = top_shape[0]\n    height = top_shape[1]\n    width = top_shape[2]\n    channels = top_shape[3]\n\n    argmax_shape = tf.to_int64([batch_size, height, width, channels])\n    argmax = unravel_argmax(argmax, argmax_shape)\n\n    t1 = tf.to_int64(tf.range(channels))\n    t1 = tf.tile(t1, [batch_size * (width // 2) * (height // 2)])\n    t1 = tf.reshape(t1, [-1, channels])\n    t1 = tf.transpose(t1, perm=[1, 0])\n    t1 = tf.reshape(t1, [channels, batch_size, height // 2, width // 2, 1])\n    t1 = tf.transpose(t1, perm=[1, 0, 2, 3, 4])\n\n    t2 = tf.to_int64(tf.range(batch_size))\n    t2 = tf.tile(t2, [channels * (width // 2) * (height // 2)])\n    t2 = tf.reshape(t2, [-1, batch_size])\n    t2 = tf.transpose(t2, perm=[1, 0])\n    t2 = tf.reshape(t2, [batch_size, channels, height // 2, width // 2, 1])\n\n    t3 = tf.transpose(argmax, perm=[1, 4, 2, 3, 0])\n\n    t = tf.concat(4, [t2, t3, t1])\n    indices = tf.reshape(t, [(height // 2) * (width // 2) * channels * batch_size, 4])\n\n    x1 = tf.transpose(bottom, perm=[0, 3, 1, 2])\n    values = tf.reshape(x1, [-1])\n    return tf.scatter_nd(indices, values, tf.to_int64(top_shape))", "body": "much faster\r\n\r\n``` python\r\ndef unpool_layer2x2_batch(bottom, argmax):\r\n    bottom_shape = tf.shape(bottom)\r\n    top_shape = [bottom_shape[0], bottom_shape[1] * 2, bottom_shape[2] * 2, bottom_shape[3]]\r\n\r\n    batch_size = top_shape[0]\r\n    height = top_shape[1]\r\n    width = top_shape[2]\r\n    channels = top_shape[3]\r\n\r\n    argmax_shape = tf.to_int64([batch_size, height, width, channels])\r\n    argmax = unravel_argmax(argmax, argmax_shape)\r\n\r\n    t1 = tf.to_int64(tf.range(channels))\r\n    t1 = tf.tile(t1, [batch_size * (width // 2) * (height // 2)])\r\n    t1 = tf.reshape(t1, [-1, channels])\r\n    t1 = tf.transpose(t1, perm=[1, 0])\r\n    t1 = tf.reshape(t1, [channels, batch_size, height // 2, width // 2, 1])\r\n    t1 = tf.transpose(t1, perm=[1, 0, 2, 3, 4])\r\n\r\n    t2 = tf.to_int64(tf.range(batch_size))\r\n    t2 = tf.tile(t2, [channels * (width // 2) * (height // 2)])\r\n    t2 = tf.reshape(t2, [-1, batch_size])\r\n    t2 = tf.transpose(t2, perm=[1, 0])\r\n    t2 = tf.reshape(t2, [batch_size, channels, height // 2, width // 2, 1])\r\n\r\n    t3 = tf.transpose(argmax, perm=[1, 4, 2, 3, 0])\r\n\r\n    t = tf.concat(4, [t2, t3, t1])\r\n    indices = tf.reshape(t, [(height // 2) * (width // 2) * channels * batch_size, 4])\r\n\r\n    x1 = tf.transpose(bottom, perm=[0, 3, 1, 2])\r\n    values = tf.reshape(x1, [-1])\r\n    return tf.scatter_nd(indices, values, tf.to_int64(top_shape))\r\n```"}