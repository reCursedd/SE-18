{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/291238088", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-291238088", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 291238088, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MTIzODA4OA==", "user": {"login": "Enet4", "id": 4738426, "node_id": "MDQ6VXNlcjQ3Mzg0MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/4738426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Enet4", "html_url": "https://github.com/Enet4", "followers_url": "https://api.github.com/users/Enet4/followers", "following_url": "https://api.github.com/users/Enet4/following{/other_user}", "gists_url": "https://api.github.com/users/Enet4/gists{/gist_id}", "starred_url": "https://api.github.com/users/Enet4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Enet4/subscriptions", "organizations_url": "https://api.github.com/users/Enet4/orgs", "repos_url": "https://api.github.com/users/Enet4/repos", "events_url": "https://api.github.com/users/Enet4/events{/privacy}", "received_events_url": "https://api.github.com/users/Enet4/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-03T18:51:53Z", "updated_at": "2017-04-03T18:51:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I took a try at adapting the unpooling function to support a partially defined input shape (without the batch size), and here it is:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">unpool</span>(<span class=\"pl-smi\">updates</span>, <span class=\"pl-smi\">mask</span>, <span class=\"pl-smi\">ksize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>unpool<span class=\"pl-pds\">\"</span></span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(ksize, <span class=\"pl-c1\">int</span>):\n        ksize <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>, ksize, ksize, <span class=\"pl-c1\">1</span>]\n    input_shape <span class=\"pl-k\">=</span> updates.get_shape().as_list()\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  calculation new shape</span>\n    output_shape <span class=\"pl-k\">=</span> [input_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">1</span>], input_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">2</span>], input_shape[<span class=\"pl-c1\">3</span>]]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> calculation indices for batch, height, width and feature maps</span>\n    one_like_mask <span class=\"pl-k\">=</span> tf.ones_like(mask)\n    bsize <span class=\"pl-k\">=</span> tf.to_int64(tf.shape(updates)[<span class=\"pl-c1\">0</span>])\n    batch_range <span class=\"pl-k\">=</span> tf.reshape(tf.range(bsize, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64),\n                             <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>])\n    b <span class=\"pl-k\">=</span> one_like_mask <span class=\"pl-k\">*</span> batch_range\n    y <span class=\"pl-k\">=</span> mask <span class=\"pl-k\">//</span> (output_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> output_shape[<span class=\"pl-c1\">2</span>])\n    x <span class=\"pl-k\">=</span> mask <span class=\"pl-k\">%</span> (output_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> output_shape[<span class=\"pl-c1\">2</span>]) <span class=\"pl-k\">//</span> output_shape[<span class=\"pl-c1\">2</span>]\n    feature_range <span class=\"pl-k\">=</span> tf.range(output_shape[<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)\n    f <span class=\"pl-k\">=</span> one_like_mask <span class=\"pl-k\">*</span> feature_range\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> transpose indices &amp; reshape update values to one dimension</span>\n    updates_size <span class=\"pl-k\">=</span> tf.size(updates)\n    indices <span class=\"pl-k\">=</span> tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [<span class=\"pl-c1\">4</span>, updates_size]))\n    values <span class=\"pl-k\">=</span> tf.reshape(updates, [updates_size])\n    ret <span class=\"pl-k\">=</span> tf.scatter_nd(indices, values, tf.concat(\n        [[bsize], tf.to_int64(output_shape)], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>))\n    <span class=\"pl-k\">return</span> ret</pre></div>\n<p>In the batch range, I simply took advantage of the <code>-1</code> dimension. Then I fetched the batch size and built the final output shape dynamically (as a Tensor). Admittedly, I am not sure if I employed the fastest and most elegant operations, but it appears to be working on this side.</p>", "body_text": "I took a try at adapting the unpooling function to support a partially defined input shape (without the batch size), and here it is:\ndef unpool(updates, mask, ksize=2, name=\"unpool\"):\n    if isinstance(ksize, int):\n        ksize = [1, ksize, ksize, 1]\n    input_shape = updates.get_shape().as_list()\n    #  calculation new shape\n    output_shape = [input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]]\n    # calculation indices for batch, height, width and feature maps\n    one_like_mask = tf.ones_like(mask)\n    bsize = tf.to_int64(tf.shape(updates)[0])\n    batch_range = tf.reshape(tf.range(bsize, dtype=tf.int64),\n                             shape=[-1, 1, 1, 1])\n    b = one_like_mask * batch_range\n    y = mask // (output_shape[1] * output_shape[2])\n    x = mask % (output_shape[1] * output_shape[2]) // output_shape[2]\n    feature_range = tf.range(output_shape[2], dtype=tf.int64)\n    f = one_like_mask * feature_range\n    # transpose indices & reshape update values to one dimension\n    updates_size = tf.size(updates)\n    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n    values = tf.reshape(updates, [updates_size])\n    ret = tf.scatter_nd(indices, values, tf.concat(\n        [[bsize], tf.to_int64(output_shape)], axis=0))\n    return ret\nIn the batch range, I simply took advantage of the -1 dimension. Then I fetched the batch size and built the final output shape dynamically (as a Tensor). Admittedly, I am not sure if I employed the fastest and most elegant operations, but it appears to be working on this side.", "body": "I took a try at adapting the unpooling function to support a partially defined input shape (without the batch size), and here it is:\r\n\r\n```python\r\ndef unpool(updates, mask, ksize=2, name=\"unpool\"):\r\n    if isinstance(ksize, int):\r\n        ksize = [1, ksize, ksize, 1]\r\n    input_shape = updates.get_shape().as_list()\r\n    #  calculation new shape\r\n    output_shape = [input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]]\r\n    # calculation indices for batch, height, width and feature maps\r\n    one_like_mask = tf.ones_like(mask)\r\n    bsize = tf.to_int64(tf.shape(updates)[0])\r\n    batch_range = tf.reshape(tf.range(bsize, dtype=tf.int64),\r\n                             shape=[-1, 1, 1, 1])\r\n    b = one_like_mask * batch_range\r\n    y = mask // (output_shape[1] * output_shape[2])\r\n    x = mask % (output_shape[1] * output_shape[2]) // output_shape[2]\r\n    feature_range = tf.range(output_shape[2], dtype=tf.int64)\r\n    f = one_like_mask * feature_range\r\n    # transpose indices & reshape update values to one dimension\r\n    updates_size = tf.size(updates)\r\n    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\r\n    values = tf.reshape(updates, [updates_size])\r\n    ret = tf.scatter_nd(indices, values, tf.concat(\r\n        [[bsize], tf.to_int64(output_shape)], axis=0))\r\n    return ret\r\n```\r\n\r\nIn the batch range, I simply took advantage of the `-1` dimension. Then I fetched the batch size and built the final output shape dynamically (as a Tensor). Admittedly, I am not sure if I employed the fastest and most elegant operations, but it appears to be working on this side."}