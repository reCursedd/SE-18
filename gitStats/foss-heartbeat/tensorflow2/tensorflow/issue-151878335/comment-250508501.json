{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/250508501", "html_url": "https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-250508501", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2169", "id": 250508501, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDUwODUwMQ==", "user": {"login": "fabianbormann", "id": 1525818, "node_id": "MDQ6VXNlcjE1MjU4MTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1525818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabianbormann", "html_url": "https://github.com/fabianbormann", "followers_url": "https://api.github.com/users/fabianbormann/followers", "following_url": "https://api.github.com/users/fabianbormann/following{/other_user}", "gists_url": "https://api.github.com/users/fabianbormann/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabianbormann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabianbormann/subscriptions", "organizations_url": "https://api.github.com/users/fabianbormann/orgs", "repos_url": "https://api.github.com/users/fabianbormann/repos", "events_url": "https://api.github.com/users/fabianbormann/events{/privacy}", "received_events_url": "https://api.github.com/users/fabianbormann/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-29T15:55:26Z", "updated_at": "2016-09-29T16:02:35Z", "author_association": "NONE", "body_html": "<p>I also try to <a href=\"https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation\">implement the DeconvNet</a> described in <a href=\"http://arxiv.org/pdf/1505.04366v1.pdf\" rel=\"nofollow\">Learning Deconvolution Network for Semantic Segmentation</a> and I'm very interested in a native method like <code>tf.max_unpool_with_argmax</code> too, but for now I want to share my python tf implementation (<a href=\"https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation/blob/master/tests/UnpoolLayerTest.ipynb\">example</a>):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">unravel_argmax</span>(<span class=\"pl-smi\">argmax</span>, <span class=\"pl-smi\">shape</span>):\n    output_list <span class=\"pl-k\">=</span> []\n    output_list.append(argmax <span class=\"pl-k\">//</span> (shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> shape[<span class=\"pl-c1\">3</span>]))\n    output_list.append(argmax <span class=\"pl-k\">%</span> (shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> shape[<span class=\"pl-c1\">3</span>]) <span class=\"pl-k\">//</span> shape[<span class=\"pl-c1\">3</span>])\n    <span class=\"pl-k\">return</span> tf.pack(output_list)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">unpool_layer2x2</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">argmax</span>):\n    x_shape <span class=\"pl-k\">=</span> tf.shape(x)\n    output <span class=\"pl-k\">=</span> tf.zeros([x_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, x_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, x_shape[<span class=\"pl-c1\">3</span>]])\n\n    height <span class=\"pl-k\">=</span> tf.shape(output)[<span class=\"pl-c1\">0</span>]\n    width <span class=\"pl-k\">=</span> tf.shape(output)[<span class=\"pl-c1\">1</span>]\n    channels <span class=\"pl-k\">=</span> tf.shape(output)[<span class=\"pl-c1\">2</span>]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> build the indices for a SparseTensor addition like http://stackoverflow.com/a/34686952/3524844</span>\n    t1 <span class=\"pl-k\">=</span> tf.to_int64(tf.range(channels))\n    t1 <span class=\"pl-k\">=</span> tf.tile(t1, [(width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">*</span> (height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>)])\n    t1 <span class=\"pl-k\">=</span> tf.reshape(t1, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, channels])\n    t1 <span class=\"pl-k\">=</span> tf.transpose(t1, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>])\n    t1 <span class=\"pl-k\">=</span> tf.reshape(t1, [channels, height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>, width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\n\n    t2 <span class=\"pl-k\">=</span> tf.squeeze(argmax)\n    t2 <span class=\"pl-k\">=</span> tf.pack((t2[<span class=\"pl-c1\">0</span>], t2[<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    t2 <span class=\"pl-k\">=</span> tf.transpose(t2, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>])\n\n    t <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">3</span>, [t2, t1])\n    indices <span class=\"pl-k\">=</span> tf.reshape(t, [(height <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">*</span> (width <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">*</span> channels, <span class=\"pl-c1\">3</span>])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get the values for max_unpooling (used in addition with argmax location)</span>\n    x1 <span class=\"pl-k\">=</span> tf.squeeze(x)\n    x1 <span class=\"pl-k\">=</span> tf.reshape(x1, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, channels])\n    x1 <span class=\"pl-k\">=</span> tf.transpose(x1, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>])\n    values <span class=\"pl-k\">=</span> tf.reshape(x1, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> perform addition</span>\n    delta <span class=\"pl-k\">=</span> tf.SparseTensor(indices, values, tf.to_int64(tf.shape(output)))\n    <span class=\"pl-k\">return</span> tf.expand_dims(tf.sparse_tensor_to_dense(tf.sparse_reorder(delta)), <span class=\"pl-c1\">0</span>)</pre></div>\n<p>of an unpooling using the unraveled argmax of  <code>tf.nn.max_pool_with_argmax</code> for everybody searching for a similar method -&gt; replace all loops with tensor transformations was a little bit tricky and maybe there is a better (more readable) way - first I tried to use nested tf.while_loop but this was very slow. My implementation assumes a batch_size == 1 but for other use cases it could be simply rewrite.</p>", "body_text": "I also try to implement the DeconvNet described in Learning Deconvolution Network for Semantic Segmentation and I'm very interested in a native method like tf.max_unpool_with_argmax too, but for now I want to share my python tf implementation (example):\ndef unravel_argmax(argmax, shape):\n    output_list = []\n    output_list.append(argmax // (shape[2] * shape[3]))\n    output_list.append(argmax % (shape[2] * shape[3]) // shape[3])\n    return tf.pack(output_list)\n\ndef unpool_layer2x2(x, argmax):\n    x_shape = tf.shape(x)\n    output = tf.zeros([x_shape[1] * 2, x_shape[2] * 2, x_shape[3]])\n\n    height = tf.shape(output)[0]\n    width = tf.shape(output)[1]\n    channels = tf.shape(output)[2]\n    # build the indices for a SparseTensor addition like http://stackoverflow.com/a/34686952/3524844\n    t1 = tf.to_int64(tf.range(channels))\n    t1 = tf.tile(t1, [(width // 2) * (height // 2)])\n    t1 = tf.reshape(t1, [-1, channels])\n    t1 = tf.transpose(t1, perm=[1, 0])\n    t1 = tf.reshape(t1, [channels, height // 2, width // 2, 1])\n\n    t2 = tf.squeeze(argmax)\n    t2 = tf.pack((t2[0], t2[1]), axis=0)\n    t2 = tf.transpose(t2, perm=[3, 1, 2, 0])\n\n    t = tf.concat(3, [t2, t1])\n    indices = tf.reshape(t, [(height // 2) * (width // 2) * channels, 3])\n    # Get the values for max_unpooling (used in addition with argmax location)\n    x1 = tf.squeeze(x)\n    x1 = tf.reshape(x1, [-1, channels])\n    x1 = tf.transpose(x1, perm=[1, 0])\n    values = tf.reshape(x1, [-1])\n    # perform addition\n    delta = tf.SparseTensor(indices, values, tf.to_int64(tf.shape(output)))\n    return tf.expand_dims(tf.sparse_tensor_to_dense(tf.sparse_reorder(delta)), 0)\nof an unpooling using the unraveled argmax of  tf.nn.max_pool_with_argmax for everybody searching for a similar method -> replace all loops with tensor transformations was a little bit tricky and maybe there is a better (more readable) way - first I tried to use nested tf.while_loop but this was very slow. My implementation assumes a batch_size == 1 but for other use cases it could be simply rewrite.", "body": "I also try to [implement the DeconvNet](https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation) described in [Learning Deconvolution Network for Semantic Segmentation](http://arxiv.org/pdf/1505.04366v1.pdf) and I'm very interested in a native method like `tf.max_unpool_with_argmax` too, but for now I want to share my python tf implementation ([example](https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation/blob/master/tests/UnpoolLayerTest.ipynb)):\n\n``` python\ndef unravel_argmax(argmax, shape):\n    output_list = []\n    output_list.append(argmax // (shape[2] * shape[3]))\n    output_list.append(argmax % (shape[2] * shape[3]) // shape[3])\n    return tf.pack(output_list)\n\ndef unpool_layer2x2(x, argmax):\n    x_shape = tf.shape(x)\n    output = tf.zeros([x_shape[1] * 2, x_shape[2] * 2, x_shape[3]])\n\n    height = tf.shape(output)[0]\n    width = tf.shape(output)[1]\n    channels = tf.shape(output)[2]\n    # build the indices for a SparseTensor addition like http://stackoverflow.com/a/34686952/3524844\n    t1 = tf.to_int64(tf.range(channels))\n    t1 = tf.tile(t1, [(width // 2) * (height // 2)])\n    t1 = tf.reshape(t1, [-1, channels])\n    t1 = tf.transpose(t1, perm=[1, 0])\n    t1 = tf.reshape(t1, [channels, height // 2, width // 2, 1])\n\n    t2 = tf.squeeze(argmax)\n    t2 = tf.pack((t2[0], t2[1]), axis=0)\n    t2 = tf.transpose(t2, perm=[3, 1, 2, 0])\n\n    t = tf.concat(3, [t2, t1])\n    indices = tf.reshape(t, [(height // 2) * (width // 2) * channels, 3])\n    # Get the values for max_unpooling (used in addition with argmax location)\n    x1 = tf.squeeze(x)\n    x1 = tf.reshape(x1, [-1, channels])\n    x1 = tf.transpose(x1, perm=[1, 0])\n    values = tf.reshape(x1, [-1])\n    # perform addition\n    delta = tf.SparseTensor(indices, values, tf.to_int64(tf.shape(output)))\n    return tf.expand_dims(tf.sparse_tensor_to_dense(tf.sparse_reorder(delta)), 0)\n```\n\nof an unpooling using the unraveled argmax of  `tf.nn.max_pool_with_argmax` for everybody searching for a similar method -> replace all loops with tensor transformations was a little bit tricky and maybe there is a better (more readable) way - first I tried to use nested tf.while_loop but this was very slow. My implementation assumes a batch_size == 1 but for other use cases it could be simply rewrite.\n"}