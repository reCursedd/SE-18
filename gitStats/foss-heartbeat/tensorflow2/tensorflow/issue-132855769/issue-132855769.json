{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1049", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1049/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1049/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1049/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1049", "id": 132855769, "node_id": "MDU6SXNzdWUxMzI4NTU3Njk=", "number": 1049, "title": "How to set the max_grad_norm with AdamOptimizer?", "user": {"login": "endeavorui", "id": 17016348, "node_id": "MDQ6VXNlcjE3MDE2MzQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/17016348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/endeavorui", "html_url": "https://github.com/endeavorui", "followers_url": "https://api.github.com/users/endeavorui/followers", "following_url": "https://api.github.com/users/endeavorui/following{/other_user}", "gists_url": "https://api.github.com/users/endeavorui/gists{/gist_id}", "starred_url": "https://api.github.com/users/endeavorui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/endeavorui/subscriptions", "organizations_url": "https://api.github.com/users/endeavorui/orgs", "repos_url": "https://api.github.com/users/endeavorui/repos", "events_url": "https://api.github.com/users/endeavorui/events{/privacy}", "received_events_url": "https://api.github.com/users/endeavorui/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-02-11T00:12:10Z", "updated_at": "2016-02-11T18:33:51Z", "closed_at": "2016-02-11T18:33:51Z", "author_association": "NONE", "body_html": "<p>New to the TensorFlow.</p>\n<p>I am working closed with RNN. To avoid overfitting, I am trying to control the max_grad_norm when the gradient passes through.</p>\n<p>Here is the code I worked with.</p>\n<pre><code>    # Define optimizer with norm limitation\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred_y, y)) # Softmax loss\ntvars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam Optimizer\noptimizer.apply_gradients(zip(grads, tvars))\n</code></pre>\n<p>I kept receiving the error that</p>\n<p>TypeError: Fetch argument &lt;tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50&gt; of &lt;tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50&gt; has invalid type &lt;class 'tensorflow.python.training.adam.AdamOptimizer'&gt;, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)</p>\n<p>All suggestions are more than welcome.<br>\nThanks</p>", "body_text": "New to the TensorFlow.\nI am working closed with RNN. To avoid overfitting, I am trying to control the max_grad_norm when the gradient passes through.\nHere is the code I worked with.\n    # Define optimizer with norm limitation\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred_y, y)) # Softmax loss\ntvars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam Optimizer\noptimizer.apply_gradients(zip(grads, tvars))\n\nI kept receiving the error that\nTypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> of <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)\nAll suggestions are more than welcome.\nThanks", "body": "New to the TensorFlow.\n\nI am working closed with RNN. To avoid overfitting, I am trying to control the max_grad_norm when the gradient passes through.\n\nHere is the code I worked with.\n\n```\n    # Define optimizer with norm limitation\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred_y, y)) # Softmax loss\ntvars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam Optimizer\noptimizer.apply_gradients(zip(grads, tvars))\n```\n\nI kept receiving the error that\n\nTypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> of <tensorflow.python.training.adam.AdamOptimizer object at 0x7f0f02b1ce50> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)\n\nAll suggestions are more than welcome.\nThanks\n"}