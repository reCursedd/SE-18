{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/182999890", "html_url": "https://github.com/tensorflow/tensorflow/issues/1049#issuecomment-182999890", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1049", "id": 182999890, "node_id": "MDEyOklzc3VlQ29tbWVudDE4Mjk5OTg5MA==", "user": {"login": "endeavorui", "id": 17016348, "node_id": "MDQ6VXNlcjE3MDE2MzQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/17016348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/endeavorui", "html_url": "https://github.com/endeavorui", "followers_url": "https://api.github.com/users/endeavorui/followers", "following_url": "https://api.github.com/users/endeavorui/following{/other_user}", "gists_url": "https://api.github.com/users/endeavorui/gists{/gist_id}", "starred_url": "https://api.github.com/users/endeavorui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/endeavorui/subscriptions", "organizations_url": "https://api.github.com/users/endeavorui/orgs", "repos_url": "https://api.github.com/users/endeavorui/repos", "events_url": "https://api.github.com/users/endeavorui/events{/privacy}", "received_events_url": "https://api.github.com/users/endeavorui/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-11T18:32:04Z", "updated_at": "2016-02-11T18:32:37Z", "author_association": "NONE", "body_html": "<p>I have figured out by myself.<br>\nOnce we configure the optimizer, it is always better to assign the optimizer to a new op. Then in the session, we need to change the sess.run on the new op. So the code should be</p>\n<p><strong>train_op</strong> = optimizer.apply_gradients(zip(grads, tvars))</p>\n<p>And in the Session</p>\n<p>sess.run(<strong>train_op</strong>, feed_dict={x: batch_xs, y: batch_ys_one_hot,<br>\nistate_fw: np.zeros((train_batch_size, 2_n_hidden)),<br>\nistate_bw: np.zeros((train_batch_size, 2_n_hidden))})</p>\n<p>Thanks for your attentions. I learned a lot</p>", "body_text": "I have figured out by myself.\nOnce we configure the optimizer, it is always better to assign the optimizer to a new op. Then in the session, we need to change the sess.run on the new op. So the code should be\ntrain_op = optimizer.apply_gradients(zip(grads, tvars))\nAnd in the Session\nsess.run(train_op, feed_dict={x: batch_xs, y: batch_ys_one_hot,\nistate_fw: np.zeros((train_batch_size, 2_n_hidden)),\nistate_bw: np.zeros((train_batch_size, 2_n_hidden))})\nThanks for your attentions. I learned a lot", "body": "I have figured out by myself.\nOnce we configure the optimizer, it is always better to assign the optimizer to a new op. Then in the session, we need to change the sess.run on the new op. So the code should be\n\n**train_op** = optimizer.apply_gradients(zip(grads, tvars))\n\nAnd in the Session\n\nsess.run(**train_op**, feed_dict={x: batch_xs, y: batch_ys_one_hot, \n                           istate_fw: np.zeros((train_batch_size, 2_n_hidden)), \n                           istate_bw: np.zeros((train_batch_size, 2_n_hidden))})\n\nThanks for your attentions. I learned a lot\n"}