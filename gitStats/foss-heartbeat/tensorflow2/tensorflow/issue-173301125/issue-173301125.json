{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4047", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4047/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4047/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4047/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4047", "id": 173301125, "node_id": "MDU6SXNzdWUxNzMzMDExMjU=", "number": 4047, "title": "GPU usage is extremely unstable in distributed setup on aws", "user": {"login": "perhapszzy", "id": 7953637, "node_id": "MDQ6VXNlcjc5NTM2Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7953637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/perhapszzy", "html_url": "https://github.com/perhapszzy", "followers_url": "https://api.github.com/users/perhapszzy/followers", "following_url": "https://api.github.com/users/perhapszzy/following{/other_user}", "gists_url": "https://api.github.com/users/perhapszzy/gists{/gist_id}", "starred_url": "https://api.github.com/users/perhapszzy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/perhapszzy/subscriptions", "organizations_url": "https://api.github.com/users/perhapszzy/orgs", "repos_url": "https://api.github.com/users/perhapszzy/repos", "events_url": "https://api.github.com/users/perhapszzy/events{/privacy}", "received_events_url": "https://api.github.com/users/perhapszzy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 27, "created_at": "2016-08-25T20:26:03Z", "updated_at": "2016-10-18T07:36:40Z", "closed_at": "2016-10-18T07:36:40Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04</p>\n<p>Installed version of CUDA and cuDNN:  CUDA 7.5, cuDNN 4.0</p>\n<p>If installed from binary pip package, provide:<br>\nInstalled from the following pip package: (0.10.0rc)<br>\n<a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl</a></p>\n<h3>Steps to reproduce</h3>\n<p>I'm running distributed tf with GPU on 3 machines (the ps machine doesn't have GPU). The commands to start the cluster:<br>\nOn ps machine0:</p>\n<pre><code>python async.py \\\n--job_name='ps' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n</code></pre>\n<p>On g2.2xlarge machine 1:</p>\n<pre><code>python async.py \\\n--job_name='worker' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n</code></pre>\n<p>On g2.2xlarge machine 2:</p>\n<pre><code>python async.py \\\n--job_name='worker' \\\n--task_id=1 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n</code></pre>\n<p>The code:  <a href=\"https://github.com/tensorflow/tensorflow/files/437980/cifar10_async_dist_train.txt\">async.txt</a></p>\n<p>For the first few thousands of iteration, it worked perfectly fine, but the GPU usage drop to almost 0 after that. (actually it fluctuated a lot from 80 -&gt; 50 -&gt;0 -&gt;80)</p>\n<h3>What have you tried?</h3>\n<ol>\n<li>I tried to restart the works and it worked for a few rounds then the problem come over again.</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>The log after restart. The speed for the first few rounds is ~0.36 sec/batch and the GPU usage is ~90%. But the speed drop down to ~1 sec/batch quickly.</p>\n<pre><code>2016-08-25 20:04:08.950429: step 110 (global_step 8318), loss = 0.85 (354.3 examples/sec; 0.361 sec/batch)\n2016-08-25 20:04:12.356555: step 120 (global_step 8338), loss = 0.77 (381.3 examples/sec; 0.336 sec/batch)\n2016-08-25 20:04:15.832075: step 130 (global_step 8358), loss = 0.93 (369.8 examples/sec; 0.346 sec/batch)\n2016-08-25 20:04:19.269496: step 140 (global_step 8377), loss = 0.92 (345.1 examples/sec; 0.371 sec/batch)\n2016-08-25 20:04:22.709284: step 150 (global_step 8398), loss = 1.00 (383.3 examples/sec; 0.334 sec/batch)\n2016-08-25 20:04:26.071762: step 160 (global_step 8418), loss = 0.75 (464.1 examples/sec; 0.276 sec/batch)\n2016-08-25 20:04:29.580850: step 170 (global_step 8438), loss = 0.99 (359.5 examples/sec; 0.356 sec/batch)\n2016-08-25 20:04:33.029791: step 180 (global_step 8457), loss = 1.04 (357.4 examples/sec; 0.358 sec/batch)\n2016-08-25 20:04:36.491314: step 190 (global_step 8478), loss = 0.93 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:39.939553: step 200 (global_step 8498), loss = 1.08 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:43.318268: step 210 (global_step 8518), loss = 1.09 (363.6 examples/sec; 0.352 sec/batch)\n2016-08-25 20:04:46.716990: step 220 (global_step 8538), loss = 0.98 (411.3 examples/sec; 0.311 sec/batch)\n2016-08-25 20:04:50.069852: step 230 (global_step 8557), loss = 0.95 (433.2 examples/sec; 0.295 sec/batch)\n2016-08-25 20:04:53.607222: step 240 (global_step 8577), loss = 1.02 (364.5 examples/sec; 0.351 sec/batch)\n2016-08-25 20:04:57.162343: step 250 (global_step 8597), loss = 0.96 (357.5 examples/sec; 0.358 sec/batch)\n2016-08-25 20:05:00.765804: step 260 (global_step 8617), loss = 1.07 (329.9 examples/sec; 0.388 sec/batch)\n2016-08-25 20:05:04.435059: step 270 (global_step 8637), loss = 0.85 (332.6 examples/sec; 0.385 sec/batch)\n2016-08-25 20:05:08.196024: step 280 (global_step 8656), loss = 0.89 (306.1 examples/sec; 0.418 sec/batch)\n2016-08-25 20:05:12.412322: step 290 (global_step 8676), loss = 1.10 (268.2 examples/sec; 0.477 sec/batch)\n2016-08-25 20:05:21.793992: step 300 (global_step 8696), loss = 0.72 (119.5 examples/sec; 1.071 sec/batch)\n2016-08-25 20:05:32.548282: step 310 (global_step 8716), loss = 0.80 (119.6 examples/sec; 1.070 sec/batch)\n2016-08-25 20:05:43.400207: step 320 (global_step 8736), loss = 1.03 (116.0 examples/sec; 1.104 sec/batch)\n2016-08-25 20:05:54.547412: step 330 (global_step 8755), loss = 0.84 (114.6 examples/sec; 1.117 sec/batch)\n2016-08-25 20:06:05.457404: step 340 (global_step 8775), loss = 1.09 (119.4 examples/sec; 1.072 sec/batch)\n2016-08-25 20:06:16.434271: step 350 (global_step 8795), loss = 0.83 (115.2 examples/sec; 1.111 sec/batch)\n2016-08-25 20:06:27.296998: step 360 (global_step 8815), loss = 0.90 (116.7 examples/sec; 1.097 sec/batch)\n2016-08-25 20:06:38.130229: step 370 (global_step 8835), loss = 0.99 (119.2 examples/sec; 1.074 sec/batch)\n2016-08-25 20:06:48.918992: step 380 (global_step 8855), loss = 1.09 (115.5 examples/sec; 1.108 sec/batch)\n2016-08-25 20:07:00.132937: step 390 (global_step 8874), loss = 0.87 (119.0 examples/sec; 1.076 sec/batch)\n</code></pre>", "body_text": "Environment info\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN:  CUDA 7.5, cuDNN 4.0\nIf installed from binary pip package, provide:\nInstalled from the following pip package: (0.10.0rc)\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\nSteps to reproduce\nI'm running distributed tf with GPU on 3 machines (the ps machine doesn't have GPU). The commands to start the cluster:\nOn ps machine0:\npython async.py \\\n--job_name='ps' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n\nOn g2.2xlarge machine 1:\npython async.py \\\n--job_name='worker' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n\nOn g2.2xlarge machine 2:\npython async.py \\\n--job_name='worker' \\\n--task_id=1 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n\nThe code:  async.txt\nFor the first few thousands of iteration, it worked perfectly fine, but the GPU usage drop to almost 0 after that. (actually it fluctuated a lot from 80 -> 50 ->0 ->80)\nWhat have you tried?\n\nI tried to restart the works and it worked for a few rounds then the problem come over again.\n\nLogs or other output that would be helpful\nThe log after restart. The speed for the first few rounds is ~0.36 sec/batch and the GPU usage is ~90%. But the speed drop down to ~1 sec/batch quickly.\n2016-08-25 20:04:08.950429: step 110 (global_step 8318), loss = 0.85 (354.3 examples/sec; 0.361 sec/batch)\n2016-08-25 20:04:12.356555: step 120 (global_step 8338), loss = 0.77 (381.3 examples/sec; 0.336 sec/batch)\n2016-08-25 20:04:15.832075: step 130 (global_step 8358), loss = 0.93 (369.8 examples/sec; 0.346 sec/batch)\n2016-08-25 20:04:19.269496: step 140 (global_step 8377), loss = 0.92 (345.1 examples/sec; 0.371 sec/batch)\n2016-08-25 20:04:22.709284: step 150 (global_step 8398), loss = 1.00 (383.3 examples/sec; 0.334 sec/batch)\n2016-08-25 20:04:26.071762: step 160 (global_step 8418), loss = 0.75 (464.1 examples/sec; 0.276 sec/batch)\n2016-08-25 20:04:29.580850: step 170 (global_step 8438), loss = 0.99 (359.5 examples/sec; 0.356 sec/batch)\n2016-08-25 20:04:33.029791: step 180 (global_step 8457), loss = 1.04 (357.4 examples/sec; 0.358 sec/batch)\n2016-08-25 20:04:36.491314: step 190 (global_step 8478), loss = 0.93 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:39.939553: step 200 (global_step 8498), loss = 1.08 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:43.318268: step 210 (global_step 8518), loss = 1.09 (363.6 examples/sec; 0.352 sec/batch)\n2016-08-25 20:04:46.716990: step 220 (global_step 8538), loss = 0.98 (411.3 examples/sec; 0.311 sec/batch)\n2016-08-25 20:04:50.069852: step 230 (global_step 8557), loss = 0.95 (433.2 examples/sec; 0.295 sec/batch)\n2016-08-25 20:04:53.607222: step 240 (global_step 8577), loss = 1.02 (364.5 examples/sec; 0.351 sec/batch)\n2016-08-25 20:04:57.162343: step 250 (global_step 8597), loss = 0.96 (357.5 examples/sec; 0.358 sec/batch)\n2016-08-25 20:05:00.765804: step 260 (global_step 8617), loss = 1.07 (329.9 examples/sec; 0.388 sec/batch)\n2016-08-25 20:05:04.435059: step 270 (global_step 8637), loss = 0.85 (332.6 examples/sec; 0.385 sec/batch)\n2016-08-25 20:05:08.196024: step 280 (global_step 8656), loss = 0.89 (306.1 examples/sec; 0.418 sec/batch)\n2016-08-25 20:05:12.412322: step 290 (global_step 8676), loss = 1.10 (268.2 examples/sec; 0.477 sec/batch)\n2016-08-25 20:05:21.793992: step 300 (global_step 8696), loss = 0.72 (119.5 examples/sec; 1.071 sec/batch)\n2016-08-25 20:05:32.548282: step 310 (global_step 8716), loss = 0.80 (119.6 examples/sec; 1.070 sec/batch)\n2016-08-25 20:05:43.400207: step 320 (global_step 8736), loss = 1.03 (116.0 examples/sec; 1.104 sec/batch)\n2016-08-25 20:05:54.547412: step 330 (global_step 8755), loss = 0.84 (114.6 examples/sec; 1.117 sec/batch)\n2016-08-25 20:06:05.457404: step 340 (global_step 8775), loss = 1.09 (119.4 examples/sec; 1.072 sec/batch)\n2016-08-25 20:06:16.434271: step 350 (global_step 8795), loss = 0.83 (115.2 examples/sec; 1.111 sec/batch)\n2016-08-25 20:06:27.296998: step 360 (global_step 8815), loss = 0.90 (116.7 examples/sec; 1.097 sec/batch)\n2016-08-25 20:06:38.130229: step 370 (global_step 8835), loss = 0.99 (119.2 examples/sec; 1.074 sec/batch)\n2016-08-25 20:06:48.918992: step 380 (global_step 8855), loss = 1.09 (115.5 examples/sec; 1.108 sec/batch)\n2016-08-25 20:07:00.132937: step 390 (global_step 8874), loss = 0.87 (119.0 examples/sec; 1.076 sec/batch)", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN:  CUDA 7.5, cuDNN 4.0\n\nIf installed from binary pip package, provide:\nInstalled from the following pip package: (0.10.0rc)\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n### Steps to reproduce\n\nI'm running distributed tf with GPU on 3 machines (the ps machine doesn't have GPU). The commands to start the cluster:\nOn ps machine0:\n\n```\npython async.py \\\n--job_name='ps' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n```\n\nOn g2.2xlarge machine 1:\n\n```\npython async.py \\\n--job_name='worker' \\\n--task_id=0 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n```\n\nOn g2.2xlarge machine 2:\n\n```\npython async.py \\\n--job_name='worker' \\\n--task_id=1 \\\n--ps_hosts='machine0:2222' \\\n--worker_hosts='machine1:2222,machine2:2222'\n```\n\nThe code:  [async.txt](https://github.com/tensorflow/tensorflow/files/437980/cifar10_async_dist_train.txt)\n\nFor the first few thousands of iteration, it worked perfectly fine, but the GPU usage drop to almost 0 after that. (actually it fluctuated a lot from 80 -> 50 ->0 ->80)\n### What have you tried?\n1.  I tried to restart the works and it worked for a few rounds then the problem come over again.\n### Logs or other output that would be helpful\n\nThe log after restart. The speed for the first few rounds is ~0.36 sec/batch and the GPU usage is ~90%. But the speed drop down to ~1 sec/batch quickly.\n\n```\n2016-08-25 20:04:08.950429: step 110 (global_step 8318), loss = 0.85 (354.3 examples/sec; 0.361 sec/batch)\n2016-08-25 20:04:12.356555: step 120 (global_step 8338), loss = 0.77 (381.3 examples/sec; 0.336 sec/batch)\n2016-08-25 20:04:15.832075: step 130 (global_step 8358), loss = 0.93 (369.8 examples/sec; 0.346 sec/batch)\n2016-08-25 20:04:19.269496: step 140 (global_step 8377), loss = 0.92 (345.1 examples/sec; 0.371 sec/batch)\n2016-08-25 20:04:22.709284: step 150 (global_step 8398), loss = 1.00 (383.3 examples/sec; 0.334 sec/batch)\n2016-08-25 20:04:26.071762: step 160 (global_step 8418), loss = 0.75 (464.1 examples/sec; 0.276 sec/batch)\n2016-08-25 20:04:29.580850: step 170 (global_step 8438), loss = 0.99 (359.5 examples/sec; 0.356 sec/batch)\n2016-08-25 20:04:33.029791: step 180 (global_step 8457), loss = 1.04 (357.4 examples/sec; 0.358 sec/batch)\n2016-08-25 20:04:36.491314: step 190 (global_step 8478), loss = 0.93 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:39.939553: step 200 (global_step 8498), loss = 1.08 (358.7 examples/sec; 0.357 sec/batch)\n2016-08-25 20:04:43.318268: step 210 (global_step 8518), loss = 1.09 (363.6 examples/sec; 0.352 sec/batch)\n2016-08-25 20:04:46.716990: step 220 (global_step 8538), loss = 0.98 (411.3 examples/sec; 0.311 sec/batch)\n2016-08-25 20:04:50.069852: step 230 (global_step 8557), loss = 0.95 (433.2 examples/sec; 0.295 sec/batch)\n2016-08-25 20:04:53.607222: step 240 (global_step 8577), loss = 1.02 (364.5 examples/sec; 0.351 sec/batch)\n2016-08-25 20:04:57.162343: step 250 (global_step 8597), loss = 0.96 (357.5 examples/sec; 0.358 sec/batch)\n2016-08-25 20:05:00.765804: step 260 (global_step 8617), loss = 1.07 (329.9 examples/sec; 0.388 sec/batch)\n2016-08-25 20:05:04.435059: step 270 (global_step 8637), loss = 0.85 (332.6 examples/sec; 0.385 sec/batch)\n2016-08-25 20:05:08.196024: step 280 (global_step 8656), loss = 0.89 (306.1 examples/sec; 0.418 sec/batch)\n2016-08-25 20:05:12.412322: step 290 (global_step 8676), loss = 1.10 (268.2 examples/sec; 0.477 sec/batch)\n2016-08-25 20:05:21.793992: step 300 (global_step 8696), loss = 0.72 (119.5 examples/sec; 1.071 sec/batch)\n2016-08-25 20:05:32.548282: step 310 (global_step 8716), loss = 0.80 (119.6 examples/sec; 1.070 sec/batch)\n2016-08-25 20:05:43.400207: step 320 (global_step 8736), loss = 1.03 (116.0 examples/sec; 1.104 sec/batch)\n2016-08-25 20:05:54.547412: step 330 (global_step 8755), loss = 0.84 (114.6 examples/sec; 1.117 sec/batch)\n2016-08-25 20:06:05.457404: step 340 (global_step 8775), loss = 1.09 (119.4 examples/sec; 1.072 sec/batch)\n2016-08-25 20:06:16.434271: step 350 (global_step 8795), loss = 0.83 (115.2 examples/sec; 1.111 sec/batch)\n2016-08-25 20:06:27.296998: step 360 (global_step 8815), loss = 0.90 (116.7 examples/sec; 1.097 sec/batch)\n2016-08-25 20:06:38.130229: step 370 (global_step 8835), loss = 0.99 (119.2 examples/sec; 1.074 sec/batch)\n2016-08-25 20:06:48.918992: step 380 (global_step 8855), loss = 1.09 (115.5 examples/sec; 1.108 sec/batch)\n2016-08-25 20:07:00.132937: step 390 (global_step 8874), loss = 0.87 (119.0 examples/sec; 1.076 sec/batch)\n```\n"}