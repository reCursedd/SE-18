{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23922", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23922/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23922/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23922/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23922", "id": 383542732, "node_id": "MDU6SXNzdWUzODM1NDI3MzI=", "number": 23922, "title": "Tensorflow uses only one from eight gpus", "user": {"login": "Lxnus", "id": 37800960, "node_id": "MDQ6VXNlcjM3ODAwOTYw", "avatar_url": "https://avatars1.githubusercontent.com/u/37800960?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lxnus", "html_url": "https://github.com/Lxnus", "followers_url": "https://api.github.com/users/Lxnus/followers", "following_url": "https://api.github.com/users/Lxnus/following{/other_user}", "gists_url": "https://api.github.com/users/Lxnus/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lxnus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lxnus/subscriptions", "organizations_url": "https://api.github.com/users/Lxnus/orgs", "repos_url": "https://api.github.com/users/Lxnus/repos", "events_url": "https://api.github.com/users/Lxnus/events{/privacy}", "received_events_url": "https://api.github.com/users/Lxnus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-22T13:46:15Z", "updated_at": "2018-11-23T09:32:05Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hey,<br>\ni have written an neural chatbot with tensorflow. But my problem is, that TF only uses one GPU.<br>\nI am training on an NVIDIA DGX Station with 8x Tesla V100, so I want to use alle memory and cores.<br>\nWhere is my mistake in the code? Who could I solve my problem?</p>\n<p>I would be very thankful about an answer! :D</p>\n<p>My code:</p>\n<p>import time</p>\n<p>import data<br>\nimport click<br>\nimport numpy as np<br>\nimport tensorflow as tf<br>\nimport tensorlayer as tl</p>\n<p>from tqdm import tqdm<br>\nfrom sklearn.utils import shuffle<br>\nfrom tensorlayer.layers import DenseLayer, EmbeddingInputlayer, Seq2Seq, retrieve_seq_length_op2</p>\n<p>sessionConfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False)</p>\n<p>@click.command()<br>\n@click.option('-dc', '--data-corpus', default = 'twitter')<br>\n@click.option('-bs', '--batch-size', default = 32)<br>\n@click.option('-e', '--epochs', default = 100000)<br>\n@click.option('-lr', '--learning-rate', default = 0.001)<br>\n@click.option('-m', '--mode', is_flag = True)<br>\ndef train(data_corpus, batch_size, epochs, learning_rate, mode):</p>\n<pre><code>metadata, trainX, trainY, testX, testY, validX, validY = setup(data_corpus)\n\nsrcLength = len(trainX)\ntargetLength = len(trainY)\n\nassert srcLength == targetLength\n\nnStep = srcLength // batch_size\nsrcVocabSize = len(metadata['index2Word'])\nembeddingDim = 1024\n\nword2Index = metadata['word2Index']  \nindex2Word = metadata['index2Word']  \n\nunk_id = word2Index['unk']  \npad_id = word2Index['_']    \n\nstart_id = srcVocabSize  \nend_id = srcVocabSize + 1 \n\nword2Index.update({'start_id': start_id})\nword2Index.update({'end_id': end_id})\nindex2Word = index2Word + ['start_id', 'end_id']\n\nsrcVocabSize = tgt_vocab_size = srcVocabSize + 2\n\ntarget_seqs = tl.prepro.sequences_add_end_id([trainY[100]], end_id=end_id)[0]\ndecode_seqs = tl.prepro.sequences_add_start_id([trainY[100]], start_id=start_id, remove_last=False)[0]\ntarget_mask = tl.prepro.sequences_get_mask([target_seqs])[0]\nif not mode:\n    print(\"encode_seqs\", [index2Word[id] for id in trainX[100]])\n    print(\"target_seqs\", [index2Word[id] for id in target_seqs])\n    print(\"decode_seqs\", [index2Word[id] for id in decode_seqs])\n    print(\"target_mask\", target_mask)\n    print(len(target_seqs), len(decode_seqs), len(target_mask))\n\t\ntf.reset_default_graph()\n\nsession = tf.Session(config=sessionConfig)\n\nencode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\ndecode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\ntarget_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\ntarget_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \n    \noutputNetwork, _ = createModel(encode_seqs, decode_seqs, srcVocabSize, embeddingDim, isTrain=True, reuse=False)\noutputNetwork.print_params(False)\n    \nencode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\ndecode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n    \nnetwork, RNNnetwork = createModel(encode_seqs2, decode_seqs2, srcVocabSize, embeddingDim, isTrain=False, reuse=True)\ny = tf.nn.softmax(network.outputs)\n    \nloss = tl.cost.cross_entropy_seq_with_mask(logits=outputNetwork.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')\n    \ntrainOp = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\nsession.run(tf.global_variables_initializer())\n\ntl.files.load_and_assign_npz(sess=session, name='model.npz', network=network)\t\n\ndef inference(seed):\n    seed_id = [word2Index.get(w, unk_id) for w in seed.split(\" \")]\n    \n    state = session.run(RNNnetwork.final_state_encode, {encode_seqs2: [seed_id]})\n    o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[start_id]]})\n    w_id = tl.nlp.sample_top(o[0], top_k=3)\n    w = index2Word[w_id]\n    sentence = [w]\n    for _ in range(200): # 30 output length\n        o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[w_id]]})\n        w_id = tl.nlp.sample_top(o[0], top_k=2)\n        w = index2Word[w_id]\n        if w_id == end_id:\n            break\n        sentence = sentence + [w]\n    return sentence\n\nif mode:\n    while True:\n        input_seq = input('Input &gt;&gt; ')\n        sentence = inference(input_seq)\n        print(\"Output &gt;&gt;\", ' '.join(sentence))\nelse:\n    seeds = [\"happy birthday have a nice day\",\n             \"donald trump won last nights presidential debate according to snap online polls\"]\n    for epoch in range(epochs):\n        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n        total_loss, n_iter = 0, 0\n        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), total=nStep, desc='Epoch[{}/{}]'.format(epoch + 1, epochs), leave=False):\n            X = tl.prepro.pad_sequences(X)\n            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n\t\t\t\n            _, loss_iter = session.run([trainOp, loss], {encode_seqs: X, decode_seqs: _decode_seqs, target_seqs: _target_seqs, target_mask: _target_mask})\n            total_loss += loss_iter\n            n_iter += 1\n\n        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, epochs, total_loss / n_iter))\n        \n        for seed in seeds:\n            print(\"Input &gt;&gt;\", seed)\n            for _ in range(5):\n                sentence = inference(seed)\n                print(\"Output &gt;&gt;\", ' '.join(sentence))\n        \n        tl.files.save_npz(network.all_params, name='model.npz', sess=session)\n\nsession.close()\n</code></pre>\n<p>def createModel(encodeSeqs, decodeSeqs, vocabSize, embeddingDim, isTrain = True, reuse = False):<br>\nwith tf.variable_scope(\"model\", reuse = reuse):<br>\nwith tf.variable_scope(\"embedding\") as vs:<br>\nnetworkworkEncoder = EmbeddingInputlayer(inputs = encodeSeqs,<br>\nvocabulary_size = vocabSize,<br>\nembedding_size = embeddingDim,<br>\nname = 'sequenceEmbedding')<br>\nvs.reuse_variables()<br>\nnetworkworkDecoder = EmbeddingInputlayer(inputs = decodeSeqs,<br>\nvocabulary_size = vocabSize,<br>\nembedding_size = embeddingDim,<br>\nname = 'sequenceEmbedding')<br>\nnetworkworkRNN = Seq2Seq(networkworkEncoder, networkworkDecoder,<br>\ncell_fn = tf.nn.rnn_cell.LSTMCell,<br>\nn_hidden = embeddingDim,<br>\ninitializer = tf.random_uniform_initializer(-0.1, 0.1),<br>\nencode_sequence_length = retrieve_seq_length_op2(encodeSeqs),<br>\ndecode_sequence_length = retrieve_seq_length_op2(decodeSeqs),<br>\ninitial_state_encode = None,<br>\ndropout = (0.5 if isTrain else None),<br>\nn_layer = 3,<br>\nreturn_seq_2d = True,<br>\nname = 'seq2seq')<br>\nnetworkworkOut = DenseLayer(networkworkRNN, n_units = vocabSize, act = tf.identity, name = 'output')<br>\nreturn networkworkOut, networkworkRNN</p>\n<p>def setup(data_corpus):<br>\nmetaData, indexQ, indexA = data.load_data(Path=''.format(data_corpus))<br>\n(trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(indexQ, indexA)<br>\ntrainX = tl.prepro.remove_pad_sequences(trainX.tolist())<br>\ntrainY = tl.prepro.remove_pad_sequences(trainY.tolist())<br>\ntestX = tl.prepro.remove_pad_sequences(testX.tolist())<br>\ntestY = tl.prepro.remove_pad_sequences(testY.tolist())<br>\nvalidX = tl.prepro.remove_pad_sequences(validX.tolist())<br>\nvalidY = tl.prepro.remove_pad_sequences(validY.tolist())<br>\nreturn metaData, trainX, trainY, testX, testY, validX, validY</p>\n<p>def main():<br>\ntry:<br>\ntrain()<br>\nexcept KeyboardInterrupt:<br>\nprint('Aborted!')</p>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nmain()</p>", "body_text": "Hey,\ni have written an neural chatbot with tensorflow. But my problem is, that TF only uses one GPU.\nI am training on an NVIDIA DGX Station with 8x Tesla V100, so I want to use alle memory and cores.\nWhere is my mistake in the code? Who could I solve my problem?\nI would be very thankful about an answer! :D\nMy code:\nimport time\nimport data\nimport click\nimport numpy as np\nimport tensorflow as tf\nimport tensorlayer as tl\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom tensorlayer.layers import DenseLayer, EmbeddingInputlayer, Seq2Seq, retrieve_seq_length_op2\nsessionConfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False)\n@click.command()\n@click.option('-dc', '--data-corpus', default = 'twitter')\n@click.option('-bs', '--batch-size', default = 32)\n@click.option('-e', '--epochs', default = 100000)\n@click.option('-lr', '--learning-rate', default = 0.001)\n@click.option('-m', '--mode', is_flag = True)\ndef train(data_corpus, batch_size, epochs, learning_rate, mode):\nmetadata, trainX, trainY, testX, testY, validX, validY = setup(data_corpus)\n\nsrcLength = len(trainX)\ntargetLength = len(trainY)\n\nassert srcLength == targetLength\n\nnStep = srcLength // batch_size\nsrcVocabSize = len(metadata['index2Word'])\nembeddingDim = 1024\n\nword2Index = metadata['word2Index']  \nindex2Word = metadata['index2Word']  \n\nunk_id = word2Index['unk']  \npad_id = word2Index['_']    \n\nstart_id = srcVocabSize  \nend_id = srcVocabSize + 1 \n\nword2Index.update({'start_id': start_id})\nword2Index.update({'end_id': end_id})\nindex2Word = index2Word + ['start_id', 'end_id']\n\nsrcVocabSize = tgt_vocab_size = srcVocabSize + 2\n\ntarget_seqs = tl.prepro.sequences_add_end_id([trainY[100]], end_id=end_id)[0]\ndecode_seqs = tl.prepro.sequences_add_start_id([trainY[100]], start_id=start_id, remove_last=False)[0]\ntarget_mask = tl.prepro.sequences_get_mask([target_seqs])[0]\nif not mode:\n    print(\"encode_seqs\", [index2Word[id] for id in trainX[100]])\n    print(\"target_seqs\", [index2Word[id] for id in target_seqs])\n    print(\"decode_seqs\", [index2Word[id] for id in decode_seqs])\n    print(\"target_mask\", target_mask)\n    print(len(target_seqs), len(decode_seqs), len(target_mask))\n\t\ntf.reset_default_graph()\n\nsession = tf.Session(config=sessionConfig)\n\nencode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\ndecode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\ntarget_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\ntarget_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \n    \noutputNetwork, _ = createModel(encode_seqs, decode_seqs, srcVocabSize, embeddingDim, isTrain=True, reuse=False)\noutputNetwork.print_params(False)\n    \nencode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\ndecode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n    \nnetwork, RNNnetwork = createModel(encode_seqs2, decode_seqs2, srcVocabSize, embeddingDim, isTrain=False, reuse=True)\ny = tf.nn.softmax(network.outputs)\n    \nloss = tl.cost.cross_entropy_seq_with_mask(logits=outputNetwork.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')\n    \ntrainOp = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\nsession.run(tf.global_variables_initializer())\n\ntl.files.load_and_assign_npz(sess=session, name='model.npz', network=network)\t\n\ndef inference(seed):\n    seed_id = [word2Index.get(w, unk_id) for w in seed.split(\" \")]\n    \n    state = session.run(RNNnetwork.final_state_encode, {encode_seqs2: [seed_id]})\n    o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[start_id]]})\n    w_id = tl.nlp.sample_top(o[0], top_k=3)\n    w = index2Word[w_id]\n    sentence = [w]\n    for _ in range(200): # 30 output length\n        o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[w_id]]})\n        w_id = tl.nlp.sample_top(o[0], top_k=2)\n        w = index2Word[w_id]\n        if w_id == end_id:\n            break\n        sentence = sentence + [w]\n    return sentence\n\nif mode:\n    while True:\n        input_seq = input('Input >> ')\n        sentence = inference(input_seq)\n        print(\"Output >>\", ' '.join(sentence))\nelse:\n    seeds = [\"happy birthday have a nice day\",\n             \"donald trump won last nights presidential debate according to snap online polls\"]\n    for epoch in range(epochs):\n        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n        total_loss, n_iter = 0, 0\n        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), total=nStep, desc='Epoch[{}/{}]'.format(epoch + 1, epochs), leave=False):\n            X = tl.prepro.pad_sequences(X)\n            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n\t\t\t\n            _, loss_iter = session.run([trainOp, loss], {encode_seqs: X, decode_seqs: _decode_seqs, target_seqs: _target_seqs, target_mask: _target_mask})\n            total_loss += loss_iter\n            n_iter += 1\n\n        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, epochs, total_loss / n_iter))\n        \n        for seed in seeds:\n            print(\"Input >>\", seed)\n            for _ in range(5):\n                sentence = inference(seed)\n                print(\"Output >>\", ' '.join(sentence))\n        \n        tl.files.save_npz(network.all_params, name='model.npz', sess=session)\n\nsession.close()\n\ndef createModel(encodeSeqs, decodeSeqs, vocabSize, embeddingDim, isTrain = True, reuse = False):\nwith tf.variable_scope(\"model\", reuse = reuse):\nwith tf.variable_scope(\"embedding\") as vs:\nnetworkworkEncoder = EmbeddingInputlayer(inputs = encodeSeqs,\nvocabulary_size = vocabSize,\nembedding_size = embeddingDim,\nname = 'sequenceEmbedding')\nvs.reuse_variables()\nnetworkworkDecoder = EmbeddingInputlayer(inputs = decodeSeqs,\nvocabulary_size = vocabSize,\nembedding_size = embeddingDim,\nname = 'sequenceEmbedding')\nnetworkworkRNN = Seq2Seq(networkworkEncoder, networkworkDecoder,\ncell_fn = tf.nn.rnn_cell.LSTMCell,\nn_hidden = embeddingDim,\ninitializer = tf.random_uniform_initializer(-0.1, 0.1),\nencode_sequence_length = retrieve_seq_length_op2(encodeSeqs),\ndecode_sequence_length = retrieve_seq_length_op2(decodeSeqs),\ninitial_state_encode = None,\ndropout = (0.5 if isTrain else None),\nn_layer = 3,\nreturn_seq_2d = True,\nname = 'seq2seq')\nnetworkworkOut = DenseLayer(networkworkRNN, n_units = vocabSize, act = tf.identity, name = 'output')\nreturn networkworkOut, networkworkRNN\ndef setup(data_corpus):\nmetaData, indexQ, indexA = data.load_data(Path=''.format(data_corpus))\n(trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(indexQ, indexA)\ntrainX = tl.prepro.remove_pad_sequences(trainX.tolist())\ntrainY = tl.prepro.remove_pad_sequences(trainY.tolist())\ntestX = tl.prepro.remove_pad_sequences(testX.tolist())\ntestY = tl.prepro.remove_pad_sequences(testY.tolist())\nvalidX = tl.prepro.remove_pad_sequences(validX.tolist())\nvalidY = tl.prepro.remove_pad_sequences(validY.tolist())\nreturn metaData, trainX, trainY, testX, testY, validX, validY\ndef main():\ntry:\ntrain()\nexcept KeyboardInterrupt:\nprint('Aborted!')\nif name == 'main':\nmain()", "body": "Hey,\r\ni have written an neural chatbot with tensorflow. But my problem is, that TF only uses one GPU.\r\nI am training on an NVIDIA DGX Station with 8x Tesla V100, so I want to use alle memory and cores.\r\nWhere is my mistake in the code? Who could I solve my problem?\r\n\r\nI would be very thankful about an answer! :D\r\n\r\n\r\nMy code:\r\n\r\n\r\nimport time\r\n\r\nimport data\r\nimport click\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorlayer as tl\r\n\r\nfrom tqdm import tqdm\r\nfrom sklearn.utils import shuffle\r\nfrom tensorlayer.layers import DenseLayer, EmbeddingInputlayer, Seq2Seq, retrieve_seq_length_op2\r\n\r\nsessionConfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False)\r\n\r\n@click.command()\r\n@click.option('-dc', '--data-corpus', default = 'twitter')\r\n@click.option('-bs', '--batch-size', default = 32)\r\n@click.option('-e', '--epochs', default = 100000)\r\n@click.option('-lr', '--learning-rate', default = 0.001)\r\n@click.option('-m', '--mode', is_flag = True)\r\ndef train(data_corpus, batch_size, epochs, learning_rate, mode):\r\n\r\n    metadata, trainX, trainY, testX, testY, validX, validY = setup(data_corpus)\r\n\r\n    srcLength = len(trainX)\r\n    targetLength = len(trainY)\r\n\r\n    assert srcLength == targetLength\r\n\r\n    nStep = srcLength // batch_size\r\n    srcVocabSize = len(metadata['index2Word'])\r\n    embeddingDim = 1024\r\n\r\n    word2Index = metadata['word2Index']  \r\n    index2Word = metadata['index2Word']  \r\n\r\n    unk_id = word2Index['unk']  \r\n    pad_id = word2Index['_']    \r\n\r\n    start_id = srcVocabSize  \r\n    end_id = srcVocabSize + 1 \r\n\r\n    word2Index.update({'start_id': start_id})\r\n    word2Index.update({'end_id': end_id})\r\n    index2Word = index2Word + ['start_id', 'end_id']\r\n\r\n    srcVocabSize = tgt_vocab_size = srcVocabSize + 2\r\n\r\n    target_seqs = tl.prepro.sequences_add_end_id([trainY[100]], end_id=end_id)[0]\r\n    decode_seqs = tl.prepro.sequences_add_start_id([trainY[100]], start_id=start_id, remove_last=False)[0]\r\n    target_mask = tl.prepro.sequences_get_mask([target_seqs])[0]\r\n    if not mode:\r\n        print(\"encode_seqs\", [index2Word[id] for id in trainX[100]])\r\n        print(\"target_seqs\", [index2Word[id] for id in target_seqs])\r\n        print(\"decode_seqs\", [index2Word[id] for id in decode_seqs])\r\n        print(\"target_mask\", target_mask)\r\n        print(len(target_seqs), len(decode_seqs), len(target_mask))\r\n\t\t\r\n    tf.reset_default_graph()\r\n    \r\n    session = tf.Session(config=sessionConfig)\r\n\t\r\n    encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\r\n    decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\r\n    target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\r\n    target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \r\n        \r\n    outputNetwork, _ = createModel(encode_seqs, decode_seqs, srcVocabSize, embeddingDim, isTrain=True, reuse=False)\r\n    outputNetwork.print_params(False)\r\n        \r\n    encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\r\n    decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\r\n        \r\n    network, RNNnetwork = createModel(encode_seqs2, decode_seqs2, srcVocabSize, embeddingDim, isTrain=False, reuse=True)\r\n    y = tf.nn.softmax(network.outputs)\r\n        \r\n    loss = tl.cost.cross_entropy_seq_with_mask(logits=outputNetwork.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')\r\n        \r\n    trainOp = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    tl.files.load_and_assign_npz(sess=session, name='model.npz', network=network)\t\r\n\r\n    def inference(seed):\r\n        seed_id = [word2Index.get(w, unk_id) for w in seed.split(\" \")]\r\n        \r\n        state = session.run(RNNnetwork.final_state_encode, {encode_seqs2: [seed_id]})\r\n        o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[start_id]]})\r\n        w_id = tl.nlp.sample_top(o[0], top_k=3)\r\n        w = index2Word[w_id]\r\n        sentence = [w]\r\n        for _ in range(200): # 30 output length\r\n            o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[w_id]]})\r\n            w_id = tl.nlp.sample_top(o[0], top_k=2)\r\n            w = index2Word[w_id]\r\n            if w_id == end_id:\r\n                break\r\n            sentence = sentence + [w]\r\n        return sentence\r\n\r\n    if mode:\r\n        while True:\r\n            input_seq = input('Input >> ')\r\n            sentence = inference(input_seq)\r\n            print(\"Output >>\", ' '.join(sentence))\r\n    else:\r\n        seeds = [\"happy birthday have a nice day\",\r\n                 \"donald trump won last nights presidential debate according to snap online polls\"]\r\n        for epoch in range(epochs):\r\n            trainX, trainY = shuffle(trainX, trainY, random_state=0)\r\n            total_loss, n_iter = 0, 0\r\n            for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), total=nStep, desc='Epoch[{}/{}]'.format(epoch + 1, epochs), leave=False):\r\n                X = tl.prepro.pad_sequences(X)\r\n                _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\r\n                _target_seqs = tl.prepro.pad_sequences(_target_seqs)\r\n                _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\r\n                _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\r\n                _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\r\n\t\t\t\t\r\n                _, loss_iter = session.run([trainOp, loss], {encode_seqs: X, decode_seqs: _decode_seqs, target_seqs: _target_seqs, target_mask: _target_mask})\r\n                total_loss += loss_iter\r\n                n_iter += 1\r\n\r\n            print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, epochs, total_loss / n_iter))\r\n            \r\n            for seed in seeds:\r\n                print(\"Input >>\", seed)\r\n                for _ in range(5):\r\n                    sentence = inference(seed)\r\n                    print(\"Output >>\", ' '.join(sentence))\r\n            \r\n            tl.files.save_npz(network.all_params, name='model.npz', sess=session)\r\n    \r\n    session.close()\r\n    \r\n    \r\ndef createModel(encodeSeqs, decodeSeqs, vocabSize, embeddingDim, isTrain = True, reuse = False):\r\n    with tf.variable_scope(\"model\", reuse = reuse):\r\n        with tf.variable_scope(\"embedding\") as vs:\r\n            networkworkEncoder = EmbeddingInputlayer(inputs = encodeSeqs, \r\n                                                 vocabulary_size = vocabSize, \r\n                                                 embedding_size = embeddingDim, \r\n                                                 name = 'sequenceEmbedding')\r\n            vs.reuse_variables()\r\n            networkworkDecoder = EmbeddingInputlayer(inputs = decodeSeqs, \r\n                                                 vocabulary_size = vocabSize, \r\n                                                 embedding_size = embeddingDim, \r\n                                                 name = 'sequenceEmbedding')\r\n        networkworkRNN = Seq2Seq(networkworkEncoder, networkworkDecoder, \r\n                             cell_fn = tf.nn.rnn_cell.LSTMCell, \r\n                             n_hidden = embeddingDim, \r\n                             initializer = tf.random_uniform_initializer(-0.1, 0.1),\r\n                             encode_sequence_length = retrieve_seq_length_op2(encodeSeqs),\r\n                             decode_sequence_length = retrieve_seq_length_op2(decodeSeqs),\r\n                             initial_state_encode = None,\r\n                             dropout = (0.5 if isTrain else None),\r\n                             n_layer = 3,\r\n                             return_seq_2d = True,\r\n                             name = 'seq2seq')\r\n        networkworkOut = DenseLayer(networkworkRNN, n_units = vocabSize, act = tf.identity, name = 'output')\r\n    return networkworkOut, networkworkRNN\r\n\r\ndef setup(data_corpus):\r\n    metaData, indexQ, indexA = data.load_data(Path=''.format(data_corpus))\r\n    (trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(indexQ, indexA)\r\n    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\r\n    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\r\n    testX = tl.prepro.remove_pad_sequences(testX.tolist())\r\n    testY = tl.prepro.remove_pad_sequences(testY.tolist())\r\n    validX = tl.prepro.remove_pad_sequences(validX.tolist())\r\n    validY = tl.prepro.remove_pad_sequences(validY.tolist())\r\n    return metaData, trainX, trainY, testX, testY, validX, validY\r\n\r\ndef main():\r\n    try:\r\n        train()\r\n    except KeyboardInterrupt:\r\n        print('Aborted!')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"}