{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13337", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13337/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13337/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13337/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13337", "id": 260899078, "node_id": "MDU6SXNzdWUyNjA4OTkwNzg=", "number": 13337, "title": "MonitoredTrainingSession: CreateSession still waiting", "user": {"login": "vlasenkov", "id": 13369474, "node_id": "MDQ6VXNlcjEzMzY5NDc0", "avatar_url": "https://avatars3.githubusercontent.com/u/13369474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlasenkov", "html_url": "https://github.com/vlasenkov", "followers_url": "https://api.github.com/users/vlasenkov/followers", "following_url": "https://api.github.com/users/vlasenkov/following{/other_user}", "gists_url": "https://api.github.com/users/vlasenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlasenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlasenkov/subscriptions", "organizations_url": "https://api.github.com/users/vlasenkov/orgs", "repos_url": "https://api.github.com/users/vlasenkov/repos", "events_url": "https://api.github.com/users/vlasenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/vlasenkov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-27T09:03:41Z", "updated_at": "2017-09-28T05:19:51Z", "closed_at": "2017-09-28T05:19:51Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: MacOS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.3.0-rc1-2195-gd86ee219e 1.4.0-dev</li>\n<li><strong>Python version</strong>: Python 3.6.1 :: Anaconda 4.4.0 (x86_64)</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.2-homebrew</li>\n<li><strong>CUDA/cuDNN version</strong>: no</li>\n<li><strong>GPU model and memory</strong>: no</li>\n<li><strong>Exact command to reproduce</strong>: <code>./run.sh</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I've already raised this problem on <a href=\"https://stackoverflow.com/questions/46429766/distributed-tensorflow-createsession-still-waiting\" rel=\"nofollow\">stackoverflow</a>, but haven't got any feedback.</p>\n<p>I run the python script four times with the bash script. There are three <code>worker</code> tasks and one <code>master</code> task. Sometimes all the workers successfully exit. But often one or two of them hang and begin emitting \"CreateSession still waiting for some other task\" messages. Such waiting worker can wait for the chief (worker task 0) or some other task which is already completed.</p>\n<p>So, the problem is that workers and even the chief do not wait for some lagging worker. Is it a bug? Chief is responsible for initialisation of variables. But if I put a variable on each worker nothing changes. The chief still do not wait.</p>\n<p>I've succeeded in synchronisation of these workers by adding <code>FIFOQueue</code> barriers to the beginning of each session.</p>\n<h3>Source code / logs</h3>\n<p>TF script (<code>train.py</code>):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--job<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--task<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)\nargs <span class=\"pl-k\">=</span> parser.parse_args()\nhosts <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>master<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2222<span class=\"pl-pds\">\"</span></span>,\n    ],\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2223<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2224<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2225<span class=\"pl-pds\">\"</span></span>,\n    ]\n}\n\nnworkers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(hosts[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>])\ncluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec(hosts)\nserver <span class=\"pl-k\">=</span> tf.train.Server(cluster, <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span>args.job, <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span>args.task)\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\">f</span><span class=\"pl-pds\">'</span><span class=\"pl-s\">/job:master/task:0</span><span class=\"pl-pds\">'</span>):\n    global_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\n    inc_global_step <span class=\"pl-k\">=</span> tf.assign(global_step, global_step <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">if</span> args.job <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>:\n    hooks <span class=\"pl-k\">=</span> [\n        tf.train.StopAtStepHook(<span class=\"pl-v\">last_step</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>),\n    ]\n    <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(<span class=\"pl-v\">master</span><span class=\"pl-k\">=</span>server.target,\n                                           <span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span>(args.task <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>),\n                                           <span class=\"pl-v\">hooks</span><span class=\"pl-k\">=</span>hooks) <span class=\"pl-k\">as</span> sess:\n        <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> sess.should_stop():\n            <span class=\"pl-c1\">print</span>(args.task, sess.run(inc_global_step))\n<span class=\"pl-k\">else</span>:\n    server.join()</pre></div>\n<p>Bash script (<code>run.sh</code>):</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#!</span>/bin/bash</span>\npython train.py --job master --task 0 <span class=\"pl-k\">&amp;</span>\npython train.py --job worker --task 0 <span class=\"pl-k\">&amp;</span>\npython train.py --job worker --task 1 <span class=\"pl-k\">&amp;</span>\npython train.py --job worker --task 2 <span class=\"pl-k\">&amp;</span></pre></div>\n<p>Example of message:</p>\n<pre><code>2017-09-27 11:52:48.973442: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): v1.3.0-rc1-2195-gd86ee219e 1.4.0-dev\nPython version: Python 3.6.1 :: Anaconda 4.4.0 (x86_64)\nBazel version (if compiling from source): 0.5.2-homebrew\nCUDA/cuDNN version: no\nGPU model and memory: no\nExact command to reproduce: ./run.sh\n\nDescribe the problem\nI've already raised this problem on stackoverflow, but haven't got any feedback.\nI run the python script four times with the bash script. There are three worker tasks and one master task. Sometimes all the workers successfully exit. But often one or two of them hang and begin emitting \"CreateSession still waiting for some other task\" messages. Such waiting worker can wait for the chief (worker task 0) or some other task which is already completed.\nSo, the problem is that workers and even the chief do not wait for some lagging worker. Is it a bug? Chief is responsible for initialisation of variables. But if I put a variable on each worker nothing changes. The chief still do not wait.\nI've succeeded in synchronisation of these workers by adding FIFOQueue barriers to the beginning of each session.\nSource code / logs\nTF script (train.py):\nimport argparse\nimport tensorflow as tf\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--job', type=str)\nparser.add_argument('--task', type=int)\nargs = parser.parse_args()\nhosts = {\n    \"master\": [\n        \"localhost:2222\",\n    ],\n    \"worker\": [\n        \"localhost:2223\",\n        \"localhost:2224\",\n        \"localhost:2225\",\n    ]\n}\n\nnworkers = len(hosts['worker'])\ncluster = tf.train.ClusterSpec(hosts)\nserver = tf.train.Server(cluster, job_name=args.job, task_index=args.task)\n\nwith tf.device(f'/job:master/task:0'):\n    global_step = tf.train.get_or_create_global_step()\n    inc_global_step = tf.assign(global_step, global_step + 1)\n\nif args.job == 'worker':\n    hooks = [\n        tf.train.StopAtStepHook(last_step=4),\n    ]\n    with tf.train.MonitoredTrainingSession(master=server.target,\n                                           is_chief=(args.task == 0),\n                                           hooks=hooks) as sess:\n        while not sess.should_stop():\n            print(args.task, sess.run(inc_global_step))\nelse:\n    server.join()\nBash script (run.sh):\n#!/bin/bash\npython train.py --job master --task 0 &\npython train.py --job worker --task 0 &\npython train.py --job worker --task 1 &\npython train.py --job worker --task 2 &\nExample of message:\n2017-09-27 11:52:48.973442: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-2195-gd86ee219e 1.4.0-dev\r\n- **Python version**: Python 3.6.1 :: Anaconda 4.4.0 (x86_64)\r\n- **Bazel version (if compiling from source)**: 0.5.2-homebrew\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: `./run.sh`\r\n\r\n### Describe the problem\r\n\r\nI've already raised this problem on [stackoverflow](https://stackoverflow.com/questions/46429766/distributed-tensorflow-createsession-still-waiting), but haven't got any feedback.\r\n\r\nI run the python script four times with the bash script. There are three `worker` tasks and one `master` task. Sometimes all the workers successfully exit. But often one or two of them hang and begin emitting \"CreateSession still waiting for some other task\" messages. Such waiting worker can wait for the chief (worker task 0) or some other task which is already completed.\r\n\r\nSo, the problem is that workers and even the chief do not wait for some lagging worker. Is it a bug? Chief is responsible for initialisation of variables. But if I put a variable on each worker nothing changes. The chief still do not wait.\r\n\r\nI've succeeded in synchronisation of these workers by adding `FIFOQueue` barriers to the beginning of each session.\r\n\r\n### Source code / logs\r\n\r\nTF script (`train.py`):\r\n\r\n```python\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--job', type=str)\r\nparser.add_argument('--task', type=int)\r\nargs = parser.parse_args()\r\nhosts = {\r\n    \"master\": [\r\n        \"localhost:2222\",\r\n    ],\r\n    \"worker\": [\r\n        \"localhost:2223\",\r\n        \"localhost:2224\",\r\n        \"localhost:2225\",\r\n    ]\r\n}\r\n\r\nnworkers = len(hosts['worker'])\r\ncluster = tf.train.ClusterSpec(hosts)\r\nserver = tf.train.Server(cluster, job_name=args.job, task_index=args.task)\r\n\r\nwith tf.device(f'/job:master/task:0'):\r\n    global_step = tf.train.get_or_create_global_step()\r\n    inc_global_step = tf.assign(global_step, global_step + 1)\r\n\r\nif args.job == 'worker':\r\n    hooks = [\r\n        tf.train.StopAtStepHook(last_step=4),\r\n    ]\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(args.task == 0),\r\n                                           hooks=hooks) as sess:\r\n        while not sess.should_stop():\r\n            print(args.task, sess.run(inc_global_step))\r\nelse:\r\n    server.join()\r\n```\r\nBash script (`run.sh`):\r\n```bash\r\n#!/bin/bash\r\npython train.py --job master --task 0 &\r\npython train.py --job worker --task 0 &\r\npython train.py --job worker --task 1 &\r\npython train.py --job worker --task 2 &\r\n```\r\nExample of message:\r\n```\r\n2017-09-27 11:52:48.973442: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n```"}