{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/221222322", "html_url": "https://github.com/tensorflow/tensorflow/issues/2463#issuecomment-221222322", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2463", "id": 221222322, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTIyMjMyMg==", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-24T09:58:24Z", "updated_at": "2016-05-24T10:02:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am experiencing the same issue.<br>\nAs far as I explored, it occurs because of the current implementation of <code>zero_state</code>.<br>\nWhen <code>state_is_tuple</code> flag is set to false, the cell state and the hidden state of a recurrent layer is concatenated, and the shape is <code>(2 * num_layers,)</code>.<br>\nHowever, when the flag is on, the cell state and the hidden state is passed separately, and the shape is <code>(num_layers, num_layers)</code> (one for the cell state, another for the hidden state).</p>\n<ol>\n<li>Without <code>MultiRNNCell</code><br>\nIn this case, <code>state_size</code> would be <code>(num_layers, num_layers)</code>.<br>\nSo line 107-109 of the current implementation would make a length-2 tuple of <code>(batch_size, num_layers)</code>-shape zero tensors.</li>\n<li>With <code>MultiRNNCell</code><br>\nIn this case, <code>state_size</code> would be <code>((num_layers, num_layers), ..., (num_layers, num_layers))</code> (whose length is <code>num_layers</code>).<br>\nSo, line 107-109 of the current implementation causes error, because <code>s</code> in line 109 is a tuple <code>(num_layers, num_layers)</code>, where a single number is expected.</li>\n</ol>\n<p>If you configure that setting the flag of a single <code>LSTMCell</code> to <code>False</code> and that of a <code>MultiRNNCell</code> to <code>True</code>, it would work well, although warnings would occur and there would be some degradation on performance. (<code>num_layer + 1</code> warnings if you set both flags to <code>False</code>, and (<code>num_layer</code> warnings if you turn on only <code>MultiRNNCell</code>'s flag.)</p>\n<p>I think it can be fixed by fixing as follows: if the type of <code>state_size</code> is tuple of tuple, initialize for all single numbers in the tuple of tuple and return with the same form.</p>", "body_text": "I am experiencing the same issue.\nAs far as I explored, it occurs because of the current implementation of zero_state.\nWhen state_is_tuple flag is set to false, the cell state and the hidden state of a recurrent layer is concatenated, and the shape is (2 * num_layers,).\nHowever, when the flag is on, the cell state and the hidden state is passed separately, and the shape is (num_layers, num_layers) (one for the cell state, another for the hidden state).\n\nWithout MultiRNNCell\nIn this case, state_size would be (num_layers, num_layers).\nSo line 107-109 of the current implementation would make a length-2 tuple of (batch_size, num_layers)-shape zero tensors.\nWith MultiRNNCell\nIn this case, state_size would be ((num_layers, num_layers), ..., (num_layers, num_layers)) (whose length is num_layers).\nSo, line 107-109 of the current implementation causes error, because s in line 109 is a tuple (num_layers, num_layers), where a single number is expected.\n\nIf you configure that setting the flag of a single LSTMCell to False and that of a MultiRNNCell to True, it would work well, although warnings would occur and there would be some degradation on performance. (num_layer + 1 warnings if you set both flags to False, and (num_layer warnings if you turn on only MultiRNNCell's flag.)\nI think it can be fixed by fixing as follows: if the type of state_size is tuple of tuple, initialize for all single numbers in the tuple of tuple and return with the same form.", "body": "I am experiencing the same issue.\nAs far as I explored, it occurs because of the current implementation of `zero_state`.\nWhen `state_is_tuple` flag is set to false, the cell state and the hidden state of a recurrent layer is concatenated, and the shape is `(2 * num_layers,)`.\nHowever, when the flag is on, the cell state and the hidden state is passed separately, and the shape is `(num_layers, num_layers)` (one for the cell state, another for the hidden state).\n1. Without `MultiRNNCell`\n   In this case, `state_size` would be `(num_layers, num_layers)`.\n   So line 107-109 of the current implementation would make a length-2 tuple of `(batch_size, num_layers)`-shape zero tensors.\n2. With `MultiRNNCell`\n   In this case, `state_size` would be `((num_layers, num_layers), ..., (num_layers, num_layers))` (whose length is `num_layers`).\n   So, line 107-109 of the current implementation causes error, because `s` in line 109 is a tuple `(num_layers, num_layers)`, where a single number is expected.\n\nIf you configure that setting the flag of a single `LSTMCell` to `False` and that of a `MultiRNNCell` to `True`, it would work well, although warnings would occur and there would be some degradation on performance. (`num_layer + 1` warnings if you set both flags to `False`, and (`num_layer` warnings if you turn on only `MultiRNNCell`'s flag.)\n\nI think it can be fixed by fixing as follows: if the type of `state_size` is tuple of tuple, initialize for all single numbers in the tuple of tuple and return with the same form.\n"}