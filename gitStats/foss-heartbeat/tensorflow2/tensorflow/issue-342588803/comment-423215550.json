{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423215550", "html_url": "https://github.com/tensorflow/tensorflow/issues/20955#issuecomment-423215550", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20955", "id": 423215550, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzIxNTU1MA==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T14:58:14Z", "updated_at": "2018-09-20T14:58:14Z", "author_association": "MEMBER", "body_html": "<p>The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet.</p>\n<p>It can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: <a href=\"https://www.tensorflow.org/performance/post_training_quantization\" rel=\"nofollow\">https://www.tensorflow.org/performance/post_training_quantization</a></p>\n<p>We do plan on making better tooling for fully quantized models as well, but as a first pass --post_training_quantize will get you the furthest, and if speed/accuracy aren't sufficient, we should add FakeQuant nodes into the graph with the contrib/quantize tool and sometimes manually for patterns that aren't recognized.</p>", "body_text": "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet.\nIt can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: https://www.tensorflow.org/performance/post_training_quantization\nWe do plan on making better tooling for fully quantized models as well, but as a first pass --post_training_quantize will get you the furthest, and if speed/accuracy aren't sufficient, we should add FakeQuant nodes into the graph with the contrib/quantize tool and sometimes manually for patterns that aren't recognized.", "body": "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. \r\n\r\nIt can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: https://www.tensorflow.org/performance/post_training_quantization\r\n\r\nWe do plan on making better tooling for fully quantized models as well, but as a first pass --post_training_quantize will get you the furthest, and if speed/accuracy aren't sufficient, we should add FakeQuant nodes into the graph with the contrib/quantize tool and sometimes manually for patterns that aren't recognized."}