{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234216368", "html_url": "https://github.com/tensorflow/tensorflow/issues/3420#issuecomment-234216368", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3420", "id": 234216368, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDIxNjM2OA==", "user": {"login": "ASDen", "id": 124146, "node_id": "MDQ6VXNlcjEyNDE0Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/124146?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ASDen", "html_url": "https://github.com/ASDen", "followers_url": "https://api.github.com/users/ASDen/followers", "following_url": "https://api.github.com/users/ASDen/following{/other_user}", "gists_url": "https://api.github.com/users/ASDen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ASDen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ASDen/subscriptions", "organizations_url": "https://api.github.com/users/ASDen/orgs", "repos_url": "https://api.github.com/users/ASDen/repos", "events_url": "https://api.github.com/users/ASDen/events{/privacy}", "received_events_url": "https://api.github.com/users/ASDen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-21T10:29:13Z", "updated_at": "2016-07-21T10:29:13Z", "author_association": "NONE", "body_html": "<p>This is somewhat lengthy, as I take the chance to also demonstrate a speed problem with dynamic_rnn when specifying sequence_length</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.ops  <span class=\"pl-k\">import</span> rnn, rnn_cell,array_ops\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> time\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\nn_input <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\nn_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\nn_hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">40</span>\n\nnp.random.seed(<span class=\"pl-c1\">1234</span>)\ntf.set_random_seed(<span class=\"pl-c1\">1234</span>)\n\ndata <span class=\"pl-k\">=</span> tf.constant(np.random.uniform(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>, [batch_size,n_steps,n_input]), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nsql  <span class=\"pl-k\">=</span> tf.constant([n_steps] <span class=\"pl-k\">*</span> batch_size, tf.int64)\nlstm_cell <span class=\"pl-k\">=</span> rnn_cell.LSTMCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>n_hidden, <span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">stacked_lstm</span>():\n    mlstm_cell <span class=\"pl-k\">=</span> rnn_cell.MultiRNNCell([lstm_cell] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">return</span> rnn.dynamic_rnn(mlstm_cell, data, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,<span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sql)[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">stacked_lstm_no_sql</span>():\n    mlstm_cell <span class=\"pl-k\">=</span> rnn_cell.MultiRNNCell([lstm_cell] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">return</span> rnn.dynamic_rnn(mlstm_cell, data, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bi_lstm</span>():\n    <span class=\"pl-k\">return</span> rnn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, data, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sql, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bi_lstm_no_sql</span>():\n    y1 <span class=\"pl-k\">=</span> rnn.dynamic_rnn(lstm_cell, data,  <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,<span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>r1<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>]\n    rdata <span class=\"pl-k\">=</span> array_ops.reverse_sequence(<span class=\"pl-v\">input</span><span class=\"pl-k\">=</span>data, <span class=\"pl-v\">seq_lengths</span><span class=\"pl-k\">=</span>sql, <span class=\"pl-v\">seq_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">batch_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    y2 <span class=\"pl-k\">=</span> rnn.dynamic_rnn(lstm_cell, rdata, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,<span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>r2<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>]\n    y2 <span class=\"pl-k\">=</span> array_ops.reverse_sequence(<span class=\"pl-v\">input</span><span class=\"pl-k\">=</span>y2, <span class=\"pl-v\">seq_lengths</span><span class=\"pl-k\">=</span>sql, <span class=\"pl-v\">seq_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">batch_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    <span class=\"pl-k\">return</span> [y1,y2]\n\ny <span class=\"pl-k\">=</span> stacked_lstm()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>y=stacked_lstm_no_sql()</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>y = bi_lstm()</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>y = bi_lstm_no_sql()</span>\n\ninit <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto(\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>device_count={'GPU': 0}, #turn GPU on and off</span>\n)\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n    sess.run(init)\n    start <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n        sess.run(y)\n        <span class=\"pl-k\">if</span>(i<span class=\"pl-k\">%</span><span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>): <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Iter : <span class=\"pl-pds\">\"</span></span>,i<span class=\"pl-k\">*</span>batch_size\n    end <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Finished!  <span class=\"pl-pds\">\"</span></span>, (end <span class=\"pl-k\">-</span> start)</pre></div>\n<p>when running on GTX 960, you get (i record Utilization from nvidia-setting)<br>\nstacked_lstm              : 14.11s     GPU Util:69%<br>\nstacked_lstm_no_sql : 10.49s     GPU Util:79%<br>\nbi_lstm              : 16.15s     GPU Util:65%<br>\nbi_lstm_no_sql  : 11.54s     GPU Util:75%</p>\n<p>we note:</p>\n<ol>\n<li>stacked_lstm is generally faster than bi_lstm (though, bi_lstm is trivial to parallelize and should be generally up to 2X faster)</li>\n<li>removing sequence_length has huge impact on speed. However, I noted that the two methods (with and without sequence_length) return different results, and in my practical use cases removing sequence_length generally returns slightly worse results</li>\n</ol>\n<p>what is really needed is method to allow parallel execution of independent parts of the graph like BLSTMs</p>", "body_text": "This is somewhat lengthy, as I take the chance to also demonstrate a speed problem with dynamic_rnn when specifying sequence_length\nimport tensorflow as tf\nfrom tensorflow.python.ops  import rnn, rnn_cell,array_ops\nimport numpy as np\nimport time\n\nbatch_size = 128\nn_input = 20\nn_steps = 50\nn_hidden = 40\n\nnp.random.seed(1234)\ntf.set_random_seed(1234)\n\ndata = tf.constant(np.random.uniform(-1.0, 1.0, [batch_size,n_steps,n_input]), dtype=tf.float32)\nsql  = tf.constant([n_steps] * batch_size, tf.int64)\nlstm_cell = rnn_cell.LSTMCell(num_units=n_hidden, state_is_tuple=True)\n\ndef stacked_lstm():\n    mlstm_cell = rnn_cell.MultiRNNCell([lstm_cell] * 2, state_is_tuple=True)\n    return rnn.dynamic_rnn(mlstm_cell, data, dtype=tf.float32,sequence_length=sql)[0]\n\ndef stacked_lstm_no_sql():\n    mlstm_cell = rnn_cell.MultiRNNCell([lstm_cell] * 2, state_is_tuple=True)\n    return rnn.dynamic_rnn(mlstm_cell, data, dtype=tf.float32)[0]\n\ndef bi_lstm():\n    return rnn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, data, sequence_length=sql, dtype=tf.float32)[0]\n\ndef bi_lstm_no_sql():\n    y1 = rnn.dynamic_rnn(lstm_cell, data,  dtype=tf.float32,scope='r1')[0]\n    rdata = array_ops.reverse_sequence(input=data, seq_lengths=sql, seq_dim=1, batch_dim=0)\n    y2 = rnn.dynamic_rnn(lstm_cell, rdata, dtype=tf.float32,scope='r2')[0]\n    y2 = array_ops.reverse_sequence(input=y2, seq_lengths=sql, seq_dim=1, batch_dim=0)\n    return [y1,y2]\n\ny = stacked_lstm()\n#y=stacked_lstm_no_sql()\n#y = bi_lstm()\n#y = bi_lstm_no_sql()\n\ninit = tf.initialize_all_variables()\n\nconfig = tf.ConfigProto(\n    #device_count={'GPU': 0}, #turn GPU on and off\n)\n\nwith tf.Session(config=config) as sess:\n    sess.run(init)\n    start = time.time()\n    for i in range(1000):\n        sess.run(y)\n        if(i%100 == 0): print \"Iter : \",i*batch_size\n    end = time.time()\n    print \"Finished!  \", (end - start)\nwhen running on GTX 960, you get (i record Utilization from nvidia-setting)\nstacked_lstm              : 14.11s     GPU Util:69%\nstacked_lstm_no_sql : 10.49s     GPU Util:79%\nbi_lstm              : 16.15s     GPU Util:65%\nbi_lstm_no_sql  : 11.54s     GPU Util:75%\nwe note:\n\nstacked_lstm is generally faster than bi_lstm (though, bi_lstm is trivial to parallelize and should be generally up to 2X faster)\nremoving sequence_length has huge impact on speed. However, I noted that the two methods (with and without sequence_length) return different results, and in my practical use cases removing sequence_length generally returns slightly worse results\n\nwhat is really needed is method to allow parallel execution of independent parts of the graph like BLSTMs", "body": "This is somewhat lengthy, as I take the chance to also demonstrate a speed problem with dynamic_rnn when specifying sequence_length\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.python.ops  import rnn, rnn_cell,array_ops\nimport numpy as np\nimport time\n\nbatch_size = 128\nn_input = 20\nn_steps = 50\nn_hidden = 40\n\nnp.random.seed(1234)\ntf.set_random_seed(1234)\n\ndata = tf.constant(np.random.uniform(-1.0, 1.0, [batch_size,n_steps,n_input]), dtype=tf.float32)\nsql  = tf.constant([n_steps] * batch_size, tf.int64)\nlstm_cell = rnn_cell.LSTMCell(num_units=n_hidden, state_is_tuple=True)\n\ndef stacked_lstm():\n    mlstm_cell = rnn_cell.MultiRNNCell([lstm_cell] * 2, state_is_tuple=True)\n    return rnn.dynamic_rnn(mlstm_cell, data, dtype=tf.float32,sequence_length=sql)[0]\n\ndef stacked_lstm_no_sql():\n    mlstm_cell = rnn_cell.MultiRNNCell([lstm_cell] * 2, state_is_tuple=True)\n    return rnn.dynamic_rnn(mlstm_cell, data, dtype=tf.float32)[0]\n\ndef bi_lstm():\n    return rnn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, data, sequence_length=sql, dtype=tf.float32)[0]\n\ndef bi_lstm_no_sql():\n    y1 = rnn.dynamic_rnn(lstm_cell, data,  dtype=tf.float32,scope='r1')[0]\n    rdata = array_ops.reverse_sequence(input=data, seq_lengths=sql, seq_dim=1, batch_dim=0)\n    y2 = rnn.dynamic_rnn(lstm_cell, rdata, dtype=tf.float32,scope='r2')[0]\n    y2 = array_ops.reverse_sequence(input=y2, seq_lengths=sql, seq_dim=1, batch_dim=0)\n    return [y1,y2]\n\ny = stacked_lstm()\n#y=stacked_lstm_no_sql()\n#y = bi_lstm()\n#y = bi_lstm_no_sql()\n\ninit = tf.initialize_all_variables()\n\nconfig = tf.ConfigProto(\n    #device_count={'GPU': 0}, #turn GPU on and off\n)\n\nwith tf.Session(config=config) as sess:\n    sess.run(init)\n    start = time.time()\n    for i in range(1000):\n        sess.run(y)\n        if(i%100 == 0): print \"Iter : \",i*batch_size\n    end = time.time()\n    print \"Finished!  \", (end - start)\n```\n\nwhen running on GTX 960, you get (i record Utilization from nvidia-setting)\nstacked_lstm              : 14.11s     GPU Util:69%\nstacked_lstm_no_sql : 10.49s     GPU Util:79%\nbi_lstm              : 16.15s     GPU Util:65%\nbi_lstm_no_sql  : 11.54s     GPU Util:75%\n\nwe note:\n1) stacked_lstm is generally faster than bi_lstm (though, bi_lstm is trivial to parallelize and should be generally up to 2X faster)\n2) removing sequence_length has huge impact on speed. However, I noted that the two methods (with and without sequence_length) return different results, and in my practical use cases removing sequence_length generally returns slightly worse results\n\nwhat is really needed is method to allow parallel execution of independent parts of the graph like BLSTMs\n"}