{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/240465553", "html_url": "https://github.com/tensorflow/tensorflow/issues/3420#issuecomment-240465553", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3420", "id": 240465553, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MDQ2NTU1Mw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-17T16:20:41Z", "updated_at": "2016-08-17T16:20:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=124146\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ASDen\">@ASDen</a> you mean for running two replicas on the same worker in parallel?  Probably your best bet is to just run multiple python training threads and use the same session in each of them.  They can even read minibatches off the same queue.  If you're using a <code>Supervisor</code>, then you can use its <code>managed_session</code> object.</p>", "body_text": "@ASDen you mean for running two replicas on the same worker in parallel?  Probably your best bet is to just run multiple python training threads and use the same session in each of them.  They can even read minibatches off the same queue.  If you're using a Supervisor, then you can use its managed_session object.", "body": "@ASDen you mean for running two replicas on the same worker in parallel?  Probably your best bet is to just run multiple python training threads and use the same session in each of them.  They can even read minibatches off the same queue.  If you're using a `Supervisor`, then you can use its `managed_session` object.\n"}