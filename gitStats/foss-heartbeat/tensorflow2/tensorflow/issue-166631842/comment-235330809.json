{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235330809", "html_url": "https://github.com/tensorflow/tensorflow/issues/3420#issuecomment-235330809", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3420", "id": 235330809, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTMzMDgwOQ==", "user": {"login": "ASDen", "id": 124146, "node_id": "MDQ6VXNlcjEyNDE0Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/124146?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ASDen", "html_url": "https://github.com/ASDen", "followers_url": "https://api.github.com/users/ASDen/followers", "following_url": "https://api.github.com/users/ASDen/following{/other_user}", "gists_url": "https://api.github.com/users/ASDen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ASDen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ASDen/subscriptions", "organizations_url": "https://api.github.com/users/ASDen/orgs", "repos_url": "https://api.github.com/users/ASDen/repos", "events_url": "https://api.github.com/users/ASDen/events{/privacy}", "received_events_url": "https://api.github.com/users/ASDen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-26T16:49:42Z", "updated_at": "2016-07-26T16:49:42Z", "author_association": "NONE", "body_html": "<p>another implicit performance penalty I noticed is that when using deep LSTM networks, using MultiRNNCell instead of manual construction can be 2-3X faster (I think because it uses a single while loop)<br>\nthe problem with deep BLSTMs is that you can't do them with a single cell or a single while loop (layer output dependency) which means it incurs such a huge penalty solely due to many while loops involved<br>\nany thoughts on a more \"light weight\" while loop ?</p>\n<p>Note: with big output size/batch size the performance gap decreases, however the big sizes are less useful, and I do think on cards bigger than GTX 960, even the big sizes would suffer same penalty</p>", "body_text": "another implicit performance penalty I noticed is that when using deep LSTM networks, using MultiRNNCell instead of manual construction can be 2-3X faster (I think because it uses a single while loop)\nthe problem with deep BLSTMs is that you can't do them with a single cell or a single while loop (layer output dependency) which means it incurs such a huge penalty solely due to many while loops involved\nany thoughts on a more \"light weight\" while loop ?\nNote: with big output size/batch size the performance gap decreases, however the big sizes are less useful, and I do think on cards bigger than GTX 960, even the big sizes would suffer same penalty", "body": "another implicit performance penalty I noticed is that when using deep LSTM networks, using MultiRNNCell instead of manual construction can be 2-3X faster (I think because it uses a single while loop)\nthe problem with deep BLSTMs is that you can't do them with a single cell or a single while loop (layer output dependency) which means it incurs such a huge penalty solely due to many while loops involved\nany thoughts on a more \"light weight\" while loop ?\n\nNote: with big output size/batch size the performance gap decreases, however the big sizes are less useful, and I do think on cards bigger than GTX 960, even the big sizes would suffer same penalty\n"}