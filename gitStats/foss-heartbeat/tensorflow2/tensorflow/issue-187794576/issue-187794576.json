{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5460", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5460/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5460/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5460/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5460", "id": 187794576, "node_id": "MDU6SXNzdWUxODc3OTQ1NzY=", "number": 5460, "title": "reduce_sum extremely slow on gpu for complex dtypes", "user": {"login": "woodshop", "id": 4654379, "node_id": "MDQ6VXNlcjQ2NTQzNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4654379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/woodshop", "html_url": "https://github.com/woodshop", "followers_url": "https://api.github.com/users/woodshop/followers", "following_url": "https://api.github.com/users/woodshop/following{/other_user}", "gists_url": "https://api.github.com/users/woodshop/gists{/gist_id}", "starred_url": "https://api.github.com/users/woodshop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/woodshop/subscriptions", "organizations_url": "https://api.github.com/users/woodshop/orgs", "repos_url": "https://api.github.com/users/woodshop/repos", "events_url": "https://api.github.com/users/woodshop/events{/privacy}", "received_events_url": "https://api.github.com/users/woodshop/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2016-11-07T18:46:31Z", "updated_at": "2017-10-02T08:13:37Z", "closed_at": "2017-06-16T18:16:12Z", "author_association": "NONE", "body_html": "<h2>Description</h2>\n<p><code>tf.reduce_sum</code> takes an order of magnitude longer to compute when the tensor is a complex dtype and the computation takes place on a gpu. I've tried to investigate the source of the problem and it's not clear to me whether this is a Tensorflow issue or an Eigen issue, but I thought I'd raise it here first.</p>\n<h2>Environment</h2>\n<p>Ubuntu 16.04 LTS<br>\nTensorflow: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/1fcd6d1294564066c6f92b121a3aaf4ed186dc1a/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/1fcd6d1294564066c6f92b121a3aaf4ed186dc1a\"><tt>1fcd6d1</tt></a><br>\nGPU: Titan X<br>\nPython 2.7.11</p>\n<h2>Diagnostics/Reproducibility:</h2>\n<p>I produced a tensorflow timeline trace. The trace shows that reductions for a real and complex tensor on the cpu, and for a real tensor on the gpu, takes no longer than 1.3 ms. The reduction of a complex tensor of the same size takes about 196 ms on the gpu.  In particular this time is spent in a dedicated GPU stream, where the input arguments are marked as undefined in the trace.</p>\n<p>More specifically, here's a table summarizing the relevant parts of the trace:</p>\n<h3>CPU, Complex Reduction</h3>\n<table>\n<thead>\n<tr>\n<th>Start</th>\n<th>Wall Duration (ms)</th>\n<th>Self time (ms)</th>\n<th>Arg: input0</th>\n<th>Arg: input1</th>\n<th>Arg: name</th>\n<th>Arg: op</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.076 ms</td>\n<td>1.174 ms</td>\n<td>1.174 ms</td>\n<td>\"cplx_var/read\"</td>\n<td>\"cplx_reduction/range\"</td>\n<td>\"cplx_reduction/Sum\"</td>\n<td>\"Sum\"</td>\n</tr>\n</tbody>\n</table>\n<h3>GPU, Real Reduction</h3>\n<table>\n<thead>\n<tr>\n<th>Start</th>\n<th>Wall Duration (ms)</th>\n<th>Self time (ms)</th>\n<th>Arg: input0</th>\n<th>Arg: input1</th>\n<th>Arg: name</th>\n<th>Arg: op</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.087 ms</td>\n<td>1.267 ms</td>\n<td>1.267 ms</td>\n<td>\"real_var/read\"</td>\n<td>\"real_reduction_1/range\"</td>\n<td>\"real_reduction_1/Sum\"</td>\n<td>\"Sum\"</td>\n</tr>\n<tr>\n<td>1.355 ms</td>\n<td>0.043 ms</td>\n<td>0.043 ms</td>\n<td>undefined</td>\n<td>undefined</td>\n<td>\"real_reduction_1/Sum\"</td>\n<td>\"Sum\"</td>\n</tr>\n</tbody>\n</table>\n<h3>CPU, Real Reduction</h3>\n<table>\n<thead>\n<tr>\n<th>Start</th>\n<th>Wall Duration (ms)</th>\n<th>Self time (ms)</th>\n<th>Arg: input0</th>\n<th>Arg: input1</th>\n<th>Arg: name</th>\n<th>Arg: op</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>4.010 ms</td>\n<td>0.465 ms</td>\n<td>0.465 ms</td>\n<td>\"real_var/read/_7\"</td>\n<td>\"real_reduction/range\"</td>\n<td>\"real_reduction/Sum\"</td>\n<td>\"Sum\"</td>\n</tr>\n</tbody>\n</table>\n<h3>GPU, Complex Reduction (Slow)</h3>\n<table>\n<thead>\n<tr>\n<th>Start</th>\n<th>Wall Duration (ms)</th>\n<th>Self time (ms)</th>\n<th>Arg: input0</th>\n<th>Arg: input1</th>\n<th>Arg: name</th>\n<th>Arg: op</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>4.019 ms</td>\n<td>0.070 ms</td>\n<td>0.070 ms</td>\n<td>\"cplx_var/read/_5\"</td>\n<td>\"cplx_reduction_1/range\"</td>\n<td>\"cplx_reduction_1/Sum\"</td>\n<td>\"Sum\"</td>\n</tr>\n<tr>\n<td>4.078 ms</td>\n<td>195.994 ms</td>\n<td>195.994 ms</td>\n<td>undefined</td>\n<td>undefined</td>\n<td>\"cplx_reduction_1/Sum\"</td>\n<td>\"Sum\"</td>\n</tr>\n</tbody>\n</table>\n<p>The trace and a screenshot are attached as a text file and pdf file, respectively. The code that generated the trace is provided below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> tensorflow.python.client <span class=\"pl-k\">import</span> timeline\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">reduction_tester</span>(<span class=\"pl-smi\">real_var</span>, <span class=\"pl-smi\">cplx_var</span>):\n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>real_reduction<span class=\"pl-pds\">'</span></span>):\n        reduced_real <span class=\"pl-k\">=</span> tf.reduce_sum(real_var)\n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cplx_reduction<span class=\"pl-pds\">'</span></span>):\n        reduced_cplx <span class=\"pl-k\">=</span> tf.reduce_sum(cplx_var)\n    <span class=\"pl-k\">return</span> reduced_real, reduced_cplx\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-c1\">R_DTYPE</span> <span class=\"pl-k\">=</span> np.float32\n    <span class=\"pl-c1\">C_DTYPE</span> <span class=\"pl-k\">=</span> np.complex64\n    r <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2000</span>).astype(<span class=\"pl-c1\">R_DTYPE</span>)\n    i <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2000</span>).astype(<span class=\"pl-c1\">R_DTYPE</span>)\n    real_var <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>real_var<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>r,\n                               <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.as_dtype(<span class=\"pl-c1\">R_DTYPE</span>))\n    cplx_var <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cplx_var<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>r<span class=\"pl-k\">+</span><span class=\"pl-c1\">1<span class=\"pl-k\">j</span></span><span class=\"pl-k\">*</span>i,\n                               <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.as_dtype(<span class=\"pl-c1\">C_DTYPE</span>))\n\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu<span class=\"pl-pds\">'</span></span>):\n        real_reduc_cpu, cplx_reduc_cpu <span class=\"pl-k\">=</span> reduction_tester(real_var, cplx_var)\n\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu<span class=\"pl-pds\">'</span></span>):\n        real_reduc_gpu, cplx_reduc_gpu <span class=\"pl-k\">=</span> reduction_tester(real_var, cplx_var)\n\n    sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                            <span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n    init <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n    sess.run(init)\n\n    run_options <span class=\"pl-k\">=</span> tf.RunOptions(<span class=\"pl-v\">trace_level</span><span class=\"pl-k\">=</span>tf.RunOptions.<span class=\"pl-c1\">FULL_TRACE</span>)\n    run_metadata <span class=\"pl-k\">=</span> tf.RunMetadata()\n    ops <span class=\"pl-k\">=</span> [real_reduc_cpu, cplx_reduc_cpu, real_reduc_gpu, cplx_reduc_gpu]\n    ret <span class=\"pl-k\">=</span> sess.run(ops, <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>run_options, <span class=\"pl-v\">run_metadata</span><span class=\"pl-k\">=</span>run_metadata)\n\n    real_np_sum <span class=\"pl-k\">=</span> np.sum(r.astype(<span class=\"pl-c1\">R_DTYPE</span>))\n    cplx_np_sum <span class=\"pl-k\">=</span> np.sum((r<span class=\"pl-k\">+</span><span class=\"pl-c1\">1<span class=\"pl-k\">j</span></span><span class=\"pl-k\">*</span>i).astype(<span class=\"pl-c1\">C_DTYPE</span>))\n    <span class=\"pl-k\">assert</span> np.allclose(ret[<span class=\"pl-c1\">0</span>], real_np_sum, <span class=\"pl-v\">rtol</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n    <span class=\"pl-k\">assert</span> np.allclose(ret[<span class=\"pl-c1\">1</span>], cplx_np_sum, <span class=\"pl-v\">rtol</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n    <span class=\"pl-k\">assert</span> np.allclose(ret[<span class=\"pl-c1\">2</span>], real_np_sum, <span class=\"pl-v\">rtol</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n    <span class=\"pl-k\">assert</span> np.allclose(ret[<span class=\"pl-c1\">3</span>], cplx_np_sum, <span class=\"pl-v\">rtol</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n\n    tl <span class=\"pl-k\">=</span> timeline.Timeline(run_metadata.step_stats)\n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/home/sarroff/tmp/timeline.trace<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n        f.write(tl.generate_chrome_trace_format(<span class=\"pl-v\">show_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<h2>Partial Workaround</h2>\n<p>It's easy to avoid placing explicit calls to <code>tf.reduce_sum</code> by flattening the tensor into a row and performing a matrix multiply with a tensor column of 1s. The performance on complex tensors using the gpu is reasonable using this approach. However Tensorflow uses reductions internally for many gradient computations and these are harder to avoid.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/576345/timeline.trace.txt\">timeline.trace.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/576234/chrome_tracing.pdf\">chrome_tracing.pdf</a></p>", "body_text": "Description\ntf.reduce_sum takes an order of magnitude longer to compute when the tensor is a complex dtype and the computation takes place on a gpu. I've tried to investigate the source of the problem and it's not clear to me whether this is a Tensorflow issue or an Eigen issue, but I thought I'd raise it here first.\nEnvironment\nUbuntu 16.04 LTS\nTensorflow: 1fcd6d1\nGPU: Titan X\nPython 2.7.11\nDiagnostics/Reproducibility:\nI produced a tensorflow timeline trace. The trace shows that reductions for a real and complex tensor on the cpu, and for a real tensor on the gpu, takes no longer than 1.3 ms. The reduction of a complex tensor of the same size takes about 196 ms on the gpu.  In particular this time is spent in a dedicated GPU stream, where the input arguments are marked as undefined in the trace.\nMore specifically, here's a table summarizing the relevant parts of the trace:\nCPU, Complex Reduction\n\n\n\nStart\nWall Duration (ms)\nSelf time (ms)\nArg: input0\nArg: input1\nArg: name\nArg: op\n\n\n\n\n0.076 ms\n1.174 ms\n1.174 ms\n\"cplx_var/read\"\n\"cplx_reduction/range\"\n\"cplx_reduction/Sum\"\n\"Sum\"\n\n\n\nGPU, Real Reduction\n\n\n\nStart\nWall Duration (ms)\nSelf time (ms)\nArg: input0\nArg: input1\nArg: name\nArg: op\n\n\n\n\n0.087 ms\n1.267 ms\n1.267 ms\n\"real_var/read\"\n\"real_reduction_1/range\"\n\"real_reduction_1/Sum\"\n\"Sum\"\n\n\n1.355 ms\n0.043 ms\n0.043 ms\nundefined\nundefined\n\"real_reduction_1/Sum\"\n\"Sum\"\n\n\n\nCPU, Real Reduction\n\n\n\nStart\nWall Duration (ms)\nSelf time (ms)\nArg: input0\nArg: input1\nArg: name\nArg: op\n\n\n\n\n4.010 ms\n0.465 ms\n0.465 ms\n\"real_var/read/_7\"\n\"real_reduction/range\"\n\"real_reduction/Sum\"\n\"Sum\"\n\n\n\nGPU, Complex Reduction (Slow)\n\n\n\nStart\nWall Duration (ms)\nSelf time (ms)\nArg: input0\nArg: input1\nArg: name\nArg: op\n\n\n\n\n4.019 ms\n0.070 ms\n0.070 ms\n\"cplx_var/read/_5\"\n\"cplx_reduction_1/range\"\n\"cplx_reduction_1/Sum\"\n\"Sum\"\n\n\n4.078 ms\n195.994 ms\n195.994 ms\nundefined\nundefined\n\"cplx_reduction_1/Sum\"\n\"Sum\"\n\n\n\nThe trace and a screenshot are attached as a text file and pdf file, respectively. The code that generated the trace is provided below:\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.client import timeline\n\ndef reduction_tester(real_var, cplx_var):\n    with tf.name_scope('real_reduction'):\n        reduced_real = tf.reduce_sum(real_var)\n    with tf.name_scope('cplx_reduction'):\n        reduced_cplx = tf.reduce_sum(cplx_var)\n    return reduced_real, reduced_cplx\n\ndef main():\n    R_DTYPE = np.float32\n    C_DTYPE = np.complex64\n    r = np.random.randn(1000, 2000).astype(R_DTYPE)\n    i = np.random.randn(1000, 2000).astype(R_DTYPE)\n    real_var = tf.get_variable('real_var', initializer=r,\n                               dtype=tf.as_dtype(R_DTYPE))\n    cplx_var = tf.get_variable('cplx_var', initializer=r+1j*i,\n                               dtype=tf.as_dtype(C_DTYPE))\n\n    with tf.device('/cpu'):\n        real_reduc_cpu, cplx_reduc_cpu = reduction_tester(real_var, cplx_var)\n\n    with tf.device('/gpu'):\n        real_reduc_gpu, cplx_reduc_gpu = reduction_tester(real_var, cplx_var)\n\n    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True,\n                                            allow_soft_placement=False))\n    init = tf.initialize_all_variables()\n    sess.run(init)\n\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    ops = [real_reduc_cpu, cplx_reduc_cpu, real_reduc_gpu, cplx_reduc_gpu]\n    ret = sess.run(ops, options=run_options, run_metadata=run_metadata)\n\n    real_np_sum = np.sum(r.astype(R_DTYPE))\n    cplx_np_sum = np.sum((r+1j*i).astype(C_DTYPE))\n    assert np.allclose(ret[0], real_np_sum, rtol=1e-4)\n    assert np.allclose(ret[1], cplx_np_sum, rtol=1e-4)\n    assert np.allclose(ret[2], real_np_sum, rtol=1e-4)\n    assert np.allclose(ret[3], cplx_np_sum, rtol=1e-4)\n\n    tl = timeline.Timeline(run_metadata.step_stats)\n    with open('/home/sarroff/tmp/timeline.trace', 'w') as f:\n        f.write(tl.generate_chrome_trace_format(show_memory=True))\n\nif __name__ == '__main__':\n    main()\nPartial Workaround\nIt's easy to avoid placing explicit calls to tf.reduce_sum by flattening the tensor into a row and performing a matrix multiply with a tensor column of 1s. The performance on complex tensors using the gpu is reasonable using this approach. However Tensorflow uses reductions internally for many gradient computations and these are harder to avoid.\ntimeline.trace.txt\nchrome_tracing.pdf", "body": "## Description\r\n`tf.reduce_sum` takes an order of magnitude longer to compute when the tensor is a complex dtype and the computation takes place on a gpu. I've tried to investigate the source of the problem and it's not clear to me whether this is a Tensorflow issue or an Eigen issue, but I thought I'd raise it here first.\r\n\r\n## Environment\r\nUbuntu 16.04 LTS\r\nTensorflow: 1fcd6d1294564066c6f92b121a3aaf4ed186dc1a\r\nGPU: Titan X\r\nPython 2.7.11\r\n\r\n## Diagnostics/Reproducibility:\r\nI produced a tensorflow timeline trace. The trace shows that reductions for a real and complex tensor on the cpu, and for a real tensor on the gpu, takes no longer than 1.3 ms. The reduction of a complex tensor of the same size takes about 196 ms on the gpu.  In particular this time is spent in a dedicated GPU stream, where the input arguments are marked as undefined in the trace.\r\n\r\nMore specifically, here's a table summarizing the relevant parts of the trace:\r\n\r\n### CPU, Complex Reduction\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n0.076 ms | 1.174 ms | 1.174 ms | \"cplx_var/read\" | \"cplx_reduction/range\" | \"cplx_reduction/Sum\" | \"Sum\"\r\n\r\n### GPU, Real Reduction\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n0.087 ms | 1.267 ms | 1.267 ms | \"real_var/read\" | \"real_reduction_1/range\" | \"real_reduction_1/Sum\" | \"Sum\"\r\n1.355 ms | 0.043 ms | 0.043 ms | undefined | undefined | \"real_reduction_1/Sum\" | \"Sum\"\r\n\r\n### CPU, Real Reduction\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n4.010 ms | 0.465 ms | 0.465 ms | \"real_var/read/_7\" | \"real_reduction/range\" | \"real_reduction/Sum\" | \"Sum\"\r\n\r\n### GPU, Complex Reduction (Slow)\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n4.019 ms | 0.070 ms | 0.070 ms | \"cplx_var/read/_5\" | \"cplx_reduction_1/range\" | \"cplx_reduction_1/Sum\" | \"Sum\"\r\n4.078 ms | 195.994 ms | 195.994 ms | undefined | undefined | \"cplx_reduction_1/Sum\" | \"Sum\"\r\n\r\nThe trace and a screenshot are attached as a text file and pdf file, respectively. The code that generated the trace is provided below:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.client import timeline\r\n\r\ndef reduction_tester(real_var, cplx_var):\r\n    with tf.name_scope('real_reduction'):\r\n        reduced_real = tf.reduce_sum(real_var)\r\n    with tf.name_scope('cplx_reduction'):\r\n        reduced_cplx = tf.reduce_sum(cplx_var)\r\n    return reduced_real, reduced_cplx\r\n\r\ndef main():\r\n    R_DTYPE = np.float32\r\n    C_DTYPE = np.complex64\r\n    r = np.random.randn(1000, 2000).astype(R_DTYPE)\r\n    i = np.random.randn(1000, 2000).astype(R_DTYPE)\r\n    real_var = tf.get_variable('real_var', initializer=r,\r\n                               dtype=tf.as_dtype(R_DTYPE))\r\n    cplx_var = tf.get_variable('cplx_var', initializer=r+1j*i,\r\n                               dtype=tf.as_dtype(C_DTYPE))\r\n\r\n    with tf.device('/cpu'):\r\n        real_reduc_cpu, cplx_reduc_cpu = reduction_tester(real_var, cplx_var)\r\n\r\n    with tf.device('/gpu'):\r\n        real_reduc_gpu, cplx_reduc_gpu = reduction_tester(real_var, cplx_var)\r\n\r\n    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True,\r\n                                            allow_soft_placement=False))\r\n    init = tf.initialize_all_variables()\r\n    sess.run(init)\r\n\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    ops = [real_reduc_cpu, cplx_reduc_cpu, real_reduc_gpu, cplx_reduc_gpu]\r\n    ret = sess.run(ops, options=run_options, run_metadata=run_metadata)\r\n\r\n    real_np_sum = np.sum(r.astype(R_DTYPE))\r\n    cplx_np_sum = np.sum((r+1j*i).astype(C_DTYPE))\r\n    assert np.allclose(ret[0], real_np_sum, rtol=1e-4)\r\n    assert np.allclose(ret[1], cplx_np_sum, rtol=1e-4)\r\n    assert np.allclose(ret[2], real_np_sum, rtol=1e-4)\r\n    assert np.allclose(ret[3], cplx_np_sum, rtol=1e-4)\r\n\r\n    tl = timeline.Timeline(run_metadata.step_stats)\r\n    with open('/home/sarroff/tmp/timeline.trace', 'w') as f:\r\n        f.write(tl.generate_chrome_trace_format(show_memory=True))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n## Partial Workaround\r\nIt's easy to avoid placing explicit calls to `tf.reduce_sum` by flattening the tensor into a row and performing a matrix multiply with a tensor column of 1s. The performance on complex tensors using the gpu is reasonable using this approach. However Tensorflow uses reductions internally for many gradient computations and these are harder to avoid.\r\n\r\n[timeline.trace.txt](https://github.com/tensorflow/tensorflow/files/576345/timeline.trace.txt)\r\n[chrome_tracing.pdf](https://github.com/tensorflow/tensorflow/files/576234/chrome_tracing.pdf)\r\n"}