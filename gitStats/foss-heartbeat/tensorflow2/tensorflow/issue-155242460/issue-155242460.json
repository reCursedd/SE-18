{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2403", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2403/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2403/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2403/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2403", "id": 155242460, "node_id": "MDU6SXNzdWUxNTUyNDI0NjA=", "number": 2403, "title": "\"Total error: nan\" for neural network model", "user": {"login": "tu-lee", "id": 18699458, "node_id": "MDQ6VXNlcjE4Njk5NDU4", "avatar_url": "https://avatars0.githubusercontent.com/u/18699458?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tu-lee", "html_url": "https://github.com/tu-lee", "followers_url": "https://api.github.com/users/tu-lee/followers", "following_url": "https://api.github.com/users/tu-lee/following{/other_user}", "gists_url": "https://api.github.com/users/tu-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/tu-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tu-lee/subscriptions", "organizations_url": "https://api.github.com/users/tu-lee/orgs", "repos_url": "https://api.github.com/users/tu-lee/repos", "events_url": "https://api.github.com/users/tu-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/tu-lee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-05-17T12:02:25Z", "updated_at": "2016-05-17T14:07:03Z", "closed_at": "2016-05-17T14:07:03Z", "author_association": "NONE", "body_html": "<p>Hi</p>\n<p>I have randomly generated exponential signal. I am training the network to predict y values depending on the random time constant (tau). I have added one hidden layer and used gardient descent for optimization. The code runs fine, but it doesnt print error properly. I get this - ('Total Error: ', nan)</p>\n<p>here is my code-</p>\n<p>`import tensorflow as tf<br>\nimport numpy as np<br>\nimport random</p>\n<h1>input data</h1>\n<p>lorange= 1<br>\nhirange= 10<br>\namplitude= random.uniform(-10,10)<br>\nt= 10<br>\nrandom.seed()<br>\ntau=random.uniform(lorange,hirange)</p>\n<p>def generate_data(randomsignal):<br>\nx= np.arange(t)<br>\ny= amplitude*np.exp(-x/tau)<br>\nreturn x, y<br>\n#tensors for input data</p>\n<p>x_input= tf.placeholder(tf.float32, shape=(10,))# t=10<br>\ny_input= tf.placeholder(tf.float32, shape=(10,))</p>\n<p>#use 10 neurons-- just one layer for now</p>\n<p>weights_1= tf.Variable(tf.truncated_normal([10,10], stddev= .1))<br>\nbias_1= tf.Variable(.1)</p>\n<p>#hidden output<br>\nhidden_output= tf.nn.relu(tf.matmul(tf.reshape(x_input,[1,10]), weights_1) + bias_1)</p>\n<p>weights_2 = tf.Variable(tf.truncated_normal([10, 10], stddev=.1))<br>\nbias_2= tf.Variable(.1)</p>\n<p>calculated_output = tf.nn.softmax(tf.matmul(hidden_output, weights_2) +</p>\n<p>bias_2)</p>\n<p>cross_entropy = tf.reduce_mean(y_input * tf.log(calculated_output))</p>\n<p>optimizer = tf.train.GradientDescentOptimizer(.5).minimize(cross_entropy)</p>\n<p>sess = tf.Session()<br>\n#session<br>\nsess.run(tf.initialize_all_variables())</p>\n<p>for i in range(1000):<br>\nx, y = generate_data(100)<br>\nsess.run(optimizer, feed_dict={x_input: x, y_input: y})</p>\n<p>error = tf.reduce_sum(tf.abs(calculated_output - y_input))</p>\n<p>x, y = generate_data(100)<br>\nprint(\"Total Error: \", sess.run(error, feed_dict={x_input: x, y_input: y}))`</p>", "body_text": "Hi\nI have randomly generated exponential signal. I am training the network to predict y values depending on the random time constant (tau). I have added one hidden layer and used gardient descent for optimization. The code runs fine, but it doesnt print error properly. I get this - ('Total Error: ', nan)\nhere is my code-\n`import tensorflow as tf\nimport numpy as np\nimport random\ninput data\nlorange= 1\nhirange= 10\namplitude= random.uniform(-10,10)\nt= 10\nrandom.seed()\ntau=random.uniform(lorange,hirange)\ndef generate_data(randomsignal):\nx= np.arange(t)\ny= amplitude*np.exp(-x/tau)\nreturn x, y\n#tensors for input data\nx_input= tf.placeholder(tf.float32, shape=(10,))# t=10\ny_input= tf.placeholder(tf.float32, shape=(10,))\n#use 10 neurons-- just one layer for now\nweights_1= tf.Variable(tf.truncated_normal([10,10], stddev= .1))\nbias_1= tf.Variable(.1)\n#hidden output\nhidden_output= tf.nn.relu(tf.matmul(tf.reshape(x_input,[1,10]), weights_1) + bias_1)\nweights_2 = tf.Variable(tf.truncated_normal([10, 10], stddev=.1))\nbias_2= tf.Variable(.1)\ncalculated_output = tf.nn.softmax(tf.matmul(hidden_output, weights_2) +\nbias_2)\ncross_entropy = tf.reduce_mean(y_input * tf.log(calculated_output))\noptimizer = tf.train.GradientDescentOptimizer(.5).minimize(cross_entropy)\nsess = tf.Session()\n#session\nsess.run(tf.initialize_all_variables())\nfor i in range(1000):\nx, y = generate_data(100)\nsess.run(optimizer, feed_dict={x_input: x, y_input: y})\nerror = tf.reduce_sum(tf.abs(calculated_output - y_input))\nx, y = generate_data(100)\nprint(\"Total Error: \", sess.run(error, feed_dict={x_input: x, y_input: y}))`", "body": "Hi \n\nI have randomly generated exponential signal. I am training the network to predict y values depending on the random time constant (tau). I have added one hidden layer and used gardient descent for optimization. The code runs fine, but it doesnt print error properly. I get this - ('Total Error: ', nan)\n\nhere is my code-\n\n`import tensorflow as tf\n import numpy as np\n import random\n# input data\n\n lorange= 1\n hirange= 10\n amplitude= random.uniform(-10,10)\n t= 10\n random.seed()\n tau=random.uniform(lorange,hirange)\n\n def generate_data(randomsignal):\n       x= np.arange(t)\n       y= amplitude*np.exp(-x/tau)\n       return x, y\n #tensors for input data\n\n x_input= tf.placeholder(tf.float32, shape=(10,))# t=10\n y_input= tf.placeholder(tf.float32, shape=(10,))\n\n #use 10 neurons-- just one layer for now\n\n weights_1= tf.Variable(tf.truncated_normal([10,10], stddev= .1)) \n bias_1= tf.Variable(.1)\n\n #hidden output \n hidden_output= tf.nn.relu(tf.matmul(tf.reshape(x_input,[1,10]), weights_1) + bias_1)\n\n weights_2 = tf.Variable(tf.truncated_normal([10, 10], stddev=.1))\n bias_2= tf.Variable(.1)\n\n calculated_output = tf.nn.softmax(tf.matmul(hidden_output, weights_2) + \n\n bias_2)\n\n cross_entropy = tf.reduce_mean(y_input \\* tf.log(calculated_output))\n\n optimizer = tf.train.GradientDescentOptimizer(.5).minimize(cross_entropy)\n\n sess = tf.Session()\n #session\n sess.run(tf.initialize_all_variables())\n\n for i in range(1000):\n      x, y = generate_data(100)\n      sess.run(optimizer, feed_dict={x_input: x, y_input: y})\n\n error = tf.reduce_sum(tf.abs(calculated_output - y_input))\n\n x, y = generate_data(100)\n print(\"Total Error: \", sess.run(error, feed_dict={x_input: x, y_input: y}))`\n"}