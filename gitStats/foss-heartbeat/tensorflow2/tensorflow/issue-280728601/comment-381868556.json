{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/381868556", "html_url": "https://github.com/tensorflow/tensorflow/issues/15237#issuecomment-381868556", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15237", "id": 381868556, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTg2ODU1Ng==", "user": {"login": "PhilJd", "id": 16101605, "node_id": "MDQ6VXNlcjE2MTAxNjA1", "avatar_url": "https://avatars2.githubusercontent.com/u/16101605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilJd", "html_url": "https://github.com/PhilJd", "followers_url": "https://api.github.com/users/PhilJd/followers", "following_url": "https://api.github.com/users/PhilJd/following{/other_user}", "gists_url": "https://api.github.com/users/PhilJd/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilJd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilJd/subscriptions", "organizations_url": "https://api.github.com/users/PhilJd/orgs", "repos_url": "https://api.github.com/users/PhilJd/repos", "events_url": "https://api.github.com/users/PhilJd/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilJd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-17T06:52:17Z", "updated_at": "2018-04-17T06:52:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1112263\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/facaiy\">@facaiy</a><br>\nI used the fact that the apply_ functions already get the precomputed gradient and Adam + Momentum optimizers don't compute values based on <code>var</code>, so it's equivalent to pre-decay. However, I just checked the apply functions of other optimizers and e.g. <code>Ftrl</code> computes factors based on <code>var</code>, so decoupling decay for such optimizers would indeed need a custom op. I'll leave a comment in the PR.</p>", "body_text": "@facaiy\nI used the fact that the apply_ functions already get the precomputed gradient and Adam + Momentum optimizers don't compute values based on var, so it's equivalent to pre-decay. However, I just checked the apply functions of other optimizers and e.g. Ftrl computes factors based on var, so decoupling decay for such optimizers would indeed need a custom op. I'll leave a comment in the PR.", "body": "@facaiy \r\nI used the fact that the apply_ functions already get the precomputed gradient and Adam + Momentum optimizers don't compute values based on `var`, so it's equivalent to pre-decay. However, I just checked the apply functions of other optimizers and e.g. `Ftrl` computes factors based on `var`, so decoupling decay for such optimizers would indeed need a custom op. I'll leave a comment in the PR.\r\n"}