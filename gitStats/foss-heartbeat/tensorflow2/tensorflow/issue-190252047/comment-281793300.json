{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281793300", "html_url": "https://github.com/tensorflow/tensorflow/issues/5688#issuecomment-281793300", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5688", "id": 281793300, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTc5MzMwMA==", "user": {"login": "mrajchl", "id": 11459497, "node_id": "MDQ6VXNlcjExNDU5NDk3", "avatar_url": "https://avatars2.githubusercontent.com/u/11459497?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrajchl", "html_url": "https://github.com/mrajchl", "followers_url": "https://api.github.com/users/mrajchl/followers", "following_url": "https://api.github.com/users/mrajchl/following{/other_user}", "gists_url": "https://api.github.com/users/mrajchl/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrajchl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrajchl/subscriptions", "organizations_url": "https://api.github.com/users/mrajchl/orgs", "repos_url": "https://api.github.com/users/mrajchl/repos", "events_url": "https://api.github.com/users/mrajchl/events{/privacy}", "received_events_url": "https://api.github.com/users/mrajchl/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-22T20:30:32Z", "updated_at": "2017-02-22T20:30:32Z", "author_association": "NONE", "body_html": "<p>Same here. I can reproduce this issue with the example code above. Can confirm CUDA 8, CUDNN 5.1 with tf 1.0.</p>\n<p>I ran into an interesting aspect when determining the maximum size of the tensor. It run when tensors with shape less than [1, 256, 256, 256,1] are used. The example below will run and sz=256 will break.</p>\n<p>Is anyone looking into this?</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\ngraph = tf.Graph()\n\nsz = 255\n\nwith graph.as_default():\n    tf_dataset = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\n    tf_label = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\n\n    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))\n    layer1_bias = tf.Variable(tf.zeros(1))\n\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\n    logits = tf.nn.relu(conv+layer1_bias)\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_label))\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    batchData = np.random.rand(1, sz, sz, sz, 1).astype(np.float32)\n    batchLabels = (np.random.rand(1, sz, sz, sz, 1)&gt;0.5).astype(np.float32)\n    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\n    _ = session.run((optimizer, ), feed_dict=feed_dict)\n</code></pre>", "body_text": "Same here. I can reproduce this issue with the example code above. Can confirm CUDA 8, CUDNN 5.1 with tf 1.0.\nI ran into an interesting aspect when determining the maximum size of the tensor. It run when tensors with shape less than [1, 256, 256, 256,1] are used. The example below will run and sz=256 will break.\nIs anyone looking into this?\nimport numpy as np\nimport tensorflow as tf\n\ngraph = tf.Graph()\n\nsz = 255\n\nwith graph.as_default():\n    tf_dataset = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\n    tf_label = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\n\n    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))\n    layer1_bias = tf.Variable(tf.zeros(1))\n\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\n    logits = tf.nn.relu(conv+layer1_bias)\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_label))\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    batchData = np.random.rand(1, sz, sz, sz, 1).astype(np.float32)\n    batchLabels = (np.random.rand(1, sz, sz, sz, 1)>0.5).astype(np.float32)\n    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\n    _ = session.run((optimizer, ), feed_dict=feed_dict)", "body": "Same here. I can reproduce this issue with the example code above. Can confirm CUDA 8, CUDNN 5.1 with tf 1.0. \r\n\r\nI ran into an interesting aspect when determining the maximum size of the tensor. It run when tensors with shape less than [1, 256, 256, 256,1] are used. The example below will run and sz=256 will break. \r\n\r\nIs anyone looking into this?\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\n\r\nsz = 255\r\n\r\nwith graph.as_default():\r\n    tf_dataset = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\r\n    tf_label = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\r\n\r\n    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))\r\n    layer1_bias = tf.Variable(tf.zeros(1))\r\n\r\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\r\n    logits = tf.nn.relu(conv+layer1_bias)\r\n\r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_label))\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n\r\nwith tf.Session(graph=graph) as session:\r\n    tf.initialize_all_variables().run()\r\n    batchData = np.random.rand(1, sz, sz, sz, 1).astype(np.float32)\r\n    batchLabels = (np.random.rand(1, sz, sz, sz, 1)>0.5).astype(np.float32)\r\n    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\r\n    _ = session.run((optimizer, ), feed_dict=feed_dict)\r\n```"}