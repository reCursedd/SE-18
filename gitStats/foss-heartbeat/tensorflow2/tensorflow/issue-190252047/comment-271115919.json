{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271115919", "html_url": "https://github.com/tensorflow/tensorflow/issues/5688#issuecomment-271115919", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5688", "id": 271115919, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTExNTkxOQ==", "user": {"login": "redst4r", "id": 5718498, "node_id": "MDQ6VXNlcjU3MTg0OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5718498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/redst4r", "html_url": "https://github.com/redst4r", "followers_url": "https://api.github.com/users/redst4r/followers", "following_url": "https://api.github.com/users/redst4r/following{/other_user}", "gists_url": "https://api.github.com/users/redst4r/gists{/gist_id}", "starred_url": "https://api.github.com/users/redst4r/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/redst4r/subscriptions", "organizations_url": "https://api.github.com/users/redst4r/orgs", "repos_url": "https://api.github.com/users/redst4r/repos", "events_url": "https://api.github.com/users/redst4r/events{/privacy}", "received_events_url": "https://api.github.com/users/redst4r/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-07T22:54:00Z", "updated_at": "2017-01-08T11:01:33Z", "author_association": "NONE", "body_html": "<p>can confirm a similar issue with large 3D convolutions:</p>\n<div class=\"highlight highlight-source-python\"><pre>inshape <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">258</span>,<span class=\"pl-c1\">258</span>,<span class=\"pl-c1\">34</span>,<span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> tf img-order</span>\nfilter_shape <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">32</span>)\nx <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>inshape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>X<span class=\"pl-pds\">'</span></span>)\nf <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>filter_shape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>filter<span class=\"pl-pds\">'</span></span>)\nc <span class=\"pl-k\">=</span> tf.nn.conv3d(x, f, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>])\ngrads <span class=\"pl-k\">=</span> tf.gradients(c,f)[<span class=\"pl-c1\">0</span>]\n\nxx <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-k\">*</span>inshape)\nff <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-k\">*</span>filter_shape)\n\n<span class=\"pl-k\">with</span> tf.Session().as_default():\n    Q <span class=\"pl-k\">=</span> c.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x:xx, f:ff})\n    gradQ <span class=\"pl-k\">=</span> grads.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x:xx, f:ff})</pre></div>\n<p>will yield <code>tensorflow/stream_executor/cuda/cuda_dnn.cc:2674] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED</code>.<br>\nIt runs fine if I set <code>inshape = (2,257,257,34,1)</code> instead, so there's some issue with the sizes. However I dont think its the usual GPU-out-of-memory issue:</p>\n<ol>\n<li>the same convolution runs perfectly fine in theano (there I can even increase to <code>(2,300,300,34,1)</code> without trouble)</li>\n<li>it doesnt respond to all dimension the same: decreasing the batchsize from 2-&gt;1 will still crash even though we just halfed the input</li>\n</ol>\n<p>Turns out that it's somehow <strong>related to the gradient computations</strong>, i.e. commenting the <code>gradQ =...</code> line, just evaluating the result of the convolutions works!</p>\n<p>Code was run on tensorflow 0.11.0rc2 (same happens for 0.12.1), Titan X, CUDA-8.0, cudnn 5</p>", "body_text": "can confirm a similar issue with large 3D convolutions:\ninshape = (2,258,258,34,1)  # tf img-order\nfilter_shape = (3,3,3,1,32)\nx = tf.placeholder('float32', shape=inshape, name='X')\nf = tf.placeholder('float32', shape=filter_shape, name='filter')\nc = tf.nn.conv3d(x, f, padding='VALID', strides=[1,1,1,1,1])\ngrads = tf.gradients(c,f)[0]\n\nxx = np.random.rand(*inshape)\nff = np.random.rand(*filter_shape)\n\nwith tf.Session().as_default():\n    Q = c.eval(feed_dict={x:xx, f:ff})\n    gradQ = grads.eval(feed_dict={x:xx, f:ff})\nwill yield tensorflow/stream_executor/cuda/cuda_dnn.cc:2674] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED.\nIt runs fine if I set inshape = (2,257,257,34,1) instead, so there's some issue with the sizes. However I dont think its the usual GPU-out-of-memory issue:\n\nthe same convolution runs perfectly fine in theano (there I can even increase to (2,300,300,34,1) without trouble)\nit doesnt respond to all dimension the same: decreasing the batchsize from 2->1 will still crash even though we just halfed the input\n\nTurns out that it's somehow related to the gradient computations, i.e. commenting the gradQ =... line, just evaluating the result of the convolutions works!\nCode was run on tensorflow 0.11.0rc2 (same happens for 0.12.1), Titan X, CUDA-8.0, cudnn 5", "body": "can confirm a similar issue with large 3D convolutions:\r\n\r\n```python\r\ninshape = (2,258,258,34,1)  # tf img-order\r\nfilter_shape = (3,3,3,1,32)\r\nx = tf.placeholder('float32', shape=inshape, name='X')\r\nf = tf.placeholder('float32', shape=filter_shape, name='filter')\r\nc = tf.nn.conv3d(x, f, padding='VALID', strides=[1,1,1,1,1])\r\ngrads = tf.gradients(c,f)[0]\r\n\r\nxx = np.random.rand(*inshape)\r\nff = np.random.rand(*filter_shape)\r\n\r\nwith tf.Session().as_default():\r\n    Q = c.eval(feed_dict={x:xx, f:ff})\r\n    gradQ = grads.eval(feed_dict={x:xx, f:ff})\r\n```\r\nwill yield `tensorflow/stream_executor/cuda/cuda_dnn.cc:2674] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED`.\r\nIt runs fine if I set `inshape = (2,257,257,34,1)` instead, so there's some issue with the sizes. However I dont think its the usual GPU-out-of-memory issue:\r\n1. the same convolution runs perfectly fine in theano (there I can even increase to `(2,300,300,34,1)` without trouble)\r\n2. it doesnt respond to all dimension the same: decreasing the batchsize from 2->1 will still crash even though we just halfed the input\r\n\r\nTurns out that it's somehow **related to the gradient computations**, i.e. commenting the `gradQ =...` line, just evaluating the result of the convolutions works!\r\n\r\nCode was run on tensorflow 0.11.0rc2 (same happens for 0.12.1), Titan X, CUDA-8.0, cudnn 5\r\n\r\n\r\n\r\n"}