{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17748", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17748/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17748/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17748/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17748", "id": 305748337, "node_id": "MDU6SXNzdWUzMDU3NDgzMzc=", "number": 17748, "title": "Inability to get tensorflow output from custom_estimator", "user": {"login": "grusovin", "id": 35687701, "node_id": "MDQ6VXNlcjM1Njg3NzAx", "avatar_url": "https://avatars2.githubusercontent.com/u/35687701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/grusovin", "html_url": "https://github.com/grusovin", "followers_url": "https://api.github.com/users/grusovin/followers", "following_url": "https://api.github.com/users/grusovin/following{/other_user}", "gists_url": "https://api.github.com/users/grusovin/gists{/gist_id}", "starred_url": "https://api.github.com/users/grusovin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/grusovin/subscriptions", "organizations_url": "https://api.github.com/users/grusovin/orgs", "repos_url": "https://api.github.com/users/grusovin/repos", "events_url": "https://api.github.com/users/grusovin/events{/privacy}", "received_events_url": "https://api.github.com/users/grusovin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-03-15T23:00:47Z", "updated_at": "2018-06-02T00:07:55Z", "closed_at": "2018-06-02T00:07:55Z", "author_association": "NONE", "body_html": "<p>For this costum_estimator, the related documentation on <a href=\"https://www.tensorflow.org/get_started/custom_estimators\" rel=\"nofollow\">https://www.tensorflow.org/get_started/custom_estimators</a> tells me that I can just input tensorboard --logdir=PATH in the directory and obtain TensorGraph visualisation.  However I do this and I get on the Tensorboard page \"Graph visualisation failed: the graph is empty. Make sure that the graph is passed to the tf.summary.filewriter after the graph is defined\"<br>\nAs far as I understood, when using custom estimators it automatically passes the data to TensorBoard. What shall I do? I would like to get the basic graphs as shown at the bottom of the tutorial I linked above.</p>\n<p>`import pandas as pd<br>\nimport numpy as np<br>\nimport matplotlib.pyplot as plt<br>\nimport tensorflow as tf<br>\nimport argparse</p>\n<p>def train_input_fn(features, labels, batch_size):<br>\n\"\"\"An input function for training\"\"\"<br>\n# Convert the inputs to a Dataset.<br>\ndataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))</p>\n<pre><code># Shuffle, repeat, and batch the examples.\ndataset = dataset.shuffle(buffer_size=1000).repeat(count=None).batch(batch_size)\n\n# Return the dataset.\nreturn dataset\n</code></pre>\n<p>def eval_input_fn(features, labels, batch_size):<br>\n\"\"\"An input function for evaluation or prediction\"\"\"<br>\nfeatures=dict(features)<br>\nif labels is None:<br>\n# No labels, use only features.<br>\ninputs = features<br>\nelse:<br>\ninputs = (features, labels)</p>\n<pre><code># Convert the inputs to a Dataset.\ndataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n# Batch the examples\nassert batch_size is not None, \"batch_size must not be None\"\ndataset = dataset.batch(batch_size)\n\n# Return the dataset.\nreturn dataset\n</code></pre>\n<p>def datasetfun(dataframe,split1,split2):</p>\n<pre><code>#split1: variable deciding the proportion of test data to training data\n#split2: variable deciding the proportion of test data to training data\n\ndataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\n#dataframe = dataframe.drop([\"dzclass\",\"num.co\",\"edu\",\"income\",\"scoma\",\"charges\",\"totcst\",\"totmcst\",\"avtisst\",\"race\",\"meanbp\",\"wblc\",\"hrt\",\"resp\",\"temp\",\"pafi\",\"alb\",\"bili\",\"crea\",\"sod\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\",\"adlsc\"], axis=1) # Remove columns we don't care about\ndataframe = dataframe.drop([\"crea\",\"avtisst\",\"wblc\",\"race\",\"edu\",\"income\",\"charges\",\"totcst\",\"totmcst\",\"pafi\",\"alb\",\"bili\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\"], axis=1)\ntrain = dataframe[0:split1] # use first 15 rows as training set\ntest = dataframe[split1:split2] # keep some as testing\npredictdata = dataframe[split2:] # find the ones to predict\n\ntrain_features, train_labels = train, train.pop(\"death\") #separate features with classification in train set\ntest_features, test_labels = test, test.pop(\"death\") #separate features with classification in train set\npredict_features, predict_labels = predictdata, predictdata.pop(\"death\")\n\ncategorical_column1 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"sex\", vocabulary_list=[\"male\", \"female\"], default_value=0)\ncategorical_column2 = tf.feature_column.categorical_column_with_vocabulary_list(key='dzgroup',vocabulary_list=[\"Lung Cancer\",\"Colon Cancer\",\"ARF/MOSF w/Sepsis\",\"MOSF w/Malig\",\"Cirrhosi\",\"CHF\"])\ncategorical_column3 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"dzclass\", vocabulary_list=[\"Cancer\", \"ARF/MOSF\",\"COPD/CHF/Cirrhosis\"], default_value=0)\n#categorical_column4 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"race\", vocabulary_list=[\"black\", \"hispanic\",\"White\",\"asian\",\"other\"], default_value=0)\n\nmy_feature_columns = [\ntf.feature_column.numeric_column(key='age'),\ntf.feature_column.indicator_column(categorical_column1),\ntf.feature_column.numeric_column(key='hospdead'),\ntf.feature_column.numeric_column(key='slos'),\ntf.feature_column.numeric_column(key='d.time'),\ntf.feature_column.indicator_column(categorical_column2),\ntf.feature_column.indicator_column(categorical_column3),\ntf.feature_column.numeric_column(key='scoma'),\n#tf.feature_column.numeric_column(key='avtisst'),\n#tf.feature_column.indicator_column(categorical_column4),\ntf.feature_column.numeric_column(key='meanbp'),\ntf.feature_column.numeric_column(key='hrt'),\ntf.feature_column.numeric_column(key='resp'),\ntf.feature_column.numeric_column(key='temp'),\n#tf.feature_column.numeric_column(key='crea'),\ntf.feature_column.numeric_column(key='sod'),\ntf.feature_column.numeric_column(key='adlsc')]\n\nreturn (train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns)\n</code></pre>\n<p>def my_model(features, labels, mode, params):<br>\n\"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"<br>\n# Create three fully connected layers each layer having a dropout<br>\n# probability of 0.1.<br>\nnet = tf.feature_column.input_layer(features, params['feature_columns']) #imput layer<br>\nfor units in params['hidden_units']: #can change type of layers!!!!<br>\nnet = tf.layers.dense(net, units=units, activation=tf.nn.relu) #can change activation function to sigmoid!!!</p>\n<pre><code># Compute logits (1 per class).\n\n#output layer\nlogits = tf.layers.dense(net, params['n_classes'], activation=None)\n\n# Compute predictions.\npredicted_classes = tf.argmax(logits, 1)\nif mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {\n        'class_ids': predicted_classes[:, tf.newaxis],\n        'probabilities': tf.nn.softmax(logits),\n        'logits': logits,\n    }\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n# Compute loss.\nloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n# Compute evaluation metrics!!!!!!!\naccuracy = tf.metrics.accuracy(labels=labels,\n                               predictions=predicted_classes,\n                               name='acc_op')\nmetrics = {'accuracy': accuracy}\ntf.summary.scalar('accuracy', accuracy[1])\n\nif mode == tf.estimator.ModeKeys.EVAL:\n    return tf.estimator.EstimatorSpec(\n        mode, loss=loss, eval_metric_ops=metrics)\n\n# Create training op.\nassert mode == tf.estimator.ModeKeys.TRAIN\n\noptimizer = tf.train.AdagradOptimizer(learning_rate=0.1) #can change optimizer\ntrain_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\nreturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n</code></pre>\n<p>def main(m):</p>\n<pre><code>#classification of death or no death\n\ndataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\n\nsplit1=500\nsplit2=980\n\n[train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns] = datasetfun(dataframe,split1,split2)\n\nclassifier = tf.estimator.Estimator(\n    model_fn=my_model,\n    params={\n        'feature_columns': my_feature_columns,\n        # Two hidden layers of 10 nodes each.\n        'hidden_units': [10, 10],\n        # The model must choose between 3 classes.\n        'n_classes': 2,\n    })\n\n# classifier = tf.estimator.DNNClassifier(\n# \tfeature_columns=my_feature_columns,\n# \thidden_units=[10,10,5,6,7],\n# \tn_classes=2)\n\nbatch_size=100\ntrain_steps=1000\n\n#writer = tf.summary.FileWriter(\"/Users/angelicagrusovin/documents/oxford/MLSP\",graph=tf.get_default_graph())\n#writer.add_graph(sess.graph)\n\nclassifier.train(input_fn=lambda:train_input_fn(train_features, train_labels, batch_size),steps=train_steps)\n\neval_result = classifier.evaluate(\ninput_fn=lambda:eval_input_fn(test_features, test_labels, batch_size))\n\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n\n#predictions = classifier.predict(\n#input_fn=lambda:eval_input_fn(predict_features,None,batch_size))\n\n# expected=['dead','alive']\n# STATE=['dead','alive']\n# print(zip(predictions))\n\n# template = ('\\nPrediction is ({:.1f}%), expected {}')\n\n# for pred_dict in zip(predictions):\n# \tclass_id = pred_dict[0]['class_ids'][0]\n# \tprobability = pred_dict[0]['probabilities'][class_id]\n# \tprint(template.format(100 * probability, STATE[class_id]))\n\n\n\n\n\n\n#print(my_feature_columns) \n\nreturn m\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nmain(10)`</p>", "body_text": "For this costum_estimator, the related documentation on https://www.tensorflow.org/get_started/custom_estimators tells me that I can just input tensorboard --logdir=PATH in the directory and obtain TensorGraph visualisation.  However I do this and I get on the Tensorboard page \"Graph visualisation failed: the graph is empty. Make sure that the graph is passed to the tf.summary.filewriter after the graph is defined\"\nAs far as I understood, when using custom estimators it automatically passes the data to TensorBoard. What shall I do? I would like to get the basic graphs as shown at the bottom of the tutorial I linked above.\n`import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport argparse\ndef train_input_fn(features, labels, batch_size):\n\"\"\"An input function for training\"\"\"\n# Convert the inputs to a Dataset.\ndataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n# Shuffle, repeat, and batch the examples.\ndataset = dataset.shuffle(buffer_size=1000).repeat(count=None).batch(batch_size)\n\n# Return the dataset.\nreturn dataset\n\ndef eval_input_fn(features, labels, batch_size):\n\"\"\"An input function for evaluation or prediction\"\"\"\nfeatures=dict(features)\nif labels is None:\n# No labels, use only features.\ninputs = features\nelse:\ninputs = (features, labels)\n# Convert the inputs to a Dataset.\ndataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n# Batch the examples\nassert batch_size is not None, \"batch_size must not be None\"\ndataset = dataset.batch(batch_size)\n\n# Return the dataset.\nreturn dataset\n\ndef datasetfun(dataframe,split1,split2):\n#split1: variable deciding the proportion of test data to training data\n#split2: variable deciding the proportion of test data to training data\n\ndataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\n#dataframe = dataframe.drop([\"dzclass\",\"num.co\",\"edu\",\"income\",\"scoma\",\"charges\",\"totcst\",\"totmcst\",\"avtisst\",\"race\",\"meanbp\",\"wblc\",\"hrt\",\"resp\",\"temp\",\"pafi\",\"alb\",\"bili\",\"crea\",\"sod\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\",\"adlsc\"], axis=1) # Remove columns we don't care about\ndataframe = dataframe.drop([\"crea\",\"avtisst\",\"wblc\",\"race\",\"edu\",\"income\",\"charges\",\"totcst\",\"totmcst\",\"pafi\",\"alb\",\"bili\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\"], axis=1)\ntrain = dataframe[0:split1] # use first 15 rows as training set\ntest = dataframe[split1:split2] # keep some as testing\npredictdata = dataframe[split2:] # find the ones to predict\n\ntrain_features, train_labels = train, train.pop(\"death\") #separate features with classification in train set\ntest_features, test_labels = test, test.pop(\"death\") #separate features with classification in train set\npredict_features, predict_labels = predictdata, predictdata.pop(\"death\")\n\ncategorical_column1 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"sex\", vocabulary_list=[\"male\", \"female\"], default_value=0)\ncategorical_column2 = tf.feature_column.categorical_column_with_vocabulary_list(key='dzgroup',vocabulary_list=[\"Lung Cancer\",\"Colon Cancer\",\"ARF/MOSF w/Sepsis\",\"MOSF w/Malig\",\"Cirrhosi\",\"CHF\"])\ncategorical_column3 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"dzclass\", vocabulary_list=[\"Cancer\", \"ARF/MOSF\",\"COPD/CHF/Cirrhosis\"], default_value=0)\n#categorical_column4 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"race\", vocabulary_list=[\"black\", \"hispanic\",\"White\",\"asian\",\"other\"], default_value=0)\n\nmy_feature_columns = [\ntf.feature_column.numeric_column(key='age'),\ntf.feature_column.indicator_column(categorical_column1),\ntf.feature_column.numeric_column(key='hospdead'),\ntf.feature_column.numeric_column(key='slos'),\ntf.feature_column.numeric_column(key='d.time'),\ntf.feature_column.indicator_column(categorical_column2),\ntf.feature_column.indicator_column(categorical_column3),\ntf.feature_column.numeric_column(key='scoma'),\n#tf.feature_column.numeric_column(key='avtisst'),\n#tf.feature_column.indicator_column(categorical_column4),\ntf.feature_column.numeric_column(key='meanbp'),\ntf.feature_column.numeric_column(key='hrt'),\ntf.feature_column.numeric_column(key='resp'),\ntf.feature_column.numeric_column(key='temp'),\n#tf.feature_column.numeric_column(key='crea'),\ntf.feature_column.numeric_column(key='sod'),\ntf.feature_column.numeric_column(key='adlsc')]\n\nreturn (train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns)\n\ndef my_model(features, labels, mode, params):\n\"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\n# Create three fully connected layers each layer having a dropout\n# probability of 0.1.\nnet = tf.feature_column.input_layer(features, params['feature_columns']) #imput layer\nfor units in params['hidden_units']: #can change type of layers!!!!\nnet = tf.layers.dense(net, units=units, activation=tf.nn.relu) #can change activation function to sigmoid!!!\n# Compute logits (1 per class).\n\n#output layer\nlogits = tf.layers.dense(net, params['n_classes'], activation=None)\n\n# Compute predictions.\npredicted_classes = tf.argmax(logits, 1)\nif mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {\n        'class_ids': predicted_classes[:, tf.newaxis],\n        'probabilities': tf.nn.softmax(logits),\n        'logits': logits,\n    }\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n# Compute loss.\nloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n# Compute evaluation metrics!!!!!!!\naccuracy = tf.metrics.accuracy(labels=labels,\n                               predictions=predicted_classes,\n                               name='acc_op')\nmetrics = {'accuracy': accuracy}\ntf.summary.scalar('accuracy', accuracy[1])\n\nif mode == tf.estimator.ModeKeys.EVAL:\n    return tf.estimator.EstimatorSpec(\n        mode, loss=loss, eval_metric_ops=metrics)\n\n# Create training op.\nassert mode == tf.estimator.ModeKeys.TRAIN\n\noptimizer = tf.train.AdagradOptimizer(learning_rate=0.1) #can change optimizer\ntrain_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\nreturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\ndef main(m):\n#classification of death or no death\n\ndataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\n\nsplit1=500\nsplit2=980\n\n[train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns] = datasetfun(dataframe,split1,split2)\n\nclassifier = tf.estimator.Estimator(\n    model_fn=my_model,\n    params={\n        'feature_columns': my_feature_columns,\n        # Two hidden layers of 10 nodes each.\n        'hidden_units': [10, 10],\n        # The model must choose between 3 classes.\n        'n_classes': 2,\n    })\n\n# classifier = tf.estimator.DNNClassifier(\n# \tfeature_columns=my_feature_columns,\n# \thidden_units=[10,10,5,6,7],\n# \tn_classes=2)\n\nbatch_size=100\ntrain_steps=1000\n\n#writer = tf.summary.FileWriter(\"/Users/angelicagrusovin/documents/oxford/MLSP\",graph=tf.get_default_graph())\n#writer.add_graph(sess.graph)\n\nclassifier.train(input_fn=lambda:train_input_fn(train_features, train_labels, batch_size),steps=train_steps)\n\neval_result = classifier.evaluate(\ninput_fn=lambda:eval_input_fn(test_features, test_labels, batch_size))\n\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n\n#predictions = classifier.predict(\n#input_fn=lambda:eval_input_fn(predict_features,None,batch_size))\n\n# expected=['dead','alive']\n# STATE=['dead','alive']\n# print(zip(predictions))\n\n# template = ('\\nPrediction is ({:.1f}%), expected {}')\n\n# for pred_dict in zip(predictions):\n# \tclass_id = pred_dict[0]['class_ids'][0]\n# \tprobability = pred_dict[0]['probabilities'][class_id]\n# \tprint(template.format(100 * probability, STATE[class_id]))\n\n\n\n\n\n\n#print(my_feature_columns) \n\nreturn m\n\nif name == 'main':\nmain(10)`", "body": "For this costum_estimator, the related documentation on https://www.tensorflow.org/get_started/custom_estimators tells me that I can just input tensorboard --logdir=PATH in the directory and obtain TensorGraph visualisation.  However I do this and I get on the Tensorboard page \"Graph visualisation failed: the graph is empty. Make sure that the graph is passed to the tf.summary.filewriter after the graph is defined\"\r\nAs far as I understood, when using custom estimators it automatically passes the data to TensorBoard. What shall I do? I would like to get the basic graphs as shown at the bottom of the tutorial I linked above.\r\n\r\n`import pandas as pd             \r\nimport numpy as np              \r\nimport matplotlib.pyplot as plt  \r\nimport tensorflow as tf\r\nimport argparse\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for training\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(buffer_size=1000).repeat(count=None).batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\t\r\n\r\ndef eval_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for evaluation or prediction\"\"\"\r\n    features=dict(features)\r\n    if labels is None:\r\n        # No labels, use only features.\r\n        inputs = features\r\n    else:\r\n        inputs = (features, labels)\r\n\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\r\n\r\n    # Batch the examples\r\n    assert batch_size is not None, \"batch_size must not be None\"\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\ndef datasetfun(dataframe,split1,split2):\r\n    \r\n    #split1: variable deciding the proportion of test data to training data\r\n    #split2: variable deciding the proportion of test data to training data\r\n\r\n    dataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\r\n    #dataframe = dataframe.drop([\"dzclass\",\"num.co\",\"edu\",\"income\",\"scoma\",\"charges\",\"totcst\",\"totmcst\",\"avtisst\",\"race\",\"meanbp\",\"wblc\",\"hrt\",\"resp\",\"temp\",\"pafi\",\"alb\",\"bili\",\"crea\",\"sod\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\",\"adlsc\"], axis=1) # Remove columns we don't care about\r\n    dataframe = dataframe.drop([\"crea\",\"avtisst\",\"wblc\",\"race\",\"edu\",\"income\",\"charges\",\"totcst\",\"totmcst\",\"pafi\",\"alb\",\"bili\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\"], axis=1)\r\n    train = dataframe[0:split1] # use first 15 rows as training set\r\n    test = dataframe[split1:split2] # keep some as testing\r\n    predictdata = dataframe[split2:] # find the ones to predict\r\n\r\n    train_features, train_labels = train, train.pop(\"death\") #separate features with classification in train set\r\n    test_features, test_labels = test, test.pop(\"death\") #separate features with classification in train set\r\n    predict_features, predict_labels = predictdata, predictdata.pop(\"death\")\r\n\r\n    categorical_column1 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"sex\", vocabulary_list=[\"male\", \"female\"], default_value=0)\r\n    categorical_column2 = tf.feature_column.categorical_column_with_vocabulary_list(key='dzgroup',vocabulary_list=[\"Lung Cancer\",\"Colon Cancer\",\"ARF/MOSF w/Sepsis\",\"MOSF w/Malig\",\"Cirrhosi\",\"CHF\"])\r\n    categorical_column3 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"dzclass\", vocabulary_list=[\"Cancer\", \"ARF/MOSF\",\"COPD/CHF/Cirrhosis\"], default_value=0)\r\n    #categorical_column4 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"race\", vocabulary_list=[\"black\", \"hispanic\",\"White\",\"asian\",\"other\"], default_value=0)\r\n\r\n    my_feature_columns = [\r\n    tf.feature_column.numeric_column(key='age'),\r\n    tf.feature_column.indicator_column(categorical_column1),\r\n    tf.feature_column.numeric_column(key='hospdead'),\r\n    tf.feature_column.numeric_column(key='slos'),\r\n    tf.feature_column.numeric_column(key='d.time'),\r\n    tf.feature_column.indicator_column(categorical_column2),\r\n    tf.feature_column.indicator_column(categorical_column3),\r\n    tf.feature_column.numeric_column(key='scoma'),\r\n    #tf.feature_column.numeric_column(key='avtisst'),\r\n    #tf.feature_column.indicator_column(categorical_column4),\r\n    tf.feature_column.numeric_column(key='meanbp'),\r\n    tf.feature_column.numeric_column(key='hrt'),\r\n    tf.feature_column.numeric_column(key='resp'),\r\n    tf.feature_column.numeric_column(key='temp'),\r\n    #tf.feature_column.numeric_column(key='crea'),\r\n    tf.feature_column.numeric_column(key='sod'),\r\n    tf.feature_column.numeric_column(key='adlsc')]\r\n\r\n    return (train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns)\r\n\r\ndef my_model(features, labels, mode, params):\r\n    \"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\r\n    # Create three fully connected layers each layer having a dropout\r\n    # probability of 0.1.\r\n    net = tf.feature_column.input_layer(features, params['feature_columns']) #imput layer\r\n    for units in params['hidden_units']: #can change type of layers!!!!\r\n        net = tf.layers.dense(net, units=units, activation=tf.nn.relu) #can change activation function to sigmoid!!!\r\n\r\n    # Compute logits (1 per class).\r\n\r\n    #output layer\r\n    logits = tf.layers.dense(net, params['n_classes'], activation=None)\r\n\r\n    # Compute predictions.\r\n    predicted_classes = tf.argmax(logits, 1)\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'class_ids': predicted_classes[:, tf.newaxis],\r\n            'probabilities': tf.nn.softmax(logits),\r\n            'logits': logits,\r\n        }\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    # Compute loss.\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n    # Compute evaluation metrics!!!!!!!\r\n    accuracy = tf.metrics.accuracy(labels=labels,\r\n                                   predictions=predicted_classes,\r\n                                   name='acc_op')\r\n    metrics = {'accuracy': accuracy}\r\n    tf.summary.scalar('accuracy', accuracy[1])\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    # Create training op.\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n    \r\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1) #can change optimizer\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n    \r\n\r\n\r\n\r\n\r\ndef main(m):\r\n\t\r\n\t#classification of death or no death\r\n\r\n    dataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\r\n\r\n    split1=500\r\n    split2=980\r\n\r\n    [train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns] = datasetfun(dataframe,split1,split2)\r\n\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=my_model,\r\n        params={\r\n            'feature_columns': my_feature_columns,\r\n            # Two hidden layers of 10 nodes each.\r\n            'hidden_units': [10, 10],\r\n            # The model must choose between 3 classes.\r\n            'n_classes': 2,\r\n        })\r\n\r\n    # classifier = tf.estimator.DNNClassifier(\r\n    # \tfeature_columns=my_feature_columns,\r\n    # \thidden_units=[10,10,5,6,7],\r\n    # \tn_classes=2)\r\n\r\n    batch_size=100\r\n    train_steps=1000\r\n\r\n    #writer = tf.summary.FileWriter(\"/Users/angelicagrusovin/documents/oxford/MLSP\",graph=tf.get_default_graph())\r\n    #writer.add_graph(sess.graph)\r\n\r\n    classifier.train(input_fn=lambda:train_input_fn(train_features, train_labels, batch_size),steps=train_steps)\r\n\r\n    eval_result = classifier.evaluate(\r\n    input_fn=lambda:eval_input_fn(test_features, test_labels, batch_size))\r\n\r\n    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\r\n\r\n\t#predictions = classifier.predict(\r\n    #input_fn=lambda:eval_input_fn(predict_features,None,batch_size))\r\n\r\n\t# expected=['dead','alive']\r\n\t# STATE=['dead','alive']\r\n\t# print(zip(predictions))\r\n\r\n\t# template = ('\\nPrediction is ({:.1f}%), expected {}')\r\n\r\n\t# for pred_dict in zip(predictions):\r\n\t# \tclass_id = pred_dict[0]['class_ids'][0]\r\n\t# \tprobability = pred_dict[0]['probabilities'][class_id]\r\n\t# \tprint(template.format(100 * probability, STATE[class_id]))\r\n\r\n    \r\n\r\n\r\n\r\n\r\n\t#print(my_feature_columns) \r\n\r\n    return m\r\n\r\nif __name__ == '__main__':\r\n    main(10)`\r\n\r\n\r\n\r\n"}