{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10006", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10006/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10006/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10006/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10006", "id": 229679700, "node_id": "MDU6SXNzdWUyMjk2Nzk3MDA=", "number": 10006, "title": "Tensorflow cifar 10 example memory leak", "user": {"login": "arashno", "id": 11036312, "node_id": "MDQ6VXNlcjExMDM2MzEy", "avatar_url": "https://avatars1.githubusercontent.com/u/11036312?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arashno", "html_url": "https://github.com/arashno", "followers_url": "https://api.github.com/users/arashno/followers", "following_url": "https://api.github.com/users/arashno/following{/other_user}", "gists_url": "https://api.github.com/users/arashno/gists{/gist_id}", "starred_url": "https://api.github.com/users/arashno/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arashno/subscriptions", "organizations_url": "https://api.github.com/users/arashno/orgs", "repos_url": "https://api.github.com/users/arashno/repos", "events_url": "https://api.github.com/users/arashno/events{/privacy}", "received_events_url": "https://api.github.com/users/arashno/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-18T13:56:22Z", "updated_at": "2017-05-18T19:26:42Z", "closed_at": "2017-05-18T19:22:19Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>I am new to Tensorflow. I tried to run the cifar10 examples from here: <a href=\"https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>\n<p>I didn't make any changes to the code, I just tried to run it on multiple GPUs. I am trying to use 6 GPUs and I am allocating 10 GB of RAM for my job but after a few minutes my jobs getting failed due to memory limit. Allocating more memory does not help, it just delays the error. I tried for up to 40GB of memory.</p>\n<p>Here is my system info:</p>\n<blockquote>\n<p>== cat /etc/issue =============================================== Linux mmmdgx01 4.4.0-45-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116010548\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/66\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/66/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/66\">#66</a>~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux DGX_OTA_VERSION=2.0.5 VERSION=\"14.04.5 LTS, Trusty Tahr\" VERSION_ID=\"14.04\"</p>\n<p>== are we in docker ============================================= No</p>\n<p>== compiler ===================================================== c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>\n<p>== uname -a ===================================================== Linux mmmdgx01 4.4.0-45-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116010548\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/66\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/66/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/66\">#66</a>~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</p>\n<p>== check pips =================================================== numpy (1.11.1) protobuf (3.2.0) tensorflow (1.1.0rc1)</p>\n<p>== check for virtualenv ========================================= False</p>\n<p>== tensorflow import ============================================ tf.VERSION = 1.1.0-rc1 tf.GIT_VERSION = v1.1.0-rc1-272-gf77f19b tf.COMPILER_VERSION = v1.1.0-rc1-272-gf77f19b Sanity check: array(1, dtype=int32)</p>\n<p>== env ========================================================== LD_LIBRARY_PATH /opt/sw/cuda/8.0/lib64/:/project/DGX/cuda/lib64/:/opt/sw/cuda/8.0/extras/CUPTI/lib64/:/project/DGX/lib DYLD_LIBRARY_PATH /project/DGX/torch/install/lib:/project/torch7new/install/lib:</p>\n<p>== nvidia-smi =================================================== Fri May 12 15:46:50 2017 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.20 Driver Version: 375.20<br>\n| |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-SXM2... On | 0000:06:00.0 Off |<br>\n0 | | N/A 34C P0 42W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-SXM2... On | 0000:07:00.0 Off |<br>\n0 | | N/A 32C P0 32W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-SXM2... On | 0000:0A:00.0 Off |<br>\n0 | | N/A 34C P0 33W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+ | 3 Tesla P100-SXM2... On | 0000:0B:00.0 Off |<br>\n0 | | N/A 33C P0 32W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+ | 4 Tesla P100-SXM2... On | 0000:85:00.0 Off |<br>\n0 | | N/A 33C P0 30W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+ | 5 Tesla P100-SXM2... On | 0000:86:00.0 Off |<br>\n0 | | N/A 33C P0 33W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+ | 6 Tesla P100-SXM2... On | 0000:89:00.0 Off |<br>\n0 | | N/A 31C P0 32W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+ | 7 Tesla P100-SXM2... On | 0000:8A:00.0 Off |<br>\n0 | | N/A 35C P0 32W / 300W | 0MiB / 16308MiB | 0%<br>\nDefault | +-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name<br>\nUsage | |=============================================================================| | No running processes found<br>\n| +-----------------------------------------------------------------------------+</p>\n<p>== cuda libs ===================================================</p>\n</blockquote>\n<h3>TensorFlow version</h3>\n<p>('v1.1.0-rc1-272-gf77f19b', '1.1.0-rc1')</p>\n<h3>Source code / logs</h3>\n<p>Here is my job submission script:</p>\n<blockquote>\n<p>#! /bin/bash<br>\n#SBATCH --account=AI<br>\n#SBATCH --time=167:00:00<br>\n#SBATCH --nodes=1<br>\n#SBATCH --ntasks-per-node=20<br>\n#SBATCH -J TFImgNet<br>\n#SBATCH -e tf.err<br>\n#SBATCH -o tf.log<br>\n#SBATCH --mem=10960<br>\n#SBATCH --gres=gpu:6<br>\ncpath=$(pwd)<br>\ncd ~<br>\nsource .bashrc<br>\ncd $cpath<br>\nwhich python<br>\npython cifar10_multi_gpu_train.py --num_gpus 6</p>\n</blockquote>\n<h3>Here is the error:</h3>\n<blockquote>\n<p>2017-05-12 15:14:07.162709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 4 5<br>\n2017-05-12 15:14:07.162718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y Y Y Y N<br>\n2017-05-12 15:14:07.162721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y Y Y N Y<br>\n2017-05-12 15:14:07.162724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   Y Y Y Y N N<br>\n2017-05-12 15:14:07.162727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   Y Y Y Y N N<br>\n2017-05-12 15:14:07.162729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 4:   Y N N N Y Y<br>\n2017-05-12 15:14:07.162732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 5:   N Y N N Y Y<br>\n2017-05-12 15:14:07.162743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:06:00.0)<br>\n2017-05-12 15:14:07.162747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:07:00.0)<br>\n2017-05-12 15:14:07.162751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0a:00.0)<br>\n2017-05-12 15:14:07.162754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0b:00.0)<br>\n2017-05-12 15:14:07.162756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:4) -&gt; (device: 4, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0)<br>\n2017-05-12 15:14:07.162759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:5) -&gt; (device: 5, name: Tesla P100-SXM2-16GB, pci bus id: 0000:86:00.0)<br>\nslurmstepd: error: Job 1313520 exceeded memory limit (11240536 &gt; 11223040), being killed<br>\nslurmstepd: error: Exceeded job memory limit<br>\nslurmstepd: error: *** JOB 1313520 ON mmmdgx01 CANCELLED AT 2017-05-12T15:28:58 ***</p>\n</blockquote>", "body_text": "Describe the problem\nI am new to Tensorflow. I tried to run the cifar10 examples from here: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\nI didn't make any changes to the code, I just tried to run it on multiple GPUs. I am trying to use 6 GPUs and I am allocating 10 GB of RAM for my job but after a few minutes my jobs getting failed due to memory limit. Allocating more memory does not help, it just delays the error. I tried for up to 40GB of memory.\nHere is my system info:\n\n== cat /etc/issue =============================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux DGX_OTA_VERSION=2.0.5 VERSION=\"14.04.5 LTS, Trusty Tahr\" VERSION_ID=\"14.04\"\n== are we in docker ============================================= No\n== compiler ===================================================== c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n== uname -a ===================================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n== check pips =================================================== numpy (1.11.1) protobuf (3.2.0) tensorflow (1.1.0rc1)\n== check for virtualenv ========================================= False\n== tensorflow import ============================================ tf.VERSION = 1.1.0-rc1 tf.GIT_VERSION = v1.1.0-rc1-272-gf77f19b tf.COMPILER_VERSION = v1.1.0-rc1-272-gf77f19b Sanity check: array(1, dtype=int32)\n== env ========================================================== LD_LIBRARY_PATH /opt/sw/cuda/8.0/lib64/:/project/DGX/cuda/lib64/:/opt/sw/cuda/8.0/extras/CUPTI/lib64/:/project/DGX/lib DYLD_LIBRARY_PATH /project/DGX/torch/install/lib:/project/torch7new/install/lib:\n== nvidia-smi =================================================== Fri May 12 15:46:50 2017 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.20 Driver Version: 375.20\n| |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-SXM2... On | 0000:06:00.0 Off |\n0 | | N/A 34C P0 42W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-SXM2... On | 0000:07:00.0 Off |\n0 | | N/A 32C P0 32W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-SXM2... On | 0000:0A:00.0 Off |\n0 | | N/A 34C P0 33W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+ | 3 Tesla P100-SXM2... On | 0000:0B:00.0 Off |\n0 | | N/A 33C P0 32W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+ | 4 Tesla P100-SXM2... On | 0000:85:00.0 Off |\n0 | | N/A 33C P0 30W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+ | 5 Tesla P100-SXM2... On | 0000:86:00.0 Off |\n0 | | N/A 33C P0 33W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+ | 6 Tesla P100-SXM2... On | 0000:89:00.0 Off |\n0 | | N/A 31C P0 32W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+ | 7 Tesla P100-SXM2... On | 0000:8A:00.0 Off |\n0 | | N/A 35C P0 32W / 300W | 0MiB / 16308MiB | 0%\nDefault | +-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name\nUsage | |=============================================================================| | No running processes found\n| +-----------------------------------------------------------------------------+\n== cuda libs ===================================================\n\nTensorFlow version\n('v1.1.0-rc1-272-gf77f19b', '1.1.0-rc1')\nSource code / logs\nHere is my job submission script:\n\n#! /bin/bash\n#SBATCH --account=AI\n#SBATCH --time=167:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20\n#SBATCH -J TFImgNet\n#SBATCH -e tf.err\n#SBATCH -o tf.log\n#SBATCH --mem=10960\n#SBATCH --gres=gpu:6\ncpath=$(pwd)\ncd ~\nsource .bashrc\ncd $cpath\nwhich python\npython cifar10_multi_gpu_train.py --num_gpus 6\n\nHere is the error:\n\n2017-05-12 15:14:07.162709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 4 5\n2017-05-12 15:14:07.162718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y Y Y Y N\n2017-05-12 15:14:07.162721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y Y Y N Y\n2017-05-12 15:14:07.162724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   Y Y Y Y N N\n2017-05-12 15:14:07.162727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   Y Y Y Y N N\n2017-05-12 15:14:07.162729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 4:   Y N N N Y Y\n2017-05-12 15:14:07.162732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 5:   N Y N N Y Y\n2017-05-12 15:14:07.162743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:06:00.0)\n2017-05-12 15:14:07.162747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:07:00.0)\n2017-05-12 15:14:07.162751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0a:00.0)\n2017-05-12 15:14:07.162754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0b:00.0)\n2017-05-12 15:14:07.162756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0)\n2017-05-12 15:14:07.162759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla P100-SXM2-16GB, pci bus id: 0000:86:00.0)\nslurmstepd: error: Job 1313520 exceeded memory limit (11240536 > 11223040), being killed\nslurmstepd: error: Exceeded job memory limit\nslurmstepd: error: *** JOB 1313520 ON mmmdgx01 CANCELLED AT 2017-05-12T15:28:58 ***", "body": "### Describe the problem\r\nI am new to Tensorflow. I tried to run the cifar10 examples from here: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\r\n\r\nI didn't make any changes to the code, I just tried to run it on multiple GPUs. I am trying to use 6 GPUs and I am allocating 10 GB of RAM for my job but after a few minutes my jobs getting failed due to memory limit. Allocating more memory does not help, it just delays the error. I tried for up to 40GB of memory.\r\n\r\nHere is my system info:\r\n\r\n> == cat /etc/issue =============================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux DGX_OTA_VERSION=2.0.5 VERSION=\"14.04.5 LTS, Trusty Tahr\" VERSION_ID=\"14.04\"\r\n> \r\n> == are we in docker ============================================= No\r\n> \r\n> == compiler ===================================================== c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n> \r\n> == uname -a ===================================================== Linux mmmdgx01 4.4.0-45-generic #66~14.04.1-Ubuntu SMP Wed Oct 19 15:05:38 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n> \r\n> == check pips =================================================== numpy (1.11.1) protobuf (3.2.0) tensorflow (1.1.0rc1)\r\n> \r\n> == check for virtualenv ========================================= False\r\n> \r\n> == tensorflow import ============================================ tf.VERSION = 1.1.0-rc1 tf.GIT_VERSION = v1.1.0-rc1-272-gf77f19b tf.COMPILER_VERSION = v1.1.0-rc1-272-gf77f19b Sanity check: array(1, dtype=int32)\r\n> \r\n> == env ========================================================== LD_LIBRARY_PATH /opt/sw/cuda/8.0/lib64/:/project/DGX/cuda/lib64/:/opt/sw/cuda/8.0/extras/CUPTI/lib64/:/project/DGX/lib DYLD_LIBRARY_PATH /project/DGX/torch/install/lib:/project/torch7new/install/lib:\r\n> \r\n> == nvidia-smi =================================================== Fri May 12 15:46:50 2017 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.20 Driver Version: 375.20\r\n> | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-SXM2... On | 0000:06:00.0 Off |\r\n> 0 | | N/A 34C P0 42W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla P100-SXM2... On | 0000:07:00.0 Off |\r\n> 0 | | N/A 32C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 2 Tesla P100-SXM2... On | 0000:0A:00.0 Off |\r\n> 0 | | N/A 34C P0 33W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 3 Tesla P100-SXM2... On | 0000:0B:00.0 Off |\r\n> 0 | | N/A 33C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 4 Tesla P100-SXM2... On | 0000:85:00.0 Off |\r\n> 0 | | N/A 33C P0 30W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 5 Tesla P100-SXM2... On | 0000:86:00.0 Off |\r\n> 0 | | N/A 33C P0 33W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 6 Tesla P100-SXM2... On | 0000:89:00.0 Off |\r\n> 0 | | N/A 31C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+ | 7 Tesla P100-SXM2... On | 0000:8A:00.0 Off |\r\n> 0 | | N/A 35C P0 32W / 300W | 0MiB / 16308MiB | 0%\r\n> Default | +-------------------------------+----------------------+----------------------+\r\n> \r\n> +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name\r\n> Usage | |=============================================================================| | No running processes found\r\n> | +-----------------------------------------------------------------------------+\r\n> \r\n> == cuda libs ===================================================\r\n> \r\n\r\n### TensorFlow version\r\n\r\n('v1.1.0-rc1-272-gf77f19b', '1.1.0-rc1')\r\n\r\n### Source code / logs\r\nHere is my job submission script:\r\n\r\n> #! /bin/bash\r\n> #SBATCH --account=AI\r\n> #SBATCH --time=167:00:00\r\n> #SBATCH --nodes=1\r\n> #SBATCH --ntasks-per-node=20\r\n> #SBATCH -J TFImgNet\r\n> #SBATCH -e tf.err\r\n> #SBATCH -o tf.log\r\n> #SBATCH --mem=10960\r\n> #SBATCH --gres=gpu:6\r\n> cpath=$(pwd)\r\n> cd ~\r\n> source .bashrc\r\n> cd $cpath\r\n> which python\r\n> python cifar10_multi_gpu_train.py --num_gpus 6\r\n\r\n### Here is the error:\r\n\r\n> 2017-05-12 15:14:07.162709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 4 5\r\n> 2017-05-12 15:14:07.162718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y Y Y Y N\r\n> 2017-05-12 15:14:07.162721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y Y Y N Y\r\n> 2017-05-12 15:14:07.162724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   Y Y Y Y N N\r\n> 2017-05-12 15:14:07.162727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   Y Y Y Y N N\r\n> 2017-05-12 15:14:07.162729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 4:   Y N N N Y Y\r\n> 2017-05-12 15:14:07.162732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 5:   N Y N N Y Y\r\n> 2017-05-12 15:14:07.162743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:06:00.0)\r\n> 2017-05-12 15:14:07.162747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:07:00.0)\r\n> 2017-05-12 15:14:07.162751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0a:00.0)\r\n> 2017-05-12 15:14:07.162754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:0b:00.0)\r\n> 2017-05-12 15:14:07.162756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0)\r\n> 2017-05-12 15:14:07.162759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla P100-SXM2-16GB, pci bus id: 0000:86:00.0)\r\n> slurmstepd: error: Job 1313520 exceeded memory limit (11240536 > 11223040), being killed\r\n> slurmstepd: error: Exceeded job memory limit\r\n> slurmstepd: error: *** JOB 1313520 ON mmmdgx01 CANCELLED AT 2017-05-12T15:28:58 ***\r\n"}