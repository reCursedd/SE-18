{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244089247", "html_url": "https://github.com/tensorflow/tensorflow/issues/4151#issuecomment-244089247", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4151", "id": 244089247, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDA4OTI0Nw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-01T14:05:46Z", "updated_at": "2016-09-01T14:05:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This memory leak is caused by adding new nodes (a <code>tf.assign()</code> node and an implicitly created <code>tf.constant()</code> node) on each iteration of the training loop. This <a href=\"http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow/13426/use-graph-finalize-to-catch-nodes-being-added-to-the-graph#t=201609011401186013951\" rel=\"nofollow\">documentation</a> has a guide to tracking down leaks like this.</p>\n<p>The solution for your particular problem is to define a single assign op that takes its input from a <code>tf.placeholder()</code>, and feed different values into that placeholder on each iteration:</p>\n<div class=\"highlight highlight-source-python\"><pre>sess <span class=\"pl-k\">=</span> tf.Session()\na <span class=\"pl-k\">=</span> tf.Variable(np.ones((<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">3</span>)))\nupdate_placeholder <span class=\"pl-k\">=</span> tf.placeholder(a.dtype, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>a.get_shape())\nupdate_op <span class=\"pl-k\">=</span> a.assign(update_placeholder)\n\nsess.run(tf.initialize_all_variables())\n\nt0 <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Obviously, you'd change the value being assigned in each step in a real program.</span>\n    sess.run(update_op, {update_placeholder: np.ones((<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">3</span>))})\n    t1 <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-c1\">print</span>(t1<span class=\"pl-k\">-</span>t0)\n    t0 <span class=\"pl-k\">=</span> t1</pre></div>", "body_text": "This memory leak is caused by adding new nodes (a tf.assign() node and an implicitly created tf.constant() node) on each iteration of the training loop. This documentation has a guide to tracking down leaks like this.\nThe solution for your particular problem is to define a single assign op that takes its input from a tf.placeholder(), and feed different values into that placeholder on each iteration:\nsess = tf.Session()\na = tf.Variable(np.ones((5, 10000, 10000, 3)))\nupdate_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\nupdate_op = a.assign(update_placeholder)\n\nsess.run(tf.initialize_all_variables())\n\nt0 = time.time()\nfor i in range(10000):\n    # Obviously, you'd change the value being assigned in each step in a real program.\n    sess.run(update_op, {update_placeholder: np.ones((5, 10000, 10000, 3))})\n    t1 = time.time()\n    print(t1-t0)\n    t0 = t1", "body": "This memory leak is caused by adding new nodes (a `tf.assign()` node and an implicitly created `tf.constant()` node) on each iteration of the training loop. This [documentation](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow/13426/use-graph-finalize-to-catch-nodes-being-added-to-the-graph#t=201609011401186013951) has a guide to tracking down leaks like this.\n\nThe solution for your particular problem is to define a single assign op that takes its input from a `tf.placeholder()`, and feed different values into that placeholder on each iteration:\n\n``` python\nsess = tf.Session()\na = tf.Variable(np.ones((5, 10000, 10000, 3)))\nupdate_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\nupdate_op = a.assign(update_placeholder)\n\nsess.run(tf.initialize_all_variables())\n\nt0 = time.time()\nfor i in range(10000):\n    # Obviously, you'd change the value being assigned in each step in a real program.\n    sess.run(update_op, {update_placeholder: np.ones((5, 10000, 10000, 3))})\n    t1 = time.time()\n    print(t1-t0)\n    t0 = t1\n```\n"}