{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13018", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13018/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13018/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13018/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13018", "id": 257404847, "node_id": "MDU6SXNzdWUyNTc0MDQ4NDc=", "number": 13018, "title": "Add `Dataset.from_variable_length` to accept numpy arrays with varying length", "user": {"login": "rasmusbergpalm", "id": 206013, "node_id": "MDQ6VXNlcjIwNjAxMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/206013?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rasmusbergpalm", "html_url": "https://github.com/rasmusbergpalm", "followers_url": "https://api.github.com/users/rasmusbergpalm/followers", "following_url": "https://api.github.com/users/rasmusbergpalm/following{/other_user}", "gists_url": "https://api.github.com/users/rasmusbergpalm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rasmusbergpalm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rasmusbergpalm/subscriptions", "organizations_url": "https://api.github.com/users/rasmusbergpalm/orgs", "repos_url": "https://api.github.com/users/rasmusbergpalm/repos", "events_url": "https://api.github.com/users/rasmusbergpalm/events{/privacy}", "received_events_url": "https://api.github.com/users/rasmusbergpalm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-09-13T14:36:39Z", "updated_at": "2017-10-12T14:44:22Z", "closed_at": "2017-09-13T20:27:31Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac OS x</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: <code>v1.3.0-rc2-20-g0787eee 1.3.0</code></li>\n<li><strong>Python version</strong>: <code>Python 3.5.2 :: Continuum Analytics, Inc.</code></li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import numpy as np\nfrom tensorflow.contrib.data import Dataset\na=[np.arange(3), np.arange(5)]\nDataset.from_tensor_slices(np.asarray(a))\n</code></pre>\n<h3>Describe the problem</h3>\n<p><code>Dataset</code> has a nice <code>.padded_batch</code> feature which allows padding batches to ensure all tensors have the same size, which is very nice for the common use-case of having sequences of different lengths.</p>\n<p>However, it seems impossible to create such a dataset from a list of numpy array with different lengths. See command above.</p>\n<p>It would be very nice to have a <code>Dataset.from_variable_length_tensor_slices</code> or such that could take such inputs.</p>\n<p>I know I'm supposed to use a <code>TextLineDataset</code> or something similar, and then use a series of map functions, but I think tensorflow should support doing all of the pre-processing outside of tensorflow instead of forcing it into a delayed execution graph paradigm, which is harder to reason about and debug. Also, my data is stored in JSON files, and requires complex pre-processing, so the <code>TextLineDataset/map</code> approach doesn't cut it.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS x\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0\nPython version: Python 3.5.2 :: Continuum Analytics, Inc.\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nimport numpy as np\nfrom tensorflow.contrib.data import Dataset\na=[np.arange(3), np.arange(5)]\nDataset.from_tensor_slices(np.asarray(a))\n\nDescribe the problem\nDataset has a nice .padded_batch feature which allows padding batches to ensure all tensors have the same size, which is very nice for the common use-case of having sequences of different lengths.\nHowever, it seems impossible to create such a dataset from a list of numpy array with different lengths. See command above.\nIt would be very nice to have a Dataset.from_variable_length_tensor_slices or such that could take such inputs.\nI know I'm supposed to use a TextLineDataset or something similar, and then use a series of map functions, but I think tensorflow should support doing all of the pre-processing outside of tensorflow instead of forcing it into a delayed execution graph paradigm, which is harder to reason about and debug. Also, my data is stored in JSON files, and requires complex pre-processing, so the TextLineDataset/map approach doesn't cut it.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS x\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: `v1.3.0-rc2-20-g0787eee 1.3.0`\r\n- **Python version**: `Python 3.5.2 :: Continuum Analytics, Inc.`\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\n```\r\nimport numpy as np\r\nfrom tensorflow.contrib.data import Dataset\r\na=[np.arange(3), np.arange(5)]\r\nDataset.from_tensor_slices(np.asarray(a))\r\n```\r\n\r\n### Describe the problem\r\n`Dataset` has a nice `.padded_batch` feature which allows padding batches to ensure all tensors have the same size, which is very nice for the common use-case of having sequences of different lengths.\r\n\r\nHowever, it seems impossible to create such a dataset from a list of numpy array with different lengths. See command above.\r\n\r\nIt would be very nice to have a `Dataset.from_variable_length_tensor_slices` or such that could take such inputs.\r\n\r\nI know I'm supposed to use a `TextLineDataset` or something similar, and then use a series of map functions, but I think tensorflow should support doing all of the pre-processing outside of tensorflow instead of forcing it into a delayed execution graph paradigm, which is harder to reason about and debug. Also, my data is stored in JSON files, and requires complex pre-processing, so the `TextLineDataset/map` approach doesn't cut it."}