{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16187", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16187/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16187/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16187/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16187", "id": 289194443, "node_id": "MDU6SXNzdWUyODkxOTQ0NDM=", "number": 16187, "title": "Faster R-CNN: too many resources requested for launch", "user": {"login": "JesperChristensen89", "id": 15313473, "node_id": "MDQ6VXNlcjE1MzEzNDcz", "avatar_url": "https://avatars1.githubusercontent.com/u/15313473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JesperChristensen89", "html_url": "https://github.com/JesperChristensen89", "followers_url": "https://api.github.com/users/JesperChristensen89/followers", "following_url": "https://api.github.com/users/JesperChristensen89/following{/other_user}", "gists_url": "https://api.github.com/users/JesperChristensen89/gists{/gist_id}", "starred_url": "https://api.github.com/users/JesperChristensen89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JesperChristensen89/subscriptions", "organizations_url": "https://api.github.com/users/JesperChristensen89/orgs", "repos_url": "https://api.github.com/users/JesperChristensen89/repos", "events_url": "https://api.github.com/users/JesperChristensen89/events{/privacy}", "received_events_url": "https://api.github.com/users/JesperChristensen89/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-01-17T09:24:26Z", "updated_at": "2018-06-12T11:05:46Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I am trying to deploy the pretrained Faster-RCNN Inception V2 from the object detection API on a Jetson TX2.<br>\nI am running CUDA 8, cuDNN 6 and have tested with both TF 1.3 and 1.5 in a Jupyter Notebook environment.<br>\nWhen I monitor the GPU memory it starts out by having 4.8 GB free and when launching these fills up immediately. When I run on my GTX1060 6 GB GPU I have effectively the same amount of memory free but are having no issues running.<br>\nSmaller models as SSD MobileNet runs without problems.</p>\n<p>From tests performed today, I can supply the following dumps.</p>\n<p>Jupyter Notebook terminal output:</p>\n<pre><code>2018-01-17 16:16:19.584106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] ARM64 does not support NUMA - returning NUMA node zero\n2018-01-17 16:16:19.584261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: \nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005\npciBusID: 0000:00:00.0\ntotalMemory: 7.67GiB freeMemory: 4.97GiB\n2018-01-17 16:16:19.584312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0\n2018-01-17 16:16:20.824479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4437 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)\n2018-01-17 16:17:09.816477: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:09.816703: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:09.816771: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:09.816912: E tensorflow/stream_executor/cuda/cuda_dnn.cc:2456] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\n2018-01-17 16:17:10.174651: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174772: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174806: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174836: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174865: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n</code></pre>\n<p>Error dump from printout inside the notebook:</p>\n<pre><code>Exception in thread Thread-4:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"&lt;ipython-input-5-a51933cd03d8&gt;\", line 19, in worker\n    im, t_elapsed = detect_objects(frame_rgb, sess, detection_graph)\n  File \"&lt;ipython-input-4-6c8da66803e2&gt;\", line 19, in detect_objects\n    feed_dict={image_tensor: image_np_expanded})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1128, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\n    options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\n    raise type(e)(node_def, op, message)\nInternalError: cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\n\nCaused by op u'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D', defined at:\n  File \"/usr/lib/python2.7/threading.py\", line 774, in __bootstrap\n    self.__bootstrap_inner()\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"&lt;ipython-input-5-a51933cd03d8&gt;\", line 10, in worker\n    tf.import_graph_def(od_graph_def, name='')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 548, in import_graph_def\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3176, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\n</code></pre>\n<p>Output of tegrastats at the point of error:</p>\n<pre><code>RAM 3151/7851MB (lfb 915x4MB) cpu [2%@345,100%@2034,99%@2034,1%@348,3%@348,6%@349] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3151/7851MB (lfb 915x4MB) cpu [0%@345,100%@1981,100%@1988,3%@348,5%@348,4%@349] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2021,100%@2021,4%@348,5%@348,2%@349] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2035,100%@2034,3%@349,4%@348,2%@348] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3152/7851MB (lfb 915x4MB) cpu [1%@345,100%@2016,100%@2019,2%@345,1%@349,3%@348] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3181/7851MB (lfb 898x4MB) cpu [21%@806,100%@2021,56%@2024,8%@499,10%@500,3%@500] EMC 5%@1866 APE 150 GR3D 24%@114\nRAM 3210/7851MB (lfb 887x4MB) cpu [8%@345,100%@2018,32%@2026,7%@345,24%@345,13%@349] EMC 5%@1866 APE 150 GR3D 99%@114\nRAM 3327/7851MB (lfb 838x4MB) cpu [2%@1573,100%@1987,31%@1992,35%@1574,13%@1575,5%@1573] EMC 5%@1866 APE 150 GR3D 8%@114\nRAM 3578/7851MB (lfb 758x4MB) cpu [19%@1806,100%@2080,0%@2035,7%@2035,2%@2035,56%@1727] EMC 5%@1866 APE 150 GR3D 10%@114\nRAM 3732/7851MB (lfb 715x4MB) cpu [2%@345,100%@2034,83%@2035,5%@348,21%@345,2%@346] EMC 7%@1866 APE 150 GR3D 99%@624\nRAM 3732/7851MB (lfb 715x4MB) cpu [94%@2036,100%@2035,97%@2034,87%@1987,13%@2035,1%@2035] EMC 4%@1866 APE 150 GR3D 43%@1032\nRAM 3659/7851MB (lfb 727x4MB) cpu [2%@653,81%@2022,20%@2027,28%@652,2%@655,4%@655] EMC 3%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [1%@345,100%@2033,0%@2035,1%@346,2%@348,3%@349] EMC 3%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2035,0%@2034,0%@348,3%@348,0%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [3%@345,100%@2034,0%@2035,1%@348,1%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2034,0%@2034,2%@348,4%@348,1%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@1988,0%@1987,2%@346,2%@345,1%@345] EMC 2%@1866 APE 150 GR3D 9%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@2026,0%@2026,1%@347,0%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [8%@345,100%@2024,0%@2028,5%@345,8%@345,1%@345] EMC 2%@1866 APE 150 GR3D 0%@114\n</code></pre>\n<p>As you can see the RAM are nowhere near full at the moment of the error.</p>\n<p>Can anybody suggest a solution to this?</p>", "body_text": "I am trying to deploy the pretrained Faster-RCNN Inception V2 from the object detection API on a Jetson TX2.\nI am running CUDA 8, cuDNN 6 and have tested with both TF 1.3 and 1.5 in a Jupyter Notebook environment.\nWhen I monitor the GPU memory it starts out by having 4.8 GB free and when launching these fills up immediately. When I run on my GTX1060 6 GB GPU I have effectively the same amount of memory free but are having no issues running.\nSmaller models as SSD MobileNet runs without problems.\nFrom tests performed today, I can supply the following dumps.\nJupyter Notebook terminal output:\n2018-01-17 16:16:19.584106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] ARM64 does not support NUMA - returning NUMA node zero\n2018-01-17 16:16:19.584261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: \nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005\npciBusID: 0000:00:00.0\ntotalMemory: 7.67GiB freeMemory: 4.97GiB\n2018-01-17 16:16:19.584312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0\n2018-01-17 16:16:20.824479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4437 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)\n2018-01-17 16:17:09.816477: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:09.816703: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:09.816771: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:09.816912: E tensorflow/stream_executor/cuda/cuda_dnn.cc:2456] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\n2018-01-17 16:17:10.174651: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174772: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174806: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174836: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n2018-01-17 16:17:10.174865: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\n\nError dump from printout inside the notebook:\nException in thread Thread-4:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"<ipython-input-5-a51933cd03d8>\", line 19, in worker\n    im, t_elapsed = detect_objects(frame_rgb, sess, detection_graph)\n  File \"<ipython-input-4-6c8da66803e2>\", line 19, in detect_objects\n    feed_dict={image_tensor: image_np_expanded})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1128, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\n    options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\n    raise type(e)(node_def, op, message)\nInternalError: cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\n\nCaused by op u'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D', defined at:\n  File \"/usr/lib/python2.7/threading.py\", line 774, in __bootstrap\n    self.__bootstrap_inner()\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"<ipython-input-5-a51933cd03d8>\", line 10, in worker\n    tf.import_graph_def(od_graph_def, name='')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 548, in import_graph_def\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3176, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\n\nOutput of tegrastats at the point of error:\nRAM 3151/7851MB (lfb 915x4MB) cpu [2%@345,100%@2034,99%@2034,1%@348,3%@348,6%@349] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3151/7851MB (lfb 915x4MB) cpu [0%@345,100%@1981,100%@1988,3%@348,5%@348,4%@349] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2021,100%@2021,4%@348,5%@348,2%@349] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2035,100%@2034,3%@349,4%@348,2%@348] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3152/7851MB (lfb 915x4MB) cpu [1%@345,100%@2016,100%@2019,2%@345,1%@349,3%@348] EMC 5%@1866 APE 150 GR3D 0%@114\nRAM 3181/7851MB (lfb 898x4MB) cpu [21%@806,100%@2021,56%@2024,8%@499,10%@500,3%@500] EMC 5%@1866 APE 150 GR3D 24%@114\nRAM 3210/7851MB (lfb 887x4MB) cpu [8%@345,100%@2018,32%@2026,7%@345,24%@345,13%@349] EMC 5%@1866 APE 150 GR3D 99%@114\nRAM 3327/7851MB (lfb 838x4MB) cpu [2%@1573,100%@1987,31%@1992,35%@1574,13%@1575,5%@1573] EMC 5%@1866 APE 150 GR3D 8%@114\nRAM 3578/7851MB (lfb 758x4MB) cpu [19%@1806,100%@2080,0%@2035,7%@2035,2%@2035,56%@1727] EMC 5%@1866 APE 150 GR3D 10%@114\nRAM 3732/7851MB (lfb 715x4MB) cpu [2%@345,100%@2034,83%@2035,5%@348,21%@345,2%@346] EMC 7%@1866 APE 150 GR3D 99%@624\nRAM 3732/7851MB (lfb 715x4MB) cpu [94%@2036,100%@2035,97%@2034,87%@1987,13%@2035,1%@2035] EMC 4%@1866 APE 150 GR3D 43%@1032\nRAM 3659/7851MB (lfb 727x4MB) cpu [2%@653,81%@2022,20%@2027,28%@652,2%@655,4%@655] EMC 3%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [1%@345,100%@2033,0%@2035,1%@346,2%@348,3%@349] EMC 3%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2035,0%@2034,0%@348,3%@348,0%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [3%@345,100%@2034,0%@2035,1%@348,1%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2034,0%@2034,2%@348,4%@348,1%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@1988,0%@1987,2%@346,2%@345,1%@345] EMC 2%@1866 APE 150 GR3D 9%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@2026,0%@2026,1%@347,0%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\nRAM 3661/7851MB (lfb 727x4MB) cpu [8%@345,100%@2024,0%@2028,5%@345,8%@345,1%@345] EMC 2%@1866 APE 150 GR3D 0%@114\n\nAs you can see the RAM are nowhere near full at the moment of the error.\nCan anybody suggest a solution to this?", "body": "I am trying to deploy the pretrained Faster-RCNN Inception V2 from the object detection API on a Jetson TX2. \r\nI am running CUDA 8, cuDNN 6 and have tested with both TF 1.3 and 1.5 in a Jupyter Notebook environment. \r\nWhen I monitor the GPU memory it starts out by having 4.8 GB free and when launching these fills up immediately. When I run on my GTX1060 6 GB GPU I have effectively the same amount of memory free but are having no issues running.\r\nSmaller models as SSD MobileNet runs without problems.\r\n\r\nFrom tests performed today, I can supply the following dumps.\r\n\r\nJupyter Notebook terminal output:\r\n\r\n```\r\n2018-01-17 16:16:19.584106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] ARM64 does not support NUMA - returning NUMA node zero\r\n2018-01-17 16:16:19.584261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: \r\nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005\r\npciBusID: 0000:00:00.0\r\ntotalMemory: 7.67GiB freeMemory: 4.97GiB\r\n2018-01-17 16:16:19.584312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0\r\n2018-01-17 16:16:20.824479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4437 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)\r\n2018-01-17 16:17:09.816477: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:09.816703: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:09.816771: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:09.816912: E tensorflow/stream_executor/cuda/cuda_dnn.cc:2456] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\r\n2018-01-17 16:17:10.174651: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174772: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174806: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174836: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n2018-01-17 16:17:10.174865: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED\r\n```\r\n\r\nError dump from printout inside the notebook:\r\n\r\n```\r\nException in thread Thread-4:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"<ipython-input-5-a51933cd03d8>\", line 19, in worker\r\n    im, t_elapsed = detect_objects(frame_rgb, sess, detection_graph)\r\n  File \"<ipython-input-4-6c8da66803e2>\", line 19, in detect_objects\r\n    feed_dict={image_tensor: image_np_expanded})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInternalError: cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\r\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\r\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\r\n\r\nCaused by op u'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D', defined at:\r\n  File \"/usr/lib/python2.7/threading.py\", line 774, in __bootstrap\r\n    self.__bootstrap_inner()\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"<ipython-input-5-a51933cd03d8>\", line 10, in worker\r\n    tf.import_graph_def(od_graph_def, name='')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 548, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3176, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])\r\n\t [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]\r\n\t [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10508...ield/Equal\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]\r\n```\r\n\r\nOutput of tegrastats at the point of error:\r\n```\r\nRAM 3151/7851MB (lfb 915x4MB) cpu [2%@345,100%@2034,99%@2034,1%@348,3%@348,6%@349] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3151/7851MB (lfb 915x4MB) cpu [0%@345,100%@1981,100%@1988,3%@348,5%@348,4%@349] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2021,100%@2021,4%@348,5%@348,2%@349] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2035,100%@2034,3%@349,4%@348,2%@348] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3152/7851MB (lfb 915x4MB) cpu [1%@345,100%@2016,100%@2019,2%@345,1%@349,3%@348] EMC 5%@1866 APE 150 GR3D 0%@114\r\nRAM 3181/7851MB (lfb 898x4MB) cpu [21%@806,100%@2021,56%@2024,8%@499,10%@500,3%@500] EMC 5%@1866 APE 150 GR3D 24%@114\r\nRAM 3210/7851MB (lfb 887x4MB) cpu [8%@345,100%@2018,32%@2026,7%@345,24%@345,13%@349] EMC 5%@1866 APE 150 GR3D 99%@114\r\nRAM 3327/7851MB (lfb 838x4MB) cpu [2%@1573,100%@1987,31%@1992,35%@1574,13%@1575,5%@1573] EMC 5%@1866 APE 150 GR3D 8%@114\r\nRAM 3578/7851MB (lfb 758x4MB) cpu [19%@1806,100%@2080,0%@2035,7%@2035,2%@2035,56%@1727] EMC 5%@1866 APE 150 GR3D 10%@114\r\nRAM 3732/7851MB (lfb 715x4MB) cpu [2%@345,100%@2034,83%@2035,5%@348,21%@345,2%@346] EMC 7%@1866 APE 150 GR3D 99%@624\r\nRAM 3732/7851MB (lfb 715x4MB) cpu [94%@2036,100%@2035,97%@2034,87%@1987,13%@2035,1%@2035] EMC 4%@1866 APE 150 GR3D 43%@1032\r\nRAM 3659/7851MB (lfb 727x4MB) cpu [2%@653,81%@2022,20%@2027,28%@652,2%@655,4%@655] EMC 3%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [1%@345,100%@2033,0%@2035,1%@346,2%@348,3%@349] EMC 3%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2035,0%@2034,0%@348,3%@348,0%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [3%@345,100%@2034,0%@2035,1%@348,1%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2034,0%@2034,2%@348,4%@348,1%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@1988,0%@1987,2%@346,2%@345,1%@345] EMC 2%@1866 APE 150 GR3D 9%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@2026,0%@2026,1%@347,0%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114\r\nRAM 3661/7851MB (lfb 727x4MB) cpu [8%@345,100%@2024,0%@2028,5%@345,8%@345,1%@345] EMC 2%@1866 APE 150 GR3D 0%@114\r\n```\r\nAs you can see the RAM are nowhere near full at the moment of the error.\r\n\r\nCan anybody suggest a solution to this?"}