{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9522", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9522/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9522/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9522/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9522", "id": 225201495, "node_id": "MDU6SXNzdWUyMjUyMDE0OTU=", "number": 9522, "title": "Changing optimizer during the training gives weird results.", "user": {"login": "abhaydoke09", "id": 13537462, "node_id": "MDQ6VXNlcjEzNTM3NDYy", "avatar_url": "https://avatars0.githubusercontent.com/u/13537462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhaydoke09", "html_url": "https://github.com/abhaydoke09", "followers_url": "https://api.github.com/users/abhaydoke09/followers", "following_url": "https://api.github.com/users/abhaydoke09/following{/other_user}", "gists_url": "https://api.github.com/users/abhaydoke09/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhaydoke09/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhaydoke09/subscriptions", "organizations_url": "https://api.github.com/users/abhaydoke09/orgs", "repos_url": "https://api.github.com/users/abhaydoke09/repos", "events_url": "https://api.github.com/users/abhaydoke09/events{/privacy}", "received_events_url": "https://api.github.com/users/abhaydoke09/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-28T21:49:21Z", "updated_at": "2017-04-28T23:11:51Z", "closed_at": "2017-04-28T23:11:51Z", "author_association": "NONE", "body_html": "<p>I am trying to change the var_list provided to the minimize() function after some iterations. I am trying to implement a two step finetuning, where for first \"n\" iterations, I am training the last layer of the network and after that finetuning the whole network. So for first \"n\" iterations i am providing variables of last layer in var_list and after \"n\" iterations i am providing all variables in the network.<br>\nIt seems like whole network is being reinitialised when I change the optimizer after \"n\" iterations.</p>", "body_text": "I am trying to change the var_list provided to the minimize() function after some iterations. I am trying to implement a two step finetuning, where for first \"n\" iterations, I am training the last layer of the network and after that finetuning the whole network. So for first \"n\" iterations i am providing variables of last layer in var_list and after \"n\" iterations i am providing all variables in the network.\nIt seems like whole network is being reinitialised when I change the optimizer after \"n\" iterations.", "body": "I am trying to change the var_list provided to the minimize() function after some iterations. I am trying to implement a two step finetuning, where for first \"n\" iterations, I am training the last layer of the network and after that finetuning the whole network. So for first \"n\" iterations i am providing variables of last layer in var_list and after \"n\" iterations i am providing all variables in the network. \r\nIt seems like whole network is being reinitialised when I change the optimizer after \"n\" iterations. "}