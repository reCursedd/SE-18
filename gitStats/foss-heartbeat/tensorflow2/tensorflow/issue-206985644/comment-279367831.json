{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279367831", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439#issuecomment-279367831", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "id": 279367831, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTM2NzgzMQ==", "user": {"login": "kvrd18", "id": 10166968, "node_id": "MDQ6VXNlcjEwMTY2OTY4", "avatar_url": "https://avatars3.githubusercontent.com/u/10166968?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kvrd18", "html_url": "https://github.com/kvrd18", "followers_url": "https://api.github.com/users/kvrd18/followers", "following_url": "https://api.github.com/users/kvrd18/following{/other_user}", "gists_url": "https://api.github.com/users/kvrd18/gists{/gist_id}", "starred_url": "https://api.github.com/users/kvrd18/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kvrd18/subscriptions", "organizations_url": "https://api.github.com/users/kvrd18/orgs", "repos_url": "https://api.github.com/users/kvrd18/repos", "events_url": "https://api.github.com/users/kvrd18/events{/privacy}", "received_events_url": "https://api.github.com/users/kvrd18/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-13T11:59:03Z", "updated_at": "2017-02-13T12:05:34Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>, in Torch, the weights updates for each module in a replica are accumulated and summed together on the first replica. Owing to Torch's modular code base, with <code>BatchNormalization</code> being a module, its internal parameters also undergo the same computational flow as described above. Also, Torch has used <a href=\"https://github.com/NVIDIA/nccl\">NCCL</a> to enable fast inter-GPU communication.</p>\n<p>To define a model in Torch, you would do this,</p>\n<div class=\"highlight highlight-source-lua\"><pre><span class=\"pl-k\">function</span> <span class=\"pl-en\">makeConvNet</span>()\n  model <span class=\"pl-k\">=</span> nn.<span class=\"pl-c1\">Sequential</span>()\n  model:<span class=\"pl-c1\">add</span>(nn.<span class=\"pl-c1\">SpatialConvolution</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>))\n  model:<span class=\"pl-c1\">add</span>(nn.<span class=\"pl-c1\">SpatialBatchNormalization</span>(<span class=\"pl-c1\">32</span>))\n  model:<span class=\"pl-c1\">add</span>(nn.<span class=\"pl-c1\">View</span>(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>):<span class=\"pl-c1\">setNumInputDims</span>(<span class=\"pl-c1\">3</span>))\n  <span class=\"pl-k\">return</span> model\n<span class=\"pl-k\">end</span></pre></div>\n<p>Here, <code>nn.SpatialConvolution</code> and <code>nn.SpatialBatchNormalization</code> are modules which have its own <code>forward</code> and <code>backward</code> passes. All you have to do to make it compatible with data parallelism is to invoke <a href=\"https://github.com/torch/cunn/blob/master/DataParallelTable.lua\"><code>nn.DataParallelTable</code></a></p>\n<div class=\"highlight highlight-source-lua\"><pre><span class=\"pl-c\"><span class=\"pl-c\">--</span> CONSTRUCT MODEL:</span>\nconv_net <span class=\"pl-k\">=</span> <span class=\"pl-c1\">makeConvNet</span>()  <span class=\"pl-c\"><span class=\"pl-c\">--</span> i.e. create nn.Sequential() and fill it</span>\nnet <span class=\"pl-k\">=</span> nn.<span class=\"pl-c1\">DataParallelTable</span>(<span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">--</span> Split along first (batch) dimension</span>\nnet:<span class=\"pl-c1\">add</span>(conv_net, {<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>}) <span class=\"pl-c\"><span class=\"pl-c\">--</span> Use GPUs 1 and 2</span>\n<span class=\"pl-c\"><span class=\"pl-c\">--</span> TRAINING:</span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, num_epochs <span class=\"pl-k\">do</span>\n  <span class=\"pl-k\">local</span> output <span class=\"pl-k\">=</span> net:<span class=\"pl-c1\">forward</span>(input)\n  <span class=\"pl-k\">local</span> err <span class=\"pl-k\">=</span> criterion:<span class=\"pl-c1\">forward</span>(output, target)\n  net:<span class=\"pl-c1\">zeroGradParameters</span>()\n  <span class=\"pl-k\">local</span> gradOutput <span class=\"pl-k\">=</span> criterion:<span class=\"pl-c1\">backward</span>(output, target)\n  <span class=\"pl-k\">local</span> gradInput <span class=\"pl-k\">=</span> net:<span class=\"pl-c1\">backward</span>(input, gradOutput)\n  net:<span class=\"pl-c1\">updateParameters</span>(lr)\n<span class=\"pl-k\">end</span></pre></div>", "body_text": "@yaroslavvb, in Torch, the weights updates for each module in a replica are accumulated and summed together on the first replica. Owing to Torch's modular code base, with BatchNormalization being a module, its internal parameters also undergo the same computational flow as described above. Also, Torch has used NCCL to enable fast inter-GPU communication.\nTo define a model in Torch, you would do this,\nfunction makeConvNet()\n  model = nn.Sequential()\n  model:add(nn.SpatialConvolution(1,32,3,3))\n  model:add(nn.SpatialBatchNormalization(32))\n  model:add(nn.View(-1):setNumInputDims(3))\n  return model\nend\nHere, nn.SpatialConvolution and nn.SpatialBatchNormalization are modules which have its own forward and backward passes. All you have to do to make it compatible with data parallelism is to invoke nn.DataParallelTable\n-- CONSTRUCT MODEL:\nconv_net = makeConvNet()  -- i.e. create nn.Sequential() and fill it\nnet = nn.DataParallelTable(1)  -- Split along first (batch) dimension\nnet:add(conv_net, {1, 2}) -- Use GPUs 1 and 2\n-- TRAINING:\nfor i = 1, num_epochs do\n  local output = net:forward(input)\n  local err = criterion:forward(output, target)\n  net:zeroGradParameters()\n  local gradOutput = criterion:backward(output, target)\n  local gradInput = net:backward(input, gradOutput)\n  net:updateParameters(lr)\nend", "body": "@yaroslavvb, in Torch, the weights updates for each module in a replica are accumulated and summed together on the first replica. Owing to Torch's modular code base, with `BatchNormalization` being a module, its internal parameters also undergo the same computational flow as described above. Also, Torch has used [NCCL](https://github.com/NVIDIA/nccl) to enable fast inter-GPU communication.\r\n\r\nTo define a model in Torch, you would do this,\r\n```lua\r\n\r\nfunction makeConvNet()\r\n  model = nn.Sequential()\r\n  model:add(nn.SpatialConvolution(1,32,3,3))\r\n  model:add(nn.SpatialBatchNormalization(32))\r\n  model:add(nn.View(-1):setNumInputDims(3))\r\n  return model\r\nend\r\n```\r\n\r\nHere, `nn.SpatialConvolution` and `nn.SpatialBatchNormalization` are modules which have its own `forward` and `backward` passes. All you have to do to make it compatible with data parallelism is to invoke [`nn.DataParallelTable`](https://github.com/torch/cunn/blob/master/DataParallelTable.lua)\r\n\r\n```lua\r\n-- CONSTRUCT MODEL:\r\nconv_net = makeConvNet()  -- i.e. create nn.Sequential() and fill it\r\nnet = nn.DataParallelTable(1)  -- Split along first (batch) dimension\r\nnet:add(conv_net, {1, 2}) -- Use GPUs 1 and 2\r\n-- TRAINING:\r\nfor i = 1, num_epochs do\r\n  local output = net:forward(input)\r\n  local err = criterion:forward(output, target)\r\n  net:zeroGradParameters()\r\n  local gradOutput = criterion:backward(output, target)\r\n  local gradInput = net:backward(input, gradOutput)\r\n  net:updateParameters(lr)\r\nend\r\n```"}