{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279761406", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439#issuecomment-279761406", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "id": 279761406, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTc2MTQwNg==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-14T16:39:16Z", "updated_at": "2017-02-14T16:39:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10166968\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kvrd18\">@kvrd18</a> It could be an improvement to aggregate the statistics (before the actual normalization), instead of normalizing by each GPU's own statistics. This can avoid potential problems that the statistics of a small batch is too unstable. You can do this in tensorflow but this is going to be very expensive.</p>\n<p>Maybe <a href=\"https://arxiv.org/abs/1702.03275\" rel=\"nofollow\">Batch Renormalization</a> is a better option in this case. It shows a better performance on small batches.</p>", "body_text": "@kvrd18 It could be an improvement to aggregate the statistics (before the actual normalization), instead of normalizing by each GPU's own statistics. This can avoid potential problems that the statistics of a small batch is too unstable. You can do this in tensorflow but this is going to be very expensive.\nMaybe Batch Renormalization is a better option in this case. It shows a better performance on small batches.", "body": "@kvrd18 It could be an improvement to aggregate the statistics (before the actual normalization), instead of normalizing by each GPU's own statistics. This can avoid potential problems that the statistics of a small batch is too unstable. You can do this in tensorflow but this is going to be very expensive.\r\n\r\nMaybe [Batch Renormalization](https://arxiv.org/abs/1702.03275) is a better option in this case. It shows a better performance on small batches."}