{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279193381", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439#issuecomment-279193381", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "id": 279193381, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTE5MzM4MQ==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-12T03:08:51Z", "updated_at": "2017-02-12T03:08:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10166968\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kvrd18\">@kvrd18</a> As pointed out above, there are just too many ways to implement batch norm across GPUs. TensorFlow now doesn't seem to provide a \"default\" way how it is implemented.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> My understanding is that most frameworks (including caffe &amp; torch) doesn't aggregate statistics across GPUs at all. Different GPUs maintain statistics independently and statistics from only one GPU are used at test time. The official inceptionv3 example in tensorflow/models also <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py#L249\">does something similar</a>.<br>\nI've used the same strategy for quite a while and it's working fine. One catch is that in this case it's more important to shuffle the training data, otherwise the statistics on different GPUs are not i.i.d any more.</p>", "body_text": "@kvrd18 As pointed out above, there are just too many ways to implement batch norm across GPUs. TensorFlow now doesn't seem to provide a \"default\" way how it is implemented.\n@yaroslavvb My understanding is that most frameworks (including caffe & torch) doesn't aggregate statistics across GPUs at all. Different GPUs maintain statistics independently and statistics from only one GPU are used at test time. The official inceptionv3 example in tensorflow/models also does something similar.\nI've used the same strategy for quite a while and it's working fine. One catch is that in this case it's more important to shuffle the training data, otherwise the statistics on different GPUs are not i.i.d any more.", "body": "@kvrd18 As pointed out above, there are just too many ways to implement batch norm across GPUs. TensorFlow now doesn't seem to provide a \"default\" way how it is implemented.\r\n\r\n@yaroslavvb My understanding is that most frameworks (including caffe & torch) doesn't aggregate statistics across GPUs at all. Different GPUs maintain statistics independently and statistics from only one GPU are used at test time. The official inceptionv3 example in tensorflow/models also [does something similar](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py#L249).\r\nI've used the same strategy for quite a while and it's working fine. One catch is that in this case it's more important to shuffle the training data, otherwise the statistics on different GPUs are not i.i.d any more. \r\n"}