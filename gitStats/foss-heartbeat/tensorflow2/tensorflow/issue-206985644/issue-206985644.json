{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439", "id": 206985644, "node_id": "MDU6SXNzdWUyMDY5ODU2NDQ=", "number": 7439, "title": "Batch Normalization for Multi-GPU / Data Parallelism", "user": {"login": "kvrd18", "id": 10166968, "node_id": "MDQ6VXNlcjEwMTY2OTY4", "avatar_url": "https://avatars3.githubusercontent.com/u/10166968?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kvrd18", "html_url": "https://github.com/kvrd18", "followers_url": "https://api.github.com/users/kvrd18/followers", "following_url": "https://api.github.com/users/kvrd18/following{/other_user}", "gists_url": "https://api.github.com/users/kvrd18/gists{/gist_id}", "starred_url": "https://api.github.com/users/kvrd18/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kvrd18/subscriptions", "organizations_url": "https://api.github.com/users/kvrd18/orgs", "repos_url": "https://api.github.com/users/kvrd18/repos", "events_url": "https://api.github.com/users/kvrd18/events{/privacy}", "received_events_url": "https://api.github.com/users/kvrd18/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 30, "created_at": "2017-02-11T15:11:24Z", "updated_at": "2018-07-31T20:48:52Z", "closed_at": "2017-02-26T10:25:44Z", "author_association": "NONE", "body_html": "<p>Where is the batch normalization implementation for Multi-GPU scenarios? How does one keep track of <code>mean</code>, <code>variance</code>, <code>offset</code> and <code>scale</code> in the context of the Multi-GPU example as given in the <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\">CIFAR-10 tutorial</a>?</p>\n<p>Why is the question on <a href=\"http://stackoverflow.com/questions/41819080/how-do-i-use-batch-normalization-in-a-multi-gpu-setting-in-tensorflow\" rel=\"nofollow\">StackOverflow</a> left unanswered for so long?</p>\n<p>For all the beauty that it brings with Tensorboard etc.. , it's kinda appalling to see Tensorflow so far behind Torch in terms of its modeling capability. I'd be really glad if someone takes up responsibility and comes up with a decent Batch Normalization implementation for all cases. Even if it is already there, could anyone care enough to make a <strong>good documentation</strong> out of it?</p>\n<p>There are so many issues pertaining to batch normalization with Tensorflow. It's important that you guys straighten this out as batch normalization enables super-fast convergence for very deep networks and it is <strong>REALLY</strong> important for modern day deep learning research.</p>\n<p>PS: Please spare my outburst. I've been a Torch user for more than a year and I had very high hopes on Tensorflow.</p>", "body_text": "Where is the batch normalization implementation for Multi-GPU scenarios? How does one keep track of mean, variance, offset and scale in the context of the Multi-GPU example as given in the CIFAR-10 tutorial?\nWhy is the question on StackOverflow left unanswered for so long?\nFor all the beauty that it brings with Tensorboard etc.. , it's kinda appalling to see Tensorflow so far behind Torch in terms of its modeling capability. I'd be really glad if someone takes up responsibility and comes up with a decent Batch Normalization implementation for all cases. Even if it is already there, could anyone care enough to make a good documentation out of it?\nThere are so many issues pertaining to batch normalization with Tensorflow. It's important that you guys straighten this out as batch normalization enables super-fast convergence for very deep networks and it is REALLY important for modern day deep learning research.\nPS: Please spare my outburst. I've been a Torch user for more than a year and I had very high hopes on Tensorflow.", "body": "Where is the batch normalization implementation for Multi-GPU scenarios? How does one keep track of `mean`, `variance`, `offset` and `scale` in the context of the Multi-GPU example as given in the [CIFAR-10 tutorial](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py)?\r\n\r\nWhy is the question on [StackOverflow](http://stackoverflow.com/questions/41819080/how-do-i-use-batch-normalization-in-a-multi-gpu-setting-in-tensorflow) left unanswered for so long?\r\n\r\nFor all the beauty that it brings with Tensorboard etc.. , it's kinda appalling to see Tensorflow so far behind Torch in terms of its modeling capability. I'd be really glad if someone takes up responsibility and comes up with a decent Batch Normalization implementation for all cases. Even if it is already there, could anyone care enough to make a **good documentation** out of it?\r\n\r\nThere are so many issues pertaining to batch normalization with Tensorflow. It's important that you guys straighten this out as batch normalization enables super-fast convergence for very deep networks and it is **REALLY** important for modern day deep learning research.\r\n\r\nPS: Please spare my outburst. I've been a Torch user for more than a year and I had very high hopes on Tensorflow."}