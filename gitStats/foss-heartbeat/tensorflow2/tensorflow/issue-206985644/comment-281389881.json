{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281389881", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439#issuecomment-281389881", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "id": 281389881, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTM4OTg4MQ==", "user": {"login": "kvrd18", "id": 10166968, "node_id": "MDQ6VXNlcjEwMTY2OTY4", "avatar_url": "https://avatars3.githubusercontent.com/u/10166968?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kvrd18", "html_url": "https://github.com/kvrd18", "followers_url": "https://api.github.com/users/kvrd18/followers", "following_url": "https://api.github.com/users/kvrd18/following{/other_user}", "gists_url": "https://api.github.com/users/kvrd18/gists{/gist_id}", "starred_url": "https://api.github.com/users/kvrd18/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kvrd18/subscriptions", "organizations_url": "https://api.github.com/users/kvrd18/orgs", "repos_url": "https://api.github.com/users/kvrd18/repos", "events_url": "https://api.github.com/users/kvrd18/events{/privacy}", "received_events_url": "https://api.github.com/users/kvrd18/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T16:08:44Z", "updated_at": "2017-02-21T16:08:44Z", "author_association": "NONE", "body_html": "<p>I've built a batch normalization layer for multi-GPU. It predicts well on the validation set only if the <code>is_training</code> is <code>True</code>. Not sure why though. Can someone help me with this?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_variable_on_cpu</span>(<span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">shape</span>, <span class=\"pl-smi\">initializer</span>, <span class=\"pl-smi\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n\n\t<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n\t\tdtype <span class=\"pl-k\">=</span> tf.float32\n\t\tvar <span class=\"pl-k\">=</span> tf.get_variable(name, shape, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>initializer, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span>trainable)\n\t<span class=\"pl-k\">return</span> var\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">BatchNorm</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">is_training</span>, <span class=\"pl-smi\">decay</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.9</span>, <span class=\"pl-smi\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-3</span>):\n\n\tscale <span class=\"pl-k\">=</span> _variable_on_cpu(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>scale<span class=\"pl-pds\">'</span></span>, inputs.get_shape()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], tf.constant_initializer(<span class=\"pl-c1\">1.0</span>))\n\tbeta <span class=\"pl-k\">=</span> _variable_on_cpu(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>beta<span class=\"pl-pds\">'</span></span>, inputs.get_shape()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], tf.constant_initializer(<span class=\"pl-c1\">0.0</span>))\n\tpop_mean <span class=\"pl-k\">=</span> _variable_on_cpu(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mean<span class=\"pl-pds\">'</span></span>, inputs.get_shape()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], tf.constant_initializer(<span class=\"pl-c1\">0.0</span>), <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\tpop_var <span class=\"pl-k\">=</span> _variable_on_cpu(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>variance<span class=\"pl-pds\">'</span></span>, inputs.get_shape()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], tf.constant_initializer(<span class=\"pl-c1\">1.0</span>), <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\taxis <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(inputs.get_shape())<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">Train</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">pop_mean</span>, <span class=\"pl-smi\">pop_var</span>, <span class=\"pl-smi\">scale</span>, <span class=\"pl-smi\">beta</span>):\n\t\tbatch_mean, batch_var <span class=\"pl-k\">=</span> tf.nn.moments(inputs,axis)\n\t\ttrain_mean <span class=\"pl-k\">=</span> tf.assign(pop_mean, pop_mean <span class=\"pl-k\">*</span> decay <span class=\"pl-k\">+</span> batch_mean <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> decay))\n\t\ttrain_var <span class=\"pl-k\">=</span> tf.assign(pop_var, pop_var <span class=\"pl-k\">*</span> decay <span class=\"pl-k\">+</span> batch_var <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> decay))\n\t\t<span class=\"pl-k\">with</span> tf.control_dependencies([train_mean,train_var]):\n\t\t\t<span class=\"pl-k\">return</span> tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, epsilon)\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">Eval</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">pop_mean</span>, <span class=\"pl-smi\">pop_var</span>, <span class=\"pl-smi\">scale</span>, <span class=\"pl-smi\">beta</span>):\n\t\t<span class=\"pl-k\">return</span> tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\n\n\t<span class=\"pl-k\">return</span> tf.cond(is_training, <span class=\"pl-k\">lambda</span>: Train(inputs, pop_mean, pop_var, scale, beta),\n\t\t<span class=\"pl-k\">lambda</span>: Eval(inputs, pop_mean, pop_var, scale, beta))</pre></div>\n<p>This is working well on multi-GPU / data parallelism as long as the module is in training mode.</p>", "body_text": "I've built a batch normalization layer for multi-GPU. It predicts well on the validation set only if the is_training is True. Not sure why though. Can someone help me with this?\ndef _variable_on_cpu(name, shape, initializer, trainable=True):\n\n\twith tf.device('/cpu:0'):\n\t\tdtype = tf.float32\n\t\tvar = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)\n\treturn var\n\ndef BatchNorm(inputs, is_training, decay = 0.9, epsilon=1e-3):\n\n\tscale = _variable_on_cpu('scale', inputs.get_shape()[-1], tf.constant_initializer(1.0))\n\tbeta = _variable_on_cpu('beta', inputs.get_shape()[-1], tf.constant_initializer(0.0))\n\tpop_mean = _variable_on_cpu('mean', inputs.get_shape()[-1], tf.constant_initializer(0.0), trainable=False)\n\tpop_var = _variable_on_cpu('variance', inputs.get_shape()[-1], tf.constant_initializer(1.0), trainable=False)\n\taxis = list(range(len(inputs.get_shape())-1))\n\n\tdef Train(inputs, pop_mean, pop_var, scale, beta):\n\t\tbatch_mean, batch_var = tf.nn.moments(inputs,axis)\n\t\ttrain_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n\t\ttrain_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n\t\twith tf.control_dependencies([train_mean,train_var]):\n\t\t\treturn tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, epsilon)\n\n\tdef Eval(inputs, pop_mean, pop_var, scale, beta):\n\t\treturn tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\n\n\treturn tf.cond(is_training, lambda: Train(inputs, pop_mean, pop_var, scale, beta),\n\t\tlambda: Eval(inputs, pop_mean, pop_var, scale, beta))\nThis is working well on multi-GPU / data parallelism as long as the module is in training mode.", "body": "I've built a batch normalization layer for multi-GPU. It predicts well on the validation set only if the `is_training` is `True`. Not sure why though. Can someone help me with this?\r\n\r\n```python\r\n\r\ndef _variable_on_cpu(name, shape, initializer, trainable=True):\r\n\r\n\twith tf.device('/cpu:0'):\r\n\t\tdtype = tf.float32\r\n\t\tvar = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)\r\n\treturn var\r\n\r\ndef BatchNorm(inputs, is_training, decay = 0.9, epsilon=1e-3):\r\n\r\n\tscale = _variable_on_cpu('scale', inputs.get_shape()[-1], tf.constant_initializer(1.0))\r\n\tbeta = _variable_on_cpu('beta', inputs.get_shape()[-1], tf.constant_initializer(0.0))\r\n\tpop_mean = _variable_on_cpu('mean', inputs.get_shape()[-1], tf.constant_initializer(0.0), trainable=False)\r\n\tpop_var = _variable_on_cpu('variance', inputs.get_shape()[-1], tf.constant_initializer(1.0), trainable=False)\r\n\taxis = list(range(len(inputs.get_shape())-1))\r\n\r\n\tdef Train(inputs, pop_mean, pop_var, scale, beta):\r\n\t\tbatch_mean, batch_var = tf.nn.moments(inputs,axis)\r\n\t\ttrain_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\r\n\t\ttrain_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\r\n\t\twith tf.control_dependencies([train_mean,train_var]):\r\n\t\t\treturn tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, epsilon)\r\n\r\n\tdef Eval(inputs, pop_mean, pop_var, scale, beta):\r\n\t\treturn tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\r\n\r\n\treturn tf.cond(is_training, lambda: Train(inputs, pop_mean, pop_var, scale, beta),\r\n\t\tlambda: Eval(inputs, pop_mean, pop_var, scale, beta))\r\n```\r\n\r\nThis is working well on multi-GPU / data parallelism as long as the module is in training mode."}