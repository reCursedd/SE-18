{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356186494", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439#issuecomment-356186494", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "id": 356186494, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjE4NjQ5NA==", "user": {"login": "John1231983", "id": 24875971, "node_id": "MDQ6VXNlcjI0ODc1OTcx", "avatar_url": "https://avatars3.githubusercontent.com/u/24875971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/John1231983", "html_url": "https://github.com/John1231983", "followers_url": "https://api.github.com/users/John1231983/followers", "following_url": "https://api.github.com/users/John1231983/following{/other_user}", "gists_url": "https://api.github.com/users/John1231983/gists{/gist_id}", "starred_url": "https://api.github.com/users/John1231983/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/John1231983/subscriptions", "organizations_url": "https://api.github.com/users/John1231983/orgs", "repos_url": "https://api.github.com/users/John1231983/repos", "events_url": "https://api.github.com/users/John1231983/events{/privacy}", "received_events_url": "https://api.github.com/users/John1231983/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-09T05:43:17Z", "updated_at": "2018-01-09T05:43:17Z", "author_association": "NONE", "body_html": "<p>Hello, I am training the BatchNorm layer in multiple GPUs using <code>tf.contrib.layers.batch_norm</code> function. In the training phase, we have to collect moving_mean and moving_variance using the function</p>\n<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n</code></pre>\n<p>However, I found that utilization of the function has some ways</p>\n<p>1.<strong>Inside a loop function</strong> <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py#L121\">cifar10_main</a></p>\n<pre><code>with tf.device('/cpu:0'):\n  update_ops=[]\n  with tf.variable_scope(tf.get_variable_scope()):\n     for i in range(self.conf.num_gpus):\n        with tf.device('/gpu:%d' % i):\n\t   with tf.name_scope('device_%d' % i):\n\t      update_ops.extend(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n  variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  with tf.control_dependencies(update_ops):\n     self.train_op = tf.group(train_op_conv,variables_averages_op)\n</code></pre>\n<p>2.<strong>Outside a loop function</strong> <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\">cifar10_multi_gpu</a></p>\n<pre><code>with tf.device('/cpu:0'):\n  with tf.variable_scope(tf.get_variable_scope()):\n     for i in range(self.conf.num_gpus):\n        with tf.device('/gpu:%d' % i):\n\t   with tf.name_scope('device_%d' % i):\n\t      #Igore the line update_ops\n  variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  \n  with tf.control_dependencies(update_ops):\n     self.train_op = tf.group(train_op_conv,variables_averages_op)\n</code></pre>\n<p>3.<strong>Both inside and outside a loop function</strong> <a href=\"https://github.com/tensorflow/models/blob/master/research/inception/inception/inception_train.py\">inception v3</a>, <a href=\"https://github.com/tensorflow/models/blob/dd9a81c03bf924db8d461a3e696ba1bd4bb193fc/tutorials/image/cifar10_estimator/cifar10_main.py\">cifar10</a></p>\n<pre><code>with tf.device('/cpu:0'):\n  with tf.variable_scope(tf.get_variable_scope()):\n     for i in range(self.conf.num_gpus):\n        with tf.device('/gpu:%d' % i):\n\t   with tf.name_scope('device_%d' % i):\n\t      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n  variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())            \n  batchnorm_updates_op = tf.group(*update_ops)\n  self.train_op = tf.group(train_op_conv, train_op_fc,variables_averages_op,batchnorm_updates_op)\n</code></pre>\n<p>What is the right way? In my opinion, it may be the third way</p>", "body_text": "Hello, I am training the BatchNorm layer in multiple GPUs using tf.contrib.layers.batch_norm function. In the training phase, we have to collect moving_mean and moving_variance using the function\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n\nHowever, I found that utilization of the function has some ways\n1.Inside a loop function cifar10_main\nwith tf.device('/cpu:0'):\n  update_ops=[]\n  with tf.variable_scope(tf.get_variable_scope()):\n     for i in range(self.conf.num_gpus):\n        with tf.device('/gpu:%d' % i):\n\t   with tf.name_scope('device_%d' % i):\n\t      update_ops.extend(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n  variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  with tf.control_dependencies(update_ops):\n     self.train_op = tf.group(train_op_conv,variables_averages_op)\n\n2.Outside a loop function cifar10_multi_gpu\nwith tf.device('/cpu:0'):\n  with tf.variable_scope(tf.get_variable_scope()):\n     for i in range(self.conf.num_gpus):\n        with tf.device('/gpu:%d' % i):\n\t   with tf.name_scope('device_%d' % i):\n\t      #Igore the line update_ops\n  variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  \n  with tf.control_dependencies(update_ops):\n     self.train_op = tf.group(train_op_conv,variables_averages_op)\n\n3.Both inside and outside a loop function inception v3, cifar10\nwith tf.device('/cpu:0'):\n  with tf.variable_scope(tf.get_variable_scope()):\n     for i in range(self.conf.num_gpus):\n        with tf.device('/gpu:%d' % i):\n\t   with tf.name_scope('device_%d' % i):\n\t      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n  variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())            \n  batchnorm_updates_op = tf.group(*update_ops)\n  self.train_op = tf.group(train_op_conv, train_op_fc,variables_averages_op,batchnorm_updates_op)\n\nWhat is the right way? In my opinion, it may be the third way", "body": "Hello, I am training the BatchNorm layer in multiple GPUs using `tf.contrib.layers.batch_norm` function. In the training phase, we have to collect moving_mean and moving_variance using the function\r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \r\n\r\nHowever, I found that utilization of the function has some ways\r\n\r\n1.**Inside a loop function** [cifar10_main][1]\r\n\r\n\r\n    with tf.device('/cpu:0'):\r\n      update_ops=[]\r\n      with tf.variable_scope(tf.get_variable_scope()):\r\n         for i in range(self.conf.num_gpus):\r\n            with tf.device('/gpu:%d' % i):\r\n    \t   with tf.name_scope('device_%d' % i):\r\n    \t      update_ops.extend(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\r\n      variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\r\n      variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n      with tf.control_dependencies(update_ops):\r\n         self.train_op = tf.group(train_op_conv,variables_averages_op)\r\n\r\n2.**Outside a loop function** [cifar10_multi_gpu][2]\r\n\r\n\r\n    with tf.device('/cpu:0'):\r\n      with tf.variable_scope(tf.get_variable_scope()):\r\n         for i in range(self.conf.num_gpus):\r\n            with tf.device('/gpu:%d' % i):\r\n    \t   with tf.name_scope('device_%d' % i):\r\n    \t      #Igore the line update_ops\r\n      variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\r\n      variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  \r\n      with tf.control_dependencies(update_ops):\r\n         self.train_op = tf.group(train_op_conv,variables_averages_op)\r\n\r\n3.**Both inside and outside a loop function** [inception v3][3], [cifar10][4]\r\n\r\n    with tf.device('/cpu:0'):\r\n      with tf.variable_scope(tf.get_variable_scope()):\r\n         for i in range(self.conf.num_gpus):\r\n            with tf.device('/gpu:%d' % i):\r\n    \t   with tf.name_scope('device_%d' % i):\r\n    \t      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \r\n      variable_averages = tf.train.ExponentialMovingAverage(self.conf.MOVING_AVERAGE_DECAY, global_step)\r\n      variables_averages_op = variable_averages.apply(tf.trainable_variables())            \r\n      batchnorm_updates_op = tf.group(*update_ops)\r\n      self.train_op = tf.group(train_op_conv, train_op_fc,variables_averages_op,batchnorm_updates_op)\r\n\r\nWhat is the right way? In my opinion, it may be the third way\r\n\r\n\r\n  [1]: https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py#L121\r\n  [2]: https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\r\n  [3]: https://github.com/tensorflow/models/blob/master/research/inception/inception/inception_train.py\r\n  [4]: https://github.com/tensorflow/models/blob/dd9a81c03bf924db8d461a3e696ba1bd4bb193fc/tutorials/image/cifar10_estimator/cifar10_main.py"}