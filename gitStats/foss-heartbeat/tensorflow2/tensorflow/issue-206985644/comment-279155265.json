{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279155265", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439#issuecomment-279155265", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "id": 279155265, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTE1NTI2NQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-11T16:01:08Z", "updated_at": "2017-02-12T16:03:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>How does Torch handle multi-GPU batch normalization? Batch normalization on multi-GPU batch incurs and extra performance penalty because statistics need to be communicated across all GPUs, so are some performance questions to consider in. You can aggregate statistics on CPU, aggregate them by going around in a ring along the lines of how Nvidia NCCL all-reduce does, or aggregate them by doing a tree reduction.</p>\n<p>Also you can also do a \"pseudo-batch normalization\", by using existing batch norm layer to normalize GPU-sized batches, and then add batches together for a single \"multi-GPU batch\".</p>\n<p>I suspect there are easier ways to handle normalization of huge batches that doesn't introduce the performance hit you would see with batch normalization, like weight normalization -- <a href=\"https://arxiv.org/pdf/1602.07868.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1602.07868.pdf</a></p>", "body_text": "How does Torch handle multi-GPU batch normalization? Batch normalization on multi-GPU batch incurs and extra performance penalty because statistics need to be communicated across all GPUs, so are some performance questions to consider in. You can aggregate statistics on CPU, aggregate them by going around in a ring along the lines of how Nvidia NCCL all-reduce does, or aggregate them by doing a tree reduction.\nAlso you can also do a \"pseudo-batch normalization\", by using existing batch norm layer to normalize GPU-sized batches, and then add batches together for a single \"multi-GPU batch\".\nI suspect there are easier ways to handle normalization of huge batches that doesn't introduce the performance hit you would see with batch normalization, like weight normalization -- https://arxiv.org/pdf/1602.07868.pdf", "body": "How does Torch handle multi-GPU batch normalization? Batch normalization on multi-GPU batch incurs and extra performance penalty because statistics need to be communicated across all GPUs, so are some performance questions to consider in. You can aggregate statistics on CPU, aggregate them by going around in a ring along the lines of how Nvidia NCCL all-reduce does, or aggregate them by doing a tree reduction.\r\n\r\nAlso you can also do a \"pseudo-batch normalization\", by using existing batch norm layer to normalize GPU-sized batches, and then add batches together for a single \"multi-GPU batch\".\r\n\r\nI suspect there are easier ways to handle normalization of huge batches that doesn't introduce the performance hit you would see with batch normalization, like weight normalization -- https://arxiv.org/pdf/1602.07868.pdf"}