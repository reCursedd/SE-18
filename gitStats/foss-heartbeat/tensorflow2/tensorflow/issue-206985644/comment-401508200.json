{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/401508200", "html_url": "https://github.com/tensorflow/tensorflow/issues/7439#issuecomment-401508200", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7439", "id": 401508200, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTUwODIwMA==", "user": {"login": "MrWanter", "id": 18298163, "node_id": "MDQ6VXNlcjE4Mjk4MTYz", "avatar_url": "https://avatars0.githubusercontent.com/u/18298163?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrWanter", "html_url": "https://github.com/MrWanter", "followers_url": "https://api.github.com/users/MrWanter/followers", "following_url": "https://api.github.com/users/MrWanter/following{/other_user}", "gists_url": "https://api.github.com/users/MrWanter/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrWanter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrWanter/subscriptions", "organizations_url": "https://api.github.com/users/MrWanter/orgs", "repos_url": "https://api.github.com/users/MrWanter/repos", "events_url": "https://api.github.com/users/MrWanter/events{/privacy}", "received_events_url": "https://api.github.com/users/MrWanter/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-30T01:26:46Z", "updated_at": "2018-06-30T01:26:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13829174\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/holyseven\">@holyseven</a>'s implementation seems split batch and construct each layer on each gpu one by one, but in distributed training with multiple workers would that be hard to integrate into existing functions like <code>tf.train.replica_device_setter</code>, also split whole batch across workers with this implementation seems infeasible?</p>", "body_text": "@holyseven's implementation seems split batch and construct each layer on each gpu one by one, but in distributed training with multiple workers would that be hard to integrate into existing functions like tf.train.replica_device_setter, also split whole batch across workers with this implementation seems infeasible?", "body": "@holyseven's implementation seems split batch and construct each layer on each gpu one by one, but in distributed training with multiple workers would that be hard to integrate into existing functions like `tf.train.replica_device_setter`, also split whole batch across workers with this implementation seems infeasible?"}