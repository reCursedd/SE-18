{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12002", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12002/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12002/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12002/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12002", "id": 247721495, "node_id": "MDU6SXNzdWUyNDc3MjE0OTU=", "number": 12002, "title": "tf.nn.sparse_softmax_cross_entropy_with_logits() seems to return bad values !", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 21, "created_at": "2017-08-03T14:17:14Z", "updated_at": "2018-05-17T01:00:14Z", "closed_at": "2018-05-17T01:00:14Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: from pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: 'v1.2.0-5-g435cdfc', '1.2.1'</li>\n<li><strong>Python version</strong>: Python 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: cudNN v8.0</li>\n<li><strong>GPU model and memory</strong>: 2* NVidia GeForce 1080Ti (11Go each)</li>\n<li><strong>Exact command to reproduce</strong>: Following code</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>It seems that <code>tf.nn.sparse_softmax_cross_entropy_with_logits() </code> and <code>tf.nn.softmax_cross_entropy_with_logits()</code> are returning bad values. According to this <a href=\"https://stackoverflow.com/questions/36078411/tensorflow-are-my-logits-in-the-right-format-for-cross-entropy-function/36086477#36086477\" rel=\"nofollow\">stackOverflow post</a>, <code>tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)</code> is almost equivalent to <code>-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1))</code>.<br>\nBut when I'm using the provided optimized function, I don't get the same results. It appears that its come from the log function when logits is equal to 0. But I've read that <code>tf.nn.sparse_softmax_cross_entropy_with_logits()</code> handle that case, and that's the case, cause I would have some <em>Nan</em> output. But instead I have huge numbers, so I (naturally) thought that to avoid <em>log(0)</em> a small constant must have been added to the problematic numbers. So I tried to reproduce this tip (with 1e-10) and I don't still have the same result. So I tried to read the code <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/nn_ops.py\">here</a> to understand what's going on. But I can't find in the repo the gen_nn_ops module to understand why this function returns such a \"strange\" result. I will be pleased to contribute to understand what happens (and to correct it, if needed of course !).<br>\nHere is a really simple piece of code to reproduce the \"error\" (if it's indeed one). And we have the same behavior with the sparse version of the function (with proper labels).<br>\nThanks,</p>\n<h3>Source code / logs</h3>\n<pre><code>graph = tf.Graph()\n\nfeatures = np.array([[6.83324017e-02, 4.55211316e-01,-1.41892820e-01, 6.41751984e-01, -5.45895865e-01, 5.38657679e-01, 1.93379897e-01, 1.60154529e-01, 1.57859872e-02, 1.36758294e-02, 4.40859703e+00, 4.96067050e+03, -5.95230431e+01, 2.29624126e+00, 4.02069655e+00], [8.82284599e-01, 6.42900440e-01,-4.27639642e-02, 1.83567706e-01, 7.52404702e-01, -6.32605771e-01, 5.40391531e-01, 5.84584613e-01,-7.15044264e-03,-8.23328268e-02, 6.29273115e+00,-4.32369561e+01, 7.07259958e+00,-1.02810233e+00,-7.04034886e-01], [5.48660773e-01, 8.08794529e-01,-5.96924524e-02,-7.26052964e-01,-2.70772000e-02, 6.87105464e-01, 5.68913359e-01, 4.76252594e-01, 4.14203699e-02,-5.79935485e-03, 9.40232256e+00,-2.01665599e+04, 1.34500232e+01,-2.24989629e-01, 2.52753983e-01], [7.46613308e-01, 8.23272733e-01,-1.04753678e-01, 7.87653516e-01, 5.33736860e-01, 3.07777360e-01, 8.51814816e-01, 7.29870149e-01,-5.67521706e-03, 2.37203887e-02, 6.33280960e+00, 4.08845288e+05, 4.48007235e+01, 5.33139458e-02, 2.37384134e-02], [4.47498908e-01, 1.49080014e-01,-9.07106172e-03,-2.67174181e-01,-5.21700457e-01, 8.10213916e-01, 9.18038857e-01, 8.36740457e-01,-7.64173908e-03,-1.18870530e-02, 6.18394833e+00, 7.37307204e+01,-5.58432681e+01, 3.83996968e-01, 9.18497562e-01], [4.71607629e-01, 1.31179570e-01,-4.56846546e-02,-9.27597302e-01,-3.63639607e-01,-8.56123912e-02, 3.32925650e-01, 2.86999292e-01,-1.37396795e-01,-2.39745171e-01, 6.28318531e+00,-9.03421275e+04,-9.83543039e+03,-1.09839821e+00, 1.05041514e+00], [4.71613040e-01, 1.31166299e-01,-4.56797268e-02,-9.27775404e-01,-3.64117510e-01,-8.15551274e-02, 3.32854008e-01, 2.86979856e-01,-1.36950051e-01,-2.39623484e-01, 6.28318531e+00,-2.85787226e+05, 1.02588457e+05,-1.09795489e+00, 1.05020120e+00], [1.72510574e-01, 3.40244123e-02,-1.78258372e-01,-1.78623912e-01, 9.82406854e-01,-5.45001987e-02, 6.49133952e-01, 4.58514334e-01,-1.05587941e-01,-1.50382361e-01, 6.56445597e+00,-7.39915259e+01,-3.39043636e+01, 8.32312454e-01, 1.66266815e+00]])\n\nlabels = np.array([[ 1., 0.], [ 0., 1.], [ 0., 1.], [ 1., 0.], [ 0., 1.], [ 0., 1.], [ 0., 1.], [ 1., 0.]])\n\ntotalLoss = 0\ntotalTest = 0\n\nwith graph.as_default():\n    x = tf.placeholder(\"float\", [None, 15], name = \"x\")\n    y = tf.placeholder(\"int64\", [None, 2], name = \"y\")\n\n    h1 = tf.Variable(tf.truncated_normal([15, 100], stddev = 0.1), name = \"h1\") \n    out = tf.Variable(tf.truncated_normal([100, 2], stddev = 0.1), name = \"out\")\n    b1 =  tf.Variable(tf.truncated_normal([100], stddev = 0.1), name = \"b1\")\n    bout = tf.Variable(tf.truncated_normal([2], stddev = 0.1), name = \"bout\")\n\n\n    def model(x):\n        layer_1 = tf.add(tf.matmul(x, h1), b1)\n        layer_1 = tf.nn.relu(layer_1)\n\n        out_layer = tf.matmul(layer_1, out) + bout\n        return out_layer\n\n    logits = model(x)\n    \n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n\n    with tf.Session(graph = graph) as session:\n        for i in xrange(10):\n            tf.global_variables_initializer().run()\n            \n            _ = session.run(optimizer, feed_dict = {x : features, y : labels})\n            l = session.run(loss, feed_dict = {x : features, y : labels})\n            test = session.run(tf.reduce_mean(-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1)), feed_dict = {x : features})\n            totalLoss += l \n            totalTest += test\n\nprint(\"mathematical : \", totalTest *1. /10)\nprint(\"sparse_softmax_cross_entropy : \", totalLoss *1. /10)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): from pip\nTensorFlow version (use command below): 'v1.2.0-5-g435cdfc', '1.2.1'\nPython version: Python 2.7.12\nBazel version (if compiling from source):\nCUDA/cuDNN version: cudNN v8.0\nGPU model and memory: 2* NVidia GeForce 1080Ti (11Go each)\nExact command to reproduce: Following code\n\nDescribe the problem\nIt seems that tf.nn.sparse_softmax_cross_entropy_with_logits()  and tf.nn.softmax_cross_entropy_with_logits() are returning bad values. According to this stackOverflow post, tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels) is almost equivalent to -tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1)).\nBut when I'm using the provided optimized function, I don't get the same results. It appears that its come from the log function when logits is equal to 0. But I've read that tf.nn.sparse_softmax_cross_entropy_with_logits() handle that case, and that's the case, cause I would have some Nan output. But instead I have huge numbers, so I (naturally) thought that to avoid log(0) a small constant must have been added to the problematic numbers. So I tried to reproduce this tip (with 1e-10) and I don't still have the same result. So I tried to read the code here to understand what's going on. But I can't find in the repo the gen_nn_ops module to understand why this function returns such a \"strange\" result. I will be pleased to contribute to understand what happens (and to correct it, if needed of course !).\nHere is a really simple piece of code to reproduce the \"error\" (if it's indeed one). And we have the same behavior with the sparse version of the function (with proper labels).\nThanks,\nSource code / logs\ngraph = tf.Graph()\n\nfeatures = np.array([[6.83324017e-02, 4.55211316e-01,-1.41892820e-01, 6.41751984e-01, -5.45895865e-01, 5.38657679e-01, 1.93379897e-01, 1.60154529e-01, 1.57859872e-02, 1.36758294e-02, 4.40859703e+00, 4.96067050e+03, -5.95230431e+01, 2.29624126e+00, 4.02069655e+00], [8.82284599e-01, 6.42900440e-01,-4.27639642e-02, 1.83567706e-01, 7.52404702e-01, -6.32605771e-01, 5.40391531e-01, 5.84584613e-01,-7.15044264e-03,-8.23328268e-02, 6.29273115e+00,-4.32369561e+01, 7.07259958e+00,-1.02810233e+00,-7.04034886e-01], [5.48660773e-01, 8.08794529e-01,-5.96924524e-02,-7.26052964e-01,-2.70772000e-02, 6.87105464e-01, 5.68913359e-01, 4.76252594e-01, 4.14203699e-02,-5.79935485e-03, 9.40232256e+00,-2.01665599e+04, 1.34500232e+01,-2.24989629e-01, 2.52753983e-01], [7.46613308e-01, 8.23272733e-01,-1.04753678e-01, 7.87653516e-01, 5.33736860e-01, 3.07777360e-01, 8.51814816e-01, 7.29870149e-01,-5.67521706e-03, 2.37203887e-02, 6.33280960e+00, 4.08845288e+05, 4.48007235e+01, 5.33139458e-02, 2.37384134e-02], [4.47498908e-01, 1.49080014e-01,-9.07106172e-03,-2.67174181e-01,-5.21700457e-01, 8.10213916e-01, 9.18038857e-01, 8.36740457e-01,-7.64173908e-03,-1.18870530e-02, 6.18394833e+00, 7.37307204e+01,-5.58432681e+01, 3.83996968e-01, 9.18497562e-01], [4.71607629e-01, 1.31179570e-01,-4.56846546e-02,-9.27597302e-01,-3.63639607e-01,-8.56123912e-02, 3.32925650e-01, 2.86999292e-01,-1.37396795e-01,-2.39745171e-01, 6.28318531e+00,-9.03421275e+04,-9.83543039e+03,-1.09839821e+00, 1.05041514e+00], [4.71613040e-01, 1.31166299e-01,-4.56797268e-02,-9.27775404e-01,-3.64117510e-01,-8.15551274e-02, 3.32854008e-01, 2.86979856e-01,-1.36950051e-01,-2.39623484e-01, 6.28318531e+00,-2.85787226e+05, 1.02588457e+05,-1.09795489e+00, 1.05020120e+00], [1.72510574e-01, 3.40244123e-02,-1.78258372e-01,-1.78623912e-01, 9.82406854e-01,-5.45001987e-02, 6.49133952e-01, 4.58514334e-01,-1.05587941e-01,-1.50382361e-01, 6.56445597e+00,-7.39915259e+01,-3.39043636e+01, 8.32312454e-01, 1.66266815e+00]])\n\nlabels = np.array([[ 1., 0.], [ 0., 1.], [ 0., 1.], [ 1., 0.], [ 0., 1.], [ 0., 1.], [ 0., 1.], [ 1., 0.]])\n\ntotalLoss = 0\ntotalTest = 0\n\nwith graph.as_default():\n    x = tf.placeholder(\"float\", [None, 15], name = \"x\")\n    y = tf.placeholder(\"int64\", [None, 2], name = \"y\")\n\n    h1 = tf.Variable(tf.truncated_normal([15, 100], stddev = 0.1), name = \"h1\") \n    out = tf.Variable(tf.truncated_normal([100, 2], stddev = 0.1), name = \"out\")\n    b1 =  tf.Variable(tf.truncated_normal([100], stddev = 0.1), name = \"b1\")\n    bout = tf.Variable(tf.truncated_normal([2], stddev = 0.1), name = \"bout\")\n\n\n    def model(x):\n        layer_1 = tf.add(tf.matmul(x, h1), b1)\n        layer_1 = tf.nn.relu(layer_1)\n\n        out_layer = tf.matmul(layer_1, out) + bout\n        return out_layer\n\n    logits = model(x)\n    \n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n\n    with tf.Session(graph = graph) as session:\n        for i in xrange(10):\n            tf.global_variables_initializer().run()\n            \n            _ = session.run(optimizer, feed_dict = {x : features, y : labels})\n            l = session.run(loss, feed_dict = {x : features, y : labels})\n            test = session.run(tf.reduce_mean(-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1)), feed_dict = {x : features})\n            totalLoss += l \n            totalTest += test\n\nprint(\"mathematical : \", totalTest *1. /10)\nprint(\"sparse_softmax_cross_entropy : \", totalLoss *1. /10)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: from pip\r\n- **TensorFlow version (use command below)**: 'v1.2.0-5-g435cdfc', '1.2.1'\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cudNN v8.0\r\n- **GPU model and memory**: 2* NVidia GeForce 1080Ti (11Go each)\r\n- **Exact command to reproduce**: Following code\r\n\r\n### Describe the problem\r\nIt seems that `tf.nn.sparse_softmax_cross_entropy_with_logits() ` and `tf.nn.softmax_cross_entropy_with_logits()` are returning bad values. According to this [stackOverflow post](https://stackoverflow.com/questions/36078411/tensorflow-are-my-logits-in-the-right-format-for-cross-entropy-function/36086477#36086477), `tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)` is almost equivalent to `-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1))`.\r\nBut when I'm using the provided optimized function, I don't get the same results. It appears that its come from the log function when logits is equal to 0. But I've read that `tf.nn.sparse_softmax_cross_entropy_with_logits()` handle that case, and that's the case, cause I would have some *Nan* output. But instead I have huge numbers, so I (naturally) thought that to avoid *log(0)* a small constant must have been added to the problematic numbers. So I tried to reproduce this tip (with 1e-10) and I don't still have the same result. So I tried to read the code [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/nn_ops.py) to understand what's going on. But I can't find in the repo the gen_nn_ops module to understand why this function returns such a \"strange\" result. I will be pleased to contribute to understand what happens (and to correct it, if needed of course !).\r\nHere is a really simple piece of code to reproduce the \"error\" (if it's indeed one). And we have the same behavior with the sparse version of the function (with proper labels).\r\nThanks, \r\n### Source code / logs\r\n```\r\ngraph = tf.Graph()\r\n\r\nfeatures = np.array([[6.83324017e-02, 4.55211316e-01,-1.41892820e-01, 6.41751984e-01, -5.45895865e-01, 5.38657679e-01, 1.93379897e-01, 1.60154529e-01, 1.57859872e-02, 1.36758294e-02, 4.40859703e+00, 4.96067050e+03, -5.95230431e+01, 2.29624126e+00, 4.02069655e+00], [8.82284599e-01, 6.42900440e-01,-4.27639642e-02, 1.83567706e-01, 7.52404702e-01, -6.32605771e-01, 5.40391531e-01, 5.84584613e-01,-7.15044264e-03,-8.23328268e-02, 6.29273115e+00,-4.32369561e+01, 7.07259958e+00,-1.02810233e+00,-7.04034886e-01], [5.48660773e-01, 8.08794529e-01,-5.96924524e-02,-7.26052964e-01,-2.70772000e-02, 6.87105464e-01, 5.68913359e-01, 4.76252594e-01, 4.14203699e-02,-5.79935485e-03, 9.40232256e+00,-2.01665599e+04, 1.34500232e+01,-2.24989629e-01, 2.52753983e-01], [7.46613308e-01, 8.23272733e-01,-1.04753678e-01, 7.87653516e-01, 5.33736860e-01, 3.07777360e-01, 8.51814816e-01, 7.29870149e-01,-5.67521706e-03, 2.37203887e-02, 6.33280960e+00, 4.08845288e+05, 4.48007235e+01, 5.33139458e-02, 2.37384134e-02], [4.47498908e-01, 1.49080014e-01,-9.07106172e-03,-2.67174181e-01,-5.21700457e-01, 8.10213916e-01, 9.18038857e-01, 8.36740457e-01,-7.64173908e-03,-1.18870530e-02, 6.18394833e+00, 7.37307204e+01,-5.58432681e+01, 3.83996968e-01, 9.18497562e-01], [4.71607629e-01, 1.31179570e-01,-4.56846546e-02,-9.27597302e-01,-3.63639607e-01,-8.56123912e-02, 3.32925650e-01, 2.86999292e-01,-1.37396795e-01,-2.39745171e-01, 6.28318531e+00,-9.03421275e+04,-9.83543039e+03,-1.09839821e+00, 1.05041514e+00], [4.71613040e-01, 1.31166299e-01,-4.56797268e-02,-9.27775404e-01,-3.64117510e-01,-8.15551274e-02, 3.32854008e-01, 2.86979856e-01,-1.36950051e-01,-2.39623484e-01, 6.28318531e+00,-2.85787226e+05, 1.02588457e+05,-1.09795489e+00, 1.05020120e+00], [1.72510574e-01, 3.40244123e-02,-1.78258372e-01,-1.78623912e-01, 9.82406854e-01,-5.45001987e-02, 6.49133952e-01, 4.58514334e-01,-1.05587941e-01,-1.50382361e-01, 6.56445597e+00,-7.39915259e+01,-3.39043636e+01, 8.32312454e-01, 1.66266815e+00]])\r\n\r\nlabels = np.array([[ 1., 0.], [ 0., 1.], [ 0., 1.], [ 1., 0.], [ 0., 1.], [ 0., 1.], [ 0., 1.], [ 1., 0.]])\r\n\r\ntotalLoss = 0\r\ntotalTest = 0\r\n\r\nwith graph.as_default():\r\n    x = tf.placeholder(\"float\", [None, 15], name = \"x\")\r\n    y = tf.placeholder(\"int64\", [None, 2], name = \"y\")\r\n\r\n    h1 = tf.Variable(tf.truncated_normal([15, 100], stddev = 0.1), name = \"h1\") \r\n    out = tf.Variable(tf.truncated_normal([100, 2], stddev = 0.1), name = \"out\")\r\n    b1 =  tf.Variable(tf.truncated_normal([100], stddev = 0.1), name = \"b1\")\r\n    bout = tf.Variable(tf.truncated_normal([2], stddev = 0.1), name = \"bout\")\r\n\r\n\r\n    def model(x):\r\n        layer_1 = tf.add(tf.matmul(x, h1), b1)\r\n        layer_1 = tf.nn.relu(layer_1)\r\n\r\n        out_layer = tf.matmul(layer_1, out) + bout\r\n        return out_layer\r\n\r\n    logits = model(x)\r\n    \r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\r\n\r\n    with tf.Session(graph = graph) as session:\r\n        for i in xrange(10):\r\n            tf.global_variables_initializer().run()\r\n            \r\n            _ = session.run(optimizer, feed_dict = {x : features, y : labels})\r\n            l = session.run(loss, feed_dict = {x : features, y : labels})\r\n            test = session.run(tf.reduce_mean(-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1)), feed_dict = {x : features})\r\n            totalLoss += l \r\n            totalTest += test\r\n\r\nprint(\"mathematical : \", totalTest *1. /10)\r\nprint(\"sparse_softmax_cross_entropy : \", totalLoss *1. /10)\r\n```\r\n"}