{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209400036", "pull_request_review_id": 145404944, "id": 209400036, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTQwMDAzNg==", "diff_hunk": "@@ -0,0 +1,345 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/dataset.h\"\n+\n+#include \"parquet/api/reader.h\"\n+\n+namespace tensorflow {\n+\n+class ParquetDatasetOp : public DatasetOpKernel {\n+ public:\n+  using DatasetOpKernel::DatasetOpKernel;\n+  explicit ParquetDatasetOp(OpKernelConstruction* ctx) : DatasetOpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"output_types\", &output_types_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"output_shapes\", &output_shapes_));\n+    for (const DataType& dt : output_types_) {\n+      OP_REQUIRES(ctx, dt == DT_INT32 || dt == DT_INT64 || dt == DT_FLOAT ||\n+                           dt == DT_DOUBLE || dt == DT_BOOL,\n+                  errors::InvalidArgument(\n+                      \"Each element of `output_types_` must be one of: \"\n+                      \"DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE, DT_BOOL \"));\n+    }\n+    for (const PartialTensorShape& pts : output_shapes_) {\n+      OP_REQUIRES(ctx, pts.dims() == 0,\n+                  errors::InvalidArgument(\n+                      \"Each element of `output_shapes_` must be a scalar.\"));\n+    }\n+  }\n+  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n+    const Tensor* filenames_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"filenames\", &filenames_tensor));\n+    OP_REQUIRES(\n+        ctx, filenames_tensor->dims() <= 1,\n+        errors::InvalidArgument(\"`filenames` must be a scalar or a vector.\"));\n+\n+    const Tensor* columns_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"columns\", &columns_tensor));\n+    OP_REQUIRES(\n+        ctx, columns_tensor->dims() <= 1,\n+        errors::InvalidArgument(\"`columns` must be a scalar or a vector.\"));\n+\n+    std::vector<string> filenames;\n+    filenames.reserve(filenames_tensor->NumElements());\n+    for (int i = 0; i < filenames_tensor->NumElements(); ++i) {\n+      filenames.push_back(filenames_tensor->flat<string>()(i));\n+    }\n+\n+    std::vector<int64> columns;\n+    columns.reserve(columns_tensor->NumElements());\n+    for (int i = 0; i < columns_tensor->NumElements(); ++i) {\n+      columns.push_back(columns_tensor->flat<int32>()(i));\n+    }\n+\n+    *output =\n+        new Dataset(ctx, filenames, columns, output_types_, output_shapes_);\n+  }\n+\n+ private:\n+  class Dataset : public GraphDatasetBase {\n+   public:\n+    Dataset(OpKernelContext* ctx, const std::vector<string>& filenames,\n+            const std::vector<int64>& columns,\n+            const DataTypeVector& output_types,\n+            const std::vector<PartialTensorShape>& output_shapes)\n+        : GraphDatasetBase(ctx),\n+          filenames_(filenames),\n+          columns_(columns),\n+          output_types_(output_types),\n+          output_shapes_(output_shapes) {}\n+\n+    std::unique_ptr<IteratorBase> MakeIterator(\n+        const string& prefix) const override {\n+      return std::unique_ptr<IteratorBase>(\n+          new Iterator({this, strings::StrCat(prefix, \"::Parquet\")}));\n+    }\n+\n+    const DataTypeVector& output_dtypes() const override {\n+      return output_types_;\n+    }\n+\n+    const std::vector<PartialTensorShape>& output_shapes() const override {\n+      return output_shapes_;\n+    }\n+\n+    string DebugString() override { return \"ParquetDatasetOp::Dataset\"; }\n+\n+   protected:\n+    Status AsGraphDefInternal(DatasetGraphDefBuilder* b,\n+                              Node** output) const override {\n+      Node* filenames = nullptr;\n+      TF_RETURN_IF_ERROR(b->AddVector(filenames_, &filenames));\n+      Node* columns = nullptr;\n+      TF_RETURN_IF_ERROR(b->AddVector(columns_, &columns));\n+      TF_RETURN_IF_ERROR(b->AddDataset(this, {filenames, columns}, output));\n+      return Status::OK();\n+    }\n+\n+   private:\n+    class Iterator : public DatasetIterator<Dataset> {\n+     public:\n+      explicit Iterator(const Params& params)\n+          : DatasetIterator<Dataset>(params) {}\n+\n+      Status GetNextInternal(IteratorContext* ctx,\n+                             std::vector<Tensor>* out_tensors,\n+                             bool* end_of_sequence) override {\n+        mutex_lock l(mu_);\n+        do {\n+          // We are currently processing a file, so try to read the next row\n+          // group.\n+          if (parquet_reader_) {\n+            while (current_row_group_ < file_metadata_->num_row_groups()) {\n+              if (current_row_ < row_group_reader_->metadata()->num_rows()) {\n+                // Read columns to outputs.\n+                for (int64 i = 0; i < dataset()->columns_.size(); i++) {\n+                  DataType dt = dataset()->output_types_[i];\n+                  int64 column = dataset()->columns_[i];\n+                  Tensor tensor(ctx->allocator({}), dt, {});\n+                  TF_RETURN_IF_ERROR(\n+                      GetTensorValue(current_row_, dt, column, &tensor));\n+                  out_tensors->emplace_back(std::move(tensor));\n+                }\n+                ++current_row_;\n+                *end_of_sequence = false;\n+                return Status::OK();\n+              }\n+              // We have reached the end of the current row group, so maybe\n+              // move on to next row group.\n+              current_row_ = 0;\n+              row_group_reader_.reset();\n+              ++current_row_group_;\n+              if (current_row_group_ < file_metadata_->num_row_groups()) {\n+                row_group_reader_ =\n+                    parquet_reader_->RowGroup(current_row_group_);\n+              }\n+            }\n+            // We have reached the end of the current file, so maybe\n+            // move on to next file.\n+            ResetStreamsLocked();\n+            ++current_file_index_;\n+          }\n+\n+          // Iteration ends when there are no more files to process.\n+          if (current_file_index_ == dataset()->filenames_.size()) {\n+            *end_of_sequence = true;\n+            return Status::OK();\n+          }\n+\n+          TF_RETURN_IF_ERROR(SetupStreamsLocked(ctx->env()));\n+        } while (true);", "path": "tensorflow/contrib/parquet/kernels/parquet_dataset_ops.cc", "position": null, "original_position": 162, "commit_id": "f3a19282c0acf60fa0495591965f36ff57979719", "original_commit_id": "00764729afceb7b36be5f1d1154023ec19d06619", "user": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "body": "nit: change this to a regular while instead of a do-while, and add a comment to the top of the loop explaining the exit conditions (e.g., \"Loop until we find a row to read or there are no more files left to read\")", "created_at": "2018-08-10T22:40:22Z", "updated_at": "2018-09-14T15:33:49Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19461#discussion_r209400036", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19461", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209400036"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19461#discussion_r209400036"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19461"}}, "body_html": "<p>nit: change this to a regular while instead of a do-while, and add a comment to the top of the loop explaining the exit conditions (e.g., \"Loop until we find a row to read or there are no more files left to read\")</p>", "body_text": "nit: change this to a regular while instead of a do-while, and add a comment to the top of the loop explaining the exit conditions (e.g., \"Loop until we find a row to read or there are no more files left to read\")"}