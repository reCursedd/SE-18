{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/378427799", "html_url": "https://github.com/tensorflow/tensorflow/issues/6850#issuecomment-378427799", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6850", "id": 378427799, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODQyNzc5OQ==", "user": {"login": "hartb", "id": 18429659, "node_id": "MDQ6VXNlcjE4NDI5NjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/18429659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hartb", "html_url": "https://github.com/hartb", "followers_url": "https://api.github.com/users/hartb/followers", "following_url": "https://api.github.com/users/hartb/following{/other_user}", "gists_url": "https://api.github.com/users/hartb/gists{/gist_id}", "starred_url": "https://api.github.com/users/hartb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hartb/subscriptions", "organizations_url": "https://api.github.com/users/hartb/orgs", "repos_url": "https://api.github.com/users/hartb/repos", "events_url": "https://api.github.com/users/hartb/events{/privacy}", "received_events_url": "https://api.github.com/users/hartb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-03T23:09:16Z", "updated_at": "2018-04-03T23:09:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22493190\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PouriaCh\">@PouriaCh</a> If you can use the .deb packages for the cuDNN install (available for cuDNN 5.1 at <a href=\"https://developer.nvidia.com/rdp/cudnn-archive\" rel=\"nofollow\">https://developer.nvidia.com/rdp/cudnn-archive</a>) then I think the install process should create the links for you. You shouldn't have to do it by hand (and you'd probably want to remove any manually-created links before doing the .deb install).</p>\n<p>With recent versions of TF, we've been using the cuDNN tarball install. That doesn't create any of those links.  We unpack the tarball so the cuDNN stuff lands under /usr/local/cuda:</p>\n<p><code>$ sudo tar -C /usr/local --no-same-owner -xvf cudnn...tgz</code></p>\n<p>And then inform TF build of the location during the configure step:</p>\n<p><code>CUDNN_INSTALL_PATH=/usr/local/cuda-x.y</code></p>\n<p>That's been working for us (on Power, rather than x86, but I think should be the same).</p>", "body_text": "@PouriaCh If you can use the .deb packages for the cuDNN install (available for cuDNN 5.1 at https://developer.nvidia.com/rdp/cudnn-archive) then I think the install process should create the links for you. You shouldn't have to do it by hand (and you'd probably want to remove any manually-created links before doing the .deb install).\nWith recent versions of TF, we've been using the cuDNN tarball install. That doesn't create any of those links.  We unpack the tarball so the cuDNN stuff lands under /usr/local/cuda:\n$ sudo tar -C /usr/local --no-same-owner -xvf cudnn...tgz\nAnd then inform TF build of the location during the configure step:\nCUDNN_INSTALL_PATH=/usr/local/cuda-x.y\nThat's been working for us (on Power, rather than x86, but I think should be the same).", "body": "@PouriaCh If you can use the .deb packages for the cuDNN install (available for cuDNN 5.1 at https://developer.nvidia.com/rdp/cudnn-archive) then I think the install process should create the links for you. You shouldn't have to do it by hand (and you'd probably want to remove any manually-created links before doing the .deb install).\r\n\r\nWith recent versions of TF, we've been using the cuDNN tarball install. That doesn't create any of those links.  We unpack the tarball so the cuDNN stuff lands under /usr/local/cuda:\r\n\r\n`$ sudo tar -C /usr/local --no-same-owner -xvf cudnn...tgz`\r\n\r\nAnd then inform TF build of the location during the configure step:\r\n\r\n`CUDNN_INSTALL_PATH=/usr/local/cuda-x.y`\r\n\r\nThat's been working for us (on Power, rather than x86, but I think should be the same)."}