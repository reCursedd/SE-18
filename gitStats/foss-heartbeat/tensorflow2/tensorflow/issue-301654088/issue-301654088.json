{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17370", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17370/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17370/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17370/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17370", "id": 301654088, "node_id": "MDU6SXNzdWUzMDE2NTQwODg=", "number": 17370, "title": "Image retraining script memory problem and issue", "user": {"login": "lcycoding", "id": 20554498, "node_id": "MDQ6VXNlcjIwNTU0NDk4", "avatar_url": "https://avatars0.githubusercontent.com/u/20554498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lcycoding", "html_url": "https://github.com/lcycoding", "followers_url": "https://api.github.com/users/lcycoding/followers", "following_url": "https://api.github.com/users/lcycoding/following{/other_user}", "gists_url": "https://api.github.com/users/lcycoding/gists{/gist_id}", "starred_url": "https://api.github.com/users/lcycoding/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lcycoding/subscriptions", "organizations_url": "https://api.github.com/users/lcycoding/orgs", "repos_url": "https://api.github.com/users/lcycoding/repos", "events_url": "https://api.github.com/users/lcycoding/events{/privacy}", "received_events_url": "https://api.github.com/users/lcycoding/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-03-02T05:00:35Z", "updated_at": "2018-04-16T08:50:05Z", "closed_at": "2018-03-03T01:11:59Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6-gpu/nightly-gpu</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:0.9.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7.04</li>\n<li><strong>GPU model and memory</strong>: Tesla K80 / 11441MiB</li>\n<li><strong>Exact command to reproduce</strong>:<br>\npython retrain_quantize.py w/ certain parameters.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I was trying the new retrain script on my own model. (In order to fully convert quantized model to tflite)</p>\n<ol>\n<li>Different memory usage in different version.<br>\nI opened allow_growth parameter in order to trace memory usage during training.<br>\nIn tf-gpu-1.6.0 :</li>\n</ol>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                                                      GPU Memory |<br>\n|  GPU       PID   Type   Process name                                                    Usage      |<br>\n|======================================================== |<br>\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |<br>\n|    0     15440      C   python                                                                     302MiB |<br>\n+-----------------------------------------------------------------------------+</p>\n<p>But in tf-nightly-gpu:</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                                                      GPU Memory |<br>\n|  GPU       PID   Type   Process name                                                    Usage      |<br>\n|======================================================== |<br>\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |<br>\n|    0     15747      C   python                                                                    4152MiB |<br>\n+-----------------------------------------------------------------------------+</p>\n<p>I was wondering what causes the tremendous difference in these two versions?</p>\n<ol start=\"2\">\n<li>Per the traceback below, my retraining process could not be done in the last step.<br>\nDue to the feed_dict issue. I saw my process restart a session after 100 steps, could the restart<br>\ncause the loss of DecodeJPGInput tensor?</li>\n</ol>\n<pre><code>After last-1 steps, system shows:\n2018-03-02 04:49:23.569386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-03-02 04:49:23.569450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-03-02 04:49:23.569460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\n2018-03-02 04:49:23.569464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\n2018-03-02 04:49:23.569688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10750 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n\n</code></pre>\n<p>Thanks in advance!</p>\n<h3>Source code / logs</h3>\n<pre><code>Traceback (most recent call last):\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1080, in _run\n    subfeed, allow_tensor=True, allow_operation=False)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3458, in as_graph_element\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3537, in _as_graph_element_locked\n    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\nValueError: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"retrain_quantize.py\", line 355, in create_bottleneck_file\n    resized_input_tensor, bottleneck_tensor)\n  File \"retrain_quantize.py\", line 290, in run_bottleneck_on_image\n    {image_data_tensor: image_data})\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1083, in _run\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"retrain_quantize.py\", line 1411, in &lt;module&gt;\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 126, in run\n    _sys.exit(main(argv))\n  File \"retrain_quantize.py\", line 1219, in main\n    export_model(model_info, class_count, FLAGS.saved_model_dir)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1558, in __exit__\n    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\n    yield g\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 4869, in get_controller\n    yield default\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\n    yield g\n  File \"retrain_quantize.py\", line 1211, in main\n    bottleneck_tensor)\n  File \"retrain_quantize.py\", line 813, in run_final_eval\n    bottleneck_tensor, FLAGS.architecture))\n  File \"retrain_quantize.py\", line 505, in get_random_cached_bottlenecks\n    resized_input_tensor, bottleneck_tensor, architecture)\n  File \"retrain_quantize.py\", line 400, in get_or_create_bottleneck\n    bottleneck_tensor)\n  File \"retrain_quantize.py\", line 358, in create_bottleneck_file\n    str(e)))\nRuntimeError: Error during processing file /home/cheyu.lin/master_dataset/dog/source_dog_wash_011682.jpg (Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.6-gpu/nightly-gpu\nPython version: 3.5.2\nBazel version (if compiling from source):0.9.0\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.0/7.04\nGPU model and memory: Tesla K80 / 11441MiB\nExact command to reproduce:\npython retrain_quantize.py w/ certain parameters.\n\nDescribe the problem\nI was trying the new retrain script on my own model. (In order to fully convert quantized model to tflite)\n\nDifferent memory usage in different version.\nI opened allow_growth parameter in order to trace memory usage during training.\nIn tf-gpu-1.6.0 :\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                                      GPU Memory |\n|  GPU       PID   Type   Process name                                                    Usage      |\n|======================================================== |\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |\n|    0     15440      C   python                                                                     302MiB |\n+-----------------------------------------------------------------------------+\nBut in tf-nightly-gpu:\n+-----------------------------------------------------------------------------+\n| Processes:                                                                                      GPU Memory |\n|  GPU       PID   Type   Process name                                                    Usage      |\n|======================================================== |\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |\n|    0     15747      C   python                                                                    4152MiB |\n+-----------------------------------------------------------------------------+\nI was wondering what causes the tremendous difference in these two versions?\n\nPer the traceback below, my retraining process could not be done in the last step.\nDue to the feed_dict issue. I saw my process restart a session after 100 steps, could the restart\ncause the loss of DecodeJPGInput tensor?\n\nAfter last-1 steps, system shows:\n2018-03-02 04:49:23.569386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-03-02 04:49:23.569450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-03-02 04:49:23.569460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\n2018-03-02 04:49:23.569464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\n2018-03-02 04:49:23.569688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10750 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n\n\nThanks in advance!\nSource code / logs\nTraceback (most recent call last):\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1080, in _run\n    subfeed, allow_tensor=True, allow_operation=False)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3458, in as_graph_element\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3537, in _as_graph_element_locked\n    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\nValueError: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"retrain_quantize.py\", line 355, in create_bottleneck_file\n    resized_input_tensor, bottleneck_tensor)\n  File \"retrain_quantize.py\", line 290, in run_bottleneck_on_image\n    {image_data_tensor: image_data})\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1083, in _run\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"retrain_quantize.py\", line 1411, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 126, in run\n    _sys.exit(main(argv))\n  File \"retrain_quantize.py\", line 1219, in main\n    export_model(model_info, class_count, FLAGS.saved_model_dir)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1558, in __exit__\n    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\n    yield g\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 4869, in get_controller\n    yield default\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\n    yield g\n  File \"retrain_quantize.py\", line 1211, in main\n    bottleneck_tensor)\n  File \"retrain_quantize.py\", line 813, in run_final_eval\n    bottleneck_tensor, FLAGS.architecture))\n  File \"retrain_quantize.py\", line 505, in get_random_cached_bottlenecks\n    resized_input_tensor, bottleneck_tensor, architecture)\n  File \"retrain_quantize.py\", line 400, in get_or_create_bottleneck\n    bottleneck_tensor)\n  File \"retrain_quantize.py\", line 358, in create_bottleneck_file\n    str(e)))\nRuntimeError: Error during processing file /home/cheyu.lin/master_dataset/dog/source_dog_wash_011682.jpg (Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n      Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6-gpu/nightly-gpu\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.04\r\n- **GPU model and memory**: Tesla K80 / 11441MiB\r\n- **Exact command to reproduce**:\r\npython retrain_quantize.py w/ certain parameters.\r\n\r\n\r\n### Describe the problem\r\nI was trying the new retrain script on my own model. (In order to fully convert quantized model to tflite)\r\n1. Different memory usage in different version.\r\nI opened allow_growth parameter in order to trace memory usage during training.\r\nIn tf-gpu-1.6.0 :\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                                      GPU Memory |\r\n|  GPU       PID   Type   Process name                                                    Usage      |\r\n|======================================================== |\r\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |\r\n|    0     15440      C   python                                                                     302MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nBut in tf-nightly-gpu:\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                                      GPU Memory |\r\n|  GPU       PID   Type   Process name                                                    Usage      |\r\n|======================================================== |\r\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |\r\n|    0     15747      C   python                                                                    4152MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nI was wondering what causes the tremendous difference in these two versions?\r\n\r\n2. Per the traceback below, my retraining process could not be done in the last step. \r\n    Due to the feed_dict issue. I saw my process restart a session after 100 steps, could the restart \r\n    cause the loss of DecodeJPGInput tensor?\r\n\r\n```\r\nAfter last-1 steps, system shows:\r\n2018-03-02 04:49:23.569386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-03-02 04:49:23.569450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-03-02 04:49:23.569460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-03-02 04:49:23.569464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-03-02 04:49:23.569688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10750 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n\r\n```\r\nThanks in advance!\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1080, in _run\r\n    subfeed, allow_tensor=True, allow_operation=False)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3458, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3537, in _as_graph_element_locked\r\n    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\r\nValueError: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"retrain_quantize.py\", line 355, in create_bottleneck_file\r\n    resized_input_tensor, bottleneck_tensor)\r\n  File \"retrain_quantize.py\", line 290, in run_bottleneck_on_image\r\n    {image_data_tensor: image_data})\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1083, in _run\r\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"retrain_quantize.py\", line 1411, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"retrain_quantize.py\", line 1219, in main\r\n    export_model(model_info, class_count, FLAGS.saved_model_dir)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1558, in __exit__\r\n    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 4869, in get_controller\r\n    yield default\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\r\n    yield g\r\n  File \"retrain_quantize.py\", line 1211, in main\r\n    bottleneck_tensor)\r\n  File \"retrain_quantize.py\", line 813, in run_final_eval\r\n    bottleneck_tensor, FLAGS.architecture))\r\n  File \"retrain_quantize.py\", line 505, in get_random_cached_bottlenecks\r\n    resized_input_tensor, bottleneck_tensor, architecture)\r\n  File \"retrain_quantize.py\", line 400, in get_or_create_bottleneck\r\n    bottleneck_tensor)\r\n  File \"retrain_quantize.py\", line 358, in create_bottleneck_file\r\n    str(e)))\r\nRuntimeError: Error during processing file /home/cheyu.lin/master_dataset/dog/source_dog_wash_011682.jpg (Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.)\r\n```"}