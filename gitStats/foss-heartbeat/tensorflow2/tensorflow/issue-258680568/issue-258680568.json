{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13142", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13142/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13142/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13142/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13142", "id": 258680568, "node_id": "MDU6SXNzdWUyNTg2ODA1Njg=", "number": 13142, "title": "ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients", "user": {"login": "nam15122013", "id": 32084272, "node_id": "MDQ6VXNlcjMyMDg0Mjcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32084272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nam15122013", "html_url": "https://github.com/nam15122013", "followers_url": "https://api.github.com/users/nam15122013/followers", "following_url": "https://api.github.com/users/nam15122013/following{/other_user}", "gists_url": "https://api.github.com/users/nam15122013/gists{/gist_id}", "starred_url": "https://api.github.com/users/nam15122013/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nam15122013/subscriptions", "organizations_url": "https://api.github.com/users/nam15122013/orgs", "repos_url": "https://api.github.com/users/nam15122013/repos", "events_url": "https://api.github.com/users/nam15122013/events{/privacy}", "received_events_url": "https://api.github.com/users/nam15122013/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-19T02:57:30Z", "updated_at": "2017-09-22T22:23:01Z", "closed_at": "2017-09-22T22:23:01Z", "author_association": "NONE", "body_html": "<p>Hello everyone,<br>\nI have error when programming tensorflow:</p>\n<p>ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"&lt;tf.Variable 'Variable:0' shape=(4, 2) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_1:0' shape=(4, 2) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_2:0' shape=(1, 3) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_3:0' shape=(1, 2) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_4:0' shape=(1, 3) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_5:0' shape=(1, 3) dtype=float32_ref&gt;\"] and loss Tensor(\"Sum:0\", dtype=float32).</p>\n<p>My code is here</p>\n<p>=============================<br>\nimport tensorflow as tf</p>\n<p>def weight_variable(shape):<br>\ninitial = tf.truncated_normal(shape, stddev = 0.1)<br>\nreturn tf.Variable(initial)</p>\n<p>def bias_variable(shape):<br>\ninitial = tf.constant(0.1, shape = shape)<br>\nreturn tf.Variable(initial)</p>\n<h1>Model parameters</h1>\n<p>#W = tf.Variable([.3], dtype=tf.float32)<br>\n#b = tf.Variable([-.3], dtype=tf.float32)<br>\n#W=weight_variable([1])<br>\n#b=bias_variable([1])<br>\nindice= tf.constant([0,1])<br>\nsegment_id= tf.constant([0,0,1,1])<br>\nW=weight_variable([4,2])<br>\nb=bias_variable([4,2])<br>\nb_=tf.Variable(tf.zeros([1,3]))<br>\nt0=tf.Variable(tf.zeros([1,2]))<br>\nt1=tf.Variable(tf.zeros([1,3]))<br>\ng=tf.Variable(tf.zeros([1,3]))</p>\n<h1>Model input and output</h1>\n<p>x = tf.placeholder(tf.float32)<br>\n#linear_model = W * x + b</p>\n<p>#forward transform</p>\n<p>linear_function = tf.add(tf.matmul(W,x),b)<br>\nlinear_function_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[0]))<br>\nlinear_function_col_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[1]))<br>\nmin_1=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))<br>\nmin_2=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))<br>\n#b_=tf.assign(b_,[[min_1,min_2,0]])<br>\nb_=tf.assign(b_,[[min_1,min_2,0.0]])<br>\nt0=tf.assign(t0,[[min_2,min_1]])<br>\ng = tf.nn.softmax(tf.scalar_mul(1000,b_),dim=-1)<br>\n#g = tf.nn.softmax(b_)</p>\n<p>#inverse transform</p>\n<p>linear_function_inv = tf.divide(tf.transpose(tf.transpose(t0)-tf.transpose(b)),W)<br>\nlinear_function_inv_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv), indice[0]))<br>\nlinear_function_inv_col_2= tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv ), indice[1]))<br>\nmax_1=tf.reduce_max(tf.segment_min(linear_function_inv_col_1,segment_id))<br>\nmax_2=tf.reduce_max(tf.segment_min(linear_function_inv_col_2,segment_id))<br>\nt1=tf.assign(t1,[[max_1,max_2,0.0]])</p>\n<p>y = tf.placeholder(tf.float32)</p>\n<h1>loss</h1>\n<p>#loss = tf.reduce_sum(tf.square(linear_model-y)) # sum of the squares<br>\n#loss = tf.reduce_sum(tf.square(g)) # sum of the squares<br>\n#loss = tf.reduce_sum(tf.square(y-tf.matmul(g,tf.transpose(t1))))<br>\nloss = tf.reduce_sum(y-tf.matmul(g,tf.transpose(t1)))</p>\n<h1>optimizer</h1>\n<p>optimizer = tf.train.GradientDescentOptimizer(0.01)<br>\ntrain = optimizer.minimize(loss)</p>\n<h1>training data</h1>\n<p>#x_train = [1, 2, 3, 4]<br>\nx_train_array = tf.constant([0.58975124,0.22815752])<br>\nx_train=tf.diag(x_train_array)<br>\n#y_train = [0, -1, -2, -3]<br>\ny_train=tf.constant([[0.530]])</p>\n<h1>training loop</h1>\n<p>init = tf.global_variables_initializer()<br>\nsess = tf.Session()<br>\nsess.run(init) # reset values to wrong<br>\nfor i in range(1000):<br>\nsess.run(train, {x: x_train, y: y_train})</p>\n<h1>evaluate training accuracy</h1>\n<h1>curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})<br>\nprint(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))</h1>\n<p>Hope to get your help.<br>\nThank you</p>", "body_text": "Hello everyone,\nI have error when programming tensorflow:\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(4, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(1, 3) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(1, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(1, 3) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(1, 3) dtype=float32_ref>\"] and loss Tensor(\"Sum:0\", dtype=float32).\nMy code is here\n=============================\nimport tensorflow as tf\ndef weight_variable(shape):\ninitial = tf.truncated_normal(shape, stddev = 0.1)\nreturn tf.Variable(initial)\ndef bias_variable(shape):\ninitial = tf.constant(0.1, shape = shape)\nreturn tf.Variable(initial)\nModel parameters\n#W = tf.Variable([.3], dtype=tf.float32)\n#b = tf.Variable([-.3], dtype=tf.float32)\n#W=weight_variable([1])\n#b=bias_variable([1])\nindice= tf.constant([0,1])\nsegment_id= tf.constant([0,0,1,1])\nW=weight_variable([4,2])\nb=bias_variable([4,2])\nb_=tf.Variable(tf.zeros([1,3]))\nt0=tf.Variable(tf.zeros([1,2]))\nt1=tf.Variable(tf.zeros([1,3]))\ng=tf.Variable(tf.zeros([1,3]))\nModel input and output\nx = tf.placeholder(tf.float32)\n#linear_model = W * x + b\n#forward transform\nlinear_function = tf.add(tf.matmul(W,x),b)\nlinear_function_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[0]))\nlinear_function_col_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[1]))\nmin_1=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))\nmin_2=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))\n#b_=tf.assign(b_,[[min_1,min_2,0]])\nb_=tf.assign(b_,[[min_1,min_2,0.0]])\nt0=tf.assign(t0,[[min_2,min_1]])\ng = tf.nn.softmax(tf.scalar_mul(1000,b_),dim=-1)\n#g = tf.nn.softmax(b_)\n#inverse transform\nlinear_function_inv = tf.divide(tf.transpose(tf.transpose(t0)-tf.transpose(b)),W)\nlinear_function_inv_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv), indice[0]))\nlinear_function_inv_col_2= tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv ), indice[1]))\nmax_1=tf.reduce_max(tf.segment_min(linear_function_inv_col_1,segment_id))\nmax_2=tf.reduce_max(tf.segment_min(linear_function_inv_col_2,segment_id))\nt1=tf.assign(t1,[[max_1,max_2,0.0]])\ny = tf.placeholder(tf.float32)\nloss\n#loss = tf.reduce_sum(tf.square(linear_model-y)) # sum of the squares\n#loss = tf.reduce_sum(tf.square(g)) # sum of the squares\n#loss = tf.reduce_sum(tf.square(y-tf.matmul(g,tf.transpose(t1))))\nloss = tf.reduce_sum(y-tf.matmul(g,tf.transpose(t1)))\noptimizer\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(loss)\ntraining data\n#x_train = [1, 2, 3, 4]\nx_train_array = tf.constant([0.58975124,0.22815752])\nx_train=tf.diag(x_train_array)\n#y_train = [0, -1, -2, -3]\ny_train=tf.constant([[0.530]])\ntraining loop\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init) # reset values to wrong\nfor i in range(1000):\nsess.run(train, {x: x_train, y: y_train})\nevaluate training accuracy\ncurr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\nprint(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\nHope to get your help.\nThank you", "body": "Hello everyone,\r\nI have error when programming tensorflow:\r\n\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(4, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(1, 3) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(1, 2) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(1, 3) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(1, 3) dtype=float32_ref>\"] and loss Tensor(\"Sum:0\", dtype=float32).\r\n\r\nMy code is here\r\n\r\n\r\n=============================\r\nimport tensorflow as tf\r\n\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape, stddev = 0.1)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape = shape)\r\n    return tf.Variable(initial)\r\n\r\n# Model parameters\r\n#W = tf.Variable([.3], dtype=tf.float32)\r\n#b = tf.Variable([-.3], dtype=tf.float32)\r\n#W=weight_variable([1])\r\n#b=bias_variable([1])\r\nindice= tf.constant([0,1])\r\nsegment_id= tf.constant([0,0,1,1])\r\nW=weight_variable([4,2])\r\nb=bias_variable([4,2])\r\nb_=tf.Variable(tf.zeros([1,3]))\r\nt0=tf.Variable(tf.zeros([1,2]))\r\nt1=tf.Variable(tf.zeros([1,3]))\r\ng=tf.Variable(tf.zeros([1,3]))\r\n\r\n\r\n# Model input and output\r\nx = tf.placeholder(tf.float32)\r\n#linear_model = W * x + b\r\n\r\n#forward transform\r\n\r\nlinear_function = tf.add(tf.matmul(W,x),b)\r\nlinear_function_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[0]))\r\nlinear_function_col_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[1]))\r\nmin_1=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))\r\nmin_2=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))\r\n#b_=tf.assign(b_,[[min_1,min_2,0]])\r\nb_=tf.assign(b_,[[min_1,min_2,0.0]])\r\nt0=tf.assign(t0,[[min_2,min_1]])\r\ng = tf.nn.softmax(tf.scalar_mul(1000,b_),dim=-1)\r\n#g = tf.nn.softmax(b_)\r\n\r\n#inverse transform\r\n\r\nlinear_function_inv = tf.divide(tf.transpose(tf.transpose(t0)-tf.transpose(b)),W)\r\nlinear_function_inv_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv), indice[0]))\r\nlinear_function_inv_col_2= tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv ), indice[1]))\r\nmax_1=tf.reduce_max(tf.segment_min(linear_function_inv_col_1,segment_id))\r\nmax_2=tf.reduce_max(tf.segment_min(linear_function_inv_col_2,segment_id))\r\nt1=tf.assign(t1,[[max_1,max_2,0.0]])\r\n\r\ny = tf.placeholder(tf.float32)\r\n\r\n# loss\r\n#loss = tf.reduce_sum(tf.square(linear_model-y)) # sum of the squares\r\n#loss = tf.reduce_sum(tf.square(g)) # sum of the squares\r\n#loss = tf.reduce_sum(tf.square(y-tf.matmul(g,tf.transpose(t1))))\r\nloss = tf.reduce_sum(y-tf.matmul(g,tf.transpose(t1)))\r\n# optimizer\r\noptimizer = tf.train.GradientDescentOptimizer(0.01)\r\ntrain = optimizer.minimize(loss)\r\n\r\n# training data\r\n#x_train = [1, 2, 3, 4]\r\nx_train_array = tf.constant([0.58975124,0.22815752])\r\nx_train=tf.diag(x_train_array)\r\n#y_train = [0, -1, -2, -3]\r\ny_train=tf.constant([[0.530]])\r\n# training loop\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init) # reset values to wrong\r\nfor i in range(1000):\r\n  sess.run(train, {x: x_train, y: y_train})\r\n\r\n# evaluate training accuracy\r\ncurr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\r\nprint(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\r\n==================================================\r\n\r\nHope to get your help.\r\nThank you\r\n\r\n\r\n\r\n\r\n"}