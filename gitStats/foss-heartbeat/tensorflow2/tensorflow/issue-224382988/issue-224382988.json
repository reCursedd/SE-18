{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9455", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9455/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9455/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9455/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9455", "id": 224382988, "node_id": "MDU6SXNzdWUyMjQzODI5ODg=", "number": 9455, "title": "Batch + dynamic_pad + squeeze + one_hot + dynamic_rnn throws shape error", "user": {"login": "EdeMeijer", "id": 5758565, "node_id": "MDQ6VXNlcjU3NTg1NjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/5758565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EdeMeijer", "html_url": "https://github.com/EdeMeijer", "followers_url": "https://api.github.com/users/EdeMeijer/followers", "following_url": "https://api.github.com/users/EdeMeijer/following{/other_user}", "gists_url": "https://api.github.com/users/EdeMeijer/gists{/gist_id}", "starred_url": "https://api.github.com/users/EdeMeijer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EdeMeijer/subscriptions", "organizations_url": "https://api.github.com/users/EdeMeijer/orgs", "repos_url": "https://api.github.com/users/EdeMeijer/repos", "events_url": "https://api.github.com/users/EdeMeijer/events{/privacy}", "received_events_url": "https://api.github.com/users/EdeMeijer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-26T08:22:35Z", "updated_at": "2017-04-26T08:43:52Z", "closed_at": "2017-04-26T08:43:52Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04.1 LTS (Xenial Xerus)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.1.0-rc2-259-g34c738c</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Exact command to reproduce</strong>: <code>python ./bug.py</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>A certain combination of ops causes <code>tf.nn.dynamic_rnn</code> to throw a <code>ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.</code> during compilation of the graph. I will attach the script to reproduce the issue and the required tfrecords file.</p>\n<p>The problem seems to be a combination of batching with <code>dynamic_pad</code> enabled, and <code>squeeze</code> + <code>one_hot</code>. I use <code>squeeze</code> + <code>one_hot</code> because <code>SequenceExample</code> gives me a <code>[examples x time_steps x 1]</code> tensor, and <code>one_hot</code> adds another dimension, so I first need to get rid of the last dimension to get a <code>[examples x time_steps x 3]</code> one-hot encoded tensor.</p>\n<p>A few observations:</p>\n<ul>\n<li>When excluding <code>availability_one_hot</code> from the <code>lstm_inputs</code> concat, the script runs ok.</li>\n<li>When commenting out the <code>dynamic_rnn</code> op and evaluating <code>print(sess.run(tf.shape(lstm_inputs)))</code> instead, the output is <code>[2 8 4]</code>, which is the correct shape (1 normal feature + 3 from the one-hot encoding = 4 features)`.</li>\n<li>I tried to recreate the batch with a <code>tf.constant()</code> literal, but using that the graph worked fine, so the batching with dynamic padding is needed to reproduce the issue.</li>\n</ul>\n<h3>Source code / logs</h3>\n<p>Script + data file: <a href=\"https://github.com/tensorflow/tensorflow/files/957645/bug.zip\">bug.zip</a></p>\n<p>Output:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/ede/repos/xxx/xxx/misc/tftest/bug.py\", line 38, in &lt;module&gt;\n    inputs=lstm_inputs\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 582, in dynamic_rnn\n    dtype=dtype)\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 652, in _dynamic_rnn_loop\n    \"Input size (depth of inputs) must be accessible via shape inference,\"\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1 LTS (Xenial Xerus)\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): v1.1.0-rc2-259-g34c738c\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nPython version: 3.5.2\nExact command to reproduce: python ./bug.py\n\nDescribe the problem\nA certain combination of ops causes tf.nn.dynamic_rnn to throw a ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None. during compilation of the graph. I will attach the script to reproduce the issue and the required tfrecords file.\nThe problem seems to be a combination of batching with dynamic_pad enabled, and squeeze + one_hot. I use squeeze + one_hot because SequenceExample gives me a [examples x time_steps x 1] tensor, and one_hot adds another dimension, so I first need to get rid of the last dimension to get a [examples x time_steps x 3] one-hot encoded tensor.\nA few observations:\n\nWhen excluding availability_one_hot from the lstm_inputs concat, the script runs ok.\nWhen commenting out the dynamic_rnn op and evaluating print(sess.run(tf.shape(lstm_inputs))) instead, the output is [2 8 4], which is the correct shape (1 normal feature + 3 from the one-hot encoding = 4 features)`.\nI tried to recreate the batch with a tf.constant() literal, but using that the graph worked fine, so the batching with dynamic padding is needed to reproduce the issue.\n\nSource code / logs\nScript + data file: bug.zip\nOutput:\nTraceback (most recent call last):\n  File \"/home/ede/repos/xxx/xxx/misc/tftest/bug.py\", line 38, in <module>\n    inputs=lstm_inputs\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 582, in dynamic_rnn\n    dtype=dtype)\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 652, in _dynamic_rnn_loop\n    \"Input size (depth of inputs) must be accessible via shape inference,\"\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.1.0-rc2-259-g34c738c\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Python version**: 3.5.2\r\n- **Exact command to reproduce**: `python ./bug.py`\r\n\r\n### Describe the problem\r\n\r\nA certain combination of ops causes `tf.nn.dynamic_rnn` to throw a `ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.` during compilation of the graph. I will attach the script to reproduce the issue and the required tfrecords file.\r\n\r\nThe problem seems to be a combination of batching with `dynamic_pad` enabled, and `squeeze` + `one_hot`. I use `squeeze` + `one_hot` because `SequenceExample` gives me a `[examples x time_steps x 1]` tensor, and `one_hot` adds another dimension, so I first need to get rid of the last dimension to get a `[examples x time_steps x 3]` one-hot encoded tensor.\r\n\r\nA few observations:\r\n\r\n- When excluding `availability_one_hot` from the `lstm_inputs` concat, the script runs ok.\r\n- When commenting out the `dynamic_rnn` op and evaluating `print(sess.run(tf.shape(lstm_inputs)))` instead, the output is `[2 8 4]`, which is the correct shape (1 normal feature + 3 from the one-hot encoding = 4 features)`.\r\n- I tried to recreate the batch with a `tf.constant()` literal, but using that the graph worked fine, so the batching with dynamic padding is needed to reproduce the issue. \r\n\r\n### Source code / logs\r\n\r\nScript + data file: [bug.zip](https://github.com/tensorflow/tensorflow/files/957645/bug.zip)\r\n\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ede/repos/xxx/xxx/misc/tftest/bug.py\", line 38, in <module>\r\n    inputs=lstm_inputs\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 582, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 652, in _dynamic_rnn_loop\r\n    \"Input size (depth of inputs) must be accessible via shape inference,\"\r\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\r\n```"}