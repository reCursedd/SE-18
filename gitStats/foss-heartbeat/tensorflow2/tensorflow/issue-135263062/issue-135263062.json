{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1228", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1228/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1228/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1228/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1228", "id": 135263062, "node_id": "MDU6SXNzdWUxMzUyNjMwNjI=", "number": 1228, "title": "sequence2sequence model padding ", "user": {"login": "nddk", "id": 16150042, "node_id": "MDQ6VXNlcjE2MTUwMDQy", "avatar_url": "https://avatars3.githubusercontent.com/u/16150042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nddk", "html_url": "https://github.com/nddk", "followers_url": "https://api.github.com/users/nddk/followers", "following_url": "https://api.github.com/users/nddk/following{/other_user}", "gists_url": "https://api.github.com/users/nddk/gists{/gist_id}", "starred_url": "https://api.github.com/users/nddk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nddk/subscriptions", "organizations_url": "https://api.github.com/users/nddk/orgs", "repos_url": "https://api.github.com/users/nddk/repos", "events_url": "https://api.github.com/users/nddk/events{/privacy}", "received_events_url": "https://api.github.com/users/nddk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2016-02-21T21:38:00Z", "updated_at": "2016-02-22T18:30:09Z", "closed_at": "2016-02-22T18:30:09Z", "author_association": "NONE", "body_html": "<p>In the seq2seq models, paddings are applied to make all sequences in a bucket have the same lengths. And apart from this, it looks like no special handling is applied to the paddings:</p>\n<ul>\n<li>the encoder encodes the paddings as well</li>\n<li>the basic decoder w/o attention decodes using the last encoding which encodes the paddings</li>\n<li>the decoder with attention attends to the hidden states of the padding inputs too</li>\n</ul>\n<p>It would be really helpful if this could be clarified: is it true that, basically the paddings are just a special id/embedding, and the current seq2seq implementation treats them just like other embeddings? And no special mechanism is needed to ignore these padding, for example when encoding a sequence containing paddings; or to decode a sequence containing paddings using the attention-based decoder? So after padding, nothing special is done to the paddings, we can just pretend a padding is just another embedding  (apart from maybe when doing the weighted x-entropy using target_weights)?</p>\n<p>If the above is true, then when testing a trained model, is padding needed at all (since at test time, each sentence is decoded separately and not in a batch)? --- It looks like from the code, at test time an input sentence is still bucketed first and then padded?</p>", "body_text": "In the seq2seq models, paddings are applied to make all sequences in a bucket have the same lengths. And apart from this, it looks like no special handling is applied to the paddings:\n\nthe encoder encodes the paddings as well\nthe basic decoder w/o attention decodes using the last encoding which encodes the paddings\nthe decoder with attention attends to the hidden states of the padding inputs too\n\nIt would be really helpful if this could be clarified: is it true that, basically the paddings are just a special id/embedding, and the current seq2seq implementation treats them just like other embeddings? And no special mechanism is needed to ignore these padding, for example when encoding a sequence containing paddings; or to decode a sequence containing paddings using the attention-based decoder? So after padding, nothing special is done to the paddings, we can just pretend a padding is just another embedding  (apart from maybe when doing the weighted x-entropy using target_weights)?\nIf the above is true, then when testing a trained model, is padding needed at all (since at test time, each sentence is decoded separately and not in a batch)? --- It looks like from the code, at test time an input sentence is still bucketed first and then padded?", "body": "In the seq2seq models, paddings are applied to make all sequences in a bucket have the same lengths. And apart from this, it looks like no special handling is applied to the paddings: \n- the encoder encodes the paddings as well\n- the basic decoder w/o attention decodes using the last encoding which encodes the paddings\n- the decoder with attention attends to the hidden states of the padding inputs too\n\nIt would be really helpful if this could be clarified: is it true that, basically the paddings are just a special id/embedding, and the current seq2seq implementation treats them just like other embeddings? And no special mechanism is needed to ignore these padding, for example when encoding a sequence containing paddings; or to decode a sequence containing paddings using the attention-based decoder? So after padding, nothing special is done to the paddings, we can just pretend a padding is just another embedding  (apart from maybe when doing the weighted x-entropy using target_weights)?\n\nIf the above is true, then when testing a trained model, is padding needed at all (since at test time, each sentence is decoded separately and not in a batch)? --- It looks like from the code, at test time an input sentence is still bucketed first and then padded?\n"}