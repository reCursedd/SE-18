{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19054", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19054/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19054/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19054/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19054", "id": 319868933, "node_id": "MDU6SXNzdWUzMTk4Njg5MzM=", "number": 19054, "title": "Regularization loss will duplicated when reusing PartitionedVariable in tf.layers", "user": {"login": "wangsiyu", "id": 5387343, "node_id": "MDQ6VXNlcjUzODczNDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5387343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangsiyu", "html_url": "https://github.com/wangsiyu", "followers_url": "https://api.github.com/users/wangsiyu/followers", "following_url": "https://api.github.com/users/wangsiyu/following{/other_user}", "gists_url": "https://api.github.com/users/wangsiyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangsiyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangsiyu/subscriptions", "organizations_url": "https://api.github.com/users/wangsiyu/orgs", "repos_url": "https://api.github.com/users/wangsiyu/repos", "events_url": "https://api.github.com/users/wangsiyu/events{/privacy}", "received_events_url": "https://api.github.com/users/wangsiyu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-05-03T10:41:18Z", "updated_at": "2018-06-25T19:00:58Z", "closed_at": "2018-06-25T19:00:58Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n<li><strong>Python version</strong>: 2.7.11</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.10.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.8.5</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8.0/ cuDNN 7</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: see below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers. I have found that there is no special treatment for reusing regularization loss of PartitionedVariables in tf.layers .</p>\n<h3>Source code / logs</h3>\n<p>Here is the small script can reproduce the result.</p>\n<pre><code>import tensorflow as tf\npartitioner = tf.fixed_size_partitioner(3)\nl2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\nfor i in xrange(2):\n  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):\n    inputs_tensor = tf.constant(1.0, shape=[100, 100])\n    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name=\"fc\", kernel_regularizer=l2_regularizer)\nprint (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n</code></pre>\n<p>This short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6.</p>\n<p>A pull request has been submitted here to fix this bug: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"319867141\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/19053\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/19053/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/19053\">#19053</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version: 2.7.11\nBazel version (if compiling from source): 0.10.0\nGCC/Compiler version (if compiling from source): 4.8.5\nCUDA/cuDNN version: CUDA 8.0/ cuDNN 7\nGPU model and memory: N/A\nExact command to reproduce: see below\n\nDescribe the problem\nWhen reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers. I have found that there is no special treatment for reusing regularization loss of PartitionedVariables in tf.layers .\nSource code / logs\nHere is the small script can reproduce the result.\nimport tensorflow as tf\npartitioner = tf.fixed_size_partitioner(3)\nl2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\nfor i in xrange(2):\n  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):\n    inputs_tensor = tf.constant(1.0, shape=[100, 100])\n    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name=\"fc\", kernel_regularizer=l2_regularizer)\nprint (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\nThis short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6.\nA pull request has been submitted here to fix this bug: #19053", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: 2.7.11\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: CUDA 8.0/ cuDNN 7\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nWhen reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers. I have found that there is no special treatment for reusing regularization loss of PartitionedVariables in tf.layers .\r\n### Source code / logs\r\nHere is the small script can reproduce the result.\r\n\r\n```\r\nimport tensorflow as tf\r\npartitioner = tf.fixed_size_partitioner(3)\r\nl2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\r\nfor i in xrange(2):\r\n  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):\r\n    inputs_tensor = tf.constant(1.0, shape=[100, 100])\r\n    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name=\"fc\", kernel_regularizer=l2_regularizer)\r\nprint (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\r\n```\r\n\r\nThis short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6. \r\n\r\nA pull request has been submitted here to fix this bug: https://github.com/tensorflow/tensorflow/pull/19053\r\n"}