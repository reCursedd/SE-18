{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336966150", "html_url": "https://github.com/tensorflow/tensorflow/issues/13530#issuecomment-336966150", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13530", "id": 336966150, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjk2NjE1MA==", "user": {"login": "mellvinbaker", "id": 26335535, "node_id": "MDQ6VXNlcjI2MzM1NTM1", "avatar_url": "https://avatars3.githubusercontent.com/u/26335535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mellvinbaker", "html_url": "https://github.com/mellvinbaker", "followers_url": "https://api.github.com/users/mellvinbaker/followers", "following_url": "https://api.github.com/users/mellvinbaker/following{/other_user}", "gists_url": "https://api.github.com/users/mellvinbaker/gists{/gist_id}", "starred_url": "https://api.github.com/users/mellvinbaker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mellvinbaker/subscriptions", "organizations_url": "https://api.github.com/users/mellvinbaker/orgs", "repos_url": "https://api.github.com/users/mellvinbaker/repos", "events_url": "https://api.github.com/users/mellvinbaker/events{/privacy}", "received_events_url": "https://api.github.com/users/mellvinbaker/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-16T17:43:00Z", "updated_at": "2017-10-16T19:26:52Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> The dataframe is 400,000 rows, not 200,000 and those 200,000 rows make for about ~.75gb of data (that is with my own dataset, with this generated dataset its closer to .3gb). I am not sure how that is any different than a .3GB array of image values. I would not think that batching .3gb of numbers to a GPU would be outside the scope of pandas_input_fn, but maybe I misunderstand its purpose (or its use, both are very possible). When would you ever need to batch a dataset of say 50mb to a tesla M60? The whole thing would fit in memory, thus avoiding the need for batching at all.</p>\n<p>The typical use would be to pull about 2,000,000 rows from SQL, batch them into the most efficient size for the GPU (without compromising training), and run the model. At the moment, I can run the entire 2,000,000 through an estimator if I use CPU only because I have ~128gb of ram on the server. That runs in half the time of batching that same 2,000,000 rows to the GPU, regardless of batch size.</p>\n<p>I have tried batch sizes ranging from 20k to 200k and all have the same problem. When one batch is done, the GPU sits idle waiting for more. The smaller the batch though, the lower the GPU utilization, thus causing slowdowns on the other end.</p>\n<p>edit: This sounded more argumentative than I meant it too. Rest assured I am trying to be complete and concise, not snarky.</p>", "body_text": "@mrry The dataframe is 400,000 rows, not 200,000 and those 200,000 rows make for about ~.75gb of data (that is with my own dataset, with this generated dataset its closer to .3gb). I am not sure how that is any different than a .3GB array of image values. I would not think that batching .3gb of numbers to a GPU would be outside the scope of pandas_input_fn, but maybe I misunderstand its purpose (or its use, both are very possible). When would you ever need to batch a dataset of say 50mb to a tesla M60? The whole thing would fit in memory, thus avoiding the need for batching at all.\nThe typical use would be to pull about 2,000,000 rows from SQL, batch them into the most efficient size for the GPU (without compromising training), and run the model. At the moment, I can run the entire 2,000,000 through an estimator if I use CPU only because I have ~128gb of ram on the server. That runs in half the time of batching that same 2,000,000 rows to the GPU, regardless of batch size.\nI have tried batch sizes ranging from 20k to 200k and all have the same problem. When one batch is done, the GPU sits idle waiting for more. The smaller the batch though, the lower the GPU utilization, thus causing slowdowns on the other end.\nedit: This sounded more argumentative than I meant it too. Rest assured I am trying to be complete and concise, not snarky.", "body": "@mrry The dataframe is 400,000 rows, not 200,000 and those 200,000 rows make for about ~.75gb of data (that is with my own dataset, with this generated dataset its closer to .3gb). I am not sure how that is any different than a .3GB array of image values. I would not think that batching .3gb of numbers to a GPU would be outside the scope of pandas_input_fn, but maybe I misunderstand its purpose (or its use, both are very possible). When would you ever need to batch a dataset of say 50mb to a tesla M60? The whole thing would fit in memory, thus avoiding the need for batching at all.\r\n\r\nThe typical use would be to pull about 2,000,000 rows from SQL, batch them into the most efficient size for the GPU (without compromising training), and run the model. At the moment, I can run the entire 2,000,000 through an estimator if I use CPU only because I have ~128gb of ram on the server. That runs in half the time of batching that same 2,000,000 rows to the GPU, regardless of batch size. \r\n\r\nI have tried batch sizes ranging from 20k to 200k and all have the same problem. When one batch is done, the GPU sits idle waiting for more. The smaller the batch though, the lower the GPU utilization, thus causing slowdowns on the other end.\r\n\r\nedit: This sounded more argumentative than I meant it too. Rest assured I am trying to be complete and concise, not snarky."}