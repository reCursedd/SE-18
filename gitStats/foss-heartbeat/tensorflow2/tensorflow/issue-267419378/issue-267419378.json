{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13892", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13892/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13892/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13892/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13892", "id": 267419378, "node_id": "MDU6SXNzdWUyNjc0MTkzNzg=", "number": 13892, "title": "Inconsistent Result of SyncReplicaOptimizer", "user": {"login": "heyucongtom", "id": 6735857, "node_id": "MDQ6VXNlcjY3MzU4NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6735857?v=4", "gravatar_id": "", "url": "https://api.github.com/users/heyucongtom", "html_url": "https://github.com/heyucongtom", "followers_url": "https://api.github.com/users/heyucongtom/followers", "following_url": "https://api.github.com/users/heyucongtom/following{/other_user}", "gists_url": "https://api.github.com/users/heyucongtom/gists{/gist_id}", "starred_url": "https://api.github.com/users/heyucongtom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/heyucongtom/subscriptions", "organizations_url": "https://api.github.com/users/heyucongtom/orgs", "repos_url": "https://api.github.com/users/heyucongtom/repos", "events_url": "https://api.github.com/users/heyucongtom/events{/privacy}", "received_events_url": "https://api.github.com/users/heyucongtom/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-21T22:37:03Z", "updated_at": "2017-10-24T17:23:09Z", "closed_at": "2017-10-23T22:25:10Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac Sierra 10.12.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.3.0-rc2-20-g0787eee 1.3.0</li>\n<li><strong>Python version</strong>: Python 3.5.2 |Anaconda custom (x86_64)</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: Not used/</li>\n<li><strong>GPU model and memory</strong>: Not used/</li>\n<li><strong>Exact command to reproduce</strong>: python synchronous_sgd.py (see below)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Training a trivial model of 2-layer fully connected MNIST, with one parameter server thread and one worker thread to reproduce this issue.</p>\n<p>The file is linked here. We run <code>python synchronized_sgd.py</code> and <code>python async_sgd.py</code> one after one <strong>in the same terminal so that they receive same random results</strong> to recreate the bug.</p>\n<p>The only difference in the two files below is: async comment out 10 trivial lines from sync. (Please diff)</p>\n<p><a href=\"https://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py\">https://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py</a><br>\n<a href=\"https://github.com/heyucongtom/PGRD/blob/master/async_sgd.py\">https://github.com/heyucongtom/PGRD/blob/master/async_sgd.py</a></p>\n<p>I make sure both trainer receive the exactly same data for each batch, and I also fixed the random seed. As a results, both model shall get exactly the same output. However, they don't.</p>\n<p>The problem is, after the first step, the two models are in sync. At exactly the <strong>second run of the train_op</strong>, this train_op of the sync replica doesn't update the model, nor does it update the global step, resulting in output:</p>\n<pre><code>Worker 0: training step 0 done (global step: 0)\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\nWorker 0: training step 1 done (global step: 1)\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\nWorker 0: training step 2 done (global step: 1)\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\nWorker 0: training step 3 done (global step: 2)\nOn trainer 0, iteration 2 ps it reaches 0.455800 accuracy\nWorker 0: training step 4 done (global step: 3)\nOn trainer 0, iteration 3 ps it reaches 0.477400 accuracy\nWorker 0: training step 5 done (global step: 4)\nOn trainer 0, iteration 4 ps it reaches 0.478100 accuracy\n</code></pre>\n<p><strong>As a comparison, let's take the output of the simple async version. With exactly</strong></p>\n<pre><code>Worker 0: training step 0 done (global step: 0)\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\nWorker 0: training step 1 done (global step: 1)\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy &lt;After the first training step, the accuracy is the same, which is expected.&gt;\nWorker 0: training step 2 done (global step: 2)\nOn trainer 0, iteration 2 ps it reaches 0.279100 accuracy &lt;Something different happening.&gt;\nWorker 0: training step 3 done (global step: 3)\nOn trainer 0, iteration 3 ps it reaches 0.427200 accuracy\nWorker 0: training step 4 done (global step: 4)\nOn trainer 0, iteration 4 ps it reaches 0.567100 accuracy\nWorker 0: training step 5 done (global step: 5)\nOn trainer 0, iteration 5 ps it reaches 0.617500 accuracy\nWorker 0: training step 6 done (global step: 6)\nOn trainer 0, iteration 6 ps it reaches 0.561400 accuracy\n</code></pre>\n<p>I read through the source code of SyncReplicaOptimizer and find out that the train_op returned by that optimizer is a Assign operation, which could be only executed after the grads were applied and global steps enqueued. So sync and async with only one process should be exactly the same.</p>\n<p>This behavior is mysterious to me now. Not sure if I got anything wrong.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Sierra 10.12.6\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0\nPython version: Python 3.5.2 |Anaconda custom (x86_64)\nBazel version (if compiling from source):\nCUDA/cuDNN version: Not used/\nGPU model and memory: Not used/\nExact command to reproduce: python synchronous_sgd.py (see below)\n\nDescribe the problem\nTraining a trivial model of 2-layer fully connected MNIST, with one parameter server thread and one worker thread to reproduce this issue.\nThe file is linked here. We run python synchronized_sgd.py and python async_sgd.py one after one in the same terminal so that they receive same random results to recreate the bug.\nThe only difference in the two files below is: async comment out 10 trivial lines from sync. (Please diff)\nhttps://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py\nhttps://github.com/heyucongtom/PGRD/blob/master/async_sgd.py\nI make sure both trainer receive the exactly same data for each batch, and I also fixed the random seed. As a results, both model shall get exactly the same output. However, they don't.\nThe problem is, after the first step, the two models are in sync. At exactly the second run of the train_op, this train_op of the sync replica doesn't update the model, nor does it update the global step, resulting in output:\nWorker 0: training step 0 done (global step: 0)\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\nWorker 0: training step 1 done (global step: 1)\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\nWorker 0: training step 2 done (global step: 1)\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\nWorker 0: training step 3 done (global step: 2)\nOn trainer 0, iteration 2 ps it reaches 0.455800 accuracy\nWorker 0: training step 4 done (global step: 3)\nOn trainer 0, iteration 3 ps it reaches 0.477400 accuracy\nWorker 0: training step 5 done (global step: 4)\nOn trainer 0, iteration 4 ps it reaches 0.478100 accuracy\n\nAs a comparison, let's take the output of the simple async version. With exactly\nWorker 0: training step 0 done (global step: 0)\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\nWorker 0: training step 1 done (global step: 1)\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy <After the first training step, the accuracy is the same, which is expected.>\nWorker 0: training step 2 done (global step: 2)\nOn trainer 0, iteration 2 ps it reaches 0.279100 accuracy <Something different happening.>\nWorker 0: training step 3 done (global step: 3)\nOn trainer 0, iteration 3 ps it reaches 0.427200 accuracy\nWorker 0: training step 4 done (global step: 4)\nOn trainer 0, iteration 4 ps it reaches 0.567100 accuracy\nWorker 0: training step 5 done (global step: 5)\nOn trainer 0, iteration 5 ps it reaches 0.617500 accuracy\nWorker 0: training step 6 done (global step: 6)\nOn trainer 0, iteration 6 ps it reaches 0.561400 accuracy\n\nI read through the source code of SyncReplicaOptimizer and find out that the train_op returned by that optimizer is a Assign operation, which could be only executed after the grads were applied and global steps enqueued. So sync and async with only one process should be exactly the same.\nThis behavior is mysterious to me now. Not sure if I got anything wrong.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: Python 3.5.2 |Anaconda custom (x86_64)\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Not used/\r\n- **GPU model and memory**: Not used/\r\n- **Exact command to reproduce**: python synchronous_sgd.py (see below)\r\n\r\n### Describe the problem\r\nTraining a trivial model of 2-layer fully connected MNIST, with one parameter server thread and one worker thread to reproduce this issue.\r\n\r\nThe file is linked here. We run `python synchronized_sgd.py` and `python async_sgd.py` one after one **in the same terminal so that they receive same random results** to recreate the bug.\r\n\r\nThe only difference in the two files below is: async comment out 10 trivial lines from sync. (Please diff)\r\n\r\nhttps://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py\r\nhttps://github.com/heyucongtom/PGRD/blob/master/async_sgd.py\r\n\r\nI make sure both trainer receive the exactly same data for each batch, and I also fixed the random seed. As a results, both model shall get exactly the same output. However, they don't.\r\n\r\nThe problem is, after the first step, the two models are in sync. At exactly the **second run of the train_op**, this train_op of the sync replica doesn't update the model, nor does it update the global step, resulting in output:\r\n\r\n```\r\nWorker 0: training step 0 done (global step: 0)\r\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\r\nWorker 0: training step 1 done (global step: 1)\r\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\r\nWorker 0: training step 2 done (global step: 1)\r\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\r\nWorker 0: training step 3 done (global step: 2)\r\nOn trainer 0, iteration 2 ps it reaches 0.455800 accuracy\r\nWorker 0: training step 4 done (global step: 3)\r\nOn trainer 0, iteration 3 ps it reaches 0.477400 accuracy\r\nWorker 0: training step 5 done (global step: 4)\r\nOn trainer 0, iteration 4 ps it reaches 0.478100 accuracy\r\n```\r\n\r\n\r\n**As a comparison, let's take the output of the simple async version. With exactly**\r\n\r\n```\r\nWorker 0: training step 0 done (global step: 0)\r\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\r\nWorker 0: training step 1 done (global step: 1)\r\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy <After the first training step, the accuracy is the same, which is expected.>\r\nWorker 0: training step 2 done (global step: 2)\r\nOn trainer 0, iteration 2 ps it reaches 0.279100 accuracy <Something different happening.>\r\nWorker 0: training step 3 done (global step: 3)\r\nOn trainer 0, iteration 3 ps it reaches 0.427200 accuracy\r\nWorker 0: training step 4 done (global step: 4)\r\nOn trainer 0, iteration 4 ps it reaches 0.567100 accuracy\r\nWorker 0: training step 5 done (global step: 5)\r\nOn trainer 0, iteration 5 ps it reaches 0.617500 accuracy\r\nWorker 0: training step 6 done (global step: 6)\r\nOn trainer 0, iteration 6 ps it reaches 0.561400 accuracy\r\n```\r\n\r\nI read through the source code of SyncReplicaOptimizer and find out that the train_op returned by that optimizer is a Assign operation, which could be only executed after the grads were applied and global steps enqueued. So sync and async with only one process should be exactly the same.\r\n\r\nThis behavior is mysterious to me now. Not sure if I got anything wrong.\r\n"}