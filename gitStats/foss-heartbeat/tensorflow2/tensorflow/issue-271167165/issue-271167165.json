{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14242", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14242/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14242/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14242/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14242", "id": 271167165, "node_id": "MDU6SXNzdWUyNzExNjcxNjU=", "number": 14242, "title": "Concert keras model to tf.Keras model", "user": {"login": "KirtoXX", "id": 18616051, "node_id": "MDQ6VXNlcjE4NjE2MDUx", "avatar_url": "https://avatars2.githubusercontent.com/u/18616051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KirtoXX", "html_url": "https://github.com/KirtoXX", "followers_url": "https://api.github.com/users/KirtoXX/followers", "following_url": "https://api.github.com/users/KirtoXX/following{/other_user}", "gists_url": "https://api.github.com/users/KirtoXX/gists{/gist_id}", "starred_url": "https://api.github.com/users/KirtoXX/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KirtoXX/subscriptions", "organizations_url": "https://api.github.com/users/KirtoXX/orgs", "repos_url": "https://api.github.com/users/KirtoXX/repos", "events_url": "https://api.github.com/users/KirtoXX/events{/privacy}", "received_events_url": "https://api.github.com/users/KirtoXX/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-04T05:02:57Z", "updated_at": "2017-11-06T16:38:26Z", "closed_at": "2017-11-06T16:38:26Z", "author_association": "NONE", "body_html": "<p>here is my original keras model</p>\n<p>def inference(image_tensor):<br>\n# the input size is 90<em>30</em>3<br>\nx = Conv2D(32,(3,3),name='conv1')(image_tensor)<br>\nx = MaxPool2D((2,2),name='pool1')(x)<br>\nx = BatchNormalization(name='bn1')(x)<br>\nx = Activation('relu')(x)</p>\n<pre><code>x = Conv2D(64,(3,3),name='conv2')(x)\nx = MaxPool2D((2, 2),name='pool2')(x)\nx = BatchNormalization(name='bn2')(x)\nx = Activation('relu')(x)\n\nx = Conv2D(64,(3,3), name='conv3')(x)\nx = MaxPool2D((2, 2), name='pool3')(x)\nx = BatchNormalization(name='bn3')(x)\nx = Activation('relu')(x)\n\nx = Flatten(name='flatte1',inputs=x)\n\nx = Dense(256,name='fc1')(x)\n\n# max_length of sentence\nx = RepeatVector(7,name='repeat')(x)\n\n#2*GRU the output shape is (None,7,256)\nx = Bidirectional(GRU(units=128,name='GRU',return_sequences=True))(x)\n\n#apply (None,7,256) to all GRUs output (None,7,16)\nx = TimeDistributed(Dense(16,name='fc2'))(x)\nx = Activation('softmax')(x)\nx = Flatten(name='Flatten2',inputs=x)\n</code></pre>\n<p>when I show my summary, it print like this</p>\n<hr>\n<h1>Layer (type)                 Output Shape              Param #</h1>\n<p>input_1 (InputLayer)         (None, 90, 30, 3)         0</p>\n<hr>\n<p>conv1 (Conv2D)               (None, 88, 28, 32)        896</p>\n<hr>\n<p>pool1 (MaxPooling2D)         (None, 44, 14, 32)        0</p>\n<hr>\n<p>bn1 (BatchNormalization)     (None, 44, 14, 32)        128</p>\n<hr>\n<p>activation_1 (Activation)    (None, 44, 14, 32)        0</p>\n<hr>\n<p>conv2 (Conv2D)               (None, 42, 12, 64)        18496</p>\n<hr>\n<p>pool2 (MaxPooling2D)         (None, 21, 6, 64)         0</p>\n<hr>\n<p>bn2 (BatchNormalization)     (None, 21, 6, 64)         256</p>\n<hr>\n<p>activation_2 (Activation)    (None, 21, 6, 64)         0</p>\n<hr>\n<p>conv3 (Conv2D)               (None, 19, 4, 64)         36928</p>\n<hr>\n<p>pool3 (MaxPooling2D)         (None, 9, 2, 64)          0</p>\n<hr>\n<p>bn3 (BatchNormalization)     (None, 9, 2, 64)          256</p>\n<hr>\n<p>activation_3 (Activation)    (None, 9, 2, 64)          0</p>\n<hr>\n<p>flatte1 (Flatten)            (None, 1152)              0</p>\n<hr>\n<p>fc1 (Dense)                  (None, 256)               295168</p>\n<hr>\n<p>repeat (RepeatVector)        (None, 7, 256)            0</p>\n<hr>\n<p>bidirectional_1 (Bidirection (None, 7, 256)            295680</p>\n<hr>\n<p>time_distributed_1 (TimeDist (None, 7, 16)             4112</p>\n<hr>\n<h1>Flatten2 (Flatten)           (None, 112)               0</h1>\n<p>but when I change my codes to tf.keras like this(the architecture do not change! )</p>\n<hr>\n<p>repeat (RepeatVector)        (None, 7, 256)            0</p>\n<hr>\n<p>bidirectional_1 (Bidirection (None, None, 256)         295680</p>\n<hr>\n<p>time_distributed_1 (TimeDist (None, None, 16)          4112</p>\n<hr>\n<p>activation_4 (Activation)    (None, None, 16)          0</p>\n<hr>\n<h1>Flatten2 (Flatten)           (None, None)              0</h1>\n<p>Total params: 651,920<br>\nTrainable params: 651,600<br>\nNon-trainable params: 320</p>\n<hr>\n<p>the output of bidirectional_1 is (None, None, 256), I just wonder why the GRU output was flase.<br>\nAnd how can I change my code?<br>\nThanks a lot!</p>", "body_text": "here is my original keras model\ndef inference(image_tensor):\n# the input size is 90303\nx = Conv2D(32,(3,3),name='conv1')(image_tensor)\nx = MaxPool2D((2,2),name='pool1')(x)\nx = BatchNormalization(name='bn1')(x)\nx = Activation('relu')(x)\nx = Conv2D(64,(3,3),name='conv2')(x)\nx = MaxPool2D((2, 2),name='pool2')(x)\nx = BatchNormalization(name='bn2')(x)\nx = Activation('relu')(x)\n\nx = Conv2D(64,(3,3), name='conv3')(x)\nx = MaxPool2D((2, 2), name='pool3')(x)\nx = BatchNormalization(name='bn3')(x)\nx = Activation('relu')(x)\n\nx = Flatten(name='flatte1',inputs=x)\n\nx = Dense(256,name='fc1')(x)\n\n# max_length of sentence\nx = RepeatVector(7,name='repeat')(x)\n\n#2*GRU the output shape is (None,7,256)\nx = Bidirectional(GRU(units=128,name='GRU',return_sequences=True))(x)\n\n#apply (None,7,256) to all GRUs output (None,7,16)\nx = TimeDistributed(Dense(16,name='fc2'))(x)\nx = Activation('softmax')(x)\nx = Flatten(name='Flatten2',inputs=x)\n\nwhen I show my summary, it print like this\n\nLayer (type)                 Output Shape              Param #\ninput_1 (InputLayer)         (None, 90, 30, 3)         0\n\nconv1 (Conv2D)               (None, 88, 28, 32)        896\n\npool1 (MaxPooling2D)         (None, 44, 14, 32)        0\n\nbn1 (BatchNormalization)     (None, 44, 14, 32)        128\n\nactivation_1 (Activation)    (None, 44, 14, 32)        0\n\nconv2 (Conv2D)               (None, 42, 12, 64)        18496\n\npool2 (MaxPooling2D)         (None, 21, 6, 64)         0\n\nbn2 (BatchNormalization)     (None, 21, 6, 64)         256\n\nactivation_2 (Activation)    (None, 21, 6, 64)         0\n\nconv3 (Conv2D)               (None, 19, 4, 64)         36928\n\npool3 (MaxPooling2D)         (None, 9, 2, 64)          0\n\nbn3 (BatchNormalization)     (None, 9, 2, 64)          256\n\nactivation_3 (Activation)    (None, 9, 2, 64)          0\n\nflatte1 (Flatten)            (None, 1152)              0\n\nfc1 (Dense)                  (None, 256)               295168\n\nrepeat (RepeatVector)        (None, 7, 256)            0\n\nbidirectional_1 (Bidirection (None, 7, 256)            295680\n\ntime_distributed_1 (TimeDist (None, 7, 16)             4112\n\nFlatten2 (Flatten)           (None, 112)               0\nbut when I change my codes to tf.keras like this(the architecture do not change! )\n\nrepeat (RepeatVector)        (None, 7, 256)            0\n\nbidirectional_1 (Bidirection (None, None, 256)         295680\n\ntime_distributed_1 (TimeDist (None, None, 16)          4112\n\nactivation_4 (Activation)    (None, None, 16)          0\n\nFlatten2 (Flatten)           (None, None)              0\nTotal params: 651,920\nTrainable params: 651,600\nNon-trainable params: 320\n\nthe output of bidirectional_1 is (None, None, 256), I just wonder why the GRU output was flase.\nAnd how can I change my code?\nThanks a lot!", "body": "here is my original keras model\r\n\r\ndef inference(image_tensor):\r\n    # the input size is 90*30*3\r\n    x = Conv2D(32,(3,3),name='conv1')(image_tensor)\r\n    x = MaxPool2D((2,2),name='pool1')(x)\r\n    x = BatchNormalization(name='bn1')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(64,(3,3),name='conv2')(x)\r\n    x = MaxPool2D((2, 2),name='pool2')(x)\r\n    x = BatchNormalization(name='bn2')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(64,(3,3), name='conv3')(x)\r\n    x = MaxPool2D((2, 2), name='pool3')(x)\r\n    x = BatchNormalization(name='bn3')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Flatten(name='flatte1',inputs=x)\r\n\r\n    x = Dense(256,name='fc1')(x)\r\n\r\n    # max_length of sentence\r\n    x = RepeatVector(7,name='repeat')(x)\r\n\r\n    #2*GRU the output shape is (None,7,256)\r\n    x = Bidirectional(GRU(units=128,name='GRU',return_sequences=True))(x)\r\n\r\n    #apply (None,7,256) to all GRUs output (None,7,16)\r\n    x = TimeDistributed(Dense(16,name='fc2'))(x)\r\n    x = Activation('softmax')(x)\r\n    x = Flatten(name='Flatten2',inputs=x)\r\n\r\nwhen I show my summary, it print like this\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 90, 30, 3)         0         \r\n_________________________________________________________________\r\nconv1 (Conv2D)               (None, 88, 28, 32)        896       \r\n_________________________________________________________________\r\npool1 (MaxPooling2D)         (None, 44, 14, 32)        0         \r\n_________________________________________________________________\r\nbn1 (BatchNormalization)     (None, 44, 14, 32)        128       \r\n_________________________________________________________________\r\nactivation_1 (Activation)    (None, 44, 14, 32)        0         \r\n_________________________________________________________________\r\nconv2 (Conv2D)               (None, 42, 12, 64)        18496     \r\n_________________________________________________________________\r\npool2 (MaxPooling2D)         (None, 21, 6, 64)         0         \r\n_________________________________________________________________\r\nbn2 (BatchNormalization)     (None, 21, 6, 64)         256       \r\n_________________________________________________________________\r\nactivation_2 (Activation)    (None, 21, 6, 64)         0         \r\n_________________________________________________________________\r\nconv3 (Conv2D)               (None, 19, 4, 64)         36928     \r\n_________________________________________________________________\r\npool3 (MaxPooling2D)         (None, 9, 2, 64)          0         \r\n_________________________________________________________________\r\nbn3 (BatchNormalization)     (None, 9, 2, 64)          256       \r\n_________________________________________________________________\r\nactivation_3 (Activation)    (None, 9, 2, 64)          0         \r\n_________________________________________________________________\r\nflatte1 (Flatten)            (None, 1152)              0         \r\n_________________________________________________________________\r\nfc1 (Dense)                  (None, 256)               295168    \r\n_________________________________________________________________\r\nrepeat (RepeatVector)        (None, 7, 256)            0         \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, 7, 256)            295680    \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, 7, 16)             4112      \r\n_________________________________________________________________\r\nFlatten2 (Flatten)           (None, 112)               0         \r\n=================================================================\r\n\r\nbut when I change my codes to tf.keras like this(the architecture do not change! )\r\n_________________________________________________________________\r\nrepeat (RepeatVector)        (None, 7, 256)            0         \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, None, 256)         295680    \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, None, 16)          4112      \r\n_________________________________________________________________\r\nactivation_4 (Activation)    (None, None, 16)          0         \r\n_________________________________________________________________\r\nFlatten2 (Flatten)           (None, None)              0         \r\n=================================================================\r\nTotal params: 651,920\r\nTrainable params: 651,600\r\nNon-trainable params: 320\r\n_________________________________________________________________\r\n\r\n\r\nthe output of bidirectional_1 is (None, None, 256), I just wonder why the GRU output was flase.\r\nAnd how can I change my code?\r\nThanks a lot!\r\n\r\n\r\n\r\n"}