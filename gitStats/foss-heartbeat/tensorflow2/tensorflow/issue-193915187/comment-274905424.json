{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/274905424", "html_url": "https://github.com/tensorflow/tensorflow/issues/6132#issuecomment-274905424", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6132", "id": 274905424, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDkwNTQyNA==", "user": {"login": "ashern", "id": 515749, "node_id": "MDQ6VXNlcjUxNTc0OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/515749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashern", "html_url": "https://github.com/ashern", "followers_url": "https://api.github.com/users/ashern/followers", "following_url": "https://api.github.com/users/ashern/following{/other_user}", "gists_url": "https://api.github.com/users/ashern/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashern/subscriptions", "organizations_url": "https://api.github.com/users/ashern/orgs", "repos_url": "https://api.github.com/users/ashern/repos", "events_url": "https://api.github.com/users/ashern/events{/privacy}", "received_events_url": "https://api.github.com/users/ashern/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-24T19:13:51Z", "updated_at": "2017-01-24T19:13:51Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16869974\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vvpreetham\">@vvpreetham</a>  I think what you're seeing is expected behavior, and unrelated to Estimator. Tensorflow will automatically claim all available memory for all of the GPUs that it sees, unless you tell it otherwise. It will do this whether you are using them for your model or not. Take a look at CUDA_VISIBLE_DEVICES environment setting to specify GPUs to use, or you can change the memory settings (see using GPUs in the docs)</p>", "body_text": "@vvpreetham  I think what you're seeing is expected behavior, and unrelated to Estimator. Tensorflow will automatically claim all available memory for all of the GPUs that it sees, unless you tell it otherwise. It will do this whether you are using them for your model or not. Take a look at CUDA_VISIBLE_DEVICES environment setting to specify GPUs to use, or you can change the memory settings (see using GPUs in the docs)", "body": "@vvpreetham  I think what you're seeing is expected behavior, and unrelated to Estimator. Tensorflow will automatically claim all available memory for all of the GPUs that it sees, unless you tell it otherwise. It will do this whether you are using them for your model or not. Take a look at CUDA_VISIBLE_DEVICES environment setting to specify GPUs to use, or you can change the memory settings (see using GPUs in the docs)"}