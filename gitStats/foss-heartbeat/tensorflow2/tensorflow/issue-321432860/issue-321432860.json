{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19171", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19171/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19171/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19171/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19171", "id": 321432860, "node_id": "MDU6SXNzdWUzMjE0MzI4NjA=", "number": 19171, "title": "Obtaining different results on declaring unused placeholders", "user": {"login": "svjan5", "id": 6361575, "node_id": "MDQ6VXNlcjYzNjE1NzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/6361575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/svjan5", "html_url": "https://github.com/svjan5", "followers_url": "https://api.github.com/users/svjan5/followers", "following_url": "https://api.github.com/users/svjan5/following{/other_user}", "gists_url": "https://api.github.com/users/svjan5/gists{/gist_id}", "starred_url": "https://api.github.com/users/svjan5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/svjan5/subscriptions", "organizations_url": "https://api.github.com/users/svjan5/orgs", "repos_url": "https://api.github.com/users/svjan5/repos", "events_url": "https://api.github.com/users/svjan5/events{/privacy}", "received_events_url": "https://api.github.com/users/svjan5/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-05-09T05:14:20Z", "updated_at": "2018-07-20T19:55:15Z", "closed_at": "2018-07-20T19:55:15Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.7</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080 Ti - 12GB</li>\n<li><strong>Exact command to reproduce</strong>: <a href=\"https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5\">Code link</a></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am getting different results on declaring some unused placeholders in the code even with the same seed. After setting the same seed in both the files, the results obtained are different although the only difference in the files is an addition of some unused placeholders in <a href=\"https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py\">placeholder_reproduce_unusedplc.py</a>. I am unable to understand the cause of this difference because the input and the computational graph is same for both the files. Also, with different seeds, sometimes the code with additional placeholders gives worse results.</p>\n<p>Difference between the two files (Line <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115997555\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/52\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/52/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/52\">#52</a>-<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116001308\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/57\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/57/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/57\">#57</a> <a href=\"https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py\">placeholder_reproduce_unusedplc.py</a>):</p>\n<pre><code>X1 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY1 = tf.placeholder(\"float\", [None, num_classes])\nX2 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY2 = tf.placeholder(\"float\", [None, num_classes])\nX3 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY3 = tf.placeholder(\"float\", [None, num_classes])\nX4 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY4 = tf.placeholder(\"float\", [None, num_classes])\n</code></pre>\n<h3>Source code / logs</h3>\n<p>Running <a href=\"https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce-py\">placeholder_reproduce.py</a> gives (same everytime):</p>\n<blockquote>\n<p>Step 1, Minibatch Loss= 2.5037, Training Accuracy= 0.164<br>\nStep 200, Minibatch Loss= 2.1293, Training Accuracy= 0.305<br>\nStep 400, Minibatch Loss= 1.8812, Training Accuracy= 0.453<br>\nStep 600, Minibatch Loss= 1.8287, Training Accuracy= 0.398<br>\nStep 800, Minibatch Loss= 1.7506, Training Accuracy= 0.430<br>\nStep 1000, Minibatch Loss= 1.6265, Training Accuracy= 0.508<br>\nStep 1200, Minibatch Loss= 1.4767, Training Accuracy= 0.516<br>\nStep 1400, Minibatch Loss= 1.4777, Training Accuracy= 0.547<br>\nStep 1600, Minibatch Loss= 1.3804, Training Accuracy= 0.586<br>\nStep 1800, Minibatch Loss= 1.2695, Training Accuracy= 0.672<br>\nStep 2000, Minibatch Loss= 1.2690, Training Accuracy= 0.594<br>\nStep 2200, Minibatch Loss= 1.2799, Training Accuracy= 0.570<br>\nStep 2400, Minibatch Loss= 1.1228, Training Accuracy= 0.680<br>\nStep 2600, Minibatch Loss= 1.0570, Training Accuracy= 0.695<br>\nStep 2800, Minibatch Loss= 1.0974, Training Accuracy= 0.625<br>\nStep 3000, Minibatch Loss= 1.1049, Training Accuracy= 0.641<br>\nStep 3200, Minibatch Loss= 1.0421, Training Accuracy= 0.688<br>\nStep 3400, Minibatch Loss= 1.0742, Training Accuracy= 0.672<br>\nStep 3600, Minibatch Loss= 1.0896, Training Accuracy= 0.648<br>\nStep 3800, Minibatch Loss= 0.8617, Training Accuracy= 0.734<br>\nStep 4000, Minibatch Loss= 0.9472, Training Accuracy= 0.719<br>\nStep 4200, Minibatch Loss= 0.7870, Training Accuracy= 0.773<br>\nStep 4400, Minibatch Loss= 0.9419, Training Accuracy= 0.656<br>\nStep 4600, Minibatch Loss= 0.7562, Training Accuracy= 0.750<br>\nStep 4800, Minibatch Loss= 0.8001, Training Accuracy= 0.758<br>\nStep 5000, Minibatch Loss= 0.9771, Training Accuracy= 0.711<br>\nStep 5200, Minibatch Loss= 0.7151, Training Accuracy= 0.805<br>\nStep 5400, Minibatch Loss= 0.7159, Training Accuracy= 0.789<br>\nStep 5600, Minibatch Loss= 0.7710, Training Accuracy= 0.797<br>\nStep 5800, Minibatch Loss= 0.7368, Training Accuracy= 0.766<br>\nStep 6000, Minibatch Loss= 0.7593, Training Accuracy= 0.758<br>\nStep 6200, Minibatch Loss= 0.6506, Training Accuracy= 0.773<br>\nStep 6400, Minibatch Loss= 0.8102, Training Accuracy= 0.766<br>\nStep 6600, Minibatch Loss= 0.5647, Training Accuracy= 0.820<br>\nStep 6800, Minibatch Loss= 0.6096, Training Accuracy= 0.828<br>\nStep 7000, Minibatch Loss= 0.6203, Training Accuracy= 0.844<br>\nStep 7200, Minibatch Loss= 0.5551, Training Accuracy= 0.820<br>\nStep 7400, Minibatch Loss= 0.5007, Training Accuracy= 0.836<br>\nStep 7600, Minibatch Loss= 0.5582, Training Accuracy= 0.844<br>\nStep 7800, Minibatch Loss= 0.6226, Training Accuracy= 0.789<br>\nStep 8000, Minibatch Loss= 0.5149, Training Accuracy= 0.812<br>\nStep 8200, Minibatch Loss= 0.5257, Training Accuracy= 0.844<br>\nStep 8400, Minibatch Loss= 0.4988, Training Accuracy= 0.844<br>\nStep 8600, Minibatch Loss= 0.5633, Training Accuracy= 0.805<br>\nStep 8800, Minibatch Loss= 0.4969, Training Accuracy= 0.797<br>\nStep 9000, Minibatch Loss= 0.4822, Training Accuracy= 0.844<br>\nStep 9200, Minibatch Loss= 0.4551, Training Accuracy= 0.844<br>\nStep 9400, Minibatch Loss= 0.4117, Training Accuracy= 0.906<br>\nStep 9600, Minibatch Loss= 0.4025, Training Accuracy= 0.883<br>\nStep 9800, Minibatch Loss= 0.5490, Training Accuracy= 0.812<br>\nStep 10000, Minibatch Loss= 0.4854, Training Accuracy= 0.836<br>\nOptimization Finished!<br>\nTesting Accuracy: 0.8515625</p>\n</blockquote>\n<p>Running <a href=\"https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py\">placeholder_reproduce_unusedplc.py</a> gives (same everytime):</p>\n<blockquote>\n<p>Step 1, Minibatch Loss= 3.9436, Training Accuracy= 0.055<br>\nStep 200, Minibatch Loss= 2.0941, Training Accuracy= 0.234<br>\nStep 400, Minibatch Loss= 1.8377, Training Accuracy= 0.422<br>\nStep 600, Minibatch Loss= 1.7426, Training Accuracy= 0.391<br>\nStep 800, Minibatch Loss= 1.7220, Training Accuracy= 0.383<br>\nStep 1000, Minibatch Loss= 1.6012, Training Accuracy= 0.469<br>\nStep 1200, Minibatch Loss= 1.4531, Training Accuracy= 0.531<br>\nStep 1400, Minibatch Loss= 1.4427, Training Accuracy= 0.516<br>\nStep 1600, Minibatch Loss= 1.2995, Training Accuracy= 0.594<br>\nStep 1800, Minibatch Loss= 1.2551, Training Accuracy= 0.664<br>\nStep 2000, Minibatch Loss= 1.2198, Training Accuracy= 0.648<br>\nStep 2200, Minibatch Loss= 1.2236, Training Accuracy= 0.609<br>\nStep 2400, Minibatch Loss= 1.0227, Training Accuracy= 0.680<br>\nStep 2600, Minibatch Loss= 0.9933, Training Accuracy= 0.695<br>\nStep 2800, Minibatch Loss= 0.9887, Training Accuracy= 0.656<br>\nStep 3000, Minibatch Loss= 1.0707, Training Accuracy= 0.578<br>\nStep 3200, Minibatch Loss= 0.9540, Training Accuracy= 0.680<br>\nStep 3400, Minibatch Loss= 0.9850, Training Accuracy= 0.695<br>\nStep 3600, Minibatch Loss= 1.0233, Training Accuracy= 0.664<br>\nStep 3800, Minibatch Loss= 0.8106, Training Accuracy= 0.727<br>\nStep 4000, Minibatch Loss= 0.8919, Training Accuracy= 0.719<br>\nStep 4200, Minibatch Loss= 0.7129, Training Accuracy= 0.773<br>\nStep 4400, Minibatch Loss= 0.8317, Training Accuracy= 0.734<br>\nStep 4600, Minibatch Loss= 0.6762, Training Accuracy= 0.789<br>\nStep 4800, Minibatch Loss= 0.6951, Training Accuracy= 0.773<br>\nStep 5000, Minibatch Loss= 0.8617, Training Accuracy= 0.727<br>\nStep 5200, Minibatch Loss= 0.6173, Training Accuracy= 0.828<br>\nStep 5400, Minibatch Loss= 0.6464, Training Accuracy= 0.797<br>\nStep 5600, Minibatch Loss= 0.6643, Training Accuracy= 0.828<br>\nStep 5800, Minibatch Loss= 0.6795, Training Accuracy= 0.758<br>\nStep 6000, Minibatch Loss= 0.6252, Training Accuracy= 0.812<br>\nStep 6200, Minibatch Loss= 0.5862, Training Accuracy= 0.812<br>\nStep 6400, Minibatch Loss= 0.7126, Training Accuracy= 0.812<br>\nStep 6600, Minibatch Loss= 0.4639, Training Accuracy= 0.891<br>\nStep 6800, Minibatch Loss= 0.5116, Training Accuracy= 0.859<br>\nStep 7000, Minibatch Loss= 0.5322, Training Accuracy= 0.852<br>\nStep 7200, Minibatch Loss= 0.4902, Training Accuracy= 0.844<br>\nStep 7400, Minibatch Loss= 0.4027, Training Accuracy= 0.867<br>\nStep 7600, Minibatch Loss= 0.4654, Training Accuracy= 0.875<br>\nStep 7800, Minibatch Loss= 0.5647, Training Accuracy= 0.812<br>\nStep 8000, Minibatch Loss= 0.4110, Training Accuracy= 0.852<br>\nStep 8200, Minibatch Loss= 0.3976, Training Accuracy= 0.844<br>\nStep 8400, Minibatch Loss= 0.3916, Training Accuracy= 0.867<br>\nStep 8600, Minibatch Loss= 0.5487, Training Accuracy= 0.797<br>\nStep 8800, Minibatch Loss= 0.4265, Training Accuracy= 0.875<br>\nStep 9000, Minibatch Loss= 0.4590, Training Accuracy= 0.859<br>\nStep 9200, Minibatch Loss= 0.4135, Training Accuracy= 0.906<br>\nStep 9400, Minibatch Loss= 0.3457, Training Accuracy= 0.914<br>\nStep 9600, Minibatch Loss= 0.4067, Training Accuracy= 0.844<br>\nStep 9800, Minibatch Loss= 0.4394, Training Accuracy= 0.859<br>\nStep 10000, Minibatch Loss= 0.3586, Training Accuracy= 0.875<br>\nOptimization Finished!<br>\nTesting Accuracy: 0.890625</p>\n</blockquote>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.7\nPython version: 3.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0\nGPU model and memory: GeForce GTX 1080 Ti - 12GB\nExact command to reproduce: Code link\n\nDescribe the problem\nI am getting different results on declaring some unused placeholders in the code even with the same seed. After setting the same seed in both the files, the results obtained are different although the only difference in the files is an addition of some unused placeholders in placeholder_reproduce_unusedplc.py. I am unable to understand the cause of this difference because the input and the computational graph is same for both the files. Also, with different seeds, sometimes the code with additional placeholders gives worse results.\nDifference between the two files (Line #52-#57 placeholder_reproduce_unusedplc.py):\nX1 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY1 = tf.placeholder(\"float\", [None, num_classes])\nX2 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY2 = tf.placeholder(\"float\", [None, num_classes])\nX3 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY3 = tf.placeholder(\"float\", [None, num_classes])\nX4 = tf.placeholder(\"float\", [None, timesteps, num_input])\nY4 = tf.placeholder(\"float\", [None, num_classes])\n\nSource code / logs\nRunning placeholder_reproduce.py gives (same everytime):\n\nStep 1, Minibatch Loss= 2.5037, Training Accuracy= 0.164\nStep 200, Minibatch Loss= 2.1293, Training Accuracy= 0.305\nStep 400, Minibatch Loss= 1.8812, Training Accuracy= 0.453\nStep 600, Minibatch Loss= 1.8287, Training Accuracy= 0.398\nStep 800, Minibatch Loss= 1.7506, Training Accuracy= 0.430\nStep 1000, Minibatch Loss= 1.6265, Training Accuracy= 0.508\nStep 1200, Minibatch Loss= 1.4767, Training Accuracy= 0.516\nStep 1400, Minibatch Loss= 1.4777, Training Accuracy= 0.547\nStep 1600, Minibatch Loss= 1.3804, Training Accuracy= 0.586\nStep 1800, Minibatch Loss= 1.2695, Training Accuracy= 0.672\nStep 2000, Minibatch Loss= 1.2690, Training Accuracy= 0.594\nStep 2200, Minibatch Loss= 1.2799, Training Accuracy= 0.570\nStep 2400, Minibatch Loss= 1.1228, Training Accuracy= 0.680\nStep 2600, Minibatch Loss= 1.0570, Training Accuracy= 0.695\nStep 2800, Minibatch Loss= 1.0974, Training Accuracy= 0.625\nStep 3000, Minibatch Loss= 1.1049, Training Accuracy= 0.641\nStep 3200, Minibatch Loss= 1.0421, Training Accuracy= 0.688\nStep 3400, Minibatch Loss= 1.0742, Training Accuracy= 0.672\nStep 3600, Minibatch Loss= 1.0896, Training Accuracy= 0.648\nStep 3800, Minibatch Loss= 0.8617, Training Accuracy= 0.734\nStep 4000, Minibatch Loss= 0.9472, Training Accuracy= 0.719\nStep 4200, Minibatch Loss= 0.7870, Training Accuracy= 0.773\nStep 4400, Minibatch Loss= 0.9419, Training Accuracy= 0.656\nStep 4600, Minibatch Loss= 0.7562, Training Accuracy= 0.750\nStep 4800, Minibatch Loss= 0.8001, Training Accuracy= 0.758\nStep 5000, Minibatch Loss= 0.9771, Training Accuracy= 0.711\nStep 5200, Minibatch Loss= 0.7151, Training Accuracy= 0.805\nStep 5400, Minibatch Loss= 0.7159, Training Accuracy= 0.789\nStep 5600, Minibatch Loss= 0.7710, Training Accuracy= 0.797\nStep 5800, Minibatch Loss= 0.7368, Training Accuracy= 0.766\nStep 6000, Minibatch Loss= 0.7593, Training Accuracy= 0.758\nStep 6200, Minibatch Loss= 0.6506, Training Accuracy= 0.773\nStep 6400, Minibatch Loss= 0.8102, Training Accuracy= 0.766\nStep 6600, Minibatch Loss= 0.5647, Training Accuracy= 0.820\nStep 6800, Minibatch Loss= 0.6096, Training Accuracy= 0.828\nStep 7000, Minibatch Loss= 0.6203, Training Accuracy= 0.844\nStep 7200, Minibatch Loss= 0.5551, Training Accuracy= 0.820\nStep 7400, Minibatch Loss= 0.5007, Training Accuracy= 0.836\nStep 7600, Minibatch Loss= 0.5582, Training Accuracy= 0.844\nStep 7800, Minibatch Loss= 0.6226, Training Accuracy= 0.789\nStep 8000, Minibatch Loss= 0.5149, Training Accuracy= 0.812\nStep 8200, Minibatch Loss= 0.5257, Training Accuracy= 0.844\nStep 8400, Minibatch Loss= 0.4988, Training Accuracy= 0.844\nStep 8600, Minibatch Loss= 0.5633, Training Accuracy= 0.805\nStep 8800, Minibatch Loss= 0.4969, Training Accuracy= 0.797\nStep 9000, Minibatch Loss= 0.4822, Training Accuracy= 0.844\nStep 9200, Minibatch Loss= 0.4551, Training Accuracy= 0.844\nStep 9400, Minibatch Loss= 0.4117, Training Accuracy= 0.906\nStep 9600, Minibatch Loss= 0.4025, Training Accuracy= 0.883\nStep 9800, Minibatch Loss= 0.5490, Training Accuracy= 0.812\nStep 10000, Minibatch Loss= 0.4854, Training Accuracy= 0.836\nOptimization Finished!\nTesting Accuracy: 0.8515625\n\nRunning placeholder_reproduce_unusedplc.py gives (same everytime):\n\nStep 1, Minibatch Loss= 3.9436, Training Accuracy= 0.055\nStep 200, Minibatch Loss= 2.0941, Training Accuracy= 0.234\nStep 400, Minibatch Loss= 1.8377, Training Accuracy= 0.422\nStep 600, Minibatch Loss= 1.7426, Training Accuracy= 0.391\nStep 800, Minibatch Loss= 1.7220, Training Accuracy= 0.383\nStep 1000, Minibatch Loss= 1.6012, Training Accuracy= 0.469\nStep 1200, Minibatch Loss= 1.4531, Training Accuracy= 0.531\nStep 1400, Minibatch Loss= 1.4427, Training Accuracy= 0.516\nStep 1600, Minibatch Loss= 1.2995, Training Accuracy= 0.594\nStep 1800, Minibatch Loss= 1.2551, Training Accuracy= 0.664\nStep 2000, Minibatch Loss= 1.2198, Training Accuracy= 0.648\nStep 2200, Minibatch Loss= 1.2236, Training Accuracy= 0.609\nStep 2400, Minibatch Loss= 1.0227, Training Accuracy= 0.680\nStep 2600, Minibatch Loss= 0.9933, Training Accuracy= 0.695\nStep 2800, Minibatch Loss= 0.9887, Training Accuracy= 0.656\nStep 3000, Minibatch Loss= 1.0707, Training Accuracy= 0.578\nStep 3200, Minibatch Loss= 0.9540, Training Accuracy= 0.680\nStep 3400, Minibatch Loss= 0.9850, Training Accuracy= 0.695\nStep 3600, Minibatch Loss= 1.0233, Training Accuracy= 0.664\nStep 3800, Minibatch Loss= 0.8106, Training Accuracy= 0.727\nStep 4000, Minibatch Loss= 0.8919, Training Accuracy= 0.719\nStep 4200, Minibatch Loss= 0.7129, Training Accuracy= 0.773\nStep 4400, Minibatch Loss= 0.8317, Training Accuracy= 0.734\nStep 4600, Minibatch Loss= 0.6762, Training Accuracy= 0.789\nStep 4800, Minibatch Loss= 0.6951, Training Accuracy= 0.773\nStep 5000, Minibatch Loss= 0.8617, Training Accuracy= 0.727\nStep 5200, Minibatch Loss= 0.6173, Training Accuracy= 0.828\nStep 5400, Minibatch Loss= 0.6464, Training Accuracy= 0.797\nStep 5600, Minibatch Loss= 0.6643, Training Accuracy= 0.828\nStep 5800, Minibatch Loss= 0.6795, Training Accuracy= 0.758\nStep 6000, Minibatch Loss= 0.6252, Training Accuracy= 0.812\nStep 6200, Minibatch Loss= 0.5862, Training Accuracy= 0.812\nStep 6400, Minibatch Loss= 0.7126, Training Accuracy= 0.812\nStep 6600, Minibatch Loss= 0.4639, Training Accuracy= 0.891\nStep 6800, Minibatch Loss= 0.5116, Training Accuracy= 0.859\nStep 7000, Minibatch Loss= 0.5322, Training Accuracy= 0.852\nStep 7200, Minibatch Loss= 0.4902, Training Accuracy= 0.844\nStep 7400, Minibatch Loss= 0.4027, Training Accuracy= 0.867\nStep 7600, Minibatch Loss= 0.4654, Training Accuracy= 0.875\nStep 7800, Minibatch Loss= 0.5647, Training Accuracy= 0.812\nStep 8000, Minibatch Loss= 0.4110, Training Accuracy= 0.852\nStep 8200, Minibatch Loss= 0.3976, Training Accuracy= 0.844\nStep 8400, Minibatch Loss= 0.3916, Training Accuracy= 0.867\nStep 8600, Minibatch Loss= 0.5487, Training Accuracy= 0.797\nStep 8800, Minibatch Loss= 0.4265, Training Accuracy= 0.875\nStep 9000, Minibatch Loss= 0.4590, Training Accuracy= 0.859\nStep 9200, Minibatch Loss= 0.4135, Training Accuracy= 0.906\nStep 9400, Minibatch Loss= 0.3457, Training Accuracy= 0.914\nStep 9600, Minibatch Loss= 0.4067, Training Accuracy= 0.844\nStep 9800, Minibatch Loss= 0.4394, Training Accuracy= 0.859\nStep 10000, Minibatch Loss= 0.3586, Training Accuracy= 0.875\nOptimization Finished!\nTesting Accuracy: 0.890625", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GeForce GTX 1080 Ti - 12GB\r\n- **Exact command to reproduce**: [Code link](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5)\r\n\r\n### Describe the problem\r\nI am getting different results on declaring some unused placeholders in the code even with the same seed. After setting the same seed in both the files, the results obtained are different although the only difference in the files is an addition of some unused placeholders in [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py). I am unable to understand the cause of this difference because the input and the computational graph is same for both the files. Also, with different seeds, sometimes the code with additional placeholders gives worse results.\r\n\r\nDifference between the two files (Line #52-#57 [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py)):\r\n```\r\nX1 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY1 = tf.placeholder(\"float\", [None, num_classes])\r\nX2 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY2 = tf.placeholder(\"float\", [None, num_classes])\r\nX3 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY3 = tf.placeholder(\"float\", [None, num_classes])\r\nX4 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY4 = tf.placeholder(\"float\", [None, num_classes])\r\n```\r\n### Source code / logs\r\nRunning [placeholder_reproduce.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce-py) gives (same everytime):\r\n\r\n> Step 1, Minibatch Loss= 2.5037, Training Accuracy= 0.164\r\n> Step 200, Minibatch Loss= 2.1293, Training Accuracy= 0.305\r\n> Step 400, Minibatch Loss= 1.8812, Training Accuracy= 0.453\r\n> Step 600, Minibatch Loss= 1.8287, Training Accuracy= 0.398\r\n> Step 800, Minibatch Loss= 1.7506, Training Accuracy= 0.430\r\n> Step 1000, Minibatch Loss= 1.6265, Training Accuracy= 0.508\r\n> Step 1200, Minibatch Loss= 1.4767, Training Accuracy= 0.516\r\n> Step 1400, Minibatch Loss= 1.4777, Training Accuracy= 0.547\r\n> Step 1600, Minibatch Loss= 1.3804, Training Accuracy= 0.586\r\n> Step 1800, Minibatch Loss= 1.2695, Training Accuracy= 0.672\r\n> Step 2000, Minibatch Loss= 1.2690, Training Accuracy= 0.594\r\n> Step 2200, Minibatch Loss= 1.2799, Training Accuracy= 0.570\r\n> Step 2400, Minibatch Loss= 1.1228, Training Accuracy= 0.680\r\n> Step 2600, Minibatch Loss= 1.0570, Training Accuracy= 0.695\r\n> Step 2800, Minibatch Loss= 1.0974, Training Accuracy= 0.625\r\n> Step 3000, Minibatch Loss= 1.1049, Training Accuracy= 0.641\r\n> Step 3200, Minibatch Loss= 1.0421, Training Accuracy= 0.688\r\n> Step 3400, Minibatch Loss= 1.0742, Training Accuracy= 0.672\r\n> Step 3600, Minibatch Loss= 1.0896, Training Accuracy= 0.648\r\n> Step 3800, Minibatch Loss= 0.8617, Training Accuracy= 0.734\r\n> Step 4000, Minibatch Loss= 0.9472, Training Accuracy= 0.719\r\n> Step 4200, Minibatch Loss= 0.7870, Training Accuracy= 0.773\r\n> Step 4400, Minibatch Loss= 0.9419, Training Accuracy= 0.656\r\n> Step 4600, Minibatch Loss= 0.7562, Training Accuracy= 0.750\r\n> Step 4800, Minibatch Loss= 0.8001, Training Accuracy= 0.758\r\n> Step 5000, Minibatch Loss= 0.9771, Training Accuracy= 0.711\r\n> Step 5200, Minibatch Loss= 0.7151, Training Accuracy= 0.805\r\n> Step 5400, Minibatch Loss= 0.7159, Training Accuracy= 0.789\r\n> Step 5600, Minibatch Loss= 0.7710, Training Accuracy= 0.797\r\n> Step 5800, Minibatch Loss= 0.7368, Training Accuracy= 0.766\r\n> Step 6000, Minibatch Loss= 0.7593, Training Accuracy= 0.758\r\n> Step 6200, Minibatch Loss= 0.6506, Training Accuracy= 0.773\r\n> Step 6400, Minibatch Loss= 0.8102, Training Accuracy= 0.766\r\n> Step 6600, Minibatch Loss= 0.5647, Training Accuracy= 0.820\r\n> Step 6800, Minibatch Loss= 0.6096, Training Accuracy= 0.828\r\n> Step 7000, Minibatch Loss= 0.6203, Training Accuracy= 0.844\r\n> Step 7200, Minibatch Loss= 0.5551, Training Accuracy= 0.820\r\n> Step 7400, Minibatch Loss= 0.5007, Training Accuracy= 0.836\r\n> Step 7600, Minibatch Loss= 0.5582, Training Accuracy= 0.844\r\n> Step 7800, Minibatch Loss= 0.6226, Training Accuracy= 0.789\r\n> Step 8000, Minibatch Loss= 0.5149, Training Accuracy= 0.812\r\n> Step 8200, Minibatch Loss= 0.5257, Training Accuracy= 0.844\r\n> Step 8400, Minibatch Loss= 0.4988, Training Accuracy= 0.844\r\n> Step 8600, Minibatch Loss= 0.5633, Training Accuracy= 0.805\r\n> Step 8800, Minibatch Loss= 0.4969, Training Accuracy= 0.797\r\n> Step 9000, Minibatch Loss= 0.4822, Training Accuracy= 0.844\r\n> Step 9200, Minibatch Loss= 0.4551, Training Accuracy= 0.844\r\n> Step 9400, Minibatch Loss= 0.4117, Training Accuracy= 0.906\r\n> Step 9600, Minibatch Loss= 0.4025, Training Accuracy= 0.883\r\n> Step 9800, Minibatch Loss= 0.5490, Training Accuracy= 0.812\r\n> Step 10000, Minibatch Loss= 0.4854, Training Accuracy= 0.836\r\n> Optimization Finished!\r\n> Testing Accuracy: 0.8515625\r\n\r\nRunning [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py) gives (same everytime):\r\n> Step 1, Minibatch Loss= 3.9436, Training Accuracy= 0.055\r\n> Step 200, Minibatch Loss= 2.0941, Training Accuracy= 0.234\r\n> Step 400, Minibatch Loss= 1.8377, Training Accuracy= 0.422\r\n> Step 600, Minibatch Loss= 1.7426, Training Accuracy= 0.391\r\n> Step 800, Minibatch Loss= 1.7220, Training Accuracy= 0.383\r\n> Step 1000, Minibatch Loss= 1.6012, Training Accuracy= 0.469\r\n> Step 1200, Minibatch Loss= 1.4531, Training Accuracy= 0.531\r\n> Step 1400, Minibatch Loss= 1.4427, Training Accuracy= 0.516\r\n> Step 1600, Minibatch Loss= 1.2995, Training Accuracy= 0.594\r\n> Step 1800, Minibatch Loss= 1.2551, Training Accuracy= 0.664\r\n> Step 2000, Minibatch Loss= 1.2198, Training Accuracy= 0.648\r\n> Step 2200, Minibatch Loss= 1.2236, Training Accuracy= 0.609\r\n> Step 2400, Minibatch Loss= 1.0227, Training Accuracy= 0.680\r\n> Step 2600, Minibatch Loss= 0.9933, Training Accuracy= 0.695\r\n> Step 2800, Minibatch Loss= 0.9887, Training Accuracy= 0.656\r\n> Step 3000, Minibatch Loss= 1.0707, Training Accuracy= 0.578\r\n> Step 3200, Minibatch Loss= 0.9540, Training Accuracy= 0.680\r\n> Step 3400, Minibatch Loss= 0.9850, Training Accuracy= 0.695\r\n> Step 3600, Minibatch Loss= 1.0233, Training Accuracy= 0.664\r\n> Step 3800, Minibatch Loss= 0.8106, Training Accuracy= 0.727\r\n> Step 4000, Minibatch Loss= 0.8919, Training Accuracy= 0.719\r\n> Step 4200, Minibatch Loss= 0.7129, Training Accuracy= 0.773\r\n> Step 4400, Minibatch Loss= 0.8317, Training Accuracy= 0.734\r\n> Step 4600, Minibatch Loss= 0.6762, Training Accuracy= 0.789\r\n> Step 4800, Minibatch Loss= 0.6951, Training Accuracy= 0.773\r\n> Step 5000, Minibatch Loss= 0.8617, Training Accuracy= 0.727\r\n> Step 5200, Minibatch Loss= 0.6173, Training Accuracy= 0.828\r\n> Step 5400, Minibatch Loss= 0.6464, Training Accuracy= 0.797\r\n> Step 5600, Minibatch Loss= 0.6643, Training Accuracy= 0.828\r\n> Step 5800, Minibatch Loss= 0.6795, Training Accuracy= 0.758\r\n> Step 6000, Minibatch Loss= 0.6252, Training Accuracy= 0.812\r\n> Step 6200, Minibatch Loss= 0.5862, Training Accuracy= 0.812\r\n> Step 6400, Minibatch Loss= 0.7126, Training Accuracy= 0.812\r\n> Step 6600, Minibatch Loss= 0.4639, Training Accuracy= 0.891\r\n> Step 6800, Minibatch Loss= 0.5116, Training Accuracy= 0.859\r\n> Step 7000, Minibatch Loss= 0.5322, Training Accuracy= 0.852\r\n> Step 7200, Minibatch Loss= 0.4902, Training Accuracy= 0.844\r\n> Step 7400, Minibatch Loss= 0.4027, Training Accuracy= 0.867\r\n> Step 7600, Minibatch Loss= 0.4654, Training Accuracy= 0.875\r\n> Step 7800, Minibatch Loss= 0.5647, Training Accuracy= 0.812\r\n> Step 8000, Minibatch Loss= 0.4110, Training Accuracy= 0.852\r\n> Step 8200, Minibatch Loss= 0.3976, Training Accuracy= 0.844\r\n> Step 8400, Minibatch Loss= 0.3916, Training Accuracy= 0.867\r\n> Step 8600, Minibatch Loss= 0.5487, Training Accuracy= 0.797\r\n> Step 8800, Minibatch Loss= 0.4265, Training Accuracy= 0.875\r\n> Step 9000, Minibatch Loss= 0.4590, Training Accuracy= 0.859\r\n> Step 9200, Minibatch Loss= 0.4135, Training Accuracy= 0.906\r\n> Step 9400, Minibatch Loss= 0.3457, Training Accuracy= 0.914\r\n> Step 9600, Minibatch Loss= 0.4067, Training Accuracy= 0.844\r\n> Step 9800, Minibatch Loss= 0.4394, Training Accuracy= 0.859\r\n> Step 10000, Minibatch Loss= 0.3586, Training Accuracy= 0.875\r\n> Optimization Finished!\r\n> Testing Accuracy: 0.890625\r\n"}