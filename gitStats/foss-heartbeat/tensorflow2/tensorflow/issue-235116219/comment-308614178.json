{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308614178", "html_url": "https://github.com/tensorflow/tensorflow/issues/10642#issuecomment-308614178", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10642", "id": 308614178, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODYxNDE3OA==", "user": {"login": "therc", "id": 13481082, "node_id": "MDQ6VXNlcjEzNDgxMDgy", "avatar_url": "https://avatars3.githubusercontent.com/u/13481082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/therc", "html_url": "https://github.com/therc", "followers_url": "https://api.github.com/users/therc/followers", "following_url": "https://api.github.com/users/therc/following{/other_user}", "gists_url": "https://api.github.com/users/therc/gists{/gist_id}", "starred_url": "https://api.github.com/users/therc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/therc/subscriptions", "organizations_url": "https://api.github.com/users/therc/orgs", "repos_url": "https://api.github.com/users/therc/repos", "events_url": "https://api.github.com/users/therc/events{/privacy}", "received_events_url": "https://api.github.com/users/therc/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-15T02:53:38Z", "updated_at": "2017-06-15T02:53:38Z", "author_association": "NONE", "body_html": "<p>That sounds like asking for trouble. You want the nvidia-smi binary and kernel driver to match. So that means that the binary should live outside of the image entirely and should live in a directory on the Kubernetes node that is kept in sync with the driver by the cluster administrator. Then the pod mounts the host directory as e.g. /usr/local/bin or /opt/bin. That's what we've been doing for the past year, going through 4-5 versions of the Nvidia drivers and never had issues (save one with nvprof, where we had erroneously included a few libraries in the image). It sucks that you have to add a couple of volumes and mounts to all your GPU YAMLs, but I have filed a feature request for Kubernetes to extend PodPreset so that you can say, e.g. \"when someone asks for GPU resources, always inject these volumes and mountpoints in the pod\".</p>", "body_text": "That sounds like asking for trouble. You want the nvidia-smi binary and kernel driver to match. So that means that the binary should live outside of the image entirely and should live in a directory on the Kubernetes node that is kept in sync with the driver by the cluster administrator. Then the pod mounts the host directory as e.g. /usr/local/bin or /opt/bin. That's what we've been doing for the past year, going through 4-5 versions of the Nvidia drivers and never had issues (save one with nvprof, where we had erroneously included a few libraries in the image). It sucks that you have to add a couple of volumes and mounts to all your GPU YAMLs, but I have filed a feature request for Kubernetes to extend PodPreset so that you can say, e.g. \"when someone asks for GPU resources, always inject these volumes and mountpoints in the pod\".", "body": "That sounds like asking for trouble. You want the nvidia-smi binary and kernel driver to match. So that means that the binary should live outside of the image entirely and should live in a directory on the Kubernetes node that is kept in sync with the driver by the cluster administrator. Then the pod mounts the host directory as e.g. /usr/local/bin or /opt/bin. That's what we've been doing for the past year, going through 4-5 versions of the Nvidia drivers and never had issues (save one with nvprof, where we had erroneously included a few libraries in the image). It sucks that you have to add a couple of volumes and mounts to all your GPU YAMLs, but I have filed a feature request for Kubernetes to extend PodPreset so that you can say, e.g. \"when someone asks for GPU resources, always inject these volumes and mountpoints in the pod\"."}