{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309323168", "html_url": "https://github.com/tensorflow/tensorflow/issues/10642#issuecomment-309323168", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10642", "id": 309323168, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTMyMzE2OA==", "user": {"login": "DjangoPeng", "id": 16943353, "node_id": "MDQ6VXNlcjE2OTQzMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16943353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DjangoPeng", "html_url": "https://github.com/DjangoPeng", "followers_url": "https://api.github.com/users/DjangoPeng/followers", "following_url": "https://api.github.com/users/DjangoPeng/following{/other_user}", "gists_url": "https://api.github.com/users/DjangoPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/DjangoPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DjangoPeng/subscriptions", "organizations_url": "https://api.github.com/users/DjangoPeng/orgs", "repos_url": "https://api.github.com/users/DjangoPeng/repos", "events_url": "https://api.github.com/users/DjangoPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/DjangoPeng/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-19T02:20:29Z", "updated_at": "2017-06-19T02:28:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4790487\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cmluciano\">@cmluciano</a> Cuda version is 8.0.44, and the nvidia driver version is 375.66.  I uninstalled the nvidia driver When I realize CUDA includes Nvidia driver actually.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13481082\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/therc\">@therc</a> Both <code>libcuda.so.1</code> and <code>libnvidia-fatbinaryloader.so.367.48</code> are copied from <code>/usr/lib64</code> to <code>/usr/local/nvidia/lib64</code> in host, and mounted into container later. The same as other shared object files about Nvidia driver.<br>\nAfter uninstalled the nvidia driver, the nvidia-smi shows that the nvidia driver, a part of CUDA, version is 367.48. What's more, I can locate the <code>nvidia-uvm</code> and <code>nvidia0</code> in the container.</p>\n<pre><code>$ nvidia-smi\nMon Jun 19 10:04:35 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          Off  | 0000:04:00.0     Off |                    0 |\n| N/A   45C    P0    71W / 235W |      0MiB / 11439MiB |     97%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>When I run the <code>deviceQuery</code> in the container, a cuda sample I built in host, I got the <code>Permission denied</code> error.</p>\n<pre><code># ./deviceQuery\n/bin/sh: 8: ./deviceQuery: Permission denied\n</code></pre>\n<p>But if run it outside the container, everything goes well. (The container run as root user too)</p>\n<pre><code># ./deviceQuery\n./deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"Tesla K40m\"\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\n  CUDA Capability Major/Minor version number:    3.5\n  Total amount of global memory:                 11440 MBytes (11995578368 bytes)\n  (15) Multiprocessors, (192) CUDA Cores/MP:     2880 CUDA Cores\n  GPU Max Clock rate:                            745 MHz (0.75 GHz)\n  Memory Clock rate:                             3004 Mhz\n  Memory Bus Width:                              384-bit\n  L2 Cache Size:                                 1572864 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Enabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0\n  Compute Mode:\n     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = Tesla K40m\nResult = PASS\n</code></pre>\n<p>It seems like the GPU is running well in host, but I'm not sure the <code>permission denied</code> error is caused by TensorFlow image or kubernetes. What I can ensure is that kubelet detected the GPU. When I run <code>kubectl describe node</code>, I got the GPU resource.</p>\n<pre><code>$kubectl describe node\n...\nCapacity:\n alpha.kubernetes.io/nvidia-gpu:        1\n cpu:                                   48\n memory:                                395679780Ki\n pods:                                  110\nAllocatable:\n alpha.kubernetes.io/nvidia-gpu:        1\n cpu:                                   48\n memory:                                395577380Ki\n pods:                                  110\n...\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> I agreed with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13481082\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/therc\">@therc</a> , if install the cuda toolkit into container, it would cause a mismatch problem between nvidia-smi binary and kernel driver.</p>", "body_text": "@cmluciano Cuda version is 8.0.44, and the nvidia driver version is 375.66.  I uninstalled the nvidia driver When I realize CUDA includes Nvidia driver actually.\n@therc Both libcuda.so.1 and libnvidia-fatbinaryloader.so.367.48 are copied from /usr/lib64 to /usr/local/nvidia/lib64 in host, and mounted into container later. The same as other shared object files about Nvidia driver.\nAfter uninstalled the nvidia driver, the nvidia-smi shows that the nvidia driver, a part of CUDA, version is 367.48. What's more, I can locate the nvidia-uvm and nvidia0 in the container.\n$ nvidia-smi\nMon Jun 19 10:04:35 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          Off  | 0000:04:00.0     Off |                    0 |\n| N/A   45C    P0    71W / 235W |      0MiB / 11439MiB |     97%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\nWhen I run the deviceQuery in the container, a cuda sample I built in host, I got the Permission denied error.\n# ./deviceQuery\n/bin/sh: 8: ./deviceQuery: Permission denied\n\nBut if run it outside the container, everything goes well. (The container run as root user too)\n# ./deviceQuery\n./deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"Tesla K40m\"\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\n  CUDA Capability Major/Minor version number:    3.5\n  Total amount of global memory:                 11440 MBytes (11995578368 bytes)\n  (15) Multiprocessors, (192) CUDA Cores/MP:     2880 CUDA Cores\n  GPU Max Clock rate:                            745 MHz (0.75 GHz)\n  Memory Clock rate:                             3004 Mhz\n  Memory Bus Width:                              384-bit\n  L2 Cache Size:                                 1572864 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Enabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = Tesla K40m\nResult = PASS\n\nIt seems like the GPU is running well in host, but I'm not sure the permission denied error is caused by TensorFlow image or kubernetes. What I can ensure is that kubelet detected the GPU. When I run kubectl describe node, I got the GPU resource.\n$kubectl describe node\n...\nCapacity:\n alpha.kubernetes.io/nvidia-gpu:        1\n cpu:                                   48\n memory:                                395679780Ki\n pods:                                  110\nAllocatable:\n alpha.kubernetes.io/nvidia-gpu:        1\n cpu:                                   48\n memory:                                395577380Ki\n pods:                                  110\n...\n\n@aselle I agreed with @therc , if install the cuda toolkit into container, it would cause a mismatch problem between nvidia-smi binary and kernel driver.", "body": "@cmluciano Cuda version is 8.0.44, and the nvidia driver version is 375.66.  I uninstalled the nvidia driver When I realize CUDA includes Nvidia driver actually.\r\n@therc Both `libcuda.so.1` and `libnvidia-fatbinaryloader.so.367.48` are copied from `/usr/lib64` to `/usr/local/nvidia/lib64` in host, and mounted into container later. The same as other shared object files about Nvidia driver.\r\nAfter uninstalled the nvidia driver, the nvidia-smi shows that the nvidia driver, a part of CUDA, version is 367.48. What's more, I can locate the `nvidia-uvm` and `nvidia0` in the container. \r\n```\r\n$ nvidia-smi\r\nMon Jun 19 10:04:35 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K40m          Off  | 0000:04:00.0     Off |                    0 |\r\n| N/A   45C    P0    71W / 235W |      0MiB / 11439MiB |     97%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nWhen I run the `deviceQuery` in the container, a cuda sample I built in host, I got the `Permission denied` error. \r\n```\r\n# ./deviceQuery\r\n/bin/sh: 8: ./deviceQuery: Permission denied\r\n```\r\n But if run it outside the container, everything goes well. (The container run as root user too)\r\n```\r\n# ./deviceQuery\r\n./deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"Tesla K40m\"\r\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\r\n  CUDA Capability Major/Minor version number:    3.5\r\n  Total amount of global memory:                 11440 MBytes (11995578368 bytes)\r\n  (15) Multiprocessors, (192) CUDA Cores/MP:     2880 CUDA Cores\r\n  GPU Max Clock rate:                            745 MHz (0.75 GHz)\r\n  Memory Clock rate:                             3004 Mhz\r\n  Memory Bus Width:                              384-bit\r\n  L2 Cache Size:                                 1572864 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     No\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Enabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = Tesla K40m\r\nResult = PASS\r\n```\r\nIt seems like the GPU is running well in host, but I'm not sure the `permission denied` error is caused by TensorFlow image or kubernetes. What I can ensure is that kubelet detected the GPU. When I run `kubectl describe node`, I got the GPU resource.\r\n```\r\n$kubectl describe node\r\n...\r\nCapacity:\r\n alpha.kubernetes.io/nvidia-gpu:        1\r\n cpu:                                   48\r\n memory:                                395679780Ki\r\n pods:                                  110\r\nAllocatable:\r\n alpha.kubernetes.io/nvidia-gpu:        1\r\n cpu:                                   48\r\n memory:                                395577380Ki\r\n pods:                                  110\r\n...\r\n```\r\n\r\n@aselle I agreed with @therc , if install the cuda toolkit into container, it would cause a mismatch problem between nvidia-smi binary and kernel driver."}