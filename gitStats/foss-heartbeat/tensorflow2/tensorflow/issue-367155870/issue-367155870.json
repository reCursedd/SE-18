{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22763", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22763/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22763/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22763/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22763", "id": 367155870, "node_id": "MDU6SXNzdWUzNjcxNTU4NzA=", "number": 22763, "title": "AdamWOptimizer and learning rate decay", "user": {"login": "benleetownsend", "id": 11413338, "node_id": "MDQ6VXNlcjExNDEzMzM4", "avatar_url": "https://avatars3.githubusercontent.com/u/11413338?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benleetownsend", "html_url": "https://github.com/benleetownsend", "followers_url": "https://api.github.com/users/benleetownsend/followers", "following_url": "https://api.github.com/users/benleetownsend/following{/other_user}", "gists_url": "https://api.github.com/users/benleetownsend/gists{/gist_id}", "starred_url": "https://api.github.com/users/benleetownsend/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benleetownsend/subscriptions", "organizations_url": "https://api.github.com/users/benleetownsend/orgs", "repos_url": "https://api.github.com/users/benleetownsend/repos", "events_url": "https://api.github.com/users/benleetownsend/events{/privacy}", "received_events_url": "https://api.github.com/users/benleetownsend/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-10-05T10:41:44Z", "updated_at": "2018-10-30T17:05:04Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li>**TensorFlow version (use command below)**1.11:</li>\n<li><strong>Python version</strong>:3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I think I have found that the contrib.opt.AdamWOptimizer and associated decoupled weight decay optimizers do not function correctly when using learning rate decay without explicitly applying the decay to the weight_decay parameter.</p>\n<p>In the original paper, as seen in algorithm 2, the schedule multiplier is factored out and applied to the whole expression, the current interface means you have to do <code>AdamWOptimizer(weight_decay=wd*decay, learning_rate=lr*decay)</code> to achieve parity with the paper, this is not in its self an issue, but I think the documentation should reflect this difference. Alternatively the API could give a schedule multiplier parameter and then fixed lr and wd parameters used.</p>\n<p>Happy to submit a PR if someone can advise whether a documentation or interface update is the desired approach.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): Source\n**TensorFlow version (use command below)**1.11:\nPython version:3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nI think I have found that the contrib.opt.AdamWOptimizer and associated decoupled weight decay optimizers do not function correctly when using learning rate decay without explicitly applying the decay to the weight_decay parameter.\nIn the original paper, as seen in algorithm 2, the schedule multiplier is factored out and applied to the whole expression, the current interface means you have to do AdamWOptimizer(weight_decay=wd*decay, learning_rate=lr*decay) to achieve parity with the paper, this is not in its self an issue, but I think the documentation should reflect this difference. Alternatively the API could give a schedule multiplier parameter and then fixed lr and wd parameters used.\nHappy to submit a PR if someone can advise whether a documentation or interface update is the desired approach.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04 \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**1.11:\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI think I have found that the contrib.opt.AdamWOptimizer and associated decoupled weight decay optimizers do not function correctly when using learning rate decay without explicitly applying the decay to the weight_decay parameter. \r\n\r\nIn the original paper, as seen in algorithm 2, the schedule multiplier is factored out and applied to the whole expression, the current interface means you have to do `AdamWOptimizer(weight_decay=wd*decay, learning_rate=lr*decay)` to achieve parity with the paper, this is not in its self an issue, but I think the documentation should reflect this difference. Alternatively the API could give a schedule multiplier parameter and then fixed lr and wd parameters used. \r\n\r\nHappy to submit a PR if someone can advise whether a documentation or interface update is the desired approach.\r\n\r\n"}