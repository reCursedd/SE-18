{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/427423060", "html_url": "https://github.com/tensorflow/tensorflow/issues/22763#issuecomment-427423060", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22763", "id": 427423060, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzQyMzA2MA==", "user": {"login": "PhilJd", "id": 16101605, "node_id": "MDQ6VXNlcjE2MTAxNjA1", "avatar_url": "https://avatars2.githubusercontent.com/u/16101605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilJd", "html_url": "https://github.com/PhilJd", "followers_url": "https://api.github.com/users/PhilJd/followers", "following_url": "https://api.github.com/users/PhilJd/following{/other_user}", "gists_url": "https://api.github.com/users/PhilJd/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilJd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilJd/subscriptions", "organizations_url": "https://api.github.com/users/PhilJd/orgs", "repos_url": "https://api.github.com/users/PhilJd/repos", "events_url": "https://api.github.com/users/PhilJd/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilJd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-05T16:26:08Z", "updated_at": "2018-10-05T17:49:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>You're right, the current implementation requires to schedule the weight decay manually. I also thought about introducing a <code>schedule_multiplier</code> argument but decided against it to keep the interface similar to the existing optimizers, which also require manual scheduling of the learning rate. This makes switching optimizers during hyperparameter search a lot easier.<br>\nI definitely agree that the documentation should be improved in this point ;)<br>\nAn alternative could be to introduce a parameter that specifies the <code>ratio weight_decay/learning_rate</code>, which could be used to compute <code>weight_decay = learning_rate * ratio</code>.<br>\nPersonally, I think this is less readable compared to explicit wd scheduling and improving the documentation is my preferred option but I'm happy to get convinced of the contrary ;)</p>", "body_text": "You're right, the current implementation requires to schedule the weight decay manually. I also thought about introducing a schedule_multiplier argument but decided against it to keep the interface similar to the existing optimizers, which also require manual scheduling of the learning rate. This makes switching optimizers during hyperparameter search a lot easier.\nI definitely agree that the documentation should be improved in this point ;)\nAn alternative could be to introduce a parameter that specifies the ratio weight_decay/learning_rate, which could be used to compute weight_decay = learning_rate * ratio.\nPersonally, I think this is less readable compared to explicit wd scheduling and improving the documentation is my preferred option but I'm happy to get convinced of the contrary ;)", "body": "You're right, the current implementation requires to schedule the weight decay manually. I also thought about introducing a `schedule_multiplier` argument but decided against it to keep the interface similar to the existing optimizers, which also require manual scheduling of the learning rate. This makes switching optimizers during hyperparameter search a lot easier.\r\nI definitely agree that the documentation should be improved in this point ;) \r\nAn alternative could be to introduce a parameter that specifies the `ratio weight_decay/learning_rate`, which could be used to compute `weight_decay = learning_rate * ratio`.\r\nPersonally, I think this is less readable compared to explicit wd scheduling and improving the documentation is my preferred option but I'm happy to get convinced of the contrary ;)"}