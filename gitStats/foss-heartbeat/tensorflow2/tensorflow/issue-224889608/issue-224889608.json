{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9498", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9498/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9498/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9498/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9498", "id": 224889608, "node_id": "MDU6SXNzdWUyMjQ4ODk2MDg=", "number": 9498, "title": "tf.metrics.accuracy maintains a running accuracy?", "user": {"login": "gongzhitaao", "id": 704995, "node_id": "MDQ6VXNlcjcwNDk5NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/704995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gongzhitaao", "html_url": "https://github.com/gongzhitaao", "followers_url": "https://api.github.com/users/gongzhitaao/followers", "following_url": "https://api.github.com/users/gongzhitaao/following{/other_user}", "gists_url": "https://api.github.com/users/gongzhitaao/gists{/gist_id}", "starred_url": "https://api.github.com/users/gongzhitaao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gongzhitaao/subscriptions", "organizations_url": "https://api.github.com/users/gongzhitaao/orgs", "repos_url": "https://api.github.com/users/gongzhitaao/repos", "events_url": "https://api.github.com/users/gongzhitaao/events{/privacy}", "received_events_url": "https://api.github.com/users/gongzhitaao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-04-27T19:32:05Z", "updated_at": "2018-08-18T19:36:21Z", "closed_at": "2017-06-12T18:53:01Z", "author_association": "NONE", "body_html": "<p>I use the <code>tf.metrics.accuracy</code>, however it is a bit <strong>counter-intuitive</strong> in that it maintains a <strong>running</strong> accuracy (the <a href=\"https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy\" rel=\"nofollow\">doc</a> agrees with this).  The following simple script illustrates the situation</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> supress tensorflow logging other than errors</span>\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TF_CPP_MIN_LOG_LEVEL<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>3<span class=\"pl-pds\">'</span></span>\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-c1\">print</span>(tf.<span class=\"pl-c1\">__version__</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 1.1.0</span>\n\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">5</span>])\ny <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">5</span>])\nacc, acc_op <span class=\"pl-k\">=</span> tf.metrics.accuracy(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>x, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>y)\n\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\nv <span class=\"pl-k\">=</span> sess.run([acc, acc_op], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>],\n                                       y: [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>]})\n<span class=\"pl-c1\">print</span>(v)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [0.0, 0.8]</span>\n\nv <span class=\"pl-k\">=</span> sess.run(acc)\n<span class=\"pl-c1\">print</span>(v)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 0.8</span>\n\nv <span class=\"pl-k\">=</span> sess.run([acc, acc_op], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>],\n                                       y: [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]})\n<span class=\"pl-c1\">print</span>(v)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [0.8, 0.4]</span>\n\nv <span class=\"pl-k\">=</span> sess.run(acc)\n<span class=\"pl-c1\">print</span>(v)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 0.4</span></pre></div>\n<p>My concerns are</p>\n<ol>\n<li>the use of accuracy is bit <strong>surprising</strong>, are we supposed to manually construct the normal accuracy?</li>\n<li>IMHO, it is better to\n<ol>\n<li>implement the normal accuracy behavior or</li>\n<li>provide a clean way to <strong>reset</strong> the local variables created by <code>tf.metrics.accuracy</code>, i.e., the <code>count</code> and <code>total</code>.</li>\n</ol>\n</li>\n</ol>", "body_text": "I use the tf.metrics.accuracy, however it is a bit counter-intuitive in that it maintains a running accuracy (the doc agrees with this).  The following simple script illustrates the situation\nimport os\n# supress tensorflow logging other than errors\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport tensorflow as tf\n\n\nprint(tf.__version__)\n# 1.1.0\n\nx = tf.placeholder(tf.int32, [5])\ny = tf.placeholder(tf.int32, [5])\nacc, acc_op = tf.metrics.accuracy(labels=x, predictions=y)\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\nv = sess.run([acc, acc_op], feed_dict={x: [1, 0, 0, 0, 0],\n                                       y: [1, 0, 0, 0, 1]})\nprint(v)\n# [0.0, 0.8]\n\nv = sess.run(acc)\nprint(v)\n# 0.8\n\nv = sess.run([acc, acc_op], feed_dict={x: [1, 0, 0, 0, 0],\n                                       y: [0, 1, 1, 1, 1]})\nprint(v)\n# [0.8, 0.4]\n\nv = sess.run(acc)\nprint(v)\n# 0.4\nMy concerns are\n\nthe use of accuracy is bit surprising, are we supposed to manually construct the normal accuracy?\nIMHO, it is better to\n\nimplement the normal accuracy behavior or\nprovide a clean way to reset the local variables created by tf.metrics.accuracy, i.e., the count and total.", "body": "I use the `tf.metrics.accuracy`, however it is a bit **counter-intuitive** in that it maintains a **running** accuracy (the [doc](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) agrees with this).  The following simple script illustrates the situation\r\n\r\n```python\r\nimport os\r\n# supress tensorflow logging other than errors\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nprint(tf.__version__)\r\n# 1.1.0\r\n\r\nx = tf.placeholder(tf.int32, [5])\r\ny = tf.placeholder(tf.int32, [5])\r\nacc, acc_op = tf.metrics.accuracy(labels=x, predictions=y)\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\n\r\nv = sess.run([acc, acc_op], feed_dict={x: [1, 0, 0, 0, 0],\r\n                                       y: [1, 0, 0, 0, 1]})\r\nprint(v)\r\n# [0.0, 0.8]\r\n\r\nv = sess.run(acc)\r\nprint(v)\r\n# 0.8\r\n\r\nv = sess.run([acc, acc_op], feed_dict={x: [1, 0, 0, 0, 0],\r\n                                       y: [0, 1, 1, 1, 1]})\r\nprint(v)\r\n# [0.8, 0.4]\r\n\r\nv = sess.run(acc)\r\nprint(v)\r\n# 0.4\r\n```\r\n\r\nMy concerns are\r\n1. the use of accuracy is bit **surprising**, are we supposed to manually construct the normal accuracy?\r\n2. IMHO, it is better to \r\n    1. implement the normal accuracy behavior or\r\n    2. provide a clean way to **reset** the local variables created by `tf.metrics.accuracy`, i.e., the `count` and `total`."}