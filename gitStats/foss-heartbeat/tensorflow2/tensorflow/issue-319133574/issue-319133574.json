{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18996", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18996/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18996/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18996/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18996", "id": 319133574, "node_id": "MDU6SXNzdWUzMTkxMzM1NzQ=", "number": 18996, "title": "Problems when implementing double backpropagation on BatchNormalization layer in residual block of ResNet", "user": {"login": "zoujx96", "id": 30309087, "node_id": "MDQ6VXNlcjMwMzA5MDg3", "avatar_url": "https://avatars1.githubusercontent.com/u/30309087?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zoujx96", "html_url": "https://github.com/zoujx96", "followers_url": "https://api.github.com/users/zoujx96/followers", "following_url": "https://api.github.com/users/zoujx96/following{/other_user}", "gists_url": "https://api.github.com/users/zoujx96/gists{/gist_id}", "starred_url": "https://api.github.com/users/zoujx96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zoujx96/subscriptions", "organizations_url": "https://api.github.com/users/zoujx96/orgs", "repos_url": "https://api.github.com/users/zoujx96/repos", "events_url": "https://api.github.com/users/zoujx96/events{/privacy}", "received_events_url": "https://api.github.com/users/zoujx96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-05-01T07:09:53Z", "updated_at": "2018-05-01T20:43:49Z", "closed_at": "2018-05-01T20:43:49Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-87-generic x86_64)</strong>:</li>\n<li><strong>TensorFlow version: 1.7.0</strong>:</li>\n<li><strong>Python version: 3.6.4</strong>:</li>\n<li><strong>GCC/Compiler version: 7.2.0</strong>:</li>\n<li><strong>CUDA/cuDNN version: CUDA 9.1.85</strong>:</li>\n<li><strong>Exact command to reproduce: python train_adv_cifar.py</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I met a problem when implementing double back-propagation when training my ResNet-20-V1 model to classify CIFAR10 images. Double back-propagation means to add a regularization term to the normal loss function. The regularization term is usually the gradient of the normal loss function W.R.T. the input tensor. The following codes are for reference. The problem happens when the following sentence is executed.<br>\n<code>train_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)</code><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/30309087/39464412-b3b3c84e-4d4f-11e8-93ce-3655c568ec14.png\"><img src=\"https://user-images.githubusercontent.com/30309087/39464412-b3b3c84e-4d4f-11e8-93ce-3655c568ec14.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>All the BN layers in the residual block have such problems.<br>\nAnd then it goes like this:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/30309087/39464422-c6fb2e24-4d4f-11e8-9f9c-a46908a0e50e.png\"><img src=\"https://user-images.githubusercontent.com/30309087/39464422-c6fb2e24-4d4f-11e8-9f9c-a46908a0e50e.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nThe above traceback is very long and it seems like the program is trapped there in a loop when constructing gradients in Tf Graph.<br>\nHowever, if I remove the BN layers in the residual block, the program works well. I also have BN layers as sequential parts of my model architecture and it works well. The residual block itself also works well. The problem happens only when there are BN layers in the residual block. But in custom ResNet architectures, there are usually BN layers in residual block. I couldn't figure out the solution.</p>\n<h3>Source code / logs</h3>\n<p>Here is my main code:<br>\n`<br>\nimport sys<br>\nimport tensorflow as tf<br>\nimport numpy as np</p>\n<p>from keras.preprocessing.image import ImageDataGenerator<br>\nfrom cifar_model_tf import Model_cifar</p>\n<p>model = Model_cifar(mode='train')</p>\n<p>x_nat = tf.placeholder(tf.float32,(None,32,32,3))</p>\n<p>y = tf.placeholder(tf.float32,(None,10))</p>\n<p>lamda = 100<br>\nlogits_nat = model._build_model(x_nat)<br>\npreds_nat = tf.nn.softmax(logits_nat)</p>\n<p>loss_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat))<br>\nloss_2 = tf.nn.l2_loss(tf.gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat)), x_nat)[0])<br>\ntotal_loss = loss_1 + loss_2 * lamda</p>\n<p>train_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)</p>\n<p>`</p>\n<p>And here is the code in my model file \"cifar_model_tf.py\":<br>\n`import numpy as np<br>\nimport tensorflow as tf</p>\n<p>class Model_cifar(object):<br>\n\"\"\"ResNet model.\"\"\"<br>\ndef <strong>init</strong>(self, mode='eval'):<br>\n\"\"\"ResNet constructor.\"\"\"<br>\nself.mode = mode</p>\n<p>def _stride_arr(self, stride):<br>\n\"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"<br>\nreturn [1, stride, stride, 1]</p>\n<p>def _build_model(self, x_input):<br>\nwith tf.variable_scope('model', reuse=tf.AUTO_REUSE):<br>\nwith tf.variable_scope('input'):<br>\nch = x_input.get_shape().as_list()[3]<br>\nx = self._conv('init_conv', x_input, 3, ch, 16, self._stride_arr(1))<br>\nx = self._batch_norm('init_bn', x)<br>\nx = self._relu(x)<br>\nres_func = self._residual<br>\nfilters = [16, 32, 64]<br>\nwith tf.variable_scope('unit_1'):<br>\nwith tf.variable_scope('unit_1_1'):<br>\nx = res_func(x, filters[0], filters[0], self._stride_arr(1))<br>\nwith tf.variable_scope('unit_1_2'):<br>\nx = res_func(x, filters[0], filters[0], self._stride_arr(1))<br>\nwith tf.variable_scope('unit_1_3'):<br>\nx = res_func(x, filters[0], filters[0], self._stride_arr(1))<br>\nwith tf.variable_scope('unit_2'):<br>\nwith tf.variable_scope('unit_2_1'):<br>\nx = res_func(x, filters[0], filters[1], self._stride_arr(2))<br>\nwith tf.variable_scope('unit_2_2'):<br>\nx = res_func(x, filters[1], filters[1], self._stride_arr(1))<br>\nwith tf.variable_scope('unit_2_3'):<br>\nx = res_func(x, filters[1], filters[1], self._stride_arr(1))<br>\nwith tf.variable_scope('unit_3'):<br>\nwith tf.variable_scope('unit_3_1'):<br>\nx = res_func(x, filters[1], filters[2], self._stride_arr(2))<br>\nwith tf.variable_scope('unit_3_2'):<br>\nx = res_func(x, filters[2], filters[2], self._stride_arr(1))<br>\nwith tf.variable_scope('unit_3_3'):<br>\nx = res_func(x, filters[2], filters[2], self._stride_arr(1))</p>\n<pre><code>  with tf.variable_scope('unit_last'):\n    x = self._avg_pool(x, 8)\n  with tf.variable_scope('logit'):\n    x = self._fully_connected(x, 10)\n  \n  return x \n</code></pre>\n<p>def _batch_norm(self, name, x):<br>\n\"\"\"Batch normalization.\"\"\"<br>\nwith tf.name_scope(name):<br>\nreturn tf.layers.batch_normalization(<br>\ninputs=x,<br>\ntraining=(self.mode == 'train'))</p>\n<p>def _residual(self, x, in_filter, out_filter, stride):<br>\n\"\"\"Residual unit with 2 sub layers.\"\"\"<br>\norig_x = x<br>\nwith tf.variable_scope('sub1'):<br>\nx = self._conv('conv1', x, 3, in_filter, out_filter, stride)<br>\nx = self._batch_norm('bn1', x)<br>\nx = self._relu(x)<br>\nwith tf.variable_scope('sub2'):<br>\nx = self._conv('conv2', x, 3, out_filter, out_filter, self._stride_arr(1))<br>\n#x = self._batch_norm('bn2', x)<br>\nwith tf.variable_scope('sub_add'):<br>\nif in_filter != out_filter:<br>\ny = self._conv('conv_match', orig_x, 1, in_filter, out_filter, stride)<br>\nelse:<br>\ny = orig_x<br>\nz = x + y<br>\nz = self._relu(z)</p>\n<pre><code>return z\n</code></pre>\n<p>def _conv(self, name, x, filter_size, in_filters, out_filters, strides):<br>\n\"\"\"Convolution.\"\"\"<br>\nwith tf.variable_scope(name):<br>\nn = filter_size * filter_size * out_filters<br>\nkernel = tf.get_variable('DW', [filter_size, filter_size, in_filters, out_filters],tf.float32, initializer=tf.keras.initializers.he_normal(), regularizer=tf.keras.regularizers.l2(l=1e-4))<br>\nbias = tf.get_variable('biases', [out_filters], initializer=tf.constant_initializer())<br>\nconv = tf.nn.conv2d(x, kernel, strides, padding='SAME')<br>\nresult = conv + bias<br>\nreturn result</p>\n<p>def _relu(self, x, leakiness=0.0):<br>\n\"\"\"Relu, with optional leaky support.\"\"\"<br>\nreturn tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')</p>\n<p>def _fully_connected(self, x, out_dim):<br>\n\"\"\"FullyConnected layer for final output.\"\"\"<br>\nnum_non_batch_dimensions = len(x.shape)<br>\nprod_non_batch_dimensions = 1<br>\nfor ii in range(num_non_batch_dimensions - 1):<br>\nprod_non_batch_dimensions *= int(x.shape[ii + 1])<br>\nx = tf.reshape(x, [tf.shape(x)[0], -1])<br>\nw = tf.get_variable(<br>\n'DW', [prod_non_batch_dimensions, out_dim],<br>\ninitializer=tf.keras.initializers.he_normal())<br>\nb = tf.get_variable('biases', [out_dim],<br>\ninitializer=tf.constant_initializer())<br>\nresult = tf.nn.xw_plus_b(x, w, b)<br>\nreturn result</p>\n<p>def _avg_pool(self, x, pool_size):<br>\nreturn tf.nn.avg_pool(x, ksize=[1,pool_size,pool_size,1], strides=[1,pool_size,pool_size,1], padding='VALID')`</p>", "body_text": "System information\n\nOS Platform and Distribution: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-87-generic x86_64):\nTensorFlow version: 1.7.0:\nPython version: 3.6.4:\nGCC/Compiler version: 7.2.0:\nCUDA/cuDNN version: CUDA 9.1.85:\nExact command to reproduce: python train_adv_cifar.py:\n\nDescribe the problem\nI met a problem when implementing double back-propagation when training my ResNet-20-V1 model to classify CIFAR10 images. Double back-propagation means to add a regularization term to the normal loss function. The regularization term is usually the gradient of the normal loss function W.R.T. the input tensor. The following codes are for reference. The problem happens when the following sentence is executed.\ntrain_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)\n\nAll the BN layers in the residual block have such problems.\nAnd then it goes like this:\n\nThe above traceback is very long and it seems like the program is trapped there in a loop when constructing gradients in Tf Graph.\nHowever, if I remove the BN layers in the residual block, the program works well. I also have BN layers as sequential parts of my model architecture and it works well. The residual block itself also works well. The problem happens only when there are BN layers in the residual block. But in custom ResNet architectures, there are usually BN layers in residual block. I couldn't figure out the solution.\nSource code / logs\nHere is my main code:\n`\nimport sys\nimport tensorflow as tf\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom cifar_model_tf import Model_cifar\nmodel = Model_cifar(mode='train')\nx_nat = tf.placeholder(tf.float32,(None,32,32,3))\ny = tf.placeholder(tf.float32,(None,10))\nlamda = 100\nlogits_nat = model._build_model(x_nat)\npreds_nat = tf.nn.softmax(logits_nat)\nloss_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat))\nloss_2 = tf.nn.l2_loss(tf.gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat)), x_nat)[0])\ntotal_loss = loss_1 + loss_2 * lamda\ntrain_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)\n`\nAnd here is the code in my model file \"cifar_model_tf.py\":\n`import numpy as np\nimport tensorflow as tf\nclass Model_cifar(object):\n\"\"\"ResNet model.\"\"\"\ndef init(self, mode='eval'):\n\"\"\"ResNet constructor.\"\"\"\nself.mode = mode\ndef _stride_arr(self, stride):\n\"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\nreturn [1, stride, stride, 1]\ndef _build_model(self, x_input):\nwith tf.variable_scope('model', reuse=tf.AUTO_REUSE):\nwith tf.variable_scope('input'):\nch = x_input.get_shape().as_list()[3]\nx = self._conv('init_conv', x_input, 3, ch, 16, self._stride_arr(1))\nx = self._batch_norm('init_bn', x)\nx = self._relu(x)\nres_func = self._residual\nfilters = [16, 32, 64]\nwith tf.variable_scope('unit_1'):\nwith tf.variable_scope('unit_1_1'):\nx = res_func(x, filters[0], filters[0], self._stride_arr(1))\nwith tf.variable_scope('unit_1_2'):\nx = res_func(x, filters[0], filters[0], self._stride_arr(1))\nwith tf.variable_scope('unit_1_3'):\nx = res_func(x, filters[0], filters[0], self._stride_arr(1))\nwith tf.variable_scope('unit_2'):\nwith tf.variable_scope('unit_2_1'):\nx = res_func(x, filters[0], filters[1], self._stride_arr(2))\nwith tf.variable_scope('unit_2_2'):\nx = res_func(x, filters[1], filters[1], self._stride_arr(1))\nwith tf.variable_scope('unit_2_3'):\nx = res_func(x, filters[1], filters[1], self._stride_arr(1))\nwith tf.variable_scope('unit_3'):\nwith tf.variable_scope('unit_3_1'):\nx = res_func(x, filters[1], filters[2], self._stride_arr(2))\nwith tf.variable_scope('unit_3_2'):\nx = res_func(x, filters[2], filters[2], self._stride_arr(1))\nwith tf.variable_scope('unit_3_3'):\nx = res_func(x, filters[2], filters[2], self._stride_arr(1))\n  with tf.variable_scope('unit_last'):\n    x = self._avg_pool(x, 8)\n  with tf.variable_scope('logit'):\n    x = self._fully_connected(x, 10)\n  \n  return x \n\ndef _batch_norm(self, name, x):\n\"\"\"Batch normalization.\"\"\"\nwith tf.name_scope(name):\nreturn tf.layers.batch_normalization(\ninputs=x,\ntraining=(self.mode == 'train'))\ndef _residual(self, x, in_filter, out_filter, stride):\n\"\"\"Residual unit with 2 sub layers.\"\"\"\norig_x = x\nwith tf.variable_scope('sub1'):\nx = self._conv('conv1', x, 3, in_filter, out_filter, stride)\nx = self._batch_norm('bn1', x)\nx = self._relu(x)\nwith tf.variable_scope('sub2'):\nx = self._conv('conv2', x, 3, out_filter, out_filter, self._stride_arr(1))\n#x = self._batch_norm('bn2', x)\nwith tf.variable_scope('sub_add'):\nif in_filter != out_filter:\ny = self._conv('conv_match', orig_x, 1, in_filter, out_filter, stride)\nelse:\ny = orig_x\nz = x + y\nz = self._relu(z)\nreturn z\n\ndef _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n\"\"\"Convolution.\"\"\"\nwith tf.variable_scope(name):\nn = filter_size * filter_size * out_filters\nkernel = tf.get_variable('DW', [filter_size, filter_size, in_filters, out_filters],tf.float32, initializer=tf.keras.initializers.he_normal(), regularizer=tf.keras.regularizers.l2(l=1e-4))\nbias = tf.get_variable('biases', [out_filters], initializer=tf.constant_initializer())\nconv = tf.nn.conv2d(x, kernel, strides, padding='SAME')\nresult = conv + bias\nreturn result\ndef _relu(self, x, leakiness=0.0):\n\"\"\"Relu, with optional leaky support.\"\"\"\nreturn tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\ndef _fully_connected(self, x, out_dim):\n\"\"\"FullyConnected layer for final output.\"\"\"\nnum_non_batch_dimensions = len(x.shape)\nprod_non_batch_dimensions = 1\nfor ii in range(num_non_batch_dimensions - 1):\nprod_non_batch_dimensions *= int(x.shape[ii + 1])\nx = tf.reshape(x, [tf.shape(x)[0], -1])\nw = tf.get_variable(\n'DW', [prod_non_batch_dimensions, out_dim],\ninitializer=tf.keras.initializers.he_normal())\nb = tf.get_variable('biases', [out_dim],\ninitializer=tf.constant_initializer())\nresult = tf.nn.xw_plus_b(x, w, b)\nreturn result\ndef _avg_pool(self, x, pool_size):\nreturn tf.nn.avg_pool(x, ksize=[1,pool_size,pool_size,1], strides=[1,pool_size,pool_size,1], padding='VALID')`", "body": "### System information\r\n- **OS Platform and Distribution: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-87-generic x86_64)**:\r\n- **TensorFlow version: 1.7.0**:\r\n- **Python version: 3.6.4**: \r\n- **GCC/Compiler version: 7.2.0**:\r\n- **CUDA/cuDNN version: CUDA 9.1.85**:\r\n- **Exact command to reproduce: python train_adv_cifar.py**:\r\n\r\n### Describe the problem\r\nI met a problem when implementing double back-propagation when training my ResNet-20-V1 model to classify CIFAR10 images. Double back-propagation means to add a regularization term to the normal loss function. The regularization term is usually the gradient of the normal loss function W.R.T. the input tensor. The following codes are for reference. The problem happens when the following sentence is executed.\r\n`train_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)`\r\n![image](https://user-images.githubusercontent.com/30309087/39464412-b3b3c84e-4d4f-11e8-93ce-3655c568ec14.png)\r\n\r\nAll the BN layers in the residual block have such problems. \r\nAnd then it goes like this:\r\n![image](https://user-images.githubusercontent.com/30309087/39464422-c6fb2e24-4d4f-11e8-9f9c-a46908a0e50e.png)\r\nThe above traceback is very long and it seems like the program is trapped there in a loop when constructing gradients in Tf Graph.\r\nHowever, if I remove the BN layers in the residual block, the program works well. I also have BN layers as sequential parts of my model architecture and it works well. The residual block itself also works well. The problem happens only when there are BN layers in the residual block. But in custom ResNet architectures, there are usually BN layers in residual block. I couldn't figure out the solution.\r\n\r\n### Source code / logs\r\nHere is my main code:\r\n`\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom cifar_model_tf import Model_cifar\r\n\r\nmodel = Model_cifar(mode='train')\r\n\r\nx_nat = tf.placeholder(tf.float32,(None,32,32,3))\r\n\r\ny = tf.placeholder(tf.float32,(None,10))\r\n\r\nlamda = 100\r\nlogits_nat = model._build_model(x_nat)\r\npreds_nat = tf.nn.softmax(logits_nat)\r\n\r\nloss_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat))\r\nloss_2 = tf.nn.l2_loss(tf.gradients(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits_nat)), x_nat)[0])\r\ntotal_loss = loss_1 + loss_2 * lamda\r\n\r\ntrain_step = tf.train.AdamOptimizer(learning_rate=0.0002, epsilon=1e-4).minimize(total_loss, global_step=global_step)\r\n\r\n`\r\n\r\nAnd here is the code in my model file \"cifar_model_tf.py\":\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\nclass Model_cifar(object):\r\n  \"\"\"ResNet model.\"\"\"\r\n  def __init__(self, mode='eval'):\r\n    \"\"\"ResNet constructor.\"\"\"\r\n    self.mode = mode\r\n     \r\n  def _stride_arr(self, stride):\r\n    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\r\n    return [1, stride, stride, 1]\r\n\r\n  def _build_model(self, x_input):\r\n    with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\r\n      with tf.variable_scope('input'):\r\n        ch = x_input.get_shape().as_list()[3]\r\n        x = self._conv('init_conv', x_input, 3, ch, 16, self._stride_arr(1))\r\n        x = self._batch_norm('init_bn', x)\r\n        x = self._relu(x)\r\n      res_func = self._residual\r\n      filters = [16, 32, 64]\r\n      with tf.variable_scope('unit_1'):\r\n        with tf.variable_scope('unit_1_1'):\r\n          x = res_func(x, filters[0], filters[0], self._stride_arr(1))\r\n        with tf.variable_scope('unit_1_2'):\r\n          x = res_func(x, filters[0], filters[0], self._stride_arr(1))\r\n        with tf.variable_scope('unit_1_3'):\r\n          x = res_func(x, filters[0], filters[0], self._stride_arr(1))\r\n      with tf.variable_scope('unit_2'):\r\n        with tf.variable_scope('unit_2_1'):\r\n          x = res_func(x, filters[0], filters[1], self._stride_arr(2))\r\n        with tf.variable_scope('unit_2_2'):\r\n          x = res_func(x, filters[1], filters[1], self._stride_arr(1))\r\n        with tf.variable_scope('unit_2_3'):\r\n          x = res_func(x, filters[1], filters[1], self._stride_arr(1))\r\n      with tf.variable_scope('unit_3'):\r\n        with tf.variable_scope('unit_3_1'):\r\n          x = res_func(x, filters[1], filters[2], self._stride_arr(2))\r\n        with tf.variable_scope('unit_3_2'):\r\n          x = res_func(x, filters[2], filters[2], self._stride_arr(1))\r\n        with tf.variable_scope('unit_3_3'):\r\n          x = res_func(x, filters[2], filters[2], self._stride_arr(1))\r\n      \r\n      with tf.variable_scope('unit_last'):\r\n        x = self._avg_pool(x, 8)\r\n      with tf.variable_scope('logit'):\r\n        x = self._fully_connected(x, 10)\r\n      \r\n      return x \r\n  \r\n  def _batch_norm(self, name, x):\r\n    \"\"\"Batch normalization.\"\"\"\r\n    with tf.name_scope(name):\r\n      return tf.layers.batch_normalization(\r\n          inputs=x,\r\n          training=(self.mode == 'train'))\r\n  \r\n\r\n  def _residual(self, x, in_filter, out_filter, stride):\r\n    \"\"\"Residual unit with 2 sub layers.\"\"\"\r\n    orig_x = x\r\n    with tf.variable_scope('sub1'):\r\n      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\r\n      x = self._batch_norm('bn1', x)\r\n      x = self._relu(x)\r\n    with tf.variable_scope('sub2'):\r\n      x = self._conv('conv2', x, 3, out_filter, out_filter, self._stride_arr(1))\r\n      #x = self._batch_norm('bn2', x)\r\n    with tf.variable_scope('sub_add'):\r\n      if in_filter != out_filter:\r\n        y = self._conv('conv_match', orig_x, 1, in_filter, out_filter, stride)\r\n      else:\r\n        y = orig_x\r\n      z = x + y\r\n      z = self._relu(z)\r\n\r\n    return z\r\n\r\n  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\r\n    \"\"\"Convolution.\"\"\"\r\n    with tf.variable_scope(name):\r\n      n = filter_size * filter_size * out_filters\r\n      kernel = tf.get_variable('DW', [filter_size, filter_size, in_filters, out_filters],tf.float32, initializer=tf.keras.initializers.he_normal(), regularizer=tf.keras.regularizers.l2(l=1e-4))\r\n      bias = tf.get_variable('biases', [out_filters], initializer=tf.constant_initializer())\r\n      conv = tf.nn.conv2d(x, kernel, strides, padding='SAME')\r\n      result = conv + bias\r\n      return result\r\n\r\n  def _relu(self, x, leakiness=0.0):\r\n    \"\"\"Relu, with optional leaky support.\"\"\"\r\n    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\r\n\r\n  def _fully_connected(self, x, out_dim):\r\n    \"\"\"FullyConnected layer for final output.\"\"\"\r\n    num_non_batch_dimensions = len(x.shape)\r\n    prod_non_batch_dimensions = 1\r\n    for ii in range(num_non_batch_dimensions - 1):\r\n      prod_non_batch_dimensions *= int(x.shape[ii + 1])\r\n    x = tf.reshape(x, [tf.shape(x)[0], -1])\r\n    w = tf.get_variable(\r\n        'DW', [prod_non_batch_dimensions, out_dim],\r\n        initializer=tf.keras.initializers.he_normal())\r\n    b = tf.get_variable('biases', [out_dim],\r\n                        initializer=tf.constant_initializer())\r\n    result = tf.nn.xw_plus_b(x, w, b)\r\n    return result\r\n\r\n  def _avg_pool(self, x, pool_size):\r\n    return tf.nn.avg_pool(x, ksize=[1,pool_size,pool_size,1], strides=[1,pool_size,pool_size,1], padding='VALID')`\r\n\r\n\r\n\r\n"}