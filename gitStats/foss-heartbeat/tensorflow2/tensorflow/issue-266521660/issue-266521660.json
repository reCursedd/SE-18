{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13806", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13806/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13806/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13806/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13806", "id": 266521660, "node_id": "MDU6SXNzdWUyNjY1MjE2NjA=", "number": 13806, "title": "What will happen if one of the worker becomes dead in distribute tensorflow?", "user": {"login": "xiechengsheng", "id": 15869774, "node_id": "MDQ6VXNlcjE1ODY5Nzc0", "avatar_url": "https://avatars2.githubusercontent.com/u/15869774?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiechengsheng", "html_url": "https://github.com/xiechengsheng", "followers_url": "https://api.github.com/users/xiechengsheng/followers", "following_url": "https://api.github.com/users/xiechengsheng/following{/other_user}", "gists_url": "https://api.github.com/users/xiechengsheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiechengsheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiechengsheng/subscriptions", "organizations_url": "https://api.github.com/users/xiechengsheng/orgs", "repos_url": "https://api.github.com/users/xiechengsheng/repos", "events_url": "https://api.github.com/users/xiechengsheng/events{/privacy}", "received_events_url": "https://api.github.com/users/xiechengsheng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-18T14:47:30Z", "updated_at": "2017-10-18T22:05:35Z", "closed_at": "2017-10-18T22:05:35Z", "author_association": "NONE", "body_html": "<p>Hi, I deployed a <strong>distribute tensorflow</strong> cluster to train a Deep Neural Network system, but in the training process, one of the worker broke down for some unknown reason(maybe the bad network), while the other workers were still training data, their training progresses didn't stop and continued going on. And after I restarted the broken worker, the ps node could send data to the broken worker, the broken worker also could train data with other workers, but the amazing thing happened: <strong>the good workers' training progresses were set to zero, and they began to retrain the data with the broken worker node. So my previous training progresses were all gone...</strong> That's to say, the tarining process restarted. is there anybody know how to solve this problem?<br>\nMy tensorflow version: 0.10.0</p>\n<p>Thanks in advance!</p>", "body_text": "Hi, I deployed a distribute tensorflow cluster to train a Deep Neural Network system, but in the training process, one of the worker broke down for some unknown reason(maybe the bad network), while the other workers were still training data, their training progresses didn't stop and continued going on. And after I restarted the broken worker, the ps node could send data to the broken worker, the broken worker also could train data with other workers, but the amazing thing happened: the good workers' training progresses were set to zero, and they began to retrain the data with the broken worker node. So my previous training progresses were all gone... That's to say, the tarining process restarted. is there anybody know how to solve this problem?\nMy tensorflow version: 0.10.0\nThanks in advance!", "body": "Hi, I deployed a **distribute tensorflow** cluster to train a Deep Neural Network system, but in the training process, one of the worker broke down for some unknown reason(maybe the bad network), while the other workers were still training data, their training progresses didn't stop and continued going on. And after I restarted the broken worker, the ps node could send data to the broken worker, the broken worker also could train data with other workers, but the amazing thing happened: **the good workers' training progresses were set to zero, and they began to retrain the data with the broken worker node. So my previous training progresses were all gone...** That's to say, the tarining process restarted. is there anybody know how to solve this problem?\r\nMy tensorflow version: 0.10.0\r\n\r\nThanks in advance!"}