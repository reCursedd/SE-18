{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20145", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20145/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20145/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20145/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20145", "id": 334056384, "node_id": "MDU6SXNzdWUzMzQwNTYzODQ=", "number": 20145, "title": "Tf lite: EXC_BAD_ACCESS when invoking the interpreter in iOS", "user": {"login": "mjuvilla", "id": 8360740, "node_id": "MDQ6VXNlcjgzNjA3NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8360740?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjuvilla", "html_url": "https://github.com/mjuvilla", "followers_url": "https://api.github.com/users/mjuvilla/followers", "following_url": "https://api.github.com/users/mjuvilla/following{/other_user}", "gists_url": "https://api.github.com/users/mjuvilla/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjuvilla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjuvilla/subscriptions", "organizations_url": "https://api.github.com/users/mjuvilla/orgs", "repos_url": "https://api.github.com/users/mjuvilla/repos", "events_url": "https://api.github.com/users/mjuvilla/events{/privacy}", "received_events_url": "https://api.github.com/users/mjuvilla/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-20T12:33:49Z", "updated_at": "2018-06-20T13:13:43Z", "closed_at": "2018-06-20T13:13:43Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS X High Sierra</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0</li>\n<li><strong>Python version</strong>: 3.5</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I created a feedforward neural network in Python and then exported the tflite model using:</p>\n<pre><code>bazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=frozen_graphdef.pb \\\n  --output_file=model.tflite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=FLOAT \\\n  --input_shape=1,200 \\\n  --input_array=inputs \\\n  --output_array=prediction  \n</code></pre>\n<p>I'm now trying to use it in iOS following the guidelines of your examples. This is my code:</p>\n<pre><code>\n  float* _fInput;\n  std::unique_ptr&lt;tflite::Interpreter&gt; _interpreter;\n\n  std::unique_ptr&lt;tflite::FlatBufferModel&gt; model = tflite::FlatBufferModel::BuildFromFile(sModel_path.c_str());\n  \n  if (!model) {\n    std::cout &lt;&lt; \"Failed to map model \" &lt;&lt; sModel_path &lt;&lt; std::endl;\n    exit(-1);\n  }\n  std::cout &lt;&lt; \"Loaded model \" &lt;&lt; sModel_path &lt;&lt; std::endl;\n  \n  tflite::ops::builtin::BuiltinOpResolver resolver;\n  \n  tflite::InterpreterBuilder(*model, resolver)(&amp;_interpreter);\n  \n  if (!_interpreter) {\n    std::cout &lt;&lt; \"Failed to construct interpreter.\" &lt;&lt; std::endl;\n    exit(-1);\n  }\n  \n  int input_idx = _interpreter-&gt;inputs()[0];\n  \n  std::vector&lt;int&gt; sizes = {1, NUM_FEAT};\n  std::string input_layer_type = \"float\";\n  \n  if (input_layer_type != \"string\") {\n    _interpreter-&gt;ResizeInputTensor(input_idx, sizes);\n  }\n\n  if (_interpreter-&gt;AllocateTensors() != kTfLiteOk) {\n    printf(\"Failed to allocate tensors!\\n\");\n  }\n  \n  _fInput = _interpreter-&gt;typed_tensor&lt;float&gt;(input_idx);\n\n  // receive vFeatures, an std::vector&lt;float&gt; of size NUM_FEAT\n\n  for(int i = 0; i &lt; NUM_FEAT; ++i)\n    _fInput[i] = vFeatures[i];\n  \n  if (_interpreter-&gt;Invoke() != kTfLiteOk) {\n    std::cout &lt;&lt; \"Failed to invoke!\" &lt;&lt; std::endl;\n    exit(-1);\n  }\n\n</code></pre>\n<p>For some reason, the execution fails on the line _interpreter-&gt;Invoke(), I get an \"EXC_BAD_ACCESS\" error.</p>\n<p>On a sidenote, do I need the ResizeInputTensor and AllocateTensors lines? It seems to me that the resizing is not necessary since I already stated the input size when exporting the tflite model, and I've seen some examples in which these lines were not present.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS X High Sierra\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.9.0\nPython version: 3.5\n\nDescribe the problem\nI created a feedforward neural network in Python and then exported the tflite model using:\nbazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=frozen_graphdef.pb \\\n  --output_file=model.tflite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=FLOAT \\\n  --input_shape=1,200 \\\n  --input_array=inputs \\\n  --output_array=prediction  \n\nI'm now trying to use it in iOS following the guidelines of your examples. This is my code:\n\n  float* _fInput;\n  std::unique_ptr<tflite::Interpreter> _interpreter;\n\n  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(sModel_path.c_str());\n  \n  if (!model) {\n    std::cout << \"Failed to map model \" << sModel_path << std::endl;\n    exit(-1);\n  }\n  std::cout << \"Loaded model \" << sModel_path << std::endl;\n  \n  tflite::ops::builtin::BuiltinOpResolver resolver;\n  \n  tflite::InterpreterBuilder(*model, resolver)(&_interpreter);\n  \n  if (!_interpreter) {\n    std::cout << \"Failed to construct interpreter.\" << std::endl;\n    exit(-1);\n  }\n  \n  int input_idx = _interpreter->inputs()[0];\n  \n  std::vector<int> sizes = {1, NUM_FEAT};\n  std::string input_layer_type = \"float\";\n  \n  if (input_layer_type != \"string\") {\n    _interpreter->ResizeInputTensor(input_idx, sizes);\n  }\n\n  if (_interpreter->AllocateTensors() != kTfLiteOk) {\n    printf(\"Failed to allocate tensors!\\n\");\n  }\n  \n  _fInput = _interpreter->typed_tensor<float>(input_idx);\n\n  // receive vFeatures, an std::vector<float> of size NUM_FEAT\n\n  for(int i = 0; i < NUM_FEAT; ++i)\n    _fInput[i] = vFeatures[i];\n  \n  if (_interpreter->Invoke() != kTfLiteOk) {\n    std::cout << \"Failed to invoke!\" << std::endl;\n    exit(-1);\n  }\n\n\nFor some reason, the execution fails on the line _interpreter->Invoke(), I get an \"EXC_BAD_ACCESS\" error.\nOn a sidenote, do I need the ResizeInputTensor and AllocateTensors lines? It seems to me that the resizing is not necessary since I already stated the input size when exporting the tflite model, and I've seen some examples in which these lines were not present.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS X High Sierra\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.5\r\n\r\n### Describe the problem\r\nI created a feedforward neural network in Python and then exported the tflite model using:\r\n\r\n```\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=frozen_graphdef.pb \\\r\n  --output_file=model.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shape=1,200 \\\r\n  --input_array=inputs \\\r\n  --output_array=prediction  \r\n```\r\n\r\nI'm now trying to use it in iOS following the guidelines of your examples. This is my code:\r\n\r\n```\r\n\r\n  float* _fInput;\r\n  std::unique_ptr<tflite::Interpreter> _interpreter;\r\n\r\n  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(sModel_path.c_str());\r\n  \r\n  if (!model) {\r\n    std::cout << \"Failed to map model \" << sModel_path << std::endl;\r\n    exit(-1);\r\n  }\r\n  std::cout << \"Loaded model \" << sModel_path << std::endl;\r\n  \r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  \r\n  tflite::InterpreterBuilder(*model, resolver)(&_interpreter);\r\n  \r\n  if (!_interpreter) {\r\n    std::cout << \"Failed to construct interpreter.\" << std::endl;\r\n    exit(-1);\r\n  }\r\n  \r\n  int input_idx = _interpreter->inputs()[0];\r\n  \r\n  std::vector<int> sizes = {1, NUM_FEAT};\r\n  std::string input_layer_type = \"float\";\r\n  \r\n  if (input_layer_type != \"string\") {\r\n    _interpreter->ResizeInputTensor(input_idx, sizes);\r\n  }\r\n\r\n  if (_interpreter->AllocateTensors() != kTfLiteOk) {\r\n    printf(\"Failed to allocate tensors!\\n\");\r\n  }\r\n  \r\n  _fInput = _interpreter->typed_tensor<float>(input_idx);\r\n\r\n  // receive vFeatures, an std::vector<float> of size NUM_FEAT\r\n\r\n  for(int i = 0; i < NUM_FEAT; ++i)\r\n    _fInput[i] = vFeatures[i];\r\n  \r\n  if (_interpreter->Invoke() != kTfLiteOk) {\r\n    std::cout << \"Failed to invoke!\" << std::endl;\r\n    exit(-1);\r\n  }\r\n\r\n```\r\n\r\nFor some reason, the execution fails on the line _interpreter->Invoke(), I get an \"EXC_BAD_ACCESS\" error. \r\n\r\nOn a sidenote, do I need the ResizeInputTensor and AllocateTensors lines? It seems to me that the resizing is not necessary since I already stated the input size when exporting the tflite model, and I've seen some examples in which these lines were not present.\r\n"}