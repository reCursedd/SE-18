{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16638", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16638/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16638/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16638/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16638", "id": 293383074, "node_id": "MDU6SXNzdWUyOTMzODMwNzQ=", "number": 16638, "title": "Feature request: use padded_batch with tf.estimator.export.build_parsing_serving_input_receiver_fn", "user": {"login": "alltom", "id": 1678, "node_id": "MDQ6VXNlcjE2Nzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alltom", "html_url": "https://github.com/alltom", "followers_url": "https://api.github.com/users/alltom/followers", "following_url": "https://api.github.com/users/alltom/following{/other_user}", "gists_url": "https://api.github.com/users/alltom/gists{/gist_id}", "starred_url": "https://api.github.com/users/alltom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alltom/subscriptions", "organizations_url": "https://api.github.com/users/alltom/orgs", "repos_url": "https://api.github.com/users/alltom/repos", "events_url": "https://api.github.com/users/alltom/events{/privacy}", "received_events_url": "https://api.github.com/users/alltom/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-02-01T02:33:34Z", "updated_at": "2018-02-19T21:16:19Z", "closed_at": "2018-02-16T01:26:46Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS 10.12.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: I forget</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 2.7.8</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>As far as I can tell, tf.estimator.export.build_parsing_serving_input_receiver_fn() doesn't allow you to control the way examples are batched. So if I have an Estimator that I've trained by using Dataset.padded_batch() in the input_fn, there doesn't seem to be a way for me to use that model with TensorFlow Serving.</p>\n<h3>Source code / logs</h3>\n<p>N/A?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12.6\nTensorFlow installed from (source or binary): I forget\nTensorFlow version (use command below): 1.4\nPython version: 2.7.8\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nAs far as I can tell, tf.estimator.export.build_parsing_serving_input_receiver_fn() doesn't allow you to control the way examples are batched. So if I have an Estimator that I've trained by using Dataset.padded_batch() in the input_fn, there doesn't seem to be a way for me to use that model with TensorFlow Serving.\nSource code / logs\nN/A?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: I forget\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.8\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nAs far as I can tell, tf.estimator.export.build_parsing_serving_input_receiver_fn() doesn't allow you to control the way examples are batched. So if I have an Estimator that I've trained by using Dataset.padded_batch() in the input_fn, there doesn't seem to be a way for me to use that model with TensorFlow Serving.\r\n\r\n### Source code / logs\r\nN/A?"}