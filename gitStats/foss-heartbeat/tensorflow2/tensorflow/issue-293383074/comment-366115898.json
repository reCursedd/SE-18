{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366115898", "html_url": "https://github.com/tensorflow/tensorflow/issues/16638#issuecomment-366115898", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16638", "id": 366115898, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjExNTg5OA==", "user": {"login": "alltom", "id": 1678, "node_id": "MDQ6VXNlcjE2Nzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alltom", "html_url": "https://github.com/alltom", "followers_url": "https://api.github.com/users/alltom/followers", "following_url": "https://api.github.com/users/alltom/following{/other_user}", "gists_url": "https://api.github.com/users/alltom/gists{/gist_id}", "starred_url": "https://api.github.com/users/alltom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alltom/subscriptions", "organizations_url": "https://api.github.com/users/alltom/orgs", "repos_url": "https://api.github.com/users/alltom/repos", "events_url": "https://api.github.com/users/alltom/events{/privacy}", "received_events_url": "https://api.github.com/users/alltom/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-16T01:09:48Z", "updated_at": "2018-02-16T01:10:35Z", "author_association": "NONE", "body_html": "<p>Whenever there's pre-processing work to be done like embedding variable-length features, is that usually done by pre-processing the data <em>before</em> sending a predict request to TensorFlow Serving / Cloud ML Engine?</p>\n<p>I filed this issue with the expectation that there should be some way to write a graph whose input is a batch of variable-length tf.strings (for example), but maybe that's not how it's meant to be done?</p>\n<p>Also, I'm not sure what tensorflower is, but is it coming soon? :)</p>", "body_text": "Whenever there's pre-processing work to be done like embedding variable-length features, is that usually done by pre-processing the data before sending a predict request to TensorFlow Serving / Cloud ML Engine?\nI filed this issue with the expectation that there should be some way to write a graph whose input is a batch of variable-length tf.strings (for example), but maybe that's not how it's meant to be done?\nAlso, I'm not sure what tensorflower is, but is it coming soon? :)", "body": "Whenever there's pre-processing work to be done like embedding variable-length features, is that usually done by pre-processing the data _before_ sending a predict request to TensorFlow Serving / Cloud ML Engine?\r\n\r\nI filed this issue with the expectation that there should be some way to write a graph whose input is a batch of variable-length tf.strings (for example), but maybe that's not how it's meant to be done?\r\n\r\nAlso, I'm not sure what tensorflower is, but is it coming soon? :)"}