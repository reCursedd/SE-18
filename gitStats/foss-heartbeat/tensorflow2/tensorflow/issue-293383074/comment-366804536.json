{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366804536", "html_url": "https://github.com/tensorflow/tensorflow/issues/16638#issuecomment-366804536", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16638", "id": 366804536, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjgwNDUzNg==", "user": {"login": "alltom", "id": 1678, "node_id": "MDQ6VXNlcjE2Nzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alltom", "html_url": "https://github.com/alltom", "followers_url": "https://api.github.com/users/alltom/followers", "following_url": "https://api.github.com/users/alltom/following{/other_user}", "gists_url": "https://api.github.com/users/alltom/gists{/gist_id}", "starred_url": "https://api.github.com/users/alltom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alltom/subscriptions", "organizations_url": "https://api.github.com/users/alltom/orgs", "repos_url": "https://api.github.com/users/alltom/repos", "events_url": "https://api.github.com/users/alltom/events{/privacy}", "received_events_url": "https://api.github.com/users/alltom/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-19T21:16:18Z", "updated_at": "2018-02-19T21:16:18Z", "author_association": "NONE", "body_html": "<p>I accidentally came across more information that I'm including here for posterity:</p>\n<p>tensorflow_model_server accepts a <a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/session_bundle_config.proto#L67\">BatchingParameters</a> proto with a <code>bool pad_variable_length_inputs</code> parameter that might emulate Dataset.padded_batch() well enough. And if not, there's a <code>Int64Value max_batch_size</code> parameter that might disable batching if set to 1.</p>\n<p><a href=\"https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators\" rel=\"nofollow\">Using SavedModel with Estimators</a> has more information about when/how to use build_parsing_serving_input_receiver_fn or write your own ServingInputReceiver.</p>", "body_text": "I accidentally came across more information that I'm including here for posterity:\ntensorflow_model_server accepts a BatchingParameters proto with a bool pad_variable_length_inputs parameter that might emulate Dataset.padded_batch() well enough. And if not, there's a Int64Value max_batch_size parameter that might disable batching if set to 1.\nUsing SavedModel with Estimators has more information about when/how to use build_parsing_serving_input_receiver_fn or write your own ServingInputReceiver.", "body": "I accidentally came across more information that I'm including here for posterity:\r\n\r\ntensorflow_model_server accepts a [BatchingParameters](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/session_bundle_config.proto#L67) proto with a `bool pad_variable_length_inputs` parameter that might emulate Dataset.padded_batch() well enough. And if not, there's a `Int64Value max_batch_size` parameter that might disable batching if set to 1.\r\n\r\n[Using SavedModel with Estimators](https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators) has more information about when/how to use build_parsing_serving_input_receiver_fn or write your own ServingInputReceiver."}