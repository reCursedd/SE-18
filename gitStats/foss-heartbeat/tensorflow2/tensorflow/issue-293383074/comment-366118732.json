{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366118732", "html_url": "https://github.com/tensorflow/tensorflow/issues/16638#issuecomment-366118732", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16638", "id": 366118732, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjExODczMg==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-16T01:26:46Z", "updated_at": "2018-02-16T01:26:46Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=414043\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sukritiramesh\">@sukritiramesh</a> FYI</p>\n<p>Serving will accept requests with any batch size and return results according to what the graph returns. Often, you will feed single examples, but there's no restriction on this.</p>\n<p><code>build_parsing_serving_input_receiver_fn</code> however is a utility for the most common case, and it does not give you control over batching. You'll have to write your own function returning a ServingInputReceiver to do that.</p>", "body_text": "@sukritiramesh FYI\nServing will accept requests with any batch size and return results according to what the graph returns. Often, you will feed single examples, but there's no restriction on this.\nbuild_parsing_serving_input_receiver_fn however is a utility for the most common case, and it does not give you control over batching. You'll have to write your own function returning a ServingInputReceiver to do that.", "body": "@sukritiramesh FYI\r\n\r\nServing will accept requests with any batch size and return results according to what the graph returns. Often, you will feed single examples, but there's no restriction on this.\r\n\r\n`build_parsing_serving_input_receiver_fn` however is a utility for the most common case, and it does not give you control over batching. You'll have to write your own function returning a ServingInputReceiver to do that."}