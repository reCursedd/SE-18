{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15868", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15868/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15868/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15868/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15868", "id": 286165668, "node_id": "MDU6SXNzdWUyODYxNjU2Njg=", "number": 15868, "title": "S3 Checkpointing fails with large graphs", "user": {"login": "winstonaws", "id": 31297115, "node_id": "MDQ6VXNlcjMxMjk3MTE1", "avatar_url": "https://avatars2.githubusercontent.com/u/31297115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/winstonaws", "html_url": "https://github.com/winstonaws", "followers_url": "https://api.github.com/users/winstonaws/followers", "following_url": "https://api.github.com/users/winstonaws/following{/other_user}", "gists_url": "https://api.github.com/users/winstonaws/gists{/gist_id}", "starred_url": "https://api.github.com/users/winstonaws/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/winstonaws/subscriptions", "organizations_url": "https://api.github.com/users/winstonaws/orgs", "repos_url": "https://api.github.com/users/winstonaws/repos", "events_url": "https://api.github.com/users/winstonaws/events{/privacy}", "received_events_url": "https://api.github.com/users/winstonaws/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-05T01:33:56Z", "updated_at": "2018-03-09T01:47:01Z", "closed_at": "2018-03-09T01:47:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: tensorflow/tensorflow:1.4.0 container running on Amazon Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Used tensorflow/tensorflow:1.4.0 image</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.4.0-rc1-11-g130a514', '1.4.0')</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<ol>\n<li>Exported AWS credentials to environment variables</li>\n<li>docker run -it --rm -p 8888:8888 -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN tensorflow/tensorflow:1.4.0</li>\n<li>Opened Jupyter UI in browser at localhost:8888</li>\n<li>Pasted code below in a new notebook, modified checkpoint_s3_dir variable to point to my S3 bucket, and ran it:</li>\n</ol>\n<pre><code># Replace with a bucket that you have write access to\ncheckpoint_s3_dir = 's3://&lt;your_bucket&gt;/checkpoint_testing'\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras.layers import Dense, LSTM\nfrom tensorflow.python.estimator.model_fn import ModeKeys\nfrom tensorflow.contrib.learn import RunConfig\n\n# I don't believe the specifics of how my Estimator is configured are important;\n# this is a toy example similar to the code where we encountered the problem.\n# By varying the embedding dimension used, we can easily generate a graph\n# large enough to trigger this problem.\n\nWINDOW_SIZE = 7\nBATCH_SIZE = 128\nMAX_VOCAB_SIZE = 100000\nHIDDEN_DIM = 512\nNUM_CLASSES = 2\nNUM_PARTITIONS = 10\n\ndef partitioned_embeddings(embedding_dim):    \n    # Randomly generate embedding\n    emb = np.random.rand(MAX_VOCAB_SIZE, embedding_dim).astype(np.float32)\n    partitioned_embeddings = []\n    for i in range(NUM_PARTITIONS):\n        partitioned_embeddings.append(tf.constant(emb[i::NUM_PARTITIONS]))\n\n    return partitioned_embeddings\n\ndef model_fn(features, labels, mode, params):\n    emb_parts = partitioned_embeddings(params['embedding_dim'])\n    embedded_vectors = tf.nn.embedding_lookup(params=emb_parts, ids=features['inputs'],\n                                          name='embedding_lookup', partition_strategy='mod')\n    l = LSTM(HIDDEN_DIM)(embedded_vectors)\n    layers = Dense(NUM_CLASSES, activation='sigmoid')(l)\n\n    global_step = tf.train.get_or_create_global_step()\n    \n    optimizer = tf.train.AdamOptimizer()\n    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=layers)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n    \n    return tf.estimator.EstimatorSpec(ModeKeys.TRAIN, loss=loss, train_op=train_op)\n\ndef train(embedding_dim, model_dir=None):\n    # Generate random training data\n    word_ids = np.random.randint(0, high=MAX_VOCAB_SIZE, size=(BATCH_SIZE * 10, WINDOW_SIZE),\n                             dtype=np.int32)\n    # Generate random labels\n    labels = np.random.randint(0, high=NUM_CLASSES, size=(BATCH_SIZE * 10, NUM_CLASSES), dtype=np.int32)\n    \n    input_fn = tf.estimator.inputs.numpy_input_fn(x={'inputs': word_ids}, y=labels, batch_size=BATCH_SIZE,\n                                              shuffle=False)\n    \n    estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},\n                                       config=RunConfig(model_dir=model_dir))\n    estimator.train(input_fn=input_fn)\n\n# Embedding dimension = 5 succeeds\ntrain(5, model_dir=checkpoint_s3_dir + '/dim_5')\n# Embedding dimension = 500 fails during checkpointing to S3\ntrain(500, model_dir=checkpoint_s3_dir + '/dim_500')\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I believe this is a TensorFlow Bug.</p>\n<p>My example code, which trains using an Estimator and checkpoints to an S3 bucket I own, succeeds when the graph it produces is small, but fails during checkpointing when I increase the embedding_dimension variable. Everything else should be the same, so I suspect file sizes are the issue.</p>\n<p>I've dug into this a bit and my guess is that when the graph size gets too large, the S3 operations time out during checkpointing due to the default client-side timeout on the AWS C++ S3 SDK. That issue is described here: <a href=\"https://stackoverflow.com/questions/38647444/aws-c-s3-sdk-putobjectrequest-unable-to-connect-to-endpoint\" rel=\"nofollow\">https://stackoverflow.com/questions/38647444/aws-c-s3-sdk-putobjectrequest-unable-to-connect-to-endpoint</a></p>\n<p>The S3 file system currently uses the default timeout: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/b3d5ec90bc6b7ae7822ea82d82b41e529c5e047a/tensorflow/core/platform/s3/s3_file_system.cc#L513\">tensorflow/tensorflow/core/platform/s3/s3_file_system.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 513\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/b3d5ec90bc6b7ae7822ea82d82b41e529c5e047a\">b3d5ec9</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L513\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"513\"></td>\n          <td id=\"LC513\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> Aws::S3::S3Client <span class=\"pl-smi\">s3Client</span>(<span class=\"pl-c1\">GetDefaultClientConfig</span>()); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>Which is 3 seconds:<br>\n<a href=\"https://sdk.amazonaws.com/cpp/api/0.12.9/df/d19/struct_aws_1_1_client_1_1_client_configuration.html#a68c35ac8d14619e4bfc77d848fd89473\" rel=\"nofollow\">https://sdk.amazonaws.com/cpp/api/0.12.9/df/d19/struct_aws_1_1_client_1_1_client_configuration.html#a68c35ac8d14619e4bfc77d848fd89473</a></p>\n<h3>Source code / logs</h3>\n<p>Example code above.</p>\n<p>Full output is below:</p>\n<pre><code>INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_5', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ad8212510&gt;, '_tf_config': gpu_options {\n  per_process_gpu_memory_fraction: 1.0\n}\n, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.\nINFO:tensorflow:loss = 0.733852, step = 1\nINFO:tensorflow:Saving checkpoints for 10 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.\nINFO:tensorflow:Loss for final step: 0.693191.\nINFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_500', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ac1672690&gt;, '_tf_config': gpu_options {\n  per_process_gpu_memory_fraction: 1.0\n}\n, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\nINFO:tensorflow:Create CheckpointSaverHook.\n---------------------------------------------------------------------------\nInternalError                             Traceback (most recent call last)\n&lt;ipython-input-1-1df0ebd892ce&gt; in &lt;module&gt;()\n     61 train(5, model_dir=checkpoint_s3_dir + '/dim_5')\n     62 # Embedding dimension = 500 fails during checkpointing to S3\n---&gt; 63 train(500, model_dir=checkpoint_s3_dir + '/dim_500')\n\n&lt;ipython-input-1-1df0ebd892ce&gt; in train(embedding_dim, model_dir)\n     56     estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},\n     57                                        config=RunConfig(model_dir=model_dir))\n---&gt; 58     estimator.train(input_fn=input_fn)\n     59 \n     60 # Embedding dimension = 5 succeeds\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    300 \n    301     saving_listeners = _check_listeners_type(saving_listeners)\n--&gt; 302     loss = self._train_model(input_fn, hooks, saving_listeners)\n    303     logging.info('Loss for final step: %s.', loss)\n    304     return self\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\n    781         loss = None\n    782         while not mon_sess.should_stop():\n--&gt; 783           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n    784       return loss\n    785 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    519                           feed_dict=feed_dict,\n    520                           options=options,\n--&gt; 521                           run_metadata=run_metadata)\n    522 \n    523   def should_stop(self):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    890                               feed_dict=feed_dict,\n    891                               options=options,\n--&gt; 892                               run_metadata=run_metadata)\n    893       except _PREEMPTION_ERRORS as e:\n    894         logging.info('An error was raised. This may be due to a preemption in '\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)\n    965         raise six.reraise(*original_exc_info)\n    966       else:\n--&gt; 967         raise six.reraise(*original_exc_info)\n    968 \n    969 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)\n    950   def run(self, *args, **kwargs):\n    951     try:\n--&gt; 952       return self._sess.run(*args, **kwargs)\n    953     except _PREEMPTION_ERRORS:\n    954       raise\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n   1014     options = options or config_pb2.RunOptions()\n   1015     feed_dict = self._call_hook_before_run(run_context, actual_fetches,\n-&gt; 1016                                            feed_dict, options)\n   1017 \n   1018     # Do session run.\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in _call_hook_before_run(self, run_context, fetch_dict, user_feed_dict, options)\n   1040     hook_feeds = {}\n   1041     for hook in self._hooks:\n-&gt; 1042       request = hook.before_run(run_context)\n   1043       if request is not None:\n   1044         if request.fetches is not None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.pyc in before_run(self, run_context)\n    432           ops.get_default_graph().as_graph_def(add_shapes=True),\n    433           self._checkpoint_dir,\n--&gt; 434           \"graph.pbtxt\")\n    435       saver_def = self._get_saver().saver_def if self._get_saver() else None\n    436       graph = ops.get_default_graph()\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_io.pyc in write_graph(graph_or_graph_def, logdir, name, as_text)\n     67   if as_text:\n     68     file_io.atomic_write_string_to_file(path,\n---&gt; 69                                         text_format.MessageToString(graph_def))\n     70   else:\n     71     file_io.atomic_write_string_to_file(path, graph_def.SerializeToString())\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in atomic_write_string_to_file(filename, contents, overwrite)\n    421   write_string_to_file(temp_pathname, contents)\n    422   try:\n--&gt; 423     rename(temp_pathname, filename, overwrite)\n    424   except errors.OpError:\n    425     delete_file(temp_pathname)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in rename(oldname, newname, overwrite)\n    400   with errors.raise_exception_on_not_ok_status() as status:\n    401     pywrap_tensorflow.RenameFile(\n--&gt; 402         compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\n    403 \n    404 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc in __exit__(self, type_arg, value_arg, traceback_arg)\n    471             None, None,\n    472             compat.as_text(c_api.TF_Message(self.status.status)),\n--&gt; 473             c_api.TF_GetCode(self.status.status))\n    474     # Delete the underlying status object from memory otherwise it stays alive\n    475     # as there is a reference to status from this from the traceback due to\n\nInternalError: : Unable to connect to endpoint\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:1.4.0 container running on Amazon Linux\nTensorFlow installed from (source or binary): Used tensorflow/tensorflow:1.4.0 image\nTensorFlow version (use command below): ('v1.4.0-rc1-11-g130a514', '1.4.0')\nPython version: 2.7.12\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:\n\n\nExported AWS credentials to environment variables\ndocker run -it --rm -p 8888:8888 -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN tensorflow/tensorflow:1.4.0\nOpened Jupyter UI in browser at localhost:8888\nPasted code below in a new notebook, modified checkpoint_s3_dir variable to point to my S3 bucket, and ran it:\n\n# Replace with a bucket that you have write access to\ncheckpoint_s3_dir = 's3://<your_bucket>/checkpoint_testing'\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras.layers import Dense, LSTM\nfrom tensorflow.python.estimator.model_fn import ModeKeys\nfrom tensorflow.contrib.learn import RunConfig\n\n# I don't believe the specifics of how my Estimator is configured are important;\n# this is a toy example similar to the code where we encountered the problem.\n# By varying the embedding dimension used, we can easily generate a graph\n# large enough to trigger this problem.\n\nWINDOW_SIZE = 7\nBATCH_SIZE = 128\nMAX_VOCAB_SIZE = 100000\nHIDDEN_DIM = 512\nNUM_CLASSES = 2\nNUM_PARTITIONS = 10\n\ndef partitioned_embeddings(embedding_dim):    \n    # Randomly generate embedding\n    emb = np.random.rand(MAX_VOCAB_SIZE, embedding_dim).astype(np.float32)\n    partitioned_embeddings = []\n    for i in range(NUM_PARTITIONS):\n        partitioned_embeddings.append(tf.constant(emb[i::NUM_PARTITIONS]))\n\n    return partitioned_embeddings\n\ndef model_fn(features, labels, mode, params):\n    emb_parts = partitioned_embeddings(params['embedding_dim'])\n    embedded_vectors = tf.nn.embedding_lookup(params=emb_parts, ids=features['inputs'],\n                                          name='embedding_lookup', partition_strategy='mod')\n    l = LSTM(HIDDEN_DIM)(embedded_vectors)\n    layers = Dense(NUM_CLASSES, activation='sigmoid')(l)\n\n    global_step = tf.train.get_or_create_global_step()\n    \n    optimizer = tf.train.AdamOptimizer()\n    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=layers)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n    \n    return tf.estimator.EstimatorSpec(ModeKeys.TRAIN, loss=loss, train_op=train_op)\n\ndef train(embedding_dim, model_dir=None):\n    # Generate random training data\n    word_ids = np.random.randint(0, high=MAX_VOCAB_SIZE, size=(BATCH_SIZE * 10, WINDOW_SIZE),\n                             dtype=np.int32)\n    # Generate random labels\n    labels = np.random.randint(0, high=NUM_CLASSES, size=(BATCH_SIZE * 10, NUM_CLASSES), dtype=np.int32)\n    \n    input_fn = tf.estimator.inputs.numpy_input_fn(x={'inputs': word_ids}, y=labels, batch_size=BATCH_SIZE,\n                                              shuffle=False)\n    \n    estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},\n                                       config=RunConfig(model_dir=model_dir))\n    estimator.train(input_fn=input_fn)\n\n# Embedding dimension = 5 succeeds\ntrain(5, model_dir=checkpoint_s3_dir + '/dim_5')\n# Embedding dimension = 500 fails during checkpointing to S3\ntrain(500, model_dir=checkpoint_s3_dir + '/dim_500')\n\nDescribe the problem\nI believe this is a TensorFlow Bug.\nMy example code, which trains using an Estimator and checkpoints to an S3 bucket I own, succeeds when the graph it produces is small, but fails during checkpointing when I increase the embedding_dimension variable. Everything else should be the same, so I suspect file sizes are the issue.\nI've dug into this a bit and my guess is that when the graph size gets too large, the S3 operations time out during checkpointing due to the default client-side timeout on the AWS C++ S3 SDK. That issue is described here: https://stackoverflow.com/questions/38647444/aws-c-s3-sdk-putobjectrequest-unable-to-connect-to-endpoint\nThe S3 file system currently uses the default timeout: \n  \n    \n      tensorflow/tensorflow/core/platform/s3/s3_file_system.cc\n    \n    \n         Line 513\n      in\n      b3d5ec9\n    \n    \n    \n    \n\n        \n          \n           Aws::S3::S3Client s3Client(GetDefaultClientConfig()); \n        \n    \n  \n\n\nWhich is 3 seconds:\nhttps://sdk.amazonaws.com/cpp/api/0.12.9/df/d19/struct_aws_1_1_client_1_1_client_configuration.html#a68c35ac8d14619e4bfc77d848fd89473\nSource code / logs\nExample code above.\nFull output is below:\nINFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_5', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ad8212510>, '_tf_config': gpu_options {\n  per_process_gpu_memory_fraction: 1.0\n}\n, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.\nINFO:tensorflow:loss = 0.733852, step = 1\nINFO:tensorflow:Saving checkpoints for 10 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.\nINFO:tensorflow:Loss for final step: 0.693191.\nINFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_500', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ac1672690>, '_tf_config': gpu_options {\n  per_process_gpu_memory_fraction: 1.0\n}\n, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\nINFO:tensorflow:Create CheckpointSaverHook.\n---------------------------------------------------------------------------\nInternalError                             Traceback (most recent call last)\n<ipython-input-1-1df0ebd892ce> in <module>()\n     61 train(5, model_dir=checkpoint_s3_dir + '/dim_5')\n     62 # Embedding dimension = 500 fails during checkpointing to S3\n---> 63 train(500, model_dir=checkpoint_s3_dir + '/dim_500')\n\n<ipython-input-1-1df0ebd892ce> in train(embedding_dim, model_dir)\n     56     estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},\n     57                                        config=RunConfig(model_dir=model_dir))\n---> 58     estimator.train(input_fn=input_fn)\n     59 \n     60 # Embedding dimension = 5 succeeds\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    300 \n    301     saving_listeners = _check_listeners_type(saving_listeners)\n--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)\n    303     logging.info('Loss for final step: %s.', loss)\n    304     return self\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\n    781         loss = None\n    782         while not mon_sess.should_stop():\n--> 783           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n    784       return loss\n    785 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    519                           feed_dict=feed_dict,\n    520                           options=options,\n--> 521                           run_metadata=run_metadata)\n    522 \n    523   def should_stop(self):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    890                               feed_dict=feed_dict,\n    891                               options=options,\n--> 892                               run_metadata=run_metadata)\n    893       except _PREEMPTION_ERRORS as e:\n    894         logging.info('An error was raised. This may be due to a preemption in '\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)\n    965         raise six.reraise(*original_exc_info)\n    966       else:\n--> 967         raise six.reraise(*original_exc_info)\n    968 \n    969 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)\n    950   def run(self, *args, **kwargs):\n    951     try:\n--> 952       return self._sess.run(*args, **kwargs)\n    953     except _PREEMPTION_ERRORS:\n    954       raise\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n   1014     options = options or config_pb2.RunOptions()\n   1015     feed_dict = self._call_hook_before_run(run_context, actual_fetches,\n-> 1016                                            feed_dict, options)\n   1017 \n   1018     # Do session run.\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in _call_hook_before_run(self, run_context, fetch_dict, user_feed_dict, options)\n   1040     hook_feeds = {}\n   1041     for hook in self._hooks:\n-> 1042       request = hook.before_run(run_context)\n   1043       if request is not None:\n   1044         if request.fetches is not None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.pyc in before_run(self, run_context)\n    432           ops.get_default_graph().as_graph_def(add_shapes=True),\n    433           self._checkpoint_dir,\n--> 434           \"graph.pbtxt\")\n    435       saver_def = self._get_saver().saver_def if self._get_saver() else None\n    436       graph = ops.get_default_graph()\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_io.pyc in write_graph(graph_or_graph_def, logdir, name, as_text)\n     67   if as_text:\n     68     file_io.atomic_write_string_to_file(path,\n---> 69                                         text_format.MessageToString(graph_def))\n     70   else:\n     71     file_io.atomic_write_string_to_file(path, graph_def.SerializeToString())\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in atomic_write_string_to_file(filename, contents, overwrite)\n    421   write_string_to_file(temp_pathname, contents)\n    422   try:\n--> 423     rename(temp_pathname, filename, overwrite)\n    424   except errors.OpError:\n    425     delete_file(temp_pathname)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in rename(oldname, newname, overwrite)\n    400   with errors.raise_exception_on_not_ok_status() as status:\n    401     pywrap_tensorflow.RenameFile(\n--> 402         compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\n    403 \n    404 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc in __exit__(self, type_arg, value_arg, traceback_arg)\n    471             None, None,\n    472             compat.as_text(c_api.TF_Message(self.status.status)),\n--> 473             c_api.TF_GetCode(self.status.status))\n    474     # Delete the underlying status object from memory otherwise it stays alive\n    475     # as there is a reference to status from this from the traceback due to\n\nInternalError: : Unable to connect to endpoint", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: tensorflow/tensorflow:1.4.0 container running on Amazon Linux\r\n- **TensorFlow installed from (source or binary)**: Used tensorflow/tensorflow:1.4.0 image\r\n- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n1. Exported AWS credentials to environment variables\r\n2. docker run -it --rm -p 8888:8888 -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN tensorflow/tensorflow:1.4.0\r\n3. Opened Jupyter UI in browser at localhost:8888\r\n4. Pasted code below in a new notebook, modified checkpoint_s3_dir variable to point to my S3 bucket, and ran it:\r\n\r\n```\r\n# Replace with a bucket that you have write access to\r\ncheckpoint_s3_dir = 's3://<your_bucket>/checkpoint_testing'\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Dense, LSTM\r\nfrom tensorflow.python.estimator.model_fn import ModeKeys\r\nfrom tensorflow.contrib.learn import RunConfig\r\n\r\n# I don't believe the specifics of how my Estimator is configured are important;\r\n# this is a toy example similar to the code where we encountered the problem.\r\n# By varying the embedding dimension used, we can easily generate a graph\r\n# large enough to trigger this problem.\r\n\r\nWINDOW_SIZE = 7\r\nBATCH_SIZE = 128\r\nMAX_VOCAB_SIZE = 100000\r\nHIDDEN_DIM = 512\r\nNUM_CLASSES = 2\r\nNUM_PARTITIONS = 10\r\n\r\ndef partitioned_embeddings(embedding_dim):    \r\n    # Randomly generate embedding\r\n    emb = np.random.rand(MAX_VOCAB_SIZE, embedding_dim).astype(np.float32)\r\n    partitioned_embeddings = []\r\n    for i in range(NUM_PARTITIONS):\r\n        partitioned_embeddings.append(tf.constant(emb[i::NUM_PARTITIONS]))\r\n\r\n    return partitioned_embeddings\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    emb_parts = partitioned_embeddings(params['embedding_dim'])\r\n    embedded_vectors = tf.nn.embedding_lookup(params=emb_parts, ids=features['inputs'],\r\n                                          name='embedding_lookup', partition_strategy='mod')\r\n    l = LSTM(HIDDEN_DIM)(embedded_vectors)\r\n    layers = Dense(NUM_CLASSES, activation='sigmoid')(l)\r\n\r\n    global_step = tf.train.get_or_create_global_step()\r\n    \r\n    optimizer = tf.train.AdamOptimizer()\r\n    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=layers)\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n    \r\n    return tf.estimator.EstimatorSpec(ModeKeys.TRAIN, loss=loss, train_op=train_op)\r\n\r\ndef train(embedding_dim, model_dir=None):\r\n    # Generate random training data\r\n    word_ids = np.random.randint(0, high=MAX_VOCAB_SIZE, size=(BATCH_SIZE * 10, WINDOW_SIZE),\r\n                             dtype=np.int32)\r\n    # Generate random labels\r\n    labels = np.random.randint(0, high=NUM_CLASSES, size=(BATCH_SIZE * 10, NUM_CLASSES), dtype=np.int32)\r\n    \r\n    input_fn = tf.estimator.inputs.numpy_input_fn(x={'inputs': word_ids}, y=labels, batch_size=BATCH_SIZE,\r\n                                              shuffle=False)\r\n    \r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},\r\n                                       config=RunConfig(model_dir=model_dir))\r\n    estimator.train(input_fn=input_fn)\r\n\r\n# Embedding dimension = 5 succeeds\r\ntrain(5, model_dir=checkpoint_s3_dir + '/dim_5')\r\n# Embedding dimension = 500 fails during checkpointing to S3\r\ntrain(500, model_dir=checkpoint_s3_dir + '/dim_500')\r\n```\r\n\r\n\r\n### Describe the problem\r\nI believe this is a TensorFlow Bug.\r\n\r\nMy example code, which trains using an Estimator and checkpoints to an S3 bucket I own, succeeds when the graph it produces is small, but fails during checkpointing when I increase the embedding_dimension variable. Everything else should be the same, so I suspect file sizes are the issue.\r\n\r\nI've dug into this a bit and my guess is that when the graph size gets too large, the S3 operations time out during checkpointing due to the default client-side timeout on the AWS C++ S3 SDK. That issue is described here: https://stackoverflow.com/questions/38647444/aws-c-s3-sdk-putobjectrequest-unable-to-connect-to-endpoint\r\n\r\nThe S3 file system currently uses the default timeout: https://github.com/tensorflow/tensorflow/blob/b3d5ec90bc6b7ae7822ea82d82b41e529c5e047a/tensorflow/core/platform/s3/s3_file_system.cc#L513\r\n\r\nWhich is 3 seconds:\r\nhttps://sdk.amazonaws.com/cpp/api/0.12.9/df/d19/struct_aws_1_1_client_1_1_client_configuration.html#a68c35ac8d14619e4bfc77d848fd89473\r\n\r\n### Source code / logs\r\nExample code above.\r\n\r\nFull output is below:\r\n```\r\nINFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_5', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ad8212510>, '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1.0\r\n}\r\n, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.\r\nINFO:tensorflow:loss = 0.733852, step = 1\r\nINFO:tensorflow:Saving checkpoints for 10 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.693191.\r\nINFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_500', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ac1672690>, '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1.0\r\n}\r\n, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-1-1df0ebd892ce> in <module>()\r\n     61 train(5, model_dir=checkpoint_s3_dir + '/dim_5')\r\n     62 # Embedding dimension = 500 fails during checkpointing to S3\r\n---> 63 train(500, model_dir=checkpoint_s3_dir + '/dim_500')\r\n\r\n<ipython-input-1-1df0ebd892ce> in train(embedding_dim, model_dir)\r\n     56     estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},\r\n     57                                        config=RunConfig(model_dir=model_dir))\r\n---> 58     estimator.train(input_fn=input_fn)\r\n     59 \r\n     60 # Embedding dimension = 5 succeeds\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    300 \r\n    301     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    303     logging.info('Loss for final step: %s.', loss)\r\n    304     return self\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\r\n    781         loss = None\r\n    782         while not mon_sess.should_stop():\r\n--> 783           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n    784       return loss\r\n    785 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    519                           feed_dict=feed_dict,\r\n    520                           options=options,\r\n--> 521                           run_metadata=run_metadata)\r\n    522 \r\n    523   def should_stop(self):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    890                               feed_dict=feed_dict,\r\n    891                               options=options,\r\n--> 892                               run_metadata=run_metadata)\r\n    893       except _PREEMPTION_ERRORS as e:\r\n    894         logging.info('An error was raised. This may be due to a preemption in '\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)\r\n    965         raise six.reraise(*original_exc_info)\r\n    966       else:\r\n--> 967         raise six.reraise(*original_exc_info)\r\n    968 \r\n    969 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)\r\n    950   def run(self, *args, **kwargs):\r\n    951     try:\r\n--> 952       return self._sess.run(*args, **kwargs)\r\n    953     except _PREEMPTION_ERRORS:\r\n    954       raise\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n   1014     options = options or config_pb2.RunOptions()\r\n   1015     feed_dict = self._call_hook_before_run(run_context, actual_fetches,\r\n-> 1016                                            feed_dict, options)\r\n   1017 \r\n   1018     # Do session run.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in _call_hook_before_run(self, run_context, fetch_dict, user_feed_dict, options)\r\n   1040     hook_feeds = {}\r\n   1041     for hook in self._hooks:\r\n-> 1042       request = hook.before_run(run_context)\r\n   1043       if request is not None:\r\n   1044         if request.fetches is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.pyc in before_run(self, run_context)\r\n    432           ops.get_default_graph().as_graph_def(add_shapes=True),\r\n    433           self._checkpoint_dir,\r\n--> 434           \"graph.pbtxt\")\r\n    435       saver_def = self._get_saver().saver_def if self._get_saver() else None\r\n    436       graph = ops.get_default_graph()\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_io.pyc in write_graph(graph_or_graph_def, logdir, name, as_text)\r\n     67   if as_text:\r\n     68     file_io.atomic_write_string_to_file(path,\r\n---> 69                                         text_format.MessageToString(graph_def))\r\n     70   else:\r\n     71     file_io.atomic_write_string_to_file(path, graph_def.SerializeToString())\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in atomic_write_string_to_file(filename, contents, overwrite)\r\n    421   write_string_to_file(temp_pathname, contents)\r\n    422   try:\r\n--> 423     rename(temp_pathname, filename, overwrite)\r\n    424   except errors.OpError:\r\n    425     delete_file(temp_pathname)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in rename(oldname, newname, overwrite)\r\n    400   with errors.raise_exception_on_not_ok_status() as status:\r\n    401     pywrap_tensorflow.RenameFile(\r\n--> 402         compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\r\n    403 \r\n    404 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    471             None, None,\r\n    472             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 473             c_api.TF_GetCode(self.status.status))\r\n    474     # Delete the underlying status object from memory otherwise it stays alive\r\n    475     # as there is a reference to status from this from the traceback due to\r\n\r\nInternalError: : Unable to connect to endpoint\r\n```\r\n"}