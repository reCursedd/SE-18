{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3320", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3320/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3320/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3320/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3320", "id": 165652374, "node_id": "MDU6SXNzdWUxNjU2NTIzNzQ=", "number": 3320, "title": "CPU vs GPU Performance", "user": {"login": "Mazecreator", "id": 18412448, "node_id": "MDQ6VXNlcjE4NDEyNDQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/18412448?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mazecreator", "html_url": "https://github.com/Mazecreator", "followers_url": "https://api.github.com/users/Mazecreator/followers", "following_url": "https://api.github.com/users/Mazecreator/following{/other_user}", "gists_url": "https://api.github.com/users/Mazecreator/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mazecreator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mazecreator/subscriptions", "organizations_url": "https://api.github.com/users/Mazecreator/orgs", "repos_url": "https://api.github.com/users/Mazecreator/repos", "events_url": "https://api.github.com/users/Mazecreator/events{/privacy}", "received_events_url": "https://api.github.com/users/Mazecreator/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-07-14T20:34:28Z", "updated_at": "2016-07-18T02:15:25Z", "closed_at": "2016-07-17T13:53:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am working on a reinforcement learning model problem.  I have been working to get the model creation running faster and bumped into a strange issue I cannot explain.  It runs much faster ~50% or so on the CPU vs the GPU.  This was unexpected and I have disabled the GPU using <strong>\"export CUDA_VISIBLE_DEVICES=-1\"</strong> so the learning runs faster.  I have been looking at upgrading my GTX 950, but not sure it makes sense if I don't get a speed improvement.</p>\n<p>I ran a profile based upon <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"146958443\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1824\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1824/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1824\">#1824</a> and got the following trace files for a single \".run()\" iteration. I am not sure how to read this, but the GPU iteration took over 10ms where the CPU alone is &lt;10ms. I am running the HEAD of TensorFlow (reports 0.9.0) on Ubuntu 15.10 with CUDA 7.5 and cuDNN 4 &amp; 5 (tried both). The CPU is a dual XEON 6 core, 2.66 GHz  processors (24 threads total) with 72 GB or RAM (DDR3).</p>\n<p>I have a GTX 950 GPU.  I can't tell if this performance difference is related to the structure of the graph or simply the data set isn't big enough to get a benefit from the GPU given the IO overhead?  I have tested the GPU with TF on a basic \"matmut()\" of a 7000x7000 matrix and it beats the CPU hands down by orders of magnitude.  So I known it is installed correctly.</p>\n<p>Then networks runs in this case a Batch of 200 x 189 into 5 layers with Dropout() between each layer. The layers are 140, 120, 100, 80, and 3 as the output.  Any advice or things to try would be much appreciated.</p>\n<p><strong>CPU Timeline:</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/18412448/16854147/ad4c8760-49dd-11e6-8919-2c0940322b53.png\"><img src=\"https://cloud.githubusercontent.com/assets/18412448/16854147/ad4c8760-49dd-11e6-8919-2c0940322b53.png\" alt=\"cpu timeline\" style=\"max-width:100%;\"></a></p>\n<p><strong>GPU Timeline:</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/18412448/16854181/cfa25844-49dd-11e6-9f20-e2268bdd4299.png\"><img src=\"https://cloud.githubusercontent.com/assets/18412448/16854181/cfa25844-49dd-11e6-9f20-e2268bdd4299.png\" alt=\"gpu timeline\" style=\"max-width:100%;\"></a></p>\n<p>NOTE: This issue was posted to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"156034307\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2444\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2444/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2444\">#2444</a> but may have got lost.  It might be related to RL performance but feel it is a separate issue.</p>\n<p>I have enclosed the TIMELINE trace files for more detail.  If it is of value, I can create a \"tensorboard\" log file so you can review the model in detail.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/364706/GPU.Slowdown.zip\">GPU Slowdown.zip</a></p>", "body_text": "I am working on a reinforcement learning model problem.  I have been working to get the model creation running faster and bumped into a strange issue I cannot explain.  It runs much faster ~50% or so on the CPU vs the GPU.  This was unexpected and I have disabled the GPU using \"export CUDA_VISIBLE_DEVICES=-1\" so the learning runs faster.  I have been looking at upgrading my GTX 950, but not sure it makes sense if I don't get a speed improvement.\nI ran a profile based upon #1824 and got the following trace files for a single \".run()\" iteration. I am not sure how to read this, but the GPU iteration took over 10ms where the CPU alone is <10ms. I am running the HEAD of TensorFlow (reports 0.9.0) on Ubuntu 15.10 with CUDA 7.5 and cuDNN 4 & 5 (tried both). The CPU is a dual XEON 6 core, 2.66 GHz  processors (24 threads total) with 72 GB or RAM (DDR3).\nI have a GTX 950 GPU.  I can't tell if this performance difference is related to the structure of the graph or simply the data set isn't big enough to get a benefit from the GPU given the IO overhead?  I have tested the GPU with TF on a basic \"matmut()\" of a 7000x7000 matrix and it beats the CPU hands down by orders of magnitude.  So I known it is installed correctly.\nThen networks runs in this case a Batch of 200 x 189 into 5 layers with Dropout() between each layer. The layers are 140, 120, 100, 80, and 3 as the output.  Any advice or things to try would be much appreciated.\nCPU Timeline:\n\nGPU Timeline:\n\nNOTE: This issue was posted to #2444 but may have got lost.  It might be related to RL performance but feel it is a separate issue.\nI have enclosed the TIMELINE trace files for more detail.  If it is of value, I can create a \"tensorboard\" log file so you can review the model in detail.\nGPU Slowdown.zip", "body": "I am working on a reinforcement learning model problem.  I have been working to get the model creation running faster and bumped into a strange issue I cannot explain.  It runs much faster ~50% or so on the CPU vs the GPU.  This was unexpected and I have disabled the GPU using **\"export CUDA_VISIBLE_DEVICES=-1\"** so the learning runs faster.  I have been looking at upgrading my GTX 950, but not sure it makes sense if I don't get a speed improvement.\n\nI ran a profile based upon #1824 and got the following trace files for a single \".run()\" iteration. I am not sure how to read this, but the GPU iteration took over 10ms where the CPU alone is <10ms. I am running the HEAD of TensorFlow (reports 0.9.0) on Ubuntu 15.10 with CUDA 7.5 and cuDNN 4 & 5 (tried both). The CPU is a dual XEON 6 core, 2.66 GHz  processors (24 threads total) with 72 GB or RAM (DDR3).\n\nI have a GTX 950 GPU.  I can't tell if this performance difference is related to the structure of the graph or simply the data set isn't big enough to get a benefit from the GPU given the IO overhead?  I have tested the GPU with TF on a basic \"matmut()\" of a 7000x7000 matrix and it beats the CPU hands down by orders of magnitude.  So I known it is installed correctly.\n\nThen networks runs in this case a Batch of 200 x 189 into 5 layers with Dropout() between each layer. The layers are 140, 120, 100, 80, and 3 as the output.  Any advice or things to try would be much appreciated.\n\n**CPU Timeline:**\n![cpu timeline](https://cloud.githubusercontent.com/assets/18412448/16854147/ad4c8760-49dd-11e6-8919-2c0940322b53.png)\n\n**GPU Timeline:**\n![gpu timeline](https://cloud.githubusercontent.com/assets/18412448/16854181/cfa25844-49dd-11e6-9f20-e2268bdd4299.png)\n\nNOTE: This issue was posted to #2444 but may have got lost.  It might be related to RL performance but feel it is a separate issue.\n\nI have enclosed the TIMELINE trace files for more detail.  If it is of value, I can create a \"tensorboard\" log file so you can review the model in detail.\n[GPU Slowdown.zip](https://github.com/tensorflow/tensorflow/files/364706/GPU.Slowdown.zip)\n"}