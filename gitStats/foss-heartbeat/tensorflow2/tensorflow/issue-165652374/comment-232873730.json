{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/232873730", "html_url": "https://github.com/tensorflow/tensorflow/issues/3320#issuecomment-232873730", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3320", "id": 232873730, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMjg3MzczMA==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-15T06:41:49Z", "updated_at": "2016-07-15T06:41:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A few observations:</p>\n<ol>\n<li>GTX 950 is definitely a low-end card for deep learning. It is roughly 1/4 of a Titan-X, which is a good card.</li>\n<li>You have a lot of small GPU kernels, which medium and large gaps in-between them. So the total amount of GPU compute time is probably much smaller than 10ms, but you GPU has a lot of idle time.</li>\n</ol>\n<p>A few possible things to try:</p>\n<p>A. Each of your kernel is too small, merge them into larger kernels. Use larger batch, and bigger shapes, so each op has more computation to it. For example, 7000 x 7000 matmul is something that is very good for GPU.<br>\nB. Reduce the overhead of each op. Make sure your have as little memory transfer between devices as possible. Make sure you don't have other CPU intensive computation that holds the kernel launch. Also it is possible that you have too many threads launching GPU kernels that are thrashing each other. Play with inter_op_parallelism_threads to a smaller number.</p>\n<p>The best way to get more help is to upload a small repro case so more people can help.</p>\n<p>Have fun.</p>", "body_text": "A few observations:\n\nGTX 950 is definitely a low-end card for deep learning. It is roughly 1/4 of a Titan-X, which is a good card.\nYou have a lot of small GPU kernels, which medium and large gaps in-between them. So the total amount of GPU compute time is probably much smaller than 10ms, but you GPU has a lot of idle time.\n\nA few possible things to try:\nA. Each of your kernel is too small, merge them into larger kernels. Use larger batch, and bigger shapes, so each op has more computation to it. For example, 7000 x 7000 matmul is something that is very good for GPU.\nB. Reduce the overhead of each op. Make sure your have as little memory transfer between devices as possible. Make sure you don't have other CPU intensive computation that holds the kernel launch. Also it is possible that you have too many threads launching GPU kernels that are thrashing each other. Play with inter_op_parallelism_threads to a smaller number.\nThe best way to get more help is to upload a small repro case so more people can help.\nHave fun.", "body": "A few observations: \n1. GTX 950 is definitely a low-end card for deep learning. It is roughly 1/4 of a Titan-X, which is a good card. \n2. You have a lot of small GPU kernels, which medium and large gaps in-between them. So the total amount of GPU compute time is probably much smaller than 10ms, but you GPU has a lot of idle time. \n\nA few possible things to try: \n\nA. Each of your kernel is too small, merge them into larger kernels. Use larger batch, and bigger shapes, so each op has more computation to it. For example, 7000 x 7000 matmul is something that is very good for GPU. \nB. Reduce the overhead of each op. Make sure your have as little memory transfer between devices as possible. Make sure you don't have other CPU intensive computation that holds the kernel launch. Also it is possible that you have too many threads launching GPU kernels that are thrashing each other. Play with inter_op_parallelism_threads to a smaller number. \n\nThe best way to get more help is to upload a small repro case so more people can help. \n\nHave fun. \n"}