{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23918", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23918/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23918/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23918/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23918", "id": 383415895, "node_id": "MDU6SXNzdWUzODM0MTU4OTU=", "number": 23918, "title": "tf.dynamic_partition may cause NaN loss when use it with multi gpus and it performs normally with single gpu", "user": {"login": "mzhaoshuai", "id": 22476764, "node_id": "MDQ6VXNlcjIyNDc2NzY0", "avatar_url": "https://avatars1.githubusercontent.com/u/22476764?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mzhaoshuai", "html_url": "https://github.com/mzhaoshuai", "followers_url": "https://api.github.com/users/mzhaoshuai/followers", "following_url": "https://api.github.com/users/mzhaoshuai/following{/other_user}", "gists_url": "https://api.github.com/users/mzhaoshuai/gists{/gist_id}", "starred_url": "https://api.github.com/users/mzhaoshuai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mzhaoshuai/subscriptions", "organizations_url": "https://api.github.com/users/mzhaoshuai/orgs", "repos_url": "https://api.github.com/users/mzhaoshuai/repos", "events_url": "https://api.github.com/users/mzhaoshuai/events{/privacy}", "received_events_url": "https://api.github.com/users/mzhaoshuai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-22T07:49:42Z", "updated_at": "2018-11-22T07:49:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): <strong><em>no</em></strong></li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): <em><strong>Ubuntu 16.04.5 LTS</strong></em></li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:<strong><em>no</em></strong></li>\n<li>TensorFlow installed from (source or binary):<strong><em>binary</em></strong></li>\n<li>TensorFlow version (use command below):<strong><em>1.12.0</em></strong></li>\n<li>Python version: <strong><em>Python 3.5.2</em></strong></li>\n<li>Bazel version (if compiling from source): <em><strong>None</strong></em></li>\n<li>GCC/Compiler version (if compiling from source):  <strong><em>5.4.0</em></strong></li>\n<li>CUDA/cuDNN version: <strong><em>cuda-9.0.176/cudnn-7.2.1</em></strong></li>\n<li>GPU model and memory: <strong><em>2 same GTX Titan X (Pascal), 12GB</em></strong></li>\n</ul>\n<p>I just use the offical docker image of the tensorflow, the tag is <code>1.12.0-devel-gpu-py3</code></p>\n<p><strong>Describe the problem</strong></p>\n<p><strong>The api <code>tf.dynamic_partition</code> may cause NaN loss when use it with multi gpus and it is normal with single gpu.</strong></p>\n<p>The code to reproduce the problem:</p>\n<pre><code>#coding=utf-8\n\nimport os\nimport time\nimport tensorflow as tf\n\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nnum_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))\n\ndef dp_and_bm_api_test(argv=None):\n\ta = tf.constant([[1, 2, 3],\n\t\t\t\t\t[4, 5, 6],\n\t\t\t\t\t[7, 8, 9],\n\t\t\t\t\t[10, 11, 12]])\n\t\n\tmask = tf.constant([1, 0, 1, 0])\n\tres_01 = tf.boolean_mask(a, mask)\n\tres_02 = tf.dynamic_partition(a, mask, num_partitions=2)[1]\n\n\twith tf.Session() as sess:\n\t\tr_1, r_2 = sess.run([res_01, res_02])\n\t\tprint(\"r_1\\n\", r_1)\n\t\tprint(\"r_2\\n\", r_2)\n\n\ndef tower_model_fn(features, labels, api_sel=0):\n\t\"\"\"a test tower fn\"\"\"\n\tnum_classes = 10\n\tnet = tf.layers.conv2d(features, num_classes * 2, [3,3], padding=\"same\")\n\tnet = tf.layers.batch_normalization(net)\n\tnet = tf.nn.relu(net)\n\tnet = tf.layers.conv2d(net, num_classes, [3,3], padding=\"same\")\n\tnet = tf.layers.batch_normalization(net)\n\tlogits = tf.nn.relu(net)\n\n\tlabels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.\n\tlogits_by_num_classes = tf.reshape(logits, [-1, num_classes])\n\tlabels_flat = tf.reshape(labels, [-1, ])\n\tvalid_indices = tf.to_int32(labels_flat &lt;= num_classes - 1)\n\n\tif api_sel == 0:\n\t\tvalid_logits = tf.boolean_mask(logits_by_num_classes, valid_indices)\n\t\tvalid_labels = tf.boolean_mask(labels_flat, valid_indices)\n\telse:\n\t\tvalid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\n\t\tvalid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\n\n\n\tcross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=valid_logits, labels=valid_labels, \n\t\t\t\t\t\t\tloss_collection=None)\n\ttf.add_to_collection(tf.GraphKeys.LOSSES, cross_entropy)\n\treturn None\n\ndef train(api_sel=0):\n\t## variable strategy\n\tvariable_strategy = 'CPU'\n\tinput_device = '/cpu:0'\n\tvar_device = '/gpu:0'\n\tnum_devices = num_gpus\n\tdevice_type = 'gpu'\n\t\n\twith tf.Graph().as_default() as graph:\n\t\twith tf.device(var_device):\n\t\t\t## some collector\n\t\t\ttower_ce_loss = []\n\n\t\t\tglobal_step = tf.train.get_or_create_global_step()\t\n\n\t\t\tname_scopes = ['tower_%d' % i for i in range(num_devices)]\n\t\t\tfor i in range(num_devices):\n\t\t\t\twith tf.variable_scope(tf.get_variable_scope(), reuse=bool(i &gt; 0)):\n\t\t\t\t\tworker_device = '/{0}:{1}'.format(device_type, i)\n\t\t\t\t\twith tf.name_scope(name_scopes[i]) as name_scope:\n\t\t\t\t\t\twith tf.device(worker_device):\t\n\n\t\t\t\t\t\t\timages = tf.ones([8, 321, 321, 3])\n\t\t\t\t\t\t\tlabels = tf.zeros([8, 321, 321, 1], dtype=tf.int32)\n\t\t\t\t\t\t\ttower_model_fn(images, labels, api_sel=api_sel)\n\t\t\t\t\t\t\tce_now = tf.get_collection(tf.GraphKeys.LOSSES, scope=name_scope)\n\t\t\t\t\t\t\ttower_ce_loss.append(tf.add_n(ce_now))\n\n\t\t\n\t\twith tf.device(var_device):\n\t\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scopes[0])\t\n\n\t\t\tce_loss = tf.add_n(tower_ce_loss)\n\t\t\tsession_config = tf.ConfigProto(allow_soft_placement=True,\n\t\t\t\t\t\t\t\t\t\t\tlog_device_placement=False\n\t\t\t\t\t\t\t\t\t\t\t)\n\t\t\t# Build an initialization operation to run below.\n\t\t\tinit_op = tf.global_variables_initializer()\n\t\t\tmax_steps = 30\n\t\t\tstep_gap_init_time = 0.0\n\t\t\twith tf.Session(config=session_config) as sess:\n\t\t\t\tsess.run(init_op)\n\t\t\t\tfor steps in range(1, max_steps + 1, 1):\n\t\t\t\t\tstep_gap_init_time = time.time()\n\t\t\t\t\tc_l = sess.run([ce_loss])\n\t\t\t\t\tif steps % 10 == 0:\n\t\t\t\t\t\tgap_time = (time.time() - step_gap_init_time) / 10\n\t\t\t\t\t\tprint(\"ce loss{0}, {1:1.4f}s per steps\".format(c_l, gap_time))\n\ttf.keras.backend.clear_session()\n\ttf.reset_default_graph()\n\nif __name__ == '__main__':\n\ttf.logging.set_verbosity(tf.logging.INFO)\n\tprint(\"\\nTest the two apis\")\n\tdp_and_bm_api_test()\n\tprint(\"The number of gpus is {0}\".format(num_gpus))\n\tprint(\"\\nUsing tf.boolean_mask\")\n\ttrain(api_sel=0)\n\tprint(\"\\nUsing tf.dynamic_partition\")\n\ttrain(api_sel=1)\n</code></pre>\n<p>When I use two gpus, I set <code>os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\"</code></p>\n<pre><code>2018-11-22 07:37:36.070340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\n2018-11-22 07:37:36.478438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-22 07:37:36.478486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1\n2018-11-22 07:37:36.478498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y\n2018-11-22 07:37:36.478511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N\n2018-11-22 07:37:36.478930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\n2018-11-22 07:37:36.479432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11421 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:18:00.0, compute capability: 5.2)\nr_1\n [[1 2 3]\n [7 8 9]]\nr_2\n [[1 2 3]\n [7 8 9]]\nThe number of gpus is 2\n\nUsing tf.boolean_mask\n\nce loss[5.088441], 0.0010s per steps\nce loss[5.088441], 0.0010s per steps\nce loss[5.088441], 0.0010s per steps\n\n\nUsing tf.dynamic_partition\nce loss[nan], 0.0010s per steps\nce loss[nan], 0.0011s per steps\nce loss[nan], 0.0011s per steps\n</code></pre>\n<p>When I use one gpus, I set <code>os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"</code><br>\nThen I get</p>\n<pre><code>\nTest the two apis\n2018-11-22 07:41:32.112369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n2018-11-22 07:41:34.385976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\npciBusID: 0000:17:00.0\ntotalMemory: 11.92GiB freeMemory: 11.80GiB\n2018-11-22 07:41:34.386009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n2018-11-22 07:41:34.586971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-22 07:41:34.587016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-22 07:41:34.587024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-22 07:41:34.587249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\nr_1\n [[1 2 3]\n [7 8 9]]\nr_2\n [[1 2 3]\n [7 8 9]]\nThe number of gpus is 1\n\n\nUsing tf.boolean_mask\n\nce loss[2.3863728], 0.0009s per steps\nce loss[2.3863728], 0.0009s per steps\nce loss[2.3863728], 0.0009s per steps\n\n\n\nUsing tf.dynamic_partition\nce loss[2.1562214], 0.0010s per steps\nce loss[2.1562214], 0.0013s per steps\nce loss[2.1562214], 0.0010s per steps\n\n</code></pre>\n<p>I think the implementation of the  api <code>tf.dynamic_partition</code> is really terrible,<br>\nI also reported that this api may cuase memory leak under certain situation<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"362857966\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/22464\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/22464/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/22464\">#22464</a></p>\n<p>It also cost me lots of time to find that it is not suitable with mulit gpus.....<br>\nI think there may other potential issues about the  <code>tf.dynamic_partition</code>.<br>\nAnd I don't figure out why I still use it......</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):1.12.0\nPython version: Python 3.5.2\nBazel version (if compiling from source): None\nGCC/Compiler version (if compiling from source):  5.4.0\nCUDA/cuDNN version: cuda-9.0.176/cudnn-7.2.1\nGPU model and memory: 2 same GTX Titan X (Pascal), 12GB\n\nI just use the offical docker image of the tensorflow, the tag is 1.12.0-devel-gpu-py3\nDescribe the problem\nThe api tf.dynamic_partition may cause NaN loss when use it with multi gpus and it is normal with single gpu.\nThe code to reproduce the problem:\n#coding=utf-8\n\nimport os\nimport time\nimport tensorflow as tf\n\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nnum_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))\n\ndef dp_and_bm_api_test(argv=None):\n\ta = tf.constant([[1, 2, 3],\n\t\t\t\t\t[4, 5, 6],\n\t\t\t\t\t[7, 8, 9],\n\t\t\t\t\t[10, 11, 12]])\n\t\n\tmask = tf.constant([1, 0, 1, 0])\n\tres_01 = tf.boolean_mask(a, mask)\n\tres_02 = tf.dynamic_partition(a, mask, num_partitions=2)[1]\n\n\twith tf.Session() as sess:\n\t\tr_1, r_2 = sess.run([res_01, res_02])\n\t\tprint(\"r_1\\n\", r_1)\n\t\tprint(\"r_2\\n\", r_2)\n\n\ndef tower_model_fn(features, labels, api_sel=0):\n\t\"\"\"a test tower fn\"\"\"\n\tnum_classes = 10\n\tnet = tf.layers.conv2d(features, num_classes * 2, [3,3], padding=\"same\")\n\tnet = tf.layers.batch_normalization(net)\n\tnet = tf.nn.relu(net)\n\tnet = tf.layers.conv2d(net, num_classes, [3,3], padding=\"same\")\n\tnet = tf.layers.batch_normalization(net)\n\tlogits = tf.nn.relu(net)\n\n\tlabels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.\n\tlogits_by_num_classes = tf.reshape(logits, [-1, num_classes])\n\tlabels_flat = tf.reshape(labels, [-1, ])\n\tvalid_indices = tf.to_int32(labels_flat <= num_classes - 1)\n\n\tif api_sel == 0:\n\t\tvalid_logits = tf.boolean_mask(logits_by_num_classes, valid_indices)\n\t\tvalid_labels = tf.boolean_mask(labels_flat, valid_indices)\n\telse:\n\t\tvalid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\n\t\tvalid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\n\n\n\tcross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=valid_logits, labels=valid_labels, \n\t\t\t\t\t\t\tloss_collection=None)\n\ttf.add_to_collection(tf.GraphKeys.LOSSES, cross_entropy)\n\treturn None\n\ndef train(api_sel=0):\n\t## variable strategy\n\tvariable_strategy = 'CPU'\n\tinput_device = '/cpu:0'\n\tvar_device = '/gpu:0'\n\tnum_devices = num_gpus\n\tdevice_type = 'gpu'\n\t\n\twith tf.Graph().as_default() as graph:\n\t\twith tf.device(var_device):\n\t\t\t## some collector\n\t\t\ttower_ce_loss = []\n\n\t\t\tglobal_step = tf.train.get_or_create_global_step()\t\n\n\t\t\tname_scopes = ['tower_%d' % i for i in range(num_devices)]\n\t\t\tfor i in range(num_devices):\n\t\t\t\twith tf.variable_scope(tf.get_variable_scope(), reuse=bool(i > 0)):\n\t\t\t\t\tworker_device = '/{0}:{1}'.format(device_type, i)\n\t\t\t\t\twith tf.name_scope(name_scopes[i]) as name_scope:\n\t\t\t\t\t\twith tf.device(worker_device):\t\n\n\t\t\t\t\t\t\timages = tf.ones([8, 321, 321, 3])\n\t\t\t\t\t\t\tlabels = tf.zeros([8, 321, 321, 1], dtype=tf.int32)\n\t\t\t\t\t\t\ttower_model_fn(images, labels, api_sel=api_sel)\n\t\t\t\t\t\t\tce_now = tf.get_collection(tf.GraphKeys.LOSSES, scope=name_scope)\n\t\t\t\t\t\t\ttower_ce_loss.append(tf.add_n(ce_now))\n\n\t\t\n\t\twith tf.device(var_device):\n\t\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scopes[0])\t\n\n\t\t\tce_loss = tf.add_n(tower_ce_loss)\n\t\t\tsession_config = tf.ConfigProto(allow_soft_placement=True,\n\t\t\t\t\t\t\t\t\t\t\tlog_device_placement=False\n\t\t\t\t\t\t\t\t\t\t\t)\n\t\t\t# Build an initialization operation to run below.\n\t\t\tinit_op = tf.global_variables_initializer()\n\t\t\tmax_steps = 30\n\t\t\tstep_gap_init_time = 0.0\n\t\t\twith tf.Session(config=session_config) as sess:\n\t\t\t\tsess.run(init_op)\n\t\t\t\tfor steps in range(1, max_steps + 1, 1):\n\t\t\t\t\tstep_gap_init_time = time.time()\n\t\t\t\t\tc_l = sess.run([ce_loss])\n\t\t\t\t\tif steps % 10 == 0:\n\t\t\t\t\t\tgap_time = (time.time() - step_gap_init_time) / 10\n\t\t\t\t\t\tprint(\"ce loss{0}, {1:1.4f}s per steps\".format(c_l, gap_time))\n\ttf.keras.backend.clear_session()\n\ttf.reset_default_graph()\n\nif __name__ == '__main__':\n\ttf.logging.set_verbosity(tf.logging.INFO)\n\tprint(\"\\nTest the two apis\")\n\tdp_and_bm_api_test()\n\tprint(\"The number of gpus is {0}\".format(num_gpus))\n\tprint(\"\\nUsing tf.boolean_mask\")\n\ttrain(api_sel=0)\n\tprint(\"\\nUsing tf.dynamic_partition\")\n\ttrain(api_sel=1)\n\nWhen I use two gpus, I set os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\"\n2018-11-22 07:37:36.070340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\n2018-11-22 07:37:36.478438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-22 07:37:36.478486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1\n2018-11-22 07:37:36.478498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y\n2018-11-22 07:37:36.478511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N\n2018-11-22 07:37:36.478930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\n2018-11-22 07:37:36.479432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11421 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:18:00.0, compute capability: 5.2)\nr_1\n [[1 2 3]\n [7 8 9]]\nr_2\n [[1 2 3]\n [7 8 9]]\nThe number of gpus is 2\n\nUsing tf.boolean_mask\n\nce loss[5.088441], 0.0010s per steps\nce loss[5.088441], 0.0010s per steps\nce loss[5.088441], 0.0010s per steps\n\n\nUsing tf.dynamic_partition\nce loss[nan], 0.0010s per steps\nce loss[nan], 0.0011s per steps\nce loss[nan], 0.0011s per steps\n\nWhen I use one gpus, I set os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nThen I get\n\nTest the two apis\n2018-11-22 07:41:32.112369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n2018-11-22 07:41:34.385976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\npciBusID: 0000:17:00.0\ntotalMemory: 11.92GiB freeMemory: 11.80GiB\n2018-11-22 07:41:34.386009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n2018-11-22 07:41:34.586971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-22 07:41:34.587016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-22 07:41:34.587024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-22 07:41:34.587249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\nr_1\n [[1 2 3]\n [7 8 9]]\nr_2\n [[1 2 3]\n [7 8 9]]\nThe number of gpus is 1\n\n\nUsing tf.boolean_mask\n\nce loss[2.3863728], 0.0009s per steps\nce loss[2.3863728], 0.0009s per steps\nce loss[2.3863728], 0.0009s per steps\n\n\n\nUsing tf.dynamic_partition\nce loss[2.1562214], 0.0010s per steps\nce loss[2.1562214], 0.0013s per steps\nce loss[2.1562214], 0.0010s per steps\n\n\nI think the implementation of the  api tf.dynamic_partition is really terrible,\nI also reported that this api may cuase memory leak under certain situation\n#22464\nIt also cost me lots of time to find that it is not suitable with mulit gpus.....\nI think there may other potential issues about the  tf.dynamic_partition.\nAnd I don't figure out why I still use it......", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **_no_**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _**Ubuntu 16.04.5 LTS**_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**_no_**\r\n- TensorFlow installed from (source or binary):**_binary_**\r\n- TensorFlow version (use command below):**_1.12.0_**\r\n- Python version: **_Python 3.5.2_**\r\n- Bazel version (if compiling from source): _**None**_\r\n- GCC/Compiler version (if compiling from source):  **_5.4.0_**\r\n- CUDA/cuDNN version: **_cuda-9.0.176/cudnn-7.2.1_**\r\n- GPU model and memory: **_2 same GTX Titan X (Pascal), 12GB_** \r\n\r\n\r\nI just use the offical docker image of the tensorflow, the tag is `1.12.0-devel-gpu-py3`\r\n\r\n**Describe the problem**\r\n\r\n**The api `tf.dynamic_partition` may cause NaN loss when use it with multi gpus and it is normal with single gpu.**\r\n\r\nThe code to reproduce the problem:\r\n```\r\n#coding=utf-8\r\n\r\nimport os\r\nimport time\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\nnum_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))\r\n\r\ndef dp_and_bm_api_test(argv=None):\r\n\ta = tf.constant([[1, 2, 3],\r\n\t\t\t\t\t[4, 5, 6],\r\n\t\t\t\t\t[7, 8, 9],\r\n\t\t\t\t\t[10, 11, 12]])\r\n\t\r\n\tmask = tf.constant([1, 0, 1, 0])\r\n\tres_01 = tf.boolean_mask(a, mask)\r\n\tres_02 = tf.dynamic_partition(a, mask, num_partitions=2)[1]\r\n\r\n\twith tf.Session() as sess:\r\n\t\tr_1, r_2 = sess.run([res_01, res_02])\r\n\t\tprint(\"r_1\\n\", r_1)\r\n\t\tprint(\"r_2\\n\", r_2)\r\n\r\n\r\ndef tower_model_fn(features, labels, api_sel=0):\r\n\t\"\"\"a test tower fn\"\"\"\r\n\tnum_classes = 10\r\n\tnet = tf.layers.conv2d(features, num_classes * 2, [3,3], padding=\"same\")\r\n\tnet = tf.layers.batch_normalization(net)\r\n\tnet = tf.nn.relu(net)\r\n\tnet = tf.layers.conv2d(net, num_classes, [3,3], padding=\"same\")\r\n\tnet = tf.layers.batch_normalization(net)\r\n\tlogits = tf.nn.relu(net)\r\n\r\n\tlabels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.\r\n\tlogits_by_num_classes = tf.reshape(logits, [-1, num_classes])\r\n\tlabels_flat = tf.reshape(labels, [-1, ])\r\n\tvalid_indices = tf.to_int32(labels_flat <= num_classes - 1)\r\n\r\n\tif api_sel == 0:\r\n\t\tvalid_logits = tf.boolean_mask(logits_by_num_classes, valid_indices)\r\n\t\tvalid_labels = tf.boolean_mask(labels_flat, valid_indices)\r\n\telse:\r\n\t\tvalid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\r\n\t\tvalid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\r\n\r\n\r\n\tcross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=valid_logits, labels=valid_labels, \r\n\t\t\t\t\t\t\tloss_collection=None)\r\n\ttf.add_to_collection(tf.GraphKeys.LOSSES, cross_entropy)\r\n\treturn None\r\n\r\ndef train(api_sel=0):\r\n\t## variable strategy\r\n\tvariable_strategy = 'CPU'\r\n\tinput_device = '/cpu:0'\r\n\tvar_device = '/gpu:0'\r\n\tnum_devices = num_gpus\r\n\tdevice_type = 'gpu'\r\n\t\r\n\twith tf.Graph().as_default() as graph:\r\n\t\twith tf.device(var_device):\r\n\t\t\t## some collector\r\n\t\t\ttower_ce_loss = []\r\n\r\n\t\t\tglobal_step = tf.train.get_or_create_global_step()\t\r\n\r\n\t\t\tname_scopes = ['tower_%d' % i for i in range(num_devices)]\r\n\t\t\tfor i in range(num_devices):\r\n\t\t\t\twith tf.variable_scope(tf.get_variable_scope(), reuse=bool(i > 0)):\r\n\t\t\t\t\tworker_device = '/{0}:{1}'.format(device_type, i)\r\n\t\t\t\t\twith tf.name_scope(name_scopes[i]) as name_scope:\r\n\t\t\t\t\t\twith tf.device(worker_device):\t\r\n\r\n\t\t\t\t\t\t\timages = tf.ones([8, 321, 321, 3])\r\n\t\t\t\t\t\t\tlabels = tf.zeros([8, 321, 321, 1], dtype=tf.int32)\r\n\t\t\t\t\t\t\ttower_model_fn(images, labels, api_sel=api_sel)\r\n\t\t\t\t\t\t\tce_now = tf.get_collection(tf.GraphKeys.LOSSES, scope=name_scope)\r\n\t\t\t\t\t\t\ttower_ce_loss.append(tf.add_n(ce_now))\r\n\r\n\t\t\r\n\t\twith tf.device(var_device):\r\n\t\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scopes[0])\t\r\n\r\n\t\t\tce_loss = tf.add_n(tower_ce_loss)\r\n\t\t\tsession_config = tf.ConfigProto(allow_soft_placement=True,\r\n\t\t\t\t\t\t\t\t\t\t\tlog_device_placement=False\r\n\t\t\t\t\t\t\t\t\t\t\t)\r\n\t\t\t# Build an initialization operation to run below.\r\n\t\t\tinit_op = tf.global_variables_initializer()\r\n\t\t\tmax_steps = 30\r\n\t\t\tstep_gap_init_time = 0.0\r\n\t\t\twith tf.Session(config=session_config) as sess:\r\n\t\t\t\tsess.run(init_op)\r\n\t\t\t\tfor steps in range(1, max_steps + 1, 1):\r\n\t\t\t\t\tstep_gap_init_time = time.time()\r\n\t\t\t\t\tc_l = sess.run([ce_loss])\r\n\t\t\t\t\tif steps % 10 == 0:\r\n\t\t\t\t\t\tgap_time = (time.time() - step_gap_init_time) / 10\r\n\t\t\t\t\t\tprint(\"ce loss{0}, {1:1.4f}s per steps\".format(c_l, gap_time))\r\n\ttf.keras.backend.clear_session()\r\n\ttf.reset_default_graph()\r\n\r\nif __name__ == '__main__':\r\n\ttf.logging.set_verbosity(tf.logging.INFO)\r\n\tprint(\"\\nTest the two apis\")\r\n\tdp_and_bm_api_test()\r\n\tprint(\"The number of gpus is {0}\".format(num_gpus))\r\n\tprint(\"\\nUsing tf.boolean_mask\")\r\n\ttrain(api_sel=0)\r\n\tprint(\"\\nUsing tf.dynamic_partition\")\r\n\ttrain(api_sel=1)\r\n```\r\n\r\nWhen I use two gpus, I set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\"`\r\n\r\n\r\n```\r\n2018-11-22 07:37:36.070340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2018-11-22 07:37:36.478438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-22 07:37:36.478486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1\r\n2018-11-22 07:37:36.478498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y\r\n2018-11-22 07:37:36.478511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N\r\n2018-11-22 07:37:36.478930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\r\n2018-11-22 07:37:36.479432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11421 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:18:00.0, compute capability: 5.2)\r\nr_1\r\n [[1 2 3]\r\n [7 8 9]]\r\nr_2\r\n [[1 2 3]\r\n [7 8 9]]\r\nThe number of gpus is 2\r\n\r\nUsing tf.boolean_mask\r\n\r\nce loss[5.088441], 0.0010s per steps\r\nce loss[5.088441], 0.0010s per steps\r\nce loss[5.088441], 0.0010s per steps\r\n\r\n\r\nUsing tf.dynamic_partition\r\nce loss[nan], 0.0010s per steps\r\nce loss[nan], 0.0011s per steps\r\nce loss[nan], 0.0011s per steps\r\n```\r\n\r\n\r\nWhen I use one gpus, I set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"`\r\nThen I get\r\n```\r\n\r\nTest the two apis\r\n2018-11-22 07:41:32.112369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-11-22 07:41:34.385976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.80GiB\r\n2018-11-22 07:41:34.386009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-22 07:41:34.586971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-22 07:41:34.587016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-22 07:41:34.587024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-22 07:41:34.587249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\r\nr_1\r\n [[1 2 3]\r\n [7 8 9]]\r\nr_2\r\n [[1 2 3]\r\n [7 8 9]]\r\nThe number of gpus is 1\r\n\r\n\r\nUsing tf.boolean_mask\r\n\r\nce loss[2.3863728], 0.0009s per steps\r\nce loss[2.3863728], 0.0009s per steps\r\nce loss[2.3863728], 0.0009s per steps\r\n\r\n\r\n\r\nUsing tf.dynamic_partition\r\nce loss[2.1562214], 0.0010s per steps\r\nce loss[2.1562214], 0.0013s per steps\r\nce loss[2.1562214], 0.0010s per steps\r\n\r\n```\r\n\r\nI think the implementation of the  api `tf.dynamic_partition` is really terrible, \r\nI also reported that this api may cuase memory leak under certain situation\r\nhttps://github.com/tensorflow/tensorflow/issues/22464\r\n\r\nIt also cost me lots of time to find that it is not suitable with mulit gpus.....\r\nI think there may other potential issues about the  `tf.dynamic_partition`.\r\nAnd I don't figure out why I still use it......\r\n\r\n"}