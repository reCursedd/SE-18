{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/372638053", "html_url": "https://github.com/tensorflow/tensorflow/issues/12132#issuecomment-372638053", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12132", "id": 372638053, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjYzODA1Mw==", "user": {"login": "stengoes", "id": 17768094, "node_id": "MDQ6VXNlcjE3NzY4MDk0", "avatar_url": "https://avatars2.githubusercontent.com/u/17768094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stengoes", "html_url": "https://github.com/stengoes", "followers_url": "https://api.github.com/users/stengoes/followers", "following_url": "https://api.github.com/users/stengoes/following{/other_user}", "gists_url": "https://api.github.com/users/stengoes/gists{/gist_id}", "starred_url": "https://api.github.com/users/stengoes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stengoes/subscriptions", "organizations_url": "https://api.github.com/users/stengoes/orgs", "repos_url": "https://api.github.com/users/stengoes/repos", "events_url": "https://api.github.com/users/stengoes/events{/privacy}", "received_events_url": "https://api.github.com/users/stengoes/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-13T11:46:11Z", "updated_at": "2018-03-15T09:06:20Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define a scenario</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\nchannels <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nimage_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nfeature_maps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\nfilter_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">15</span>\ndepthwise_filters <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Dummy images</span>\nimages <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[batch_size, channels, image_size, image_size], \n                          <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Filter definitions</span>\nbasis_filters <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[filter_size, filter_size, channels, depthwise_filters], \n                                 <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\ncoeffs <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[channels, depthwise_filters, feature_maps], \n                          <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Normal method</span>\neffective_filters <span class=\"pl-k\">=</span> tf.einsum(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>hwcm,cmn-&gt;hwcn<span class=\"pl-pds\">'</span></span>, basis_filters, coeffs)\nnormal <span class=\"pl-k\">=</span> tf.nn.conv2d(images, \n                      effective_filters, \n                      <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], \n                      <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SAME<span class=\"pl-pds\">\"</span></span>, \n                      <span class=\"pl-v\">use_cudnn_on_gpu</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, \n                      <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NCHW<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Separable method</span>\ndepthwise <span class=\"pl-k\">=</span> tf.nn.depthwise_conv2d_native(images, \n                                          basis_filters, \n                                          <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], \n                                          <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SAME<span class=\"pl-pds\">\"</span></span>, \n                                          <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NCHW<span class=\"pl-pds\">\"</span></span>)\n\ncoeffs <span class=\"pl-k\">=</span> tf.reshape(coeffs, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, channels<span class=\"pl-k\">*</span>depthwise_filters, feature_maps])\nseparable <span class=\"pl-k\">=</span> tf.nn.conv2d(depthwise, \n                         coeffs, \n                         <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], \n                         <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>VALID<span class=\"pl-pds\">\"</span></span>, \n                         <span class=\"pl-v\">use_cudnn_on_gpu</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, \n                         <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NCHW<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert equality of the different methods</span>\n    norm, sep <span class=\"pl-k\">=</span> sess.run([normal, separable])\n    np.testing.assert_almost_equal(norm, sep, <span class=\"pl-v\">decimal</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n\n    repeats <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Benchmark normal method</span>\n    start <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(repeats):\n        _ <span class=\"pl-k\">=</span> sess.run(normal)\n    end <span class=\"pl-k\">=</span> time.time()\n    d1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> repeats <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Benchmark seperable method</span>\n    start <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(repeats):\n        _ <span class=\"pl-k\">=</span> sess.run(separable)\n    end <span class=\"pl-k\">=</span> time.time()\n    d2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> repeats <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Print results</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Normal method: <span class=\"pl-c1\">{}</span>ms <span class=\"pl-cce\">\\t</span> Separable method: <span class=\"pl-c1\">{}</span>ms<span class=\"pl-pds\">\"</span></span>.format(d1, d2))</pre></div>\n<p>I evaluated the code snippet once more. This time on a Nvidia Pascal Titan X:<br>\nThe results are listed below:</p>\n<pre><code>Normal method: 8ms \t Separable method: 116ms  # Tesla M60 with Tensorflow-v1.1.0\nNormal method: 7ms \t Separable method: 35ms   # Pascal Titan X with Tensorflow-v1.1.0\nNormal method: 5ms \t Separable method: 23ms   # Pascal Titan X with Tensorflow-v1.6.0\n</code></pre>\n<p>In tensorflow-v1.6.0 (on the the pascal titan X) the separable method is ~4-5x slower than the normal method. So one could argue that the relative performance of the separable conv has improved with the newer versions of tensorflow.</p>\n<p>I guess that the normal method is still faster because it only has to apply 1 CUDA kernel?! Whereas the separable convolution needs 2 kernels: a depthwise and a pointwise kernel?! Maybe this could be solved by fusing the depthwise and pointwise CUDA kernel? But I am no CUDA expert.</p>\n<p><strong>TLDR:</strong><br>\nThe current implementation of the seperable conv layer is still useless in my opinion. Because fusing the separated filters first, and then applying a normal conv2d is much faster. Therefore this is still an issue in my opinion.</p>", "body_text": "import tensorflow as tf\nimport numpy as np\nimport time\n\n# Define a scenario\nbatch_size = 64\nchannels = 32\nimage_size = 32\nfeature_maps = 64\nfilter_size = 15\ndepthwise_filters = 8\n\n# Dummy images\nimages = tf.random_normal(shape=[batch_size, channels, image_size, image_size], \n                          dtype=tf.float32)\n\n# Filter definitions\nbasis_filters = tf.random_normal(shape=[filter_size, filter_size, channels, depthwise_filters], \n                                 dtype=tf.float32)\ncoeffs = tf.random_normal(shape=[channels, depthwise_filters, feature_maps], \n                          dtype=tf.float32)\n\n# Normal method\neffective_filters = tf.einsum('hwcm,cmn->hwcn', basis_filters, coeffs)\nnormal = tf.nn.conv2d(images, \n                      effective_filters, \n                      strides=[1, 1, 1, 1], \n                      padding=\"SAME\", \n                      use_cudnn_on_gpu=True, \n                      data_format=\"NCHW\")\n\n# Separable method\ndepthwise = tf.nn.depthwise_conv2d_native(images, \n                                          basis_filters, \n                                          strides=[1, 1, 1, 1], \n                                          padding=\"SAME\", \n                                          data_format=\"NCHW\")\n\ncoeffs = tf.reshape(coeffs, [1, 1, channels*depthwise_filters, feature_maps])\nseparable = tf.nn.conv2d(depthwise, \n                         coeffs, \n                         strides=[1, 1, 1, 1], \n                         padding=\"VALID\", \n                         use_cudnn_on_gpu=True, \n                         data_format=\"NCHW\")\n\n\nwith tf.Session() as sess:\n\n    # Assert equality of the different methods\n    norm, sep = sess.run([normal, separable])\n    np.testing.assert_almost_equal(norm, sep, decimal=3)\n\n    repeats = 100\n\n    # Benchmark normal method\n    start = time.time()\n    for _ in xrange(repeats):\n        _ = sess.run(normal)\n    end = time.time()\n    d1 = int((end - start) / repeats * 1000)\n\n    # Benchmark seperable method\n    start = time.time()\n    for _ in xrange(repeats):\n        _ = sess.run(separable)\n    end = time.time()\n    d2 = int((end - start) / repeats * 1000)\n\n    # Print results\n    print(\"Normal method: {}ms \\t Separable method: {}ms\".format(d1, d2))\nI evaluated the code snippet once more. This time on a Nvidia Pascal Titan X:\nThe results are listed below:\nNormal method: 8ms \t Separable method: 116ms  # Tesla M60 with Tensorflow-v1.1.0\nNormal method: 7ms \t Separable method: 35ms   # Pascal Titan X with Tensorflow-v1.1.0\nNormal method: 5ms \t Separable method: 23ms   # Pascal Titan X with Tensorflow-v1.6.0\n\nIn tensorflow-v1.6.0 (on the the pascal titan X) the separable method is ~4-5x slower than the normal method. So one could argue that the relative performance of the separable conv has improved with the newer versions of tensorflow.\nI guess that the normal method is still faster because it only has to apply 1 CUDA kernel?! Whereas the separable convolution needs 2 kernels: a depthwise and a pointwise kernel?! Maybe this could be solved by fusing the depthwise and pointwise CUDA kernel? But I am no CUDA expert.\nTLDR:\nThe current implementation of the seperable conv layer is still useless in my opinion. Because fusing the separated filters first, and then applying a normal conv2d is much faster. Therefore this is still an issue in my opinion.", "body": "```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n# Define a scenario\r\nbatch_size = 64\r\nchannels = 32\r\nimage_size = 32\r\nfeature_maps = 64\r\nfilter_size = 15\r\ndepthwise_filters = 8\r\n\r\n# Dummy images\r\nimages = tf.random_normal(shape=[batch_size, channels, image_size, image_size], \r\n                          dtype=tf.float32)\r\n\r\n# Filter definitions\r\nbasis_filters = tf.random_normal(shape=[filter_size, filter_size, channels, depthwise_filters], \r\n                                 dtype=tf.float32)\r\ncoeffs = tf.random_normal(shape=[channels, depthwise_filters, feature_maps], \r\n                          dtype=tf.float32)\r\n\r\n# Normal method\r\neffective_filters = tf.einsum('hwcm,cmn->hwcn', basis_filters, coeffs)\r\nnormal = tf.nn.conv2d(images, \r\n                      effective_filters, \r\n                      strides=[1, 1, 1, 1], \r\n                      padding=\"SAME\", \r\n                      use_cudnn_on_gpu=True, \r\n                      data_format=\"NCHW\")\r\n\r\n# Separable method\r\ndepthwise = tf.nn.depthwise_conv2d_native(images, \r\n                                          basis_filters, \r\n                                          strides=[1, 1, 1, 1], \r\n                                          padding=\"SAME\", \r\n                                          data_format=\"NCHW\")\r\n\r\ncoeffs = tf.reshape(coeffs, [1, 1, channels*depthwise_filters, feature_maps])\r\nseparable = tf.nn.conv2d(depthwise, \r\n                         coeffs, \r\n                         strides=[1, 1, 1, 1], \r\n                         padding=\"VALID\", \r\n                         use_cudnn_on_gpu=True, \r\n                         data_format=\"NCHW\")\r\n\r\n\r\nwith tf.Session() as sess:\r\n\r\n    # Assert equality of the different methods\r\n    norm, sep = sess.run([normal, separable])\r\n    np.testing.assert_almost_equal(norm, sep, decimal=3)\r\n\r\n    repeats = 100\r\n\r\n    # Benchmark normal method\r\n    start = time.time()\r\n    for _ in xrange(repeats):\r\n        _ = sess.run(normal)\r\n    end = time.time()\r\n    d1 = int((end - start) / repeats * 1000)\r\n\r\n    # Benchmark seperable method\r\n    start = time.time()\r\n    for _ in xrange(repeats):\r\n        _ = sess.run(separable)\r\n    end = time.time()\r\n    d2 = int((end - start) / repeats * 1000)\r\n\r\n    # Print results\r\n    print(\"Normal method: {}ms \\t Separable method: {}ms\".format(d1, d2))\r\n```\r\n\r\nI evaluated the code snippet once more. This time on a Nvidia Pascal Titan X: \r\nThe results are listed below:\r\n```\r\nNormal method: 8ms \t Separable method: 116ms  # Tesla M60 with Tensorflow-v1.1.0\r\nNormal method: 7ms \t Separable method: 35ms   # Pascal Titan X with Tensorflow-v1.1.0\r\nNormal method: 5ms \t Separable method: 23ms   # Pascal Titan X with Tensorflow-v1.6.0\r\n```\r\nIn tensorflow-v1.6.0 (on the the pascal titan X) the separable method is ~4-5x slower than the normal method. So one could argue that the relative performance of the separable conv has improved with the newer versions of tensorflow.\r\n\r\nI guess that the normal method is still faster because it only has to apply 1 CUDA kernel?! Whereas the separable convolution needs 2 kernels: a depthwise and a pointwise kernel?! Maybe this could be solved by fusing the depthwise and pointwise CUDA kernel? But I am no CUDA expert.\r\n\r\n**TLDR:**\r\nThe current implementation of the seperable conv layer is still useless in my opinion. Because fusing the separated filters first, and then applying a normal conv2d is much faster. Therefore this is still an issue in my opinion."}