{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/183734702", "html_url": "https://github.com/tensorflow/tensorflow/issues/1070#issuecomment-183734702", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1070", "id": 183734702, "node_id": "MDEyOklzc3VlQ29tbWVudDE4MzczNDcwMg==", "user": {"login": "sotelo", "id": 1833660, "node_id": "MDQ6VXNlcjE4MzM2NjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1833660?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sotelo", "html_url": "https://github.com/sotelo", "followers_url": "https://api.github.com/users/sotelo/followers", "following_url": "https://api.github.com/users/sotelo/following{/other_user}", "gists_url": "https://api.github.com/users/sotelo/gists{/gist_id}", "starred_url": "https://api.github.com/users/sotelo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sotelo/subscriptions", "organizations_url": "https://api.github.com/users/sotelo/orgs", "repos_url": "https://api.github.com/users/sotelo/repos", "events_url": "https://api.github.com/users/sotelo/events{/privacy}", "received_events_url": "https://api.github.com/users/sotelo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-13T19:42:22Z", "updated_at": "2016-02-13T19:42:22Z", "author_association": "NONE", "body_html": "<p>Thanks for answering :)</p>\n<p>Dropout it's very similar to what I would like to do. The only problem is that I would like to apply it AFTER the rest of the computation graph is defined. As far as I understand, the current dropout function has to be applied before. This is what I am referring to when I say graph replacement. In theano, the mechanism to do this is use the theano.clone function with a replace dictionary as argument. The reason that I would like to do it like that, is that I want to use this kind of function in all the weights. So, for example, the weights that correspond to a LSTM that I don't have access to. Ideally, I would like to do something like this:</p>\n<p>vars = tf.variables.all_variables()<br>\nnew_vars = [tf.nn.dropout(var) for var in vars ]<br>\ntf.replace(vars, new_vars)</p>\n<p>For a less general example: How can I apply dropout to the weights of a lstm?</p>\n<p>I saw this discussion that is related, but I would like to know if there is a simpler way of doing things:<br>\n<a href=\"http://stackoverflow.com/questions/33748552/tensorflow-how-to-replace-a-node-in-a-calculation-graph\" rel=\"nofollow\">http://stackoverflow.com/questions/33748552/tensorflow-how-to-replace-a-node-in-a-calculation-graph</a></p>\n<p>Thanks!</p>", "body_text": "Thanks for answering :)\nDropout it's very similar to what I would like to do. The only problem is that I would like to apply it AFTER the rest of the computation graph is defined. As far as I understand, the current dropout function has to be applied before. This is what I am referring to when I say graph replacement. In theano, the mechanism to do this is use the theano.clone function with a replace dictionary as argument. The reason that I would like to do it like that, is that I want to use this kind of function in all the weights. So, for example, the weights that correspond to a LSTM that I don't have access to. Ideally, I would like to do something like this:\nvars = tf.variables.all_variables()\nnew_vars = [tf.nn.dropout(var) for var in vars ]\ntf.replace(vars, new_vars)\nFor a less general example: How can I apply dropout to the weights of a lstm?\nI saw this discussion that is related, but I would like to know if there is a simpler way of doing things:\nhttp://stackoverflow.com/questions/33748552/tensorflow-how-to-replace-a-node-in-a-calculation-graph\nThanks!", "body": "Thanks for answering :)\n\nDropout it's very similar to what I would like to do. The only problem is that I would like to apply it AFTER the rest of the computation graph is defined. As far as I understand, the current dropout function has to be applied before. This is what I am referring to when I say graph replacement. In theano, the mechanism to do this is use the theano.clone function with a replace dictionary as argument. The reason that I would like to do it like that, is that I want to use this kind of function in all the weights. So, for example, the weights that correspond to a LSTM that I don't have access to. Ideally, I would like to do something like this:\n\nvars = tf.variables.all_variables()\nnew_vars = [tf.nn.dropout(var) for var in vars ]\ntf.replace(vars, new_vars)\n\nFor a less general example: How can I apply dropout to the weights of a lstm?\n\nI saw this discussion that is related, but I would like to know if there is a simpler way of doing things:\nhttp://stackoverflow.com/questions/33748552/tensorflow-how-to-replace-a-node-in-a-calculation-graph\n\nThanks!\n"}