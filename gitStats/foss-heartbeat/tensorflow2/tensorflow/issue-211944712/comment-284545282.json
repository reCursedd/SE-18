{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284545282", "html_url": "https://github.com/tensorflow/tensorflow/issues/8100#issuecomment-284545282", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8100", "id": 284545282, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDU0NTI4Mg==", "user": {"login": "jpienaar", "id": 706766, "node_id": "MDQ6VXNlcjcwNjc2Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/706766?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpienaar", "html_url": "https://github.com/jpienaar", "followers_url": "https://api.github.com/users/jpienaar/followers", "following_url": "https://api.github.com/users/jpienaar/following{/other_user}", "gists_url": "https://api.github.com/users/jpienaar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpienaar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpienaar/subscriptions", "organizations_url": "https://api.github.com/users/jpienaar/orgs", "repos_url": "https://api.github.com/users/jpienaar/repos", "events_url": "https://api.github.com/users/jpienaar/events{/privacy}", "received_events_url": "https://api.github.com/users/jpienaar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-06T21:52:16Z", "updated_at": "2017-03-06T21:52:16Z", "author_association": "MEMBER", "body_html": "<p>Definitely would be good to try XLA with Polly. Forking and modifying the CPU/GPU compiler backend might be the easiest. The [CG]PuCompiler's Compile function takes as input the HloModule with all the HloComputations. From there you'd have access to all the graphs translated from TF to XLA and can perform the optimizations you wish. You could look at entire graphs or just operations within the graph (for example, look at IrEmitter). The requirement of the entire graph is more of an optimization (large input graph, better return on optimization) requirement than a correctness one (correct me if wrong).</p>\n<p>I think there are multiple parts/steps here: 1) being able to use Polly in a backend, 2) quantifying performance gains possible from using Polly vs the regular backend, 3) improving the compile time performance. As a note the CPU backend is not the most optimized, so for performance evaluations the GPU backend is a better target.</p>", "body_text": "Definitely would be good to try XLA with Polly. Forking and modifying the CPU/GPU compiler backend might be the easiest. The [CG]PuCompiler's Compile function takes as input the HloModule with all the HloComputations. From there you'd have access to all the graphs translated from TF to XLA and can perform the optimizations you wish. You could look at entire graphs or just operations within the graph (for example, look at IrEmitter). The requirement of the entire graph is more of an optimization (large input graph, better return on optimization) requirement than a correctness one (correct me if wrong).\nI think there are multiple parts/steps here: 1) being able to use Polly in a backend, 2) quantifying performance gains possible from using Polly vs the regular backend, 3) improving the compile time performance. As a note the CPU backend is not the most optimized, so for performance evaluations the GPU backend is a better target.", "body": "Definitely would be good to try XLA with Polly. Forking and modifying the CPU/GPU compiler backend might be the easiest. The [CG]PuCompiler's Compile function takes as input the HloModule with all the HloComputations. From there you'd have access to all the graphs translated from TF to XLA and can perform the optimizations you wish. You could look at entire graphs or just operations within the graph (for example, look at IrEmitter). The requirement of the entire graph is more of an optimization (large input graph, better return on optimization) requirement than a correctness one (correct me if wrong).\r\n\r\nI think there are multiple parts/steps here: 1) being able to use Polly in a backend, 2) quantifying performance gains possible from using Polly vs the regular backend, 3) improving the compile time performance. As a note the CPU backend is not the most optimized, so for performance evaluations the GPU backend is a better target."}