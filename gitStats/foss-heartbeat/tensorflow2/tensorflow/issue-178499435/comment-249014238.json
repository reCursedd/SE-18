{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/249014238", "html_url": "https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249014238", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4526", "id": 249014238, "node_id": "MDEyOklzc3VlQ29tbWVudDI0OTAxNDIzOA==", "user": {"login": "mkolod", "id": 476135, "node_id": "MDQ6VXNlcjQ3NjEzNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/476135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mkolod", "html_url": "https://github.com/mkolod", "followers_url": "https://api.github.com/users/mkolod/followers", "following_url": "https://api.github.com/users/mkolod/following{/other_user}", "gists_url": "https://api.github.com/users/mkolod/gists{/gist_id}", "starred_url": "https://api.github.com/users/mkolod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mkolod/subscriptions", "organizations_url": "https://api.github.com/users/mkolod/orgs", "repos_url": "https://api.github.com/users/mkolod/repos", "events_url": "https://api.github.com/users/mkolod/events{/privacy}", "received_events_url": "https://api.github.com/users/mkolod/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-22T20:10:53Z", "updated_at": "2016-09-22T20:11:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think the issues with H2D transfers are quite real. Here's an example from AlexNet where the TF queues are feeding the GPUs with data for every mini-batch. The lack of overlap is demonstrated here for 2 GPUs (see screenshots). Because AlexNet has a low compute-to-I/O ratio, the problem is more clearly visible than in say Inception v3 or ResNet, so even though AlexNet isn't interesting for academic or business application reasons anymore, it's interesting for infrastructural/engineering reasons. Note how for 2 GPUs, H2D transfers are causing the GPU to wait for &gt;2 seconds at each iteration. Hence, GPU compute efficiency drops to about 26%. This is on two GTX 1080s with batch sizes at 1024 per GPU. With smaller batch sizes, the problem is more pronounced. For a single GPU, the GPU compute utilization is significantly better (80%) but the kernel pipeline still blocks on H2D transfer, even though the transfer is taking place in a separate CUDA stream. I'd like to support <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3979096\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benbarsdell\">@benbarsdell</a>'s point here. This doesn't look like an optimized prefetch - the compute/memcpy overlap for H2D is rather hard to see.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/476135/18764266/7220b368-80c5-11e6-9d09-8dfcb65943c0.png\"><img src=\"https://cloud.githubusercontent.com/assets/476135/18764266/7220b368-80c5-11e6-9d09-8dfcb65943c0.png\" alt=\"alexnet_2_gpus\" style=\"max-width:100%;\"></a></p>", "body_text": "I think the issues with H2D transfers are quite real. Here's an example from AlexNet where the TF queues are feeding the GPUs with data for every mini-batch. The lack of overlap is demonstrated here for 2 GPUs (see screenshots). Because AlexNet has a low compute-to-I/O ratio, the problem is more clearly visible than in say Inception v3 or ResNet, so even though AlexNet isn't interesting for academic or business application reasons anymore, it's interesting for infrastructural/engineering reasons. Note how for 2 GPUs, H2D transfers are causing the GPU to wait for >2 seconds at each iteration. Hence, GPU compute efficiency drops to about 26%. This is on two GTX 1080s with batch sizes at 1024 per GPU. With smaller batch sizes, the problem is more pronounced. For a single GPU, the GPU compute utilization is significantly better (80%) but the kernel pipeline still blocks on H2D transfer, even though the transfer is taking place in a separate CUDA stream. I'd like to support @benbarsdell's point here. This doesn't look like an optimized prefetch - the compute/memcpy overlap for H2D is rather hard to see.", "body": "I think the issues with H2D transfers are quite real. Here's an example from AlexNet where the TF queues are feeding the GPUs with data for every mini-batch. The lack of overlap is demonstrated here for 2 GPUs (see screenshots). Because AlexNet has a low compute-to-I/O ratio, the problem is more clearly visible than in say Inception v3 or ResNet, so even though AlexNet isn't interesting for academic or business application reasons anymore, it's interesting for infrastructural/engineering reasons. Note how for 2 GPUs, H2D transfers are causing the GPU to wait for >2 seconds at each iteration. Hence, GPU compute efficiency drops to about 26%. This is on two GTX 1080s with batch sizes at 1024 per GPU. With smaller batch sizes, the problem is more pronounced. For a single GPU, the GPU compute utilization is significantly better (80%) but the kernel pipeline still blocks on H2D transfer, even though the transfer is taking place in a separate CUDA stream. I'd like to support @benbarsdell's point here. This doesn't look like an optimized prefetch - the compute/memcpy overlap for H2D is rather hard to see.\n\n![alexnet_2_gpus](https://cloud.githubusercontent.com/assets/476135/18764266/7220b368-80c5-11e6-9d09-8dfcb65943c0.png)\n"}