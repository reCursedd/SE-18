{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/249262561", "html_url": "https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249262561", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4526", "id": 249262561, "node_id": "MDEyOklzc3VlQ29tbWVudDI0OTI2MjU2MQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-23T18:06:27Z", "updated_at": "2016-09-23T18:06:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We had some discussions about this. It seems a good idea to have a separate GPU queue in this case. We still need a larger CPU queue to hide latency between Python and TF. But this introduces a separate transfer stage from CPU to each GPU. We expect the GPU queue on each device to be much smaller: one or two in queue capacity.</p>\n<p>The down-side is that this introduces more client side threads to drive the new data transfer. But there is a separate effort to migrate them into TF itself. So we will ignore that problem for now.</p>", "body_text": "We had some discussions about this. It seems a good idea to have a separate GPU queue in this case. We still need a larger CPU queue to hide latency between Python and TF. But this introduces a separate transfer stage from CPU to each GPU. We expect the GPU queue on each device to be much smaller: one or two in queue capacity.\nThe down-side is that this introduces more client side threads to drive the new data transfer. But there is a separate effort to migrate them into TF itself. So we will ignore that problem for now.", "body": "We had some discussions about this. It seems a good idea to have a separate GPU queue in this case. We still need a larger CPU queue to hide latency between Python and TF. But this introduces a separate transfer stage from CPU to each GPU. We expect the GPU queue on each device to be much smaller: one or two in queue capacity. \n\nThe down-side is that this introduces more client side threads to drive the new data transfer. But there is a separate effort to migrate them into TF itself. So we will ignore that problem for now. \n"}