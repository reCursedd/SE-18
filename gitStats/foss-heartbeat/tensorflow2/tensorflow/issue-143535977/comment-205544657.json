{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/205544657", "html_url": "https://github.com/tensorflow/tensorflow/issues/1642#issuecomment-205544657", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1642", "id": 205544657, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNTU0NDY1Nw==", "user": {"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-04T23:42:11Z", "updated_at": "2016-04-04T23:44:00Z", "author_association": "MEMBER", "body_html": "<p>Further testing using the standalone benchmark tool reveals that adding a delay between inference passes of 1 seconds gives an per-inference average of 427ms over 50 runs.</p>\n<p>This is compared to a 321ms result on an otherwise identical test with the inter-inference delay set to 0.</p>\n<p>I'd speculate that in the Android camera demo, the delay between receiving frames to process is significant enough that the processor scales down, and needs to ramp back up once inference starts again. A busy-loop in the inference thread between frames might help with that behavior.</p>", "body_text": "Further testing using the standalone benchmark tool reveals that adding a delay between inference passes of 1 seconds gives an per-inference average of 427ms over 50 runs.\nThis is compared to a 321ms result on an otherwise identical test with the inter-inference delay set to 0.\nI'd speculate that in the Android camera demo, the delay between receiving frames to process is significant enough that the processor scales down, and needs to ramp back up once inference starts again. A busy-loop in the inference thread between frames might help with that behavior.", "body": "Further testing using the standalone benchmark tool reveals that adding a delay between inference passes of 1 seconds gives an per-inference average of 427ms over 50 runs.\n\nThis is compared to a 321ms result on an otherwise identical test with the inter-inference delay set to 0.\n\nI'd speculate that in the Android camera demo, the delay between receiving frames to process is significant enough that the processor scales down, and needs to ramp back up once inference starts again. A busy-loop in the inference thread between frames might help with that behavior.\n"}