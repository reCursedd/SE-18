{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/424993535", "html_url": "https://github.com/tensorflow/tensorflow/issues/22535#issuecomment-424993535", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22535", "id": 424993535, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDk5MzUzNQ==", "user": {"login": "freedomtan", "id": 3395998, "node_id": "MDQ6VXNlcjMzOTU5OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/3395998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/freedomtan", "html_url": "https://github.com/freedomtan", "followers_url": "https://api.github.com/users/freedomtan/followers", "following_url": "https://api.github.com/users/freedomtan/following{/other_user}", "gists_url": "https://api.github.com/users/freedomtan/gists{/gist_id}", "starred_url": "https://api.github.com/users/freedomtan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/freedomtan/subscriptions", "organizations_url": "https://api.github.com/users/freedomtan/orgs", "repos_url": "https://api.github.com/users/freedomtan/repos", "events_url": "https://api.github.com/users/freedomtan/events{/privacy}", "received_events_url": "https://api.github.com/users/freedomtan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-27T07:50:44Z", "updated_at": "2018-09-27T07:54:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11333107\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/minizon\">@minizon</a> sorry about confusion. What I meant to say is that for post-training quantization, the type of input tensors remains floating point. For real quantized model, the type of input tensors is expected to be quantized uint8 (type 3). To put it simply, a post-training model should be treated as a floating point model rather than a quantized one. That's why it doesn't work in the demo app (which, if not modified, expects a quantized one, see tflitecamera source <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java#L334\">here</a></p>", "body_text": "@minizon sorry about confusion. What I meant to say is that for post-training quantization, the type of input tensors remains floating point. For real quantized model, the type of input tensors is expected to be quantized uint8 (type 3). To put it simply, a post-training model should be treated as a floating point model rather than a quantized one. That's why it doesn't work in the demo app (which, if not modified, expects a quantized one, see tflitecamera source here", "body": "@minizon sorry about confusion. What I meant to say is that for post-training quantization, the type of input tensors remains floating point. For real quantized model, the type of input tensors is expected to be quantized uint8 (type 3). To put it simply, a post-training model should be treated as a floating point model rather than a quantized one. That's why it doesn't work in the demo app (which, if not modified, expects a quantized one, see tflitecamera source [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java#L334)\r\n"}