{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/425351409", "html_url": "https://github.com/tensorflow/tensorflow/issues/22535#issuecomment-425351409", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22535", "id": 425351409, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTM1MTQwOQ==", "user": {"login": "minizon", "id": 11333107, "node_id": "MDQ6VXNlcjExMzMzMTA3", "avatar_url": "https://avatars2.githubusercontent.com/u/11333107?v=4", "gravatar_id": "", "url": "https://api.github.com/users/minizon", "html_url": "https://github.com/minizon", "followers_url": "https://api.github.com/users/minizon/followers", "following_url": "https://api.github.com/users/minizon/following{/other_user}", "gists_url": "https://api.github.com/users/minizon/gists{/gist_id}", "starred_url": "https://api.github.com/users/minizon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/minizon/subscriptions", "organizations_url": "https://api.github.com/users/minizon/orgs", "repos_url": "https://api.github.com/users/minizon/repos", "events_url": "https://api.github.com/users/minizon/events{/privacy}", "received_events_url": "https://api.github.com/users/minizon/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-28T07:44:46Z", "updated_at": "2018-09-28T07:44:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3395998\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/freedomtan\">@freedomtan</a> What I did before is that I just simply replaced the retrained_graph.lite by the quantized_graph.lite in the assets folder (<a href=\"https://github.com/googlecodelabs/tensorflow-for-poets-2/tree/master/android/tflite/app/src/main/assets\">here</a>), the classifier for the tf interpreter was not changed (<a href=\"https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/android/tflite/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L97\">here</a>). I think this demo is still for the floating point model, but the error ( <strong>Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter-&gt;type != data_type (3 != 1)Node 4 failed to prepare</strong>) did happen. Or do you mean only after post-training quantization, the model is still not proper for android deployment?</p>\n<p>Any way, I couldn't find clear guides for how to change the floating point model into a real quantized one. The official model zoo provides some pre-converted models (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md#image-classification-quantized-models\">here</a>) with no details for the conversion process.</p>", "body_text": "@freedomtan What I did before is that I just simply replaced the retrained_graph.lite by the quantized_graph.lite in the assets folder (here), the classifier for the tf interpreter was not changed (here). I think this demo is still for the floating point model, but the error ( Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare) did happen. Or do you mean only after post-training quantization, the model is still not proper for android deployment?\nAny way, I couldn't find clear guides for how to change the floating point model into a real quantized one. The official model zoo provides some pre-converted models (here) with no details for the conversion process.", "body": "@freedomtan What I did before is that I just simply replaced the retrained_graph.lite by the quantized_graph.lite in the assets folder ([here](https://github.com/googlecodelabs/tensorflow-for-poets-2/tree/master/android/tflite/app/src/main/assets)), the classifier for the tf interpreter was not changed ([here](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/android/tflite/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L97)). I think this demo is still for the floating point model, but the error ( **Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare**) did happen. Or do you mean only after post-training quantization, the model is still not proper for android deployment?\r\n\r\nAny way, I couldn't find clear guides for how to change the floating point model into a real quantized one. The official model zoo provides some pre-converted models ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md#image-classification-quantized-models)) with no details for the conversion process."}