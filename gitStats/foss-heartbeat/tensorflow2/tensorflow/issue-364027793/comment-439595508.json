{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439595508", "html_url": "https://github.com/tensorflow/tensorflow/issues/22535#issuecomment-439595508", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22535", "id": 439595508, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTU5NTUwOA==", "user": {"login": "brucechou1983", "id": 8468820, "node_id": "MDQ6VXNlcjg0Njg4MjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8468820?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brucechou1983", "html_url": "https://github.com/brucechou1983", "followers_url": "https://api.github.com/users/brucechou1983/followers", "following_url": "https://api.github.com/users/brucechou1983/following{/other_user}", "gists_url": "https://api.github.com/users/brucechou1983/gists{/gist_id}", "starred_url": "https://api.github.com/users/brucechou1983/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brucechou1983/subscriptions", "organizations_url": "https://api.github.com/users/brucechou1983/orgs", "repos_url": "https://api.github.com/users/brucechou1983/repos", "events_url": "https://api.github.com/users/brucechou1983/events{/privacy}", "received_events_url": "https://api.github.com/users/brucechou1983/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-17T07:22:32Z", "updated_at": "2018-11-17T07:22:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20431255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/damhurmuller\">@damhurmuller</a></p>\n<p>If you use the toco command line tool for generating post-training quantization models, make sure the <code>--inference_type</code> and <code>--inference_input_type</code> are both <em>FLOAT</em> rather than <em>QUANTIZED_UINT8</em>. If you want a full quantized model, go for the quantization aware training flow.</p>\n<p>Also you might need to build the TensorflowLite AAR from source because none of the online-available versions supported this feature when I tried them. I used this command:</p>\n<pre><code>bazel build --cxxopt='--std=c++11' -c opt        \\\n--fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   \\\n//tensorflow/lite/java:tensorflow-lite\n</code></pre>\n<p>Make sure to edit the <code>WORKSPACE</code> file so that bazel knows where the android sdk and ndk are.</p>", "body_text": "@damhurmuller\nIf you use the toco command line tool for generating post-training quantization models, make sure the --inference_type and --inference_input_type are both FLOAT rather than QUANTIZED_UINT8. If you want a full quantized model, go for the quantization aware training flow.\nAlso you might need to build the TensorflowLite AAR from source because none of the online-available versions supported this feature when I tried them. I used this command:\nbazel build --cxxopt='--std=c++11' -c opt        \\\n--fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   \\\n//tensorflow/lite/java:tensorflow-lite\n\nMake sure to edit the WORKSPACE file so that bazel knows where the android sdk and ndk are.", "body": "@damhurmuller \r\n\r\nIf you use the toco command line tool for generating post-training quantization models, make sure the `--inference_type` and `--inference_input_type` are both *FLOAT* rather than *QUANTIZED_UINT8*. If you want a full quantized model, go for the quantization aware training flow.\r\n\r\nAlso you might need to build the TensorflowLite AAR from source because none of the online-available versions supported this feature when I tried them. I used this command:\r\n```\r\nbazel build --cxxopt='--std=c++11' -c opt        \\\r\n--fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   \\\r\n//tensorflow/lite/java:tensorflow-lite\r\n```\r\nMake sure to edit the `WORKSPACE` file so that bazel knows where the android sdk and ndk are."}