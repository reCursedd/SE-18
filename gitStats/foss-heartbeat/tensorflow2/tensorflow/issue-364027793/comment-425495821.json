{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/425495821", "html_url": "https://github.com/tensorflow/tensorflow/issues/22535#issuecomment-425495821", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22535", "id": 425495821, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTQ5NTgyMQ==", "user": {"login": "freedomtan", "id": 3395998, "node_id": "MDQ6VXNlcjMzOTU5OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/3395998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/freedomtan", "html_url": "https://github.com/freedomtan", "followers_url": "https://api.github.com/users/freedomtan/followers", "following_url": "https://api.github.com/users/freedomtan/following{/other_user}", "gists_url": "https://api.github.com/users/freedomtan/gists{/gist_id}", "starred_url": "https://api.github.com/users/freedomtan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/freedomtan/subscriptions", "organizations_url": "https://api.github.com/users/freedomtan/orgs", "repos_url": "https://api.github.com/users/freedomtan/repos", "events_url": "https://api.github.com/users/freedomtan/events{/privacy}", "received_events_url": "https://api.github.com/users/freedomtan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-28T16:43:21Z", "updated_at": "2018-09-28T16:43:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11333107\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/minizon\">@minizon</a> I finally figured out what your environment/setting is. Put it simply, <strong>the TF Lite runtime used by the TensorFlow for Poets 2: TFLite Android example at this time doesn't support post-training quantization</strong>. Quick solution, build your own tensorflow-lite aar with more recent source code.</p>\n<p>More details:</p>\n<ol>\n<li>yes, the app you used uses floating-point model, not quantized uint8. I thought you were trying the tflitecamera demo in the TF source code rather than the one in the TensorFlow for Poets 2.</li>\n<li>post-training quantization is to quantize weights of convolutions, that is, it's weights only quantization. Weights are quantized from float32 to uint8. But computation is still in floating point, that is, dequantization should be done before computing. Code related to this dequantization was added relative late. So it's not in AAR you used.</li>\n<li>for full quantization / quantization-aware training see <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md\">this doc</a></li>\n<li>Since TensorFlow 1.11.0 was released. Maybe there will be updated AAR soon. Maybe <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785337\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wt-huang\">@wt-huang</a> can help?</li>\n</ol>", "body_text": "@minizon I finally figured out what your environment/setting is. Put it simply, the TF Lite runtime used by the TensorFlow for Poets 2: TFLite Android example at this time doesn't support post-training quantization. Quick solution, build your own tensorflow-lite aar with more recent source code.\nMore details:\n\nyes, the app you used uses floating-point model, not quantized uint8. I thought you were trying the tflitecamera demo in the TF source code rather than the one in the TensorFlow for Poets 2.\npost-training quantization is to quantize weights of convolutions, that is, it's weights only quantization. Weights are quantized from float32 to uint8. But computation is still in floating point, that is, dequantization should be done before computing. Code related to this dequantization was added relative late. So it's not in AAR you used.\nfor full quantization / quantization-aware training see this doc\nSince TensorFlow 1.11.0 was released. Maybe there will be updated AAR soon. Maybe @wt-huang can help?", "body": "@minizon I finally figured out what your environment/setting is. Put it simply, **the TF Lite runtime used by the TensorFlow for Poets 2: TFLite Android example at this time doesn't support post-training quantization**. Quick solution, build your own tensorflow-lite aar with more recent source code.\r\n\r\nMore details:\r\n1. yes, the app you used uses floating-point model, not quantized uint8. I thought you were trying the tflitecamera demo in the TF source code rather than the one in the TensorFlow for Poets 2.\r\n2. post-training quantization is to quantize weights of convolutions, that is, it's weights only quantization. Weights are quantized from float32 to uint8. But computation is still in floating point, that is, dequantization should be done before computing. Code related to this dequantization was added relative late. So it's not in AAR you used.\r\n3. for full quantization / quantization-aware training see [this doc]( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md)\r\n4. Since TensorFlow 1.11.0 was released. Maybe there will be updated AAR soon. Maybe @wt-huang can help?\r\n"}