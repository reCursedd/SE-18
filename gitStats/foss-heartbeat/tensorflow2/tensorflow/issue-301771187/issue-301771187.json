{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17378", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17378/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17378/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17378/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17378", "id": 301771187, "node_id": "MDU6SXNzdWUzMDE3NzExODc=", "number": 17378, "title": "Please explain what is going on in tf.nn.raw_rnn function?", "user": {"login": "tastyminerals", "id": 7676160, "node_id": "MDQ6VXNlcjc2NzYxNjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7676160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tastyminerals", "html_url": "https://github.com/tastyminerals", "followers_url": "https://api.github.com/users/tastyminerals/followers", "following_url": "https://api.github.com/users/tastyminerals/following{/other_user}", "gists_url": "https://api.github.com/users/tastyminerals/gists{/gist_id}", "starred_url": "https://api.github.com/users/tastyminerals/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tastyminerals/subscriptions", "organizations_url": "https://api.github.com/users/tastyminerals/orgs", "repos_url": "https://api.github.com/users/tastyminerals/repos", "events_url": "https://api.github.com/users/tastyminerals/events{/privacy}", "received_events_url": "https://api.github.com/users/tastyminerals/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-02T13:47:49Z", "updated_at": "2018-03-12T17:45:07Z", "closed_at": "2018-03-03T00:35:08Z", "author_association": "NONE", "body_html": "<p>I am trying to implement custom hidden state computation with a help of <code>tf.nn.raw_rnn</code> function. However, using the API example provided I am receiving a strange message:</p>\n<pre><code>&lt;tensorflow.python.util.tf_should_use._add_should_use_warning.&lt;locals&gt;.TFShouldUseWarningWrapper at 0x7f80ac138dd8&gt;\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7676160/36901389-45c88efc-1e27-11e8-93b1-f42c3018057e.png\"><img src=\"https://user-images.githubusercontent.com/7676160/36901389-45c88efc-1e27-11e8-93b1-f42c3018057e.png\" alt=\"2018-03-02-143635_1049x358_scrot\" style=\"max-width:100%;\"></a></p>\n<p>What I want to do is the following:</p>\n<pre><code>                    |--   hidden recurrent layer   --|\n[10 x 32 x 100] --&gt; LSTM cell [200] + Linear [200 x 1] --&gt; [10 x 32 x 1]\n</code></pre>\n<p>Here is my implementation, well basically it is the example from official docs:</p>\n<div class=\"highlight highlight-source-python\"><pre>   outputs, state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._get_raw_rnn_graph(inputs, config, is_training)\n\n   <span class=\"pl-k\">def</span> <span class=\"pl-en\">_get_raw_rnn_graph</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">config</span>, <span class=\"pl-smi\">is_training</span>):\n        time <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> define placeholders</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>_inputs = tf.placeholder(shape=(config.num_steps, config.batch_size, config.input_size),</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>                         dtype=tf.float32)</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>_batch_len = tf.placeholder(shape=(config.batch_size,), dtype=tf.int32)</span>\n        _inputs_ta <span class=\"pl-k\">=</span> tf.TensorArray(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>config.num_steps, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TA<span class=\"pl-pds\">\"</span></span>)\n        _inputs_ta <span class=\"pl-k\">=</span> _inputs_ta.unstack(inputs)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;-- throws a warning </span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> create simple LSTM cell</span>\n        cell <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMCell(config.hidden_size)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> create loop_fn for raw_rnn</span>\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">loop_fn</span>(<span class=\"pl-smi\">time</span>, <span class=\"pl-smi\">cell_output</span>, <span class=\"pl-smi\">cell_state</span>, <span class=\"pl-smi\">loop_state</span>):\n            emit_output <span class=\"pl-k\">=</span> cell_output  <span class=\"pl-c\"><span class=\"pl-c\">#</span> == None if time = 0</span>\n\n            <span class=\"pl-k\">if</span> cell_output <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> time = 0</span>\n                next_cell_state <span class=\"pl-k\">=</span> cell.zero_state(config.batch_size, tf.float32)\n                <span class=\"pl-c1\">self</span>._initial_state <span class=\"pl-k\">=</span> next_cell_state\n            <span class=\"pl-k\">else</span>:\n                next_cell_state <span class=\"pl-k\">=</span> cell_state\n\n            elements_finished <span class=\"pl-k\">=</span> (time <span class=\"pl-k\">&gt;=</span> config.num_steps)\n            finished <span class=\"pl-k\">=</span> tf.reduce_all(elements_finished)\n            next_input <span class=\"pl-k\">=</span> tf.cond(finished,\n                                 <span class=\"pl-k\">lambda</span>: tf.zeros([config.batch_size, config.input_size],\n                                                   <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32),\n                                 <span class=\"pl-k\">lambda</span>: _inputs_ta.read(time))\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> apply linear + sig transform here</span>\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>before lin+sig<span class=\"pl-pds\">\"</span></span>, next_input)\n            next_input <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._linear_transform(next_input)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [32, 200] --&gt; [32, 1]</span>\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>after lin+sig<span class=\"pl-pds\">\"</span></span>, next_input)\n\n            next_loop_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n            <span class=\"pl-k\">return</span> (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n\n        outputs_ta, final_state, _ <span class=\"pl-k\">=</span> tf.nn.raw_rnn(cell, loop_fn)\n        outputs <span class=\"pl-k\">=</span> outputs_ta.stack()\n        <span class=\"pl-k\">return</span> outputs, final_state</pre></div>\n<p>The line <code>_inputs_ta = _inputs_ta.unstack(inputs)</code> causes the message above and I wonder if this is actually a bug or am I doing something wrong completely. The end result is that I am not getting the correct output shape because the <code>next_input</code> comes as <code>[32, 100]</code> into my linear transform function as if LSTM cell was never applied to it. Please clarify whether <code>raw_rnn</code> is usable under Tensorflow 1.5.0.</p>", "body_text": "I am trying to implement custom hidden state computation with a help of tf.nn.raw_rnn function. However, using the API example provided I am receiving a strange message:\n<tensorflow.python.util.tf_should_use._add_should_use_warning.<locals>.TFShouldUseWarningWrapper at 0x7f80ac138dd8>\n\n\nWhat I want to do is the following:\n                    |--   hidden recurrent layer   --|\n[10 x 32 x 100] --> LSTM cell [200] + Linear [200 x 1] --> [10 x 32 x 1]\n\nHere is my implementation, well basically it is the example from official docs:\n   outputs, state = self._get_raw_rnn_graph(inputs, config, is_training)\n\n   def _get_raw_rnn_graph(self, inputs, config, is_training):\n        time = tf.constant(0, dtype=tf.int32)\n        # define placeholders\n        #_inputs = tf.placeholder(shape=(config.num_steps, config.batch_size, config.input_size),\n        #                         dtype=tf.float32)\n        #_batch_len = tf.placeholder(shape=(config.batch_size,), dtype=tf.int32)\n        _inputs_ta = tf.TensorArray(dtype=tf.float32, size=config.num_steps, name=\"TA\")\n        _inputs_ta = _inputs_ta.unstack(inputs)  # <-- throws a warning \n\n        # create simple LSTM cell\n        cell = tf.contrib.rnn.LSTMCell(config.hidden_size)\n\n        # create loop_fn for raw_rnn\n        def loop_fn(time, cell_output, cell_state, loop_state):\n            emit_output = cell_output  # == None if time = 0\n\n            if cell_output is None:  # time = 0\n                next_cell_state = cell.zero_state(config.batch_size, tf.float32)\n                self._initial_state = next_cell_state\n            else:\n                next_cell_state = cell_state\n\n            elements_finished = (time >= config.num_steps)\n            finished = tf.reduce_all(elements_finished)\n            next_input = tf.cond(finished,\n                                 lambda: tf.zeros([config.batch_size, config.input_size],\n                                                   dtype=tf.float32),\n                                 lambda: _inputs_ta.read(time))\n\n            # apply linear + sig transform here\n            print(\"before lin+sig\", next_input)\n            next_input = self._linear_transform(next_input)  # [32, 200] --> [32, 1]\n            print(\"after lin+sig\", next_input)\n\n            next_loop_state = None\n            return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n\n        outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n        outputs = outputs_ta.stack()\n        return outputs, final_state\nThe line _inputs_ta = _inputs_ta.unstack(inputs) causes the message above and I wonder if this is actually a bug or am I doing something wrong completely. The end result is that I am not getting the correct output shape because the next_input comes as [32, 100] into my linear transform function as if LSTM cell was never applied to it. Please clarify whether raw_rnn is usable under Tensorflow 1.5.0.", "body": "I am trying to implement custom hidden state computation with a help of `tf.nn.raw_rnn` function. However, using the API example provided I am receiving a strange message:\r\n\r\n```\r\n<tensorflow.python.util.tf_should_use._add_should_use_warning.<locals>.TFShouldUseWarningWrapper at 0x7f80ac138dd8>\r\n```\r\n\r\n\r\n![2018-03-02-143635_1049x358_scrot](https://user-images.githubusercontent.com/7676160/36901389-45c88efc-1e27-11e8-93b1-f42c3018057e.png)\r\n\r\nWhat I want to do is the following:\r\n\r\n```\r\n                    |--   hidden recurrent layer   --|\r\n[10 x 32 x 100] --> LSTM cell [200] + Linear [200 x 1] --> [10 x 32 x 1]\r\n```\r\n\r\nHere is my implementation, well basically it is the example from official docs:\r\n\r\n```python\r\n   outputs, state = self._get_raw_rnn_graph(inputs, config, is_training)\r\n\r\n   def _get_raw_rnn_graph(self, inputs, config, is_training):\r\n        time = tf.constant(0, dtype=tf.int32)\r\n        # define placeholders\r\n        #_inputs = tf.placeholder(shape=(config.num_steps, config.batch_size, config.input_size),\r\n        #                         dtype=tf.float32)\r\n        #_batch_len = tf.placeholder(shape=(config.batch_size,), dtype=tf.int32)\r\n        _inputs_ta = tf.TensorArray(dtype=tf.float32, size=config.num_steps, name=\"TA\")\r\n        _inputs_ta = _inputs_ta.unstack(inputs)  # <-- throws a warning \r\n\r\n        # create simple LSTM cell\r\n        cell = tf.contrib.rnn.LSTMCell(config.hidden_size)\r\n\r\n        # create loop_fn for raw_rnn\r\n        def loop_fn(time, cell_output, cell_state, loop_state):\r\n            emit_output = cell_output  # == None if time = 0\r\n\r\n            if cell_output is None:  # time = 0\r\n                next_cell_state = cell.zero_state(config.batch_size, tf.float32)\r\n                self._initial_state = next_cell_state\r\n            else:\r\n                next_cell_state = cell_state\r\n\r\n            elements_finished = (time >= config.num_steps)\r\n            finished = tf.reduce_all(elements_finished)\r\n            next_input = tf.cond(finished,\r\n                                 lambda: tf.zeros([config.batch_size, config.input_size],\r\n                                                   dtype=tf.float32),\r\n                                 lambda: _inputs_ta.read(time))\r\n\r\n            # apply linear + sig transform here\r\n            print(\"before lin+sig\", next_input)\r\n            next_input = self._linear_transform(next_input)  # [32, 200] --> [32, 1]\r\n            print(\"after lin+sig\", next_input)\r\n\r\n            next_loop_state = None\r\n            return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\r\n\r\n        outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\r\n        outputs = outputs_ta.stack()\r\n        return outputs, final_state\r\n```\r\n\r\nThe line `_inputs_ta = _inputs_ta.unstack(inputs)` causes the message above and I wonder if this is actually a bug or am I doing something wrong completely. The end result is that I am not getting the correct output shape because the `next_input` comes as `[32, 100]` into my linear transform function as if LSTM cell was never applied to it. Please clarify whether `raw_rnn` is usable under Tensorflow 1.5.0. \r\n"}