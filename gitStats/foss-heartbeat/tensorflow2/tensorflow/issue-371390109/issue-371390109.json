{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23064", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23064/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23064/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23064/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23064", "id": 371390109, "node_id": "MDU6SXNzdWUzNzEzOTAxMDk=", "number": 23064, "title": "Is there something wrong with AttentionWrapper when use BahdanauAttention or LuongAttention? ", "user": {"login": "qzfnihao", "id": 5810767, "node_id": "MDQ6VXNlcjU4MTA3Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5810767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qzfnihao", "html_url": "https://github.com/qzfnihao", "followers_url": "https://api.github.com/users/qzfnihao/followers", "following_url": "https://api.github.com/users/qzfnihao/following{/other_user}", "gists_url": "https://api.github.com/users/qzfnihao/gists{/gist_id}", "starred_url": "https://api.github.com/users/qzfnihao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qzfnihao/subscriptions", "organizations_url": "https://api.github.com/users/qzfnihao/orgs", "repos_url": "https://api.github.com/users/qzfnihao/repos", "events_url": "https://api.github.com/users/qzfnihao/events{/privacy}", "received_events_url": "https://api.github.com/users/qzfnihao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097545817, "node_id": "MDU6TGFiZWwxMDk3NTQ1ODE3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:apis", "name": "comp:apis", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-10-18T07:10:50Z", "updated_at": "2018-11-01T20:49:38Z", "closed_at": "2018-11-01T20:49:38Z", "author_association": "NONE", "body_html": "<p>When using attention model, we need to get a AttentionWrapper object,  which defined in tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py. When focused on the func call, i find something wrong.</p>\n<p>tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py: In class AttentionWrapper:<br>\ndef call(self, inputs, state):<br>\n\"\"\"Perform a step of attention-wrapped RNN.<br>\n- Step 1: Mix the <code>inputs</code> and previous step's <code>attention</code> output via<br>\n<code>cell_input_fn</code>.<br>\n- Step 2: Call the wrapped <code>cell</code> with this input and its previous state.<br>\n- Step 3: Score the cell's output with <code>attention_mechanism</code>.<br>\n- Step 4: Calculate the alignments by passing the score through the<br>\n<code>normalizer</code>.<br>\n- Step 5: Calculate the context vector as the inner product between the<br>\nalignments and the attention_mechanism's values (memory).<br>\n- Step 6: Calculate the attention output by concatenating the cell output<br>\nand context through the attention layer (a linear layer with<br>\n<code>attention_layer_size</code> outputs).<br>\n....<br>\n# Step 1: Calculate the true inputs to the cell based on the<br>\n# previous attention value.<br>\ncell_inputs = self._cell_input_fn(inputs, state.attention)<br>\ncell_state = state.cell_state<br>\ncell_output, next_cell_state = self._cell(cell_inputs, cell_state)<br>\n.....<br>\nfor i, attention_mechanism in enumerate(self._attention_mechanisms):<br>\nattention, alignments, next_attention_state = _compute_attention(<br>\nattention_mechanism, cell_output, previous_attention_state[i],<br>\nself._attention_layers[i] if self._attention_layers else None)<br>\nalignment_history = previous_alignment_history[i].write(<br>\nstate.time, alignments) if self._alignment_history else ()</p>\n<pre><code>  all_attention_states.append(next_attention_state)\n  all_alignments.append(alignments)\n  all_attentions.append(attention)\n  maybe_all_histories.append(alignment_history)\n</code></pre>\n<p>.....</p>\n<p>The code is just copyed.  In _compute_attention(...),  we get alignment and attention, but here the second parameter  we used is  cell_ouput , which was calculted from self._cell(cell_inputs, cell_state).</p>\n<p>But In paper: neuaral machine translation by  jonintly learning to align and trainslate(<a href=\"https://arxiv.org/pdf/1409.0473.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1409.0473.pdf</a>), the BahdanauAttention, which should be calculated by cell_state,  not cell_output.</p>\n<p>In paper: effective Approaches to Attention-based Neural Machine Translation(<a href=\"https://arxiv.org/pdf/1508.04025.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1508.04025.pdf</a>), i cannot  find the LuongAttention used to calculate current cell state and output, so, how should step 1 occured? The default self._cell_input_fn(inputs, state.attention) just cancat input and stattion.</p>\n<p>Is there someting wrong?</p>", "body_text": "When using attention model, we need to get a AttentionWrapper object,  which defined in tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py. When focused on the func call, i find something wrong.\ntensorflow/contrib/seq2seq/python/ops/attention_wrapper.py: In class AttentionWrapper:\ndef call(self, inputs, state):\n\"\"\"Perform a step of attention-wrapped RNN.\n- Step 1: Mix the inputs and previous step's attention output via\ncell_input_fn.\n- Step 2: Call the wrapped cell with this input and its previous state.\n- Step 3: Score the cell's output with attention_mechanism.\n- Step 4: Calculate the alignments by passing the score through the\nnormalizer.\n- Step 5: Calculate the context vector as the inner product between the\nalignments and the attention_mechanism's values (memory).\n- Step 6: Calculate the attention output by concatenating the cell output\nand context through the attention layer (a linear layer with\nattention_layer_size outputs).\n....\n# Step 1: Calculate the true inputs to the cell based on the\n# previous attention value.\ncell_inputs = self._cell_input_fn(inputs, state.attention)\ncell_state = state.cell_state\ncell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n.....\nfor i, attention_mechanism in enumerate(self._attention_mechanisms):\nattention, alignments, next_attention_state = _compute_attention(\nattention_mechanism, cell_output, previous_attention_state[i],\nself._attention_layers[i] if self._attention_layers else None)\nalignment_history = previous_alignment_history[i].write(\nstate.time, alignments) if self._alignment_history else ()\n  all_attention_states.append(next_attention_state)\n  all_alignments.append(alignments)\n  all_attentions.append(attention)\n  maybe_all_histories.append(alignment_history)\n\n.....\nThe code is just copyed.  In _compute_attention(...),  we get alignment and attention, but here the second parameter  we used is  cell_ouput , which was calculted from self._cell(cell_inputs, cell_state).\nBut In paper: neuaral machine translation by  jonintly learning to align and trainslate(https://arxiv.org/pdf/1409.0473.pdf), the BahdanauAttention, which should be calculated by cell_state,  not cell_output.\nIn paper: effective Approaches to Attention-based Neural Machine Translation(https://arxiv.org/pdf/1508.04025.pdf), i cannot  find the LuongAttention used to calculate current cell state and output, so, how should step 1 occured? The default self._cell_input_fn(inputs, state.attention) just cancat input and stattion.\nIs there someting wrong?", "body": "When using attention model, we need to get a AttentionWrapper object,  which defined in tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py. When focused on the func call, i find something wrong. \r\n\r\ntensorflow/contrib/seq2seq/python/ops/attention_wrapper.py: In class AttentionWrapper:\r\n  def call(self, inputs, state):\r\n    \"\"\"Perform a step of attention-wrapped RNN.\r\n    - Step 1: Mix the `inputs` and previous step's `attention` output via\r\n      `cell_input_fn`.\r\n    - Step 2: Call the wrapped `cell` with this input and its previous state.\r\n    - Step 3: Score the cell's output with `attention_mechanism`.\r\n    - Step 4: Calculate the alignments by passing the score through the\r\n      `normalizer`.\r\n    - Step 5: Calculate the context vector as the inner product between the\r\n      alignments and the attention_mechanism's values (memory).\r\n    - Step 6: Calculate the attention output by concatenating the cell output\r\n      and context through the attention layer (a linear layer with\r\n      `attention_layer_size` outputs).\r\n....\r\n    # Step 1: Calculate the true inputs to the cell based on the\r\n    # previous attention value.\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\n    cell_state = state.cell_state\r\n    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n.....\r\n    for i, attention_mechanism in enumerate(self._attention_mechanisms):\r\n      attention, alignments, next_attention_state = _compute_attention(\r\n          attention_mechanism, cell_output, previous_attention_state[i],\r\n          self._attention_layers[i] if self._attention_layers else None)\r\n      alignment_history = previous_alignment_history[i].write(\r\n          state.time, alignments) if self._alignment_history else ()\r\n\r\n      all_attention_states.append(next_attention_state)\r\n      all_alignments.append(alignments)\r\n      all_attentions.append(attention)\r\n      maybe_all_histories.append(alignment_history)\r\n.....\r\n\r\nThe code is just copyed.  In _compute_attention(...),  we get alignment and attention, but here the second parameter  we used is  cell_ouput , which was calculted from self._cell(cell_inputs, cell_state). \r\n\r\nBut In paper: neuaral machine translation by  jonintly learning to align and trainslate(https://arxiv.org/pdf/1409.0473.pdf), the BahdanauAttention, which should be calculated by cell_state,  not cell_output.\r\n\r\nIn paper: effective Approaches to Attention-based Neural Machine Translation(https://arxiv.org/pdf/1508.04025.pdf), i cannot  find the LuongAttention used to calculate current cell state and output, so, how should step 1 occured? The default self._cell_input_fn(inputs, state.attention) just cancat input and stattion.\r\n\r\nIs there someting wrong?\r\n"}