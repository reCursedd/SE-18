{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18425", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18425/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18425/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18425/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18425", "id": 313406999, "node_id": "MDU6SXNzdWUzMTM0MDY5OTk=", "number": 18425, "title": "Non chief worker is throwing exception while saving the model", "user": {"login": "deepak-2017", "id": 32203263, "node_id": "MDQ6VXNlcjMyMjAzMjYz", "avatar_url": "https://avatars3.githubusercontent.com/u/32203263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepak-2017", "html_url": "https://github.com/deepak-2017", "followers_url": "https://api.github.com/users/deepak-2017/followers", "following_url": "https://api.github.com/users/deepak-2017/following{/other_user}", "gists_url": "https://api.github.com/users/deepak-2017/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepak-2017/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepak-2017/subscriptions", "organizations_url": "https://api.github.com/users/deepak-2017/orgs", "repos_url": "https://api.github.com/users/deepak-2017/repos", "events_url": "https://api.github.com/users/deepak-2017/events{/privacy}", "received_events_url": "https://api.github.com/users/deepak-2017/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-04-11T16:53:27Z", "updated_at": "2018-06-25T19:23:10Z", "closed_at": "2018-06-25T19:00:22Z", "author_association": "NONE", "body_html": "<h2>System information</h2>\n<ul>\n<li>Have I written custom code - Yes</li>\n<li>OS Platform CentOS - 7.2.1511</li>\n<li>TensorFlow installed from - Binary</li>\n<li>TensorFlow version - 1.3.0</li>\n<li>Python version - 2.7</li>\n<li>Bazel version - N/A</li>\n<li>CUDA/cuDNN version - N/A</li>\n<li>GPU model and memory - N/A</li>\n</ul>\n<p>I am having three nodes Tensorflow cluster with one parameter server, two workers, first one is chief worker.<br>\nAll three servers are running on the single machine but on different ports.</p>\n<h2>Source Code</h2>\n<p>Sharing my code snippet below,</p>\n<pre><code>sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\n                                 summary_op = my_summary_op, init_fn = restore_fn)\n with sv.prepare_or_wait_for_session(server.target) as sess:\n            for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\n                #Log info at each epoch:\n                if step % num_batches_per_epoch == 0:\n                    logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\n                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n                    logging.info('Current Learning Rate: %s', learning_rate_value)\n                    logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\n\t\t\t\t\t\n                #Log the summaries every 10 step.\n                if step % FLAGS.steps_update_frequency == 0 and step != 0:\n                    loss, gs = train_step(sess, train_op, sv.global_step)\n                    summaries = sess.run(my_summary_op)\n                    \n                    sv.summary_computed(sess, summaries)\n                    print \"**** SAVE THE MODEL ****\"\n                    sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\n\n                else:\n                    loss, _ = train_step(sess, train_op, sv.global_step)\n\n            #We log the final training loss and accuracy\n            logging.info('Final Loss: %s', loss)\n            logging.info('Final Training Accuracy: %s', sess.run(accuracy))\n\n            #Save the model on completing the training\n            logging.info('Finished training! Saving model to disk now.')\n            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n</code></pre>\n<h2>Describe the problem</h2>\n<p>Parameter server and workers have been started fine and training is running on the both the workers.<br>\nPlz note that I am using tensorflow Supervisor API to run my training.</p>\n<p>Problem starts at non-chief worker when I try to save summaries and checkpoint after certain number of training steps.<br>\nI am using the single script for running parameter server and workers.<br>\nI don't understand why is chief worker not reporting any issue while saving summaries and checkpoint and only non-chief worker is complaining about it.<br>\nAm I doing anything wrong in my code ?</p>\n<p>As per my understanding, every worker will run training on it's own data and store the checkpoints periodically.<br>\nHowever, I have read somewhere that it's the responsibility of only chief worker to save summaries and checkpoints.<br>\nIs that true and that's the reason saving model is failing for me at non-worker node?</p>\n<p>If yes, then my question is what is the use of other worker here if it doesn't store the model, don't we loose pottentially a better model at other non-chief worker ?</p>\n<h2>Error Logs</h2>\n<p>Posting error logs received at non-chief worker while saving summaries.Similarly some other exception was thrown while saving the model.</p>\n<pre><code>Traceback (most recent call last):\n  File \"./DTF_train_image_classifier.py\", line 469, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"./DTF_train_image_classifier.py\", line 433, in main\n    sv.summary_computed(sess, summaries)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 852, in summary_computed\n    raise RuntimeError(\"Writing a summary requires a summary writer.\")\nRuntimeError: Writing a summary requires a summary writer.\n</code></pre>\n<p>Plz note that error is thrown at below two lines,</p>\n<pre><code>sv.summary_computed(sess, summaries)\nsv.saver.save(sess,sv.save_path,global_step=sv.global_step)\n</code></pre>", "body_text": "System information\n\nHave I written custom code - Yes\nOS Platform CentOS - 7.2.1511\nTensorFlow installed from - Binary\nTensorFlow version - 1.3.0\nPython version - 2.7\nBazel version - N/A\nCUDA/cuDNN version - N/A\nGPU model and memory - N/A\n\nI am having three nodes Tensorflow cluster with one parameter server, two workers, first one is chief worker.\nAll three servers are running on the single machine but on different ports.\nSource Code\nSharing my code snippet below,\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\n                                 summary_op = my_summary_op, init_fn = restore_fn)\n with sv.prepare_or_wait_for_session(server.target) as sess:\n            for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\n                #Log info at each epoch:\n                if step % num_batches_per_epoch == 0:\n                    logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\n                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n                    logging.info('Current Learning Rate: %s', learning_rate_value)\n                    logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\n\t\t\t\t\t\n                #Log the summaries every 10 step.\n                if step % FLAGS.steps_update_frequency == 0 and step != 0:\n                    loss, gs = train_step(sess, train_op, sv.global_step)\n                    summaries = sess.run(my_summary_op)\n                    \n                    sv.summary_computed(sess, summaries)\n                    print \"**** SAVE THE MODEL ****\"\n                    sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\n\n                else:\n                    loss, _ = train_step(sess, train_op, sv.global_step)\n\n            #We log the final training loss and accuracy\n            logging.info('Final Loss: %s', loss)\n            logging.info('Final Training Accuracy: %s', sess.run(accuracy))\n\n            #Save the model on completing the training\n            logging.info('Finished training! Saving model to disk now.')\n            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n\nDescribe the problem\nParameter server and workers have been started fine and training is running on the both the workers.\nPlz note that I am using tensorflow Supervisor API to run my training.\nProblem starts at non-chief worker when I try to save summaries and checkpoint after certain number of training steps.\nI am using the single script for running parameter server and workers.\nI don't understand why is chief worker not reporting any issue while saving summaries and checkpoint and only non-chief worker is complaining about it.\nAm I doing anything wrong in my code ?\nAs per my understanding, every worker will run training on it's own data and store the checkpoints periodically.\nHowever, I have read somewhere that it's the responsibility of only chief worker to save summaries and checkpoints.\nIs that true and that's the reason saving model is failing for me at non-worker node?\nIf yes, then my question is what is the use of other worker here if it doesn't store the model, don't we loose pottentially a better model at other non-chief worker ?\nError Logs\nPosting error logs received at non-chief worker while saving summaries.Similarly some other exception was thrown while saving the model.\nTraceback (most recent call last):\n  File \"./DTF_train_image_classifier.py\", line 469, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"./DTF_train_image_classifier.py\", line 433, in main\n    sv.summary_computed(sess, summaries)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 852, in summary_computed\n    raise RuntimeError(\"Writing a summary requires a summary writer.\")\nRuntimeError: Writing a summary requires a summary writer.\n\nPlz note that error is thrown at below two lines,\nsv.summary_computed(sess, summaries)\nsv.saver.save(sess,sv.save_path,global_step=sv.global_step)", "body": "System information\r\n------------------\r\n\r\n - Have I written custom code - Yes\r\n - OS Platform CentOS - 7.2.1511\r\n - TensorFlow installed from - Binary\r\n - TensorFlow version - 1.3.0\r\n - Python version - 2.7\r\n - Bazel version - N/A\r\n - CUDA/cuDNN version - N/A\r\n - GPU model and memory - N/A\r\n\r\nI am having three nodes Tensorflow cluster with one parameter server, two workers, first one is chief worker.\r\nAll three servers are running on the single machine but on different ports.\r\n\r\nSource Code\r\n-----------\r\nSharing my code snippet below,\r\n\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\r\n                                     summary_op = my_summary_op, init_fn = restore_fn)\r\n     with sv.prepare_or_wait_for_session(server.target) as sess:\r\n                for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\r\n                    #Log info at each epoch:\r\n                    if step % num_batches_per_epoch == 0:\r\n                        logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\r\n                        learning_rate_value, accuracy_value = sess.run([lr, accuracy])\r\n                        logging.info('Current Learning Rate: %s', learning_rate_value)\r\n                        logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\r\n    \t\t\t\t\t\r\n                    #Log the summaries every 10 step.\r\n                    if step % FLAGS.steps_update_frequency == 0 and step != 0:\r\n                        loss, gs = train_step(sess, train_op, sv.global_step)\r\n                        summaries = sess.run(my_summary_op)\r\n                        \r\n                        sv.summary_computed(sess, summaries)\r\n                        print \"**** SAVE THE MODEL ****\"\r\n                        sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\r\n    \r\n                    else:\r\n                        loss, _ = train_step(sess, train_op, sv.global_step)\r\n    \r\n                #We log the final training loss and accuracy\r\n                logging.info('Final Loss: %s', loss)\r\n                logging.info('Final Training Accuracy: %s', sess.run(accuracy))\r\n    \r\n                #Save the model on completing the training\r\n                logging.info('Finished training! Saving model to disk now.')\r\n                sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\r\n\r\n\r\nDescribe the problem\r\n--------------------\r\n\r\nParameter server and workers have been started fine and training is running on the both the workers.\r\nPlz note that I am using tensorflow Supervisor API to run my training.\r\n\r\nProblem starts at non-chief worker when I try to save summaries and checkpoint after certain number of training steps.\r\nI am using the single script for running parameter server and workers.\r\nI don't understand why is chief worker not reporting any issue while saving summaries and checkpoint and only non-chief worker is complaining about it.\r\nAm I doing anything wrong in my code ?\r\n\r\nAs per my understanding, every worker will run training on it's own data and store the checkpoints periodically.\r\nHowever, I have read somewhere that it's the responsibility of only chief worker to save summaries and checkpoints.\r\nIs that true and that's the reason saving model is failing for me at non-worker node?\r\n\r\nIf yes, then my question is what is the use of other worker here if it doesn't store the model, don't we loose pottentially a better model at other non-chief worker ?\r\n\r\nError Logs\r\n----------\r\nPosting error logs received at non-chief worker while saving summaries.Similarly some other exception was thrown while saving the model.\r\n\r\n    Traceback (most recent call last):\r\n      File \"./DTF_train_image_classifier.py\", line 469, in <module>\r\n        tf.app.run()\r\n      File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n      File \"./DTF_train_image_classifier.py\", line 433, in main\r\n        sv.summary_computed(sess, summaries)\r\n      File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 852, in summary_computed\r\n        raise RuntimeError(\"Writing a summary requires a summary writer.\")\r\n    RuntimeError: Writing a summary requires a summary writer.\r\n\r\nPlz note that error is thrown at below two lines,\r\n\r\n    sv.summary_computed(sess, summaries)\r\n    sv.saver.save(sess,sv.save_path,global_step=sv.global_step)"}