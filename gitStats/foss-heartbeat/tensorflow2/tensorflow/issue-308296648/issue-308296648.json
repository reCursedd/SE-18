{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17974", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17974/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17974/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17974/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17974", "id": 308296648, "node_id": "MDU6SXNzdWUzMDgyOTY2NDg=", "number": 17974, "title": "QuantizedConv2D dimension mismatch", "user": {"login": "elbaro", "id": 1851290, "node_id": "MDQ6VXNlcjE4NTEyOTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1851290?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elbaro", "html_url": "https://github.com/elbaro", "followers_url": "https://api.github.com/users/elbaro/followers", "following_url": "https://api.github.com/users/elbaro/following{/other_user}", "gists_url": "https://api.github.com/users/elbaro/gists{/gist_id}", "starred_url": "https://api.github.com/users/elbaro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elbaro/subscriptions", "organizations_url": "https://api.github.com/users/elbaro/orgs", "repos_url": "https://api.github.com/users/elbaro/repos", "events_url": "https://api.github.com/users/elbaro/events{/privacy}", "received_events_url": "https://api.github.com/users/elbaro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-03-24T20:33:23Z", "updated_at": "2018-11-14T19:16:22Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Y</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Arch</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary / source for transform_graph</li>\n<li><strong>TensorFlow version (use command below)</strong>:  1.6</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.11.1-1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.1</li>\n<li><strong>GPU model and memory</strong>: GTX  1060</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>I have a frozen model (frozen.pb) and followed the guideline to produce <code>quantized.pb</code>.<br>\nInference with frozen.pb is ok but with quantized.pb it crashes on <code>tf.import_graph_def</code>.</p>\n<p>If the quantized model expects the same shape of input/output, just replacing frozen.pb with quantized.pb should work.</p>\n<ul>\n<li>I followed <a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">https://www.tensorflow.org/performance/quantization</a></li>\n<li>quantized with this command:</li>\n</ul>\n<pre><code>../tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n  --in_graph=graph_def/frozen.pb \\\n  --out_graph=graph_def/quantized.pb \\\n  --inputs=img \\\n  --outputs=out1,out2,out3,out4,out5,out6 \\\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,3,256,256\")\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\n    strip_unused_nodes sort_by_execution_order'\n</code></pre>\n<ul>\n<li>backtrace:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (Most recent call last):\n<span class=\"pl-c1\">8</span>    test_tf.py                                                                    <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>                <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> detector <span class=\"pl-k\">=</span> Detector()                              \n<span class=\"pl-c1\">114</span>  <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>user<span class=\"pl-k\">/</span>project<span class=\"pl-k\">/</span>net_tf.py                                             <span class=\"pl-c1\">__init__</span>                <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> tf.import_graph_def(graph_def, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)            \n<span class=\"pl-c1\">432</span>  <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>util<span class=\"pl-k\">/</span>deprecation.py        new_func                <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">return</span> func(<span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)                       \n<span class=\"pl-c1\">663</span>  <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>importer.py      import_graph_def        <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> ops.set_shapes_for_outputs(op)                     \n<span class=\"pl-c1\">2501</span> <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>ops.py           set_shapes_for_outputs  <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">return</span> _set_shapes_for_outputs(op)                 \n<span class=\"pl-c1\">2474</span> <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>ops.py           _set_shapes_for_outputs <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> shapes <span class=\"pl-k\">=</span> shape_func(op)                            \n<span class=\"pl-c1\">2404</span> <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>ops.py           call_with_requiring     <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">return</span> call_cpp_shape_fn(op, <span class=\"pl-v\">require_shape_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">627</span>  <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>common_shapes.py call_cpp_shape_fn       <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> require_shape_fn)                                  \n<span class=\"pl-c1\">691</span>  <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>common_shapes.py _call_cpp_shape_fn_impl <span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(err.message)                      \n<span class=\"pl-c1\">ValueError</span>: Dimensions must be equal, but are <span class=\"pl-c1\">32</span> <span class=\"pl-k\">and</span> <span class=\"pl-c1\">64</span> <span class=\"pl-k\">for</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2_1/Conv2D/eightbit<span class=\"pl-pds\">'</span></span> (op: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>QuantizedConv2D<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">with</span> <span class=\"pl-c1\">input</span> shapes: [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-ii\">?</span>,<span class=\"pl-c1\">32</span>], [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">128</span>], [], [], [], [].\n<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>common_shapes.py(<span class=\"pl-c1\">691</span>)_call_cpp_shape_fn_impl()</pre></div>\n<ul>\n<li>related model code:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre>    max_pool <span class=\"pl-k\">=</span> tf.contrib.layers.max_pool2d\n\n    x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>img<span class=\"pl-pds\">'</span></span>)\n\n    x <span class=\"pl-k\">=</span> relu(conv2d(x, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1_1<span class=\"pl-pds\">'</span></span>))\n    x <span class=\"pl-k\">=</span> relu(conv2d(x, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1_2<span class=\"pl-pds\">'</span></span>))\n    x <span class=\"pl-k\">=</span> max_pool(x, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>NCHW<span class=\"pl-pds\">'</span></span>)\n\n    x <span class=\"pl-k\">=</span> relu(conv2d(x, <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2_1<span class=\"pl-pds\">'</span></span>))\n    x <span class=\"pl-k\">=</span> relu(conv2d(x, <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2_2<span class=\"pl-pds\">'</span></span>))\n    x <span class=\"pl-k\">=</span> max_pool(x, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>NCHW<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">...</span></pre></div>\n<p>This is fully convolutional, and the channel number goes from 3 to 64 and 128.<br>\nSo 32 in the error message comes out of nowhere. (Is [1,3,?,32] a NCHW shape or conv2d kernel shape?)</p>\n<p>Can it be related to NCHW? Somehow max_pool halves the channel number instead of spatial dimensions, then it explains how 32 appears (64/2=32).</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch\nTensorFlow installed from (source or binary): binary / source for transform_graph\nTensorFlow version (use command below):  1.6\nPython version:  3.6\nBazel version (if compiling from source): 0.11.1-1\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.1\nGPU model and memory: GTX  1060\nExact command to reproduce:\n\nI have a frozen model (frozen.pb) and followed the guideline to produce quantized.pb.\nInference with frozen.pb is ok but with quantized.pb it crashes on tf.import_graph_def.\nIf the quantized model expects the same shape of input/output, just replacing frozen.pb with quantized.pb should work.\n\nI followed https://www.tensorflow.org/performance/quantization\nquantized with this command:\n\n../tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n  --in_graph=graph_def/frozen.pb \\\n  --out_graph=graph_def/quantized.pb \\\n  --inputs=img \\\n  --outputs=out1,out2,out3,out4,out5,out6 \\\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,3,256,256\")\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\n    strip_unused_nodes sort_by_execution_order'\n\n\nbacktrace:\n\nTraceback (Most recent call last):\n8    test_tf.py                                                                    <module>                --> detector = Detector()                              \n114  /home/user/project/net_tf.py                                             __init__                --> tf.import_graph_def(graph_def, name='')            \n432  /usr/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py        new_func                --> return func(*args, **kwargs)                       \n663  /usr/lib/python3.6/site-packages/tensorflow/python/framework/importer.py      import_graph_def        --> ops.set_shapes_for_outputs(op)                     \n2501 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           set_shapes_for_outputs  --> return _set_shapes_for_outputs(op)                 \n2474 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           _set_shapes_for_outputs --> shapes = shape_func(op)                            \n2404 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           call_with_requiring     --> return call_cpp_shape_fn(op, require_shape_fn=True)\n627  /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py call_cpp_shape_fn       --> require_shape_fn)                                  \n691  /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py _call_cpp_shape_fn_impl --> raise ValueError(err.message)                      \nValueError: Dimensions must be equal, but are 32 and 64 for 'conv2_1/Conv2D/eightbit' (op: 'QuantizedConv2D') with input shapes: [1,3,?,32], [3,3,64,128], [], [], [], [].\n> /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py(691)_call_cpp_shape_fn_impl()\n\nrelated model code:\n\n    max_pool = tf.contrib.layers.max_pool2d\n\n    x = tf.placeholder(tf.float32, shape=[1, 3, None, None], name='img')\n\n    x = relu(conv2d(x, 64, kernel_size=3, padding='same', data_format='channels_first', name='conv1_1'))\n    x = relu(conv2d(x, 64, kernel_size=3, padding='same', data_format='channels_first', name='conv1_2'))\n    x = max_pool(x, kernel_size=2, data_format='NCHW')\n\n    x = relu(conv2d(x, 128, kernel_size=3, padding='same', data_format='channels_first', name='conv2_1'))\n    x = relu(conv2d(x, 128, kernel_size=3, padding='same', data_format='channels_first', name='conv2_2'))\n    x = max_pool(x, kernel_size=2, data_format='NCHW')\n...\nThis is fully convolutional, and the channel number goes from 3 to 64 and 128.\nSo 32 in the error message comes out of nowhere. (Is [1,3,?,32] a NCHW shape or conv2d kernel shape?)\nCan it be related to NCHW? Somehow max_pool halves the channel number instead of spatial dimensions, then it explains how 32 appears (64/2=32).", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch\r\n- **TensorFlow installed from (source or binary)**: binary / source for transform_graph\r\n- **TensorFlow version (use command below)**:  1.6\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: 0.11.1-1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: GTX  1060\r\n- **Exact command to reproduce**: \r\n\r\nI have a frozen model (frozen.pb) and followed the guideline to produce `quantized.pb`.\r\nInference with frozen.pb is ok but with quantized.pb it crashes on `tf.import_graph_def`.\r\n\r\nIf the quantized model expects the same shape of input/output, just replacing frozen.pb with quantized.pb should work.\r\n\r\n- I followed https://www.tensorflow.org/performance/quantization\r\n- quantized with this command:\r\n```\r\n../tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=graph_def/frozen.pb \\\r\n  --out_graph=graph_def/quantized.pb \\\r\n  --inputs=img \\\r\n  --outputs=out1,out2,out3,out4,out5,out6 \\\r\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,3,256,256\")\r\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\r\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\r\n    strip_unused_nodes sort_by_execution_order'\r\n```\r\n- backtrace:\r\n```py\r\nTraceback (Most recent call last):\r\n8    test_tf.py                                                                    <module>                --> detector = Detector()                              \r\n114  /home/user/project/net_tf.py                                             __init__                --> tf.import_graph_def(graph_def, name='')            \r\n432  /usr/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py        new_func                --> return func(*args, **kwargs)                       \r\n663  /usr/lib/python3.6/site-packages/tensorflow/python/framework/importer.py      import_graph_def        --> ops.set_shapes_for_outputs(op)                     \r\n2501 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           set_shapes_for_outputs  --> return _set_shapes_for_outputs(op)                 \r\n2474 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           _set_shapes_for_outputs --> shapes = shape_func(op)                            \r\n2404 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           call_with_requiring     --> return call_cpp_shape_fn(op, require_shape_fn=True)\r\n627  /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py call_cpp_shape_fn       --> require_shape_fn)                                  \r\n691  /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py _call_cpp_shape_fn_impl --> raise ValueError(err.message)                      \r\nValueError: Dimensions must be equal, but are 32 and 64 for 'conv2_1/Conv2D/eightbit' (op: 'QuantizedConv2D') with input shapes: [1,3,?,32], [3,3,64,128], [], [], [], [].\r\n> /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py(691)_call_cpp_shape_fn_impl()\r\n```\r\n\r\n- related model code:\r\n```py\r\n    max_pool = tf.contrib.layers.max_pool2d\r\n\r\n    x = tf.placeholder(tf.float32, shape=[1, 3, None, None], name='img')\r\n\r\n    x = relu(conv2d(x, 64, kernel_size=3, padding='same', data_format='channels_first', name='conv1_1'))\r\n    x = relu(conv2d(x, 64, kernel_size=3, padding='same', data_format='channels_first', name='conv1_2'))\r\n    x = max_pool(x, kernel_size=2, data_format='NCHW')\r\n\r\n    x = relu(conv2d(x, 128, kernel_size=3, padding='same', data_format='channels_first', name='conv2_1'))\r\n    x = relu(conv2d(x, 128, kernel_size=3, padding='same', data_format='channels_first', name='conv2_2'))\r\n    x = max_pool(x, kernel_size=2, data_format='NCHW')\r\n...\r\n```\r\n\r\nThis is fully convolutional, and the channel number goes from 3 to 64 and 128.\r\nSo 32 in the error message comes out of nowhere. (Is [1,3,?,32] a NCHW shape or conv2d kernel shape?)\r\n\r\nCan it be related to NCHW? Somehow max_pool halves the channel number instead of spatial dimensions, then it explains how 32 appears (64/2=32).\r\n"}