{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133318821", "pull_request_review_id": 56478281, "id": 133318821, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMzMxODgyMQ==", "diff_hunk": "@@ -0,0 +1,578 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/contrib/s3/s3_file_system.h\"\n+#include \"tensorflow/contrib/s3/s3_crypto.h\"\n+#include \"tensorflow/core/lib/io/path.h\"\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/utils/FileSystemUtils.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/S3Errors.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListObjectsRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include <cstdlib>\n+#include <mutex>\n+\n+namespace tensorflow {\n+\n+static const char* S3FileSystemAllocationTag = \"S3FileSystemAllocation\";\n+static const size_t S3ReadAppendableFileBufferSize = 1024 * 1024;\n+static const int S3GetChildrenMaxKeys = 100;\n+\n+Aws::Client::ClientConfiguration& GetDefaultClientConfig() {\n+  static std::mutex cfg_lock;\n+  static bool init(false);\n+  static Aws::Client::ClientConfiguration cfg;\n+\n+  std::lock_guard<std::mutex> lock(cfg_lock);\n+\n+  if (!init) {\n+    const char* endpoint = getenv(\"TFS3_ENDPOINT\");\n+    if (endpoint) {\n+      cfg.endpointOverride = Aws::String(endpoint);\n+    }\n+    const char* region = getenv(\"TFS3_REGION\");\n+    if (region) {\n+      cfg.region = Aws::String(region);\n+    }\n+    const char* use_https = getenv(\"TFS3_USE_HTTPS\");\n+    if (use_https) {\n+      if (use_https[0] == '0') {\n+        cfg.scheme = Aws::Http::Scheme::HTTP;\n+      } else {\n+        cfg.scheme = Aws::Http::Scheme::HTTPS;\n+      }\n+    }\n+    const char* verify_ssl = getenv(\"TFS3_VERIFY_SSL\");\n+    if (verify_ssl) {\n+      if (verify_ssl[0] == '0') {\n+        cfg.verifySSL = false;\n+      } else {\n+        cfg.verifySSL = true;\n+      }\n+    }\n+\n+    init = true;\n+  }\n+\n+  return cfg;\n+};\n+\n+Status ParseS3Path(const string& fname, bool empty_object_ok, string* bucket,\n+                   string* object) {\n+  if (!bucket || !object) {\n+    return errors::Internal(\"bucket and object cannot be null.\");\n+  }\n+  StringPiece scheme, bucketp, objectp;\n+  io::ParseURI(fname, &scheme, &bucketp, &objectp);\n+  if (scheme != \"s3\") {\n+    return errors::InvalidArgument(\"S3 path doesn't start with 's3://': \",\n+                                   fname);\n+  }\n+  *bucket = bucketp.ToString();\n+  if (bucket->empty() || *bucket == \".\") {\n+    return errors::InvalidArgument(\"S3 path doesn't contain a bucket name: \",\n+                                   fname);\n+  }\n+  objectp.Consume(\"/\");\n+  *object = objectp.ToString();\n+  if (!empty_object_ok && object->empty()) {\n+    return errors::InvalidArgument(\"S3 path doesn't contain an object name: \",\n+                                   fname);\n+  }\n+  return Status::OK();\n+}\n+\n+class S3RandomAccessFile : public RandomAccessFile {\n+ public:\n+  S3RandomAccessFile(const string& bucket, const string& object)\n+      : bucket_(bucket), object_(object) {}\n+\n+  Status Read(uint64 offset, size_t n, StringPiece* result,\n+              char* scratch) const override {\n+    Aws::S3::S3Client s3Client(GetDefaultClientConfig());\n+    Aws::S3::Model::GetObjectRequest getObjectRequest;\n+    getObjectRequest.WithBucket(bucket_.c_str()).WithKey(object_.c_str());\n+    char buffer[50];\n+    memset(buffer, 0x00, sizeof(buffer));\n+    snprintf(buffer, sizeof(buffer) - 1, \"bytes=%lld-%lld\", offset,\n+             offset + n - 1);\n+    getObjectRequest.SetRange(buffer);\n+    getObjectRequest.SetResponseStreamFactory([]() {\n+      return Aws::New<Aws::StringStream>(S3FileSystemAllocationTag);\n+    });\n+    auto getObjectOutcome = s3Client.GetObject(getObjectRequest);\n+    if (!getObjectOutcome.IsSuccess()) {\n+      n = 0;\n+      *result = StringPiece(scratch, n);\n+      return Status(error::OUT_OF_RANGE, \"Read less bytes than requested\");\n+    }\n+    n = getObjectOutcome.GetResult().GetContentLength();\n+    std::stringstream ss;\n+    ss << getObjectOutcome.GetResult().GetBody().rdbuf();\n+    ss.read(scratch, n);\n+\n+    *result = StringPiece(scratch, n);\n+    return Status::OK();\n+  }\n+\n+ private:\n+  string bucket_;\n+  string object_;\n+};\n+\n+class S3WritableFile : public WritableFile {\n+ public:\n+  S3WritableFile(const string& bucket, const string& object)\n+      : bucket_(bucket),\n+        object_(object),\n+        sync_needed_(true),\n+        outfile_(Aws::MakeShared<Aws::Utils::TempFile>(\n+            S3FileSystemAllocationTag, \"/tmp/s3_filesystem_XXXXXX\",\n+            std::ios_base::binary | std::ios_base::trunc | std::ios_base::in |\n+                std::ios_base::out)) {}\n+\n+  Status Append(const StringPiece& data) override {\n+    if (!outfile_) {\n+      return errors::FailedPrecondition(\n+          \"The internal temporary file is not writable.\");\n+    }\n+    sync_needed_ = true;\n+    outfile_->write(data.data(), data.size());\n+    if (!outfile_->good()) {\n+      return errors::Internal(\n+          \"Could not append to the internal temporary file.\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status Close() override {\n+    if (outfile_) {\n+      TF_RETURN_IF_ERROR(Sync());\n+      outfile_.reset();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status Flush() override { return Sync(); }\n+\n+  Status Sync() override {\n+    if (!outfile_) {\n+      return errors::FailedPrecondition(\n+          \"The internal temporary file is not writable.\");\n+    }\n+    if (!sync_needed_) {\n+      return Status::OK();\n+    }\n+    Aws::Client::ClientConfiguration clientConfig = GetDefaultClientConfig();\n+    clientConfig.connectTimeoutMs = 300000;\n+    clientConfig.requestTimeoutMs = 600000;\n+    Aws::S3::S3Client s3Client(clientConfig);\n+    Aws::S3::Model::PutObjectRequest putObjectRequest;\n+    putObjectRequest.WithBucket(bucket_.c_str()).WithKey(object_.c_str());\n+    long offset = outfile_->tellp();\n+    outfile_->seekg(0);\n+    putObjectRequest.SetBody(outfile_);\n+    putObjectRequest.SetContentLength(offset);\n+    auto putObjectOutcome = s3Client.PutObject(putObjectRequest);\n+    outfile_->clear();\n+    outfile_->seekp(offset);\n+    if (!putObjectOutcome.IsSuccess()) {\n+      std::stringstream ss;\n+      ss << putObjectOutcome.GetError().GetExceptionName() << \": \"\n+         << putObjectOutcome.GetError().GetMessage();\n+      return errors::Internal(ss.str());\n+    }\n+    return Status::OK();\n+  }\n+\n+ private:\n+  string bucket_;\n+  string object_;\n+  bool sync_needed_;\n+  std::shared_ptr<Aws::Utils::TempFile> outfile_;\n+};\n+\n+class S3ReadOnlyMemoryRegion : public ReadOnlyMemoryRegion {\n+ public:\n+  S3ReadOnlyMemoryRegion(std::unique_ptr<char[]> data, uint64 length)\n+      : data_(std::move(data)), length_(length) {}\n+  const void* data() override { return reinterpret_cast<void*>(data_.get()); }\n+  uint64 length() override { return length_; }\n+\n+ private:\n+  std::unique_ptr<char[]> data_;\n+  uint64 length_;\n+};\n+\n+S3FileSystem::S3FileSystem() {\n+  Aws::SDKOptions options;\n+  options.loggingOptions.logLevel = Aws::Utils::Logging::LogLevel::Info;\n+  options.cryptoOptions.sha256Factory_create_fn = []() {\n+    return Aws::MakeShared<S3SHA256Factory>(S3CryptoAllocationTag);\n+  };\n+  options.cryptoOptions.sha256HMACFactory_create_fn = []() {\n+    return Aws::MakeShared<S3SHA256HmacFactory>(S3CryptoAllocationTag);\n+  };\n+  Aws::InitAPI(options);\n+}\n+\n+S3FileSystem::~S3FileSystem() {\n+  Aws::SDKOptions options;\n+  options.loggingOptions.logLevel = Aws::Utils::Logging::LogLevel::Info;\n+  Aws::ShutdownAPI(options);\n+}\n+\n+Status S3FileSystem::NewRandomAccessFile(\n+    const string& fname, std::unique_ptr<RandomAccessFile>* result) {\n+  string bucket, object;\n+  TF_RETURN_IF_ERROR(ParseS3Path(fname, false, &bucket, &object));\n+  result->reset(new S3RandomAccessFile(bucket, object));\n+  return Status::OK();\n+}\n+\n+Status S3FileSystem::NewWritableFile(const string& fname,\n+                                     std::unique_ptr<WritableFile>* result) {\n+  string bucket, object;\n+  TF_RETURN_IF_ERROR(ParseS3Path(fname, false, &bucket, &object));\n+  result->reset(new S3WritableFile(bucket, object));\n+  return Status::OK();\n+}\n+\n+Status S3FileSystem::NewAppendableFile(const string& fname,\n+                                       std::unique_ptr<WritableFile>* result) {\n+  std::unique_ptr<RandomAccessFile> reader;\n+  TF_RETURN_IF_ERROR(NewRandomAccessFile(fname, &reader));\n+  std::unique_ptr<char[]> buffer(new char[S3ReadAppendableFileBufferSize]);\n+  Status status;\n+  uint64 offset = 0;\n+  StringPiece read_chunk;\n+\n+  string bucket, object;\n+  TF_RETURN_IF_ERROR(ParseS3Path(fname, false, &bucket, &object));\n+  result->reset(new S3WritableFile(bucket, object));\n+\n+  while (true) {\n+    status = reader->Read(offset, S3ReadAppendableFileBufferSize, &read_chunk,\n+                          buffer.get());\n+    if (status.ok()) {\n+      (*result)->Append(read_chunk);\n+      offset += S3ReadAppendableFileBufferSize;\n+    } else if (status.code() == error::OUT_OF_RANGE) {\n+      (*result)->Append(read_chunk);\n+      break;\n+    } else {\n+      (*result).reset();\n+      return status;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status S3FileSystem::NewReadOnlyMemoryRegionFromFile(\n+    const string& fname, std::unique_ptr<ReadOnlyMemoryRegion>* result) {\n+  uint64 size;\n+  TF_RETURN_IF_ERROR(GetFileSize(fname, &size));\n+  std::unique_ptr<char[]> data(new char[size]);\n+\n+  std::unique_ptr<RandomAccessFile> file;\n+  TF_RETURN_IF_ERROR(NewRandomAccessFile(fname, &file));\n+\n+  StringPiece piece;\n+  TF_RETURN_IF_ERROR(file->Read(0, size, &piece, data.get()));\n+\n+  result->reset(new S3ReadOnlyMemoryRegion(std::move(data), size));\n+  return Status::OK();\n+}\n+\n+Status S3FileSystem::FileExists(const string& fname) {\n+  FileStatistics stats;\n+  TF_RETURN_IF_ERROR(this->Stat(fname, &stats));\n+  return Status::OK();\n+}\n+\n+Status S3FileSystem::GetChildren(const string& dir,\n+                                 std::vector<string>* result) {\n+  string bucket, prefix;\n+  TF_RETURN_IF_ERROR(ParseS3Path(dir, false, &bucket, &prefix));\n+\n+  if (prefix.back() != '/') {\n+    prefix.push_back('/');\n+  }\n+\n+  Aws::S3::S3Client s3Client(GetDefaultClientConfig());\n+  Aws::S3::Model::ListObjectsRequest listObjectsRequest;\n+  listObjectsRequest.WithBucket(bucket.c_str())\n+      .WithPrefix(prefix.c_str())\n+      .WithMaxKeys(S3GetChildrenMaxKeys)\n+      .WithDelimiter(\"/\");\n+  listObjectsRequest.SetResponseStreamFactory(\n+      []() { return Aws::New<Aws::StringStream>(S3FileSystemAllocationTag); });\n+\n+  Aws::S3::Model::ListObjectsResult listObjectsResult;\n+  do {\n+    auto listObjectsOutcome = s3Client.ListObjects(listObjectsRequest);\n+    if (!listObjectsOutcome.IsSuccess()) {\n+      std::stringstream ss;\n+      ss << listObjectsOutcome.GetError().GetExceptionName() << \": \"\n+         << listObjectsOutcome.GetError().GetMessage();\n+      return errors::Internal(ss.str());\n+    }\n+\n+    listObjectsResult = listObjectsOutcome.GetResult();\n+    for (const auto& object : listObjectsResult.GetCommonPrefixes()) {\n+      Aws::String s = object.GetPrefix();\n+      s.erase(s.length() - 1);\n+      Aws::String entry = s.substr(strlen(prefix.c_str()));\n+      if (entry.length() > 0) {\n+        result->push_back(entry.c_str());\n+      }\n+    }\n+    for (const auto& object : listObjectsResult.GetContents()) {\n+      Aws::String s = object.GetKey();\n+      Aws::String entry = s.substr(strlen(prefix.c_str()));\n+      if (entry.length() > 0) {\n+        result->push_back(entry.c_str());\n+      }\n+    }\n+    listObjectsRequest.SetMarker(listObjectsResult.GetNextMarker());\n+  } while (listObjectsResult.GetIsTruncated());\n+\n+  return Status::OK();\n+}\n+\n+Status S3FileSystem::Stat(const string& fname, FileStatistics* stats) {\n+  string bucket, object;\n+  TF_RETURN_IF_ERROR(ParseS3Path(fname, true, &bucket, &object));\n+\n+  Aws::S3::S3Client s3Client(GetDefaultClientConfig());\n+  if (object.empty()) {\n+    Aws::S3::Model::HeadBucketRequest headBucketRequest;\n+    headBucketRequest.WithBucket(bucket.c_str());\n+    auto headBucketOutcome = s3Client.HeadBucket(headBucketRequest);\n+    if (!headBucketOutcome.IsSuccess()) {\n+      std::stringstream ss;\n+      ss << headBucketOutcome.GetError().GetExceptionName() << \": \"", "path": "tensorflow/contrib/s3/s3_file_system.cc", "position": null, "original_position": 374, "commit_id": "2babd181e9899907f5dd018cc1ca84662c650c26", "original_commit_id": "bb65ec0023eb2264f8c24338d667820f47cd3749", "user": {"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}, "body": "strings::StrCat", "created_at": "2017-08-15T22:21:06Z", "updated_at": "2017-09-15T01:53:03Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11089#discussion_r133318821", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11089", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133318821"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11089#discussion_r133318821"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11089"}}, "body_html": "<p>strings::StrCat</p>", "body_text": "strings::StrCat"}