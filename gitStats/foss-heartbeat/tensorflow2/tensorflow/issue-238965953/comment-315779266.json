{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/315779266", "html_url": "https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-315779266", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11089", "id": 315779266, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTc3OTI2Ng==", "user": {"login": "sswv", "id": 529391, "node_id": "MDQ6VXNlcjUyOTM5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/529391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sswv", "html_url": "https://github.com/sswv", "followers_url": "https://api.github.com/users/sswv/followers", "following_url": "https://api.github.com/users/sswv/following{/other_user}", "gists_url": "https://api.github.com/users/sswv/gists{/gist_id}", "starred_url": "https://api.github.com/users/sswv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sswv/subscriptions", "organizations_url": "https://api.github.com/users/sswv/orgs", "repos_url": "https://api.github.com/users/sswv/repos", "events_url": "https://api.github.com/users/sswv/events{/privacy}", "received_events_url": "https://api.github.com/users/sswv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-17T14:53:03Z", "updated_at": "2017-07-17T14:53:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6932348\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yongtang\">@yongtang</a>,</p>\n<p>I wrote some small patches to allow running the S3 component with 3rd-party S3 implementations, and I also fixed a potential bug. Please see <a href=\"https://github.com/sswv/tensorflow/commits/s3-patch\">my branch</a>. Note that these patches are all simple but not mature solutions, so I did not make them as pull requests. You can refer to them or pick up code from them, and provide more complete solutions based on my patches.</p>\n<ul>\n<li><a href=\"https://github.com/sswv/tensorflow/commit/d3eb70e56fd1c590245414f628e168168339ec56\">Patch 1</a>:</li>\n</ul>\n<p>Get S3 configurations from environment variables to support 3rd-party S3 implementations. Do you think it is a good way to get configurations from environment variables? If not, we can choose a better way. More configuration items of <code>Aws::Client::ClientConfiguration</code> might be added.</p>\n<ul>\n<li><a href=\"https://github.com/sswv/tensorflow/commit/a89294358662d0f82a1f51530f016ae3e353b18e\">Patch 2</a>:</li>\n</ul>\n<p>Use old <code>ListObject</code> (V1) instead of <code>ListObjectV2</code> to support some 3rd-party S3 implementations, especially legacy systems. A better way is adding a switch to allow users choosing from V1 and V2.</p>\n<ul>\n<li><a href=\"https://github.com/sswv/tensorflow/commit/c3f28c9cd233ecf1923dfaa435b069acf8937dc1\">Patch 3</a>:</li>\n</ul>\n<p>A workaround for the \"Invalid Range Error\" when reading ImageNet data set. When I train the <code>inception_v3</code> network from <code>tensorflow/models/slim</code> by using the ImageNet data set, I see an \"Invalid Range Error\" after 120 steps and the program crashes immediately. It happens only with the S3 file system, but not happen with local file system and HDFS. I can reproduce it with 2 different kinds of 3rd-party S3 implementations. After some debugging, I found it is because of <code>offset == size of file</code> in <code>S3RandomAccessFile::Read</code>. In this case, <code>s3Client.GetObject</code> will report the \"Invalid Range Error\".</p>\n<p>Refer to the <code>HDFSRandomAccessFile::Read</code> function in <code>hadoop_file_system.cc</code>. It has a more elegant behavior when out-of-range error happens. For the HDFS case, A non-fatal <code>error::OUT_OF_RANGE</code> status will be returned to <code>TFRecordReader::ReadLocked</code> when out-of-range error happens, and the program will not crash.</p>\n<p>Thus, I made a workaround patch. It works well in my simple tests. However, I am not a S3 expert, so I am not sure whether my solution is correct and good enough. I think you can find a more reasonable way to fix the issue.</p>", "body_text": "Hi, @yongtang,\nI wrote some small patches to allow running the S3 component with 3rd-party S3 implementations, and I also fixed a potential bug. Please see my branch. Note that these patches are all simple but not mature solutions, so I did not make them as pull requests. You can refer to them or pick up code from them, and provide more complete solutions based on my patches.\n\nPatch 1:\n\nGet S3 configurations from environment variables to support 3rd-party S3 implementations. Do you think it is a good way to get configurations from environment variables? If not, we can choose a better way. More configuration items of Aws::Client::ClientConfiguration might be added.\n\nPatch 2:\n\nUse old ListObject (V1) instead of ListObjectV2 to support some 3rd-party S3 implementations, especially legacy systems. A better way is adding a switch to allow users choosing from V1 and V2.\n\nPatch 3:\n\nA workaround for the \"Invalid Range Error\" when reading ImageNet data set. When I train the inception_v3 network from tensorflow/models/slim by using the ImageNet data set, I see an \"Invalid Range Error\" after 120 steps and the program crashes immediately. It happens only with the S3 file system, but not happen with local file system and HDFS. I can reproduce it with 2 different kinds of 3rd-party S3 implementations. After some debugging, I found it is because of offset == size of file in S3RandomAccessFile::Read. In this case, s3Client.GetObject will report the \"Invalid Range Error\".\nRefer to the HDFSRandomAccessFile::Read function in hadoop_file_system.cc. It has a more elegant behavior when out-of-range error happens. For the HDFS case, A non-fatal error::OUT_OF_RANGE status will be returned to TFRecordReader::ReadLocked when out-of-range error happens, and the program will not crash.\nThus, I made a workaround patch. It works well in my simple tests. However, I am not a S3 expert, so I am not sure whether my solution is correct and good enough. I think you can find a more reasonable way to fix the issue.", "body": "Hi, @yongtang,\r\n\r\nI wrote some small patches to allow running the S3 component with 3rd-party S3 implementations, and I also fixed a potential bug. Please see [my branch](https://github.com/sswv/tensorflow/commits/s3-patch). Note that these patches are all simple but not mature solutions, so I did not make them as pull requests. You can refer to them or pick up code from them, and provide more complete solutions based on my patches.\r\n\r\n* [Patch 1](https://github.com/sswv/tensorflow/commit/d3eb70e56fd1c590245414f628e168168339ec56):\r\n\r\nGet S3 configurations from environment variables to support 3rd-party S3 implementations. Do you think it is a good way to get configurations from environment variables? If not, we can choose a better way. More configuration items of `Aws::Client::ClientConfiguration` might be added.\r\n\r\n* [Patch 2](https://github.com/sswv/tensorflow/commit/a89294358662d0f82a1f51530f016ae3e353b18e):\r\n\r\nUse old `ListObject` (V1) instead of `ListObjectV2` to support some 3rd-party S3 implementations, especially legacy systems. A better way is adding a switch to allow users choosing from V1 and V2.\r\n\r\n* [Patch 3](https://github.com/sswv/tensorflow/commit/c3f28c9cd233ecf1923dfaa435b069acf8937dc1):\r\n\r\nA workaround for the \"Invalid Range Error\" when reading ImageNet data set. When I train the `inception_v3` network from `tensorflow/models/slim` by using the ImageNet data set, I see an \"Invalid Range Error\" after 120 steps and the program crashes immediately. It happens only with the S3 file system, but not happen with local file system and HDFS. I can reproduce it with 2 different kinds of 3rd-party S3 implementations. After some debugging, I found it is because of `offset == size of file` in `S3RandomAccessFile::Read`. In this case, `s3Client.GetObject` will report the \"Invalid Range Error\".\r\n\r\nRefer to the `HDFSRandomAccessFile::Read` function in `hadoop_file_system.cc`. It has a more elegant behavior when out-of-range error happens. For the HDFS case, A non-fatal `error::OUT_OF_RANGE` status will be returned to `TFRecordReader::ReadLocked` when out-of-range error happens, and the program will not crash.\r\n\r\nThus, I made a workaround patch. It works well in my simple tests. However, I am not a S3 expert, so I am not sure whether my solution is correct and good enough. I think you can find a more reasonable way to fix the issue.\r\n"}