{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/127295812", "pull_request_review_id": 49866693, "id": 127295812, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNzI5NTgxMg==", "diff_hunk": "@@ -0,0 +1,546 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/core/lib/io/path.h\"\n+#include \"tensorflow/core/platform/env.h\"\n+\n+#include \"tensorflow/contrib/s3/s3_crypto.h\"\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/utils/FileSystemUtils.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/S3Errors.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+namespace tensorflow {\n+\n+static const char* S3FileSystemAllocationTag = \"S3FileSystemAllocation\";\n+static const size_t S3ReadAppendableFileBufferSize = 1024 * 1024;\n+static const int S3GetChildrenMaxKeys = 100;\n+\n+Status ParseS3Path(const string& fname, bool empty_object_ok, string* bucket,\n+                   string* object) {\n+  if (!bucket || !object) {\n+    return errors::Internal(\"bucket and object cannot be null.\");\n+  }\n+  StringPiece scheme, bucketp, objectp;\n+  io::ParseURI(fname, &scheme, &bucketp, &objectp);\n+  if (scheme != \"s3\") {\n+    return errors::InvalidArgument(\"S3 path doesn't start with 's3://': \",\n+                                   fname);\n+  }\n+  *bucket = bucketp.ToString();\n+  if (bucket->empty() || *bucket == \".\") {\n+    return errors::InvalidArgument(\"S3 path doesn't contain a bucket name: \",\n+                                   fname);\n+  }\n+  objectp.Consume(\"/\");\n+  *object = objectp.ToString();\n+  if (!empty_object_ok && object->empty()) {\n+    return errors::InvalidArgument(\"S3 path doesn't contain an object name: \",\n+                                   fname);\n+  }\n+  return Status::OK();\n+}\n+\n+class S3RandomAccessFile : public RandomAccessFile {\n+ public:\n+  S3RandomAccessFile(const string& bucket, const string& object)\n+      : bucket_(bucket), object_(object) {}\n+\n+  Status Read(uint64 offset, size_t n, StringPiece* result,\n+              char* scratch) const override {\n+    Aws::S3::S3Client s3Client;\n+    Aws::S3::Model::GetObjectRequest getObjectRequest;\n+    getObjectRequest.WithBucket(bucket_.c_str()).WithKey(object_.c_str());\n+    char buffer[50];\n+    memset(buffer, 0x00, sizeof(buffer));\n+    snprintf(buffer, sizeof(buffer) - 1, \"bytes=%lld-%lld\", offset,\n+             offset + n - 1);\n+    getObjectRequest.SetRange(buffer);\n+    getObjectRequest.SetResponseStreamFactory([]() {\n+      return Aws::New<Aws::StringStream>(S3FileSystemAllocationTag);\n+    });\n+    auto getObjectOutcome = s3Client.GetObject(getObjectRequest);\n+    if (!getObjectOutcome.IsSuccess()) {\n+      std::stringstream ss;\n+      ss << getObjectOutcome.GetError().GetExceptionName() << \": \"\n+         << getObjectOutcome.GetError().GetMessage();\n+      return errors::Internal(ss.str());\n+    }\n+    n = getObjectOutcome.GetResult().GetContentLength();\n+    std::stringstream ss;\n+    ss << getObjectOutcome.GetResult().GetBody().rdbuf();\n+    ss.read(scratch, n);\n+\n+    *result = StringPiece(scratch, n);\n+    return Status::OK();\n+  }\n+\n+ private:\n+  string bucket_;\n+  string object_;\n+};\n+\n+class S3WritableFile : public WritableFile {\n+ public:\n+  S3WritableFile(const string& bucket, const string& object)\n+      : bucket_(bucket),\n+        object_(object),\n+        sync_needed_(true),\n+        outfile_(Aws::MakeShared<Aws::Utils::TempFile>(\n+            S3FileSystemAllocationTag, \"/tmp/s3_filesystem_XXXXXX\",\n+            std::ios_base::binary | std::ios_base::trunc | std::ios_base::in |\n+                std::ios_base::out)) {}\n+\n+  Status Append(const StringPiece& data) override {\n+    if (!outfile_) {\n+      return errors::FailedPrecondition(\n+          \"The internal temporary file is not writable.\");\n+    }\n+    sync_needed_ = true;\n+    outfile_->write(data.data(), data.size());\n+    if (!outfile_->good()) {\n+      return errors::Internal(\n+          \"Could not append to the internal temporary file.\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status Close() override {\n+    if (outfile_) {\n+      TF_RETURN_IF_ERROR(Sync());\n+      outfile_.reset();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status Flush() override { return Sync(); }\n+\n+  Status Sync() override {\n+    if (!outfile_) {\n+      return errors::FailedPrecondition(\n+          \"The internal temporary file is not writable.\");\n+    }\n+    if (!sync_needed_) {\n+      return Status::OK();\n+    }\n+    Aws::Client::ClientConfiguration clientConfig;", "path": "tensorflow/contrib/s3/s3_file_system.cc", "position": null, "original_position": 145, "commit_id": "2babd181e9899907f5dd018cc1ca84662c650c26", "original_commit_id": "41db57724b08b7c36f44b0ec3986229af2d1fb97", "user": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "body": "We need to be able to change region etc", "created_at": "2017-07-13T18:31:42Z", "updated_at": "2017-09-15T01:53:03Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11089#discussion_r127295812", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11089", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/127295812"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11089#discussion_r127295812"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11089"}}, "body_html": "<p>We need to be able to change region etc</p>", "body_text": "We need to be able to change region etc"}