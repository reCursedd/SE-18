{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345554261", "html_url": "https://github.com/tensorflow/tensorflow/issues/6633#issuecomment-345554261", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6633", "id": 345554261, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTU1NDI2MQ==", "user": {"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-19T22:11:56Z", "updated_at": "2017-11-19T22:36:47Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5205204\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alquraishi\">@alquraishi</a></p>\n<p>The following conclusion isn't correct:</p>\n<blockquote>\n<p>if both the inputs and previous states are zero, then the output of an LSTM cell is also zero</p>\n</blockquote>\n<p>Each gate in CuDNN LSTM (which isn't the same as LSTM from wiki) has two bias terms -- which means with zero input and state, you get zero output/new state only if the biases are zero.</p>\n<p>As for:</p>\n<blockquote>\n<p>but AFAICT there's no speed difference for bidirectional LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN.</p>\n</blockquote>\n<p>I don't think so. CuDNN fused bidirectional multi-layer impl is much faster than if manually stacking concatenated outputs from each layer.</p>", "body_text": "@alquraishi\nThe following conclusion isn't correct:\n\nif both the inputs and previous states are zero, then the output of an LSTM cell is also zero\n\nEach gate in CuDNN LSTM (which isn't the same as LSTM from wiki) has two bias terms -- which means with zero input and state, you get zero output/new state only if the biases are zero.\nAs for:\n\nbut AFAICT there's no speed difference for bidirectional LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN.\n\nI don't think so. CuDNN fused bidirectional multi-layer impl is much faster than if manually stacking concatenated outputs from each layer.", "body": "@alquraishi \r\n\r\nThe following conclusion isn't correct:\r\n> if both the inputs and previous states are zero, then the output of an LSTM cell is also zero\r\n\r\nEach gate in CuDNN LSTM (which isn't the same as LSTM from wiki) has two bias terms -- which means with zero input and state, you get zero output/new state only if the biases are zero.\r\n\r\nAs for:\r\n>  but AFAICT there's no speed difference for bidirectional LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN.\r\n\r\nI don't think so. CuDNN fused bidirectional multi-layer impl is much faster than if manually stacking concatenated outputs from each layer."}