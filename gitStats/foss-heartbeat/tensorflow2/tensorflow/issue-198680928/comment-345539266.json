{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345539266", "html_url": "https://github.com/tensorflow/tensorflow/issues/6633#issuecomment-345539266", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6633", "id": 345539266, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTUzOTI2Ng==", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-19T18:40:35Z", "updated_at": "2017-11-19T18:41:48Z", "author_association": "NONE", "body_html": "<p>One question / comment about supporting the correct zero-padding semantics for variable lengths within a batch. It appears to me that if one assumes all zeros for the initial hidden and cell states, which is the default behavior (the user must explicitly pass in an initial state tensor to change this), then the fix to get the correct semantics is rather simple. That's because, looking at the math for LSTMs, if both the inputs and previous states are zero, then the output of an LSTM cell is also zero. And so as long as the initial state is zero for the backward direction, then all the padding zeros encountered before the real sequence begins will simply be ignored. The only issue is that the forward direction will carry on some residual signal even after it encounters the zero padding. Thus, the solution would be to zero out all the outputs of the LSTM that correspond to the zero padding regions, before feeding these outputs to the next layer in a multi-layer stack. This precludes using the native cuDNN functionality for multiple layers, but AFAICT there's no speed difference for <em>bidirectional</em> LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN. So this seems like a much simpler and cheaper solution than using <code>tf.reverse_sequence</code>, etc. The only limitation is that it would prevent one from having an initial state other than all zeros.</p>", "body_text": "One question / comment about supporting the correct zero-padding semantics for variable lengths within a batch. It appears to me that if one assumes all zeros for the initial hidden and cell states, which is the default behavior (the user must explicitly pass in an initial state tensor to change this), then the fix to get the correct semantics is rather simple. That's because, looking at the math for LSTMs, if both the inputs and previous states are zero, then the output of an LSTM cell is also zero. And so as long as the initial state is zero for the backward direction, then all the padding zeros encountered before the real sequence begins will simply be ignored. The only issue is that the forward direction will carry on some residual signal even after it encounters the zero padding. Thus, the solution would be to zero out all the outputs of the LSTM that correspond to the zero padding regions, before feeding these outputs to the next layer in a multi-layer stack. This precludes using the native cuDNN functionality for multiple layers, but AFAICT there's no speed difference for bidirectional LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN. So this seems like a much simpler and cheaper solution than using tf.reverse_sequence, etc. The only limitation is that it would prevent one from having an initial state other than all zeros.", "body": "One question / comment about supporting the correct zero-padding semantics for variable lengths within a batch. It appears to me that if one assumes all zeros for the initial hidden and cell states, which is the default behavior (the user must explicitly pass in an initial state tensor to change this), then the fix to get the correct semantics is rather simple. That's because, looking at the math for LSTMs, if both the inputs and previous states are zero, then the output of an LSTM cell is also zero. And so as long as the initial state is zero for the backward direction, then all the padding zeros encountered before the real sequence begins will simply be ignored. The only issue is that the forward direction will carry on some residual signal even after it encounters the zero padding. Thus, the solution would be to zero out all the outputs of the LSTM that correspond to the zero padding regions, before feeding these outputs to the next layer in a multi-layer stack. This precludes using the native cuDNN functionality for multiple layers, but AFAICT there's no speed difference for _bidirectional_ LSTMs between manually constructing multiple layers, and using the native functionality in cuDNN. So this seems like a much simpler and cheaper solution than using `tf.reverse_sequence`, etc. The only limitation is that it would prevent one from having an initial state other than all zeros."}