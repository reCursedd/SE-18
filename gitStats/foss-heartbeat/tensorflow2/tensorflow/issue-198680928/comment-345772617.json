{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345772617", "html_url": "https://github.com/tensorflow/tensorflow/issues/6633#issuecomment-345772617", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6633", "id": 345772617, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTc3MjYxNw==", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-20T17:44:53Z", "updated_at": "2017-11-21T17:37:32Z", "author_association": "NONE", "body_html": "<p>I went ahead and implemented a corrected (hopefully for real this time) version by splitting the two directions and then using <code>tf.reverse_sequence</code> on the inputs and outputs of the backward direction. This appears to work correctly with more sensitive tests that I now have that fail with the previous incorrect approach. Surprisingly, the performance penalty appears non-existent! On a fairly bare bones model with two bidirectional layers with 800 units for each direction, and a maximum sequence length of around 1,000, I'm seeing absolutely no difference in speed between the fused bidirectional cuDNN op and this split approach (in fact the latter is 5% faster, but that's likely noise). This is on a system with Titan X Pascal, running on TF 1.4, Ubuntu 14.04, CUDA 8, and cuDNN 7 (compiled from source.) The tests were done on a fully loaded system, and are an average of separate 10 runs, each of which consumed 100 batches, so the numbers should be trustworthy.</p>\n<p>UPDATE: With other model configurations, I am seeing around 20% speed penalty for using this approach. This is not because the multilayer stacking is being done in TF--I still don't see any appreciable speed loss there. The slowdown is due to splitting the two directions and then doing the reversal before and after.</p>", "body_text": "I went ahead and implemented a corrected (hopefully for real this time) version by splitting the two directions and then using tf.reverse_sequence on the inputs and outputs of the backward direction. This appears to work correctly with more sensitive tests that I now have that fail with the previous incorrect approach. Surprisingly, the performance penalty appears non-existent! On a fairly bare bones model with two bidirectional layers with 800 units for each direction, and a maximum sequence length of around 1,000, I'm seeing absolutely no difference in speed between the fused bidirectional cuDNN op and this split approach (in fact the latter is 5% faster, but that's likely noise). This is on a system with Titan X Pascal, running on TF 1.4, Ubuntu 14.04, CUDA 8, and cuDNN 7 (compiled from source.) The tests were done on a fully loaded system, and are an average of separate 10 runs, each of which consumed 100 batches, so the numbers should be trustworthy.\nUPDATE: With other model configurations, I am seeing around 20% speed penalty for using this approach. This is not because the multilayer stacking is being done in TF--I still don't see any appreciable speed loss there. The slowdown is due to splitting the two directions and then doing the reversal before and after.", "body": "I went ahead and implemented a corrected (hopefully for real this time) version by splitting the two directions and then using `tf.reverse_sequence` on the inputs and outputs of the backward direction. This appears to work correctly with more sensitive tests that I now have that fail with the previous incorrect approach. Surprisingly, the performance penalty appears non-existent! On a fairly bare bones model with two bidirectional layers with 800 units for each direction, and a maximum sequence length of around 1,000, I'm seeing absolutely no difference in speed between the fused bidirectional cuDNN op and this split approach (in fact the latter is 5% faster, but that's likely noise). This is on a system with Titan X Pascal, running on TF 1.4, Ubuntu 14.04, CUDA 8, and cuDNN 7 (compiled from source.) The tests were done on a fully loaded system, and are an average of separate 10 runs, each of which consumed 100 batches, so the numbers should be trustworthy.\r\n\r\nUPDATE: With other model configurations, I am seeing around 20% speed penalty for using this approach. This is not because the multilayer stacking is being done in TF--I still don't see any appreciable speed loss there. The slowdown is due to splitting the two directions and then doing the reversal before and after."}