{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13090", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13090/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13090/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13090/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13090", "id": 258267048, "node_id": "MDU6SXNzdWUyNTgyNjcwNDg=", "number": 13090, "title": "Session creation silently failing on iOS when loading a SavedModel", "user": {"login": "justinshapiro", "id": 14143046, "node_id": "MDQ6VXNlcjE0MTQzMDQ2", "avatar_url": "https://avatars2.githubusercontent.com/u/14143046?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justinshapiro", "html_url": "https://github.com/justinshapiro", "followers_url": "https://api.github.com/users/justinshapiro/followers", "following_url": "https://api.github.com/users/justinshapiro/following{/other_user}", "gists_url": "https://api.github.com/users/justinshapiro/gists{/gist_id}", "starred_url": "https://api.github.com/users/justinshapiro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justinshapiro/subscriptions", "organizations_url": "https://api.github.com/users/justinshapiro/orgs", "repos_url": "https://api.github.com/users/justinshapiro/repos", "events_url": "https://api.github.com/users/justinshapiro/events{/privacy}", "received_events_url": "https://api.github.com/users/justinshapiro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-17T00:01:57Z", "updated_at": "2017-09-18T15:47:05Z", "closed_at": "2017-09-18T15:47:05Z", "author_association": "NONE", "body_html": "<h2>Issue:</h2>\n<p>I am trying to integrate a TensorFlow solution into my iOS apps, but inference doesn't seem to work when I try to run simple graphs created in Python. In fact, the <code>tensorflow_inception_graph</code> from the examples is the only graph that seems to work with iOS inference. Every other inference attempt is met with the following error:</p>\n<blockquote>\n<p>Invalid argument: Session was not created with a graph before Run()!</p>\n</blockquote>\n<p>So what I'm finding is that on mobile if we try to run a canned neural network like the <code>tensorflow_inception_graph</code>, inference works perfectly but if we try to run any kind of custom model like 1 + 1 = 2, the graph won't run.</p>\n<p>To demonstrate this, I used Python to write and export to a <code>SavedModel</code> the simplest graph I can think of: it just adds 1 + 1:</p>\n<pre><code>g = tf.Graph()\nwith g.as_default():\n    output = tf.add(tf.constant(1), 1, name=\"output_tensor\")\n\nbuilder = tf.saved_model.builder.SavedModelBuilder('/path/to/export')\nwith tf.Session(graph=g) as sess:\n    builder.add_meta_graph_and_variables(sess, tags=[])\n\nbuilder.save()\n</code></pre>\n<p>Now to test that inference is actually possible, I reload the <code>saved_model.pb</code> into Python:</p>\n<pre><code>with tf.Session() as sess:\n    tf.saved_model.loader.load(sess, [], '/path/to/export')\n\n    print(sess.run('output_tensor:0')) // prints 2\n</code></pre>\n<p>Following in the footsteps of the <code>simple</code> example located at <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios</a>, I go to perform inference on iOS using the static C++ library built using the makefile and the instructions provided at the link.</p>\n<pre><code>using namespace tensorflow;\nusing namespace std;\nusing namespace ::google::protobuf;\nusing namespace ::google::protobuf::io;\n\nSessionOptions options;\nSession* session_pointer = nullptr;\nStatus session_status = NewSession(options, &amp;session_pointer);\n\ncout &lt;&lt; session_status.ToString() &lt;&lt; endl; // prints OK\n\nunique_ptr&lt;Session&gt; session(session_pointer);\n    \nGraphDef model_graph;\nNSString* model_path = FilePathForResourceName(@\"saved_model\", @\"pb\");\nPortableReadFileToProto([model_path UTF8String], &amp;model_graph);\n    \nStatus session_init = session-&gt;Create(model_graph);\n\ncout &lt;&lt; session_init.ToString() &lt;&lt; endl; // prints OK, proves the graph was indeed created before run\n\nvector&lt;Tensor&gt; outputs;\nStatus session_run = session-&gt;Run({}, {\"output_tensor:0\"}, {}, &amp;outputs);\n\ncout &lt;&lt; session_run.ToString() &lt;&lt; endl; // prints Invalid argument: Session was not created with a graph before Run()!\n</code></pre>\n<h2>Troubleshooting:</h2>\n<p>I've tried rebuilding multiple times from <code>1.3.0</code> (latest, nightly), <code>1.2.1</code> and tried using the <code>TensorFlow-experimental</code> Pod. (I also tried using the <code>TensorFlow-iOS</code> pod, but it seems to be empty.)</p>\n<p>There are (unanswered) Stack Overflow questions that refer to this issue occurring on both platforms:</p>\n<ul>\n<li>\n<p><strong>Android</strong>: <a href=\"https://stackoverflow.com/questions/41252122/tensorflow-on-android-error-during-inference-invalid-argument-session-was-not\" rel=\"nofollow\">https://stackoverflow.com/questions/41252122/tensorflow-on-android-error-during-inference-invalid-argument-session-was-not</a></p>\n</li>\n<li>\n<p><strong>iOS</strong>: <a href=\"https://stackoverflow.com/questions/46201109/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c\" rel=\"nofollow\">https://stackoverflow.com/questions/46201109/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c</a></p>\n</li>\n</ul>\n<p>On GitHub, there's been several issues related to this reported over the last year: <a href=\"https://github.com/tensorflow/tensorflow/issues/7088\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7088/hovercard\">#7088</a>, <a href=\"https://github.com/tensorflow/tensorflow/issues/5553\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5553/hovercard\">#5553</a>, <a href=\"https://github.com/tensorflow/tensorflow/issues/3480\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3480/hovercard\">#3480</a>, <a href=\"https://github.com/tensorflow/tensorflow/issues/6806\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6806/hovercard\">#6806</a>, and <a href=\"https://github.com/tensorflow/tensorflow/issues/3352\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3352/hovercard\">#3352</a>. None of the resolutions (if any) for these issues provide a concrete answer of how to get around this problem.</p>\n<p>However, found in the comments of <a href=\"https://github.com/tensorflow/tensorflow/issues/3480\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3480/hovercard\">#3480</a> , <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a>'s <a href=\"https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/\" rel=\"nofollow\">post</a> about using <code>optimize_for_inference</code>, <code>quantize_graph</code>, and <code>convert_graphdef_memmapped_format</code> on the model before running inference present a possible solution. So I ran those three commands and got the same error:</p>\n<blockquote>\n<p>google.protobuf.message.DecodeError: Truncated message.</p>\n</blockquote>\n<p>Since we can perform successful inference on the model in Python (as shown above), the possibility of the model being corrupt (as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a> suggested in <a href=\"https://github.com/tensorflow/tensorflow/issues/3002\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3002/hovercard\">#3002</a>) is ruled out. Therefore, the above <em>\"Truncated Error\"</em> message may point to where the problem is on mobile: Unsuccessful parsing of general models stored in protobuf files.</p>\n<h2>Closing thoughts on issue:</h2>\n<p>The point is this: The error message <em>\"Invalid argument: Session was not created with a graph before Run()!\"</em> doesn't provide any information to me regarding what should be debugged in my implementation. Additionally, it appears to not even point to the source of the error as I have verified three things:</p>\n<ol>\n<li>All arguments passed to the session and graph are valid (<em>Invalid Argument</em>)</li>\n<li>Session was created with a graph before Run() and the <code>Status</code> was reported to be <code>OK</code> (<em>Session was not created with a graph before Run()!</em>)</li>\n<li>The error is likely triggered due to unsuccessful parsing on mobile devices of custom models serialized in protobuf files</li>\n</ol>\n<h2>Library Versions:</h2>\n<p><strong>TensorFlow Python version</strong>: <code>('v1.3.0-rc2-20-g0787eee', '1.3.0')</code><br>\n<strong>TensorFlow C++ version</strong>: 1.3.0, built using <code>build_all_ios.sh</code>. Also tried with version 1.2.1 and <code>TensorFlow-experimental</code> pod</p>\n<h2>System information:</h2>\n<pre><code>== cat /etc/issue ===============================================\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\nMac OS X 10.12.4\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nApple LLVM version 8.1.0 (clang-802.0.42)\nTarget: x86_64-apple-darwin16.5.0\nThread model: posix\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\n\n== uname -a =====================================================\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\n\n== check pips ===================================================\nnumpy (1.13.1)\nprotobuf (3.1.0.post1)\ntensorflow (1.3.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.3.0\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\n./tf.sh: line 105: nvidia-smi: command not found\n\n== cuda libs  ===================================================\n</code></pre>", "body_text": "Issue:\nI am trying to integrate a TensorFlow solution into my iOS apps, but inference doesn't seem to work when I try to run simple graphs created in Python. In fact, the tensorflow_inception_graph from the examples is the only graph that seems to work with iOS inference. Every other inference attempt is met with the following error:\n\nInvalid argument: Session was not created with a graph before Run()!\n\nSo what I'm finding is that on mobile if we try to run a canned neural network like the tensorflow_inception_graph, inference works perfectly but if we try to run any kind of custom model like 1 + 1 = 2, the graph won't run.\nTo demonstrate this, I used Python to write and export to a SavedModel the simplest graph I can think of: it just adds 1 + 1:\ng = tf.Graph()\nwith g.as_default():\n    output = tf.add(tf.constant(1), 1, name=\"output_tensor\")\n\nbuilder = tf.saved_model.builder.SavedModelBuilder('/path/to/export')\nwith tf.Session(graph=g) as sess:\n    builder.add_meta_graph_and_variables(sess, tags=[])\n\nbuilder.save()\n\nNow to test that inference is actually possible, I reload the saved_model.pb into Python:\nwith tf.Session() as sess:\n    tf.saved_model.loader.load(sess, [], '/path/to/export')\n\n    print(sess.run('output_tensor:0')) // prints 2\n\nFollowing in the footsteps of the simple example located at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios, I go to perform inference on iOS using the static C++ library built using the makefile and the instructions provided at the link.\nusing namespace tensorflow;\nusing namespace std;\nusing namespace ::google::protobuf;\nusing namespace ::google::protobuf::io;\n\nSessionOptions options;\nSession* session_pointer = nullptr;\nStatus session_status = NewSession(options, &session_pointer);\n\ncout << session_status.ToString() << endl; // prints OK\n\nunique_ptr<Session> session(session_pointer);\n    \nGraphDef model_graph;\nNSString* model_path = FilePathForResourceName(@\"saved_model\", @\"pb\");\nPortableReadFileToProto([model_path UTF8String], &model_graph);\n    \nStatus session_init = session->Create(model_graph);\n\ncout << session_init.ToString() << endl; // prints OK, proves the graph was indeed created before run\n\nvector<Tensor> outputs;\nStatus session_run = session->Run({}, {\"output_tensor:0\"}, {}, &outputs);\n\ncout << session_run.ToString() << endl; // prints Invalid argument: Session was not created with a graph before Run()!\n\nTroubleshooting:\nI've tried rebuilding multiple times from 1.3.0 (latest, nightly), 1.2.1 and tried using the TensorFlow-experimental Pod. (I also tried using the TensorFlow-iOS pod, but it seems to be empty.)\nThere are (unanswered) Stack Overflow questions that refer to this issue occurring on both platforms:\n\n\nAndroid: https://stackoverflow.com/questions/41252122/tensorflow-on-android-error-during-inference-invalid-argument-session-was-not\n\n\niOS: https://stackoverflow.com/questions/46201109/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c\n\n\nOn GitHub, there's been several issues related to this reported over the last year: #7088, #5553, #3480, #6806, and #3352. None of the resolutions (if any) for these issues provide a concrete answer of how to get around this problem.\nHowever, found in the comments of #3480 , @petewarden's post about using optimize_for_inference, quantize_graph, and convert_graphdef_memmapped_format on the model before running inference present a possible solution. So I ran those three commands and got the same error:\n\ngoogle.protobuf.message.DecodeError: Truncated message.\n\nSince we can perform successful inference on the model in Python (as shown above), the possibility of the model being corrupt (as @petewarden suggested in #3002) is ruled out. Therefore, the above \"Truncated Error\" message may point to where the problem is on mobile: Unsuccessful parsing of general models stored in protobuf files.\nClosing thoughts on issue:\nThe point is this: The error message \"Invalid argument: Session was not created with a graph before Run()!\" doesn't provide any information to me regarding what should be debugged in my implementation. Additionally, it appears to not even point to the source of the error as I have verified three things:\n\nAll arguments passed to the session and graph are valid (Invalid Argument)\nSession was created with a graph before Run() and the Status was reported to be OK (Session was not created with a graph before Run()!)\nThe error is likely triggered due to unsuccessful parsing on mobile devices of custom models serialized in protobuf files\n\nLibrary Versions:\nTensorFlow Python version: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\nTensorFlow C++ version: 1.3.0, built using build_all_ios.sh. Also tried with version 1.2.1 and TensorFlow-experimental pod\nSystem information:\n== cat /etc/issue ===============================================\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\nMac OS X 10.12.4\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nApple LLVM version 8.1.0 (clang-802.0.42)\nTarget: x86_64-apple-darwin16.5.0\nThread model: posix\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\n\n== uname -a =====================================================\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\n\n== check pips ===================================================\nnumpy (1.13.1)\nprotobuf (3.1.0.post1)\ntensorflow (1.3.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.3.0\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\n./tf.sh: line 105: nvidia-smi: command not found\n\n== cuda libs  ===================================================", "body": "## Issue:\r\n\r\nI am trying to integrate a TensorFlow solution into my iOS apps, but inference doesn't seem to work when I try to run simple graphs created in Python. In fact, the `tensorflow_inception_graph` from the examples is the only graph that seems to work with iOS inference. Every other inference attempt is met with the following error:\r\n\r\n> Invalid argument: Session was not created with a graph before Run()!\r\n\r\nSo what I'm finding is that on mobile if we try to run a canned neural network like the `tensorflow_inception_graph`, inference works perfectly but if we try to run any kind of custom model like 1 + 1 = 2, the graph won't run.\r\n\r\nTo demonstrate this, I used Python to write and export to a `SavedModel` the simplest graph I can think of: it just adds 1 + 1:\r\n\r\n```\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    output = tf.add(tf.constant(1), 1, name=\"output_tensor\")\r\n\r\nbuilder = tf.saved_model.builder.SavedModelBuilder('/path/to/export')\r\nwith tf.Session(graph=g) as sess:\r\n    builder.add_meta_graph_and_variables(sess, tags=[])\r\n\r\nbuilder.save()\r\n```\r\n\r\nNow to test that inference is actually possible, I reload the `saved_model.pb` into Python:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    tf.saved_model.loader.load(sess, [], '/path/to/export')\r\n\r\n    print(sess.run('output_tensor:0')) // prints 2\r\n```\r\n\r\nFollowing in the footsteps of the `simple` example located at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios, I go to perform inference on iOS using the static C++ library built using the makefile and the instructions provided at the link.\r\n\r\n```\r\nusing namespace tensorflow;\r\nusing namespace std;\r\nusing namespace ::google::protobuf;\r\nusing namespace ::google::protobuf::io;\r\n\r\nSessionOptions options;\r\nSession* session_pointer = nullptr;\r\nStatus session_status = NewSession(options, &session_pointer);\r\n\r\ncout << session_status.ToString() << endl; // prints OK\r\n\r\nunique_ptr<Session> session(session_pointer);\r\n    \r\nGraphDef model_graph;\r\nNSString* model_path = FilePathForResourceName(@\"saved_model\", @\"pb\");\r\nPortableReadFileToProto([model_path UTF8String], &model_graph);\r\n    \r\nStatus session_init = session->Create(model_graph);\r\n\r\ncout << session_init.ToString() << endl; // prints OK, proves the graph was indeed created before run\r\n\r\nvector<Tensor> outputs;\r\nStatus session_run = session->Run({}, {\"output_tensor:0\"}, {}, &outputs);\r\n\r\ncout << session_run.ToString() << endl; // prints Invalid argument: Session was not created with a graph before Run()!\r\n```\r\n\r\n## Troubleshooting:\r\n\r\nI've tried rebuilding multiple times from `1.3.0` (latest, nightly), `1.2.1` and tried using the `TensorFlow-experimental` Pod. (I also tried using the `TensorFlow-iOS` pod, but it seems to be empty.)\r\n\r\nThere are (unanswered) Stack Overflow questions that refer to this issue occurring on both platforms: \r\n\r\n- **Android**: https://stackoverflow.com/questions/41252122/tensorflow-on-android-error-during-inference-invalid-argument-session-was-not\r\n\r\n- **iOS**: https://stackoverflow.com/questions/46201109/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c\r\n\r\nOn GitHub, there's been several issues related to this reported over the last year: [#7088](https://github.com/tensorflow/tensorflow/issues/7088), [#5553](https://github.com/tensorflow/tensorflow/issues/5553), [#3480](https://github.com/tensorflow/tensorflow/issues/3480), [#6806](https://github.com/tensorflow/tensorflow/issues/6806), and [#3352](https://github.com/tensorflow/tensorflow/issues/3352). None of the resolutions (if any) for these issues provide a concrete answer of how to get around this problem. \r\n\r\nHowever, found in the comments of [#3480](https://github.com/tensorflow/tensorflow/issues/3480) , @petewarden's [post](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/) about using `optimize_for_inference`, `quantize_graph`, and `convert_graphdef_memmapped_format` on the model before running inference present a possible solution. So I ran those three commands and got the same error:\r\n\r\n> google.protobuf.message.DecodeError: Truncated message.\r\n\r\nSince we can perform successful inference on the model in Python (as shown above), the possibility of the model being corrupt (as @petewarden suggested in [#3002](https://github.com/tensorflow/tensorflow/issues/3002)) is ruled out. Therefore, the above _\"Truncated Error\"_ message may point to where the problem is on mobile: Unsuccessful parsing of general models stored in protobuf files.\r\n\r\n## Closing thoughts on issue:\r\nThe point is this: The error message _\"Invalid argument: Session was not created with a graph before Run()!\"_ doesn't provide any information to me regarding what should be debugged in my implementation. Additionally, it appears to not even point to the source of the error as I have verified three things:\r\n\r\n1. All arguments passed to the session and graph are valid (_Invalid Argument_)\r\n2. Session was created with a graph before Run() and the `Status` was reported to be `OK` (_Session was not created with a graph before Run()!_)\r\n3. The error is likely triggered due to unsuccessful parsing on mobile devices of custom models serialized in protobuf files\r\n\r\n## Library Versions:\r\n**TensorFlow Python version**: `('v1.3.0-rc2-20-g0787eee', '1.3.0')`\r\n**TensorFlow C++ version**: 1.3.0, built using `build_all_ios.sh`. Also tried with version 1.2.1 and `TensorFlow-experimental` pod\r\n\r\n## System information:\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.1.0.post1)\r\ntensorflow (1.3.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n"}