{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289991829", "html_url": "https://github.com/tensorflow/tensorflow/issues/8550#issuecomment-289991829", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8550", "id": 289991829, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTk5MTgyOQ==", "user": {"login": "BrianOn99", "id": 8319689, "node_id": "MDQ6VXNlcjgzMTk2ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/8319689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BrianOn99", "html_url": "https://github.com/BrianOn99", "followers_url": "https://api.github.com/users/BrianOn99/followers", "following_url": "https://api.github.com/users/BrianOn99/following{/other_user}", "gists_url": "https://api.github.com/users/BrianOn99/gists{/gist_id}", "starred_url": "https://api.github.com/users/BrianOn99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BrianOn99/subscriptions", "organizations_url": "https://api.github.com/users/BrianOn99/orgs", "repos_url": "https://api.github.com/users/BrianOn99/repos", "events_url": "https://api.github.com/users/BrianOn99/events{/privacy}", "received_events_url": "https://api.github.com/users/BrianOn99/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-29T06:04:42Z", "updated_at": "2017-03-29T09:22:41Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a> Thanks for the help from tensorflowers.  I think I have isolated the problem.  The root of the <code>NaN</code> comes from the models.  For example inspecting the magenta official pretrained <code>multistyle-pastiche-generator-varied.ckpt</code> from <a href=\"http://download.magenta.tensorflow.org/models/multistyle-pastiche-generator-varied.ckpt\" rel=\"nofollow\">here</a> by inspect_checkpoint:</p>\n<pre><code>bazel run tensorflow/python/tools:inspect_checkpoint -- --tensor_name=transformer/contract/conv1/weights --file_name=/Users/brianchiu/repos/magenta/multistyle-pastiche-generator-varied.ckpt | less\n</code></pre>\n<p>Then you will see:</p>\n<blockquote>\n<p>tensor_name:  transformer/contract/conv1/weights<br>\n[[[[  2.50682741e-01  -5.05237103e-01  -1.81590185e-01 ...,<br>\n-1.78870350e-01  -2.17410363e-02              nan]<br>\n[  7.23770410e-02  -2.00065762e-01  -3.67106795e-02 ...,<br>\n1.28681719e-01  -1.13809042e-01              nan]<br>\n[ -3.85547698e-01   3.57045144e-01   2.51499917e-02 ...,<br>\n-1.22621819e-01  -1.01349823e-01              nan]]<br>\n[ I skipped other output here... ]</p>\n</blockquote>\n<p>Please notice the <code>nan</code>s.</p>\n<p>In the magenta model this weight will be fed to <code>transformer/contract/conv1/convolution</code>, then instance normalization, and then to a relu <code>transformer/contract/conv1/Relu</code>.</p>\n<p>In a python shell, the output of relu looks like this:</p>\n<pre><code>print(relu_out.shape, relu_out[0][0][0], sep=\"\\n\")\n\n(1, 256, 256, 32)\n[ 0.64510179  0.          0.          0.          0.          0.\n  0.09357953  0.85106254  0.          0.          0.          0.6241231\n  0.77822381  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          1.0667696   0.61502713  0.\n  0.33891544  0.          1.0223285   0.04408208  0.09238994  0.46202463\n  0.        ]\n</code></pre>\n<p>However, using the same image in android, the output of the relu is (only copied the first 32)</p>\n<pre><code>[0.51562506, 0.0, 0.0, 0.0, NaN, 0.0,\n0.03912145, 0.8153123, NaN, NaN, 0.0, 0.6545086,\n1.4677725, 0.0, 0.0, 0.0, 0.0, NaN, 0.0,\nNaN, NaN, 0.0, 0.0, 0.62697417, 0.6989541,\n0.068830535, NaN, 0.73944664, 0.0, 0.0, 0.746428, NaN]\n</code></pre>\n<p>The activations are different slightly probably due to different way to decode image.  The problem is in python interface the <code>nan</code> in weights from <code>transformer/contract/conv1/weights</code> does not propagate to the relu activation, but in android it does.</p>\n<p>May I know if it is expected to have <code>nan</code> in a checkpoint?  And why there is different behavior in android and in a PC with python interface?</p>", "body_text": "@andrewharp Thanks for the help from tensorflowers.  I think I have isolated the problem.  The root of the NaN comes from the models.  For example inspecting the magenta official pretrained multistyle-pastiche-generator-varied.ckpt from here by inspect_checkpoint:\nbazel run tensorflow/python/tools:inspect_checkpoint -- --tensor_name=transformer/contract/conv1/weights --file_name=/Users/brianchiu/repos/magenta/multistyle-pastiche-generator-varied.ckpt | less\n\nThen you will see:\n\ntensor_name:  transformer/contract/conv1/weights\n[[[[  2.50682741e-01  -5.05237103e-01  -1.81590185e-01 ...,\n-1.78870350e-01  -2.17410363e-02              nan]\n[  7.23770410e-02  -2.00065762e-01  -3.67106795e-02 ...,\n1.28681719e-01  -1.13809042e-01              nan]\n[ -3.85547698e-01   3.57045144e-01   2.51499917e-02 ...,\n-1.22621819e-01  -1.01349823e-01              nan]]\n[ I skipped other output here... ]\n\nPlease notice the nans.\nIn the magenta model this weight will be fed to transformer/contract/conv1/convolution, then instance normalization, and then to a relu transformer/contract/conv1/Relu.\nIn a python shell, the output of relu looks like this:\nprint(relu_out.shape, relu_out[0][0][0], sep=\"\\n\")\n\n(1, 256, 256, 32)\n[ 0.64510179  0.          0.          0.          0.          0.\n  0.09357953  0.85106254  0.          0.          0.          0.6241231\n  0.77822381  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          1.0667696   0.61502713  0.\n  0.33891544  0.          1.0223285   0.04408208  0.09238994  0.46202463\n  0.        ]\n\nHowever, using the same image in android, the output of the relu is (only copied the first 32)\n[0.51562506, 0.0, 0.0, 0.0, NaN, 0.0,\n0.03912145, 0.8153123, NaN, NaN, 0.0, 0.6545086,\n1.4677725, 0.0, 0.0, 0.0, 0.0, NaN, 0.0,\nNaN, NaN, 0.0, 0.0, 0.62697417, 0.6989541,\n0.068830535, NaN, 0.73944664, 0.0, 0.0, 0.746428, NaN]\n\nThe activations are different slightly probably due to different way to decode image.  The problem is in python interface the nan in weights from transformer/contract/conv1/weights does not propagate to the relu activation, but in android it does.\nMay I know if it is expected to have nan in a checkpoint?  And why there is different behavior in android and in a PC with python interface?", "body": "@andrewharp Thanks for the help from tensorflowers.  I think I have isolated the problem.  The root of the `NaN` comes from the models.  For example inspecting the magenta official pretrained `multistyle-pastiche-generator-varied.ckpt` from [here](http://download.magenta.tensorflow.org/models/multistyle-pastiche-generator-varied.ckpt) by inspect_checkpoint:\r\n\r\n```\r\nbazel run tensorflow/python/tools:inspect_checkpoint -- --tensor_name=transformer/contract/conv1/weights --file_name=/Users/brianchiu/repos/magenta/multistyle-pastiche-generator-varied.ckpt | less\r\n```\r\nThen you will see:\r\n> tensor_name:  transformer/contract/conv1/weights\r\n> [[[[  2.50682741e-01  -5.05237103e-01  -1.81590185e-01 ...,\r\n>     -1.78870350e-01  -2.17410363e-02              nan]\r\n>   [  7.23770410e-02  -2.00065762e-01  -3.67106795e-02 ...,\r\n>      1.28681719e-01  -1.13809042e-01              nan]\r\n>   [ -3.85547698e-01   3.57045144e-01   2.51499917e-02 ...,\r\n>     -1.22621819e-01  -1.01349823e-01              nan]]\r\n>  [ I skipped other output here... ]\r\n\r\nPlease notice the `nan`s.\r\n\r\nIn the magenta model this weight will be fed to `transformer/contract/conv1/convolution`, then instance normalization, and then to a relu `transformer/contract/conv1/Relu`.\r\n\r\nIn a python shell, the output of relu looks like this:\r\n```\r\nprint(relu_out.shape, relu_out[0][0][0], sep=\"\\n\")\r\n\r\n(1, 256, 256, 32)\r\n[ 0.64510179  0.          0.          0.          0.          0.\r\n  0.09357953  0.85106254  0.          0.          0.          0.6241231\r\n  0.77822381  0.          0.          0.          0.          0.          0.\r\n  0.          0.          0.          1.0667696   0.61502713  0.\r\n  0.33891544  0.          1.0223285   0.04408208  0.09238994  0.46202463\r\n  0.        ]\r\n```\r\n\r\nHowever, using the same image in android, the output of the relu is (only copied the first 32)\r\n```\r\n[0.51562506, 0.0, 0.0, 0.0, NaN, 0.0,\r\n0.03912145, 0.8153123, NaN, NaN, 0.0, 0.6545086,\r\n1.4677725, 0.0, 0.0, 0.0, 0.0, NaN, 0.0,\r\nNaN, NaN, 0.0, 0.0, 0.62697417, 0.6989541,\r\n0.068830535, NaN, 0.73944664, 0.0, 0.0, 0.746428, NaN]\r\n```\r\n\r\nThe activations are different slightly probably due to different way to decode image.  The problem is in python interface the `nan` in weights from `transformer/contract/conv1/weights` does not propagate to the relu activation, but in android it does.\r\n\r\nMay I know if it is expected to have `nan` in a checkpoint?  And why there is different behavior in android and in a PC with python interface?"}