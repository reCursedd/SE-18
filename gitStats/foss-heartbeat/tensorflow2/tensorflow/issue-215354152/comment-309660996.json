{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309660996", "html_url": "https://github.com/tensorflow/tensorflow/issues/8550#issuecomment-309660996", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8550", "id": 309660996, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTY2MDk5Ng==", "user": {"login": "futurely", "id": 9004594, "node_id": "MDQ6VXNlcjkwMDQ1OTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9004594?v=4", "gravatar_id": "", "url": "https://api.github.com/users/futurely", "html_url": "https://github.com/futurely", "followers_url": "https://api.github.com/users/futurely/followers", "following_url": "https://api.github.com/users/futurely/following{/other_user}", "gists_url": "https://api.github.com/users/futurely/gists{/gist_id}", "starred_url": "https://api.github.com/users/futurely/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/futurely/subscriptions", "organizations_url": "https://api.github.com/users/futurely/orgs", "repos_url": "https://api.github.com/users/futurely/repos", "events_url": "https://api.github.com/users/futurely/events{/privacy}", "received_events_url": "https://api.github.com/users/futurely/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-20T06:49:53Z", "updated_at": "2017-06-20T06:51:44Z", "author_association": "NONE", "body_html": "<p>This happens to some graphs generated with the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\">Graph Transform Tool</a> too.</p>\n<p>Using the commands detailed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"236835021\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/models/issues/1609\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/1609/hovercard\" href=\"https://github.com/tensorflow/models/issues/1609\">tensorflow/models#1609</a>, I was able to benchmark the graph transformed from frozen_inference_graph.pb of one of the the official <a href=\"https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md\">object detection models</a>.</p>\n<p>After changing <code>num_classes</code> to 90 and <code>score_threshold</code> to 0.1 or bigger values in <a href=\"https://github.com/tensorflow/models/blob/master/object_detection/samples/configs/ssd_mobilenet_v1_pets.config\">ssd_mobilenet_v1_pets.config</a>, I exported, transformed and benchmarked new inference graphs from the checkpoints with the following commands.</p>\n<div class=\"highlight highlight-source-shell\"><pre>python object_detection/export_inference_graph.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path ssd_mobilenet_v1_pets.config \\\n    --checkpoint_path model.ckpt \\\n    --inference_graph_path output_inference_graph.pb\n\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=output_inference_graph.score_threshold_0.1.pb \\\n--out_graph=transformed_inference_graph.score_threshold_0.1.pb \\\n--inputs=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image_tensor<span class=\"pl-pds\">'</span></span> \\\n--outputs=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>detection_boxes,detection_scores,detection_classes,num_detections<span class=\"pl-pds\">'</span></span> \\\n--transforms=<span class=\"pl-s\"><span class=\"pl-pds\">'</span></span>\n<span class=\"pl-s\">  add_default_attributes</span>\n<span class=\"pl-s\">  strip_unused_nodes(type=float)</span>\n<span class=\"pl-s\">  remove_nodes(op=CheckNumerics)</span>\n<span class=\"pl-s\">  fold_constants(ignore_errors=true)</span>\n<span class=\"pl-s\">  fold_batch_norms</span>\n<span class=\"pl-s\">  fold_old_batch_norms</span>\n<span class=\"pl-s\">  fuse_resize_pad_and_conv</span>\n<span class=\"pl-s\">  fuse_pad_and_conv</span>\n<span class=\"pl-s\">  fuse_resize_and_conv</span>\n<span class=\"pl-s\">  quantize_weights</span>\n<span class=\"pl-s\">  quantize_nodes</span>\n<span class=\"pl-s\">  strip_unused_nodes</span>\n<span class=\"pl-s\">  sort_by_execution_order<span class=\"pl-pds\">'</span></span>\n\nadb shell /data/local/tmp/benchmark_model \\\n --graph=/data/local/tmp/transformed_inference_graph.score_threshold_0.1.pb \\\n --input_layer=image_tensor:0 \\\n --input_layer_shape=1,224,224,3 \\\n --input_layer_type=uint8 \\\n --output_layer=detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0 \\\n <span class=\"pl-k\">&gt;</span> transformed_inference_graph.score_threshold_0.1.benchmark</pre></div>\n<p>The benchmark failed with the logs shown below.</p>\n<pre><code>native : benchmark_model.cc:382 Graph: [/data/local/tmp/transformed_inference_graph.score_threshold_0.3.pb]\n\nnative : benchmark_model.cc:383 Input layers: [image_tensor:0]\n\nnative : benchmark_model.cc:384 Input shapes: [1,224,224,3]\n\nnative : benchmark_model.cc:385 Input types: [uint8]\n\nnative : benchmark_model.cc:386 Output layers: [detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0]\n\nnative : benchmark_model.cc:387 Num runs: [50]\n\nnative : benchmark_model.cc:388 Inter-run delay (seconds): [-1.0]\n\nnative : benchmark_model.cc:389 Num threads: [-1]\n\nnative : benchmark_model.cc:390 Benchmark name: []\n\nnative : benchmark_model.cc:391 Output prefix: []\n\nnative : benchmark_model.cc:392 Show sizes: [0]\n\nnative : benchmark_model.cc:393 Warmup runs: [2]\n\nnative : benchmark_model.cc:53 Loading TensorFlow.\n\nnative : benchmark_model.cc:60 Got config, 0 devices\n\ncan't determine number of CPU cores: assuming 4\n\ncan't determine number of CPU cores: assuming 4\n\nnative : benchmark_model.cc:258 Running benchmark for 2 iterations without detailed stat logging:\n\nnative : benchmark_model.cc:234 Error during inference: Invalid argument: input_max_range must be larger than input_min_range.\n\n\t [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/quantize = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/min, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/max)]]\n\nnative : benchmark_model.cc:269 Failed on run 0\n\nnative : benchmark_model.cc:452 Timing failed with Invalid argument: input_max_range must be larger than input_min_range.\n\n\t [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/quantize = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/min, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/max)]]\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a> please have a look at the possible causes.</p>", "body_text": "This happens to some graphs generated with the Graph Transform Tool too.\nUsing the commands detailed in tensorflow/models#1609, I was able to benchmark the graph transformed from frozen_inference_graph.pb of one of the the official object detection models.\nAfter changing num_classes to 90 and score_threshold to 0.1 or bigger values in ssd_mobilenet_v1_pets.config, I exported, transformed and benchmarked new inference graphs from the checkpoints with the following commands.\npython object_detection/export_inference_graph.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path ssd_mobilenet_v1_pets.config \\\n    --checkpoint_path model.ckpt \\\n    --inference_graph_path output_inference_graph.pb\n\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=output_inference_graph.score_threshold_0.1.pb \\\n--out_graph=transformed_inference_graph.score_threshold_0.1.pb \\\n--inputs='image_tensor' \\\n--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\\n--transforms='\n  add_default_attributes\n  strip_unused_nodes(type=float)\n  remove_nodes(op=CheckNumerics)\n  fold_constants(ignore_errors=true)\n  fold_batch_norms\n  fold_old_batch_norms\n  fuse_resize_pad_and_conv\n  fuse_pad_and_conv\n  fuse_resize_and_conv\n  quantize_weights\n  quantize_nodes\n  strip_unused_nodes\n  sort_by_execution_order'\n\nadb shell /data/local/tmp/benchmark_model \\\n --graph=/data/local/tmp/transformed_inference_graph.score_threshold_0.1.pb \\\n --input_layer=image_tensor:0 \\\n --input_layer_shape=1,224,224,3 \\\n --input_layer_type=uint8 \\\n --output_layer=detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0 \\\n > transformed_inference_graph.score_threshold_0.1.benchmark\nThe benchmark failed with the logs shown below.\nnative : benchmark_model.cc:382 Graph: [/data/local/tmp/transformed_inference_graph.score_threshold_0.3.pb]\n\nnative : benchmark_model.cc:383 Input layers: [image_tensor:0]\n\nnative : benchmark_model.cc:384 Input shapes: [1,224,224,3]\n\nnative : benchmark_model.cc:385 Input types: [uint8]\n\nnative : benchmark_model.cc:386 Output layers: [detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0]\n\nnative : benchmark_model.cc:387 Num runs: [50]\n\nnative : benchmark_model.cc:388 Inter-run delay (seconds): [-1.0]\n\nnative : benchmark_model.cc:389 Num threads: [-1]\n\nnative : benchmark_model.cc:390 Benchmark name: []\n\nnative : benchmark_model.cc:391 Output prefix: []\n\nnative : benchmark_model.cc:392 Show sizes: [0]\n\nnative : benchmark_model.cc:393 Warmup runs: [2]\n\nnative : benchmark_model.cc:53 Loading TensorFlow.\n\nnative : benchmark_model.cc:60 Got config, 0 devices\n\ncan't determine number of CPU cores: assuming 4\n\ncan't determine number of CPU cores: assuming 4\n\nnative : benchmark_model.cc:258 Running benchmark for 2 iterations without detailed stat logging:\n\nnative : benchmark_model.cc:234 Error during inference: Invalid argument: input_max_range must be larger than input_min_range.\n\n\t [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/quantize = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/min, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/max)]]\n\nnative : benchmark_model.cc:269 Failed on run 0\n\nnative : benchmark_model.cc:452 Timing failed with Invalid argument: input_max_range must be larger than input_min_range.\n\n\t [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/quantize = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/min, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/max)]]\n\n@petewarden please have a look at the possible causes.", "body": "This happens to some graphs generated with the [Graph Transform Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) too.\r\n\r\nUsing the commands detailed in https://github.com/tensorflow/models/issues/1609, I was able to benchmark the graph transformed from frozen_inference_graph.pb of one of the the official [object detection models](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md).\r\n\r\nAfter changing `num_classes` to 90 and `score_threshold` to 0.1 or bigger values in [ssd_mobilenet_v1_pets.config](https://github.com/tensorflow/models/blob/master/object_detection/samples/configs/ssd_mobilenet_v1_pets.config), I exported, transformed and benchmarked new inference graphs from the checkpoints with the following commands.\r\n```shell\r\npython object_detection/export_inference_graph.py \\\r\n    --input_type image_tensor \\\r\n    --pipeline_config_path ssd_mobilenet_v1_pets.config \\\r\n    --checkpoint_path model.ckpt \\\r\n    --inference_graph_path output_inference_graph.pb\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=output_inference_graph.score_threshold_0.1.pb \\\r\n--out_graph=transformed_inference_graph.score_threshold_0.1.pb \\\r\n--inputs='image_tensor' \\\r\n--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float)\r\n  remove_nodes(op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  fuse_resize_pad_and_conv\r\n  fuse_pad_and_conv\r\n  fuse_resize_and_conv\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n\r\nadb shell /data/local/tmp/benchmark_model \\\r\n --graph=/data/local/tmp/transformed_inference_graph.score_threshold_0.1.pb \\\r\n --input_layer=image_tensor:0 \\\r\n --input_layer_shape=1,224,224,3 \\\r\n --input_layer_type=uint8 \\\r\n --output_layer=detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0 \\\r\n > transformed_inference_graph.score_threshold_0.1.benchmark\r\n```\r\nThe benchmark failed with the logs shown below.\r\n```\r\nnative : benchmark_model.cc:382 Graph: [/data/local/tmp/transformed_inference_graph.score_threshold_0.3.pb]\r\n\r\nnative : benchmark_model.cc:383 Input layers: [image_tensor:0]\r\n\r\nnative : benchmark_model.cc:384 Input shapes: [1,224,224,3]\r\n\r\nnative : benchmark_model.cc:385 Input types: [uint8]\r\n\r\nnative : benchmark_model.cc:386 Output layers: [detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0]\r\n\r\nnative : benchmark_model.cc:387 Num runs: [50]\r\n\r\nnative : benchmark_model.cc:388 Inter-run delay (seconds): [-1.0]\r\n\r\nnative : benchmark_model.cc:389 Num threads: [-1]\r\n\r\nnative : benchmark_model.cc:390 Benchmark name: []\r\n\r\nnative : benchmark_model.cc:391 Output prefix: []\r\n\r\nnative : benchmark_model.cc:392 Show sizes: [0]\r\n\r\nnative : benchmark_model.cc:393 Warmup runs: [2]\r\n\r\nnative : benchmark_model.cc:53 Loading TensorFlow.\r\n\r\nnative : benchmark_model.cc:60 Got config, 0 devices\r\n\r\ncan't determine number of CPU cores: assuming 4\r\n\r\ncan't determine number of CPU cores: assuming 4\r\n\r\nnative : benchmark_model.cc:258 Running benchmark for 2 iterations without detailed stat logging:\r\n\r\nnative : benchmark_model.cc:234 Error during inference: Invalid argument: input_max_range must be larger than input_min_range.\r\n\r\n\t [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/quantize = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/min, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/max)]]\r\n\r\nnative : benchmark_model.cc:269 Failed on run 0\r\n\r\nnative : benchmark_model.cc:452 Timing failed with Invalid argument: input_max_range must be larger than input_min_range.\r\n\r\n\t [[Node: FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/quantize = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/min, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6_eightbit/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm/max)]]\r\n```\r\n@petewarden please have a look at the possible causes."}