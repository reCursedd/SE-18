{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/408654128", "html_url": "https://github.com/tensorflow/tensorflow/issues/21216#issuecomment-408654128", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21216", "id": 408654128, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODY1NDEyOA==", "user": {"login": "skeydan", "id": 469371, "node_id": "MDQ6VXNlcjQ2OTM3MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/469371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skeydan", "html_url": "https://github.com/skeydan", "followers_url": "https://api.github.com/users/skeydan/followers", "following_url": "https://api.github.com/users/skeydan/following{/other_user}", "gists_url": "https://api.github.com/users/skeydan/gists{/gist_id}", "starred_url": "https://api.github.com/users/skeydan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skeydan/subscriptions", "organizations_url": "https://api.github.com/users/skeydan/orgs", "repos_url": "https://api.github.com/users/skeydan/repos", "events_url": "https://api.github.com/users/skeydan/events{/privacy}", "received_events_url": "https://api.github.com/users/skeydan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-29T05:56:21Z", "updated_at": "2018-07-29T05:56:21Z", "author_association": "NONE", "body_html": "<pre><code>from __future__ import absolute_import, division, print_function\n\n# Import TensorFlow &gt;= 1.9 and enable eager execution\nimport tensorflow as tf\ntfe = tf.contrib.eager\n\ntf.enable_eager_execution()\n\nfrom sklearn.model_selection import train_test_split\n\nimport unicodedata\nimport re\nimport numpy as np\nimport os\nimport time\n\nprint(tf.__version__)\n\n# Download the file\n#path_to_zip = tf.keras.utils.get_file(\n#    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n#    extract=True)\n\npath_to_file = \"data/spa.txt\"\n\n\n# Add a start and end token to each sentence.\n# Clean the sentences by removing special characters.\n# Create a word index and reverse word index (dictionaries mapping from word -&gt; id and id -&gt; word).\n# Pad each sentence to a maximum length.\n\n\n# Converts the unicode file to ascii\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n    \n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" =&gt; \"he is a boy .\" \n    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    \n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n    \n    w = w.rstrip().strip()\n    \n    # adding a start and an end token to the sentence\n    # so that the model know when to start and stop predicting.\n    w = '&lt;start&gt; ' + w + ' &lt;end&gt;'\n    return w\n\ndef create_dataset(path, num_examples):\n    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n    \n    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n    \n    return word_pairs\n\n    # This class creates a word -&gt; index mapping (e.g,. \"dad\" -&gt; 5) and vice-versa \n# (e.g., 5 -&gt; \"dad\") for each language,\nclass LanguageIndex():\n  def __init__(self, lang):\n    self.lang = lang\n    self.word2idx = {}\n    self.idx2word = {}\n    self.vocab = set()\n    \n    self.create_index()\n    \n  def create_index(self):\n    for phrase in self.lang:\n      self.vocab.update(phrase.split(' '))\n    \n    self.vocab = sorted(self.vocab)\n    \n    self.word2idx['&lt;pad&gt;'] = 0\n    for index, word in enumerate(self.vocab):\n      self.word2idx[word] = index + 1\n    \n    for word, index in self.word2idx.items():\n      self.idx2word[index] = word\n\ndef max_length(tensor):\n    return max(len(t) for t in tensor)\n\ndef load_dataset(path, num_examples):\n    # creating cleaned input, output pairs\n    pairs = create_dataset(path, num_examples)\n    # index language using the class defined above    \n    inp_lang = LanguageIndex(sp for en, sp in pairs)\n    targ_lang = LanguageIndex(en for en, sp in pairs)\n    # Vectorize the input and target languages\n    # Spanish sentences\n    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n    # English sentences\n    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n    # Calculate max_length of input and output tensor\n    # Here, we'll set those to the longest sentence in the dataset\n    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n    # Padding the input and output tensor to the maximum length\n    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n                                                                 maxlen=max_length_inp,\n                                                                 padding='post')\n    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n                                                                  maxlen=max_length_tar, \n                                                                  padding='post')\n    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n\nnum_examples = 100\ninput_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n\n# Creating training and validation sets using an 80-20 split\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n\n# Show length\nlen(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n\nBUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nembedding_dim = 256\nunits = 1024\nvocab_inp_size = len(inp_lang.word2idx)\nvocab_tar_size = len(targ_lang.word2idx)\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n\n# The input is put through an encoder model which gives us the encoder output of shape \n# (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size). \n\n# We're using Bahdanau attention. Lets decide on notation before writing the simplified form:\n# \n#     FC = Fully connected (dense) layer\n#     EO = Encoder output\n#     H = hidden state\n#     X = input to the decoder\n# \n# And the pseudo-code:\n# \n#     score = FC(tanh(FC(EO) + FC(H)))\n#     attention weights = softmax(score, axis = 1). \n#          Softmax by default is applied on the last axis but here we want to apply it on the 1st axis,\n#          since the shape of score is (batch_size, max_length, hidden_size). \n#          Max_length is the length of our input. Since we are trying to assign a weight to each input,\n#          softmax should be applied on that axis.\n#     context vector = sum(attention weights * EO, axis = 1). Same reason as above for choosing axis as 1.\n#     embedding output = The input to the decoder X is passed through an embedding layer.\n#     merged vector = concat(embedding output, context vector)\n#     This merged vector is then given to the GRU\n\ndef gru(units):\n  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n  # the code automatically does that.\n  if tf.test.is_gpu_available():\n    return tf.keras.layers.CuDNNGRU(units, \n                                    return_sequences=True, \n                                    return_state=True, \n                                    recurrent_initializer='glorot_uniform')\n  else:\n    return tf.keras.layers.GRU(units, \n                               return_sequences=True, \n                               return_state=True, \n                               recurrent_activation='sigmoid', \n                               recurrent_initializer='glorot_uniform')\n\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = gru(self.enc_units)\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)        \n        return output, state\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))\n\n\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = gru(self.dec_units)\n        self.fc = tf.keras.layers.Dense(vocab_size)\n        # used for attention\n        self.W1 = tf.keras.layers.Dense(self.dec_units)\n        self.W2 = tf.keras.layers.Dense(self.dec_units)\n        self.V = tf.keras.layers.Dense(1)\n    def call(self, x, hidden, enc_output):\n        # enc_output shape == (batch_size, max_length, hidden_size)\n        # hidden shape == (batch_size, hidden size)\n        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n        # we are doing this to perform addition to calculate the score\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        # score shape == (batch_size, max_length, hidden_size)\n        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n        # attention_weights shape == (batch_size, max_length, 1)\n        # we get 1 at the last axis because we are applying score to self.V\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * enc_output\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n        # output shape == (batch_size * max_length, hidden_size)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        # output shape == (batch_size * max_length, vocab)\n        x = self.fc(output)\n        return x, state, attention_weights\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.dec_units))\n\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nrestore = True\ncheckpoint_dir = \"./ckpt-py/\"\nrestore_from = \"./ckpt-py/-1\"\n\ncheckpoint = tfe.Checkpoint(optimizer=optimizer,\n                            encoder = encoder,\n                            decoder = decoder,\n                            optimizer_step=tf.train.get_or_create_global_step())\n\n\ndef loss_function(real, pred):\n  mask = 1 - np.equal(real, 0)\n  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n  return tf.reduce_mean(loss_)\n\n\n# The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n# Stop predicting when the model predicts the end token.\n# And store the attention weights for every time step.\n\ndef evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n    sentence = preprocess_sentence(sentence)\n    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    result = ''\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word2idx['&lt;start&gt;']], 0)\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n        # storing the attention weigths to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n        predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n        result += targ_lang.idx2word[predicted_id] + ' '\n        if targ_lang.idx2word[predicted_id] == '&lt;end&gt;':\n            return result, sentence, attention_plot\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n    return result, sentence, attention_plot\n\n\n# function for plotting the attention weights\ndef plot_attention(attention, sentence, predicted_sentence):\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap='viridis')\n    \n    fontdict = {'fontsize': 14}\n    \n    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n\n    plt.show()\n    \ndef translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n    print('Input: {}'.format(sentence))\n    print('Predicted translation: {}'.format(result))\n    #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n    #plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n\n\n#translate('hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n\n# Training\n# \n# Pass the input through the encoder which return encoder output and the encoder hidden state.\n# The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n# The decoder returns the predictions and the decoder hidden state.\n# The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n# Use teacher forcing to decide the next input to the decoder.\n# Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n# The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n\n\nEPOCHS = 5\n\nif restore == False:\n  for epoch in range(EPOCHS):\n      start = time.time()\n      hidden = encoder.initialize_hidden_state()\n      total_loss = 0\n      for (batch, (inp, targ)) in enumerate(dataset):\n          loss = 0\n          with tf.GradientTape() as tape:\n              enc_output, enc_hidden = encoder(inp, hidden)\n              dec_hidden = enc_hidden\n              dec_input = tf.expand_dims([targ_lang.word2idx['&lt;start&gt;']] * BATCH_SIZE, 1)       \n              # Teacher forcing - feeding the target as the next input\n              for t in range(1, targ.shape[1]):\n                  # passing enc_output to the decoder\n                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n                  loss += loss_function(targ[:, t], predictions)\n                  # using teacher forcing\n                  dec_input = tf.expand_dims(targ[:, t], 1)\n          total_loss += (loss / int(targ.shape[1]))\n          variables = encoder.variables + decoder.variables\n          gradients = tape.gradient(loss, variables)\n          optimizer.apply_gradients(zip(gradients, variables), tf.train.get_or_create_global_step())\n          #if batch % 100 == 0:\n          #    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n          #                                                 batch,\n          #                                                 loss.numpy() / int(targ.shape[1])))\n      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                          total_loss/len(input_tensor)))\n      #print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n      save_path = checkpoint.save(checkpoint_dir)\n      print(save_path)\n      metadata = tf.contrib.checkpoint.object_metadata(save_path)\n      with open(\"save_python.txt\", \"w\") as f: \n        f.write(str(metadata))\n      translate('Sean gentiles.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n      translate('Escuche.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n      translate('Estoy trabajando.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\nelse: \n  rst = checkpoint.restore(restore_from)\n  #rst = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n  metadata = tf.contrib.checkpoint.object_metadata(restore_from)\n  with open(\"restore_python.txt\", \"w\") as f: \n    f.write(str(metadata))\n  print(rst.assert_consumed())\n  translate('Sean gentiles.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n  translate('Escuche.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n  translate('Estoy trabajando.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n\n</code></pre>", "body_text": "from __future__ import absolute_import, division, print_function\n\n# Import TensorFlow >= 1.9 and enable eager execution\nimport tensorflow as tf\ntfe = tf.contrib.eager\n\ntf.enable_eager_execution()\n\nfrom sklearn.model_selection import train_test_split\n\nimport unicodedata\nimport re\nimport numpy as np\nimport os\nimport time\n\nprint(tf.__version__)\n\n# Download the file\n#path_to_zip = tf.keras.utils.get_file(\n#    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n#    extract=True)\n\npath_to_file = \"data/spa.txt\"\n\n\n# Add a start and end token to each sentence.\n# Clean the sentences by removing special characters.\n# Create a word index and reverse word index (dictionaries mapping from word -> id and id -> word).\n# Pad each sentence to a maximum length.\n\n\n# Converts the unicode file to ascii\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n    \n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\" \n    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    \n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n    \n    w = w.rstrip().strip()\n    \n    # adding a start and an end token to the sentence\n    # so that the model know when to start and stop predicting.\n    w = '<start> ' + w + ' <end>'\n    return w\n\ndef create_dataset(path, num_examples):\n    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n    \n    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n    \n    return word_pairs\n\n    # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n# (e.g., 5 -> \"dad\") for each language,\nclass LanguageIndex():\n  def __init__(self, lang):\n    self.lang = lang\n    self.word2idx = {}\n    self.idx2word = {}\n    self.vocab = set()\n    \n    self.create_index()\n    \n  def create_index(self):\n    for phrase in self.lang:\n      self.vocab.update(phrase.split(' '))\n    \n    self.vocab = sorted(self.vocab)\n    \n    self.word2idx['<pad>'] = 0\n    for index, word in enumerate(self.vocab):\n      self.word2idx[word] = index + 1\n    \n    for word, index in self.word2idx.items():\n      self.idx2word[index] = word\n\ndef max_length(tensor):\n    return max(len(t) for t in tensor)\n\ndef load_dataset(path, num_examples):\n    # creating cleaned input, output pairs\n    pairs = create_dataset(path, num_examples)\n    # index language using the class defined above    \n    inp_lang = LanguageIndex(sp for en, sp in pairs)\n    targ_lang = LanguageIndex(en for en, sp in pairs)\n    # Vectorize the input and target languages\n    # Spanish sentences\n    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n    # English sentences\n    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n    # Calculate max_length of input and output tensor\n    # Here, we'll set those to the longest sentence in the dataset\n    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n    # Padding the input and output tensor to the maximum length\n    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n                                                                 maxlen=max_length_inp,\n                                                                 padding='post')\n    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n                                                                  maxlen=max_length_tar, \n                                                                  padding='post')\n    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n\nnum_examples = 100\ninput_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n\n# Creating training and validation sets using an 80-20 split\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n\n# Show length\nlen(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n\nBUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nembedding_dim = 256\nunits = 1024\nvocab_inp_size = len(inp_lang.word2idx)\nvocab_tar_size = len(targ_lang.word2idx)\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n\n# The input is put through an encoder model which gives us the encoder output of shape \n# (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size). \n\n# We're using Bahdanau attention. Lets decide on notation before writing the simplified form:\n# \n#     FC = Fully connected (dense) layer\n#     EO = Encoder output\n#     H = hidden state\n#     X = input to the decoder\n# \n# And the pseudo-code:\n# \n#     score = FC(tanh(FC(EO) + FC(H)))\n#     attention weights = softmax(score, axis = 1). \n#          Softmax by default is applied on the last axis but here we want to apply it on the 1st axis,\n#          since the shape of score is (batch_size, max_length, hidden_size). \n#          Max_length is the length of our input. Since we are trying to assign a weight to each input,\n#          softmax should be applied on that axis.\n#     context vector = sum(attention weights * EO, axis = 1). Same reason as above for choosing axis as 1.\n#     embedding output = The input to the decoder X is passed through an embedding layer.\n#     merged vector = concat(embedding output, context vector)\n#     This merged vector is then given to the GRU\n\ndef gru(units):\n  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n  # the code automatically does that.\n  if tf.test.is_gpu_available():\n    return tf.keras.layers.CuDNNGRU(units, \n                                    return_sequences=True, \n                                    return_state=True, \n                                    recurrent_initializer='glorot_uniform')\n  else:\n    return tf.keras.layers.GRU(units, \n                               return_sequences=True, \n                               return_state=True, \n                               recurrent_activation='sigmoid', \n                               recurrent_initializer='glorot_uniform')\n\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = gru(self.enc_units)\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)        \n        return output, state\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))\n\n\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = gru(self.dec_units)\n        self.fc = tf.keras.layers.Dense(vocab_size)\n        # used for attention\n        self.W1 = tf.keras.layers.Dense(self.dec_units)\n        self.W2 = tf.keras.layers.Dense(self.dec_units)\n        self.V = tf.keras.layers.Dense(1)\n    def call(self, x, hidden, enc_output):\n        # enc_output shape == (batch_size, max_length, hidden_size)\n        # hidden shape == (batch_size, hidden size)\n        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n        # we are doing this to perform addition to calculate the score\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        # score shape == (batch_size, max_length, hidden_size)\n        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n        # attention_weights shape == (batch_size, max_length, 1)\n        # we get 1 at the last axis because we are applying score to self.V\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * enc_output\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n        # output shape == (batch_size * max_length, hidden_size)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        # output shape == (batch_size * max_length, vocab)\n        x = self.fc(output)\n        return x, state, attention_weights\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.dec_units))\n\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nrestore = True\ncheckpoint_dir = \"./ckpt-py/\"\nrestore_from = \"./ckpt-py/-1\"\n\ncheckpoint = tfe.Checkpoint(optimizer=optimizer,\n                            encoder = encoder,\n                            decoder = decoder,\n                            optimizer_step=tf.train.get_or_create_global_step())\n\n\ndef loss_function(real, pred):\n  mask = 1 - np.equal(real, 0)\n  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n  return tf.reduce_mean(loss_)\n\n\n# The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n# Stop predicting when the model predicts the end token.\n# And store the attention weights for every time step.\n\ndef evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n    sentence = preprocess_sentence(sentence)\n    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    result = ''\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n        # storing the attention weigths to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n        predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n        result += targ_lang.idx2word[predicted_id] + ' '\n        if targ_lang.idx2word[predicted_id] == '<end>':\n            return result, sentence, attention_plot\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n    return result, sentence, attention_plot\n\n\n# function for plotting the attention weights\ndef plot_attention(attention, sentence, predicted_sentence):\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap='viridis')\n    \n    fontdict = {'fontsize': 14}\n    \n    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n\n    plt.show()\n    \ndef translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n    print('Input: {}'.format(sentence))\n    print('Predicted translation: {}'.format(result))\n    #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n    #plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n\n\n#translate('hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n\n# Training\n# \n# Pass the input through the encoder which return encoder output and the encoder hidden state.\n# The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n# The decoder returns the predictions and the decoder hidden state.\n# The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n# Use teacher forcing to decide the next input to the decoder.\n# Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n# The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n\n\nEPOCHS = 5\n\nif restore == False:\n  for epoch in range(EPOCHS):\n      start = time.time()\n      hidden = encoder.initialize_hidden_state()\n      total_loss = 0\n      for (batch, (inp, targ)) in enumerate(dataset):\n          loss = 0\n          with tf.GradientTape() as tape:\n              enc_output, enc_hidden = encoder(inp, hidden)\n              dec_hidden = enc_hidden\n              dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n              # Teacher forcing - feeding the target as the next input\n              for t in range(1, targ.shape[1]):\n                  # passing enc_output to the decoder\n                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n                  loss += loss_function(targ[:, t], predictions)\n                  # using teacher forcing\n                  dec_input = tf.expand_dims(targ[:, t], 1)\n          total_loss += (loss / int(targ.shape[1]))\n          variables = encoder.variables + decoder.variables\n          gradients = tape.gradient(loss, variables)\n          optimizer.apply_gradients(zip(gradients, variables), tf.train.get_or_create_global_step())\n          #if batch % 100 == 0:\n          #    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n          #                                                 batch,\n          #                                                 loss.numpy() / int(targ.shape[1])))\n      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                          total_loss/len(input_tensor)))\n      #print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n      save_path = checkpoint.save(checkpoint_dir)\n      print(save_path)\n      metadata = tf.contrib.checkpoint.object_metadata(save_path)\n      with open(\"save_python.txt\", \"w\") as f: \n        f.write(str(metadata))\n      translate('Sean gentiles.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n      translate('Escuche.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n      translate('Estoy trabajando.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\nelse: \n  rst = checkpoint.restore(restore_from)\n  #rst = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n  metadata = tf.contrib.checkpoint.object_metadata(restore_from)\n  with open(\"restore_python.txt\", \"w\") as f: \n    f.write(str(metadata))\n  print(rst.assert_consumed())\n  translate('Sean gentiles.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n  translate('Escuche.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n  translate('Estoy trabajando.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)", "body": "```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\n# Import TensorFlow >= 1.9 and enable eager execution\r\nimport tensorflow as tf\r\ntfe = tf.contrib.eager\r\n\r\ntf.enable_eager_execution()\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimport unicodedata\r\nimport re\r\nimport numpy as np\r\nimport os\r\nimport time\r\n\r\nprint(tf.__version__)\r\n\r\n# Download the file\r\n#path_to_zip = tf.keras.utils.get_file(\r\n#    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \r\n#    extract=True)\r\n\r\npath_to_file = \"data/spa.txt\"\r\n\r\n\r\n# Add a start and end token to each sentence.\r\n# Clean the sentences by removing special characters.\r\n# Create a word index and reverse word index (dictionaries mapping from word -> id and id -> word).\r\n# Pad each sentence to a maximum length.\r\n\r\n\r\n# Converts the unicode file to ascii\r\ndef unicode_to_ascii(s):\r\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\r\n        if unicodedata.category(c) != 'Mn')\r\n\r\n\r\ndef preprocess_sentence(w):\r\n    w = unicode_to_ascii(w.lower().strip())\r\n    \r\n    # creating a space between a word and the punctuation following it\r\n    # eg: \"he is a boy.\" => \"he is a boy .\" \r\n    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\r\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\r\n    w = re.sub(r'[\" \"]+', \" \", w)\r\n    \r\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\r\n    \r\n    w = w.rstrip().strip()\r\n    \r\n    # adding a start and an end token to the sentence\r\n    # so that the model know when to start and stop predicting.\r\n    w = '<start> ' + w + ' <end>'\r\n    return w\r\n\r\ndef create_dataset(path, num_examples):\r\n    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\r\n    \r\n    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\r\n    \r\n    return word_pairs\r\n\r\n    # This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \r\n# (e.g., 5 -> \"dad\") for each language,\r\nclass LanguageIndex():\r\n  def __init__(self, lang):\r\n    self.lang = lang\r\n    self.word2idx = {}\r\n    self.idx2word = {}\r\n    self.vocab = set()\r\n    \r\n    self.create_index()\r\n    \r\n  def create_index(self):\r\n    for phrase in self.lang:\r\n      self.vocab.update(phrase.split(' '))\r\n    \r\n    self.vocab = sorted(self.vocab)\r\n    \r\n    self.word2idx['<pad>'] = 0\r\n    for index, word in enumerate(self.vocab):\r\n      self.word2idx[word] = index + 1\r\n    \r\n    for word, index in self.word2idx.items():\r\n      self.idx2word[index] = word\r\n\r\ndef max_length(tensor):\r\n    return max(len(t) for t in tensor)\r\n\r\ndef load_dataset(path, num_examples):\r\n    # creating cleaned input, output pairs\r\n    pairs = create_dataset(path, num_examples)\r\n    # index language using the class defined above    \r\n    inp_lang = LanguageIndex(sp for en, sp in pairs)\r\n    targ_lang = LanguageIndex(en for en, sp in pairs)\r\n    # Vectorize the input and target languages\r\n    # Spanish sentences\r\n    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\r\n    # English sentences\r\n    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\r\n    # Calculate max_length of input and output tensor\r\n    # Here, we'll set those to the longest sentence in the dataset\r\n    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\r\n    # Padding the input and output tensor to the maximum length\r\n    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \r\n                                                                 maxlen=max_length_inp,\r\n                                                                 padding='post')\r\n    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \r\n                                                                  maxlen=max_length_tar, \r\n                                                                  padding='post')\r\n    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\r\n\r\nnum_examples = 100\r\ninput_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\r\n\r\n# Creating training and validation sets using an 80-20 split\r\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\r\n\r\n# Show length\r\nlen(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\r\n\r\nBUFFER_SIZE = len(input_tensor_train)\r\nBATCH_SIZE = 64\r\nembedding_dim = 256\r\nunits = 1024\r\nvocab_inp_size = len(inp_lang.word2idx)\r\nvocab_tar_size = len(targ_lang.word2idx)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\r\ndataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\r\n\r\n# The input is put through an encoder model which gives us the encoder output of shape \r\n# (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size). \r\n\r\n# We're using Bahdanau attention. Lets decide on notation before writing the simplified form:\r\n# \r\n#     FC = Fully connected (dense) layer\r\n#     EO = Encoder output\r\n#     H = hidden state\r\n#     X = input to the decoder\r\n# \r\n# And the pseudo-code:\r\n# \r\n#     score = FC(tanh(FC(EO) + FC(H)))\r\n#     attention weights = softmax(score, axis = 1). \r\n#          Softmax by default is applied on the last axis but here we want to apply it on the 1st axis,\r\n#          since the shape of score is (batch_size, max_length, hidden_size). \r\n#          Max_length is the length of our input. Since we are trying to assign a weight to each input,\r\n#          softmax should be applied on that axis.\r\n#     context vector = sum(attention weights * EO, axis = 1). Same reason as above for choosing axis as 1.\r\n#     embedding output = The input to the decoder X is passed through an embedding layer.\r\n#     merged vector = concat(embedding output, context vector)\r\n#     This merged vector is then given to the GRU\r\n\r\ndef gru(units):\r\n  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\r\n  # the code automatically does that.\r\n  if tf.test.is_gpu_available():\r\n    return tf.keras.layers.CuDNNGRU(units, \r\n                                    return_sequences=True, \r\n                                    return_state=True, \r\n                                    recurrent_initializer='glorot_uniform')\r\n  else:\r\n    return tf.keras.layers.GRU(units, \r\n                               return_sequences=True, \r\n                               return_state=True, \r\n                               recurrent_activation='sigmoid', \r\n                               recurrent_initializer='glorot_uniform')\r\n\r\nclass Encoder(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\r\n        super(Encoder, self).__init__()\r\n        self.batch_sz = batch_sz\r\n        self.enc_units = enc_units\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n        self.gru = gru(self.enc_units)\r\n    def call(self, x, hidden):\r\n        x = self.embedding(x)\r\n        output, state = self.gru(x, initial_state = hidden)        \r\n        return output, state\r\n    def initialize_hidden_state(self):\r\n        return tf.zeros((self.batch_sz, self.enc_units))\r\n\r\n\r\nclass Decoder(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\r\n        super(Decoder, self).__init__()\r\n        self.batch_sz = batch_sz\r\n        self.dec_units = dec_units\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n        self.gru = gru(self.dec_units)\r\n        self.fc = tf.keras.layers.Dense(vocab_size)\r\n        # used for attention\r\n        self.W1 = tf.keras.layers.Dense(self.dec_units)\r\n        self.W2 = tf.keras.layers.Dense(self.dec_units)\r\n        self.V = tf.keras.layers.Dense(1)\r\n    def call(self, x, hidden, enc_output):\r\n        # enc_output shape == (batch_size, max_length, hidden_size)\r\n        # hidden shape == (batch_size, hidden size)\r\n        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\r\n        # we are doing this to perform addition to calculate the score\r\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\r\n        # score shape == (batch_size, max_length, hidden_size)\r\n        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\r\n        # attention_weights shape == (batch_size, max_length, 1)\r\n        # we get 1 at the last axis because we are applying score to self.V\r\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\r\n        # context_vector shape after sum == (batch_size, hidden_size)\r\n        context_vector = attention_weights * enc_output\r\n        context_vector = tf.reduce_sum(context_vector, axis=1)\r\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n        x = self.embedding(x)\r\n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\r\n        # passing the concatenated vector to the GRU\r\n        output, state = self.gru(x)\r\n        # output shape == (batch_size * max_length, hidden_size)\r\n        output = tf.reshape(output, (-1, output.shape[2]))\r\n        # output shape == (batch_size * max_length, vocab)\r\n        x = self.fc(output)\r\n        return x, state, attention_weights\r\n    def initialize_hidden_state(self):\r\n        return tf.zeros((self.batch_sz, self.dec_units))\r\n\r\n\r\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\r\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nrestore = True\r\ncheckpoint_dir = \"./ckpt-py/\"\r\nrestore_from = \"./ckpt-py/-1\"\r\n\r\ncheckpoint = tfe.Checkpoint(optimizer=optimizer,\r\n                            encoder = encoder,\r\n                            decoder = decoder,\r\n                            optimizer_step=tf.train.get_or_create_global_step())\r\n\r\n\r\ndef loss_function(real, pred):\r\n  mask = 1 - np.equal(real, 0)\r\n  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\r\n  return tf.reduce_mean(loss_)\r\n\r\n\r\n# The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\r\n# Stop predicting when the model predicts the end token.\r\n# And store the attention weights for every time step.\r\n\r\ndef evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\r\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\r\n    sentence = preprocess_sentence(sentence)\r\n    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\r\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\r\n    inputs = tf.convert_to_tensor(inputs)\r\n    result = ''\r\n    hidden = [tf.zeros((1, units))]\r\n    enc_out, enc_hidden = encoder(inputs, hidden)\r\n    dec_hidden = enc_hidden\r\n    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\r\n    for t in range(max_length_targ):\r\n        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\r\n        # storing the attention weigths to plot later on\r\n        attention_weights = tf.reshape(attention_weights, (-1, ))\r\n        attention_plot[t] = attention_weights.numpy()\r\n        predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\r\n        result += targ_lang.idx2word[predicted_id] + ' '\r\n        if targ_lang.idx2word[predicted_id] == '<end>':\r\n            return result, sentence, attention_plot\r\n        # the predicted ID is fed back into the model\r\n        dec_input = tf.expand_dims([predicted_id], 0)\r\n    return result, sentence, attention_plot\r\n\r\n\r\n# function for plotting the attention weights\r\ndef plot_attention(attention, sentence, predicted_sentence):\r\n    fig = plt.figure(figsize=(10,10))\r\n    ax = fig.add_subplot(1, 1, 1)\r\n    ax.matshow(attention, cmap='viridis')\r\n    \r\n    fontdict = {'fontsize': 14}\r\n    \r\n    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\r\n    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\r\n\r\n    plt.show()\r\n    \r\ndef translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\r\n    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\n    print('Input: {}'.format(sentence))\r\n    print('Predicted translation: {}'.format(result))\r\n    #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\r\n    #plot_attention(attention_plot, sentence.split(' '), result.split(' '))\r\n\r\n\r\n#translate('hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\n\r\n# Training\r\n# \r\n# Pass the input through the encoder which return encoder output and the encoder hidden state.\r\n# The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\r\n# The decoder returns the predictions and the decoder hidden state.\r\n# The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\r\n# Use teacher forcing to decide the next input to the decoder.\r\n# Teacher forcing is the technique where the target word is passed as the next input to the decoder.\r\n# The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\r\n\r\n\r\nEPOCHS = 5\r\n\r\nif restore == False:\r\n  for epoch in range(EPOCHS):\r\n      start = time.time()\r\n      hidden = encoder.initialize_hidden_state()\r\n      total_loss = 0\r\n      for (batch, (inp, targ)) in enumerate(dataset):\r\n          loss = 0\r\n          with tf.GradientTape() as tape:\r\n              enc_output, enc_hidden = encoder(inp, hidden)\r\n              dec_hidden = enc_hidden\r\n              dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \r\n              # Teacher forcing - feeding the target as the next input\r\n              for t in range(1, targ.shape[1]):\r\n                  # passing enc_output to the decoder\r\n                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\r\n                  loss += loss_function(targ[:, t], predictions)\r\n                  # using teacher forcing\r\n                  dec_input = tf.expand_dims(targ[:, t], 1)\r\n          total_loss += (loss / int(targ.shape[1]))\r\n          variables = encoder.variables + decoder.variables\r\n          gradients = tape.gradient(loss, variables)\r\n          optimizer.apply_gradients(zip(gradients, variables), tf.train.get_or_create_global_step())\r\n          #if batch % 100 == 0:\r\n          #    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\r\n          #                                                 batch,\r\n          #                                                 loss.numpy() / int(targ.shape[1])))\r\n      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\r\n                                          total_loss/len(input_tensor)))\r\n      #print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\r\n      save_path = checkpoint.save(checkpoint_dir)\r\n      print(save_path)\r\n      metadata = tf.contrib.checkpoint.object_metadata(save_path)\r\n      with open(\"save_python.txt\", \"w\") as f: \r\n        f.write(str(metadata))\r\n      translate('Sean gentiles.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\n      translate('Escuche.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\n      translate('Estoy trabajando.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\nelse: \r\n  rst = checkpoint.restore(restore_from)\r\n  #rst = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n  metadata = tf.contrib.checkpoint.object_metadata(restore_from)\r\n  with open(\"restore_python.txt\", \"w\") as f: \r\n    f.write(str(metadata))\r\n  print(rst.assert_consumed())\r\n  translate('Sean gentiles.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\n  translate('Escuche.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\n  translate('Estoy trabajando.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\r\n\r\n```"}