{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10805", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10805/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10805/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10805/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10805", "id": 236706717, "node_id": "MDU6SXNzdWUyMzY3MDY3MTc=", "number": 10805, "title": "Make `tf.contrib.seq2seq._BaseAttentionMechanism` public", "user": {"login": "danielwatson6", "id": 3270063, "node_id": "MDQ6VXNlcjMyNzAwNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3270063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielwatson6", "html_url": "https://github.com/danielwatson6", "followers_url": "https://api.github.com/users/danielwatson6/followers", "following_url": "https://api.github.com/users/danielwatson6/following{/other_user}", "gists_url": "https://api.github.com/users/danielwatson6/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielwatson6/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielwatson6/subscriptions", "organizations_url": "https://api.github.com/users/danielwatson6/orgs", "repos_url": "https://api.github.com/users/danielwatson6/repos", "events_url": "https://api.github.com/users/danielwatson6/events{/privacy}", "received_events_url": "https://api.github.com/users/danielwatson6/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2017-06-18T07:52:50Z", "updated_at": "2018-07-28T21:49:06Z", "closed_at": "2018-01-04T22:24:19Z", "author_association": "NONE", "body_html": "<p>Currently, if users want to write their own attention mechanisms, they have to do so from scratch by extending the <code>tf.contrib.seq2seq.AttentionMechanism</code> class (which has nothing) or import the <code>_BaseAttentionMechanism</code> from <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\">attention_wrapper.py</a> with the following, inconvenient import statement:</p>\n<pre><code>from tensorflow.contrib.seq2seq.python.ops.attention_wrapper import _BaseAttentionMechanism\n</code></pre>\n<p>It's worth including this class directly accessible from <code>tf.contrib.seq2seq</code>, considering that it adds good defaults and that both forms of attention currently available, Bahdanau and Luong, inherit this class.</p>\n<p>My proposal is to simply rename the class to <code>BaseAttentionMechanism</code> without an underscore (or perhaps a more meaningful name such as <code>BasicAttentionMechanism</code>to differentiate it from the parent class <code>AttentionMechanism</code>), and add the class to the <code>__all__</code> array in <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\">attention_wrapper.py</a>.</p>", "body_text": "Currently, if users want to write their own attention mechanisms, they have to do so from scratch by extending the tf.contrib.seq2seq.AttentionMechanism class (which has nothing) or import the _BaseAttentionMechanism from attention_wrapper.py with the following, inconvenient import statement:\nfrom tensorflow.contrib.seq2seq.python.ops.attention_wrapper import _BaseAttentionMechanism\n\nIt's worth including this class directly accessible from tf.contrib.seq2seq, considering that it adds good defaults and that both forms of attention currently available, Bahdanau and Luong, inherit this class.\nMy proposal is to simply rename the class to BaseAttentionMechanism without an underscore (or perhaps a more meaningful name such as BasicAttentionMechanismto differentiate it from the parent class AttentionMechanism), and add the class to the __all__ array in attention_wrapper.py.", "body": "Currently, if users want to write their own attention mechanisms, they have to do so from scratch by extending the `tf.contrib.seq2seq.AttentionMechanism` class (which has nothing) or import the `_BaseAttentionMechanism` from [attention_wrapper.py](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py) with the following, inconvenient import statement:\r\n\r\n    from tensorflow.contrib.seq2seq.python.ops.attention_wrapper import _BaseAttentionMechanism\r\n\r\nIt's worth including this class directly accessible from `tf.contrib.seq2seq`, considering that it adds good defaults and that both forms of attention currently available, Bahdanau and Luong, inherit this class.\r\n\r\nMy proposal is to simply rename the class to `BaseAttentionMechanism` without an underscore (or perhaps a more meaningful name such as `BasicAttentionMechanism`to differentiate it from the parent class `AttentionMechanism`), and add the class to the `__all__` array in [attention_wrapper.py](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py)."}