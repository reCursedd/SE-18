{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/349671386", "html_url": "https://github.com/tensorflow/tensorflow/issues/15152#issuecomment-349671386", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15152", "id": 349671386, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTY3MTM4Ng==", "user": {"login": "sofeikov", "id": 6204851, "node_id": "MDQ6VXNlcjYyMDQ4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6204851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sofeikov", "html_url": "https://github.com/sofeikov", "followers_url": "https://api.github.com/users/sofeikov/followers", "following_url": "https://api.github.com/users/sofeikov/following{/other_user}", "gists_url": "https://api.github.com/users/sofeikov/gists{/gist_id}", "starred_url": "https://api.github.com/users/sofeikov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sofeikov/subscriptions", "organizations_url": "https://api.github.com/users/sofeikov/orgs", "repos_url": "https://api.github.com/users/sofeikov/repos", "events_url": "https://api.github.com/users/sofeikov/events{/privacy}", "received_events_url": "https://api.github.com/users/sofeikov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-06T15:21:10Z", "updated_at": "2017-12-06T15:36:09Z", "author_association": "NONE", "body_html": "<p>Me and my colleague have made a small investigation of this behaviour.<br>\nThe reason of this happening is not very surprising. All images in Kodak set are either 512x768 or 768x512 pixels. That said, it is just a pure luck that every time you precisely the same amount of pixels.</p>\n<p>Every time an image is read with a correct shape, which can be seen if we put<br>\n<code>tf.Print(image_source, [tf.shape(image_source)], summarize=1000)</code> somewhere in <code>simply_read_image</code> function.</p>\n<p>However, after Dataset object takes it place, it \"memorises\" the shape of the <em>first</em> entry and then tries to fill image buffer <em>somehow</em>. There are two indicators of this:</p>\n<ol>\n<li>\n<p><code>dataset = dataset.map(lambda x: tf.Print(x, [tf.shape(x)], summarize=1000), num_parallel_calls=12)</code> This line will print the same shape for all image. This shape will of the first image met in file list</p>\n</li>\n<li>\n<p>This can be easily checked by rescaling one of images so that is has different from 512x768 amount of pixels and try to do the same.</p>\n</li>\n</ol>\n<p>After this the code above will crush with an obvious error:</p>\n<pre><code>InternalError (see above for traceback): HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [512,768,3], [parent slice]: [433,288,3]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,3]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n</code></pre>\n<p>I think this is a wrong behaviour for <code>dataset</code> object, because:</p>\n<ol>\n<li>I may want to use <code>Dataset</code> API for running my autoencoders on real world images of different sizes.</li>\n<li>In image processing tasks you can't rescale image to standard size, because images will seen by humans after network does it's job</li>\n</ol>\n<p>Taking these into account and that (to the best of my knowledge) there is no other way to use <code>Dataset</code> API for image reading images of different sizes, I think this poses a problem.</p>\n<p>Probably the resolution could be allow dispatching images of different sizes by <code>dataset</code> object, if batch size is equal to 1.</p>", "body_text": "Me and my colleague have made a small investigation of this behaviour.\nThe reason of this happening is not very surprising. All images in Kodak set are either 512x768 or 768x512 pixels. That said, it is just a pure luck that every time you precisely the same amount of pixels.\nEvery time an image is read with a correct shape, which can be seen if we put\ntf.Print(image_source, [tf.shape(image_source)], summarize=1000) somewhere in simply_read_image function.\nHowever, after Dataset object takes it place, it \"memorises\" the shape of the first entry and then tries to fill image buffer somehow. There are two indicators of this:\n\n\ndataset = dataset.map(lambda x: tf.Print(x, [tf.shape(x)], summarize=1000), num_parallel_calls=12) This line will print the same shape for all image. This shape will of the first image met in file list\n\n\nThis can be easily checked by rescaling one of images so that is has different from 512x768 amount of pixels and try to do the same.\n\n\nAfter this the code above will crush with an obvious error:\nInternalError (see above for traceback): HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [512,768,3], [parent slice]: [433,288,3]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,3]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nI think this is a wrong behaviour for dataset object, because:\n\nI may want to use Dataset API for running my autoencoders on real world images of different sizes.\nIn image processing tasks you can't rescale image to standard size, because images will seen by humans after network does it's job\n\nTaking these into account and that (to the best of my knowledge) there is no other way to use Dataset API for image reading images of different sizes, I think this poses a problem.\nProbably the resolution could be allow dispatching images of different sizes by dataset object, if batch size is equal to 1.", "body": "Me and my colleague have made a small investigation of this behaviour. \r\nThe reason of this happening is not very surprising. All images in Kodak set are either 512x768 or 768x512 pixels. That said, it is just a pure luck that every time you precisely the same amount of pixels. \r\n\r\nEvery time an image is read with a correct shape, which can be seen if we put \r\n`tf.Print(image_source, [tf.shape(image_source)], summarize=1000)` somewhere in `simply_read_image` function.\r\n\r\nHowever, after Dataset object takes it place, it \"memorises\" the shape of the _first_ entry and then tries to fill image buffer _somehow_. There are two indicators of this: \r\n\r\n1. `dataset = dataset.map(lambda x: tf.Print(x, [tf.shape(x)], summarize=1000), num_parallel_calls=12)` This line will print the same shape for all image. This shape will of the first image met in file list\r\n\r\n2. This can be easily checked by rescaling one of images so that is has different from 512x768 amount of pixels and try to do the same.\r\n\r\nAfter this the code above will crush with an obvious error:\r\n```\r\nInternalError (see above for traceback): HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [512,768,3], [parent slice]: [433,288,3]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,3]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n```\r\n\r\nI think this is a wrong behaviour for `dataset` object, because:\r\n\r\n1. I may want to use `Dataset` API for running my autoencoders on real world images of different sizes.\r\n2. In image processing tasks you can't rescale image to standard size, because images will seen by humans after network does it's job\r\n\r\nTaking these into account and that (to the best of my knowledge) there is no other way to use `Dataset` API for image reading images of different sizes, I think this poses a problem. \r\n\r\nProbably the resolution could be allow dispatching images of different sizes by `dataset` object, if batch size is equal to 1."}