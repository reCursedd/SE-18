{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/288603190", "html_url": "https://github.com/tensorflow/tensorflow/issues/8602#issuecomment-288603190", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8602", "id": 288603190, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODYwMzE5MA==", "user": {"login": "pathak22", "id": 9029761, "node_id": "MDQ6VXNlcjkwMjk3NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/9029761?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pathak22", "html_url": "https://github.com/pathak22", "followers_url": "https://api.github.com/users/pathak22/followers", "following_url": "https://api.github.com/users/pathak22/following{/other_user}", "gists_url": "https://api.github.com/users/pathak22/gists{/gist_id}", "starred_url": "https://api.github.com/users/pathak22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pathak22/subscriptions", "organizations_url": "https://api.github.com/users/pathak22/orgs", "repos_url": "https://api.github.com/users/pathak22/repos", "events_url": "https://api.github.com/users/pathak22/events{/privacy}", "received_events_url": "https://api.github.com/users/pathak22/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-23T02:57:28Z", "updated_at": "2017-03-23T02:58:33Z", "author_association": "NONE", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a> for your comment. Sorry for the confusion, but no I am not trying to run single model. I am trying to run <em>4 completely different programs on same machine</em>. These four programs have no relation to each other. Each program has 3 workers and 1 parameter server of its own, all 3 workers update their unique parameter server asynchronously using hogwild SGD. This is what I meant when I say <code>Run</code> in my main comment above, sorry if it was not clear. To concretize the model, I am training 4 A3C models on 4 different tasks. Each A3C has 3 workers.</p>\n<p>Actually, I think you have pin-pointed my question:</p>\n<blockquote>\n<p>TF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other</p>\n</blockquote>\n<p>I have two questions:</p>\n<ol>\n<li>Does different worker processes in distributed-TF handle parallelization if I just say <code>CPU:0</code> in all of them ? I would believe they should, given the documentation, because TF knows about different worker processes via the cluster spec.</li>\n<li>If answer to 1) is yes, then what happens when I run multiple independent programs (or \"runs\") in which each program is \"a distributed-TF session with multiple worker processes\" ? Should I still use <code>CPU:0</code> in all of them ?</li>\n</ol>\n<p>I would believe TF won't handle the second case, because each distributed-TF run does not know about other programs being running. So, is there a way to define affinity for different processes for different cpu cores ? For example:</p>\n<pre><code>ps_device = \"/job:ps\"\nworker_device = \"/job:worker/task:0/cpu:2\"\nworker_device = \"/job:worker/task:1/cpu:4\"\nworker_device = \"/job:worker/task:2/cpu:6\"\n</code></pre>\n<p>Can I do something like above, i.e., tag different cpu devices ?</p>\n<p>My question is more of a usage question and let me know if this is not the right place to ask. But tensor-flow docs talk only about <code>cpu:0</code> and do not talk about assigning affinity for each cpu core e.g. <code>cpu:1, cpu:2</code> .. similar to what TF has for gpu. Are these supported ? If yes, what is the right usage.</p>\n<p>I found one answer by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> has some answer here regarding this: <a href=\"http://stackoverflow.com/a/37864489\" rel=\"nofollow\">http://stackoverflow.com/a/37864489</a> , but don't know the usage in context of distributed-TF.</p>", "body_text": "Thanks @tfboyd for your comment. Sorry for the confusion, but no I am not trying to run single model. I am trying to run 4 completely different programs on same machine. These four programs have no relation to each other. Each program has 3 workers and 1 parameter server of its own, all 3 workers update their unique parameter server asynchronously using hogwild SGD. This is what I meant when I say Run in my main comment above, sorry if it was not clear. To concretize the model, I am training 4 A3C models on 4 different tasks. Each A3C has 3 workers.\nActually, I think you have pin-pointed my question:\n\nTF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other\n\nI have two questions:\n\nDoes different worker processes in distributed-TF handle parallelization if I just say CPU:0 in all of them ? I would believe they should, given the documentation, because TF knows about different worker processes via the cluster spec.\nIf answer to 1) is yes, then what happens when I run multiple independent programs (or \"runs\") in which each program is \"a distributed-TF session with multiple worker processes\" ? Should I still use CPU:0 in all of them ?\n\nI would believe TF won't handle the second case, because each distributed-TF run does not know about other programs being running. So, is there a way to define affinity for different processes for different cpu cores ? For example:\nps_device = \"/job:ps\"\nworker_device = \"/job:worker/task:0/cpu:2\"\nworker_device = \"/job:worker/task:1/cpu:4\"\nworker_device = \"/job:worker/task:2/cpu:6\"\n\nCan I do something like above, i.e., tag different cpu devices ?\nMy question is more of a usage question and let me know if this is not the right place to ask. But tensor-flow docs talk only about cpu:0 and do not talk about assigning affinity for each cpu core e.g. cpu:1, cpu:2 .. similar to what TF has for gpu. Are these supported ? If yes, what is the right usage.\nI found one answer by @mrry has some answer here regarding this: http://stackoverflow.com/a/37864489 , but don't know the usage in context of distributed-TF.", "body": "Thanks @tfboyd for your comment. Sorry for the confusion, but no I am not trying to run single model. I am trying to run _4 completely different programs on same machine_. These four programs have no relation to each other. Each program has 3 workers and 1 parameter server of its own, all 3 workers update their unique parameter server asynchronously using hogwild SGD. This is what I meant when I say `Run` in my main comment above, sorry if it was not clear. To concretize the model, I am training 4 A3C models on 4 different tasks. Each A3C has 3 workers.\r\n\r\nActually, I think you have pin-pointed my question: \r\n> TF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other\r\n\r\nI have two questions:\r\n1) Does different worker processes in distributed-TF handle parallelization if I just say `CPU:0` in all of them ? I would believe they should, given the documentation, because TF knows about different worker processes via the cluster spec.\r\n2) If answer to 1) is yes, then what happens when I run multiple independent programs (or \"runs\") in which each program is \"a distributed-TF session with multiple worker processes\" ? Should I still use `CPU:0` in all of them ? \r\n\r\nI would believe TF won't handle the second case, because each distributed-TF run does not know about other programs being running. So, is there a way to define affinity for different processes for different cpu cores ? For example:\r\n```\r\nps_device = \"/job:ps\"\r\nworker_device = \"/job:worker/task:0/cpu:2\"\r\nworker_device = \"/job:worker/task:1/cpu:4\"\r\nworker_device = \"/job:worker/task:2/cpu:6\"\r\n```\r\nCan I do something like above, i.e., tag different cpu devices ?\r\n\r\nMy question is more of a usage question and let me know if this is not the right place to ask. But tensor-flow docs talk only about `cpu:0` and do not talk about assigning affinity for each cpu core e.g. `cpu:1, cpu:2` .. similar to what TF has for gpu. Are these supported ? If yes, what is the right usage. \r\n\r\nI found one answer by @mrry has some answer here regarding this: http://stackoverflow.com/a/37864489 , but don't know the usage in context of distributed-TF."}