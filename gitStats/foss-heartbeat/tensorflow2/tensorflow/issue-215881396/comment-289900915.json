{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289900915", "html_url": "https://github.com/tensorflow/tensorflow/issues/8602#issuecomment-289900915", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8602", "id": 289900915, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTkwMDkxNQ==", "user": {"login": "gunan", "id": 7946809, "node_id": "MDQ6VXNlcjc5NDY4MDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/7946809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gunan", "html_url": "https://github.com/gunan", "followers_url": "https://api.github.com/users/gunan/followers", "following_url": "https://api.github.com/users/gunan/following{/other_user}", "gists_url": "https://api.github.com/users/gunan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gunan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gunan/subscriptions", "organizations_url": "https://api.github.com/users/gunan/orgs", "repos_url": "https://api.github.com/users/gunan/repos", "events_url": "https://api.github.com/users/gunan/events{/privacy}", "received_events_url": "https://api.github.com/users/gunan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-28T20:52:24Z", "updated_at": "2017-03-28T20:52:24Z", "author_association": "MEMBER", "body_html": "<p>I don't think this is a TF bug. This is an issue with user setup, so should be asked in Stackoverflow. I will close this issue. I also want to clarify a few things.</p>\n<p>TF will only see <code>CPU:0</code>. CPU:x where x is not 0 will give you errors in your code.<br>\nI do not understand what you mean by handling parallelization. Operating system manages parallel processes running on CPU. If you want one process to be reserved to run on a specific core, you will need</p>\n<p>It does not make sense to run distributed TF on a single machine, and expect good performance out of it. On the same machine, a single process training will always be faster, because distributed TF always has the RPC communication overheads, even if we do not consider any other overhead of distributed training.<br>\nIn the end, on the same machine distributed TF and single process TF has access to the same \"pool\" of resources, and distributed TF needs to do more things than just training, so it is slower.</p>", "body_text": "I don't think this is a TF bug. This is an issue with user setup, so should be asked in Stackoverflow. I will close this issue. I also want to clarify a few things.\nTF will only see CPU:0. CPU:x where x is not 0 will give you errors in your code.\nI do not understand what you mean by handling parallelization. Operating system manages parallel processes running on CPU. If you want one process to be reserved to run on a specific core, you will need\nIt does not make sense to run distributed TF on a single machine, and expect good performance out of it. On the same machine, a single process training will always be faster, because distributed TF always has the RPC communication overheads, even if we do not consider any other overhead of distributed training.\nIn the end, on the same machine distributed TF and single process TF has access to the same \"pool\" of resources, and distributed TF needs to do more things than just training, so it is slower.", "body": "I don't think this is a TF bug. This is an issue with user setup, so should be asked in Stackoverflow. I will close this issue. I also want to clarify a few things.\r\n\r\nTF will only see `CPU:0`. CPU:x where x is not 0 will give you errors in your code.\r\nI do not understand what you mean by handling parallelization. Operating system manages parallel processes running on CPU. If you want one process to be reserved to run on a specific core, you will need\r\n\r\nIt does not make sense to run distributed TF on a single machine, and expect good performance out of it. On the same machine, a single process training will always be faster, because distributed TF always has the RPC communication overheads, even if we do not consider any other overhead of distributed training.\r\nIn the end, on the same machine distributed TF and single process TF has access to the same \"pool\" of resources, and distributed TF needs to do more things than just training, so it is slower."}