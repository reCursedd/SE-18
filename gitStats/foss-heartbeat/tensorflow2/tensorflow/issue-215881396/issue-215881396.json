{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8602", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8602/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8602/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8602/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8602", "id": 215881396, "node_id": "MDU6SXNzdWUyMTU4ODEzOTY=", "number": 8602, "title": "Multiple simultaneous distributed-TF runs on single machine", "user": {"login": "pathak22", "id": 9029761, "node_id": "MDQ6VXNlcjkwMjk3NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/9029761?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pathak22", "html_url": "https://github.com/pathak22", "followers_url": "https://api.github.com/users/pathak22/followers", "following_url": "https://api.github.com/users/pathak22/following{/other_user}", "gists_url": "https://api.github.com/users/pathak22/gists{/gist_id}", "starred_url": "https://api.github.com/users/pathak22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pathak22/subscriptions", "organizations_url": "https://api.github.com/users/pathak22/orgs", "repos_url": "https://api.github.com/users/pathak22/repos", "events_url": "https://api.github.com/users/pathak22/events{/privacy}", "received_events_url": "https://api.github.com/users/pathak22/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-03-21T21:19:08Z", "updated_at": "2017-03-28T20:52:24Z", "closed_at": "2017-03-28T20:52:24Z", "author_association": "NONE", "body_html": "<p>I am running multiple distributed-TF sessions simultaneously on a single machine. I am setting up the configuration as described below. However, I am not getting the expected parallelism speedup.</p>\n<p>I am running 4 different runs simultaneously, each of them with following config:</p>\n<pre><code># Run - 1\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:5000\"]\n\t\"worker\": [\n\t\t\"localhost:5001\",\n\t\t\"localhost:5002\",\n\t\t\"localhost:5003\"],\n\t})\n\n# Run - 2\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:6000\"]\n\t\"worker\": [\n\t\t\"localhost:6001\",\n\t\t\"localhost:6002\",\n\t\t\"localhost:6003\"],\n\t})\n\n# Run - 3\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:7000\"]\n\t\"worker\": [\n\t\t\"localhost:7001\",\n\t\t\"localhost:7002\",\n\t\t\"localhost:7003\"],\n\t})\n\n# Run - 4\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:8000\"]\n\t\"worker\": [\n\t\t\"localhost:8001\",\n\t\t\"localhost:8002\",\n\t\t\"localhost:8003\"],\n\t})\n</code></pre>\n<p>The command that I use to setup each server looks <strong>exactly</strong> like this for each worker and parameter server respectively:</p>\n<pre><code># worker: index varies from 0 to 2\nserver = tf.train.Server(clusterSpec, job_name=\"worker\", task_index=0,\n                                 config=tf.ConfigProto(intra_op_parallelism_threads=1,\n                                 inter_op_parallelism_threads=2))\n\n### ps\nserver = tf.train.Server(clusterSpec, job_name=\"ps\", task_index=0,\n                                 config=tf.ConfigProto(device_filters=[\"/job:ps\"]))\n</code></pre>\n<p>While setting up the network variables, the device is setup <strong>exactly</strong> like this for all runs:</p>\n<pre><code>ps_device = \"/job:ps\"\nworker_device = \"/job:worker/task:0/cpu:0\"\nworker_device = \"/job:worker/task:1/cpu:0\"\nworker_device = \"/job:worker/task:2/cpu:0\"\n</code></pre>\n<p>There are <code>80</code> cores and enough memory. Each job has <code>3 workers and 1 ps</code>. In some fixed amount of time, I get following performance:</p>\n<pre><code>1 run on machine: x iterations\n4 runs on machine: approx. x/2 iterations\n6 runs on machine: approx. x/3 iterations\n</code></pre>\n<p>The number of cores are enough such that I would expect 4 simultaneous runs to go as fast as single run, but it is no where close. Is there some mistake in my usage of the distributed-tf api ? I suspect there might be issue in the way I am using cpu device for worker: <code>cpu:0</code>. But as I read from docs, it seems to mention that <code>cpu:0</code> would do automatic scheduling. Does this hold even if I am running 4 different runs of distributed-tf on single machine? Should I be setting up <code>cpu:1 , cpu:2, cpu:3, cpu:4</code> for workers of 4 different runs ? Any help would be greatly appreciated.</p>\n<p>Thanks in advance !!</p>", "body_text": "I am running multiple distributed-TF sessions simultaneously on a single machine. I am setting up the configuration as described below. However, I am not getting the expected parallelism speedup.\nI am running 4 different runs simultaneously, each of them with following config:\n# Run - 1\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:5000\"]\n\t\"worker\": [\n\t\t\"localhost:5001\",\n\t\t\"localhost:5002\",\n\t\t\"localhost:5003\"],\n\t})\n\n# Run - 2\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:6000\"]\n\t\"worker\": [\n\t\t\"localhost:6001\",\n\t\t\"localhost:6002\",\n\t\t\"localhost:6003\"],\n\t})\n\n# Run - 3\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:7000\"]\n\t\"worker\": [\n\t\t\"localhost:7001\",\n\t\t\"localhost:7002\",\n\t\t\"localhost:7003\"],\n\t})\n\n# Run - 4\ntf.train.ClusterSpec({\n\t\"ps\": [\"localhost:8000\"]\n\t\"worker\": [\n\t\t\"localhost:8001\",\n\t\t\"localhost:8002\",\n\t\t\"localhost:8003\"],\n\t})\n\nThe command that I use to setup each server looks exactly like this for each worker and parameter server respectively:\n# worker: index varies from 0 to 2\nserver = tf.train.Server(clusterSpec, job_name=\"worker\", task_index=0,\n                                 config=tf.ConfigProto(intra_op_parallelism_threads=1,\n                                 inter_op_parallelism_threads=2))\n\n### ps\nserver = tf.train.Server(clusterSpec, job_name=\"ps\", task_index=0,\n                                 config=tf.ConfigProto(device_filters=[\"/job:ps\"]))\n\nWhile setting up the network variables, the device is setup exactly like this for all runs:\nps_device = \"/job:ps\"\nworker_device = \"/job:worker/task:0/cpu:0\"\nworker_device = \"/job:worker/task:1/cpu:0\"\nworker_device = \"/job:worker/task:2/cpu:0\"\n\nThere are 80 cores and enough memory. Each job has 3 workers and 1 ps. In some fixed amount of time, I get following performance:\n1 run on machine: x iterations\n4 runs on machine: approx. x/2 iterations\n6 runs on machine: approx. x/3 iterations\n\nThe number of cores are enough such that I would expect 4 simultaneous runs to go as fast as single run, but it is no where close. Is there some mistake in my usage of the distributed-tf api ? I suspect there might be issue in the way I am using cpu device for worker: cpu:0. But as I read from docs, it seems to mention that cpu:0 would do automatic scheduling. Does this hold even if I am running 4 different runs of distributed-tf on single machine? Should I be setting up cpu:1 , cpu:2, cpu:3, cpu:4 for workers of 4 different runs ? Any help would be greatly appreciated.\nThanks in advance !!", "body": "I am running multiple distributed-TF sessions simultaneously on a single machine. I am setting up the configuration as described below. However, I am not getting the expected parallelism speedup.\r\n\r\nI am running 4 different runs simultaneously, each of them with following config:\r\n```\r\n# Run - 1\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:5000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:5001\",\r\n\t\t\"localhost:5002\",\r\n\t\t\"localhost:5003\"],\r\n\t})\r\n\r\n# Run - 2\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:6000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:6001\",\r\n\t\t\"localhost:6002\",\r\n\t\t\"localhost:6003\"],\r\n\t})\r\n\r\n# Run - 3\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:7000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:7001\",\r\n\t\t\"localhost:7002\",\r\n\t\t\"localhost:7003\"],\r\n\t})\r\n\r\n# Run - 4\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:8000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:8001\",\r\n\t\t\"localhost:8002\",\r\n\t\t\"localhost:8003\"],\r\n\t})\r\n```\r\n\r\nThe command that I use to setup each server looks **exactly** like this for each worker and parameter server respectively:\r\n```\r\n# worker: index varies from 0 to 2\r\nserver = tf.train.Server(clusterSpec, job_name=\"worker\", task_index=0,\r\n                                 config=tf.ConfigProto(intra_op_parallelism_threads=1,\r\n                                 inter_op_parallelism_threads=2))\r\n\r\n### ps\r\nserver = tf.train.Server(clusterSpec, job_name=\"ps\", task_index=0,\r\n                                 config=tf.ConfigProto(device_filters=[\"/job:ps\"]))\r\n```\r\n\r\nWhile setting up the network variables, the device is setup **exactly** like this for all runs:\r\n```\r\nps_device = \"/job:ps\"\r\nworker_device = \"/job:worker/task:0/cpu:0\"\r\nworker_device = \"/job:worker/task:1/cpu:0\"\r\nworker_device = \"/job:worker/task:2/cpu:0\"\r\n```\r\n\r\nThere are `80` cores and enough memory. Each job has `3 workers and 1 ps`. In some fixed amount of time, I get following performance:\r\n```\r\n1 run on machine: x iterations\r\n4 runs on machine: approx. x/2 iterations\r\n6 runs on machine: approx. x/3 iterations\r\n```\r\n\r\nThe number of cores are enough such that I would expect 4 simultaneous runs to go as fast as single run, but it is no where close. Is there some mistake in my usage of the distributed-tf api ? I suspect there might be issue in the way I am using cpu device for worker: `cpu:0`. But as I read from docs, it seems to mention that `cpu:0` would do automatic scheduling. Does this hold even if I am running 4 different runs of distributed-tf on single machine? Should I be setting up `cpu:1 , cpu:2, cpu:3, cpu:4` for workers of 4 different runs ? Any help would be greatly appreciated. \r\n\r\nThanks in advance !!"}