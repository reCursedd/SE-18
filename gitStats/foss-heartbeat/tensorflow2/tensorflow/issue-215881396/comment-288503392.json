{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/288503392", "html_url": "https://github.com/tensorflow/tensorflow/issues/8602#issuecomment-288503392", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8602", "id": 288503392, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODUwMzM5Mg==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-22T18:55:12Z", "updated_at": "2017-03-22T18:55:12Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9029761\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pathak22\">@pathak22</a> ,</p>\n<p>What is your end goal?  If you are training a single model on a single machine there is usually no need to run distributed, which creates more to keep track of.  If I am reading your config correctly you are running 12 workers + 4 ps_servers simultaneously on one server.  If I made a random guess, you are getting contention for resources.  TF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other and the OS at that point owns the scheduling.</p>\n<p>Some questions that might help get someone give you a better answer</p>\n<ul>\n<li>Which model are you training?</li>\n<li>What CPU are you using?  I did not research but to get 80 cores seem like a possibly but not necessarily exotic setup.</li>\n<li>What was the CPU utilization when running a single setup, then 2, then 3 and then 4?  No need to report it in this issue if you do not want but I would check that.</li>\n<li>What performance do you get with nothing distributed and what is the CPU utilization?</li>\n</ul>\n<p>If you are trying to get the most out of your single server.  I would start with nothing distributed and assuming the code is fully optimized and I thought the server was not fully utilized I would try a distributed setup.  I have not personally setup this situation and others might have more or better advice</p>\n<p>Sample code to run would also be useful.  I am not guaranteeing someone will try it.  Your details are good but there are a lot of variables and unknowns.</p>", "body_text": "Hi @pathak22 ,\nWhat is your end goal?  If you are training a single model on a single machine there is usually no need to run distributed, which creates more to keep track of.  If I am reading your config correctly you are running 12 workers + 4 ps_servers simultaneously on one server.  If I made a random guess, you are getting contention for resources.  TF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other and the OS at that point owns the scheduling.\nSome questions that might help get someone give you a better answer\n\nWhich model are you training?\nWhat CPU are you using?  I did not research but to get 80 cores seem like a possibly but not necessarily exotic setup.\nWhat was the CPU utilization when running a single setup, then 2, then 3 and then 4?  No need to report it in this issue if you do not want but I would check that.\nWhat performance do you get with nothing distributed and what is the CPU utilization?\n\nIf you are trying to get the most out of your single server.  I would start with nothing distributed and assuming the code is fully optimized and I thought the server was not fully utilized I would try a distributed setup.  I have not personally setup this situation and others might have more or better advice\nSample code to run would also be useful.  I am not guaranteeing someone will try it.  Your details are good but there are a lot of variables and unknowns.", "body": "Hi @pathak22 ,\r\n\r\nWhat is your end goal?  If you are training a single model on a single machine there is usually no need to run distributed, which creates more to keep track of.  If I am reading your config correctly you are running 12 workers + 4 ps_servers simultaneously on one server.  If I made a random guess, you are getting contention for resources.  TF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other and the OS at that point owns the scheduling.  \r\n\r\nSome questions that might help get someone give you a better answer\r\n- Which model are you training?\r\n- What CPU are you using?  I did not research but to get 80 cores seem like a possibly but not necessarily exotic setup.  \r\n- What was the CPU utilization when running a single setup, then 2, then 3 and then 4?  No need to report it in this issue if you do not want but I would check that.  \r\n- What performance do you get with nothing distributed and what is the CPU utilization?  \r\n\r\nIf you are trying to get the most out of your single server.  I would start with nothing distributed and assuming the code is fully optimized and I thought the server was not fully utilized I would try a distributed setup.  I have not personally setup this situation and others might have more or better advice  \r\n\r\nSample code to run would also be useful.  I am not guaranteeing someone will try it.  Your details are good but there are a lot of variables and unknowns.  \r\n"}