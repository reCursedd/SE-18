{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2849", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2849/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2849/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2849/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2849", "id": 160152365, "node_id": "MDU6SXNzdWUxNjAxNTIzNjU=", "number": 2849, "title": "Lack of support for word-embedding on GPU", "user": {"login": "smartcat2010", "id": 10429114, "node_id": "MDQ6VXNlcjEwNDI5MTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/10429114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smartcat2010", "html_url": "https://github.com/smartcat2010", "followers_url": "https://api.github.com/users/smartcat2010/followers", "following_url": "https://api.github.com/users/smartcat2010/following{/other_user}", "gists_url": "https://api.github.com/users/smartcat2010/gists{/gist_id}", "starred_url": "https://api.github.com/users/smartcat2010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smartcat2010/subscriptions", "organizations_url": "https://api.github.com/users/smartcat2010/orgs", "repos_url": "https://api.github.com/users/smartcat2010/repos", "events_url": "https://api.github.com/users/smartcat2010/events{/privacy}", "received_events_url": "https://api.github.com/users/smartcat2010/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-06-14T10:54:35Z", "updated_at": "2017-02-23T22:25:35Z", "closed_at": "2016-06-20T16:12:53Z", "author_association": "NONE", "body_html": "<p>Dear experts,</p>\n<p>I faced some problems when training model with word-embedding. For simplify the case, I make a helloworld example in this file:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/313712/WE_example.py.txt\">WE_example.py.txt</a></p>\n<p>The task is to classify each sentence into 10 classes. Each sentence is length of 30 words. Firstly I use embedding_lookup to get all 30 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input.</p>\n<p>The following are problems &amp; questions:<br>\na.    If I use \u201ctf.train.RMSPropOptimizer\u201d as optimizer, it reports \u201cNotImplementedError\u201d, the details is in the following stderr file:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/313717/RMSPropOptimizer_NotImplemented.stderr.txt\">RMSPropOptimizer_NotImplemented.stderr.txt</a><br>\nSo, is there a plan to implement RMSPropOptimizer\u2019s support for word-embedding in next release?</p>\n<p>b.    Then I replace \u201ctf.train.RMSPropOptimizer\u201d by \u201ctf.train.MomentumOptimizer\u201d.  It works! But the GPU usage is only around 22%. I think putting embedding matrix on host memory (\u201c/cpu:0\u201d) causes data-transfer delay, which may lower down the GPU usage. Then I remove the \u201cwith tf.device(\u2018/cpu:0\u2019)\u201d in line 21(just before definition of word-embedding variable), to put the word-embedding matrix into GPU memory. And run it again. It shows \u201ctensorflow.python.framework.errors.InvalidArgumentError: AttrValue must not have reference type value of float_ref\u201d.<br>\nI put the stdout &amp; stderr as follows:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/313725/MomentumOptimizer_on_GPU.stdout.txt\">MomentumOptimizer_on_GPU.stdout.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/313727/MomentumOptimizer_on_GPU.stderr.txt\">MomentumOptimizer_on_GPU.stderr.txt</a></p>\n<p>It seems that putting word-embedding into GPU memory is not supported by all operators. I also found the same issue in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\">word2vec_basic.py</a> , the line 144 \u201c# Ops and variables pinned to the CPU because of missing GPU implementation\u201d .<br>\nAnd also <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/embedding/word2vec.py\">word2vec.py</a> Ln508. It also put word-embedding definition onto /cpu:0.</p>\n<p>Word-embedding is very popular in NLP domain. It is usually very large(more than 1GB in internet-level data). So putting it from host memory side to GPU global memory side may save a lot of data-transfer time cost. Is there any plan to fully support it in ALL operators in future release?</p>\n<p>c.    Then I replace \u201ctf.train.MomentumOptimizer\u201d by \u201ctf.train.GradientDescentOptimizer\u201d, still removing \u201c/cpu:0\u201d. Then it works! The GPU usage is around 25%~30%. If adding \u201c/cpu:0\u201d, the GPU usage is around 20%~25%.</p>\n<p>d.    Because the GPU usage is always below 30%(also in my real code running real data). Is there any way to profiling it to find where is the speed bottleneck?</p>\n<p>Sorry for so many questions... But it's really a blocking issue for NLP guys...</p>", "body_text": "Dear experts,\nI faced some problems when training model with word-embedding. For simplify the case, I make a helloworld example in this file:\nWE_example.py.txt\nThe task is to classify each sentence into 10 classes. Each sentence is length of 30 words. Firstly I use embedding_lookup to get all 30 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input.\nThe following are problems & questions:\na.    If I use \u201ctf.train.RMSPropOptimizer\u201d as optimizer, it reports \u201cNotImplementedError\u201d, the details is in the following stderr file:\nRMSPropOptimizer_NotImplemented.stderr.txt\nSo, is there a plan to implement RMSPropOptimizer\u2019s support for word-embedding in next release?\nb.    Then I replace \u201ctf.train.RMSPropOptimizer\u201d by \u201ctf.train.MomentumOptimizer\u201d.  It works! But the GPU usage is only around 22%. I think putting embedding matrix on host memory (\u201c/cpu:0\u201d) causes data-transfer delay, which may lower down the GPU usage. Then I remove the \u201cwith tf.device(\u2018/cpu:0\u2019)\u201d in line 21(just before definition of word-embedding variable), to put the word-embedding matrix into GPU memory. And run it again. It shows \u201ctensorflow.python.framework.errors.InvalidArgumentError: AttrValue must not have reference type value of float_ref\u201d.\nI put the stdout & stderr as follows:\nMomentumOptimizer_on_GPU.stdout.txt\nMomentumOptimizer_on_GPU.stderr.txt\nIt seems that putting word-embedding into GPU memory is not supported by all operators. I also found the same issue in word2vec_basic.py , the line 144 \u201c# Ops and variables pinned to the CPU because of missing GPU implementation\u201d .\nAnd also word2vec.py Ln508. It also put word-embedding definition onto /cpu:0.\nWord-embedding is very popular in NLP domain. It is usually very large(more than 1GB in internet-level data). So putting it from host memory side to GPU global memory side may save a lot of data-transfer time cost. Is there any plan to fully support it in ALL operators in future release?\nc.    Then I replace \u201ctf.train.MomentumOptimizer\u201d by \u201ctf.train.GradientDescentOptimizer\u201d, still removing \u201c/cpu:0\u201d. Then it works! The GPU usage is around 25%~30%. If adding \u201c/cpu:0\u201d, the GPU usage is around 20%~25%.\nd.    Because the GPU usage is always below 30%(also in my real code running real data). Is there any way to profiling it to find where is the speed bottleneck?\nSorry for so many questions... But it's really a blocking issue for NLP guys...", "body": "Dear experts,\n\nI faced some problems when training model with word-embedding. For simplify the case, I make a helloworld example in this file:\n[WE_example.py.txt](https://github.com/tensorflow/tensorflow/files/313712/WE_example.py.txt)\n\nThe task is to classify each sentence into 10 classes. Each sentence is length of 30 words. Firstly I use embedding_lookup to get all 30 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input.\n\nThe following are problems & questions:\na.    If I use \u201ctf.train.RMSPropOptimizer\u201d as optimizer, it reports \u201cNotImplementedError\u201d, the details is in the following stderr file:\n[RMSPropOptimizer_NotImplemented.stderr.txt](https://github.com/tensorflow/tensorflow/files/313717/RMSPropOptimizer_NotImplemented.stderr.txt)\nSo, is there a plan to implement RMSPropOptimizer\u2019s support for word-embedding in next release?\n\nb.    Then I replace \u201ctf.train.RMSPropOptimizer\u201d by \u201ctf.train.MomentumOptimizer\u201d.  It works! But the GPU usage is only around 22%. I think putting embedding matrix on host memory (\u201c/cpu:0\u201d) causes data-transfer delay, which may lower down the GPU usage. Then I remove the \u201cwith tf.device(\u2018/cpu:0\u2019)\u201d in line 21(just before definition of word-embedding variable), to put the word-embedding matrix into GPU memory. And run it again. It shows \u201ctensorflow.python.framework.errors.InvalidArgumentError: AttrValue must not have reference type value of float_ref\u201d. \nI put the stdout & stderr as follows:\n[MomentumOptimizer_on_GPU.stdout.txt](https://github.com/tensorflow/tensorflow/files/313725/MomentumOptimizer_on_GPU.stdout.txt)\n[MomentumOptimizer_on_GPU.stderr.txt](https://github.com/tensorflow/tensorflow/files/313727/MomentumOptimizer_on_GPU.stderr.txt)\n\nIt seems that putting word-embedding into GPU memory is not supported by all operators. I also found the same issue in [word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py) , the line 144 \u201c# Ops and variables pinned to the CPU because of missing GPU implementation\u201d .\nAnd also [word2vec.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/embedding/word2vec.py) Ln508. It also put word-embedding definition onto /cpu:0.\n\nWord-embedding is very popular in NLP domain. It is usually very large(more than 1GB in internet-level data). So putting it from host memory side to GPU global memory side may save a lot of data-transfer time cost. Is there any plan to fully support it in ALL operators in future release?\n\nc.    Then I replace \u201ctf.train.MomentumOptimizer\u201d by \u201ctf.train.GradientDescentOptimizer\u201d, still removing \u201c/cpu:0\u201d. Then it works! The GPU usage is around 25%~30%. If adding \u201c/cpu:0\u201d, the GPU usage is around 20%~25%.\n\nd.    Because the GPU usage is always below 30%(also in my real code running real data). Is there any way to profiling it to find where is the speed bottleneck?\n\nSorry for so many questions... But it's really a blocking issue for NLP guys...\n"}