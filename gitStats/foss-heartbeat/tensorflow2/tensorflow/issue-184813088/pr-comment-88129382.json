{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/88129382", "pull_request_review_id": 8712314, "id": 88129382, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDg4MTI5Mzgy", "diff_hunk": "@@ -340,3 +341,55 @@ def __call__(self, shape, dtype=dtypes.float32, partition_info=None):\n     \"\"\"Generate a tensor used to initialize a variable.\"\"\"\n     return random_ops._random_walk(shape, self._nonlinearity, dtype,\n                                    seed=self._seed)\n+\n+\n+def orthogonal_initializer(gain=1.0, dtype=dtypes.float32, seed=None):\n+  \"\"\"Returns an initializer that generates an orthogonal matrix or a reshaped \n+  orthogonal matrix.\n+\n+  If the shape of the tensor to initialize is two-dimensional, i is initialized \n+  with an orthogonal matrix obtained from the singular value decomposition of a \n+  matrix of uniform random numbers.\n+\n+  If the shape of the tensor to initialize is more than two-dimensional, a matrix\n+  of shape `(shape[0] * ... * shape[n - 2], shape[n - 1])` is initialized, where\n+  `n` is the length of the shape vector. The matrix is subsequently reshaped to\n+  give a tensor of the desired shape.\n+\n+  Args:\n+    gain: multiplicative factor to apply to the orthogonal matrix\n+    dtype: The type of the output.\n+    seed: A Python integer. Used to create random seeds. See\n+      [`set_random_seed`](../../api_docs/python/constant_op.md#set_random_seed)\n+      for behavior.\n+\n+  Returns:\n+    An initializer that generates orthogonal tensors\n+\n+  Raises:\n+    ValueError: if `dtype` is not a floating point type or if `shape` has fewer than two entries.\n+  \"\"\"\n+  def _initializer(shape, dtype=_assert_float_dtype(dtype), partition_info=None):\n+    # Check the shape\n+    if len(shape) < 2:\n+      raise ValueError('the tensor to initialize must be at least two-dimensional')\n+    # Flatten the input shape with the last dimension remaining its original shape so it works for conv2d\n+    num_rows = 1\n+    for dim in shape[:-1]:\n+      num_rows *= dim\n+    num_cols = shape[-1]\n+    flat_shape = (num_rows, num_cols)\n+\n+    # Generate a random matrix\n+    a = random_ops.random_uniform(flat_shape, dtype=dtype, seed=seed)\n+    # Compute the svd\n+    _, u, v = linalg_ops.svd(a, full_matrices=False)", "path": "tensorflow/python/ops/init_ops.py", "position": 54, "original_position": 54, "commit_id": "1cb31e3ea41334e3718bad0f33a92dd5030ca229", "original_commit_id": "2ef58d52e2318b6a1aeb842aaa15de0b819c63d8", "user": {"login": "poulson-google", "id": 19613671, "node_id": "MDQ6VXNlcjE5NjEzNjcx", "avatar_url": "https://avatars3.githubusercontent.com/u/19613671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poulson-google", "html_url": "https://github.com/poulson-google", "followers_url": "https://api.github.com/users/poulson-google/followers", "following_url": "https://api.github.com/users/poulson-google/following{/other_user}", "gists_url": "https://api.github.com/users/poulson-google/gists{/gist_id}", "starred_url": "https://api.github.com/users/poulson-google/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poulson-google/subscriptions", "organizations_url": "https://api.github.com/users/poulson-google/orgs", "repos_url": "https://api.github.com/users/poulson-google/repos", "events_url": "https://api.github.com/users/poulson-google/events{/privacy}", "received_events_url": "https://api.github.com/users/poulson-google/received_events", "type": "User", "site_admin": false}, "body": "Agreed. And, further, I would recommend generating the entries with a normal distribution and then taking the Q from the QR factorization so that a Haar matrix is generated. Diaconis has a nice paper that discusses this and an alternative implicit version: http://www.ams.org/journals/bull/2003-40-02/S0273-0979-03-00975-3/S0273-0979-03-00975-3.pdf\n", "created_at": "2016-11-15T22:32:16Z", "updated_at": "2016-11-15T22:32:17Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/5164#discussion_r88129382", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/5164", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/88129382"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/5164#discussion_r88129382"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/5164"}}, "body_html": "<p>Agreed. And, further, I would recommend generating the entries with a normal distribution and then taking the Q from the QR factorization so that a Haar matrix is generated. Diaconis has a nice paper that discusses this and an alternative implicit version: <a href=\"http://www.ams.org/journals/bull/2003-40-02/S0273-0979-03-00975-3/S0273-0979-03-00975-3.pdf\" rel=\"nofollow\">http://www.ams.org/journals/bull/2003-40-02/S0273-0979-03-00975-3/S0273-0979-03-00975-3.pdf</a></p>", "body_text": "Agreed. And, further, I would recommend generating the entries with a normal distribution and then taking the Q from the QR factorization so that a Haar matrix is generated. Diaconis has a nice paper that discusses this and an alternative implicit version: http://www.ams.org/journals/bull/2003-40-02/S0273-0979-03-00975-3/S0273-0979-03-00975-3.pdf", "in_reply_to_id": 87862479}