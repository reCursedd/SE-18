{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9213", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9213/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9213/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9213/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9213", "id": 221762745, "node_id": "MDU6SXNzdWUyMjE3NjI3NDU=", "number": 9213, "title": "System hangs when computing gradients on GPU", "user": {"login": "121onto", "id": 13111271, "node_id": "MDQ6VXNlcjEzMTExMjcx", "avatar_url": "https://avatars0.githubusercontent.com/u/13111271?v=4", "gravatar_id": "", "url": "https://api.github.com/users/121onto", "html_url": "https://github.com/121onto", "followers_url": "https://api.github.com/users/121onto/followers", "following_url": "https://api.github.com/users/121onto/following{/other_user}", "gists_url": "https://api.github.com/users/121onto/gists{/gist_id}", "starred_url": "https://api.github.com/users/121onto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/121onto/subscriptions", "organizations_url": "https://api.github.com/users/121onto/orgs", "repos_url": "https://api.github.com/users/121onto/repos", "events_url": "https://api.github.com/users/121onto/events{/privacy}", "received_events_url": "https://api.github.com/users/121onto/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-14T08:00:46Z", "updated_at": "2017-04-14T20:02:09Z", "closed_at": "2017-04-14T20:02:09Z", "author_association": "NONE", "body_html": "<h3>Problem description</h3>\n<p>My system hangs when computing gradients on the GPU.  I am able to compute gradients on the CPU without issue.  I can compute all the nodes in my graph, including my loss function, on the GPU without issue.  My system hangs when computing gradients on the GPU irrespective of which optimizer I use.  The code that produces this issue is:</p>\n<pre lang=\"{python}\"><code>sess = tf.Session()\n\n# ...\n\ncnn = CNNClass()\n\n#...\n\nstep_size = tf.placeholder(tf.float32, name=\"step_size\") \nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False) \noptimizer = tf.train.MomentumOptimizer(step_size, 0.9)\n\n#...\n\n# CRASHES WHEN I RUN THIS\ngrads_and_vars = optimizer.compute_gradients(cnn.loss)\nsess.run([grads_and_vars], feed_dict)\n\n# DOES NOT CRASH WHEN I RUN THIS\nsess.run([cnn.loss])\n</code></pre>\n<h3>Further information</h3>\n<p>I tried setting <code>gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)</code> in my ConfigProto but that didn't help.</p>\n<p>Launching <code>watch -n 0.1 nvidia-smi</code> before running the code shows Volatile GPU-Util goes to 100% just before hanging.</p>\n<p>Computing gradients on the CPU and printing them out shows normal output (I'm not getting any NaNs or 0's or anything like that).</p>\n<h3>System information</h3>\n<ul>\n<li>System: Ubuntu 16.04</li>\n<li>TensorFlow installed from: binary</li>\n<li>TensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1</li>\n<li>CUDA version: 8.0, V8.0.61</li>\n<li>cuDNN version: 5.1</li>\n<li>GPU model and memory: NVidia GeForce GTX TITAN X (12GB)</li>\n<li>Video card driver: 375.39</li>\n</ul>", "body_text": "Problem description\nMy system hangs when computing gradients on the GPU.  I am able to compute gradients on the CPU without issue.  I can compute all the nodes in my graph, including my loss function, on the GPU without issue.  My system hangs when computing gradients on the GPU irrespective of which optimizer I use.  The code that produces this issue is:\nsess = tf.Session()\n\n# ...\n\ncnn = CNNClass()\n\n#...\n\nstep_size = tf.placeholder(tf.float32, name=\"step_size\") \nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False) \noptimizer = tf.train.MomentumOptimizer(step_size, 0.9)\n\n#...\n\n# CRASHES WHEN I RUN THIS\ngrads_and_vars = optimizer.compute_gradients(cnn.loss)\nsess.run([grads_and_vars], feed_dict)\n\n# DOES NOT CRASH WHEN I RUN THIS\nsess.run([cnn.loss])\n\nFurther information\nI tried setting gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333) in my ConfigProto but that didn't help.\nLaunching watch -n 0.1 nvidia-smi before running the code shows Volatile GPU-Util goes to 100% just before hanging.\nComputing gradients on the CPU and printing them out shows normal output (I'm not getting any NaNs or 0's or anything like that).\nSystem information\n\nSystem: Ubuntu 16.04\nTensorFlow installed from: binary\nTensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1\nCUDA version: 8.0, V8.0.61\ncuDNN version: 5.1\nGPU model and memory: NVidia GeForce GTX TITAN X (12GB)\nVideo card driver: 375.39", "body": "### Problem description\r\n\r\nMy system hangs when computing gradients on the GPU.  I am able to compute gradients on the CPU without issue.  I can compute all the nodes in my graph, including my loss function, on the GPU without issue.  My system hangs when computing gradients on the GPU irrespective of which optimizer I use.  The code that produces this issue is:\r\n\r\n```{python}\r\nsess = tf.Session()\r\n\r\n# ...\r\n\r\ncnn = CNNClass()\r\n\r\n#...\r\n\r\nstep_size = tf.placeholder(tf.float32, name=\"step_size\") \r\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False) \r\noptimizer = tf.train.MomentumOptimizer(step_size, 0.9)\r\n\r\n#...\r\n\r\n# CRASHES WHEN I RUN THIS\r\ngrads_and_vars = optimizer.compute_gradients(cnn.loss)\r\nsess.run([grads_and_vars], feed_dict)\r\n\r\n# DOES NOT CRASH WHEN I RUN THIS\r\nsess.run([cnn.loss])\r\n```\r\n\r\n### Further information\r\n\r\nI tried setting `gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)` in my ConfigProto but that didn't help.  \r\n\r\nLaunching `watch -n 0.1 nvidia-smi` before running the code shows Volatile GPU-Util goes to 100% just before hanging.\r\n\r\nComputing gradients on the CPU and printing them out shows normal output (I'm not getting any NaNs or 0's or anything like that).\r\n\r\n### System information\r\n\r\n- System: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1\r\n- CUDA version: 8.0, V8.0.61\r\n- cuDNN version: 5.1\r\n- GPU model and memory: NVidia GeForce GTX TITAN X (12GB)\r\n- Video card driver: 375.39"}