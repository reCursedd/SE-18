{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/171641218", "pull_request_review_id": 100530931, "id": 171641218, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MTY0MTIxOA==", "diff_hunk": "@@ -0,0 +1,167 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/kernels/trt_calib_op.h\"\n+#include \"tensorflow/contrib/tensorrt/resources/trt_int8_calibrator.h\"\n+#include \"tensorflow/contrib/tensorrt/resources/trt_resource_manager.h\"\n+#include \"tensorflow/contrib/tensorrt/resources/trt_resources.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+\n+#if GOOGLE_CUDA\n+#if GOOGLE_TENSORRT\n+#include \"cuda_runtime_api.h\"\n+#include \"tensorrt/include/NvInfer.h\"\n+\n+namespace tensorflow {\n+namespace trt {\n+TRTCalibOp::TRTCalibOp(OpKernelConstruction* context) : OpKernel(context) {\n+  OP_REQUIRES_OK(context, context->GetAttr(\"segment_nodes\", &segment_nodes_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"input_names\", &input_names_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"resource_name\", &resource_name_));\n+};\n+\n+void TRTCalibOp::Compute(tensorflow::OpKernelContext* ctx) {\n+  auto trt_rm = tensorflow::trt::TRTResourceManager::instance();\n+  auto res_mgr = trt_rm->getManager(\"TRTCalibOps\");\n+  tensorflow::trt::TRTCalibrationResource* calib_res = nullptr;\n+  auto status = res_mgr->Lookup(resource_name_, resource_name_, &calib_res);\n+\n+  if (!status.ok()) {\n+    ctx->SetStatus(status);\n+    return;\n+  }\n+  int num_inputs = ctx->num_inputs();\n+  // first run instantiate calibrator\n+  if (calib_res->calibrator_ == nullptr) {\n+    dev_tensors_.resize(num_inputs);\n+    int batch_size = ctx->input(0).dim_size(0);\n+    VLOG(1) << \" Constructing calibrator\";\n+    for (int i = 0; i < num_inputs; i++) {\n+      // allocate workspace on device for inputs\n+      const tensorflow::Tensor& t = ctx->input(i);\n+      OP_REQUIRES_OK(ctx,\n+                     ctx->allocate_persistent(t.dtype(), t.shape(),\n+                                              &dev_tensors_.at(i), nullptr));\n+      const auto device_tensor = dev_tensors_.at(i).AccessTensor(ctx);\n+      CHECK_EQ(t.TotalBytes(), device_tensor->TotalBytes());\n+      void* device_address = nullptr;\n+      {\n+        auto tensor_type = device_tensor->dtype();\n+        switch (tensor_type) {\n+          case tensorflow::DT_FLOAT: {\n+            device_address = (void*)device_tensor\n+                                 ->flat<tensorflow::EnumToDataType<\n+                                     tensorflow::DT_FLOAT>::Type>()\n+                                 .data();", "path": "tensorflow/contrib/tensorrt/kernels/trt_calib_op.cc", "position": null, "original_position": 71, "commit_id": "5e5671e692db0533dfec66d63b8e7c8d06bc4942", "original_commit_id": "feb6a619d29f1760f1b46408181c0ded2055677c", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "That is correct sorry I missed that. The reason I chose macro was due to the discussion about the making copies of weights in previous pr. My understanding was that the tensors are not necessarily contiguous and thus we needed to make a copy of it. Thus I assumed flat() would need to make a copy in order for it to be contiguous. If we pass get the flat in another function, we have to ensure the lifetime of the Flat object. Am I wrong? Are tensors contiguous and Flat objects are just wrappers around internal buffer? If not, how are you ensuring that the memory is still there when you return from the function?\r\n", "created_at": "2018-03-01T17:57:16Z", "updated_at": "2018-03-01T22:59:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17309#discussion_r171641218", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17309", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/171641218"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17309#discussion_r171641218"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17309"}}, "body_html": "<p>That is correct sorry I missed that. The reason I chose macro was due to the discussion about the making copies of weights in previous pr. My understanding was that the tensors are not necessarily contiguous and thus we needed to make a copy of it. Thus I assumed flat() would need to make a copy in order for it to be contiguous. If we pass get the flat in another function, we have to ensure the lifetime of the Flat object. Am I wrong? Are tensors contiguous and Flat objects are just wrappers around internal buffer? If not, how are you ensuring that the memory is still there when you return from the function?</p>", "body_text": "That is correct sorry I missed that. The reason I chose macro was due to the discussion about the making copies of weights in previous pr. My understanding was that the tensors are not necessarily contiguous and thus we needed to make a copy of it. Thus I assumed flat() would need to make a copy in order for it to be contiguous. If we pass get the flat in another function, we have to ensure the lifetime of the Flat object. Am I wrong? Are tensors contiguous and Flat objects are just wrappers around internal buffer? If not, how are you ensuring that the memory is still there when you return from the function?", "in_reply_to_id": 171611265}