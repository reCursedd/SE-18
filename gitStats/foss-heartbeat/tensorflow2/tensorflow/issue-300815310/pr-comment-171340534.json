{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/171340534", "pull_request_review_id": 99897985, "id": 171340534, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MTM0MDUzNA==", "diff_hunk": "@@ -0,0 +1,104 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/resources/TRTInt8Calibrator.h\"\n+\n+#include <atomic>\n+#include <chrono>\n+#include <unordered_map>\n+#include \"cuda_runtime_api.h\"\n+\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+namespace trt {\n+// set the batch size before constructing the thread to execute engine\n+int TRTInt8Calibrator::getBatchSize() const { return batch_size_; }\n+\n+TRTInt8Calibrator::TRTInt8Calibrator(\n+    const std::unordered_map<string, std::pair<void*, size_t>>& dev_buffers,\n+    int batch_size, string engineName)\n+    : batch_size_(batch_size),\n+      done_(false),\n+      dev_buffers_(dev_buffers),\n+      calib_running_(false),\n+      engine_name_(engineName) {}\n+\n+bool TRTInt8Calibrator::setBatch(\n+    const std::unordered_map<string, void*>& data) {\n+  if (done_) return false;\n+  while (calib_running_.load(\n+      std::memory_order_acquire)) {  // wait while calibration is running\n+    tensorflow::mutex_lock l(cond_mtx_);\n+    cond_.wait_for(l, std::chrono::milliseconds(50));\n+    if (done_) return false;\n+  }\n+  VLOG(1) << \"Set Batch Waiting finished\";\n+  for (const auto it : data) {\n+    auto devptr = dev_buffers_.find(it.first);\n+    if (devptr == dev_buffers_.end()) {\n+      LOG(FATAL) << \"FATAL \" << engine_name_ << \" input name '\" << it.first\n+                 << \"' does not match with the buffer names\";\n+    }\n+    const auto& d = devptr->second;\n+\n+    auto status =\n+        cudaMemcpy(d.first, it.second, d.second, cudaMemcpyDeviceToDevice);\n+    if (status != cudaSuccess) {\n+      LOG(FATAL) << \"cudaMemcpy \" << engine_name_ << \" for '\" << it.first\n+                 << \"' failed with \" << status;\n+    }\n+  }\n+  calib_running_.store(true, std::memory_order_release);  // release builder\n+  cond_.notify_all();\n+  return true;\n+}\n+\n+bool TRTInt8Calibrator::getBatch(void** bindings, const char** names,\n+                                 int nbBindings) {\n+  calib_running_.store(false, std::memory_order_release);  // wait for new batch\n+  cond_.notify_all();\n+  while (!calib_running_.load(\n+      std::memory_order_acquire)) {  // wait until new batch arrives\n+    tensorflow::mutex_lock l(cond_mtx_);", "path": "tensorflow/contrib/tensorrt/resources/TRTInt8Calibrator.cc", "position": null, "original_position": 75, "commit_id": "5e5671e692db0533dfec66d63b8e7c8d06bc4942", "original_commit_id": "811c5ebd9510d723217363fffcb258126bec3ea2", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "I have same comment as above about where to put the lock. Basically I'm proposing:\r\n```\r\nsetBatch() {\r\n  {\r\n    mutex_lock l(cond_mtx_);\r\n    while (calib_running_) cond_.wait(l);\r\n    // do things\r\n    calib_running_ = true;\r\n  }\r\n  cond_.notify_all();\r\n}\r\ngetBatch() {\r\n  {\r\n    // It's unfortunate that TF's mutex_lock doesn't provide lock and unlock\r\n    // functions, so the calibration process is not under protection. But it\r\n    // should be fine. \r\n    mutex_lock l(cond_mtx_);\r\n    calib_running_ = false;\r\n  }\r\n  cond_.notify_all();\r\n  {\r\n    mutex_lock l(cond_mtx_);\r\n    while (!calib_running_) cond_.wait(l);\r\n    // do things\r\n  }\r\n}\r\n```\r\nThis way we don't need calib_running_ to be an atomic.", "created_at": "2018-02-28T18:30:27Z", "updated_at": "2018-03-01T22:59:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17309#discussion_r171340534", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17309", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/171340534"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17309#discussion_r171340534"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17309"}}, "body_html": "<p>I have same comment as above about where to put the lock. Basically I'm proposing:</p>\n<pre><code>setBatch() {\n  {\n    mutex_lock l(cond_mtx_);\n    while (calib_running_) cond_.wait(l);\n    // do things\n    calib_running_ = true;\n  }\n  cond_.notify_all();\n}\ngetBatch() {\n  {\n    // It's unfortunate that TF's mutex_lock doesn't provide lock and unlock\n    // functions, so the calibration process is not under protection. But it\n    // should be fine. \n    mutex_lock l(cond_mtx_);\n    calib_running_ = false;\n  }\n  cond_.notify_all();\n  {\n    mutex_lock l(cond_mtx_);\n    while (!calib_running_) cond_.wait(l);\n    // do things\n  }\n}\n</code></pre>\n<p>This way we don't need calib_running_ to be an atomic.</p>", "body_text": "I have same comment as above about where to put the lock. Basically I'm proposing:\nsetBatch() {\n  {\n    mutex_lock l(cond_mtx_);\n    while (calib_running_) cond_.wait(l);\n    // do things\n    calib_running_ = true;\n  }\n  cond_.notify_all();\n}\ngetBatch() {\n  {\n    // It's unfortunate that TF's mutex_lock doesn't provide lock and unlock\n    // functions, so the calibration process is not under protection. But it\n    // should be fine. \n    mutex_lock l(cond_mtx_);\n    calib_running_ = false;\n  }\n  cond_.notify_all();\n  {\n    mutex_lock l(cond_mtx_);\n    while (!calib_running_) cond_.wait(l);\n    // do things\n  }\n}\n\nThis way we don't need calib_running_ to be an atomic."}