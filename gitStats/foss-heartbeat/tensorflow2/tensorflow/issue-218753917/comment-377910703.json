{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377910703", "html_url": "https://github.com/tensorflow/tensorflow/issues/8904#issuecomment-377910703", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8904", "id": 377910703, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzkxMDcwMw==", "user": {"login": "Lidaguo", "id": 15832113, "node_id": "MDQ6VXNlcjE1ODMyMTEz", "avatar_url": "https://avatars0.githubusercontent.com/u/15832113?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lidaguo", "html_url": "https://github.com/Lidaguo", "followers_url": "https://api.github.com/users/Lidaguo/followers", "following_url": "https://api.github.com/users/Lidaguo/following{/other_user}", "gists_url": "https://api.github.com/users/Lidaguo/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lidaguo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lidaguo/subscriptions", "organizations_url": "https://api.github.com/users/Lidaguo/orgs", "repos_url": "https://api.github.com/users/Lidaguo/repos", "events_url": "https://api.github.com/users/Lidaguo/events{/privacy}", "received_events_url": "https://api.github.com/users/Lidaguo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-02T11:25:30Z", "updated_at": "2018-04-02T11:25:30Z", "author_association": "NONE", "body_html": "<p>Hello, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22332205\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petrosmol\">@petrosmol</a> ! Thanks for your particular reply. But now, I have another problem. In my case, I'd like to use the implemented tf.contrib.kernel_methods.RandomFourierFeatureMapper to transform the features from network, and then use the transformed features to construct my loss function.  All the things go well at the beginning of my training procedure. However,  with the increasing of the training epochs, the occupation of GPU grow up. After tens of epochs, the OOM error of GPU occurs. If I don't use the transformation, everything works fine. my simple code as following:<br>\n`<br>\nkernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim = 100, output_dim = 1000,  stddev =5.0, name='rffm')</p>\n<p>model_train = My_model(..., kernel_mapper)</p>\n<p>for epoch_index in xrange(total_epochs):<br>\nrun_epoch(..., model_train)</p>\n<p>`<br>\nHere is the definition of the model,</p>\n<p>`<br>\nclass My_model(..., kernel_mapper):<br>\n....<br>\ninput_feature = ....<br>\nmapped_feature = kernel_mapper.map(input_feature)<br>\n....</p>\n<p>`<br>\nI guess that the  tf.contrib.kernel_methods.RandomFourierFeatureMapper class always stores values for each epoch(or each called situation) and don't free the Memory. Is there any solution for this?</p>", "body_text": "Hello, @petrosmol ! Thanks for your particular reply. But now, I have another problem. In my case, I'd like to use the implemented tf.contrib.kernel_methods.RandomFourierFeatureMapper to transform the features from network, and then use the transformed features to construct my loss function.  All the things go well at the beginning of my training procedure. However,  with the increasing of the training epochs, the occupation of GPU grow up. After tens of epochs, the OOM error of GPU occurs. If I don't use the transformation, everything works fine. my simple code as following:\n`\nkernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim = 100, output_dim = 1000,  stddev =5.0, name='rffm')\nmodel_train = My_model(..., kernel_mapper)\nfor epoch_index in xrange(total_epochs):\nrun_epoch(..., model_train)\n`\nHere is the definition of the model,\n`\nclass My_model(..., kernel_mapper):\n....\ninput_feature = ....\nmapped_feature = kernel_mapper.map(input_feature)\n....\n`\nI guess that the  tf.contrib.kernel_methods.RandomFourierFeatureMapper class always stores values for each epoch(or each called situation) and don't free the Memory. Is there any solution for this?", "body": "Hello, @petrosmol ! Thanks for your particular reply. But now, I have another problem. In my case, I'd like to use the implemented tf.contrib.kernel_methods.RandomFourierFeatureMapper to transform the features from network, and then use the transformed features to construct my loss function.  All the things go well at the beginning of my training procedure. However,  with the increasing of the training epochs, the occupation of GPU grow up. After tens of epochs, the OOM error of GPU occurs. If I don't use the transformation, everything works fine. my simple code as following:\r\n`\r\nkernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim = 100, output_dim = 1000,  stddev =5.0, name='rffm')\r\n\r\nmodel_train = My_model(..., kernel_mapper)\r\n\r\nfor epoch_index in xrange(total_epochs):\r\n    run_epoch(..., model_train)\r\n\r\n`\r\nHere is the definition of the model,\r\n\r\n`\r\nclass My_model(..., kernel_mapper):\r\n     ....\r\n     input_feature = ....\r\n     mapped_feature = kernel_mapper.map(input_feature)\r\n    ....\r\n\r\n`\r\nI guess that the  tf.contrib.kernel_methods.RandomFourierFeatureMapper class always stores values for each epoch(or each called situation) and don't free the Memory. Is there any solution for this?"}