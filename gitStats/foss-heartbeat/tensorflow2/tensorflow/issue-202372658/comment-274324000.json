{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/274324000", "html_url": "https://github.com/tensorflow/tensorflow/issues/7006#issuecomment-274324000", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7006", "id": 274324000, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDMyNDAwMA==", "user": {"login": "robbine", "id": 16010351, "node_id": "MDQ6VXNlcjE2MDEwMzUx", "avatar_url": "https://avatars1.githubusercontent.com/u/16010351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robbine", "html_url": "https://github.com/robbine", "followers_url": "https://api.github.com/users/robbine/followers", "following_url": "https://api.github.com/users/robbine/following{/other_user}", "gists_url": "https://api.github.com/users/robbine/gists{/gist_id}", "starred_url": "https://api.github.com/users/robbine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robbine/subscriptions", "organizations_url": "https://api.github.com/users/robbine/orgs", "repos_url": "https://api.github.com/users/robbine/repos", "events_url": "https://api.github.com/users/robbine/events{/privacy}", "received_events_url": "https://api.github.com/users/robbine/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-22T11:00:47Z", "updated_at": "2017-01-22T18:06:49Z", "author_association": "NONE", "body_html": "<p>here is a more detailed version, I omitted training calls to make it more readable</p>\n<pre><code>graph = tf.Graph()\n        with graph.as_default():\n            input = tf.placeholder(tf.float32, shape=[None, n], name='input')\n            embedding = tf.Variable(tf.random_normal([n, embedding_size]), name='embedding')\n            embedded = tf.matmul(input, embedding, name='embedded')\n            weights = tf.Variable(tf.random_normal([128, 1], stddev=0.01, name='random-normal'), name='weights')\n            logits = tf.matmul(embedded, weights, name='logits')\n            labels_batch = tf.placeholder(tf.float32, name='y-input')\n            cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, labels_batch, name='cross_entropy'), name='cross_entropy_mean')\n            train_op = tf.train.AdamOptimizer().minimize(cross_entropy)\n            tmp_tensor = tf.Variable(valid_data)\n            result = tf.matmul(tmp_tensor, embedding, name='res')\n        with tf.Session(graph=graph) as sess:\n            projector_config = projector.ProjectorConfig()\n            add_embedding = projector_config.embeddings.add()\n            add_embedding.tensor_name = result.name\n            add_embedding.metadata_path = metadata_path\n            summary_writer = tf.summary.FileWriter(log_dir + '/projector', sess.graph)\n            projector.visualize_embeddings(summary_writer, projector_config)\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(result)\n            saver = tf.train.Saver()\n            saver.save(sess, log_dir + '/model.ckpt')\n            summary_writer.close()\n</code></pre>", "body_text": "here is a more detailed version, I omitted training calls to make it more readable\ngraph = tf.Graph()\n        with graph.as_default():\n            input = tf.placeholder(tf.float32, shape=[None, n], name='input')\n            embedding = tf.Variable(tf.random_normal([n, embedding_size]), name='embedding')\n            embedded = tf.matmul(input, embedding, name='embedded')\n            weights = tf.Variable(tf.random_normal([128, 1], stddev=0.01, name='random-normal'), name='weights')\n            logits = tf.matmul(embedded, weights, name='logits')\n            labels_batch = tf.placeholder(tf.float32, name='y-input')\n            cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, labels_batch, name='cross_entropy'), name='cross_entropy_mean')\n            train_op = tf.train.AdamOptimizer().minimize(cross_entropy)\n            tmp_tensor = tf.Variable(valid_data)\n            result = tf.matmul(tmp_tensor, embedding, name='res')\n        with tf.Session(graph=graph) as sess:\n            projector_config = projector.ProjectorConfig()\n            add_embedding = projector_config.embeddings.add()\n            add_embedding.tensor_name = result.name\n            add_embedding.metadata_path = metadata_path\n            summary_writer = tf.summary.FileWriter(log_dir + '/projector', sess.graph)\n            projector.visualize_embeddings(summary_writer, projector_config)\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(result)\n            saver = tf.train.Saver()\n            saver.save(sess, log_dir + '/model.ckpt')\n            summary_writer.close()", "body": "here is a more detailed version, I omitted training calls to make it more readable\r\n\r\n```\r\ngraph = tf.Graph()\r\n        with graph.as_default():\r\n            input = tf.placeholder(tf.float32, shape=[None, n], name='input')\r\n            embedding = tf.Variable(tf.random_normal([n, embedding_size]), name='embedding')\r\n            embedded = tf.matmul(input, embedding, name='embedded')\r\n            weights = tf.Variable(tf.random_normal([128, 1], stddev=0.01, name='random-normal'), name='weights')\r\n            logits = tf.matmul(embedded, weights, name='logits')\r\n            labels_batch = tf.placeholder(tf.float32, name='y-input')\r\n            cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, labels_batch, name='cross_entropy'), name='cross_entropy_mean')\r\n            train_op = tf.train.AdamOptimizer().minimize(cross_entropy)\r\n            tmp_tensor = tf.Variable(valid_data)\r\n            result = tf.matmul(tmp_tensor, embedding, name='res')\r\n        with tf.Session(graph=graph) as sess:\r\n            projector_config = projector.ProjectorConfig()\r\n            add_embedding = projector_config.embeddings.add()\r\n            add_embedding.tensor_name = result.name\r\n            add_embedding.metadata_path = metadata_path\r\n            summary_writer = tf.summary.FileWriter(log_dir + '/projector', sess.graph)\r\n            projector.visualize_embeddings(summary_writer, projector_config)\r\n            sess.run(tf.global_variables_initializer())\r\n            sess.run(tf.local_variables_initializer())\r\n            sess.run(result)\r\n            saver = tf.train.Saver()\r\n            saver.save(sess, log_dir + '/model.ckpt')\r\n            summary_writer.close()\r\n```"}