{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3920", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3920/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3920/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3920/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3920", "id": 172079595, "node_id": "MDU6SXNzdWUxNzIwNzk1OTU=", "number": 3920, "title": "about attention_seq2seq function without embedding", "user": {"login": "Mev-z", "id": 14156269, "node_id": "MDQ6VXNlcjE0MTU2MjY5", "avatar_url": "https://avatars2.githubusercontent.com/u/14156269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mev-z", "html_url": "https://github.com/Mev-z", "followers_url": "https://api.github.com/users/Mev-z/followers", "following_url": "https://api.github.com/users/Mev-z/following{/other_user}", "gists_url": "https://api.github.com/users/Mev-z/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mev-z/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mev-z/subscriptions", "organizations_url": "https://api.github.com/users/Mev-z/orgs", "repos_url": "https://api.github.com/users/Mev-z/repos", "events_url": "https://api.github.com/users/Mev-z/events{/privacy}", "received_events_url": "https://api.github.com/users/Mev-z/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-08-19T08:24:19Z", "updated_at": "2016-08-22T21:19:58Z", "closed_at": "2016-08-22T21:19:58Z", "author_association": "NONE", "body_html": "<p>we already have a function tf.nn.seq2seq.embedding_attention_seq2seq(), but if i want to use an embedding trained from other model, the function is not convenient, do we have a function like tf.nn.seq2seq.attention_seq2seq() that need an embedding para or the inputs embedded.<br>\nThanks!</p>", "body_text": "we already have a function tf.nn.seq2seq.embedding_attention_seq2seq(), but if i want to use an embedding trained from other model, the function is not convenient, do we have a function like tf.nn.seq2seq.attention_seq2seq() that need an embedding para or the inputs embedded.\nThanks!", "body": "we already have a function tf.nn.seq2seq.embedding_attention_seq2seq(), but if i want to use an embedding trained from other model, the function is not convenient, do we have a function like tf.nn.seq2seq.attention_seq2seq() that need an embedding para or the inputs embedded.\nThanks!\n"}