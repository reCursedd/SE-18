{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/381297008", "html_url": "https://github.com/tensorflow/tensorflow/issues/18463#issuecomment-381297008", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18463", "id": 381297008, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTI5NzAwOA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-14T02:20:33Z", "updated_at": "2018-04-14T02:20:33Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6130352\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/eric-czech\">@eric-czech</a> : Is there more text around the exception which would provide more details of the error that you can share?</p>\n<p>The Java API and the Python API have the exact same set of operations compiled in (they both depend on the same underlying native libraries), so it's not as if <code>GatherV2</code> should be excluded from the Java API (though some operations invoked by <code>tf.contrib</code> operations in Python may have to be explicitly loaded - see this, but <code>GatherV2</code> is decidedly a \"core\" operation).</p>\n<p>FWIW, I was unable to reproduce the problem. I used the Python script your provided to generate the saved model and then used the following <code>TFDebug.java</code>:</p>\n<div class=\"highlight highlight-source-java\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-smi\">java.util.Arrays</span>;\n<span class=\"pl-k\">import</span> <span class=\"pl-smi\">org.tensorflow.*</span>;\n<span class=\"pl-k\">import</span> <span class=\"pl-smi\">org.tensorflow.framework.MetaGraphDef</span>;\n\n<span class=\"pl-k\">public</span> <span class=\"pl-k\">class</span> <span class=\"pl-en\">TFDebug</span> {\n  <span class=\"pl-k\">public</span> <span class=\"pl-k\">static</span> <span class=\"pl-k\">void</span> <span class=\"pl-en\">main</span>(<span class=\"pl-k\">String</span>[] <span class=\"pl-v\">args</span>) <span class=\"pl-k\">throws</span> <span class=\"pl-smi\">Exception</span> {\n    <span class=\"pl-smi\">String</span> path <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./saved_model<span class=\"pl-pds\">\"</span></span>;\n    <span class=\"pl-k\">try</span> (<span class=\"pl-smi\">SavedModelBundle</span> model <span class=\"pl-k\">=</span> <span class=\"pl-smi\">SavedModelBundle</span><span class=\"pl-k\">.</span>load(path, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>serve<span class=\"pl-pds\">\"</span></span>);\n        <span class=\"pl-k\">Tensor&lt;<span class=\"pl-smi\">Float</span>&gt;</span> input <span class=\"pl-k\">=</span> <span class=\"pl-smi\">Tensors</span><span class=\"pl-k\">.</span>create(<span class=\"pl-k\">new</span> <span class=\"pl-smi\">float</span>[<span class=\"pl-c1\">5</span>][<span class=\"pl-c1\">5</span>])) {\n      <span class=\"pl-k\">try</span> (<span class=\"pl-k\">Tensor&lt;<span class=\"pl-smi\">Float</span>&gt;</span> output <span class=\"pl-k\">=</span>\n          model\n              .session()\n              .runner()\n              .feed(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Placeholder<span class=\"pl-pds\">\"</span></span>, input)\n              .fetch(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>GatherV2<span class=\"pl-pds\">\"</span></span>)\n              .run()\n              .get(<span class=\"pl-c1\">0</span>)\n              .expect(<span class=\"pl-smi\">Float</span><span class=\"pl-k\">.</span>class)) {\n        <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>out<span class=\"pl-k\">.</span>println(<span class=\"pl-smi\">Arrays</span><span class=\"pl-k\">.</span>deepToString(output<span class=\"pl-k\">.</span>copyTo(<span class=\"pl-k\">new</span> <span class=\"pl-smi\">float</span>[<span class=\"pl-c1\">5</span>][<span class=\"pl-c1\">5</span>])));\n      }\n      <span class=\"pl-smi\">MetaGraphDef</span> metaGraph <span class=\"pl-k\">=</span> <span class=\"pl-smi\">MetaGraphDef</span><span class=\"pl-k\">.</span>parseFrom(model<span class=\"pl-k\">.</span>metaGraphDef());\n    }\n  }\n}</pre></div>\n<p>With the following <code>pom.xml</code>:</p>\n<div class=\"highlight highlight-text-xml\"><pre>&lt;<span class=\"pl-ent\">project</span>&gt;\n    &lt;<span class=\"pl-ent\">modelVersion</span>&gt;4.0.0&lt;/<span class=\"pl-ent\">modelVersion</span>&gt;\n    &lt;<span class=\"pl-ent\">groupId</span>&gt;org.myorg&lt;/<span class=\"pl-ent\">groupId</span>&gt;\n    &lt;<span class=\"pl-ent\">artifactId</span>&gt;issue18463&lt;/<span class=\"pl-ent\">artifactId</span>&gt;\n    &lt;<span class=\"pl-ent\">version</span>&gt;1.0-SNAPSHOT&lt;/<span class=\"pl-ent\">version</span>&gt;\n    &lt;<span class=\"pl-ent\">properties</span>&gt;\n      &lt;<span class=\"pl-ent\">exec</span>.mainClass&gt;TFDebug&lt;/<span class=\"pl-ent\">exec</span>.mainClass&gt;\n      &lt;<span class=\"pl-ent\">maven</span>.compiler.source&gt;1.7&lt;/<span class=\"pl-ent\">maven</span>.compiler.source&gt;\n      &lt;<span class=\"pl-ent\">maven</span>.compiler.target&gt;1.7&lt;/<span class=\"pl-ent\">maven</span>.compiler.target&gt;\n    &lt;/<span class=\"pl-ent\">properties</span>&gt;\n    &lt;<span class=\"pl-ent\">dependencies</span>&gt;\n      &lt;<span class=\"pl-ent\">dependency</span>&gt;\n        &lt;<span class=\"pl-ent\">groupId</span>&gt;org.tensorflow&lt;/<span class=\"pl-ent\">groupId</span>&gt;\n        &lt;<span class=\"pl-ent\">artifactId</span>&gt;libtensorflow&lt;/<span class=\"pl-ent\">artifactId</span>&gt;\n        &lt;<span class=\"pl-ent\">version</span>&gt;1.7.0&lt;/<span class=\"pl-ent\">version</span>&gt;\n      &lt;/<span class=\"pl-ent\">dependency</span>&gt;\n      &lt;<span class=\"pl-ent\">dependency</span>&gt;\n        &lt;<span class=\"pl-ent\">groupId</span>&gt;org.tensorflow&lt;/<span class=\"pl-ent\">groupId</span>&gt;\n        &lt;<span class=\"pl-ent\">artifactId</span>&gt;libtensorflow_jni_gpu&lt;/<span class=\"pl-ent\">artifactId</span>&gt;\n        &lt;<span class=\"pl-ent\">version</span>&gt;1.7.0&lt;/<span class=\"pl-ent\">version</span>&gt;\n      &lt;/<span class=\"pl-ent\">dependency</span>&gt;\n      &lt;<span class=\"pl-ent\">dependency</span>&gt;\n        &lt;<span class=\"pl-ent\">groupId</span>&gt;org.tensorflow&lt;/<span class=\"pl-ent\">groupId</span>&gt;\n        &lt;<span class=\"pl-ent\">artifactId</span>&gt;proto&lt;/<span class=\"pl-ent\">artifactId</span>&gt;\n        &lt;<span class=\"pl-ent\">version</span>&gt;1.7.0&lt;/<span class=\"pl-ent\">version</span>&gt;\n      &lt;/<span class=\"pl-ent\">dependency</span>&gt;\n    &lt;/<span class=\"pl-ent\">dependencies</span>&gt;\n&lt;/<span class=\"pl-ent\">project</span>&gt;</pre></div>\n<p>I ran this with <code>mvn -q clean compile exec:java</code> and got the following:</p>\n<pre><code>mvn -q clean compile exec:java\n2018-04-14 02:00:37.765349: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-04-14 02:00:37.766017: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: ./saved_model\n2018-04-14 02:00:37.979333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-04-14 02:00:37.979768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:02:00.0\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\n2018-04-14 02:00:38.093554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-04-14 02:00:38.093843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: \nname: Quadro K620 major: 5 minor: 0 memoryClockRate(GHz): 1.124\npciBusID: 0000:03:00.0\ntotalMemory: 1.93GiB freeMemory: 759.38MiB\n2018-04-14 02:00:38.093887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1408] Ignoring visible gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0, compute capability: 5.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n2018-04-14 02:00:38.093905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-04-14 02:00:38.545211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-04-14 02:00:38.545246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 \n2018-04-14 02:00:38.545254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N \n2018-04-14 02:00:38.545259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N \n2018-04-14 02:00:38.545512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11374 MB memory) -&gt; physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\n2018-04-14 02:00:38.663901: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\n2018-04-14 02:00:38.663960: I tensorflow/cc/saved_model/loader.cc:171] The specified SavedModel has no variables; no checkpoints were restored.\n2018-04-14 02:00:38.663971: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\n2018-04-14 02:00:38.664041: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 898045 microseconds.\n2018-04-14 02:00:38.669242: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\n[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\n</code></pre>\n<p>What results do you get from running the same?</p>", "body_text": "@eric-czech : Is there more text around the exception which would provide more details of the error that you can share?\nThe Java API and the Python API have the exact same set of operations compiled in (they both depend on the same underlying native libraries), so it's not as if GatherV2 should be excluded from the Java API (though some operations invoked by tf.contrib operations in Python may have to be explicitly loaded - see this, but GatherV2 is decidedly a \"core\" operation).\nFWIW, I was unable to reproduce the problem. I used the Python script your provided to generate the saved model and then used the following TFDebug.java:\nimport java.util.Arrays;\nimport org.tensorflow.*;\nimport org.tensorflow.framework.MetaGraphDef;\n\npublic class TFDebug {\n  public static void main(String[] args) throws Exception {\n    String path = \"./saved_model\";\n    try (SavedModelBundle model = SavedModelBundle.load(path, \"serve\");\n        Tensor<Float> input = Tensors.create(new float[5][5])) {\n      try (Tensor<Float> output =\n          model\n              .session()\n              .runner()\n              .feed(\"Placeholder\", input)\n              .fetch(\"GatherV2\")\n              .run()\n              .get(0)\n              .expect(Float.class)) {\n        System.out.println(Arrays.deepToString(output.copyTo(new float[5][5])));\n      }\n      MetaGraphDef metaGraph = MetaGraphDef.parseFrom(model.metaGraphDef());\n    }\n  }\n}\nWith the following pom.xml:\n<project>\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>org.myorg</groupId>\n    <artifactId>issue18463</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <properties>\n      <exec.mainClass>TFDebug</exec.mainClass>\n      <maven.compiler.source>1.7</maven.compiler.source>\n      <maven.compiler.target>1.7</maven.compiler.target>\n    </properties>\n    <dependencies>\n      <dependency>\n        <groupId>org.tensorflow</groupId>\n        <artifactId>libtensorflow</artifactId>\n        <version>1.7.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.tensorflow</groupId>\n        <artifactId>libtensorflow_jni_gpu</artifactId>\n        <version>1.7.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.tensorflow</groupId>\n        <artifactId>proto</artifactId>\n        <version>1.7.0</version>\n      </dependency>\n    </dependencies>\n</project>\nI ran this with mvn -q clean compile exec:java and got the following:\nmvn -q clean compile exec:java\n2018-04-14 02:00:37.765349: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-04-14 02:00:37.766017: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: ./saved_model\n2018-04-14 02:00:37.979333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-04-14 02:00:37.979768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:02:00.0\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\n2018-04-14 02:00:38.093554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-04-14 02:00:38.093843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: \nname: Quadro K620 major: 5 minor: 0 memoryClockRate(GHz): 1.124\npciBusID: 0000:03:00.0\ntotalMemory: 1.93GiB freeMemory: 759.38MiB\n2018-04-14 02:00:38.093887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1408] Ignoring visible gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0, compute capability: 5.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n2018-04-14 02:00:38.093905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-04-14 02:00:38.545211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-04-14 02:00:38.545246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 \n2018-04-14 02:00:38.545254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N \n2018-04-14 02:00:38.545259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N \n2018-04-14 02:00:38.545512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11374 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\n2018-04-14 02:00:38.663901: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\n2018-04-14 02:00:38.663960: I tensorflow/cc/saved_model/loader.cc:171] The specified SavedModel has no variables; no checkpoints were restored.\n2018-04-14 02:00:38.663971: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\n2018-04-14 02:00:38.664041: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 898045 microseconds.\n2018-04-14 02:00:38.669242: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\n[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\n\nWhat results do you get from running the same?", "body": "@eric-czech : Is there more text around the exception which would provide more details of the error that you can share?\r\n\r\nThe Java API and the Python API have the exact same set of operations compiled in (they both depend on the same underlying native libraries), so it's not as if `GatherV2` should be excluded from the Java API (though some operations invoked by `tf.contrib` operations in Python may have to be explicitly loaded - see this, but `GatherV2` is decidedly a \"core\" operation).\r\n\r\nFWIW, I was unable to reproduce the problem. I used the Python script your provided to generate the saved model and then used the following `TFDebug.java`:\r\n\r\n```java\r\nimport java.util.Arrays;\r\nimport org.tensorflow.*;\r\nimport org.tensorflow.framework.MetaGraphDef;\r\n\r\npublic class TFDebug {\r\n  public static void main(String[] args) throws Exception {\r\n    String path = \"./saved_model\";\r\n    try (SavedModelBundle model = SavedModelBundle.load(path, \"serve\");\r\n        Tensor<Float> input = Tensors.create(new float[5][5])) {\r\n      try (Tensor<Float> output =\r\n          model\r\n              .session()\r\n              .runner()\r\n              .feed(\"Placeholder\", input)\r\n              .fetch(\"GatherV2\")\r\n              .run()\r\n              .get(0)\r\n              .expect(Float.class)) {\r\n        System.out.println(Arrays.deepToString(output.copyTo(new float[5][5])));\r\n      }\r\n      MetaGraphDef metaGraph = MetaGraphDef.parseFrom(model.metaGraphDef());\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWith the following `pom.xml`:\r\n\r\n```xml\r\n<project>\r\n    <modelVersion>4.0.0</modelVersion>\r\n    <groupId>org.myorg</groupId>\r\n    <artifactId>issue18463</artifactId>\r\n    <version>1.0-SNAPSHOT</version>\r\n    <properties>\r\n      <exec.mainClass>TFDebug</exec.mainClass>\r\n      <maven.compiler.source>1.7</maven.compiler.source>\r\n      <maven.compiler.target>1.7</maven.compiler.target>\r\n    </properties>\r\n    <dependencies>\r\n      <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>libtensorflow</artifactId>\r\n        <version>1.7.0</version>\r\n      </dependency>\r\n      <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>libtensorflow_jni_gpu</artifactId>\r\n        <version>1.7.0</version>\r\n      </dependency>\r\n      <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>proto</artifactId>\r\n        <version>1.7.0</version>\r\n      </dependency>\r\n    </dependencies>\r\n</project>\r\n```\r\n\r\nI ran this with `mvn -q clean compile exec:java` and got the following:\r\n\r\n```\r\nmvn -q clean compile exec:java\r\n2018-04-14 02:00:37.765349: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-04-14 02:00:37.766017: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: ./saved_model\r\n2018-04-14 02:00:37.979333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-04-14 02:00:37.979768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\r\n2018-04-14 02:00:38.093554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-04-14 02:00:38.093843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: \r\nname: Quadro K620 major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 1.93GiB freeMemory: 759.38MiB\r\n2018-04-14 02:00:38.093887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1408] Ignoring visible gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0, compute capability: 5.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\n2018-04-14 02:00:38.093905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-14 02:00:38.545211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-14 02:00:38.545246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 \r\n2018-04-14 02:00:38.545254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N \r\n2018-04-14 02:00:38.545259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N \r\n2018-04-14 02:00:38.545512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11374 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-04-14 02:00:38.663901: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\r\n2018-04-14 02:00:38.663960: I tensorflow/cc/saved_model/loader.cc:171] The specified SavedModel has no variables; no checkpoints were restored.\r\n2018-04-14 02:00:38.663971: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\r\n2018-04-14 02:00:38.664041: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 898045 microseconds.\r\n2018-04-14 02:00:38.669242: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\r\n[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]\r\n```\r\n\r\nWhat results do you get from running the same? "}