{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7910", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7910/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7910/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7910/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7910", "id": 210375623, "node_id": "MDU6SXNzdWUyMTAzNzU2MjM=", "number": 7910, "title": "How tensorflow handles complex gradient ?", "user": {"login": "zzd1992", "id": 11853283, "node_id": "MDQ6VXNlcjExODUzMjgz", "avatar_url": "https://avatars3.githubusercontent.com/u/11853283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zzd1992", "html_url": "https://github.com/zzd1992", "followers_url": "https://api.github.com/users/zzd1992/followers", "following_url": "https://api.github.com/users/zzd1992/following{/other_user}", "gists_url": "https://api.github.com/users/zzd1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/zzd1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zzd1992/subscriptions", "organizations_url": "https://api.github.com/users/zzd1992/orgs", "repos_url": "https://api.github.com/users/zzd1992/repos", "events_url": "https://api.github.com/users/zzd1992/events{/privacy}", "received_events_url": "https://api.github.com/users/zzd1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-02-27T02:54:29Z", "updated_at": "2017-02-27T05:27:09Z", "closed_at": "2017-02-27T05:27:09Z", "author_association": "NONE", "body_html": "<p>Let <strong>z</strong> is a complex variable, <strong>C(z)</strong> is its conjugation.<br>\nIn complex analysis theory, the derivative of <strong>C(z)</strong> w.r.t <strong>z</strong> don't exist. But in tesnsorflow, we can calculate <strong>dC(z)/dz</strong> and the result is just <strong>1</strong>.<br>\nHere is an example:</p>\n<blockquote>\n<p>x = tf.placeholder('complex64',(2,2))<br>\ny = tf.reduce_sum(tf.conj(x))<br>\nz = tf.gradients(y,x)<br>\nsess = tf.Session()<br>\nX = np.random.rand(2,2)+1.j*np.random.rand(2,2)<br>\nX = X.astype('complex64')<br>\nZ = sess.run(z,{x:X})[0]</p>\n</blockquote>\n<p>The input <strong>X</strong> is</p>\n<blockquote>\n<p>[[ 0.17014372+0.71475762j  0.57455420+0.00144318j]<br>\n[0.57871044+0.61303568j  0.48074263+0.7623235j ]]</p>\n</blockquote>\n<p>and the result <strong>Z</strong> is</p>\n<blockquote>\n<p>[[ 1.-0.j  1.-0.j]<br>\n[1.-0.j  1.-0.j]]</p>\n</blockquote>\n<p>I don't understand why the gradient is set to be <strong>1</strong>?<br>\nAnd I want to know <strong>how tensorflow handles the complex gradients in general</strong>.</p>", "body_text": "Let z is a complex variable, C(z) is its conjugation.\nIn complex analysis theory, the derivative of C(z) w.r.t z don't exist. But in tesnsorflow, we can calculate dC(z)/dz and the result is just 1.\nHere is an example:\n\nx = tf.placeholder('complex64',(2,2))\ny = tf.reduce_sum(tf.conj(x))\nz = tf.gradients(y,x)\nsess = tf.Session()\nX = np.random.rand(2,2)+1.j*np.random.rand(2,2)\nX = X.astype('complex64')\nZ = sess.run(z,{x:X})[0]\n\nThe input X is\n\n[[ 0.17014372+0.71475762j  0.57455420+0.00144318j]\n[0.57871044+0.61303568j  0.48074263+0.7623235j ]]\n\nand the result Z is\n\n[[ 1.-0.j  1.-0.j]\n[1.-0.j  1.-0.j]]\n\nI don't understand why the gradient is set to be 1?\nAnd I want to know how tensorflow handles the complex gradients in general.", "body": "Let **z** is a complex variable, **C(z)** is its conjugation.\r\nIn complex analysis theory, the derivative of **C(z)** w.r.t **z** don't exist. But in tesnsorflow, we can calculate **dC(z)/dz** and the result is just **1**.\r\nHere is an example:\r\n>x = tf.placeholder('complex64',(2,2))\r\ny = tf.reduce_sum(tf.conj(x))\r\nz = tf.gradients(y,x)\r\nsess = tf.Session()\r\nX = np.random.rand(2,2)+1.j*np.random.rand(2,2)\r\nX = X.astype('complex64')\r\nZ = sess.run(z,{x:X})[0]\r\n\r\nThe input **X** is\r\n>[[ 0.17014372+0.71475762j  0.57455420+0.00144318j]\r\n    [0.57871044+0.61303568j  0.48074263+0.7623235j ]]\r\n\r\nand the result **Z** is\r\n>[[ 1.-0.j  1.-0.j]\r\n    [1.-0.j  1.-0.j]]\r\n       \r\nI don't understand why the gradient is set to be **1**?\r\nAnd I want to know **how tensorflow handles the complex gradients in general**."}