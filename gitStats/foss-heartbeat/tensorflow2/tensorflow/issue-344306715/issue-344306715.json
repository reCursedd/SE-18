{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21118", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21118/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21118/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21118/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21118", "id": 344306715, "node_id": "MDU6SXNzdWUzNDQzMDY3MTU=", "number": 21118, "title": "use custom operators  Failed to allocate tensors.", "user": {"login": "iChiaGuo", "id": 13581022, "node_id": "MDQ6VXNlcjEzNTgxMDIy", "avatar_url": "https://avatars3.githubusercontent.com/u/13581022?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iChiaGuo", "html_url": "https://github.com/iChiaGuo", "followers_url": "https://api.github.com/users/iChiaGuo/followers", "following_url": "https://api.github.com/users/iChiaGuo/following{/other_user}", "gists_url": "https://api.github.com/users/iChiaGuo/gists{/gist_id}", "starred_url": "https://api.github.com/users/iChiaGuo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iChiaGuo/subscriptions", "organizations_url": "https://api.github.com/users/iChiaGuo/orgs", "repos_url": "https://api.github.com/users/iChiaGuo/repos", "events_url": "https://api.github.com/users/iChiaGuo/events{/privacy}", "received_events_url": "https://api.github.com/users/iChiaGuo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-07-25T06:15:11Z", "updated_at": "2018-08-23T18:31:51Z", "closed_at": "2018-08-23T18:31:51Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:iphone 6s</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.9</li>\n<li><strong>Python version</strong>:3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I follow <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md\">this</a> custom Abs operator,but I ran my code throw error:<br>\ntensorflow/contrib/lite/simple_memory_arena.cc:82 it-&gt;size != alloc.size (52272 != 0)<br>\n2018-07-25 14:03:50.795332+0800 testTF[1477:647442] Failed to allocate tensors.</p>\n<h3>Source code / logs</h3>\n<pre><code>TfLiteStatus AbsPrepare(TfLiteContext* context, TfLiteNode* node) {\nusing namespace tflite;\nTF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\nTF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\nconst TfLiteTensor* input = GetInput(context, node, 0);\nTfLiteTensor* output = GetOutput(context, node, 0);\n\nint num_dims = NumDimensions(input);\n\nTfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);\nfor (int i=0; i&lt;num_dims; ++i) {\n    output_size-&gt;data[i] = input-&gt;dims-&gt;data[i];\n}\n\nreturn context-&gt;ResizeTensor(context, output, output_size);\n}\n\nTfLiteStatus AbsEval(TfLiteContext* context, TfLiteNode* node) {\nusing namespace tflite;\nconst TfLiteTensor* input = GetInput(context, node,0);\nTfLiteTensor* output = GetOutput(context, node,0);\n\nfloat* input_data = input-&gt;data.f;\nfloat* output_data = output-&gt;data.f;\n\nsize_t count = 1;\nint num_dims = NumDimensions(input);\nfor (int i = 0; i &lt; num_dims; ++i) {\n    count *= input-&gt;dims-&gt;data[i];\n}\n\nfor (size_t i=0; i&lt;count; ++i) {\n    output_data[i] = std::abs(input_data[i]);\n}\nreturn kTfLiteOk;\n}\n\nTfLiteRegistration* Register_ABS() {\nstatic TfLiteRegistration r = {nullptr, nullptr, AbsPrepare, AbsEval};\nreturn &amp;r;\n}\nint width = 24;\n int height = 24;\n NSString* RunInferenceOnImage() {\n NSString* graph = @\"tf_Rnet\";\n const int num_threads = 4;\n std::string input_layer_type = \"float\";\n std::vector&lt;int&gt; sizes = {1, height, width, 3};\n //std::vector&lt;int&gt; sizes = {1, 224, 224, 3};\n\nconst NSString* graph_path = FilePathForResourceName(graph, @\"tflite\");\n\nstd::unique_ptr&lt;tflite::FlatBufferModel&gt; model(\n                                               tflite::FlatBufferModel::BuildFromFile([graph_path UTF8String]));\nif (!model) {\n    NSLog(@\"Failed to mmap model %@.\", graph);\n    exit(-1);\n}\nNSLog(@\"Loaded model %@.\", graph);\nmodel-&gt;error_reporter();\nNSLog(@\"Resolved reporter.\");\n\n #ifdef TFLITE_CUSTOM_OPS_HEADER\ntflite::MutableOpResolver resolver;\nRegisterSelectedOps(&amp;resolver);\n#else\ntflite::ops::builtin::BuiltinOpResolver resolver;\nresolver.AddCustom(\"Abs\", Register_ABS());\n#endif\n\nstd::unique_ptr&lt;tflite::Interpreter&gt; interpreter;\ntflite::InterpreterBuilder(*model, resolver)(&amp;interpreter);\nif (!interpreter) {\n    NSLog(@\"Failed to construct interpreter.\");\n    exit(-1);\n}\n\nif (num_threads != -1) {\n    interpreter-&gt;SetNumThreads(num_threads);\n}\n\nint input = interpreter-&gt;inputs()[0];\nstd::vector&lt;int&gt; tensor_input =interpreter-&gt;inputs();\nfor (int index = 0; index &lt; tensor_input.size(); index++)\n{\n    TfLiteTensor* tensor = interpreter-&gt;tensor(tensor_input[index]);\n    TfLiteIntArray* dims = tensor-&gt;dims;\n    NSLog(@\"fi\");\n}\nif (input_layer_type != \"string\") {\n    interpreter-&gt;ResizeInputTensor(input, sizes);\n}\n\nif (interpreter-&gt;AllocateTensors() != kTfLiteOk) {\n    NSLog(@\"Failed to allocate tensors.\");\n    exit(-1);\n}\nNSString* image_path = FilePathForResourceName(@\"face\", @\"jpg\");\nint image_width;\nint image_height;\nint image_channels;\nstd::vector&lt;uint8_t&gt; image_data =\nLoadImageFromFile([image_path UTF8String], &amp;image_width, &amp;image_height, &amp;image_channels);\nconst int wanted_width = width;\nconst int wanted_height = height;\nconst int wanted_channels = 3;\nconst float input_mean = 127.5f;\nconst float input_std = 127.5f;\n//  const float input_mean = 127.5f;\n//  const float input_std = 127.5f;\nassert(image_channels &gt;= wanted_channels);\nuint8_t* in = image_data.data();\nfloat* out = interpreter-&gt;typed_tensor&lt;float&gt;(input);\nfor (int y = 0; y &lt; wanted_height; ++y) {\n    const int in_y = (y * image_height) / wanted_height;\n    uint8_t* in_row = in + (in_y * image_width * image_channels);\n    float* out_row = out + (y * wanted_width * wanted_channels);\n    for (int x = 0; x &lt; wanted_width; ++x) {\n        const int in_x = (x * image_width) / wanted_width;\n        uint8_t* in_pixel = in_row + (in_x * image_channels);\n        float* out_pixel = out_row + (x * wanted_channels);\n        for (int c = 0; c &lt; wanted_channels; ++c) {\n            out_pixel[c] = (in_pixel[c] - input_mean) / input_std;\n        }\n    }\n}\n\nif (interpreter-&gt;Invoke() != kTfLiteOk) {\n    NSLog(@\"Failed to invoke!\");\n    exit(-1);\n}\n\nstd::vector&lt;int&gt; tensor_out =interpreter-&gt;outputs();\n\nsize_t size = tensor_out.size();\nfor (int index = 0; index &lt; size; index++)\n{\n    TfLiteTensor* tensor = interpreter-&gt;tensor(tensor_out[index]);\n    TfLiteIntArray* dims = tensor-&gt;dims;\n    float* output = interpreter-&gt;typed_output_tensor&lt;float&gt;(index);\n    for (int i = 0; i&lt;dims-&gt;data[1]; i++)\n    {\n        NSLog(@\"%@\",@(output[i]));\n    }\n    NSLog(@\"finish\");\n}\nreturn NULL;\n}\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iphone 6s\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):1.9\nPython version:3.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nI follow this custom Abs operator,but I ran my code throw error:\ntensorflow/contrib/lite/simple_memory_arena.cc:82 it->size != alloc.size (52272 != 0)\n2018-07-25 14:03:50.795332+0800 testTF[1477:647442] Failed to allocate tensors.\nSource code / logs\nTfLiteStatus AbsPrepare(TfLiteContext* context, TfLiteNode* node) {\nusing namespace tflite;\nTF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\nTF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\nconst TfLiteTensor* input = GetInput(context, node, 0);\nTfLiteTensor* output = GetOutput(context, node, 0);\n\nint num_dims = NumDimensions(input);\n\nTfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);\nfor (int i=0; i<num_dims; ++i) {\n    output_size->data[i] = input->dims->data[i];\n}\n\nreturn context->ResizeTensor(context, output, output_size);\n}\n\nTfLiteStatus AbsEval(TfLiteContext* context, TfLiteNode* node) {\nusing namespace tflite;\nconst TfLiteTensor* input = GetInput(context, node,0);\nTfLiteTensor* output = GetOutput(context, node,0);\n\nfloat* input_data = input->data.f;\nfloat* output_data = output->data.f;\n\nsize_t count = 1;\nint num_dims = NumDimensions(input);\nfor (int i = 0; i < num_dims; ++i) {\n    count *= input->dims->data[i];\n}\n\nfor (size_t i=0; i<count; ++i) {\n    output_data[i] = std::abs(input_data[i]);\n}\nreturn kTfLiteOk;\n}\n\nTfLiteRegistration* Register_ABS() {\nstatic TfLiteRegistration r = {nullptr, nullptr, AbsPrepare, AbsEval};\nreturn &r;\n}\nint width = 24;\n int height = 24;\n NSString* RunInferenceOnImage() {\n NSString* graph = @\"tf_Rnet\";\n const int num_threads = 4;\n std::string input_layer_type = \"float\";\n std::vector<int> sizes = {1, height, width, 3};\n //std::vector<int> sizes = {1, 224, 224, 3};\n\nconst NSString* graph_path = FilePathForResourceName(graph, @\"tflite\");\n\nstd::unique_ptr<tflite::FlatBufferModel> model(\n                                               tflite::FlatBufferModel::BuildFromFile([graph_path UTF8String]));\nif (!model) {\n    NSLog(@\"Failed to mmap model %@.\", graph);\n    exit(-1);\n}\nNSLog(@\"Loaded model %@.\", graph);\nmodel->error_reporter();\nNSLog(@\"Resolved reporter.\");\n\n #ifdef TFLITE_CUSTOM_OPS_HEADER\ntflite::MutableOpResolver resolver;\nRegisterSelectedOps(&resolver);\n#else\ntflite::ops::builtin::BuiltinOpResolver resolver;\nresolver.AddCustom(\"Abs\", Register_ABS());\n#endif\n\nstd::unique_ptr<tflite::Interpreter> interpreter;\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\nif (!interpreter) {\n    NSLog(@\"Failed to construct interpreter.\");\n    exit(-1);\n}\n\nif (num_threads != -1) {\n    interpreter->SetNumThreads(num_threads);\n}\n\nint input = interpreter->inputs()[0];\nstd::vector<int> tensor_input =interpreter->inputs();\nfor (int index = 0; index < tensor_input.size(); index++)\n{\n    TfLiteTensor* tensor = interpreter->tensor(tensor_input[index]);\n    TfLiteIntArray* dims = tensor->dims;\n    NSLog(@\"fi\");\n}\nif (input_layer_type != \"string\") {\n    interpreter->ResizeInputTensor(input, sizes);\n}\n\nif (interpreter->AllocateTensors() != kTfLiteOk) {\n    NSLog(@\"Failed to allocate tensors.\");\n    exit(-1);\n}\nNSString* image_path = FilePathForResourceName(@\"face\", @\"jpg\");\nint image_width;\nint image_height;\nint image_channels;\nstd::vector<uint8_t> image_data =\nLoadImageFromFile([image_path UTF8String], &image_width, &image_height, &image_channels);\nconst int wanted_width = width;\nconst int wanted_height = height;\nconst int wanted_channels = 3;\nconst float input_mean = 127.5f;\nconst float input_std = 127.5f;\n//  const float input_mean = 127.5f;\n//  const float input_std = 127.5f;\nassert(image_channels >= wanted_channels);\nuint8_t* in = image_data.data();\nfloat* out = interpreter->typed_tensor<float>(input);\nfor (int y = 0; y < wanted_height; ++y) {\n    const int in_y = (y * image_height) / wanted_height;\n    uint8_t* in_row = in + (in_y * image_width * image_channels);\n    float* out_row = out + (y * wanted_width * wanted_channels);\n    for (int x = 0; x < wanted_width; ++x) {\n        const int in_x = (x * image_width) / wanted_width;\n        uint8_t* in_pixel = in_row + (in_x * image_channels);\n        float* out_pixel = out_row + (x * wanted_channels);\n        for (int c = 0; c < wanted_channels; ++c) {\n            out_pixel[c] = (in_pixel[c] - input_mean) / input_std;\n        }\n    }\n}\n\nif (interpreter->Invoke() != kTfLiteOk) {\n    NSLog(@\"Failed to invoke!\");\n    exit(-1);\n}\n\nstd::vector<int> tensor_out =interpreter->outputs();\n\nsize_t size = tensor_out.size();\nfor (int index = 0; index < size; index++)\n{\n    TfLiteTensor* tensor = interpreter->tensor(tensor_out[index]);\n    TfLiteIntArray* dims = tensor->dims;\n    float* output = interpreter->typed_output_tensor<float>(index);\n    for (int i = 0; i<dims->data[1]; i++)\n    {\n        NSLog(@\"%@\",@(output[i]));\n    }\n    NSLog(@\"finish\");\n}\nreturn NULL;\n}", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:iphone 6s\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.9\r\n- **Python version**:3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI follow [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md) custom Abs operator,but I ran my code throw error:\r\ntensorflow/contrib/lite/simple_memory_arena.cc:82 it->size != alloc.size (52272 != 0)\r\n2018-07-25 14:03:50.795332+0800 testTF[1477:647442] Failed to allocate tensors.\r\n\r\n### Source code / logs\r\n    TfLiteStatus AbsPrepare(TfLiteContext* context, TfLiteNode* node) {\r\n    using namespace tflite;\r\n    TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\r\n    TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\r\n    \r\n    const TfLiteTensor* input = GetInput(context, node, 0);\r\n    TfLiteTensor* output = GetOutput(context, node, 0);\r\n    \r\n    int num_dims = NumDimensions(input);\r\n    \r\n    TfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);\r\n    for (int i=0; i<num_dims; ++i) {\r\n        output_size->data[i] = input->dims->data[i];\r\n    }\r\n    \r\n    return context->ResizeTensor(context, output, output_size);\r\n    }\r\n\r\n    TfLiteStatus AbsEval(TfLiteContext* context, TfLiteNode* node) {\r\n    using namespace tflite;\r\n    const TfLiteTensor* input = GetInput(context, node,0);\r\n    TfLiteTensor* output = GetOutput(context, node,0);\r\n    \r\n    float* input_data = input->data.f;\r\n    float* output_data = output->data.f;\r\n    \r\n    size_t count = 1;\r\n    int num_dims = NumDimensions(input);\r\n    for (int i = 0; i < num_dims; ++i) {\r\n        count *= input->dims->data[i];\r\n    }\r\n    \r\n    for (size_t i=0; i<count; ++i) {\r\n        output_data[i] = std::abs(input_data[i]);\r\n    }\r\n    return kTfLiteOk;\r\n    }\r\n\r\n    TfLiteRegistration* Register_ABS() {\r\n    static TfLiteRegistration r = {nullptr, nullptr, AbsPrepare, AbsEval};\r\n    return &r;\r\n    }\r\n    int width = 24;\r\n     int height = 24;\r\n     NSString* RunInferenceOnImage() {\r\n     NSString* graph = @\"tf_Rnet\";\r\n     const int num_threads = 4;\r\n     std::string input_layer_type = \"float\";\r\n     std::vector<int> sizes = {1, height, width, 3};\r\n     //std::vector<int> sizes = {1, 224, 224, 3};\r\n    \r\n    const NSString* graph_path = FilePathForResourceName(graph, @\"tflite\");\r\n    \r\n    std::unique_ptr<tflite::FlatBufferModel> model(\r\n                                                   tflite::FlatBufferModel::BuildFromFile([graph_path UTF8String]));\r\n    if (!model) {\r\n        NSLog(@\"Failed to mmap model %@.\", graph);\r\n        exit(-1);\r\n    }\r\n    NSLog(@\"Loaded model %@.\", graph);\r\n    model->error_reporter();\r\n    NSLog(@\"Resolved reporter.\");\r\n    \r\n     #ifdef TFLITE_CUSTOM_OPS_HEADER\r\n    tflite::MutableOpResolver resolver;\r\n    RegisterSelectedOps(&resolver);\r\n    #else\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    resolver.AddCustom(\"Abs\", Register_ABS());\r\n    #endif\r\n    \r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n    if (!interpreter) {\r\n        NSLog(@\"Failed to construct interpreter.\");\r\n        exit(-1);\r\n    }\r\n    \r\n    if (num_threads != -1) {\r\n        interpreter->SetNumThreads(num_threads);\r\n    }\r\n    \r\n    int input = interpreter->inputs()[0];\r\n    std::vector<int> tensor_input =interpreter->inputs();\r\n    for (int index = 0; index < tensor_input.size(); index++)\r\n    {\r\n        TfLiteTensor* tensor = interpreter->tensor(tensor_input[index]);\r\n        TfLiteIntArray* dims = tensor->dims;\r\n        NSLog(@\"fi\");\r\n    }\r\n    if (input_layer_type != \"string\") {\r\n        interpreter->ResizeInputTensor(input, sizes);\r\n    }\r\n    \r\n    if (interpreter->AllocateTensors() != kTfLiteOk) {\r\n        NSLog(@\"Failed to allocate tensors.\");\r\n        exit(-1);\r\n    }\r\n    NSString* image_path = FilePathForResourceName(@\"face\", @\"jpg\");\r\n    int image_width;\r\n    int image_height;\r\n    int image_channels;\r\n    std::vector<uint8_t> image_data =\r\n    LoadImageFromFile([image_path UTF8String], &image_width, &image_height, &image_channels);\r\n    const int wanted_width = width;\r\n    const int wanted_height = height;\r\n    const int wanted_channels = 3;\r\n    const float input_mean = 127.5f;\r\n    const float input_std = 127.5f;\r\n    //  const float input_mean = 127.5f;\r\n    //  const float input_std = 127.5f;\r\n    assert(image_channels >= wanted_channels);\r\n    uint8_t* in = image_data.data();\r\n    float* out = interpreter->typed_tensor<float>(input);\r\n    for (int y = 0; y < wanted_height; ++y) {\r\n        const int in_y = (y * image_height) / wanted_height;\r\n        uint8_t* in_row = in + (in_y * image_width * image_channels);\r\n        float* out_row = out + (y * wanted_width * wanted_channels);\r\n        for (int x = 0; x < wanted_width; ++x) {\r\n            const int in_x = (x * image_width) / wanted_width;\r\n            uint8_t* in_pixel = in_row + (in_x * image_channels);\r\n            float* out_pixel = out_row + (x * wanted_channels);\r\n            for (int c = 0; c < wanted_channels; ++c) {\r\n                out_pixel[c] = (in_pixel[c] - input_mean) / input_std;\r\n            }\r\n        }\r\n    }\r\n    \r\n    if (interpreter->Invoke() != kTfLiteOk) {\r\n        NSLog(@\"Failed to invoke!\");\r\n        exit(-1);\r\n    }\r\n    \r\n    std::vector<int> tensor_out =interpreter->outputs();\r\n\r\n    size_t size = tensor_out.size();\r\n    for (int index = 0; index < size; index++)\r\n    {\r\n        TfLiteTensor* tensor = interpreter->tensor(tensor_out[index]);\r\n        TfLiteIntArray* dims = tensor->dims;\r\n        float* output = interpreter->typed_output_tensor<float>(index);\r\n        for (int i = 0; i<dims->data[1]; i++)\r\n        {\r\n            NSLog(@\"%@\",@(output[i]));\r\n        }\r\n        NSLog(@\"finish\");\r\n    }\r\n    return NULL;\r\n    }"}