{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362182756", "html_url": "https://github.com/tensorflow/tensorflow/issues/9868#issuecomment-362182756", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9868", "id": 362182756, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjE4Mjc1Ng==", "user": {"login": "sakaia", "id": 144597, "node_id": "MDQ6VXNlcjE0NDU5Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/144597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sakaia", "html_url": "https://github.com/sakaia", "followers_url": "https://api.github.com/users/sakaia/followers", "following_url": "https://api.github.com/users/sakaia/following{/other_user}", "gists_url": "https://api.github.com/users/sakaia/gists{/gist_id}", "starred_url": "https://api.github.com/users/sakaia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sakaia/subscriptions", "organizations_url": "https://api.github.com/users/sakaia/orgs", "repos_url": "https://api.github.com/users/sakaia/repos", "events_url": "https://api.github.com/users/sakaia/events{/privacy}", "received_events_url": "https://api.github.com/users/sakaia/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T07:36:48Z", "updated_at": "2018-02-01T07:36:48Z", "author_association": "NONE", "body_html": "<p>I am trying to modify imdb_fasttext.py as follows (4points added).<br>\nBut the chrome shows simple timeline for timeline.keras.json.<br>\nIs this correct usage for chrome timeline ?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">'''</span>This example demonstrates the use of fasttext for text classification</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Based on Joulin et al's paper:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Bags of Tricks for Efficient Text Classification</span>\n<span class=\"pl-s\">https://arxiv.org/abs/1607.01759</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Results on IMDB datasets with uni and bi-gram embeddings:</span>\n<span class=\"pl-s\">    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.</span>\n<span class=\"pl-s\">    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">from</span> keras.preprocessing <span class=\"pl-k\">import</span> sequence\n<span class=\"pl-k\">from</span> keras.models <span class=\"pl-k\">import</span> Sequential\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> Dense\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> Embedding\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> GlobalAveragePooling1D\n<span class=\"pl-k\">from</span> keras.datasets <span class=\"pl-k\">import</span> imdb\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Add RunMetadata for timeline 1/4</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.client <span class=\"pl-k\">import</span> timeline\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_ngram_set</span>(<span class=\"pl-smi\">input_list</span>, <span class=\"pl-smi\">ngram_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Extract a set of n-grams from a list of integers.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)</span>\n<span class=\"pl-s\">    {(4, 9), (4, 1), (1, 4), (9, 4)}</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)</span>\n<span class=\"pl-s\">    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">set</span>(<span class=\"pl-c1\">zip</span>(<span class=\"pl-k\">*</span>[input_list[i:] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(ngram_value)]))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">add_ngram</span>(<span class=\"pl-smi\">sequences</span>, <span class=\"pl-smi\">token_indice</span>, <span class=\"pl-smi\">ngram_range</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Augment the input list of list (sequences) by appending n-grams values.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Example: adding bi-gram</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>add_ngram(sequences, token_indice, ngram_range=2)</span>\n<span class=\"pl-s\">    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Example: adding tri-gram</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>add_ngram(sequences, token_indice, ngram_range=3)</span>\n<span class=\"pl-s\">    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    new_sequences <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> input_list <span class=\"pl-k\">in</span> sequences:\n        new_list <span class=\"pl-k\">=</span> input_list[:]\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(new_list) <span class=\"pl-k\">-</span> ngram_range <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n            <span class=\"pl-k\">for</span> ngram_value <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>, ngram_range <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n                ngram <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>(new_list[i:i <span class=\"pl-k\">+</span> ngram_value])\n                <span class=\"pl-k\">if</span> ngram <span class=\"pl-k\">in</span> token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    <span class=\"pl-k\">return</span> new_sequences\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Set parameters:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ngram_range = 2 will add bi-grams features</span>\nngram_range <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nmax_features <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20000</span>\nmaxlen <span class=\"pl-k\">=</span> <span class=\"pl-c1\">400</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nembedding_dims <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loading data...<span class=\"pl-pds\">'</span></span>)\n(x_train, y_train), (x_test, y_test) <span class=\"pl-k\">=</span> imdb.load_data(<span class=\"pl-v\">num_words</span><span class=\"pl-k\">=</span>max_features)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">len</span>(x_train), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train sequences<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">len</span>(x_test), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test sequences<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average train sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_train)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average test sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_test)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n\n<span class=\"pl-k\">if</span> ngram_range <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span>:\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Adding <span class=\"pl-c1\">{}</span>-gram features<span class=\"pl-pds\">'</span></span>.format(ngram_range))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create set of unique n-gram from the training set.</span>\n    ngram_set <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>()\n    <span class=\"pl-k\">for</span> input_list <span class=\"pl-k\">in</span> x_train:\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>, ngram_range <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n            set_of_ngram <span class=\"pl-k\">=</span> create_ngram_set(input_list, <span class=\"pl-v\">ngram_value</span><span class=\"pl-k\">=</span>i)\n            ngram_set.update(set_of_ngram)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Dictionary mapping n-gram token to a unique integer.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Integer values are greater than max_features in order</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> to avoid collision with existing features.</span>\n    start_index <span class=\"pl-k\">=</span> max_features <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n    token_indice <span class=\"pl-k\">=</span> {v: k <span class=\"pl-k\">+</span> start_index <span class=\"pl-k\">for</span> k, v <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(ngram_set)}\n    indice_token <span class=\"pl-k\">=</span> {token_indice[k]: k <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> token_indice}\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> max_features is the highest integer that could be found in the dataset.</span>\n    max_features <span class=\"pl-k\">=</span> np.max(<span class=\"pl-c1\">list</span>(indice_token.keys())) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Augmenting x_train and x_test with n-grams features</span>\n    x_train <span class=\"pl-k\">=</span> add_ngram(x_train, token_indice, ngram_range)\n    x_test <span class=\"pl-k\">=</span> add_ngram(x_test, token_indice, ngram_range)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average train sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_train)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average test sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_test)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Pad sequences (samples x time)<span class=\"pl-pds\">'</span></span>)\nx_train <span class=\"pl-k\">=</span> sequence.pad_sequences(x_train, <span class=\"pl-v\">maxlen</span><span class=\"pl-k\">=</span>maxlen)\nx_test <span class=\"pl-k\">=</span> sequence.pad_sequences(x_test, <span class=\"pl-v\">maxlen</span><span class=\"pl-k\">=</span>maxlen)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x_train shape:<span class=\"pl-pds\">'</span></span>, x_train.shape)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x_test shape:<span class=\"pl-pds\">'</span></span>, x_test.shape)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Build model...<span class=\"pl-pds\">'</span></span>)\nmodel <span class=\"pl-k\">=</span> Sequential()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> we start off with an efficient embedding layer which maps</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> our vocab indices into embedding_dims dimensions</span>\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    <span class=\"pl-v\">input_length</span><span class=\"pl-k\">=</span>maxlen))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> we add a GlobalAveragePooling1D, which will average the embeddings</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> of all words in the document</span>\nmodel.add(GlobalAveragePooling1D())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> We project onto a single unit output layer, and squash it with a sigmoid:</span>\nmodel.add(Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Add RunMetadata for timeline 2/4</span>\nrun_options <span class=\"pl-k\">=</span> tf.RunOptions(<span class=\"pl-v\">trace_level</span><span class=\"pl-k\">=</span>tf.RunOptions.<span class=\"pl-c1\">FULL_TRACE</span>)\nrun_metadata <span class=\"pl-k\">=</span> tf.RunMetadata()\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>binary_crossentropy<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>adam<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>],\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Add RunMetadata for timeline 3/4</span>\n              <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>run_options,\n              <span class=\"pl-v\">run_metadata</span><span class=\"pl-k\">=</span>run_metadata)\n\nmodel.fit(x_train, y_train,\n          <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n          <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span>epochs,\n          <span class=\"pl-v\">validation_data</span><span class=\"pl-k\">=</span>(x_test, y_test))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Add RunMetadata for timeline 4/4</span>\ntrace <span class=\"pl-k\">=</span> timeline.Timeline(<span class=\"pl-v\">step_stats</span><span class=\"pl-k\">=</span>run_metadata.step_stats)\n<span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timeline.keras.json<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n    f.write(trace.generate_chrome_trace_format())</pre></div>", "body_text": "I am trying to modify imdb_fasttext.py as follows (4points added).\nBut the chrome shows simple timeline for timeline.keras.json.\nIs this correct usage for chrome timeline ?\n'''This example demonstrates the use of fasttext for text classification\n\nBased on Joulin et al's paper:\n\nBags of Tricks for Efficient Text Classification\nhttps://arxiv.org/abs/1607.01759\n\nResults on IMDB datasets with uni and bi-gram embeddings:\n    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n'''\n\nfrom __future__ import print_function\nimport numpy as np\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import GlobalAveragePooling1D\nfrom keras.datasets import imdb\n# Add RunMetadata for timeline 1/4\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\ndef create_ngram_set(input_list, ngram_value=2):\n    \"\"\"\n    Extract a set of n-grams from a list of integers.\n\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    \"\"\"\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n\n\ndef add_ngram(sequences, token_indice, ngram_range=2):\n    \"\"\"\n    Augment the input list of list (sequences) by appending n-grams values.\n\n    Example: adding bi-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n\n    Example: adding tri-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n    \"\"\"\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for i in range(len(new_list) - ngram_range + 1):\n            for ngram_value in range(2, ngram_range + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences\n\n# Set parameters:\n# ngram_range = 2 will add bi-grams features\nngram_range = 1\nmax_features = 20000\nmaxlen = 400\nbatch_size = 32\nembedding_dims = 50\nepochs = 5\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n\nif ngram_range > 1:\n    print('Adding {}-gram features'.format(ngram_range))\n    # Create set of unique n-gram from the training set.\n    ngram_set = set()\n    for input_list in x_train:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index = max_features + 1\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token = {token_indice[k]: k for k in token_indice}\n\n    # max_features is the highest integer that could be found in the dataset.\n    max_features = np.max(list(indice_token.keys())) + 1\n\n    # Augmenting x_train and x_test with n-grams features\n    x_train = add_ngram(x_train, token_indice, ngram_range)\n    x_test = add_ngram(x_test, token_indice, ngram_range)\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\n\n# we add a GlobalAveragePooling1D, which will average the embeddings\n# of all words in the document\nmodel.add(GlobalAveragePooling1D())\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Add RunMetadata for timeline 2/4\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'],\n# Add RunMetadata for timeline 3/4\n              options=run_options,\n              run_metadata=run_metadata)\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_test, y_test))\n\n# Add RunMetadata for timeline 4/4\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\nwith open('timeline.keras.json', 'w') as f:\n    f.write(trace.generate_chrome_trace_format())", "body": "I am trying to modify imdb_fasttext.py as follows (4points added). \r\nBut the chrome shows simple timeline for timeline.keras.json.\r\nIs this correct usage for chrome timeline ?\r\n\r\n```Python\r\n'''This example demonstrates the use of fasttext for text classification\r\n\r\nBased on Joulin et al's paper:\r\n\r\nBags of Tricks for Efficient Text Classification\r\nhttps://arxiv.org/abs/1607.01759\r\n\r\nResults on IMDB datasets with uni and bi-gram embeddings:\r\n    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\r\n    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\r\n'''\r\n\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import Embedding\r\nfrom keras.layers import GlobalAveragePooling1D\r\nfrom keras.datasets import imdb\r\n# Add RunMetadata for timeline 1/4\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\ndef create_ngram_set(input_list, ngram_value=2):\r\n    \"\"\"\r\n    Extract a set of n-grams from a list of integers.\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\r\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\r\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\r\n    \"\"\"\r\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\r\n\r\n\r\ndef add_ngram(sequences, token_indice, ngram_range=2):\r\n    \"\"\"\r\n    Augment the input list of list (sequences) by appending n-grams values.\r\n\r\n    Example: adding bi-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\r\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\r\n\r\n    Example: adding tri-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\r\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\r\n    \"\"\"\r\n    new_sequences = []\r\n    for input_list in sequences:\r\n        new_list = input_list[:]\r\n        for i in range(len(new_list) - ngram_range + 1):\r\n            for ngram_value in range(2, ngram_range + 1):\r\n                ngram = tuple(new_list[i:i + ngram_value])\r\n                if ngram in token_indice:\r\n                    new_list.append(token_indice[ngram])\r\n        new_sequences.append(new_list)\r\n\r\n    return new_sequences\r\n\r\n# Set parameters:\r\n# ngram_range = 2 will add bi-grams features\r\nngram_range = 1\r\nmax_features = 20000\r\nmaxlen = 400\r\nbatch_size = 32\r\nembedding_dims = 50\r\nepochs = 5\r\n\r\nprint('Loading data...')\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\nprint(len(x_train), 'train sequences')\r\nprint(len(x_test), 'test sequences')\r\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nif ngram_range > 1:\r\n    print('Adding {}-gram features'.format(ngram_range))\r\n    # Create set of unique n-gram from the training set.\r\n    ngram_set = set()\r\n    for input_list in x_train:\r\n        for i in range(2, ngram_range + 1):\r\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\r\n            ngram_set.update(set_of_ngram)\r\n\r\n    # Dictionary mapping n-gram token to a unique integer.\r\n    # Integer values are greater than max_features in order\r\n    # to avoid collision with existing features.\r\n    start_index = max_features + 1\r\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\r\n    indice_token = {token_indice[k]: k for k in token_indice}\r\n\r\n    # max_features is the highest integer that could be found in the dataset.\r\n    max_features = np.max(list(indice_token.keys())) + 1\r\n\r\n    # Augmenting x_train and x_test with n-grams features\r\n    x_train = add_ngram(x_train, token_indice, ngram_range)\r\n    x_test = add_ngram(x_test, token_indice, ngram_range)\r\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nprint('Pad sequences (samples x time)')\r\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\nprint('x_train shape:', x_train.shape)\r\nprint('x_test shape:', x_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\n\r\n# we start off with an efficient embedding layer which maps\r\n# our vocab indices into embedding_dims dimensions\r\nmodel.add(Embedding(max_features,\r\n                    embedding_dims,\r\n                    input_length=maxlen))\r\n\r\n# we add a GlobalAveragePooling1D, which will average the embeddings\r\n# of all words in the document\r\nmodel.add(GlobalAveragePooling1D())\r\n\r\n# We project onto a single unit output layer, and squash it with a sigmoid:\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\n# Add RunMetadata for timeline 2/4\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'],\r\n# Add RunMetadata for timeline 3/4\r\n              options=run_options,\r\n              run_metadata=run_metadata)\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          validation_data=(x_test, y_test))\r\n\r\n# Add RunMetadata for timeline 4/4\r\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\nwith open('timeline.keras.json', 'w') as f:\r\n    f.write(trace.generate_chrome_trace_format())\r\n```"}