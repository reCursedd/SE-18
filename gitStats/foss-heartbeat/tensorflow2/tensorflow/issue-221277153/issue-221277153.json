{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9164", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9164/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9164/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9164/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9164", "id": 221277153, "node_id": "MDU6SXNzdWUyMjEyNzcxNTM=", "number": 9164, "title": "tf.slim should not use relu as a default activation function", "user": {"login": "carpedm20", "id": 3346407, "node_id": "MDQ6VXNlcjMzNDY0MDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3346407?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carpedm20", "html_url": "https://github.com/carpedm20", "followers_url": "https://api.github.com/users/carpedm20/followers", "following_url": "https://api.github.com/users/carpedm20/following{/other_user}", "gists_url": "https://api.github.com/users/carpedm20/gists{/gist_id}", "starred_url": "https://api.github.com/users/carpedm20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carpedm20/subscriptions", "organizations_url": "https://api.github.com/users/carpedm20/orgs", "repos_url": "https://api.github.com/users/carpedm20/repos", "events_url": "https://api.github.com/users/carpedm20/events{/privacy}", "received_events_url": "https://api.github.com/users/carpedm20/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-12T14:15:41Z", "updated_at": "2017-04-12T15:17:15Z", "closed_at": "2017-04-12T15:17:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It's hard to understand the reason why relu is chosen as a default activation function. It looks like the choice of default activation function is against common sense. If someone never read the actual code or forget about it, it's so easy to make a mistake.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1387\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1387</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L822\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L822</a></p>", "body_text": "It's hard to understand the reason why relu is chosen as a default activation function. It looks like the choice of default activation function is against common sense. If someone never read the actual code or forget about it, it's so easy to make a mistake.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1387\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L822", "body": "It's hard to understand the reason why relu is chosen as a default activation function. It looks like the choice of default activation function is against common sense. If someone never read the actual code or forget about it, it's so easy to make a mistake.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1387\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L822"}