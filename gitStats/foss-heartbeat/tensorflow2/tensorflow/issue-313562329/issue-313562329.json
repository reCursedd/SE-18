{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18443", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18443/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18443/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18443/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18443", "id": 313562329, "node_id": "MDU6SXNzdWUzMTM1NjIzMjk=", "number": 18443, "title": "In Matmul, is FP16 x FP16 accumulated to FP16 or FP32?", "user": {"login": "jiazhe0909", "id": 8175586, "node_id": "MDQ6VXNlcjgxNzU1ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8175586?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiazhe0909", "html_url": "https://github.com/jiazhe0909", "followers_url": "https://api.github.com/users/jiazhe0909/followers", "following_url": "https://api.github.com/users/jiazhe0909/following{/other_user}", "gists_url": "https://api.github.com/users/jiazhe0909/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiazhe0909/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiazhe0909/subscriptions", "organizations_url": "https://api.github.com/users/jiazhe0909/orgs", "repos_url": "https://api.github.com/users/jiazhe0909/repos", "events_url": "https://api.github.com/users/jiazhe0909/events{/privacy}", "received_events_url": "https://api.github.com/users/jiazhe0909/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-12T03:16:33Z", "updated_at": "2018-05-15T00:52:31Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.7</li>\n<li><strong>Python version</strong>:</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.0</li>\n<li><strong>GPU model and memory</strong>:<br>\nV100 16GB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<p>It is not clear in documents that, in Matmul, FP16xFP16 is accumulated to FP16 or FP32.<br>\nThis choice affects not only training accuracy, but also TensorCore computing performance on Volta GPU.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 16.04\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\n1.7\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\n9.0\nGPU model and memory:\nV100 16GB\nExact command to reproduce:\n\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nIt is not clear in documents that, in Matmul, FP16xFP16 is accumulated to FP16 or FP32.\nThis choice affects not only training accuracy, but also TensorCore computing performance on Volta GPU.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.7\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **GPU model and memory**:\r\nV100 16GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIt is not clear in documents that, in Matmul, FP16xFP16 is accumulated to FP16 or FP32. \r\nThis choice affects not only training accuracy, but also TensorCore computing performance on Volta GPU.\r\n\r\n"}