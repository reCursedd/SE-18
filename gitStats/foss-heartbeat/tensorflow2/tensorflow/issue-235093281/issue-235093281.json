{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10638", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10638/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10638/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10638/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10638", "id": 235093281, "node_id": "MDU6SXNzdWUyMzUwOTMyODE=", "number": 10638, "title": "Might be a bug for contrib.legacy_seq2seq", "user": {"login": "liu115", "id": 8998128, "node_id": "MDQ6VXNlcjg5OTgxMjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/8998128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liu115", "html_url": "https://github.com/liu115", "followers_url": "https://api.github.com/users/liu115/followers", "following_url": "https://api.github.com/users/liu115/following{/other_user}", "gists_url": "https://api.github.com/users/liu115/gists{/gist_id}", "starred_url": "https://api.github.com/users/liu115/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liu115/subscriptions", "organizations_url": "https://api.github.com/users/liu115/orgs", "repos_url": "https://api.github.com/users/liu115/repos", "events_url": "https://api.github.com/users/liu115/events{/privacy}", "received_events_url": "https://api.github.com/users/liu115/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-06-11T21:30:21Z", "updated_at": "2017-08-16T17:15:24Z", "closed_at": "2017-08-16T17:10:25Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I am using the embedding_attention_seq2seq with output_projection. The document from <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_rnn_decoder\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_rnn_decoder</a> says,</p>\n<blockquote>\n<p>outputs: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x num_decoder_symbols] containing the generated outputs.</p>\n</blockquote>\n<p>But the output seems to be the output before projection when I used it. So I go through the source code from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py</a> ,</p>\n<p>embedding_attention_seq2seq is base on the embedding_attention_decoder and attention_decoder. It has to give output_size to attention_decoder, but output_size is set to None when output_projection is not None.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">embedding_attention_seq2seq</span>(<span class=\"pl-smi\">encoder_inputs</span>,\n                                <span class=\"pl-smi\">decoder_inputs</span>,\n                                <span class=\"pl-smi\">cell</span>,\n                                <span class=\"pl-smi\">num_encoder_symbols</span>,\n                                <span class=\"pl-smi\">num_decoder_symbols</span>,\n                                <span class=\"pl-smi\">embedding_size</span>,\n                                <span class=\"pl-smi\">num_heads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                                <span class=\"pl-smi\">output_projection</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                <span class=\"pl-smi\">feed_previous</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                <span class=\"pl-smi\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                <span class=\"pl-smi\">initial_state_attention</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-c1\">...</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> skip</span>\n    output_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    <span class=\"pl-k\">if</span> output_projection <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n      cell <span class=\"pl-k\">=</span> core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size <span class=\"pl-k\">=</span> num_decoder_symbols\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(feed_previous, <span class=\"pl-c1\">bool</span>):\n      <span class=\"pl-k\">return</span> embedding_attention_decoder(\n          decoder_inputs,\n          encoder_state,\n          attention_states,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          <span class=\"pl-v\">num_heads</span><span class=\"pl-k\">=</span>num_heads,\n          <span class=\"pl-v\">output_size</span><span class=\"pl-k\">=</span>output_size,\n          <span class=\"pl-v\">output_projection</span><span class=\"pl-k\">=</span>output_projection,\n          <span class=\"pl-v\">feed_previous</span><span class=\"pl-k\">=</span>feed_previous,\n          <span class=\"pl-v\">initial_state_attention</span><span class=\"pl-k\">=</span>initial_state_attention)</pre></div>\n<p>When output_size is None, the output_size is simply the cell's output_size. And so the shape of output for embedding_attention_seq2seq will be [batch_size x cell's output_size]  rather than [batch_size x num_decoder_symbols]</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">attention_decoder</span>(<span class=\"pl-smi\">decoder_inputs</span>,\n                      <span class=\"pl-smi\">initial_state</span>,\n                      <span class=\"pl-smi\">attention_states</span>,\n                      <span class=\"pl-smi\">cell</span>,\n                      <span class=\"pl-smi\">output_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                      <span class=\"pl-smi\">num_heads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                      <span class=\"pl-smi\">loop_function</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                      <span class=\"pl-smi\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                      <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                      <span class=\"pl-smi\">initial_state_attention</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n  <span class=\"pl-c1\">...</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> skip</span>\n  <span class=\"pl-k\">if</span> output_size <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n    output_size <span class=\"pl-k\">=</span> cell.output_size\n  <span class=\"pl-c1\">...</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> skip</span>\n      <span class=\"pl-k\">with</span> variable_scope.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>AttnOutputProjection<span class=\"pl-pds\">\"</span></span>):\n        output <span class=\"pl-k\">=</span> linear([cell_output] <span class=\"pl-k\">+</span> attns, output_size, <span class=\"pl-c1\">True</span>)\n      <span class=\"pl-k\">if</span> loop_function <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        prev <span class=\"pl-k\">=</span> output\n      outputs.append(output)\n\n  <span class=\"pl-k\">return</span> outputs, state</pre></div>\n<p>Thanks.</p>", "body_text": "Hi,\nI am using the embedding_attention_seq2seq with output_projection. The document from https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_rnn_decoder says,\n\noutputs: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x num_decoder_symbols] containing the generated outputs.\n\nBut the output seems to be the output before projection when I used it. So I go through the source code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py ,\nembedding_attention_seq2seq is base on the embedding_attention_decoder and attention_decoder. It has to give output_size to attention_decoder, but output_size is set to None when output_projection is not None.\ndef embedding_attention_seq2seq(encoder_inputs,\n                                decoder_inputs,\n                                cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n    ... # skip\n    output_size = None\n    if output_projection is None:\n      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n    if isinstance(feed_previous, bool):\n      return embedding_attention_decoder(\n          decoder_inputs,\n          encoder_state,\n          attention_states,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          num_heads=num_heads,\n          output_size=output_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          initial_state_attention=initial_state_attention)\nWhen output_size is None, the output_size is simply the cell's output_size. And so the shape of output for embedding_attention_seq2seq will be [batch_size x cell's output_size]  rather than [batch_size x num_decoder_symbols]\ndef attention_decoder(decoder_inputs,\n                      initial_state,\n                      attention_states,\n                      cell,\n                      output_size=None,\n                      num_heads=1,\n                      loop_function=None,\n                      dtype=None,\n                      scope=None,\n                      initial_state_attention=False):\n  ... # skip\n  if output_size is None:\n    output_size = cell.output_size\n  ... # skip\n      with variable_scope.variable_scope(\"AttnOutputProjection\"):\n        output = linear([cell_output] + attns, output_size, True)\n      if loop_function is not None:\n        prev = output\n      outputs.append(output)\n\n  return outputs, state\nThanks.", "body": "Hi, \r\n\r\nI am using the embedding_attention_seq2seq with output_projection. The document from https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_rnn_decoder says,\r\n> outputs: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x num_decoder_symbols] containing the generated outputs.\r\n\r\nBut the output seems to be the output before projection when I used it. So I go through the source code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py , \r\n\r\nembedding_attention_seq2seq is base on the embedding_attention_decoder and attention_decoder. It has to give output_size to attention_decoder, but output_size is set to None when output_projection is not None.\r\n```python\r\ndef embedding_attention_seq2seq(encoder_inputs,\r\n                                decoder_inputs,\r\n                                cell,\r\n                                num_encoder_symbols,\r\n                                num_decoder_symbols,\r\n                                embedding_size,\r\n                                num_heads=1,\r\n                                output_projection=None,\r\n                                feed_previous=False,\r\n                                dtype=None,\r\n                                scope=None,\r\n                                initial_state_attention=False):\r\n    ... # skip\r\n    output_size = None\r\n    if output_projection is None:\r\n      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n    if isinstance(feed_previous, bool):\r\n      return embedding_attention_decoder(\r\n          decoder_inputs,\r\n          encoder_state,\r\n          attention_states,\r\n          cell,\r\n          num_decoder_symbols,\r\n          embedding_size,\r\n          num_heads=num_heads,\r\n          output_size=output_size,\r\n          output_projection=output_projection,\r\n          feed_previous=feed_previous,\r\n          initial_state_attention=initial_state_attention)\r\n```\r\n\r\nWhen output_size is None, the output_size is simply the cell's output_size. And so the shape of output for embedding_attention_seq2seq will be [batch_size x cell's output_size]  rather than [batch_size x num_decoder_symbols]\r\n```python\r\ndef attention_decoder(decoder_inputs,\r\n                      initial_state,\r\n                      attention_states,\r\n                      cell,\r\n                      output_size=None,\r\n                      num_heads=1,\r\n                      loop_function=None,\r\n                      dtype=None,\r\n                      scope=None,\r\n                      initial_state_attention=False):\r\n  ... # skip\r\n  if output_size is None:\r\n    output_size = cell.output_size\r\n  ... # skip\r\n      with variable_scope.variable_scope(\"AttnOutputProjection\"):\r\n        output = linear([cell_output] + attns, output_size, True)\r\n      if loop_function is not None:\r\n        prev = output\r\n      outputs.append(output)\r\n\r\n  return outputs, state\r\n```\r\n\r\n\r\nThanks.\r\n\r\n\r\n"}