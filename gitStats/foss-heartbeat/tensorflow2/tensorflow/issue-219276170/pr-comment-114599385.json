{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/114599385", "pull_request_review_id": 36065451, "id": 114599385, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNDU5OTM4NQ==", "diff_hunk": "@@ -0,0 +1,236 @@\n+/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/util/work_sharder.h\"\n+\n+namespace tensorflow{\n+\n+namespace {\n+\n+template <typename T>\n+struct MemCopier {\n+  \n+  inline void Copy(const T* input, T* output, int64 size){\n+    std::copy(input, input + size, output);\n+  }\n+};\n+\n+} // end namespace\n+\n+template <typename T>\n+void RepeatCPUImpl(const Tensor& input,\n+                   const typename TTypes<int32>::ConstFlat& repeats_flat,\n+                   int axis, Tensor* output) {\n+  auto input_flat = input.flat<T>();\n+  auto output_flat = output->flat<T>();\n+  MemCopier<T> copier;\n+    \n+  // A batch is inner dimensions > axis\n+  size_t batch_size = 1;\n+  int32 dims = input.shape().dims();\n+  for (int32 i = axis + 1; i < dims; ++i) {\n+    batch_size *= input.shape().dim_size(i);\n+  }\n+  int64 num_batch = input_flat.size() / batch_size;\n+  \n+  const T* in = input_flat.data();\n+  T* out = output_flat.data();\n+  if (repeats_flat.size() == 1) {\n+    for (int64 i = 0; i < num_batch; ++i) {\n+      int32 repeat = repeats_flat(0);\n+      for (int64 j = 0; j < repeat; ++j) {\n+        copier.Copy(in, out, batch_size);\n+        out += batch_size;\n+      }\n+      in += batch_size;\n+    }\n+  } else {\n+    for (int64 i = 0; i < num_batch; ++i) {\n+      int32 repeat = repeats_flat(i % repeats_flat.size());\n+      for (int64 j = 0; j < repeat; ++j) {\n+        copier.Copy(in, out, batch_size);\n+        out += batch_size;\n+      }\n+      in += batch_size;\n+    }\n+  }\n+}\n+\n+template <typename T>\n+void RepeatCPUImplV2(DeviceBase* d, const Tensor& input,\n+                     const typename TTypes<int32>::ConstFlat& repeats_flat,\n+                     int axis, int64 cost_per_unit, Tensor* output) {\n+  auto input_flat = input.flat<T>();\n+  auto output_flat = output->flat<T>();\n+  MemCopier<T> copier;\n+  \n+  // A batch is inner dimensions > axis\n+  // A group is inner dimensions >= axis\n+  int64 batch_size = 1;\n+  int32 dims = input.shape().dims();\n+  for (int32 i = axis + 1; i < dims; ++i) {\n+    batch_size *= input.shape().dim_size(i);\n+  }\n+  int64 group_pre_size = batch_size * input.shape().dim_size(axis);\n+  int64 group_size = batch_size * output->shape().dim_size(axis);\n+  \n+  auto worker_threads = d->tensorflow_cpu_worker_threads();\n+  int num_threads = std::min(4, worker_threads->num_threads);\n+  // strings define a different amount of work (generally much more) compared\n+  // with standard POD, so we parallelize differently.\n+  if (!std::is_same<T, string>::value) {\n+    num_threads =\n+        static_cast<int>(std::min<int64>(num_threads, output_flat.size() / 4096));\n+  }\n+  \n+  if (num_threads == 0) {\n+    RepeatCPUImpl<T>(input, repeats_flat, axis, output);\n+  }\n+  \n+  auto work = [input_flat, repeats_flat, axis, &copier,\n+               batch_size, group_pre_size, group_size, &output_flat](\n+      int64 start, int64 end) {\n+    const T* in = input_flat.data();\n+    T* out = output_flat.data();\n+    T* out_start = out + start;\n+    T* out_end = out + end;\n+              \n+    if (repeats_flat.size() == 1) {\n+      int64 out_batch_size = batch_size * repeats_flat(0);\n+      in += (start/out_batch_size) * batch_size;\n+      out += (start/batch_size) * batch_size;\n+      \n+      // handle partial out_batch at start\n+      if (start % out_batch_size != 0) {\n+        int64 offset = start % batch_size;\n+        // handle partial batch at start\n+        if (offset != 0) {\n+          if (out + batch_size > out_end) {\n+            copier.Copy(in + offset, out_start, (out_end-out) - offset);\n+            return ;\n+          }\n+          copier.Copy(in + offset, out_start, batch_size - offset);\n+          out += batch_size;\n+        }\n+        \n+        int64 repeat_skip = (out-output_flat.data())/batch_size % repeats_flat(0);\n+        for(int64 i = repeat_skip; i < repeats_flat(0); ++i) {\n+          if (out + batch_size > out_end) {\n+            copier.Copy(in, out, out_end - out);\n+            return ;\n+          }\n+          copier.Copy(in, out, batch_size);\n+          out += batch_size;\n+        }\n+        in += batch_size;\n+      }\n+      \n+      // handle remaining data\n+      int64 batch_to_cpy = (out_end-out) / out_batch_size + 1;\n+      for (int64 i = 0; i < batch_to_cpy; ++i) {\n+        for (int64 j = 0; j < repeats_flat(0); ++j) {\n+          if (out + batch_size > out_end) {\n+            copier.Copy(in, out, out_end - out);\n+            return ;\n+          }\n+          copier.Copy(in, out, batch_size);\n+          out += batch_size;\n+        }\n+        in += batch_size;\n+      }\n+      \n+    } else {\n+      int64 skip_group = start / group_size;\n+      in += skip_group * group_pre_size;", "path": "tensorflow/contrib/repeat/kernels/repeat_op_cpu_impl.cc", "position": null, "original_position": 161, "commit_id": "a81df6aff5a399d566502ef7fdf3dedd38ee0f46", "original_commit_id": "6d725adcfa53bda6126e4097d096e9601e1902e3", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "body": "why a different mechanism here than above?  group_pre_size and group_size could have been used above too?", "created_at": "2017-05-03T17:06:09Z", "updated_at": "2017-09-27T06:06:37Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8954#discussion_r114599385", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8954", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/114599385"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8954#discussion_r114599385"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8954"}}, "body_html": "<p>why a different mechanism here than above?  group_pre_size and group_size could have been used above too?</p>", "body_text": "why a different mechanism here than above?  group_pre_size and group_size could have been used above too?"}