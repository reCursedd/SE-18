{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/167660317", "html_url": "https://github.com/tensorflow/tensorflow/issues/620#issuecomment-167660317", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/620", "id": 167660317, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NzY2MDMxNw==", "user": {"login": "keveman", "id": 229914, "node_id": "MDQ6VXNlcjIyOTkxNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/229914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keveman", "html_url": "https://github.com/keveman", "followers_url": "https://api.github.com/users/keveman/followers", "following_url": "https://api.github.com/users/keveman/following{/other_user}", "gists_url": "https://api.github.com/users/keveman/gists{/gist_id}", "starred_url": "https://api.github.com/users/keveman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keveman/subscriptions", "organizations_url": "https://api.github.com/users/keveman/orgs", "repos_url": "https://api.github.com/users/keveman/repos", "events_url": "https://api.github.com/users/keveman/events{/privacy}", "received_events_url": "https://api.github.com/users/keveman/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-28T21:18:36Z", "updated_at": "2015-12-28T21:18:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Jacob,</p>\n<p>The 'small trick' refers to the fact that the image tensor returned by <code>mnist.train.next_batch(100)</code> has the shape <code>[100, 784]</code>. Note that the batch is the innermost dimension. To use it directly, we shape the <code>W</code> tensor as <code>[784, 10]</code> to use it directly in <code>tf.matmul(x, W)</code> to produce <code>10</code> activations. So if you transposed the output of <code>mnist.train.next_batch(100)</code> to <code>[784, 100]</code> and shaped your <code>W</code> as <code>[10, 784]</code>, then indeed you can call <code>tf.matmul(W, x)</code>. Is that what you were trying and were seeing a different accuracy?</p>", "body_text": "Hi Jacob,\nThe 'small trick' refers to the fact that the image tensor returned by mnist.train.next_batch(100) has the shape [100, 784]. Note that the batch is the innermost dimension. To use it directly, we shape the W tensor as [784, 10] to use it directly in tf.matmul(x, W) to produce 10 activations. So if you transposed the output of mnist.train.next_batch(100) to [784, 100] and shaped your W as [10, 784], then indeed you can call tf.matmul(W, x). Is that what you were trying and were seeing a different accuracy?", "body": "Hi Jacob,\n\nThe 'small trick' refers to the fact that the image tensor returned by `mnist.train.next_batch(100)` has the shape `[100, 784]`. Note that the batch is the innermost dimension. To use it directly, we shape the `W` tensor as `[784, 10]` to use it directly in `tf.matmul(x, W)` to produce `10` activations. So if you transposed the output of `mnist.train.next_batch(100)` to `[784, 100]` and shaped your `W` as `[10, 784]`, then indeed you can call `tf.matmul(W, x)`. Is that what you were trying and were seeing a different accuracy?\n"}