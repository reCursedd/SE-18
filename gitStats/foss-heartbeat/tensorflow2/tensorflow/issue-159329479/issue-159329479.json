{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2750", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2750/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2750/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2750/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2750", "id": 159329479, "node_id": "MDU6SXNzdWUxNTkzMjk0Nzk=", "number": 2750, "title": "Host a TensorBuilder patch for TensorFlow", "user": {"login": "cgarciae", "id": 5862228, "node_id": "MDQ6VXNlcjU4NjIyMjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5862228?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cgarciae", "html_url": "https://github.com/cgarciae", "followers_url": "https://api.github.com/users/cgarciae/followers", "following_url": "https://api.github.com/users/cgarciae/following{/other_user}", "gists_url": "https://api.github.com/users/cgarciae/gists{/gist_id}", "starred_url": "https://api.github.com/users/cgarciae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cgarciae/subscriptions", "organizations_url": "https://api.github.com/users/cgarciae/orgs", "repos_url": "https://api.github.com/users/cgarciae/repos", "events_url": "https://api.github.com/users/cgarciae/events{/privacy}", "received_events_url": "https://api.github.com/users/cgarciae/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2016-06-09T05:13:37Z", "updated_at": "2016-06-17T16:56:22Z", "closed_at": "2016-06-11T00:03:36Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<blockquote>\n<p><a href=\"https://github.com/cgarciae/tensorbuilder\">TensorBuilder</a> is light-weight extensible library that enables you to easily create complex deep neural networks using functions from any Tensor-based library through a functional fluent immutable API based on the Builder Pattern. As a side effect, TensorBuilder is a library that gives expressive power to any Tensor-based library that decide to implement a TensorBuilder patch.</p>\n</blockquote>\n<p>TensorBuilder hosts a patch for <code>tensorflow</code>, using this patch you can rewrite the code like this</p>\n<pre><code>import tensorflow as tf\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nnet = tf.contrib.layers.fully_connected(x, 10, activation_fn=tf.nn.tanh) # tanh(x * w + b)\nnet = tf.nn.dropout(net, keep_prob) # dropout(x, keep_prob)\nh =  tf.contrib.layers.fully_connected(net, 3, activation_fn=tf.nn.softmax) # softmax(x * w + b)\n</code></pre>\n<p>as this</p>\n<pre><code>import tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.slim\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .fully_connected(10, activation_fn=tf.nn.tanh)\n    .map(tf.nn.dropout, keep_prob)\n    .fully_connected(3, activation_fn=tf.nn.softmax)\n    .tensor()\n)\n</code></pre>\n<p>or even like this using a stronger patch</p>\n<pre><code>import tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .tanh_layer(10) \n    .dropout(keep_prob)\n    .softmax_layer(3)\n    .tensor()\n)\n</code></pre>\n<p>The patch for the <code>tensorflow</code> library currently has full support for branching, see this example with the strong patch</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorbuilder <span class=\"pl-k\">as</span> tb\n<span class=\"pl-k\">import</span> tensorbuilder.patches.tensorflow.patch\n\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">5</span>])\nkeep_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\n\nh <span class=\"pl-k\">=</span> (\n    x.builder()\n    .fully_connected(<span class=\"pl-c1\">10</span>)\n    .branch(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">root</span>:\n    [\n        root\n        .relu_layer(<span class=\"pl-c1\">3</span>)\n    ,\n        root\n        .tanh_layer(<span class=\"pl-c1\">9</span>)\n        .branch(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">root2</span>:\n        [\n          root2\n          .sigmoid_layer(<span class=\"pl-c1\">6</span>)\n        ,\n          root2\n          .dropout(keep_prob)\n          .softmax_layer(<span class=\"pl-c1\">8</span>)\n        ])\n    ])\n    .sigmoid_layer(<span class=\"pl-c1\">6</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span>&lt;== This connects all the branches to a single output layer</span>\n    .tensor()\n)</pre></div>\n<p>TensorBuilder even has a DSL that you get for free, so you could rewrite the previous as</p>\n<pre><code>import tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\nimport tensorbuilder.dsl as dl #&lt;== Notice the alias\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = x.builder().pipe(\n    dl.fully_connected(10),\n    [\n        dl.relu_layer(3)\n    ,\n        (dl.tanh_layer(9),\n        [\n            dl.sigmoid_layer(6)\n        ,\n            dl\n            .dropout(keep_prob)\n            .softmax_layer(8)\n        ])\n    ],\n    dl.sigmoid_layer(6)\n    .tensor()\n)\n</code></pre>\n<h3>Invitation</h3>\n<p>I'd like to invite you to eventually host the TensorFlow patch for TensorBuilder on the TensorFlow library, because TensorBuilder really is just an empty shell, it doesn't even know about tensors, its a sort of functional tool for making non-composable/fluent APIs into composable/fluent APIs (in e.g. Elixir or Elm you wouldn't even need it), but it benefits a lot from libraries, and I am sure that tensorflow users will benefit a lot from it too.</p>", "body_text": "Hi,\n\nTensorBuilder is light-weight extensible library that enables you to easily create complex deep neural networks using functions from any Tensor-based library through a functional fluent immutable API based on the Builder Pattern. As a side effect, TensorBuilder is a library that gives expressive power to any Tensor-based library that decide to implement a TensorBuilder patch.\n\nTensorBuilder hosts a patch for tensorflow, using this patch you can rewrite the code like this\nimport tensorflow as tf\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nnet = tf.contrib.layers.fully_connected(x, 10, activation_fn=tf.nn.tanh) # tanh(x * w + b)\nnet = tf.nn.dropout(net, keep_prob) # dropout(x, keep_prob)\nh =  tf.contrib.layers.fully_connected(net, 3, activation_fn=tf.nn.softmax) # softmax(x * w + b)\n\nas this\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.slim\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .fully_connected(10, activation_fn=tf.nn.tanh)\n    .map(tf.nn.dropout, keep_prob)\n    .fully_connected(3, activation_fn=tf.nn.softmax)\n    .tensor()\n)\n\nor even like this using a stronger patch\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .tanh_layer(10) \n    .dropout(keep_prob)\n    .softmax_layer(3)\n    .tensor()\n)\n\nThe patch for the tensorflow library currently has full support for branching, see this example with the strong patch\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .fully_connected(10)\n    .branch(lambda root:\n    [\n        root\n        .relu_layer(3)\n    ,\n        root\n        .tanh_layer(9)\n        .branch(lambda root2:\n        [\n          root2\n          .sigmoid_layer(6)\n        ,\n          root2\n          .dropout(keep_prob)\n          .softmax_layer(8)\n        ])\n    ])\n    .sigmoid_layer(6) #<== This connects all the branches to a single output layer\n    .tensor()\n)\nTensorBuilder even has a DSL that you get for free, so you could rewrite the previous as\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\nimport tensorbuilder.dsl as dl #<== Notice the alias\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = x.builder().pipe(\n    dl.fully_connected(10),\n    [\n        dl.relu_layer(3)\n    ,\n        (dl.tanh_layer(9),\n        [\n            dl.sigmoid_layer(6)\n        ,\n            dl\n            .dropout(keep_prob)\n            .softmax_layer(8)\n        ])\n    ],\n    dl.sigmoid_layer(6)\n    .tensor()\n)\n\nInvitation\nI'd like to invite you to eventually host the TensorFlow patch for TensorBuilder on the TensorFlow library, because TensorBuilder really is just an empty shell, it doesn't even know about tensors, its a sort of functional tool for making non-composable/fluent APIs into composable/fluent APIs (in e.g. Elixir or Elm you wouldn't even need it), but it benefits a lot from libraries, and I am sure that tensorflow users will benefit a lot from it too.", "body": "Hi,\n\n> [TensorBuilder](https://github.com/cgarciae/tensorbuilder) is light-weight extensible library that enables you to easily create complex deep neural networks using functions from any Tensor-based library through a functional fluent immutable API based on the Builder Pattern. As a side effect, TensorBuilder is a library that gives expressive power to any Tensor-based library that decide to implement a TensorBuilder patch.\n\nTensorBuilder hosts a patch for `tensorflow`, using this patch you can rewrite the code like this\n\n```\nimport tensorflow as tf\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nnet = tf.contrib.layers.fully_connected(x, 10, activation_fn=tf.nn.tanh) # tanh(x * w + b)\nnet = tf.nn.dropout(net, keep_prob) # dropout(x, keep_prob)\nh =  tf.contrib.layers.fully_connected(net, 3, activation_fn=tf.nn.softmax) # softmax(x * w + b)\n```\n\nas this \n\n```\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.slim\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .fully_connected(10, activation_fn=tf.nn.tanh)\n    .map(tf.nn.dropout, keep_prob)\n    .fully_connected(3, activation_fn=tf.nn.softmax)\n    .tensor()\n)\n```\n\nor even like this using a stronger patch\n\n```\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .tanh_layer(10) \n    .dropout(keep_prob)\n    .softmax_layer(3)\n    .tensor()\n)\n```\n\nThe patch for the `tensorflow` library currently has full support for branching, see this example with the strong patch\n\n``` python\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = (\n    x.builder()\n    .fully_connected(10)\n    .branch(lambda root:\n    [\n        root\n        .relu_layer(3)\n    ,\n        root\n        .tanh_layer(9)\n        .branch(lambda root2:\n        [\n          root2\n          .sigmoid_layer(6)\n        ,\n          root2\n          .dropout(keep_prob)\n          .softmax_layer(8)\n        ])\n    ])\n    .sigmoid_layer(6) #<== This connects all the branches to a single output layer\n    .tensor()\n)\n```\n\nTensorBuilder even has a DSL that you get for free, so you could rewrite the previous as\n\n```\nimport tensorflow as tf\nimport tensorbuilder as tb\nimport tensorbuilder.patches.tensorflow.patch\nimport tensorbuilder.dsl as dl #<== Notice the alias\n\nx = tf.placeholder(tf.float32, shape=[None, 5])\nkeep_prob = tf.placeholder(tf.float32)\n\nh = x.builder().pipe(\n    dl.fully_connected(10),\n    [\n        dl.relu_layer(3)\n    ,\n        (dl.tanh_layer(9),\n        [\n            dl.sigmoid_layer(6)\n        ,\n            dl\n            .dropout(keep_prob)\n            .softmax_layer(8)\n        ])\n    ],\n    dl.sigmoid_layer(6)\n    .tensor()\n)\n```\n### Invitation\n\nI'd like to invite you to eventually host the TensorFlow patch for TensorBuilder on the TensorFlow library, because TensorBuilder really is just an empty shell, it doesn't even know about tensors, its a sort of functional tool for making non-composable/fluent APIs into composable/fluent APIs (in e.g. Elixir or Elm you wouldn't even need it), but it benefits a lot from libraries, and I am sure that tensorflow users will benefit a lot from it too.\n"}