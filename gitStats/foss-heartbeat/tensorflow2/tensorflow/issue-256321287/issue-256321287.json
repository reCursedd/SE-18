{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12916", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12916/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12916/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12916/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12916", "id": 256321287, "node_id": "MDU6SXNzdWUyNTYzMjEyODc=", "number": 12916, "title": "inconsistent behavior in variable sharing ", "user": {"login": "malsulaimi", "id": 31324944, "node_id": "MDQ6VXNlcjMxMzI0OTQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/31324944?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malsulaimi", "html_url": "https://github.com/malsulaimi", "followers_url": "https://api.github.com/users/malsulaimi/followers", "following_url": "https://api.github.com/users/malsulaimi/following{/other_user}", "gists_url": "https://api.github.com/users/malsulaimi/gists{/gist_id}", "starred_url": "https://api.github.com/users/malsulaimi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malsulaimi/subscriptions", "organizations_url": "https://api.github.com/users/malsulaimi/orgs", "repos_url": "https://api.github.com/users/malsulaimi/repos", "events_url": "https://api.github.com/users/malsulaimi/events{/privacy}", "received_events_url": "https://api.github.com/users/malsulaimi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2017-09-08T17:29:15Z", "updated_at": "2017-09-12T21:35:13Z", "closed_at": "2017-09-12T21:35:13Z", "author_association": "NONE", "body_html": "<p>hello ,</p>\n<p>I have been struggling for few days with an issue that I'm facing in TensorFlow , I hesitated before opening this GitHub as an issue because I'm not sure if what I'm facing is a bug or a mere confusion on my side    .</p>\n<p>anyways it seems that I'm experiencing an inconsistent behavior in tensorFlow with variable name/reusing where I have a function that creates some cells and attentions objects : below is the function :</p>\n<pre><code>def create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width ):\n\n    with tf.variable_scope(\"InnerScope\" , reuse=tf.AUTO_REUSE):\n        encoder_outputs_tr =tf.contrib.seq2seq.tile_batch(encoder_outputs_tr, multiplier=beam_width) \n        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width) \n        batch_size =  batch_size * beam_width \n        dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs_tr ) \n\n        attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size , output_attention=False)\n        attn_zero = attn_cell.zero_state(batch_size , tf.float32 )\n        attn_zero = attn_zero.clone(cell_state = encoder_state)\n    return attn_zero ,  attn_cell \n</code></pre>\n<p>and then I call the function with scope reusing as shown below :</p>\n<pre><code>with tf.variable_scope('scope' ):\n    intial_train_state , train_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , 1  )\nwith tf.variable_scope('scope' ,reuse=True):\n    intial_infer_state , infer_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width  )\nprint(\"intial_train_state\" , intial_train_state)\nprint(\"intial_infer_state\" , intial_infer_state)\n</code></pre>\n<p>the print commands parings the returned objects , first print outputs :<br>\n<code>  ('intial_train_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32&gt;)), attention=&lt;tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(100, 512) dtype=float32&gt;, time=&lt;tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32&gt;, alignments=&lt;tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(100, ?) dtype=float32&gt;, alignment_history=()))</code></p>\n<p>and the second print outputs :<br>\n<code>  ('intial_infer_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32&gt;, h=&lt;tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32&gt;)), attention=&lt;tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(300, 512) dtype=float32&gt;, time=&lt;tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32&gt;, alignments=&lt;tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(300, ?) dtype=float32&gt;, alignment_history=()))</code></p>\n<p>I was expecting that both output would be the same since I'm reusing the variables but as you can see that for example in the first variable the output has something like this<br>\n<strong>scope/InnerScope/tile_batch_1/Reshape_1:0</strong></p>\n<p>and in the second variable</p>\n<p><strong>scope_1/InnerScope/tile_batch_1/Reshape_1:0</strong></p>\n<p>I do not know why _1 is added to <strong>scope</strong> in the second call , and I'm a bit confused if the variable is being shared or not , further more when I set the reuse option to False , I get the below error in the second function call :<br>\n<code>ValueError: Variable scope/memory_layer/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</code></p>\n<p>which leads me to think that the function was reusing the variables when the reuse flag was True , but I still do not understand why is tensorFlow adding _1 to the variable name <strong>scope_1</strong> , and does that mean it not being reused ? and how do I fix this .</p>\n<p>I'm using Tensorflow 1.3<br>\nON MacOs 10.12.4</p>\n<p>I have also opened a stackoverflow question : <a href=\"https://stackoverflow.com/questions/46081793/sharing-and-reusing-tensorflow-variables\" rel=\"nofollow\">https://stackoverflow.com/questions/46081793/sharing-and-reusing-tensorflow-variables</a></p>\n<p>thank you</p>", "body_text": "hello ,\nI have been struggling for few days with an issue that I'm facing in TensorFlow , I hesitated before opening this GitHub as an issue because I'm not sure if what I'm facing is a bug or a mere confusion on my side    .\nanyways it seems that I'm experiencing an inconsistent behavior in tensorFlow with variable name/reusing where I have a function that creates some cells and attentions objects : below is the function :\ndef create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width ):\n\n    with tf.variable_scope(\"InnerScope\" , reuse=tf.AUTO_REUSE):\n        encoder_outputs_tr =tf.contrib.seq2seq.tile_batch(encoder_outputs_tr, multiplier=beam_width) \n        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width) \n        batch_size =  batch_size * beam_width \n        dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs_tr ) \n\n        attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size , output_attention=False)\n        attn_zero = attn_cell.zero_state(batch_size , tf.float32 )\n        attn_zero = attn_zero.clone(cell_state = encoder_state)\n    return attn_zero ,  attn_cell \n\nand then I call the function with scope reusing as shown below :\nwith tf.variable_scope('scope' ):\n    intial_train_state , train_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , 1  )\nwith tf.variable_scope('scope' ,reuse=True):\n    intial_infer_state , infer_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width  )\nprint(\"intial_train_state\" , intial_train_state)\nprint(\"intial_infer_state\" , intial_infer_state)\n\nthe print commands parings the returned objects , first print outputs :\n  ('intial_train_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(100, 512) dtype=float32>, time=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(100, ?) dtype=float32>, alignment_history=()))\nand the second print outputs :\n  ('intial_infer_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(300, 512) dtype=float32>, time=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(300, ?) dtype=float32>, alignment_history=()))\nI was expecting that both output would be the same since I'm reusing the variables but as you can see that for example in the first variable the output has something like this\nscope/InnerScope/tile_batch_1/Reshape_1:0\nand in the second variable\nscope_1/InnerScope/tile_batch_1/Reshape_1:0\nI do not know why _1 is added to scope in the second call , and I'm a bit confused if the variable is being shared or not , further more when I set the reuse option to False , I get the below error in the second function call :\nValueError: Variable scope/memory_layer/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\nwhich leads me to think that the function was reusing the variables when the reuse flag was True , but I still do not understand why is tensorFlow adding _1 to the variable name scope_1 , and does that mean it not being reused ? and how do I fix this .\nI'm using Tensorflow 1.3\nON MacOs 10.12.4\nI have also opened a stackoverflow question : https://stackoverflow.com/questions/46081793/sharing-and-reusing-tensorflow-variables\nthank you", "body": "hello , \r\n\r\nI have been struggling for few days with an issue that I'm facing in TensorFlow , I hesitated before opening this GitHub as an issue because I'm not sure if what I'm facing is a bug or a mere confusion on my side    . \r\n\r\nanyways it seems that I'm experiencing an inconsistent behavior in tensorFlow with variable name/reusing where I have a function that creates some cells and attentions objects : below is the function : \r\n\r\n```\r\ndef create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width ):\r\n\r\n    with tf.variable_scope(\"InnerScope\" , reuse=tf.AUTO_REUSE):\r\n        encoder_outputs_tr =tf.contrib.seq2seq.tile_batch(encoder_outputs_tr, multiplier=beam_width) \r\n        encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width) \r\n        batch_size =  batch_size * beam_width \r\n        dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\r\n\r\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs_tr ) \r\n\r\n        attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size , output_attention=False)\r\n        attn_zero = attn_cell.zero_state(batch_size , tf.float32 )\r\n        attn_zero = attn_zero.clone(cell_state = encoder_state)\r\n    return attn_zero ,  attn_cell \r\n```\r\n\r\nand then I call the function with scope reusing as shown below : \r\n```\r\nwith tf.variable_scope('scope' ):\r\n    intial_train_state , train_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , 1  )\r\nwith tf.variable_scope('scope' ,reuse=True):\r\n    intial_infer_state , infer_cell = create_complete_cell(rnn_size,num_layers,encoder_outputs_tr,batch_size,encoder_state , beam_width  )\r\nprint(\"intial_train_state\" , intial_train_state)\r\nprint(\"intial_infer_state\" , intial_infer_state)\r\n```\r\n\r\nthe print commands parings the returned objects , first print outputs : \r\n`  ('intial_train_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(100, 512) dtype=float32>, time=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(100, ?) dtype=float32>, alignment_history=()))`\r\n\r\n\r\nand the second print outputs : \r\n`  ('intial_infer_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_1:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_3:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_4:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_5:0' shape=(?, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_6:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'scope_1/InnerScope/tile_batch_1/Reshape_7:0' shape=(?, 512) dtype=float32>)), attention=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_1:0' shape=(300, 512) dtype=float32>, time=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'scope_1/InnerScope/AttentionWrapperZeroState/zeros_2:0' shape=(300, ?) dtype=float32>, alignment_history=()))`\r\n\r\nI was expecting that both output would be the same since I'm reusing the variables but as you can see that for example in the first variable the output has something like this \r\n**scope/InnerScope/tile_batch_1/Reshape_1:0**\r\n\r\nand in the second variable \r\n\r\n**scope_1/InnerScope/tile_batch_1/Reshape_1:0**\r\n\r\nI do not know why _1 is added to **scope** in the second call , and I'm a bit confused if the variable is being shared or not , further more when I set the reuse option to False , I get the below error in the second function call : \r\n`ValueError: Variable scope/memory_layer/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:`\r\n\r\nwhich leads me to think that the function was reusing the variables when the reuse flag was True , but I still do not understand why is tensorFlow adding _1 to the variable name **scope_1** , and does that mean it not being reused ? and how do I fix this . \r\n\r\nI'm using Tensorflow 1.3 \r\nON MacOs 10.12.4\r\n\r\nI have also opened a stackoverflow question : https://stackoverflow.com/questions/46081793/sharing-and-reusing-tensorflow-variables\r\n\r\nthank you "}