{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6146", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6146/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6146/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6146/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6146", "id": 193974418, "node_id": "MDU6SXNzdWUxOTM5NzQ0MTg=", "number": 6146, "title": "Fast Layer Normalization GPU kernel", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-12-07T07:11:44Z", "updated_at": "2017-06-16T19:26:56Z", "closed_at": "2017-06-16T19:26:56Z", "author_association": "NONE", "body_html": "<p>Hi, I wrote a <a href=\"https://github.com/MycChiu/fast-LayerNorm-TF\">custom CUDA op for layer normalization</a>, which is about 5-10x faster than the current <code>tf.contrib.layers.layer_norm</code>.</p>\n<p>After I posted the link on reddit, some people suggested that I should try to merge this kernel into the trunk, but I have never done this before, so I need some guidance on how this custom kernel could be integrated into TensorFlow's current code.</p>\n<p>With my current understanding on how TensorFlow's code is organized, here are some of the problems I think should be solved before the custom op can be integrated into the trunk:</p>\n<ol>\n<li>It doesn't have CPU kernels.</li>\n<li>Since the kernel uses shuffle instructions, it only supports cards newer than Kepler.</li>\n<li>Current implementation has a restriction on the size of last dimension (cannot be larger than 5102)</li>\n</ol>\n<p>Without solving these problems, the fastest way to merge this op into the trunk is probably by putting it under the <code>tf.contrib</code> folder as it is right now. I will fill out the CLA and make a pull request if you guys think this custom kernel is ready to be merged into the <code>tf.contrib</code> folder.</p>", "body_text": "Hi, I wrote a custom CUDA op for layer normalization, which is about 5-10x faster than the current tf.contrib.layers.layer_norm.\nAfter I posted the link on reddit, some people suggested that I should try to merge this kernel into the trunk, but I have never done this before, so I need some guidance on how this custom kernel could be integrated into TensorFlow's current code.\nWith my current understanding on how TensorFlow's code is organized, here are some of the problems I think should be solved before the custom op can be integrated into the trunk:\n\nIt doesn't have CPU kernels.\nSince the kernel uses shuffle instructions, it only supports cards newer than Kepler.\nCurrent implementation has a restriction on the size of last dimension (cannot be larger than 5102)\n\nWithout solving these problems, the fastest way to merge this op into the trunk is probably by putting it under the tf.contrib folder as it is right now. I will fill out the CLA and make a pull request if you guys think this custom kernel is ready to be merged into the tf.contrib folder.", "body": "Hi, I wrote a [custom CUDA op for layer normalization](https://github.com/MycChiu/fast-LayerNorm-TF), which is about 5-10x faster than the current `tf.contrib.layers.layer_norm`.\r\n\r\nAfter I posted the link on reddit, some people suggested that I should try to merge this kernel into the trunk, but I have never done this before, so I need some guidance on how this custom kernel could be integrated into TensorFlow's current code.\r\n\r\nWith my current understanding on how TensorFlow's code is organized, here are some of the problems I think should be solved before the custom op can be integrated into the trunk:\r\n1. It doesn't have CPU kernels.\r\n2. Since the kernel uses shuffle instructions, it only supports cards newer than Kepler.\r\n3. Current implementation has a restriction on the size of last dimension (cannot be larger than 5102)\r\n\r\nWithout solving these problems, the fastest way to merge this op into the trunk is probably by putting it under the `tf.contrib` folder as it is right now. I will fill out the CLA and make a pull request if you guys think this custom kernel is ready to be merged into the `tf.contrib` folder."}