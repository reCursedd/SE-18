{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328239369", "html_url": "https://github.com/tensorflow/tensorflow/pull/12922#issuecomment-328239369", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12922", "id": 328239369, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODIzOTM2OQ==", "user": {"login": "cloudbank", "id": 2288294, "node_id": "MDQ6VXNlcjIyODgyOTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2288294?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cloudbank", "html_url": "https://github.com/cloudbank", "followers_url": "https://api.github.com/users/cloudbank/followers", "following_url": "https://api.github.com/users/cloudbank/following{/other_user}", "gists_url": "https://api.github.com/users/cloudbank/gists{/gist_id}", "starred_url": "https://api.github.com/users/cloudbank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cloudbank/subscriptions", "organizations_url": "https://api.github.com/users/cloudbank/orgs", "repos_url": "https://api.github.com/users/cloudbank/repos", "events_url": "https://api.github.com/users/cloudbank/events{/privacy}", "received_events_url": "https://api.github.com/users/cloudbank/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-08T23:57:08Z", "updated_at": "2017-09-11T01:30:29Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a>  Ideally, this heavy create operation involving the big graph file is done off heap avoiding IO exactly once and the classifier object persisted for further usage. Of course all of this is taken care of outside of the TFII in my work and I could add an example in another PR.<br>\nWhile doing the benchmarking, I started to get</p>\n<pre><code>Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef\n                                                                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph\n\nfor the change to remove the loadGraph(InputStream ...\nreplaced by loadGraph(byte[] and the byte[] creation in \npublic TensorFlowInferenceInterface(AssetManager assetManager, String model) {\n</code></pre>\n<p>I reverted my code wrt those changes, but did not push them back.<br>\nbenchmarks:<br>\nIO :TensorFlowInferenceInterface(AssetManager assetManager, String model)  ~ 1.6 secs<br>\nmmap: TensorFlowInferenceInterface(Context ctx, String model)  ~.6 secs w/o copy, 2.2secs w copy</p>\n<p>Memmap is faster by far the second time/copied already (without the copy), which is useful if called more than once, or if separate from the blasted copy.</p>", "body_text": "@andrewharp  Ideally, this heavy create operation involving the big graph file is done off heap avoiding IO exactly once and the classifier object persisted for further usage. Of course all of this is taken care of outside of the TFII in my work and I could add an example in another PR.\nWhile doing the benchmarking, I started to get\nCaused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef\n                                                                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph\n\nfor the change to remove the loadGraph(InputStream ...\nreplaced by loadGraph(byte[] and the byte[] creation in \npublic TensorFlowInferenceInterface(AssetManager assetManager, String model) {\n\nI reverted my code wrt those changes, but did not push them back.\nbenchmarks:\nIO :TensorFlowInferenceInterface(AssetManager assetManager, String model)  ~ 1.6 secs\nmmap: TensorFlowInferenceInterface(Context ctx, String model)  ~.6 secs w/o copy, 2.2secs w copy\nMemmap is faster by far the second time/copied already (without the copy), which is useful if called more than once, or if separate from the blasted copy.", "body": "@andrewharp  Ideally, this heavy create operation involving the big graph file is done off heap avoiding IO exactly once and the classifier object persisted for further usage. Of course all of this is taken care of outside of the TFII in my work and I could add an example in another PR. \r\nWhile doing the benchmarking, I started to get \r\n ```\r\n Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef\r\n                                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph\r\n\r\nfor the change to remove the loadGraph(InputStream ...\r\n replaced by loadGraph(byte[] and the byte[] creation in \r\n public TensorFlowInferenceInterface(AssetManager assetManager, String model) {\r\n```\r\nI reverted my code wrt those changes, but did not push them back.\r\n benchmarks:\r\nIO :TensorFlowInferenceInterface(AssetManager assetManager, String model)  ~ 1.6 secs\r\nmmap: TensorFlowInferenceInterface(Context ctx, String model)  ~.6 secs w/o copy, 2.2secs w copy\r\n \r\nMemmap is faster by far the second time/copied already (without the copy), which is useful if called more than once, or if separate from the blasted copy."}