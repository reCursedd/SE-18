{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/340069746", "html_url": "https://github.com/tensorflow/tensorflow/issues/13641#issuecomment-340069746", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13641", "id": 340069746, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDA2OTc0Ng==", "user": {"login": "vnavkal", "id": 5924638, "node_id": "MDQ6VXNlcjU5MjQ2Mzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5924638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vnavkal", "html_url": "https://github.com/vnavkal", "followers_url": "https://api.github.com/users/vnavkal/followers", "following_url": "https://api.github.com/users/vnavkal/following{/other_user}", "gists_url": "https://api.github.com/users/vnavkal/gists{/gist_id}", "starred_url": "https://api.github.com/users/vnavkal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vnavkal/subscriptions", "organizations_url": "https://api.github.com/users/vnavkal/orgs", "repos_url": "https://api.github.com/users/vnavkal/repos", "events_url": "https://api.github.com/users/vnavkal/events{/privacy}", "received_events_url": "https://api.github.com/users/vnavkal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-27T19:53:21Z", "updated_at": "2017-10-27T19:53:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Would a \"minimum norm\" Jacobian actually be the one we need?  It seems to me that we need the Jacobian of the specific SVD function that's used.  I suspect this is an obstacle not only to removing the \"almost square\" restriction but also to extending support to complex tensors, since in the complex case there are always infinitely many choices of U and V.</p>\n<p>Even in the real case, I could imagine the indeterminacy in U and V being a problem if someone tries to optimize over a function that depends on the signs of columns in U and V.  According to the paper <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10676786\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/guivenca\">@guivenca</a> linked,</p>\n<blockquote>\n<p>...even state of the art algorithms for SVD computation (eg LAPACK \u2019s dgesvd family of routines [1]) are \u201cunstable\u201d with respect to small perturbations of the input. By unstable, we mean that the signs associated with the columns and can change arbitrarily even with the slightest perturbation.</p>\n</blockquote>\n<p>Anyway, it looks to me like it wouldn't be too hard to add support for <code>full_matrices=False</code>.  And I think it wouldn't need the restriction that the matrices are almost square.  If it seems useful, I'd be happy to work on a pull request for that; it would essentially be a port of the autograd code.</p>", "body_text": "Would a \"minimum norm\" Jacobian actually be the one we need?  It seems to me that we need the Jacobian of the specific SVD function that's used.  I suspect this is an obstacle not only to removing the \"almost square\" restriction but also to extending support to complex tensors, since in the complex case there are always infinitely many choices of U and V.\nEven in the real case, I could imagine the indeterminacy in U and V being a problem if someone tries to optimize over a function that depends on the signs of columns in U and V.  According to the paper @guivenca linked,\n\n...even state of the art algorithms for SVD computation (eg LAPACK \u2019s dgesvd family of routines [1]) are \u201cunstable\u201d with respect to small perturbations of the input. By unstable, we mean that the signs associated with the columns and can change arbitrarily even with the slightest perturbation.\n\nAnyway, it looks to me like it wouldn't be too hard to add support for full_matrices=False.  And I think it wouldn't need the restriction that the matrices are almost square.  If it seems useful, I'd be happy to work on a pull request for that; it would essentially be a port of the autograd code.", "body": "Would a \"minimum norm\" Jacobian actually be the one we need?  It seems to me that we need the Jacobian of the specific SVD function that's used.  I suspect this is an obstacle not only to removing the \"almost square\" restriction but also to extending support to complex tensors, since in the complex case there are always infinitely many choices of U and V.\r\n\r\nEven in the real case, I could imagine the indeterminacy in U and V being a problem if someone tries to optimize over a function that depends on the signs of columns in U and V.  According to the paper @guivenca linked,\r\n> ...even state of the art algorithms for SVD computation (eg LAPACK \u2019s dgesvd family of routines [1]) are \u201cunstable\u201d with respect to small perturbations of the input. By unstable, we mean that the signs associated with the columns and can change arbitrarily even with the slightest perturbation.\r\n\r\nAnyway, it looks to me like it wouldn't be too hard to add support for `full_matrices=False`.  And I think it wouldn't need the restriction that the matrices are almost square.  If it seems useful, I'd be happy to work on a pull request for that; it would essentially be a port of the autograd code."}