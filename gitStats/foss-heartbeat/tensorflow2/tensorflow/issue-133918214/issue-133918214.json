{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1117", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1117/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1117/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1117/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1117", "id": 133918214, "node_id": "MDU6SXNzdWUxMzM5MTgyMTQ=", "number": 1117, "title": "embedding_lookup on multiple dimensions with AdagradOptimizer throwing exception", "user": {"login": "shaileshahuja", "id": 3991741, "node_id": "MDQ6VXNlcjM5OTE3NDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/3991741?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shaileshahuja", "html_url": "https://github.com/shaileshahuja", "followers_url": "https://api.github.com/users/shaileshahuja/followers", "following_url": "https://api.github.com/users/shaileshahuja/following{/other_user}", "gists_url": "https://api.github.com/users/shaileshahuja/gists{/gist_id}", "starred_url": "https://api.github.com/users/shaileshahuja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shaileshahuja/subscriptions", "organizations_url": "https://api.github.com/users/shaileshahuja/orgs", "repos_url": "https://api.github.com/users/shaileshahuja/repos", "events_url": "https://api.github.com/users/shaileshahuja/events{/privacy}", "received_events_url": "https://api.github.com/users/shaileshahuja/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "vincentvanhoucke", "id": 15737127, "node_id": "MDQ6VXNlcjE1NzM3MTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/15737127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentvanhoucke", "html_url": "https://github.com/vincentvanhoucke", "followers_url": "https://api.github.com/users/vincentvanhoucke/followers", "following_url": "https://api.github.com/users/vincentvanhoucke/following{/other_user}", "gists_url": "https://api.github.com/users/vincentvanhoucke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentvanhoucke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentvanhoucke/subscriptions", "organizations_url": "https://api.github.com/users/vincentvanhoucke/orgs", "repos_url": "https://api.github.com/users/vincentvanhoucke/repos", "events_url": "https://api.github.com/users/vincentvanhoucke/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentvanhoucke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "vincentvanhoucke", "id": 15737127, "node_id": "MDQ6VXNlcjE1NzM3MTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/15737127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentvanhoucke", "html_url": "https://github.com/vincentvanhoucke", "followers_url": "https://api.github.com/users/vincentvanhoucke/followers", "following_url": "https://api.github.com/users/vincentvanhoucke/following{/other_user}", "gists_url": "https://api.github.com/users/vincentvanhoucke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentvanhoucke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentvanhoucke/subscriptions", "organizations_url": "https://api.github.com/users/vincentvanhoucke/orgs", "repos_url": "https://api.github.com/users/vincentvanhoucke/repos", "events_url": "https://api.github.com/users/vincentvanhoucke/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentvanhoucke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2016-02-16T08:46:20Z", "updated_at": "2016-02-28T23:19:55Z", "closed_at": "2016-02-19T14:17:29Z", "author_association": "NONE", "body_html": "<p>I am completing the Udacity course on Tensorflow, and noticed that when embedding_lookup is used in 3 dimensions with AdagradOptimizer, the optimizer throws an error:</p>\n<p>CODE (with error):</p>\n<pre><code>import math\nimport tensorflow as tf\n\nbatch_size = 128\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 1 # How many words to consider left and right.\nnum_sampled = 64 # Number of negative examples to sample.\nvocabulary_size = 50000\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2 * skip_window])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n  # Variables.\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += embed[:, i, :]\n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n                               train_labels, num_sampled, vocabulary_size))\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n</code></pre>\n<p>Error message:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-32-104452f9cf81&gt; in &lt;module&gt;()\n     39     tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n     40                                train_labels, num_sampled, vocabulary_size))\n---&gt; 41   optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, name)\n    186         aggregation_method=aggregation_method)\n    187     return self.apply_gradients(grads_and_vars, global_step=global_step,\n--&gt; 188                                 name=name)\n    189 \n    190   def compute_gradients(self, loss, var_list=None, gate_gradients=GATE_OP,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\n    287             update_ops.append(self._apply_dense(grad, var))\n    288           else:\n--&gt; 289             update_ops.append(self._apply_sparse(grad, var))\n    290       if global_step is None:\n    291         return self._finish(update_ops, name)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.pyc in _apply_sparse(self, grad, var)\n     75     return training_ops.sparse_apply_adagrad(\n     76         var, acc, self._learning_rate_tensor, grad.values, grad.indices,\n---&gt; 77         use_locking=self._use_locking)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.pyc in sparse_apply_adagrad(var, accum, lr, grad, indices, use_locking, name)\n    200   return _op_def_lib.apply_op(\"SparseApplyAdagrad\", var=var, accum=accum,\n    201                               lr=lr, grad=grad, indices=indices,\n--&gt; 202                               use_locking=use_locking, name=name)\n    203 \n    204 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, g, name, **keywords)\n    662         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    663                          input_types=input_types, attrs=attr_protos,\n--&gt; 664                          op_def=op_def)\n    665         outputs = op.outputs\n    666         return _Restructure(ops.convert_n_to_tensor_or_indexed_slices(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)\n   1834                     original_op=self._default_original_op, op_def=op_def)\n   1835     if compute_shapes:\n-&gt; 1836       set_shapes_for_outputs(ret)\n   1837     self._add_op(ret)\n   1838     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1474       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1475                          % op.type)\n-&gt; 1476   shapes = shape_func(op)\n   1477   if len(op.outputs) != len(shapes):\n   1478     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.pyc in _SparseApplyAdagradShape(op)\n    115   _AssertInputIsScalar(op, 2)  # lr\n    116   grad_shape = op.inputs[3].get_shape().merge_with(\n--&gt; 117       tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n    118   unused_indices_shape = op.inputs[4].get_shape().merge_with(\n    119       tensor_shape.vector(grad_shape[0]))\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)\n    525       return other\n    526     else:\n--&gt; 527       self.assert_same_rank(other)\n    528       new_dims = []\n    529       for i, dim in enumerate(self._dims):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in assert_same_rank(self, other)\n    568       if self.ndims != other.ndims:\n    569         raise ValueError(\n--&gt; 570             \"Shapes %s and %s must have the same rank\" % (self, other))\n    571 \n    572   def assert_has_rank(self, rank):\n\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(128)]) must have the same rank\n</code></pre>\n<p>Just change it to get embedding look up once for each time for the third dimension:</p>\n<pre><code>  # Look up embeddings for inputs.\n  # embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  # print embed.get_shape()\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += tf.nn.embedding_lookup(embeddings, train_dataset[:, i])\n</code></pre>\n<p>The code runs smoothly!</p>", "body_text": "I am completing the Udacity course on Tensorflow, and noticed that when embedding_lookup is used in 3 dimensions with AdagradOptimizer, the optimizer throws an error:\nCODE (with error):\nimport math\nimport tensorflow as tf\n\nbatch_size = 128\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 1 # How many words to consider left and right.\nnum_sampled = 64 # Number of negative examples to sample.\nvocabulary_size = 50000\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2 * skip_window])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n  # Variables.\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += embed[:, i, :]\n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n                               train_labels, num_sampled, vocabulary_size))\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\nError message:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-32-104452f9cf81> in <module>()\n     39     tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n     40                                train_labels, num_sampled, vocabulary_size))\n---> 41   optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, name)\n    186         aggregation_method=aggregation_method)\n    187     return self.apply_gradients(grads_and_vars, global_step=global_step,\n--> 188                                 name=name)\n    189 \n    190   def compute_gradients(self, loss, var_list=None, gate_gradients=GATE_OP,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\n    287             update_ops.append(self._apply_dense(grad, var))\n    288           else:\n--> 289             update_ops.append(self._apply_sparse(grad, var))\n    290       if global_step is None:\n    291         return self._finish(update_ops, name)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.pyc in _apply_sparse(self, grad, var)\n     75     return training_ops.sparse_apply_adagrad(\n     76         var, acc, self._learning_rate_tensor, grad.values, grad.indices,\n---> 77         use_locking=self._use_locking)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.pyc in sparse_apply_adagrad(var, accum, lr, grad, indices, use_locking, name)\n    200   return _op_def_lib.apply_op(\"SparseApplyAdagrad\", var=var, accum=accum,\n    201                               lr=lr, grad=grad, indices=indices,\n--> 202                               use_locking=use_locking, name=name)\n    203 \n    204 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, g, name, **keywords)\n    662         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    663                          input_types=input_types, attrs=attr_protos,\n--> 664                          op_def=op_def)\n    665         outputs = op.outputs\n    666         return _Restructure(ops.convert_n_to_tensor_or_indexed_slices(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)\n   1834                     original_op=self._default_original_op, op_def=op_def)\n   1835     if compute_shapes:\n-> 1836       set_shapes_for_outputs(ret)\n   1837     self._add_op(ret)\n   1838     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1474       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1475                          % op.type)\n-> 1476   shapes = shape_func(op)\n   1477   if len(op.outputs) != len(shapes):\n   1478     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.pyc in _SparseApplyAdagradShape(op)\n    115   _AssertInputIsScalar(op, 2)  # lr\n    116   grad_shape = op.inputs[3].get_shape().merge_with(\n--> 117       tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n    118   unused_indices_shape = op.inputs[4].get_shape().merge_with(\n    119       tensor_shape.vector(grad_shape[0]))\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)\n    525       return other\n    526     else:\n--> 527       self.assert_same_rank(other)\n    528       new_dims = []\n    529       for i, dim in enumerate(self._dims):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in assert_same_rank(self, other)\n    568       if self.ndims != other.ndims:\n    569         raise ValueError(\n--> 570             \"Shapes %s and %s must have the same rank\" % (self, other))\n    571 \n    572   def assert_has_rank(self, rank):\n\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(128)]) must have the same rank\n\nJust change it to get embedding look up once for each time for the third dimension:\n  # Look up embeddings for inputs.\n  # embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  # print embed.get_shape()\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += tf.nn.embedding_lookup(embeddings, train_dataset[:, i])\n\nThe code runs smoothly!", "body": "I am completing the Udacity course on Tensorflow, and noticed that when embedding_lookup is used in 3 dimensions with AdagradOptimizer, the optimizer throws an error:\n\nCODE (with error):\n\n```\nimport math\nimport tensorflow as tf\n\nbatch_size = 128\nembedding_size = 128 # Dimension of the embedding vector.\nskip_window = 1 # How many words to consider left and right.\nnum_sampled = 64 # Number of negative examples to sample.\nvocabulary_size = 50000\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2 * skip_window])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n  # Variables.\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Model.\n  # Look up embeddings for inputs.\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += embed[:, i, :]\n  # Compute the softmax loss, using a sample of the negative labels each time.\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n                               train_labels, num_sampled, vocabulary_size))\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n```\n\nError message:\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-32-104452f9cf81> in <module>()\n     39     tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed2,\n     40                                train_labels, num_sampled, vocabulary_size))\n---> 41   optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, name)\n    186         aggregation_method=aggregation_method)\n    187     return self.apply_gradients(grads_and_vars, global_step=global_step,\n--> 188                                 name=name)\n    189 \n    190   def compute_gradients(self, loss, var_list=None, gate_gradients=GATE_OP,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\n    287             update_ops.append(self._apply_dense(grad, var))\n    288           else:\n--> 289             update_ops.append(self._apply_sparse(grad, var))\n    290       if global_step is None:\n    291         return self._finish(update_ops, name)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.pyc in _apply_sparse(self, grad, var)\n     75     return training_ops.sparse_apply_adagrad(\n     76         var, acc, self._learning_rate_tensor, grad.values, grad.indices,\n---> 77         use_locking=self._use_locking)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.pyc in sparse_apply_adagrad(var, accum, lr, grad, indices, use_locking, name)\n    200   return _op_def_lib.apply_op(\"SparseApplyAdagrad\", var=var, accum=accum,\n    201                               lr=lr, grad=grad, indices=indices,\n--> 202                               use_locking=use_locking, name=name)\n    203 \n    204 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, g, name, **keywords)\n    662         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    663                          input_types=input_types, attrs=attr_protos,\n--> 664                          op_def=op_def)\n    665         outputs = op.outputs\n    666         return _Restructure(ops.convert_n_to_tensor_or_indexed_slices(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)\n   1834                     original_op=self._default_original_op, op_def=op_def)\n   1835     if compute_shapes:\n-> 1836       set_shapes_for_outputs(ret)\n   1837     self._add_op(ret)\n   1838     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1474       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1475                          % op.type)\n-> 1476   shapes = shape_func(op)\n   1477   if len(op.outputs) != len(shapes):\n   1478     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.pyc in _SparseApplyAdagradShape(op)\n    115   _AssertInputIsScalar(op, 2)  # lr\n    116   grad_shape = op.inputs[3].get_shape().merge_with(\n--> 117       tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n    118   unused_indices_shape = op.inputs[4].get_shape().merge_with(\n    119       tensor_shape.vector(grad_shape[0]))\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)\n    525       return other\n    526     else:\n--> 527       self.assert_same_rank(other)\n    528       new_dims = []\n    529       for i, dim in enumerate(self._dims):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in assert_same_rank(self, other)\n    568       if self.ndims != other.ndims:\n    569         raise ValueError(\n--> 570             \"Shapes %s and %s must have the same rank\" % (self, other))\n    571 \n    572   def assert_has_rank(self, rank):\n\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(128)]) must have the same rank\n```\n\nJust change it to get embedding look up once for each time for the third dimension:\n\n```\n  # Look up embeddings for inputs.\n  # embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n  # print embed.get_shape()\n  embed2 = tf.Variable(tf.zeros([batch_size, embedding_size]))\n  for i in xrange(2*skip_window):\n    embed2 += tf.nn.embedding_lookup(embeddings, train_dataset[:, i])\n```\n\nThe code runs smoothly!\n"}