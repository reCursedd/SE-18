{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237613086", "html_url": "https://github.com/tensorflow/tensorflow/issues/3636#issuecomment-237613086", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3636", "id": 237613086, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzYxMzA4Ng==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-04T16:49:18Z", "updated_at": "2016-08-04T17:15:46Z", "author_association": "MEMBER", "body_html": "<p>This retraining program was written to be a relatively simple example, not a high-performance distributed program.  That said, it does seem to be a very popular application and a multi-gpu version would be nice.  It's not as simple as just adding a flag and running on a machine with more gpus; the model needs to be modified so that it can effectively use more gpus in parallel.  TF is able to use multiple cpu cores in parallel because large operations like matrix multiply can easily be split across multiple cores by libraries like BLAS.  But a single TF Op can't (yet) automatically be split over multiple gpus.</p>\n<p>Making available a multi-gpu version of image re-training would be a nice community contribution.</p>", "body_text": "This retraining program was written to be a relatively simple example, not a high-performance distributed program.  That said, it does seem to be a very popular application and a multi-gpu version would be nice.  It's not as simple as just adding a flag and running on a machine with more gpus; the model needs to be modified so that it can effectively use more gpus in parallel.  TF is able to use multiple cpu cores in parallel because large operations like matrix multiply can easily be split across multiple cores by libraries like BLAS.  But a single TF Op can't (yet) automatically be split over multiple gpus.\nMaking available a multi-gpu version of image re-training would be a nice community contribution.", "body": "This retraining program was written to be a relatively simple example, not a high-performance distributed program.  That said, it does seem to be a very popular application and a multi-gpu version would be nice.  It's not as simple as just adding a flag and running on a machine with more gpus; the model needs to be modified so that it can effectively use more gpus in parallel.  TF is able to use multiple cpu cores in parallel because large operations like matrix multiply can easily be split across multiple cores by libraries like BLAS.  But a single TF Op can't (yet) automatically be split over multiple gpus.  \n\nMaking available a multi-gpu version of image re-training would be a nice community contribution.\n"}