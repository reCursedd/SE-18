{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16755", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16755/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16755/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16755/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16755", "id": 294219094, "node_id": "MDU6SXNzdWUyOTQyMTkwOTQ=", "number": 16755, "title": "Multilayer CNN Softmax Script Error", "user": {"login": "peggyn7", "id": 36136066, "node_id": "MDQ6VXNlcjM2MTM2MDY2", "avatar_url": "https://avatars2.githubusercontent.com/u/36136066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peggyn7", "html_url": "https://github.com/peggyn7", "followers_url": "https://api.github.com/users/peggyn7/followers", "following_url": "https://api.github.com/users/peggyn7/following{/other_user}", "gists_url": "https://api.github.com/users/peggyn7/gists{/gist_id}", "starred_url": "https://api.github.com/users/peggyn7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peggyn7/subscriptions", "organizations_url": "https://api.github.com/users/peggyn7/orgs", "repos_url": "https://api.github.com/users/peggyn7/repos", "events_url": "https://api.github.com/users/peggyn7/events{/privacy}", "received_events_url": "https://api.github.com/users/peggyn7/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-02-04T18:34:51Z", "updated_at": "2018-04-10T23:58:04Z", "closed_at": "2018-04-10T23:58:03Z", "author_association": "NONE", "body_html": "<p>Hi, I transferred your script for the Softmax regression and Multilayer CNN from your guide:<br>\n<a href=\"https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/</a>.</p>\n<p>I get the following error whether I am running just the simple Softmax regression model or the full Multilayer CNN. Seems like an issue with the arguments used for the cross_entropy, but I don't know what the issue is...could you please help?</p>\n<p>ValueError                                Traceback (most recent call last)<br>\n in ()<br>\n39     # between the target and the softmax activation function applied to the model's prediction.<br>\n40<br>\n---&gt; 41 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))<br>\n42<br>\n43     # Note that tf.nn.softmax_cross_entropy_with_logits internally applies the softmax</p>\n<p>~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in softmax_cross_entropy_with_logits(_sentinel, labels, logits, dim, name)<br>\n1742   \"\"\"<br>\n1743   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,<br>\n-&gt; 1744                     labels, logits)<br>\n1745<br>\n1746   # TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This</p>\n<p>~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in _ensure_xent_args(name, sentinel, labels, logits)<br>\n1696   if sentinel is not None:<br>\n1697     raise ValueError(\"Only call <code>%s</code> with \"<br>\n-&gt; 1698                      \"named arguments (labels=..., logits=..., ...)\" % name)<br>\n1699   if labels is None or logits is None:<br>\n1700     raise ValueError(\"Both labels and logits must be provided.\")</p>\n<p>ValueError: Only call <code>softmax_cross_entropy_with_logits</code> with named arguments (labels=..., logits=..., ...)</p>\n<p>Here is my script (essentially copied from the guide):</p>\n<h1>Multilayer CNN using Tensorflow</h1>\n<h1>Load Data</h1>\n<p>from tensorflow.examples.tutorials.mnist import input_data<br>\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)</p>\n<h1>Start Tensorflow InteractiveSession</h1>\n<p>import tensorflow as tf<br>\nsess = tf.InteractiveSession()</p>\n<h1>Build Softmax regression model. Define our model and training loss function.</h1>\n<pre><code># creating nodes for the input images and target output classes\n</code></pre>\n<p>x = tf.placeholder(tf.float32, shape=[None, 784])<br>\ny_ = tf.placeholder(tf.float32, shape=[None, 10])<br>\nsess.run(tf.global_variables_initializer())</p>\n<h1>Weight initialization</h1>\n<p>def weight_variable(shape):<br>\ninitial = tf.truncated_normal(shape, stddev=0.1)<br>\nreturn tf.Variable(initial)</p>\n<p>def bias_variable(shape):<br>\ninitial = tf.constant(0.1, shape=shape)<br>\nreturn tf.Variable(initial)</p>\n<h1>Convolutional and pooling operations</h1>\n<h1>Our convolutions uses a stride of one and are zero padded so that</h1>\n<h1>the output is the same size as the input. Our pooling is plain</h1>\n<h1>old max pooling over 2x2 blocks. To keep our code cleaner, let's</h1>\n<h1>also abstract those operations into functions.</h1>\n<p>def conv2d(x, W):<br>\nreturn tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')</p>\n<p>def max_pool_2x2(x):<br>\nreturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1],<br>\nstrides=[1, 2, 2, 1], padding='SAME')</p>\n<h1>Implement first convolutional layer. t will consist of convolution,</h1>\n<h1>followed by max pooling. The convolution will compute 32 features</h1>\n<h1>for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32].</h1>\n<h1>The first two dimensions are the patch size, the next is the number of</h1>\n<h1>input channels, aTo apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels.nd the last is the number of output channels. We will</h1>\n<h1>also have a bias vector with a component for each output channel.</h1>\n<p>W_conv1 = weight_variable([5, 5, 1, 32])<br>\nb_conv1 = bias_variable([32])</p>\n<h1>To apply the layer, we first reshape x to a 4d tensor, with the second</h1>\n<h1>and third dimensions corresponding to image width and height, and the</h1>\n<h1>final dimension corresponding to the number of color channels.</h1>\n<p>x_image = tf.reshape(x, [-1,28,28,1])</p>\n<h1>Then convolve x_image with the weight tensor, add the bias, apply the</h1>\n<h1>ReLU function, and finally max pool. The max_pool_2x2 method will reduce</h1>\n<h1>the image size to 14x14.</h1>\n<p>h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)<br>\nh_pool1 = max_pool_2x2(h_conv1)</p>\n<h1>Second convolutional layer.</h1>\n<h1>In order to build a deep network, we stack several layers of this type.</h1>\n<h1>The second layer will have 64 features for each 5x5 patch.</h1>\n<p>W_conv2 = weight_variable([5, 5, 32, 64])<br>\nb_conv2 = bias_variable([64])</p>\n<p>h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)<br>\nh_pool2 = max_pool_2x2(h_conv2)</p>\n<h1>Densely connected layer.</h1>\n<h1>Now that the image size has been reduced to 7x7, we add a fully-connected</h1>\n<h1>layer with 1024 neurons to allow processing on the entire image. We</h1>\n<h1>reshape the tensor from the pooling layer into a batch of vectors,</h1>\n<h1>multiply by a weight matrix, add a bias, and apply a ReLU.</h1>\n<p>W_fc1 = weight_variable([7 * 7 * 64, 1024])<br>\nb_fc1 = bias_variable([1024])</p>\n<p>h_pool2_flat = tf.reshape(h_pool2, [-1, 7<em>7</em>64])<br>\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</p>\n<h1>Dropout</h1>\n<h1>To reduce overfitting, we will apply dropout before the readout layer.</h1>\n<h1>We create a placeholder for the probability that a neuron's output is</h1>\n<h1>kept during dropout. This allows us to turn dropout on during training,</h1>\n<h1>and turn it off during testing. TensorFlow's tf.nn.dropout op</h1>\n<h1>automatically handles scaling neuron outputs in addition to masking them,</h1>\n<h1>so dropout just works without any additional scaling.</h1>\n<p>keep_prob = tf.placeholder(tf.float32)<br>\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</p>\n<h1>Readout layer.  one layer softmax regression.</h1>\n<p>W_fc2 = weight_variable([1024, 10])<br>\nb_fc2 = bias_variable([10])</p>\n<p>y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</p>\n<h1>Train and Evaluate model.</h1>\n<p>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))</p>\n<p>train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</p>\n<p>correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))</p>\n<p>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</p>\n<p>sess.run(tf.global_variables_initializer())</p>\n<p>for i in range(20000):<br>\nbatch = mnist.train.next_batch(50)<br>\nif i%100 == 0:<br>\ntrain_accuracy = accuracy.eval(feed_dict={<br>\nx:batch[0], y_: batch[1], keep_prob: 1.0})<br>\nprint(\"step %d, training accuracy %g\"%(i, train_accuracy))<br>\ntrain_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})<br>\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))</p>", "body_text": "Hi, I transferred your script for the Softmax regression and Multilayer CNN from your guide:\nhttps://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/.\nI get the following error whether I am running just the simple Softmax regression model or the full Multilayer CNN. Seems like an issue with the arguments used for the cross_entropy, but I don't know what the issue is...could you please help?\nValueError                                Traceback (most recent call last)\n in ()\n39     # between the target and the softmax activation function applied to the model's prediction.\n40\n---> 41 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n42\n43     # Note that tf.nn.softmax_cross_entropy_with_logits internally applies the softmax\n~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in softmax_cross_entropy_with_logits(_sentinel, labels, logits, dim, name)\n1742   \"\"\"\n1743   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n-> 1744                     labels, logits)\n1745\n1746   # TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\n~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in _ensure_xent_args(name, sentinel, labels, logits)\n1696   if sentinel is not None:\n1697     raise ValueError(\"Only call %s with \"\n-> 1698                      \"named arguments (labels=..., logits=..., ...)\" % name)\n1699   if labels is None or logits is None:\n1700     raise ValueError(\"Both labels and logits must be provided.\")\nValueError: Only call softmax_cross_entropy_with_logits with named arguments (labels=..., logits=..., ...)\nHere is my script (essentially copied from the guide):\nMultilayer CNN using Tensorflow\nLoad Data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\nStart Tensorflow InteractiveSession\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nBuild Softmax regression model. Define our model and training loss function.\n# creating nodes for the input images and target output classes\n\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nsess.run(tf.global_variables_initializer())\nWeight initialization\ndef weight_variable(shape):\ninitial = tf.truncated_normal(shape, stddev=0.1)\nreturn tf.Variable(initial)\ndef bias_variable(shape):\ninitial = tf.constant(0.1, shape=shape)\nreturn tf.Variable(initial)\nConvolutional and pooling operations\nOur convolutions uses a stride of one and are zero padded so that\nthe output is the same size as the input. Our pooling is plain\nold max pooling over 2x2 blocks. To keep our code cleaner, let's\nalso abstract those operations into functions.\ndef conv2d(x, W):\nreturn tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\nreturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\nstrides=[1, 2, 2, 1], padding='SAME')\nImplement first convolutional layer. t will consist of convolution,\nfollowed by max pooling. The convolution will compute 32 features\nfor each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32].\nThe first two dimensions are the patch size, the next is the number of\ninput channels, aTo apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels.nd the last is the number of output channels. We will\nalso have a bias vector with a component for each output channel.\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nTo apply the layer, we first reshape x to a 4d tensor, with the second\nand third dimensions corresponding to image width and height, and the\nfinal dimension corresponding to the number of color channels.\nx_image = tf.reshape(x, [-1,28,28,1])\nThen convolve x_image with the weight tensor, add the bias, apply the\nReLU function, and finally max pool. The max_pool_2x2 method will reduce\nthe image size to 14x14.\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\nSecond convolutional layer.\nIn order to build a deep network, we stack several layers of this type.\nThe second layer will have 64 features for each 5x5 patch.\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\nDensely connected layer.\nNow that the image size has been reduced to 7x7, we add a fully-connected\nlayer with 1024 neurons to allow processing on the entire image. We\nreshape the tensor from the pooling layer into a batch of vectors,\nmultiply by a weight matrix, add a bias, and apply a ReLU.\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7764])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\nDropout\nTo reduce overfitting, we will apply dropout before the readout layer.\nWe create a placeholder for the probability that a neuron's output is\nkept during dropout. This allows us to turn dropout on during training,\nand turn it off during testing. TensorFlow's tf.nn.dropout op\nautomatically handles scaling neuron outputs in addition to masking them,\nso dropout just works without any additional scaling.\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\nReadout layer.  one layer softmax regression.\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\nTrain and Evaluate model.\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.global_variables_initializer())\nfor i in range(20000):\nbatch = mnist.train.next_batch(50)\nif i%100 == 0:\ntrain_accuracy = accuracy.eval(feed_dict={\nx:batch[0], y_: batch[1], keep_prob: 1.0})\nprint(\"step %d, training accuracy %g\"%(i, train_accuracy))\ntrain_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))", "body": "Hi, I transferred your script for the Softmax regression and Multilayer CNN from your guide:\r\nhttps://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/.\r\n\r\nI get the following error whether I am running just the simple Softmax regression model or the full Multilayer CNN. Seems like an issue with the arguments used for the cross_entropy, but I don't know what the issue is...could you please help?\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-997a50686e0d> in <module>()\r\n     39     # between the target and the softmax activation function applied to the model's prediction.\r\n     40 \r\n---> 41 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\r\n     42 \r\n     43     # Note that tf.nn.softmax_cross_entropy_with_logits internally applies the softmax\r\n\r\n~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in softmax_cross_entropy_with_logits(_sentinel, labels, logits, dim, name)\r\n   1742   \"\"\"\r\n   1743   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\r\n-> 1744                     labels, logits)\r\n   1745 \r\n   1746   # TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\r\n\r\n~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py in _ensure_xent_args(name, sentinel, labels, logits)\r\n   1696   if sentinel is not None:\r\n   1697     raise ValueError(\"Only call `%s` with \"\r\n-> 1698                      \"named arguments (labels=..., logits=..., ...)\" % name)\r\n   1699   if labels is None or logits is None:\r\n   1700     raise ValueError(\"Both labels and logits must be provided.\")\r\n\r\nValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)\r\n\r\nHere is my script (essentially copied from the guide):\r\n\r\n# Multilayer CNN using Tensorflow\r\n# Load Data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\n# Start Tensorflow InteractiveSession\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\n# Build Softmax regression model. Define our model and training loss function.\r\n      \r\n    # creating nodes for the input images and target output classes\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, 784])\r\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\r\nsess.run(tf.global_variables_initializer())\r\n\r\n\r\n# Weight initialization\r\ndef weight_variable(shape):\r\n  initial = tf.truncated_normal(shape, stddev=0.1)\r\n  return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n  initial = tf.constant(0.1, shape=shape)\r\n  return tf.Variable(initial)\r\n\r\n# Convolutional and pooling operations\r\n# Our convolutions uses a stride of one and are zero padded so that\r\n# the output is the same size as the input. Our pooling is plain \r\n# old max pooling over 2x2 blocks. To keep our code cleaner, let's \r\n# also abstract those operations into functions.\r\n\r\ndef conv2d(x, W):\r\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\r\n                        strides=[1, 2, 2, 1], padding='SAME')\r\n\r\n# Implement first convolutional layer. t will consist of convolution, \r\n# followed by max pooling. The convolution will compute 32 features \r\n# for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32].\r\n# The first two dimensions are the patch size, the next is the number of \r\n# input channels, aTo apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels.nd the last is the number of output channels. We will \r\n# also have a bias vector with a component for each output channel.\r\n\r\nW_conv1 = weight_variable([5, 5, 1, 32])\r\nb_conv1 = bias_variable([32])\r\n\r\n# To apply the layer, we first reshape x to a 4d tensor, with the second \r\n# and third dimensions corresponding to image width and height, and the \r\n# final dimension corresponding to the number of color channels.\r\n\r\nx_image = tf.reshape(x, [-1,28,28,1])\r\n\r\n# Then convolve x_image with the weight tensor, add the bias, apply the\r\n# ReLU function, and finally max pool. The max_pool_2x2 method will reduce \r\n# the image size to 14x14.\r\n\r\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\nh_pool1 = max_pool_2x2(h_conv1)\r\n\r\n# Second convolutional layer.\r\n# In order to build a deep network, we stack several layers of this type. \r\n# The second layer will have 64 features for each 5x5 patch.\r\n\r\nW_conv2 = weight_variable([5, 5, 32, 64])\r\nb_conv2 = bias_variable([64])\r\n\r\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\r\nh_pool2 = max_pool_2x2(h_conv2)\r\n\r\n# Densely connected layer. \r\n# Now that the image size has been reduced to 7x7, we add a fully-connected\r\n# layer with 1024 neurons to allow processing on the entire image. We \r\n# reshape the tensor from the pooling layer into a batch of vectors, \r\n# multiply by a weight matrix, add a bias, and apply a ReLU.\r\n\r\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\r\nb_fc1 = bias_variable([1024])\r\n\r\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\r\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\r\n\r\n# Dropout\r\n# To reduce overfitting, we will apply dropout before the readout layer. \r\n# We create a placeholder for the probability that a neuron's output is \r\n# kept during dropout. This allows us to turn dropout on during training, \r\n# and turn it off during testing. TensorFlow's tf.nn.dropout op \r\n# automatically handles scaling neuron outputs in addition to masking them, \r\n# so dropout just works without any additional scaling.\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\r\n\r\n# Readout layer.  one layer softmax regression.\r\n\r\nW_fc2 = weight_variable([1024, 10])\r\nb_fc2 = bias_variable([10])\r\n\r\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\r\n\r\n# Train and Evaluate model.\r\n\r\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\r\n\r\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\r\n\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\nfor i in range(20000):\r\n  batch = mnist.train.next_batch(50)\r\n  if i%100 == 0:\r\n    train_accuracy = accuracy.eval(feed_dict={\r\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\r\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\r\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\r\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\r\n\r\n                                              "}