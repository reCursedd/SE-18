{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6164", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6164/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6164/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6164/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6164", "id": 194098611, "node_id": "MDU6SXNzdWUxOTQwOTg2MTE=", "number": 6164, "title": "TFRecords: DataLossError (see above for traceback): corrupted record at XXX", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2016-12-07T16:32:56Z", "updated_at": "2018-06-11T06:55:33Z", "closed_at": "2017-06-28T00:25:36Z", "author_association": "NONE", "body_html": "<p>I convert large data from csv to tfrecords using tf.python_io.TFRecordWriter in hadoop<br>\nthere're some error happens:</p>\n<ul>\n<li>BUG 1) if I use zlib or gzip when create tfrecord writer , I can convert the csv successfully. But when I  loop all files to read , it's  stuck at some lines without any error.  when use these tfrecords to train model ,  errors \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\"<br>\n(I use writer.close() when write done. so I maybe it's  a bug )</li>\n<li>BUG 2) When I create tfrecord write without any compress option, the convert process goes well, and when I use the same program loop all the converted files , It all good . BUT when I use these tfrecords to train model in tensorflow , it's report \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\" again\uff08some step after, not at the begining, so maybe some part of the tfrecords error\uff09 .</li>\n</ul>\n<p>my question:</p>\n<ul>\n<li>(if solve these bugs are diffcult ) how to skip the corruted records?<br>\nOR</li>\n<li>how to solve these errors ?</li>\n</ul>\n<h3>Environment info</h3>\n<p>Operating System:<br>\ncentos 6 + hadoop</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>my convert code:</p>\n<pre><code>          writer = tf.python_io.TFRecordWriter(pb_path)\n          example = tf.train.Example(features=tf.train.Features(feature={\n              \"label\":\n                  tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n              \"ids\":\n                  tf.train.Feature(int64_list=tf.train.Int64List(value=ids)),\n              \"values\":\n                  tf.train.Feature(float_list=tf.train.FloatList(value=values))\n          }))\n\n          writer.write(example.SerializeToString())\n          writer.close()\n</code></pre>\n<p>my debug code (for loop all files):</p>\n<pre><code>  for f in files:\n   for serialized_example in tf.python_io.tf_record_iterator(f):\n      example = tf.train.Example()\n      example.ParseFromString(serialized_example)\n\n      # Read data in specified format\n      label = example.features.feature[\"label\"].float_list.value\n      ids = example.features.feature[\"ids\"].int64_list.value\n      values = example.features.feature[\"values\"].float_list.value\n</code></pre>\n<p>my train code</p>\n<pre><code># Read TFRecords files for training\nfilename_queue = tf.train.string_input_producer(\n    tf.train.match_filenames_once(FLAGS.train),\n    num_epochs=epoch_number)\nserialized_example = read_and_decode(filename_queue)\nbatch_serialized_example = tf.train.shuffle_batch(\n    [serialized_example],\n    batch_size=batch_size,\n    num_threads=thread_number,\n    capacity=capacity,\n    min_after_dequeue=min_after_dequeue)\nfeatures = tf.parse_example(\n    batch_serialized_example,\n    features={\n        \"label\": tf.FixedLenFeature([], tf.float32),\n        \"ids\": tf.VarLenFeature(tf.int64),\n        \"values\": tf.VarLenFeature(tf.float32),\n    })\nbatch_labels = features[\"label\"]\nbatch_ids = features[\"ids\"]\nbatch_values = features[\"values\"]\n</code></pre>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>\n<pre><code>coord stopped\nTraceback (most recent call last):\n  File \"deepcake.py\", line 327, in &lt;module&gt;\n    coord.join(threads)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 2863006\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]\n\nCaused by op u'ReaderRead', defined at:\n  File \"deepcake.py\", line 99, in &lt;module&gt;\n    serialized_example = read_and_decode(filename_queue)\n  File \"deepcake.py\", line 92, in read_and_decode\n    _, serialized_example = reader.read(filename_queue)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 265, in read\n    return gen_io_ops._reader_read(self._reader_ref, queue_ref, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 213, in _reader_read\n    queue_handle=queue_handle, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): corrupted record at 2863006\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]\n</code></pre>", "body_text": "I convert large data from csv to tfrecords using tf.python_io.TFRecordWriter in hadoop\nthere're some error happens:\n\nBUG 1) if I use zlib or gzip when create tfrecord writer , I can convert the csv successfully. But when I  loop all files to read , it's  stuck at some lines without any error.  when use these tfrecords to train model ,  errors \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\"\n(I use writer.close() when write done. so I maybe it's  a bug )\nBUG 2) When I create tfrecord write without any compress option, the convert process goes well, and when I use the same program loop all the converted files , It all good . BUT when I use these tfrecords to train model in tensorflow , it's report \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\" again\uff08some step after, not at the begining, so maybe some part of the tfrecords error\uff09 .\n\nmy question:\n\n(if solve these bugs are diffcult ) how to skip the corruted records?\nOR\nhow to solve these errors ?\n\nEnvironment info\nOperating System:\ncentos 6 + hadoop\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nmy convert code:\n          writer = tf.python_io.TFRecordWriter(pb_path)\n          example = tf.train.Example(features=tf.train.Features(feature={\n              \"label\":\n                  tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n              \"ids\":\n                  tf.train.Feature(int64_list=tf.train.Int64List(value=ids)),\n              \"values\":\n                  tf.train.Feature(float_list=tf.train.FloatList(value=values))\n          }))\n\n          writer.write(example.SerializeToString())\n          writer.close()\n\nmy debug code (for loop all files):\n  for f in files:\n   for serialized_example in tf.python_io.tf_record_iterator(f):\n      example = tf.train.Example()\n      example.ParseFromString(serialized_example)\n\n      # Read data in specified format\n      label = example.features.feature[\"label\"].float_list.value\n      ids = example.features.feature[\"ids\"].int64_list.value\n      values = example.features.feature[\"values\"].float_list.value\n\nmy train code\n# Read TFRecords files for training\nfilename_queue = tf.train.string_input_producer(\n    tf.train.match_filenames_once(FLAGS.train),\n    num_epochs=epoch_number)\nserialized_example = read_and_decode(filename_queue)\nbatch_serialized_example = tf.train.shuffle_batch(\n    [serialized_example],\n    batch_size=batch_size,\n    num_threads=thread_number,\n    capacity=capacity,\n    min_after_dequeue=min_after_dequeue)\nfeatures = tf.parse_example(\n    batch_serialized_example,\n    features={\n        \"label\": tf.FixedLenFeature([], tf.float32),\n        \"ids\": tf.VarLenFeature(tf.int64),\n        \"values\": tf.VarLenFeature(tf.float32),\n    })\nbatch_labels = features[\"label\"]\nbatch_ids = features[\"ids\"]\nbatch_values = features[\"values\"]\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\ncoord stopped\nTraceback (most recent call last):\n  File \"deepcake.py\", line 327, in <module>\n    coord.join(threads)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 2863006\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]\n\nCaused by op u'ReaderRead', defined at:\n  File \"deepcake.py\", line 99, in <module>\n    serialized_example = read_and_decode(filename_queue)\n  File \"deepcake.py\", line 92, in read_and_decode\n    _, serialized_example = reader.read(filename_queue)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 265, in read\n    return gen_io_ops._reader_read(self._reader_ref, queue_ref, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 213, in _reader_read\n    queue_handle=queue_handle, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): corrupted record at 2863006\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]", "body": "I convert large data from csv to tfrecords using tf.python_io.TFRecordWriter in hadoop\r\nthere're some error happens:\r\n* BUG 1) if I use zlib or gzip when create tfrecord writer , I can convert the csv successfully. But when I  loop all files to read , it's  stuck at some lines without any error.  when use these tfrecords to train model ,  errors \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\"\r\n(I use writer.close() when write done. so I maybe it's  a bug )\r\n* BUG 2) When I create tfrecord write without any compress option, the convert process goes well, and when I use the same program loop all the converted files , It all good . BUT when I use these tfrecords to train model in tensorflow , it's report \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\" again\uff08some step after, not at the begining, so maybe some part of the tfrecords error\uff09 . \r\n\r\nmy question:\r\n* (if solve these bugs are diffcult ) how to skip the corruted records? \r\nOR\r\n* how to solve these errors ?\r\n\r\n### Environment info\r\nOperating System:\r\ncentos 6 + hadoop\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nmy convert code:\r\n```\r\n          writer = tf.python_io.TFRecordWriter(pb_path)\r\n          example = tf.train.Example(features=tf.train.Features(feature={\r\n              \"label\":\r\n                  tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\r\n              \"ids\":\r\n                  tf.train.Feature(int64_list=tf.train.Int64List(value=ids)),\r\n              \"values\":\r\n                  tf.train.Feature(float_list=tf.train.FloatList(value=values))\r\n          }))\r\n\r\n          writer.write(example.SerializeToString())\r\n          writer.close()\r\n```\r\n\r\nmy debug code (for loop all files):\r\n```\r\n  for f in files:\r\n   for serialized_example in tf.python_io.tf_record_iterator(f):\r\n      example = tf.train.Example()\r\n      example.ParseFromString(serialized_example)\r\n\r\n      # Read data in specified format\r\n      label = example.features.feature[\"label\"].float_list.value\r\n      ids = example.features.feature[\"ids\"].int64_list.value\r\n      values = example.features.feature[\"values\"].float_list.value\r\n```\r\n\r\nmy train code\r\n```\r\n# Read TFRecords files for training\r\nfilename_queue = tf.train.string_input_producer(\r\n    tf.train.match_filenames_once(FLAGS.train),\r\n    num_epochs=epoch_number)\r\nserialized_example = read_and_decode(filename_queue)\r\nbatch_serialized_example = tf.train.shuffle_batch(\r\n    [serialized_example],\r\n    batch_size=batch_size,\r\n    num_threads=thread_number,\r\n    capacity=capacity,\r\n    min_after_dequeue=min_after_dequeue)\r\nfeatures = tf.parse_example(\r\n    batch_serialized_example,\r\n    features={\r\n        \"label\": tf.FixedLenFeature([], tf.float32),\r\n        \"ids\": tf.VarLenFeature(tf.int64),\r\n        \"values\": tf.VarLenFeature(tf.float32),\r\n    })\r\nbatch_labels = features[\"label\"]\r\nbatch_ids = features[\"ids\"]\r\nbatch_values = features[\"values\"]\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\ncoord stopped\r\nTraceback (most recent call last):\r\n  File \"deepcake.py\", line 327, in <module>\r\n    coord.join(threads)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 2863006\r\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]\r\n\r\nCaused by op u'ReaderRead', defined at:\r\n  File \"deepcake.py\", line 99, in <module>\r\n    serialized_example = read_and_decode(filename_queue)\r\n  File \"deepcake.py\", line 92, in read_and_decode\r\n    _, serialized_example = reader.read(filename_queue)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 265, in read\r\n    return gen_io_ops._reader_read(self._reader_ref, queue_ref, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 213, in _reader_read\r\n    queue_handle=queue_handle, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nDataLossError (see above for traceback): corrupted record at 2863006\r\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]\r\n```"}