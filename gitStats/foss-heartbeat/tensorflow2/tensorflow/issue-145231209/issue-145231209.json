{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1742", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1742/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1742/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1742/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1742", "id": 145231209, "node_id": "MDU6SXNzdWUxNDUyMzEyMDk=", "number": 1742, "title": "CTC: Add support for dense label matrix", "user": {"login": "mbhenry", "id": 6217742, "node_id": "MDQ6VXNlcjYyMTc3NDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/6217742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbhenry", "html_url": "https://github.com/mbhenry", "followers_url": "https://api.github.com/users/mbhenry/followers", "following_url": "https://api.github.com/users/mbhenry/following{/other_user}", "gists_url": "https://api.github.com/users/mbhenry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbhenry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbhenry/subscriptions", "organizations_url": "https://api.github.com/users/mbhenry/orgs", "repos_url": "https://api.github.com/users/mbhenry/repos", "events_url": "https://api.github.com/users/mbhenry/events{/privacy}", "received_events_url": "https://api.github.com/users/mbhenry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2016-04-01T16:59:24Z", "updated_at": "2018-02-26T03:35:01Z", "closed_at": "2016-06-10T18:48:49Z", "author_association": "NONE", "body_html": "<p>I'm working on integrating the new CTC operation into Keras and the SparseTensor used to pass the labels is causing a lot challenges due internal Keras requirements about the shape of the data going into the layers. What would be make integration trivial is an alternative dense label input format:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">ctc_loss_dense</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">sequence_length</span>, <span class=\"pl-smi\">label_length</span>,\n             <span class=\"pl-smi\">preprocess_collapse_repeated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">ctc_merge_repeated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Computes the CTC (Connectionist Temporal Classification) Loss.</span>\n<span class=\"pl-s\">  Requires:</span>\n<span class=\"pl-s\">       max_label_length &lt;= sequence_length</span>\n<span class=\"pl-s\">  If ctc_merge_repeated is set False, then *during* CTC calculation</span>\n<span class=\"pl-s\">  repeated non-blank labels will not be merged and are interpreted</span>\n<span class=\"pl-s\">  as individual labels.  This is a simplified version of CTC.</span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    inputs: 3-D `float` `Tensor` sized</span>\n<span class=\"pl-s\">      `[max_time x batch_size x num_classes]`.  The logits.</span>\n<span class=\"pl-s\">    labels: 2-D `int` `Tensor` sized</span>\n<span class=\"pl-s\">       `[max_label_length x batch_size]</span>\n<span class=\"pl-s\">    sequence_length: 1-D `int32` vector, size `[batch_size]`.</span>\n<span class=\"pl-s\">      The sequence lengths.</span>\n<span class=\"pl-s\">    label_length: 1-D `int32` vector, size `[batch_size]`.</span>\n<span class=\"pl-s\">      The label lengths.</span>\n<span class=\"pl-s\">    preprocess_collapse_repeated: Boolean.  Default: False.</span>\n<span class=\"pl-s\">      If True, repeated labels are collapsed prior to the CTC calculation.</span>\n<span class=\"pl-s\">    ctc_merge_repeated: Boolean.  Default: True.</span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    A 1-D `float` `Tensor`, size `[batch]`, containing logits.</span>\n<span class=\"pl-s\">  Raises:</span>\n<span class=\"pl-s\">    TypeError: if labels is not a `SparseTensor`.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span></pre></div>\n<p>This dense input format would also be consistent with Baidu's Warp-CTC and various Theano CTC implementations, so projects using cross-library platforms like Keras would be a lot cleaner.  Also, for common CTC uses like OCR and speech processing, the sequence lengths are often a lot smaller than the input lengths, so I don't think the increased bandwidth from not having a spare tensor would be too noticeable.</p>", "body_text": "I'm working on integrating the new CTC operation into Keras and the SparseTensor used to pass the labels is causing a lot challenges due internal Keras requirements about the shape of the data going into the layers. What would be make integration trivial is an alternative dense label input format:\ndef ctc_loss_dense(inputs, labels, sequence_length, label_length,\n             preprocess_collapse_repeated=False, ctc_merge_repeated=True):\n  \"\"\"Computes the CTC (Connectionist Temporal Classification) Loss.\n  Requires:\n       max_label_length <= sequence_length\n  If ctc_merge_repeated is set False, then *during* CTC calculation\n  repeated non-blank labels will not be merged and are interpreted\n  as individual labels.  This is a simplified version of CTC.\n  Args:\n    inputs: 3-D `float` `Tensor` sized\n      `[max_time x batch_size x num_classes]`.  The logits.\n    labels: 2-D `int` `Tensor` sized\n       `[max_label_length x batch_size]\n    sequence_length: 1-D `int32` vector, size `[batch_size]`.\n      The sequence lengths.\n    label_length: 1-D `int32` vector, size `[batch_size]`.\n      The label lengths.\n    preprocess_collapse_repeated: Boolean.  Default: False.\n      If True, repeated labels are collapsed prior to the CTC calculation.\n    ctc_merge_repeated: Boolean.  Default: True.\n  Returns:\n    A 1-D `float` `Tensor`, size `[batch]`, containing logits.\n  Raises:\n    TypeError: if labels is not a `SparseTensor`.\n  \"\"\"\nThis dense input format would also be consistent with Baidu's Warp-CTC and various Theano CTC implementations, so projects using cross-library platforms like Keras would be a lot cleaner.  Also, for common CTC uses like OCR and speech processing, the sequence lengths are often a lot smaller than the input lengths, so I don't think the increased bandwidth from not having a spare tensor would be too noticeable.", "body": "I'm working on integrating the new CTC operation into Keras and the SparseTensor used to pass the labels is causing a lot challenges due internal Keras requirements about the shape of the data going into the layers. What would be make integration trivial is an alternative dense label input format:\n\n``` python\ndef ctc_loss_dense(inputs, labels, sequence_length, label_length,\n             preprocess_collapse_repeated=False, ctc_merge_repeated=True):\n  \"\"\"Computes the CTC (Connectionist Temporal Classification) Loss.\n  Requires:\n       max_label_length <= sequence_length\n  If ctc_merge_repeated is set False, then *during* CTC calculation\n  repeated non-blank labels will not be merged and are interpreted\n  as individual labels.  This is a simplified version of CTC.\n  Args:\n    inputs: 3-D `float` `Tensor` sized\n      `[max_time x batch_size x num_classes]`.  The logits.\n    labels: 2-D `int` `Tensor` sized\n       `[max_label_length x batch_size]\n    sequence_length: 1-D `int32` vector, size `[batch_size]`.\n      The sequence lengths.\n    label_length: 1-D `int32` vector, size `[batch_size]`.\n      The label lengths.\n    preprocess_collapse_repeated: Boolean.  Default: False.\n      If True, repeated labels are collapsed prior to the CTC calculation.\n    ctc_merge_repeated: Boolean.  Default: True.\n  Returns:\n    A 1-D `float` `Tensor`, size `[batch]`, containing logits.\n  Raises:\n    TypeError: if labels is not a `SparseTensor`.\n  \"\"\"\n```\n\nThis dense input format would also be consistent with Baidu's Warp-CTC and various Theano CTC implementations, so projects using cross-library platforms like Keras would be a lot cleaner.  Also, for common CTC uses like OCR and speech processing, the sequence lengths are often a lot smaller than the input lengths, so I don't think the increased bandwidth from not having a spare tensor would be too noticeable. \n"}