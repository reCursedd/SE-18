{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225065522", "html_url": "https://github.com/tensorflow/tensorflow/issues/2768#issuecomment-225065522", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2768", "id": 225065522, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTA2NTUyMg==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-10T00:33:06Z", "updated_at": "2016-06-10T00:33:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The easiest way to work around this is to specify a list of explicit variable names when creating your <code>tf.train.Saver()</code>. Creating a <code>tf.train.Saver</code> with no arguments is equivalent to doing:</p>\n<div class=\"highlight highlight-source-python\"><pre>saver <span class=\"pl-k\">=</span> tf.train.Saver({v.op.name: v <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> tf.all_variables()})</pre></div>\n<p>The dictionary keys correspond to the names in the checkpoint file that will be read/written by the saver. You can customize the dictionary so that it matches your old checkpoint. For example:</p>\n<div class=\"highlight highlight-source-python\"><pre>names_to_vars <span class=\"pl-k\">=</span> {v.op.name: v <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> tf.all_variables()}\nbias_var <span class=\"pl-k\">=</span> names_to_vars[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model/attention_decoder/Attention_0/fully_connected/biases<span class=\"pl-pds\">\"</span></span>]\nnames_to_vars[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model/attention_decoder/Attention_0/fully_connected/bias<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> bias_var\n<span class=\"pl-k\">del</span> names_to_vars[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model/attention_decoder/Attention_0/fully_connected/biases<span class=\"pl-pds\">\"</span></span>]\nsaver <span class=\"pl-k\">=</span> tf.train.Saver(<span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>names_to_vars)</pre></div>\n<p>PS. This is probably more appropriate as a Stack Overflow question, rather than a GitHub issue.</p>", "body_text": "The easiest way to work around this is to specify a list of explicit variable names when creating your tf.train.Saver(). Creating a tf.train.Saver with no arguments is equivalent to doing:\nsaver = tf.train.Saver({v.op.name: v for v in tf.all_variables()})\nThe dictionary keys correspond to the names in the checkpoint file that will be read/written by the saver. You can customize the dictionary so that it matches your old checkpoint. For example:\nnames_to_vars = {v.op.name: v for v in tf.all_variables()}\nbias_var = names_to_vars[\"model/attention_decoder/Attention_0/fully_connected/biases\"]\nnames_to_vars[\"model/attention_decoder/Attention_0/fully_connected/bias\"] = bias_var\ndel names_to_vars[\"model/attention_decoder/Attention_0/fully_connected/biases\"]\nsaver = tf.train.Saver(var_list=names_to_vars)\nPS. This is probably more appropriate as a Stack Overflow question, rather than a GitHub issue.", "body": "The easiest way to work around this is to specify a list of explicit variable names when creating your `tf.train.Saver()`. Creating a `tf.train.Saver` with no arguments is equivalent to doing:\n\n``` python\nsaver = tf.train.Saver({v.op.name: v for v in tf.all_variables()})\n```\n\nThe dictionary keys correspond to the names in the checkpoint file that will be read/written by the saver. You can customize the dictionary so that it matches your old checkpoint. For example:\n\n``` python\nnames_to_vars = {v.op.name: v for v in tf.all_variables()}\nbias_var = names_to_vars[\"model/attention_decoder/Attention_0/fully_connected/biases\"]\nnames_to_vars[\"model/attention_decoder/Attention_0/fully_connected/bias\"] = bias_var\ndel names_to_vars[\"model/attention_decoder/Attention_0/fully_connected/biases\"]\nsaver = tf.train.Saver(var_list=names_to_vars)\n```\n\nPS. This is probably more appropriate as a Stack Overflow question, rather than a GitHub issue.\n"}