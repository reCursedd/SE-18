{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19007", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19007/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19007/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19007/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19007", "id": 319307302, "node_id": "MDU6SXNzdWUzMTkzMDczMDI=", "number": 19007, "title": "Using different optimizers on a trained model leads to poor test accuracy.", "user": {"login": "ankit1997", "id": 12840347, "node_id": "MDQ6VXNlcjEyODQwMzQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/12840347?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankit1997", "html_url": "https://github.com/ankit1997", "followers_url": "https://api.github.com/users/ankit1997/followers", "following_url": "https://api.github.com/users/ankit1997/following{/other_user}", "gists_url": "https://api.github.com/users/ankit1997/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankit1997/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankit1997/subscriptions", "organizations_url": "https://api.github.com/users/ankit1997/orgs", "repos_url": "https://api.github.com/users/ankit1997/repos", "events_url": "https://api.github.com/users/ankit1997/events{/privacy}", "received_events_url": "https://api.github.com/users/ankit1997/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-05-01T19:56:55Z", "updated_at": "2018-05-17T00:46:44Z", "closed_at": "2018-05-17T00:46:44Z", "author_association": "NONE", "body_html": "<p>I have trained a convolutional neural network model using <code>AdamOptimizer</code>. The test accuracy is pretty good. This was expected since the validation loss was decreasing while training.</p>\n<p>Now I restored the model weights using <code>saver.restore()</code> method, but this time I built the model with <code>RMSPropOptimizer</code> just for experimenting. Please note that I <strong>DID NOT RETRAIN THE MODEL</strong>. To my surprise, the test accuracy was horrible with the new optimizer. The accuracy becomes good again when I use the previously used <code>AdamOptimizer</code>.</p>\n<p>This is peculiar in the sense that the train_op should not affect the test data results.</p>\n<p>Below is the sample code I used for calculating the test accuracy. Please note I did not run the <code>train_op</code> here.</p>\n<pre><code>accuracies = []\n\n# Data generator\nfor batchx, batchy in dataLoader.next_validation():\n    # Run the graph.\n    pred = sess.run(model['prediction'], \n                        feed_dict={\n                            model['sensor_data']: batchx,\n                            model['label']: batchy\n                        })\n        \n    accuracies.append(np.count_nonzero(pred == label) / pred.shape[0] * 100)\n\naccuracies = np.array(accuracies)\nprint(\"Average Validation set accuracy: {} %\".format(accuracies.mean()))\n</code></pre>\n<p>Am I missing something here? Or is this a possible bug?</p>", "body_text": "I have trained a convolutional neural network model using AdamOptimizer. The test accuracy is pretty good. This was expected since the validation loss was decreasing while training.\nNow I restored the model weights using saver.restore() method, but this time I built the model with RMSPropOptimizer just for experimenting. Please note that I DID NOT RETRAIN THE MODEL. To my surprise, the test accuracy was horrible with the new optimizer. The accuracy becomes good again when I use the previously used AdamOptimizer.\nThis is peculiar in the sense that the train_op should not affect the test data results.\nBelow is the sample code I used for calculating the test accuracy. Please note I did not run the train_op here.\naccuracies = []\n\n# Data generator\nfor batchx, batchy in dataLoader.next_validation():\n    # Run the graph.\n    pred = sess.run(model['prediction'], \n                        feed_dict={\n                            model['sensor_data']: batchx,\n                            model['label']: batchy\n                        })\n        \n    accuracies.append(np.count_nonzero(pred == label) / pred.shape[0] * 100)\n\naccuracies = np.array(accuracies)\nprint(\"Average Validation set accuracy: {} %\".format(accuracies.mean()))\n\nAm I missing something here? Or is this a possible bug?", "body": "I have trained a convolutional neural network model using `AdamOptimizer`. The test accuracy is pretty good. This was expected since the validation loss was decreasing while training.\r\n\r\nNow I restored the model weights using `saver.restore()` method, but this time I built the model with `RMSPropOptimizer` just for experimenting. Please note that I **DID NOT RETRAIN THE MODEL**. To my surprise, the test accuracy was horrible with the new optimizer. The accuracy becomes good again when I use the previously used `AdamOptimizer`.\r\n\r\nThis is peculiar in the sense that the train_op should not affect the test data results.\r\n\r\nBelow is the sample code I used for calculating the test accuracy. Please note I did not run the `train_op` here.\r\n\r\n```\r\naccuracies = []\r\n\r\n# Data generator\r\nfor batchx, batchy in dataLoader.next_validation():\r\n    # Run the graph.\r\n    pred = sess.run(model['prediction'], \r\n                        feed_dict={\r\n                            model['sensor_data']: batchx,\r\n                            model['label']: batchy\r\n                        })\r\n        \r\n    accuracies.append(np.count_nonzero(pred == label) / pred.shape[0] * 100)\r\n\r\naccuracies = np.array(accuracies)\r\nprint(\"Average Validation set accuracy: {} %\".format(accuracies.mean()))\r\n```\r\n\r\nAm I missing something here? Or is this a possible bug?"}