{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/401613329", "html_url": "https://github.com/tensorflow/tensorflow/pull/19649#issuecomment-401613329", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19649", "id": 401613329, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTYxMzMyOQ==", "user": {"login": "jperl", "id": 1136652, "node_id": "MDQ6VXNlcjExMzY2NTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1136652?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jperl", "html_url": "https://github.com/jperl", "followers_url": "https://api.github.com/users/jperl/followers", "following_url": "https://api.github.com/users/jperl/following{/other_user}", "gists_url": "https://api.github.com/users/jperl/gists{/gist_id}", "starred_url": "https://api.github.com/users/jperl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jperl/subscriptions", "organizations_url": "https://api.github.com/users/jperl/orgs", "repos_url": "https://api.github.com/users/jperl/repos", "events_url": "https://api.github.com/users/jperl/events{/privacy}", "received_events_url": "https://api.github.com/users/jperl/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-01T15:20:02Z", "updated_at": "2018-07-01T15:20:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19293677\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ispirmustafa\">@ispirmustafa</a> I modifier this to add a <code>dense_normalizer_fn</code> that runs after the sparse tensor is converted to a dense tensor. This is because it is easier for us to normalize with dense tensor ops.</p>\n<p><code>sequence_numeric_column(key, dense_normalizer_fn=lambda dense_tensor:  (dense_tensor - mean) / std</code>)</p>\n<p>Should I work on a PR with tests to TF core -- IE something you think it worth adding to TF? Or should we be approaching this differently?</p>\n<pre><code>  def _get_sequence_dense_tensor(self, inputs, weight_collections=None, trainable=None):\n......\n    dense_shape = array_ops.concat(\n        [array_ops.shape(dense_tensor)[:1], [-1], self._variable_shape], axis=0)\n    dense_tensor = array_ops.reshape(dense_tensor, shape=dense_shape)\n\n    # OUR CHANGE IS HERE\n    if self.dense_normalizer_fn is not None:\n      dense_tensor = self.dense_normalizer_fn(dense_tensor)\n</code></pre>", "body_text": "@ispirmustafa I modifier this to add a dense_normalizer_fn that runs after the sparse tensor is converted to a dense tensor. This is because it is easier for us to normalize with dense tensor ops.\nsequence_numeric_column(key, dense_normalizer_fn=lambda dense_tensor:  (dense_tensor - mean) / std)\nShould I work on a PR with tests to TF core -- IE something you think it worth adding to TF? Or should we be approaching this differently?\n  def _get_sequence_dense_tensor(self, inputs, weight_collections=None, trainable=None):\n......\n    dense_shape = array_ops.concat(\n        [array_ops.shape(dense_tensor)[:1], [-1], self._variable_shape], axis=0)\n    dense_tensor = array_ops.reshape(dense_tensor, shape=dense_shape)\n\n    # OUR CHANGE IS HERE\n    if self.dense_normalizer_fn is not None:\n      dense_tensor = self.dense_normalizer_fn(dense_tensor)", "body": "@ispirmustafa I modifier this to add a `dense_normalizer_fn` that runs after the sparse tensor is converted to a dense tensor. This is because it is easier for us to normalize with dense tensor ops.\r\n\r\n`sequence_numeric_column(key, dense_normalizer_fn=lambda dense_tensor:  (dense_tensor - mean) / std`)\r\n\r\nShould I work on a PR with tests to TF core -- IE something you think it worth adding to TF? Or should we be approaching this differently?\r\n\r\n```\r\n  def _get_sequence_dense_tensor(self, inputs, weight_collections=None, trainable=None):\r\n......\r\n    dense_shape = array_ops.concat(\r\n        [array_ops.shape(dense_tensor)[:1], [-1], self._variable_shape], axis=0)\r\n    dense_tensor = array_ops.reshape(dense_tensor, shape=dense_shape)\r\n\r\n    # OUR CHANGE IS HERE\r\n    if self.dense_normalizer_fn is not None:\r\n      dense_tensor = self.dense_normalizer_fn(dense_tensor)\r\n```"}