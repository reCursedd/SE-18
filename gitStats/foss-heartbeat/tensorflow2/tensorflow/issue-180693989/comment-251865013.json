{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251865013", "html_url": "https://github.com/tensorflow/tensorflow/issues/4732#issuecomment-251865013", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4732", "id": 251865013, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTg2NTAxMw==", "user": {"login": "danielschonfeld", "id": 522598, "node_id": "MDQ6VXNlcjUyMjU5OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/522598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielschonfeld", "html_url": "https://github.com/danielschonfeld", "followers_url": "https://api.github.com/users/danielschonfeld/followers", "following_url": "https://api.github.com/users/danielschonfeld/following{/other_user}", "gists_url": "https://api.github.com/users/danielschonfeld/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielschonfeld/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielschonfeld/subscriptions", "organizations_url": "https://api.github.com/users/danielschonfeld/orgs", "repos_url": "https://api.github.com/users/danielschonfeld/repos", "events_url": "https://api.github.com/users/danielschonfeld/events{/privacy}", "received_events_url": "https://api.github.com/users/danielschonfeld/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-06T04:36:03Z", "updated_at": "2016-10-06T04:42:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> so doing that revealed something I didn't realize before and that now i'm not sure how to proceed.</p>\n<p>First things first, there's no problem with <code>Matmul</code>.  It produces the same amount of execution time for the same amount of bytes. I recorded only the first example from the two below, which in the feed_dict run didn't include the test data run.  I found that out by looking at the dim size and the output bytes as you suggested.</p>\n<p>Basically I do something like the following:<br>\n<code>sess.run([optimizer, cost, summeryMerge])</code><br>\nAnd later on I do something like <code>accuracy.eval()</code></p>\n<p>In the <code>Queue</code> based run, the value of accuracy is calculated at every run, even without me calling <code>accuracy.eval()</code> - I'm assuming because of the shared variables being updated since the graph has them built in.  That in turn is why my runs are considerably slower overall compared to <code>feed_dict</code>.  The operation takes the same amount of time, but is happening at every batch now instead of once per epoch.  I'm assuming that's also why the <code>Queue</code> that holds the test data gets depleted fast and takes a long while to continue producing sample data, causing <code>QueueDequeueMany</code> to appear slow.</p>\n<p>My question then is how does one go about using <code>Queues</code> and feeding in test data but have it computed only once per epoch instead of at every batch run.  From reading <code>Sharing Variables</code>, I thought thats the way to go with training/test data.  So I have a structure similar to the following code:</p>\n<pre><code>def neural_network_model(x):\n    with tf.variable_scope(\"hidden_layer_1\"):\n        relu1 = hl_relu(x, [max_words_len, n_nodes_hl1], [n_nodes_hl1]) #hl_relu utilizes get_variable for weights and biases\n</code></pre>\n<p>And later on i'd do something like this:</p>\n<pre><code>def train_neural_network(x_batch, y_batch, test_x, test_y):\n    with tf.device(\"/cpu:0\"):\n        with tf.variable_scope(\"dnn\") as scope:\n            logits = neural_network_model(x_batch)\n            scope.reuse_variables()\n            test_logits = neural_network_model(test_x)\n</code></pre>\n<p>I'd use <code>logits</code> to compute the cost with <code>softmax_cross_entropy_with_logits</code>, as follows:</p>\n<pre><code>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_batch), name=\"cost\")\n</code></pre>\n<p>and <code>test_logits</code> to compute the accuracy in a three step operation looking something like this:</p>\n<pre><code>prediction = tf.nn.softmax(test_logits)\ncorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(test_y, 1), name=\"correct\")\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n</code></pre>\n<p>How does one do it the 'right' way? My newbieness shows unfortunately</p>", "body_text": "@yaroslavvb so doing that revealed something I didn't realize before and that now i'm not sure how to proceed.\nFirst things first, there's no problem with Matmul.  It produces the same amount of execution time for the same amount of bytes. I recorded only the first example from the two below, which in the feed_dict run didn't include the test data run.  I found that out by looking at the dim size and the output bytes as you suggested.\nBasically I do something like the following:\nsess.run([optimizer, cost, summeryMerge])\nAnd later on I do something like accuracy.eval()\nIn the Queue based run, the value of accuracy is calculated at every run, even without me calling accuracy.eval() - I'm assuming because of the shared variables being updated since the graph has them built in.  That in turn is why my runs are considerably slower overall compared to feed_dict.  The operation takes the same amount of time, but is happening at every batch now instead of once per epoch.  I'm assuming that's also why the Queue that holds the test data gets depleted fast and takes a long while to continue producing sample data, causing QueueDequeueMany to appear slow.\nMy question then is how does one go about using Queues and feeding in test data but have it computed only once per epoch instead of at every batch run.  From reading Sharing Variables, I thought thats the way to go with training/test data.  So I have a structure similar to the following code:\ndef neural_network_model(x):\n    with tf.variable_scope(\"hidden_layer_1\"):\n        relu1 = hl_relu(x, [max_words_len, n_nodes_hl1], [n_nodes_hl1]) #hl_relu utilizes get_variable for weights and biases\n\nAnd later on i'd do something like this:\ndef train_neural_network(x_batch, y_batch, test_x, test_y):\n    with tf.device(\"/cpu:0\"):\n        with tf.variable_scope(\"dnn\") as scope:\n            logits = neural_network_model(x_batch)\n            scope.reuse_variables()\n            test_logits = neural_network_model(test_x)\n\nI'd use logits to compute the cost with softmax_cross_entropy_with_logits, as follows:\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_batch), name=\"cost\")\n\nand test_logits to compute the accuracy in a three step operation looking something like this:\nprediction = tf.nn.softmax(test_logits)\ncorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(test_y, 1), name=\"correct\")\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n\nHow does one do it the 'right' way? My newbieness shows unfortunately", "body": "@yaroslavvb so doing that revealed something I didn't realize before and that now i'm not sure how to proceed.  \n\nFirst things first, there's no problem with `Matmul`.  It produces the same amount of execution time for the same amount of bytes. I recorded only the first example from the two below, which in the feed_dict run didn't include the test data run.  I found that out by looking at the dim size and the output bytes as you suggested.\n\nBasically I do something like the following:\n`sess.run([optimizer, cost, summeryMerge])`\nAnd later on I do something like `accuracy.eval()`\n\nIn the `Queue` based run, the value of accuracy is calculated at every run, even without me calling `accuracy.eval()` - I'm assuming because of the shared variables being updated since the graph has them built in.  That in turn is why my runs are considerably slower overall compared to `feed_dict`.  The operation takes the same amount of time, but is happening at every batch now instead of once per epoch.  I'm assuming that's also why the `Queue` that holds the test data gets depleted fast and takes a long while to continue producing sample data, causing `QueueDequeueMany` to appear slow.\n\nMy question then is how does one go about using `Queues` and feeding in test data but have it computed only once per epoch instead of at every batch run.  From reading `Sharing Variables`, I thought thats the way to go with training/test data.  So I have a structure similar to the following code:\n\n```\ndef neural_network_model(x):\n    with tf.variable_scope(\"hidden_layer_1\"):\n        relu1 = hl_relu(x, [max_words_len, n_nodes_hl1], [n_nodes_hl1]) #hl_relu utilizes get_variable for weights and biases\n```\n\nAnd later on i'd do something like this:\n\n```\ndef train_neural_network(x_batch, y_batch, test_x, test_y):\n    with tf.device(\"/cpu:0\"):\n        with tf.variable_scope(\"dnn\") as scope:\n            logits = neural_network_model(x_batch)\n            scope.reuse_variables()\n            test_logits = neural_network_model(test_x)\n```\n\nI'd use `logits` to compute the cost with `softmax_cross_entropy_with_logits`, as follows:\n\n```\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_batch), name=\"cost\")\n```\n\nand `test_logits` to compute the accuracy in a three step operation looking something like this:\n\n```\nprediction = tf.nn.softmax(test_logits)\ncorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(test_y, 1), name=\"correct\")\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n```\n\nHow does one do it the 'right' way? My newbieness shows unfortunately\n"}