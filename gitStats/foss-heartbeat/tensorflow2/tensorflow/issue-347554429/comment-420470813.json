{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/420470813", "html_url": "https://github.com/tensorflow/tensorflow/issues/21368#issuecomment-420470813", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21368", "id": 420470813, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDQ3MDgxMw==", "user": {"login": "gargn", "id": 1900612, "node_id": "MDQ6VXNlcjE5MDA2MTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1900612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gargn", "html_url": "https://github.com/gargn", "followers_url": "https://api.github.com/users/gargn/followers", "following_url": "https://api.github.com/users/gargn/following{/other_user}", "gists_url": "https://api.github.com/users/gargn/gists{/gist_id}", "starred_url": "https://api.github.com/users/gargn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gargn/subscriptions", "organizations_url": "https://api.github.com/users/gargn/orgs", "repos_url": "https://api.github.com/users/gargn/repos", "events_url": "https://api.github.com/users/gargn/events{/privacy}", "received_events_url": "https://api.github.com/users/gargn/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-12T00:28:46Z", "updated_at": "2018-09-12T00:30:48Z", "author_association": "MEMBER", "body_html": "<p>I was able to get the following code working with last night's tf-nightly. It is based off of the Python code that you provided. The main difference is that it freezes the graph and converts the Flatbuffer to a TFLite model within the Python code itself. Can you clarify if this is what you are looking for:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nslim = tf.contrib.slim\nfrom nets import mobilenet_v1\n\nNUM_CLASSES = 10\nMOBILENET_FILENAME = 'PATH-TO-DATA/mobilenet_v1_eval.pbtxt'\n\nINPUT_ARRAYS = ['input']\nOUTPUT_ARRAYS = ['MobilenetV1/Predictions/Reshape']\n\ndef export_eval_pbtxt():\n  \"\"\"Export eval.pbtxt.\"\"\"\n  with tf.Graph().as_default() as g:\n    # Need to provide the name in order to have the name of the input arrays for conversion.\n    images = tf.placeholder(dtype=tf.float32,shape=[None,32,32,3], name=INPUT_ARRAYS[0])\n    # using one of the following methods to create graph, depends on you\n    # _, _ = mobilenet_v1.mobilenet_v1(inputs=images,num_classes=NUM_CLASSES, is_training=False)\n    with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=False,regularize_depthwise=True)):\n     _, _ = mobilenet_v1.mobilenet_v1(inputs=images, is_training=False, depth_multiplier=1.0, num_classes=NUM_CLASSES)\n\n    with tf.Session().as_default() as sess:\n        sess.run(tf.global_variables_initializer())\n        # Freeze the graph so that you can convert to TFLite it later.\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\n            sess, sess.graph_def, OUTPUT_ARRAYS)\n        with open(MOBILENET_FILENAME, 'w') as f:\n            f.write(str(frozen_graph))\n\ndef main():\n    print(\"python main function\")\n    export_eval_pbtxt()\n\n    # Convert the graph.\n    converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\n            MOBILENET_FILENAME, INPUT_ARRAYS, OUTPUT_ARRAYS)\n    tflite_model = converter.convert()\n\n    # Load TFLite model and allocate tensors.\n    interpreter = tf.contrib.lite.Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors.\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Test model on random input data.\n    input_shape = input_details[0]['shape']\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    print(output_data)\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n<p>In order to get the <code>from models import mobilenet_v1</code> working, I had to download the <code>models</code> repository and update the PYTHONPATH via the command: <code>export PYTHONPATH=PATH-TO-MODELS/models/research/slim:$PYTHONPATH</code></p>", "body_text": "I was able to get the following code working with last night's tf-nightly. It is based off of the Python code that you provided. The main difference is that it freezes the graph and converts the Flatbuffer to a TFLite model within the Python code itself. Can you clarify if this is what you are looking for:\nimport tensorflow as tf\nimport numpy as np\nslim = tf.contrib.slim\nfrom nets import mobilenet_v1\n\nNUM_CLASSES = 10\nMOBILENET_FILENAME = 'PATH-TO-DATA/mobilenet_v1_eval.pbtxt'\n\nINPUT_ARRAYS = ['input']\nOUTPUT_ARRAYS = ['MobilenetV1/Predictions/Reshape']\n\ndef export_eval_pbtxt():\n  \"\"\"Export eval.pbtxt.\"\"\"\n  with tf.Graph().as_default() as g:\n    # Need to provide the name in order to have the name of the input arrays for conversion.\n    images = tf.placeholder(dtype=tf.float32,shape=[None,32,32,3], name=INPUT_ARRAYS[0])\n    # using one of the following methods to create graph, depends on you\n    # _, _ = mobilenet_v1.mobilenet_v1(inputs=images,num_classes=NUM_CLASSES, is_training=False)\n    with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=False,regularize_depthwise=True)):\n     _, _ = mobilenet_v1.mobilenet_v1(inputs=images, is_training=False, depth_multiplier=1.0, num_classes=NUM_CLASSES)\n\n    with tf.Session().as_default() as sess:\n        sess.run(tf.global_variables_initializer())\n        # Freeze the graph so that you can convert to TFLite it later.\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\n            sess, sess.graph_def, OUTPUT_ARRAYS)\n        with open(MOBILENET_FILENAME, 'w') as f:\n            f.write(str(frozen_graph))\n\ndef main():\n    print(\"python main function\")\n    export_eval_pbtxt()\n\n    # Convert the graph.\n    converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\n            MOBILENET_FILENAME, INPUT_ARRAYS, OUTPUT_ARRAYS)\n    tflite_model = converter.convert()\n\n    # Load TFLite model and allocate tensors.\n    interpreter = tf.contrib.lite.Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n\n    # Get input and output tensors.\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Test model on random input data.\n    input_shape = input_details[0]['shape']\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    print(output_data)\n\nif __name__ == '__main__':\n    main()\n\nIn order to get the from models import mobilenet_v1 working, I had to download the models repository and update the PYTHONPATH via the command: export PYTHONPATH=PATH-TO-MODELS/models/research/slim:$PYTHONPATH", "body": "I was able to get the following code working with last night's tf-nightly. It is based off of the Python code that you provided. The main difference is that it freezes the graph and converts the Flatbuffer to a TFLite model within the Python code itself. Can you clarify if this is what you are looking for:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nslim = tf.contrib.slim\r\nfrom nets import mobilenet_v1\r\n\r\nNUM_CLASSES = 10\r\nMOBILENET_FILENAME = 'PATH-TO-DATA/mobilenet_v1_eval.pbtxt'\r\n\r\nINPUT_ARRAYS = ['input']\r\nOUTPUT_ARRAYS = ['MobilenetV1/Predictions/Reshape']\r\n\r\ndef export_eval_pbtxt():\r\n  \"\"\"Export eval.pbtxt.\"\"\"\r\n  with tf.Graph().as_default() as g:\r\n    # Need to provide the name in order to have the name of the input arrays for conversion.\r\n    images = tf.placeholder(dtype=tf.float32,shape=[None,32,32,3], name=INPUT_ARRAYS[0])\r\n    # using one of the following methods to create graph, depends on you\r\n    # _, _ = mobilenet_v1.mobilenet_v1(inputs=images,num_classes=NUM_CLASSES, is_training=False)\r\n    with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=False,regularize_depthwise=True)):\r\n     _, _ = mobilenet_v1.mobilenet_v1(inputs=images, is_training=False, depth_multiplier=1.0, num_classes=NUM_CLASSES)\r\n\r\n    with tf.Session().as_default() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        # Freeze the graph so that you can convert to TFLite it later.\r\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n            sess, sess.graph_def, OUTPUT_ARRAYS)\r\n        with open(MOBILENET_FILENAME, 'w') as f:\r\n            f.write(str(frozen_graph))\r\n\r\ndef main():\r\n    print(\"python main function\")\r\n    export_eval_pbtxt()\r\n\r\n    # Convert the graph.\r\n    converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n            MOBILENET_FILENAME, INPUT_ARRAYS, OUTPUT_ARRAYS)\r\n    tflite_model = converter.convert()\r\n\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.contrib.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(output_data)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nIn order to get the `from models import mobilenet_v1` working, I had to download the `models` repository and update the PYTHONPATH via the command: `export PYTHONPATH=PATH-TO-MODELS/models/research/slim:$PYTHONPATH`"}