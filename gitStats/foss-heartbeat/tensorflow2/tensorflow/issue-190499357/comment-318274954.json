{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318274954", "html_url": "https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-318274954", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5719", "id": 318274954, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODI3NDk1NA==", "user": {"login": "ed-alertedh", "id": 24605895, "node_id": "MDQ6VXNlcjI0NjA1ODk1", "avatar_url": "https://avatars1.githubusercontent.com/u/24605895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ed-alertedh", "html_url": "https://github.com/ed-alertedh", "followers_url": "https://api.github.com/users/ed-alertedh/followers", "following_url": "https://api.github.com/users/ed-alertedh/following{/other_user}", "gists_url": "https://api.github.com/users/ed-alertedh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ed-alertedh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ed-alertedh/subscriptions", "organizations_url": "https://api.github.com/users/ed-alertedh/orgs", "repos_url": "https://api.github.com/users/ed-alertedh/repos", "events_url": "https://api.github.com/users/ed-alertedh/events{/privacy}", "received_events_url": "https://api.github.com/users/ed-alertedh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T06:50:37Z", "updated_at": "2017-07-28T00:50:36Z", "author_association": "NONE", "body_html": "<p>Well, it looks like we got what we wanted! <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> looks to have implemented this and it's in 1.3.0rc0!<br>\nInterestingly, it's a heap-based algorithm: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc\">code here</a></p>\n<p>I'm a little sad that I didn't manage to release my code in time due to unforeseen \"red tape\" at work. But OTOH, I'm a newbie with CUDA and my thrust-based implementation wasn't really that great anyway :P</p>\n<p>edit: It's worth noting that the CPU benchmarks are probably not indicative of real-world performance, as it seems likely to me that most of us wanting a GPU implementation of this op have the rest of their graph running on GPU. To simulate the performance most people are currently getting, you need the inputs resident on the GPU and the TopK op running on CPU. For large input tensors, I've seen the DtoH transfer times start having a significant impact on throughput.</p>", "body_text": "Well, it looks like we got what we wanted! @ebrevdo looks to have implemented this and it's in 1.3.0rc0!\nInterestingly, it's a heap-based algorithm: code here\nI'm a little sad that I didn't manage to release my code in time due to unforeseen \"red tape\" at work. But OTOH, I'm a newbie with CUDA and my thrust-based implementation wasn't really that great anyway :P\nedit: It's worth noting that the CPU benchmarks are probably not indicative of real-world performance, as it seems likely to me that most of us wanting a GPU implementation of this op have the rest of their graph running on GPU. To simulate the performance most people are currently getting, you need the inputs resident on the GPU and the TopK op running on CPU. For large input tensors, I've seen the DtoH transfer times start having a significant impact on throughput.", "body": "Well, it looks like we got what we wanted! @ebrevdo looks to have implemented this and it's in 1.3.0rc0!\r\nInterestingly, it's a heap-based algorithm: [code here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc)\r\n\r\nI'm a little sad that I didn't manage to release my code in time due to unforeseen \"red tape\" at work. But OTOH, I'm a newbie with CUDA and my thrust-based implementation wasn't really that great anyway :P\r\n\r\nedit: It's worth noting that the CPU benchmarks are probably not indicative of real-world performance, as it seems likely to me that most of us wanting a GPU implementation of this op have the rest of their graph running on GPU. To simulate the performance most people are currently getting, you need the inputs resident on the GPU and the TopK op running on CPU. For large input tensors, I've seen the DtoH transfer times start having a significant impact on throughput."}