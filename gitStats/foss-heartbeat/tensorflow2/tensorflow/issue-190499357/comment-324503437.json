{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324503437", "html_url": "https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-324503437", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5719", "id": 324503437, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDUwMzQzNw==", "user": {"login": "ed-alertedh", "id": 24605895, "node_id": "MDQ6VXNlcjI0NjA1ODk1", "avatar_url": "https://avatars1.githubusercontent.com/u/24605895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ed-alertedh", "html_url": "https://github.com/ed-alertedh", "followers_url": "https://api.github.com/users/ed-alertedh/followers", "following_url": "https://api.github.com/users/ed-alertedh/following{/other_user}", "gists_url": "https://api.github.com/users/ed-alertedh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ed-alertedh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ed-alertedh/subscriptions", "organizations_url": "https://api.github.com/users/ed-alertedh/orgs", "repos_url": "https://api.github.com/users/ed-alertedh/repos", "events_url": "https://api.github.com/users/ed-alertedh/events{/privacy}", "received_events_url": "https://api.github.com/users/ed-alertedh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-24T01:02:39Z", "updated_at": "2017-08-24T01:02:39Z", "author_association": "NONE", "body_html": "<p>Yeah, I think it is just inherent in how this problem scales to GPU - \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it doesn't come across that we're criticising your implementation! Just figuring out what the scaling behaviour is.</p>\n<p>For the sake of clarity, in my above statements I was using the convention [rows, columns] whereas <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=729312\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/BlackHC\">@BlackHC</a> seems to be using the convention [batch size, rows].</p>\n<p>If it only launches one thread block per batch (=1024 threads on most devices), wouldn't that mean the implementation scales well to large batch sizes and doesn't scale well to large numbers of rows? (using your convention [batch size, rows]).</p>", "body_text": "Yeah, I think it is just inherent in how this problem scales to GPU - \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it doesn't come across that we're criticising your implementation! Just figuring out what the scaling behaviour is.\nFor the sake of clarity, in my above statements I was using the convention [rows, columns] whereas @BlackHC seems to be using the convention [batch size, rows].\nIf it only launches one thread block per batch (=1024 threads on most devices), wouldn't that mean the implementation scales well to large batch sizes and doesn't scale well to large numbers of rows? (using your convention [batch size, rows]).", "body": "Yeah, I think it is just inherent in how this problem scales to GPU - \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it doesn't come across that we're criticising your implementation! Just figuring out what the scaling behaviour is.\r\n\r\nFor the sake of clarity, in my above statements I was using the convention [rows, columns] whereas @BlackHC seems to be using the convention [batch size, rows].\r\n\r\nIf it only launches one thread block per batch (=1024 threads on most devices), wouldn't that mean the implementation scales well to large batch sizes and doesn't scale well to large numbers of rows? (using your convention [batch size, rows])."}