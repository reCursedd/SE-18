{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324522464", "html_url": "https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-324522464", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5719", "id": 324522464, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDUyMjQ2NA==", "user": {"login": "ed-alertedh", "id": 24605895, "node_id": "MDQ6VXNlcjI0NjA1ODk1", "avatar_url": "https://avatars1.githubusercontent.com/u/24605895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ed-alertedh", "html_url": "https://github.com/ed-alertedh", "followers_url": "https://api.github.com/users/ed-alertedh/followers", "following_url": "https://api.github.com/users/ed-alertedh/following{/other_user}", "gists_url": "https://api.github.com/users/ed-alertedh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ed-alertedh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ed-alertedh/subscriptions", "organizations_url": "https://api.github.com/users/ed-alertedh/orgs", "repos_url": "https://api.github.com/users/ed-alertedh/repos", "events_url": "https://api.github.com/users/ed-alertedh/events{/privacy}", "received_events_url": "https://api.github.com/users/ed-alertedh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-24T03:25:14Z", "updated_at": "2017-08-24T03:25:14Z", "author_association": "NONE", "body_html": "<p>Ah right, forgot about that detail. So for [20, 10 ** 6] the heap will be used whereas for [10**6, 20] the cub sort will be used.</p>\n<p>Using cub for <code>num_cols&lt;=1000</code> seems intuitive to me - use a heap for larger problem sizes to save the effort of sorting the whole thing. But once k gets large enough it approaches a heapsort, hence why you'd switch to a more performant sorting algo. For non-parallel versions of the algorithms I believe the complexity comparison is O(n + k log(n)) for heap versus O(n log(n)) for sort. Obviously you can't directly apply that comparison to the parallel algorithms, but you'd expect the heap to become faster at some point where  k &lt;&lt; n.</p>\n<p>I think my previous comment still makes sense - the heap implementation currently shards into a maximum of 1024 threads per batch all in one thread block. Each thread builds its own heap on a subset of the data. If the shards were split over multiple blocks you could increase the parallelism. But that would require a parallel merge to gain any benefit and it would be merging between blocks which means you risk getting bottlenecked by global memory. I can see why you'd want to keep it within a single block. If you had large enough num_cols, it would eventually become worth it to use multiple blocks.</p>", "body_text": "Ah right, forgot about that detail. So for [20, 10 ** 6] the heap will be used whereas for [10**6, 20] the cub sort will be used.\nUsing cub for num_cols<=1000 seems intuitive to me - use a heap for larger problem sizes to save the effort of sorting the whole thing. But once k gets large enough it approaches a heapsort, hence why you'd switch to a more performant sorting algo. For non-parallel versions of the algorithms I believe the complexity comparison is O(n + k log(n)) for heap versus O(n log(n)) for sort. Obviously you can't directly apply that comparison to the parallel algorithms, but you'd expect the heap to become faster at some point where  k << n.\nI think my previous comment still makes sense - the heap implementation currently shards into a maximum of 1024 threads per batch all in one thread block. Each thread builds its own heap on a subset of the data. If the shards were split over multiple blocks you could increase the parallelism. But that would require a parallel merge to gain any benefit and it would be merging between blocks which means you risk getting bottlenecked by global memory. I can see why you'd want to keep it within a single block. If you had large enough num_cols, it would eventually become worth it to use multiple blocks.", "body": "Ah right, forgot about that detail. So for [20, 10 ** 6] the heap will be used whereas for [10**6, 20] the cub sort will be used.\r\n\r\nUsing cub for `num_cols<=1000` seems intuitive to me - use a heap for larger problem sizes to save the effort of sorting the whole thing. But once k gets large enough it approaches a heapsort, hence why you'd switch to a more performant sorting algo. For non-parallel versions of the algorithms I believe the complexity comparison is O(n + k log(n)) for heap versus O(n log(n)) for sort. Obviously you can't directly apply that comparison to the parallel algorithms, but you'd expect the heap to become faster at some point where  k << n.\r\n\r\nI think my previous comment still makes sense - the heap implementation currently shards into a maximum of 1024 threads per batch all in one thread block. Each thread builds its own heap on a subset of the data. If the shards were split over multiple blocks you could increase the parallelism. But that would require a parallel merge to gain any benefit and it would be merging between blocks which means you risk getting bottlenecked by global memory. I can see why you'd want to keep it within a single block. If you had large enough num_cols, it would eventually become worth it to use multiple blocks."}