{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322936701", "html_url": "https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-322936701", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5719", "id": 322936701, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjkzNjcwMQ==", "user": {"login": "yliapis", "id": 14884413, "node_id": "MDQ6VXNlcjE0ODg0NDEz", "avatar_url": "https://avatars1.githubusercontent.com/u/14884413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yliapis", "html_url": "https://github.com/yliapis", "followers_url": "https://api.github.com/users/yliapis/followers", "following_url": "https://api.github.com/users/yliapis/following{/other_user}", "gists_url": "https://api.github.com/users/yliapis/gists{/gist_id}", "starred_url": "https://api.github.com/users/yliapis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yliapis/subscriptions", "organizations_url": "https://api.github.com/users/yliapis/orgs", "repos_url": "https://api.github.com/users/yliapis/repos", "events_url": "https://api.github.com/users/yliapis/events{/privacy}", "received_events_url": "https://api.github.com/users/yliapis/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T00:36:04Z", "updated_at": "2017-08-17T00:36:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24605895\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ed-alertedh\">@ed-alertedh</a> The tensors I am using the op on are generally [256, 20] up to [3600, 20]. Note that I am using tf.map_fn to map a tensorflow operator using top_k to a batch (32-128 usually) of matrices, so the top_k function is getting called batch_sz times by map_fn. I haven't been getting any cuda errors, but the performance makes me question whether the gpu is being properly utilized.</p>", "body_text": "@ed-alertedh The tensors I am using the op on are generally [256, 20] up to [3600, 20]. Note that I am using tf.map_fn to map a tensorflow operator using top_k to a batch (32-128 usually) of matrices, so the top_k function is getting called batch_sz times by map_fn. I haven't been getting any cuda errors, but the performance makes me question whether the gpu is being properly utilized.", "body": "@ed-alertedh The tensors I am using the op on are generally [256, 20] up to [3600, 20]. Note that I am using tf.map_fn to map a tensorflow operator using top_k to a batch (32-128 usually) of matrices, so the top_k function is getting called batch_sz times by map_fn. I haven't been getting any cuda errors, but the performance makes me question whether the gpu is being properly utilized."}