{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324512699", "html_url": "https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-324512699", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5719", "id": 324512699, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDUxMjY5OQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-24T02:11:23Z", "updated_at": "2017-08-24T02:11:23Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">There are two different implementations, depending on the input sizes.\nCode is here\n&lt;<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc#L545\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc#L545</a>&gt;.\nWe use cub sort followed by copy of the top k for one range of parameters\nand we use <a class=\"user-mention\" href=\"https://github.com/BlackHC\">@BlackHC</a>'s heap for another range.\n\nThere may actually be a bug where we decide to use cub for num_cols &lt;=\n1000; but perhaps that should have been use cub for num_cols &gt;= 1000\ninstead.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Aug 23, 2017 at 6:03 PM, Edward Bordin ***@***.***&gt; wrote:\n Yeah, I think it is just inherent in how this problem scales to GPU -\n \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it\n doesn't come across that we're criticising your implementation! Just\n figuring out what the scaling behaviour is.\n\n For the sake of clarity, in my above statements I was using the convention\n [rows, columns] whereas <a class=\"user-mention\" href=\"https://github.com/BlackHC\">@BlackHC</a> &lt;<a href=\"https://github.com/blackhc\">https://github.com/blackhc</a>&gt; seems to be\n using the convention [batch size, rows].\n\n If it only launches one thread block per batch (=1024 threads on most\n devices), wouldn't that mean the implementation scales well to large batch\n sizes and doesn't scale well to large numbers of rows? (using your\n convention [batch size, rows]).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"190499357\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5719\" href=\"https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-324503437\">#5719 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim9nNvXbjQect7wv9UN3tBW7qt3A8ks5sbMvMgaJpZM4K3RGK\">https://github.com/notifications/unsubscribe-auth/ABtim9nNvXbjQect7wv9UN3tBW7qt3A8ks5sbMvMgaJpZM4K3RGK</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "There are two different implementations, depending on the input sizes.\nCode is here\n<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc#L545>.\nWe use cub sort followed by copy of the top k for one range of parameters\nand we use @BlackHC's heap for another range.\n\nThere may actually be a bug where we decide to use cub for num_cols <=\n1000; but perhaps that should have been use cub for num_cols >= 1000\ninstead.\n\u2026\nOn Wed, Aug 23, 2017 at 6:03 PM, Edward Bordin ***@***.***> wrote:\n Yeah, I think it is just inherent in how this problem scales to GPU -\n \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it\n doesn't come across that we're criticising your implementation! Just\n figuring out what the scaling behaviour is.\n\n For the sake of clarity, in my above statements I was using the convention\n [rows, columns] whereas @BlackHC <https://github.com/blackhc> seems to be\n using the convention [batch size, rows].\n\n If it only launches one thread block per batch (=1024 threads on most\n devices), wouldn't that mean the implementation scales well to large batch\n sizes and doesn't scale well to large numbers of rows? (using your\n convention [batch size, rows]).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#5719 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim9nNvXbjQect7wv9UN3tBW7qt3A8ks5sbMvMgaJpZM4K3RGK>\n .", "body": "There are two different implementations, depending on the input sizes.\nCode is here\n<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op_gpu.cu.cc#L545>.\nWe use cub sort followed by copy of the top k for one range of parameters\nand we use @BlackHC's heap for another range.\n\nThere may actually be a bug where we decide to use cub for num_cols <=\n1000; but perhaps that should have been use cub for num_cols >= 1000\ninstead.\n\nOn Wed, Aug 23, 2017 at 6:03 PM, Edward Bordin <notifications@github.com>\nwrote:\n\n> Yeah, I think it is just inherent in how this problem scales to GPU -\n> \"small\" inputs don't overcome the fixed costs of kernel launch. I hope it\n> doesn't come across that we're criticising your implementation! Just\n> figuring out what the scaling behaviour is.\n>\n> For the sake of clarity, in my above statements I was using the convention\n> [rows, columns] whereas @BlackHC <https://github.com/blackhc> seems to be\n> using the convention [batch size, rows].\n>\n> If it only launches one thread block per batch (=1024 threads on most\n> devices), wouldn't that mean the implementation scales well to large batch\n> sizes and doesn't scale well to large numbers of rows? (using your\n> convention [batch size, rows]).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-324503437>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9nNvXbjQect7wv9UN3tBW7qt3A8ks5sbMvMgaJpZM4K3RGK>\n> .\n>\n"}