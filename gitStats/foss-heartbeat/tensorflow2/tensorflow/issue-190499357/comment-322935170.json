{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322935170", "html_url": "https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-322935170", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5719", "id": 322935170, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjkzNTE3MA==", "user": {"login": "ed-alertedh", "id": 24605895, "node_id": "MDQ6VXNlcjI0NjA1ODk1", "avatar_url": "https://avatars1.githubusercontent.com/u/24605895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ed-alertedh", "html_url": "https://github.com/ed-alertedh", "followers_url": "https://api.github.com/users/ed-alertedh/followers", "following_url": "https://api.github.com/users/ed-alertedh/following{/other_user}", "gists_url": "https://api.github.com/users/ed-alertedh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ed-alertedh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ed-alertedh/subscriptions", "organizations_url": "https://api.github.com/users/ed-alertedh/orgs", "repos_url": "https://api.github.com/users/ed-alertedh/repos", "events_url": "https://api.github.com/users/ed-alertedh/events{/privacy}", "received_events_url": "https://api.github.com/users/ed-alertedh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T00:24:16Z", "updated_at": "2017-08-17T00:24:16Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=14884413\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yliapis\">@yliapis</a> What shape are the tensors you're running top_k on? They're probably just too small to benefit from the GPU op. I've just been using the CPU op, although in my use-case I've found k=1 is often sufficient so in that case I can just use the max op instead.</p>\n<p>On another note, I tried to run the new GPU op on a tensor of shape [1, 10,000] and got some obscure CUDA error. Haven't had time to track down possible cause or file a bug report yet. I know with thrust I had a lot of fun trying to integrate with TF's memory allocator - as far as I could tell, it doesn't allow kernels to dynamically allocate memory so if CUB is using malloc inside kernels it could just be running out of memory because TF has grabbed it all...</p>", "body_text": "@yliapis What shape are the tensors you're running top_k on? They're probably just too small to benefit from the GPU op. I've just been using the CPU op, although in my use-case I've found k=1 is often sufficient so in that case I can just use the max op instead.\nOn another note, I tried to run the new GPU op on a tensor of shape [1, 10,000] and got some obscure CUDA error. Haven't had time to track down possible cause or file a bug report yet. I know with thrust I had a lot of fun trying to integrate with TF's memory allocator - as far as I could tell, it doesn't allow kernels to dynamically allocate memory so if CUB is using malloc inside kernels it could just be running out of memory because TF has grabbed it all...", "body": "@yliapis What shape are the tensors you're running top_k on? They're probably just too small to benefit from the GPU op. I've just been using the CPU op, although in my use-case I've found k=1 is often sufficient so in that case I can just use the max op instead.\r\n\r\nOn another note, I tried to run the new GPU op on a tensor of shape [1, 10,000] and got some obscure CUDA error. Haven't had time to track down possible cause or file a bug report yet. I know with thrust I had a lot of fun trying to integrate with TF's memory allocator - as far as I could tell, it doesn't allow kernels to dynamically allocate memory so if CUB is using malloc inside kernels it could just be running out of memory because TF has grabbed it all..."}