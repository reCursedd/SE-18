{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322856935", "html_url": "https://github.com/tensorflow/tensorflow/issues/5719#issuecomment-322856935", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5719", "id": 322856935, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjg1NjkzNQ==", "user": {"login": "yliapis", "id": 14884413, "node_id": "MDQ6VXNlcjE0ODg0NDEz", "avatar_url": "https://avatars1.githubusercontent.com/u/14884413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yliapis", "html_url": "https://github.com/yliapis", "followers_url": "https://api.github.com/users/yliapis/followers", "following_url": "https://api.github.com/users/yliapis/following{/other_user}", "gists_url": "https://api.github.com/users/yliapis/gists{/gist_id}", "starred_url": "https://api.github.com/users/yliapis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yliapis/subscriptions", "organizations_url": "https://api.github.com/users/yliapis/orgs", "repos_url": "https://api.github.com/users/yliapis/repos", "events_url": "https://api.github.com/users/yliapis/events{/privacy}", "received_events_url": "https://api.github.com/users/yliapis/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-16T18:20:32Z", "updated_at": "2017-08-16T18:20:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24605895\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ed-alertedh\">@ed-alertedh</a> I tried running the new top_k operator as an intermediate layer on the gpu, and am still getting cpu-like speeds. Any idea what could be going on? I logged the device placement, and it does seem like the operator is still getting placed on the gpu.</p>\n<p>I tested a script with the operator in between two convolutional layers on both 1.3.0-rc2 (gpu) and 1.2.1 (cpu), and the cpu version was faster. Any idea what the bottleneck could be?</p>", "body_text": "@ebrevdo @ed-alertedh I tried running the new top_k operator as an intermediate layer on the gpu, and am still getting cpu-like speeds. Any idea what could be going on? I logged the device placement, and it does seem like the operator is still getting placed on the gpu.\nI tested a script with the operator in between two convolutional layers on both 1.3.0-rc2 (gpu) and 1.2.1 (cpu), and the cpu version was faster. Any idea what the bottleneck could be?", "body": "@ebrevdo @ed-alertedh I tried running the new top_k operator as an intermediate layer on the gpu, and am still getting cpu-like speeds. Any idea what could be going on? I logged the device placement, and it does seem like the operator is still getting placed on the gpu. \r\n\r\nI tested a script with the operator in between two convolutional layers on both 1.3.0-rc2 (gpu) and 1.2.1 (cpu), and the cpu version was faster. Any idea what the bottleneck could be?"}