{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16552", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16552/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16552/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16552/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16552", "id": 292510853, "node_id": "MDU6SXNzdWUyOTI1MTA4NTM=", "number": 16552, "title": "Sampled softmax loss stops gradients on sampled classes", "user": {"login": "markostam", "id": 14855383, "node_id": "MDQ6VXNlcjE0ODU1Mzgz", "avatar_url": "https://avatars2.githubusercontent.com/u/14855383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markostam", "html_url": "https://github.com/markostam", "followers_url": "https://api.github.com/users/markostam/followers", "following_url": "https://api.github.com/users/markostam/following{/other_user}", "gists_url": "https://api.github.com/users/markostam/gists{/gist_id}", "starred_url": "https://api.github.com/users/markostam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markostam/subscriptions", "organizations_url": "https://api.github.com/users/markostam/orgs", "repos_url": "https://api.github.com/users/markostam/repos", "events_url": "https://api.github.com/users/markostam/events{/privacy}", "received_events_url": "https://api.github.com/users/markostam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-01-29T18:24:23Z", "updated_at": "2018-02-13T20:40:48Z", "closed_at": "2018-02-13T20:40:48Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.3</li>\n<li><strong>Python version</strong>:<br>\n2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nCUDA 8 / cuDNN 6</li>\n<li><strong>GPU model and memory</strong>:<br>\n4 x TITAN X (Pascal)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The backbone of TensorFlow's sampled loss functions <code>nce_loss</code> and <code>sampled_softmax_loss</code> is a helper function called <code>_compute_sampled_logits</code><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L961-L1139\">tensorflow/tensorflow/python/ops/nn_impl.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 961 to 1139\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/8fb12848d3a81a010714a4612ffd735106ea83d8\">8fb1284</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L961\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"961\"></td>\n          <td id=\"LC961\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">def</span> <span class=\"pl-en\">_compute_sampled_logits</span>(<span class=\"pl-smi\">weights</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L962\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"962\"></td>\n          <td id=\"LC962\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">biases</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L963\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"963\"></td>\n          <td id=\"LC963\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">labels</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L964\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"964\"></td>\n          <td id=\"LC964\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">inputs</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L965\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"965\"></td>\n          <td id=\"LC965\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">num_sampled</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L966\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"966\"></td>\n          <td id=\"LC966\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">num_classes</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L967\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"967\"></td>\n          <td id=\"LC967\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">num_true</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L968\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"968\"></td>\n          <td id=\"LC968\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">sampled_values</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L969\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"969\"></td>\n          <td id=\"LC969\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">subtract_log_q</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L970\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"970\"></td>\n          <td id=\"LC970\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">remove_accidental_hits</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L971\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"971\"></td>\n          <td id=\"LC971\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">partition_strategy</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mod<span class=\"pl-pds\">\"</span></span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L972\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"972\"></td>\n          <td id=\"LC972\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L973\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"973\"></td>\n          <td id=\"LC973\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                             <span class=\"pl-smi\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L974\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"974\"></td>\n          <td id=\"LC974\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Helper function for nce_loss and sampled_softmax_loss functions.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L975\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"975\"></td>\n          <td id=\"LC975\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L976\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"976\"></td>\n          <td id=\"LC976\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  Computes sampled output training logits and labels suitable for implementing</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L977\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"977\"></td>\n          <td id=\"LC977\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  e.g. noise-contrastive estimation (see nce_loss) or sampled softmax (see</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L978\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"978\"></td>\n          <td id=\"LC978\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  sampled_softmax_loss).</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L979\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"979\"></td>\n          <td id=\"LC979\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L980\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"980\"></td>\n          <td id=\"LC980\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  Note: In the case where num_true &gt; 1, we assign to each target class</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L981\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"981\"></td>\n          <td id=\"LC981\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  the target probability 1 / num_true so that the target probabilities</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L982\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"982\"></td>\n          <td id=\"LC982\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  sum to 1 per-example.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L983\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"983\"></td>\n          <td id=\"LC983\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L984\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"984\"></td>\n          <td id=\"LC984\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  Args:</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L985\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"985\"></td>\n          <td id=\"LC985\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L986\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"986\"></td>\n          <td id=\"LC986\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        objects whose concatenation along dimension 0 has shape</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L987\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"987\"></td>\n          <td id=\"LC987\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        `[num_classes, dim]`.  The (possibly-partitioned) class embeddings.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L988\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"988\"></td>\n          <td id=\"LC988\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    biases: A `Tensor` of shape `[num_classes]`.  The (possibly-partitioned)</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L989\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"989\"></td>\n          <td id=\"LC989\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        class biases.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L990\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"990\"></td>\n          <td id=\"LC990\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    labels: A `Tensor` of type `int64` and shape `[batch_size,</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L991\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"991\"></td>\n          <td id=\"LC991\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        num_true]`. The target classes.  Note that this format differs from</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L992\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"992\"></td>\n          <td id=\"LC992\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        the `labels` argument of `nn.softmax_cross_entropy_with_logits`.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L993\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"993\"></td>\n          <td id=\"LC993\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L994\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"994\"></td>\n          <td id=\"LC994\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        activations of the input network.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L995\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"995\"></td>\n          <td id=\"LC995\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    num_sampled: An `int`.  The number of classes to randomly sample per batch.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L996\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"996\"></td>\n          <td id=\"LC996\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    num_classes: An `int`. The number of possible classes.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L997\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"997\"></td>\n          <td id=\"LC997\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    num_true: An `int`.  The number of target classes per training example.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L998\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"998\"></td>\n          <td id=\"LC998\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`,</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L999\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"999\"></td>\n          <td id=\"LC999\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        `sampled_expected_count`) returned by a `*_candidate_sampler` function.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1000\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1000\"></td>\n          <td id=\"LC1000\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        (if None, we default to `log_uniform_candidate_sampler`)</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1001\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1001\"></td>\n          <td id=\"LC1001\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    subtract_log_q: A `bool`.  whether to subtract the log expected count of</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1002\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1002\"></td>\n          <td id=\"LC1002\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        the labels in the sample to get the logits of the true labels.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1003\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1003\"></td>\n          <td id=\"LC1003\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        Default is True.  Turn off for Negative Sampling.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1004\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1004\"></td>\n          <td id=\"LC1004\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    remove_accidental_hits:  A `bool`.  whether to remove \"accidental hits\"</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1005\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1005\"></td>\n          <td id=\"LC1005\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        where a sampled class equals one of the target classes.  Default is</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1006\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1006\"></td>\n          <td id=\"LC1006\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        False.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1007\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1007\"></td>\n          <td id=\"LC1007\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    partition_strategy: A string specifying the partitioning strategy, relevant</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1008\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1008\"></td>\n          <td id=\"LC1008\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        if `len(weights) &gt; 1`. Currently `\"div\"` and `\"mod\"` are supported.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1009\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1009\"></td>\n          <td id=\"LC1009\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1010\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1010\"></td>\n          <td id=\"LC1010\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    name: A name for the operation (optional).</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1011\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1011\"></td>\n          <td id=\"LC1011\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    seed: random seed for candidate sampling. Default to None, which doesn't set</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1012\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1012\"></td>\n          <td id=\"LC1012\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        the op-level random seed for candidate sampling.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1013\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1013\"></td>\n          <td id=\"LC1013\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  Returns:</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1014\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1014\"></td>\n          <td id=\"LC1014\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    out_logits: `Tensor` object with shape</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1015\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1015\"></td>\n          <td id=\"LC1015\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        `[batch_size, num_true + num_sampled]`, for passing to either</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1016\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1016\"></td>\n          <td id=\"LC1016\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        `nn.sigmoid_cross_entropy_with_logits` (NCE) or</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1017\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1017\"></td>\n          <td id=\"LC1017\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">        `nn.softmax_cross_entropy_with_logits` (sampled softmax).</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1018\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1018\"></td>\n          <td id=\"LC1018\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    out_labels: A Tensor object with the same shape as `out_logits`.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1019\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1019\"></td>\n          <td id=\"LC1019\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1020\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1020\"></td>\n          <td id=\"LC1020\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1021\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1021\"></td>\n          <td id=\"LC1021\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(weights, variables.PartitionedVariable): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1022\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1022\"></td>\n          <td id=\"LC1022\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     weights <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(weights) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1023\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1023\"></td>\n          <td id=\"LC1023\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(weights, <span class=\"pl-c1\">list</span>): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1024\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1024\"></td>\n          <td id=\"LC1024\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     weights <span class=\"pl-k\">=</span> [weights] </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1025\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1025\"></td>\n          <td id=\"LC1025\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1026\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1026\"></td>\n          <td id=\"LC1026\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">with</span> ops.name_scope(name, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>compute_sampled_logits<span class=\"pl-pds\">\"</span></span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1027\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1027\"></td>\n          <td id=\"LC1027\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                       weights <span class=\"pl-k\">+</span> [biases, inputs, labels]): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1028\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1028\"></td>\n          <td id=\"LC1028\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">if</span> labels.dtype <span class=\"pl-k\">!=</span> dtypes.int64: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1029\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1029\"></td>\n          <td id=\"LC1029\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       labels <span class=\"pl-k\">=</span> math_ops.cast(labels, dtypes.int64) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1030\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1030\"></td>\n          <td id=\"LC1030\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     labels_flat <span class=\"pl-k\">=</span> array_ops.reshape(labels, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1031\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1031\"></td>\n          <td id=\"LC1031\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1032\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1032\"></td>\n          <td id=\"LC1032\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> Sample the negative labels.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1033\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1033\"></td>\n          <td id=\"LC1033\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span>   sampled shape: [num_sampled] tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1034\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1034\"></td>\n          <td id=\"LC1034\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span>   true_expected_count shape = [batch_size, 1] tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1035\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1035\"></td>\n          <td id=\"LC1035\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span>   sampled_expected_count shape = [num_sampled] tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1036\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1036\"></td>\n          <td id=\"LC1036\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">if</span> sampled_values <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1037\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1037\"></td>\n          <td id=\"LC1037\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       sampled_values <span class=\"pl-k\">=</span> candidate_sampling_ops.log_uniform_candidate_sampler( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1038\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1038\"></td>\n          <td id=\"LC1038\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">true_classes</span><span class=\"pl-k\">=</span>labels, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1039\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1039\"></td>\n          <td id=\"LC1039\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">num_true</span><span class=\"pl-k\">=</span>num_true, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1040\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1040\"></td>\n          <td id=\"LC1040\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">num_sampled</span><span class=\"pl-k\">=</span>num_sampled, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1041\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1041\"></td>\n          <td id=\"LC1041\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">unique</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1042\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1042\"></td>\n          <td id=\"LC1042\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">range_max</span><span class=\"pl-k\">=</span>num_classes, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1043\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1043\"></td>\n          <td id=\"LC1043\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1044\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1044\"></td>\n          <td id=\"LC1044\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">NOTE</span>: pylint cannot tell that 'sampled_values' is a sequence</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1045\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1045\"></td>\n          <td id=\"LC1045\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=unpacking-non-sequence</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1046\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1046\"></td>\n          <td id=\"LC1046\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     sampled, true_expected_count, sampled_expected_count <span class=\"pl-k\">=</span> ( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1047\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1047\"></td>\n          <td id=\"LC1047\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         array_ops.stop_gradient(s) <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> sampled_values) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1048\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1048\"></td>\n          <td id=\"LC1048\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: enable=unpacking-non-sequence</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1049\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1049\"></td>\n          <td id=\"LC1049\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     sampled <span class=\"pl-k\">=</span> math_ops.cast(sampled, dtypes.int64) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1050\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1050\"></td>\n          <td id=\"LC1050\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1051\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1051\"></td>\n          <td id=\"LC1051\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> labels_flat is a [batch_size * num_true] tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1052\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1052\"></td>\n          <td id=\"LC1052\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> sampled is a [num_sampled] int tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1053\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1053\"></td>\n          <td id=\"LC1053\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     all_ids <span class=\"pl-k\">=</span> array_ops.concat([labels_flat, sampled], <span class=\"pl-c1\">0</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1054\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1054\"></td>\n          <td id=\"LC1054\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1055\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1055\"></td>\n          <td id=\"LC1055\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> Retrieve the true weights and the logits of the sampled weights.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1056\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1056\"></td>\n          <td id=\"LC1056\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1057\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1057\"></td>\n          <td id=\"LC1057\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> weights shape is [num_classes, dim]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1058\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1058\"></td>\n          <td id=\"LC1058\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     all_w <span class=\"pl-k\">=</span> embedding_ops.embedding_lookup( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1059\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1059\"></td>\n          <td id=\"LC1059\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         weights, all_ids, <span class=\"pl-v\">partition_strategy</span><span class=\"pl-k\">=</span>partition_strategy) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1060\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1060\"></td>\n          <td id=\"LC1060\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1061\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1061\"></td>\n          <td id=\"LC1061\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> true_w shape is [batch_size * num_true, dim]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1062\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1062\"></td>\n          <td id=\"LC1062\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     true_w <span class=\"pl-k\">=</span> array_ops.slice(all_w, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1063\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1063\"></td>\n          <td id=\"LC1063\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                              array_ops.stack( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1064\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1064\"></td>\n          <td id=\"LC1064\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                                  [array_ops.shape(labels_flat)[<span class=\"pl-c1\">0</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1065\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1065\"></td>\n          <td id=\"LC1065\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1066\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1066\"></td>\n          <td id=\"LC1066\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     sampled_w <span class=\"pl-k\">=</span> array_ops.slice( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1067\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1067\"></td>\n          <td id=\"LC1067\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         all_w, array_ops.stack([array_ops.shape(labels_flat)[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">0</span>]), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1068\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1068\"></td>\n          <td id=\"LC1068\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> inputs has shape [batch_size, dim]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1069\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1069\"></td>\n          <td id=\"LC1069\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> sampled_w has shape [num_sampled, dim]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1070\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1070\"></td>\n          <td id=\"LC1070\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> Apply X*W', which yields [batch_size, num_sampled]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1071\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1071\"></td>\n          <td id=\"LC1071\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     sampled_logits <span class=\"pl-k\">=</span> math_ops.matmul(inputs, sampled_w, <span class=\"pl-v\">transpose_b</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1072\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1072\"></td>\n          <td id=\"LC1072\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1073\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1073\"></td>\n          <td id=\"LC1073\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> Retrieve the true and sampled biases, compute the true logits, and</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1074\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1074\"></td>\n          <td id=\"LC1074\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> add the biases to the true and sampled logits.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1075\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1075\"></td>\n          <td id=\"LC1075\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     all_b <span class=\"pl-k\">=</span> embedding_ops.embedding_lookup( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1076\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1076\"></td>\n          <td id=\"LC1076\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         biases, all_ids, <span class=\"pl-v\">partition_strategy</span><span class=\"pl-k\">=</span>partition_strategy) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1077\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1077\"></td>\n          <td id=\"LC1077\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> true_b is a [batch_size * num_true] tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1078\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1078\"></td>\n          <td id=\"LC1078\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> sampled_b is a [num_sampled] float tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1079\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1079\"></td>\n          <td id=\"LC1079\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     true_b <span class=\"pl-k\">=</span> array_ops.slice(all_b, [<span class=\"pl-c1\">0</span>], array_ops.shape(labels_flat)) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1080\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1080\"></td>\n          <td id=\"LC1080\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     sampled_b <span class=\"pl-k\">=</span> array_ops.slice(all_b, array_ops.shape(labels_flat), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1081\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1081\"></td>\n          <td id=\"LC1081\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1082\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1082\"></td>\n          <td id=\"LC1082\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> inputs shape is [batch_size, dim]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1083\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1083\"></td>\n          <td id=\"LC1083\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> true_w shape is [batch_size * num_true, dim]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1084\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1084\"></td>\n          <td id=\"LC1084\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> row_wise_dots is [batch_size, num_true, dim]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1085\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1085\"></td>\n          <td id=\"LC1085\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     dim <span class=\"pl-k\">=</span> array_ops.shape(true_w)[<span class=\"pl-c1\">1</span>:<span class=\"pl-c1\">2</span>] </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1086\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1086\"></td>\n          <td id=\"LC1086\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     new_true_w_shape <span class=\"pl-k\">=</span> array_ops.concat([[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, num_true], dim], <span class=\"pl-c1\">0</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1087\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1087\"></td>\n          <td id=\"LC1087\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     row_wise_dots <span class=\"pl-k\">=</span> math_ops.multiply( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1088\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1088\"></td>\n          <td id=\"LC1088\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         array_ops.expand_dims(inputs, <span class=\"pl-c1\">1</span>), </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1089\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1089\"></td>\n          <td id=\"LC1089\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         array_ops.reshape(true_w, new_true_w_shape)) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1090\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1090\"></td>\n          <td id=\"LC1090\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> We want the row-wise dot plus biases which yields a</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1091\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1091\"></td>\n          <td id=\"LC1091\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> [batch_size, num_true] tensor of true_logits.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1092\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1092\"></td>\n          <td id=\"LC1092\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     dots_as_matrix <span class=\"pl-k\">=</span> array_ops.reshape(row_wise_dots, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1093\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1093\"></td>\n          <td id=\"LC1093\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                                        array_ops.concat([[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], dim], <span class=\"pl-c1\">0</span>)) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1094\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1094\"></td>\n          <td id=\"LC1094\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     true_logits <span class=\"pl-k\">=</span> array_ops.reshape(_sum_rows(dots_as_matrix), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, num_true]) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1095\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1095\"></td>\n          <td id=\"LC1095\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     true_b <span class=\"pl-k\">=</span> array_ops.reshape(true_b, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, num_true]) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1096\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1096\"></td>\n          <td id=\"LC1096\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     true_logits <span class=\"pl-k\">+=</span> true_b </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1097\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1097\"></td>\n          <td id=\"LC1097\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     sampled_logits <span class=\"pl-k\">+=</span> sampled_b </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1098\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1098\"></td>\n          <td id=\"LC1098\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1099\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1099\"></td>\n          <td id=\"LC1099\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">if</span> remove_accidental_hits: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1100\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1100\"></td>\n          <td id=\"LC1100\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       acc_hits <span class=\"pl-k\">=</span> candidate_sampling_ops.compute_accidental_hits( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1101\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1101\"></td>\n          <td id=\"LC1101\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           labels, sampled, <span class=\"pl-v\">num_true</span><span class=\"pl-k\">=</span>num_true) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1102\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1102\"></td>\n          <td id=\"LC1102\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       acc_indices, acc_ids, acc_weights <span class=\"pl-k\">=</span> acc_hits </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1103\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1103\"></td>\n          <td id=\"LC1103\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1104\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1104\"></td>\n          <td id=\"LC1104\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       <span class=\"pl-c\"><span class=\"pl-c\">#</span> This is how SparseToDense expects the indices.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1105\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1105\"></td>\n          <td id=\"LC1105\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       acc_indices_2d <span class=\"pl-k\">=</span> array_ops.reshape(acc_indices, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1106\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1106\"></td>\n          <td id=\"LC1106\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       acc_ids_2d_int32 <span class=\"pl-k\">=</span> array_ops.reshape( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1107\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1107\"></td>\n          <td id=\"LC1107\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           math_ops.cast(acc_ids, dtypes.int32), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1108\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1108\"></td>\n          <td id=\"LC1108\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       sparse_indices <span class=\"pl-k\">=</span> array_ops.concat([acc_indices_2d, acc_ids_2d_int32], <span class=\"pl-c1\">1</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1109\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1109\"></td>\n          <td id=\"LC1109\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                                         <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sparse_indices<span class=\"pl-pds\">\"</span></span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1110\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1110\"></td>\n          <td id=\"LC1110\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create sampled_logits_shape = [batch_size, num_sampled]</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1111\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1111\"></td>\n          <td id=\"LC1111\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       sampled_logits_shape <span class=\"pl-k\">=</span> array_ops.concat( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1112\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1112\"></td>\n          <td id=\"LC1112\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           [array_ops.shape(labels)[:<span class=\"pl-c1\">1</span>], </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1113\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1113\"></td>\n          <td id=\"LC1113\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">            array_ops.expand_dims(num_sampled, <span class=\"pl-c1\">0</span>)], <span class=\"pl-c1\">0</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1114\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1114\"></td>\n          <td id=\"LC1114\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       <span class=\"pl-k\">if</span> sampled_logits.dtype <span class=\"pl-k\">!=</span> acc_weights.dtype: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1115\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1115\"></td>\n          <td id=\"LC1115\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         acc_weights <span class=\"pl-k\">=</span> math_ops.cast(acc_weights, sampled_logits.dtype) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1116\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1116\"></td>\n          <td id=\"LC1116\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       sampled_logits <span class=\"pl-k\">+=</span> sparse_ops.sparse_to_dense( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1117\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1117\"></td>\n          <td id=\"LC1117\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           sparse_indices, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1118\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1118\"></td>\n          <td id=\"LC1118\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           sampled_logits_shape, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1119\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1119\"></td>\n          <td id=\"LC1119\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           acc_weights, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1120\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1120\"></td>\n          <td id=\"LC1120\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1121\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1121\"></td>\n          <td id=\"LC1121\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">validate_indices</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1122\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1122\"></td>\n          <td id=\"LC1122\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1123\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1123\"></td>\n          <td id=\"LC1123\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">if</span> subtract_log_q: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1124\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1124\"></td>\n          <td id=\"LC1124\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       <span class=\"pl-c\"><span class=\"pl-c\">#</span> Subtract log of Q(l), prior probability that l appears in sampled.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1125\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1125\"></td>\n          <td id=\"LC1125\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       true_logits <span class=\"pl-k\">-=</span> math_ops.log(true_expected_count) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1126\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1126\"></td>\n          <td id=\"LC1126\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       sampled_logits <span class=\"pl-k\">-=</span> math_ops.log(sampled_expected_count) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1127\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1127\"></td>\n          <td id=\"LC1127\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1128\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1128\"></td>\n          <td id=\"LC1128\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> Construct output logits and labels. The true labels/logits start at col 0.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1129\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1129\"></td>\n          <td id=\"LC1129\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     out_logits <span class=\"pl-k\">=</span> array_ops.concat([true_logits, sampled_logits], <span class=\"pl-c1\">1</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1130\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1130\"></td>\n          <td id=\"LC1130\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1131\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1131\"></td>\n          <td id=\"LC1131\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> true_logits is a float tensor, ones_like(true_logits) is a float</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1132\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1132\"></td>\n          <td id=\"LC1132\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor of ones. We then divide by num_true to ensure the per-example</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1133\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1133\"></td>\n          <td id=\"LC1133\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">#</span> labels sum to 1.0, i.e. form a proper probability distribution.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1134\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1134\"></td>\n          <td id=\"LC1134\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     out_labels <span class=\"pl-k\">=</span> array_ops.concat([ </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1135\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1135\"></td>\n          <td id=\"LC1135\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         array_ops.ones_like(true_logits) <span class=\"pl-k\">/</span> num_true, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1136\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1136\"></td>\n          <td id=\"LC1136\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         array_ops.zeros_like(sampled_logits) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1137\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1137\"></td>\n          <td id=\"LC1137\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     ], <span class=\"pl-c1\">1</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1138\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1138\"></td>\n          <td id=\"LC1138\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1139\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1139\"></td>\n          <td id=\"LC1139\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">return</span> out_logits, out_labels </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n.</p>\n<p><code>_compute_sampled_logits</code> takes as input:</p>\n<ul>\n<li>weights and biases of the final layer,</li>\n<li>the output labels</li>\n<li>the inputs to the final layer inputs</li>\n<li>the sampled values of the output layer</li>\n<li>a few other things</li>\n</ul>\n<p>and returns the logits and labels of only the requested sampled labels.</p>\n<p>One of the first ops executed is <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L1046-L1047\">tensorflow/tensorflow/python/ops/nn_impl.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 1046 to 1047\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/8fb12848d3a81a010714a4612ffd735106ea83d8\">8fb1284</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L1046\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1046\"></td>\n          <td id=\"LC1046\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> sampled, true_expected_count, sampled_expected_count <span class=\"pl-k\">=</span> ( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1047\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1047\"></td>\n          <td id=\"LC1047\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     array_ops.stop_gradient(s) <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> sampled_values) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>This line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.</p>\n<p>Shouldn't the gradients be stopped from flowing back through the <em>non-sampled values</em> as opposed to the <em>sampled values</em>? Why are gradients being stopped at the sampled values?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 16.04\nTensorFlow installed from (source or binary):\nBinary\nTensorFlow version (use command below):\n1.3\nPython version:\n2.7\nBazel version (if compiling from source):\nN/A\nGCC/Compiler version (if compiling from source):\nN/A\nCUDA/cuDNN version:\nCUDA 8 / cuDNN 6\nGPU model and memory:\n4 x TITAN X (Pascal)\n\nDescribe the problem\nThe backbone of TensorFlow's sampled loss functions nce_loss and sampled_softmax_loss is a helper function called _compute_sampled_logits\n  \n    \n      tensorflow/tensorflow/python/ops/nn_impl.py\n    \n    \n        Lines 961 to 1139\n      in\n      8fb1284\n    \n    \n    \n    \n\n        \n          \n           def _compute_sampled_logits(weights, \n        \n\n        \n          \n                                       biases, \n        \n\n        \n          \n                                       labels, \n        \n\n        \n          \n                                       inputs, \n        \n\n        \n          \n                                       num_sampled, \n        \n\n        \n          \n                                       num_classes, \n        \n\n        \n          \n                                       num_true=1, \n        \n\n        \n          \n                                       sampled_values=None, \n        \n\n        \n          \n                                       subtract_log_q=True, \n        \n\n        \n          \n                                       remove_accidental_hits=False, \n        \n\n        \n          \n                                       partition_strategy=\"mod\", \n        \n\n        \n          \n                                       name=None, \n        \n\n        \n          \n                                       seed=None): \n        \n\n        \n          \n             \"\"\"Helper function for nce_loss and sampled_softmax_loss functions. \n        \n\n        \n          \n            \n        \n\n        \n          \n             Computes sampled output training logits and labels suitable for implementing \n        \n\n        \n          \n             e.g. noise-contrastive estimation (see nce_loss) or sampled softmax (see \n        \n\n        \n          \n             sampled_softmax_loss). \n        \n\n        \n          \n            \n        \n\n        \n          \n             Note: In the case where num_true > 1, we assign to each target class \n        \n\n        \n          \n             the target probability 1 / num_true so that the target probabilities \n        \n\n        \n          \n             sum to 1 per-example. \n        \n\n        \n          \n            \n        \n\n        \n          \n             Args: \n        \n\n        \n          \n               weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor` \n        \n\n        \n          \n                   objects whose concatenation along dimension 0 has shape \n        \n\n        \n          \n                   `[num_classes, dim]`.  The (possibly-partitioned) class embeddings. \n        \n\n        \n          \n               biases: A `Tensor` of shape `[num_classes]`.  The (possibly-partitioned) \n        \n\n        \n          \n                   class biases. \n        \n\n        \n          \n               labels: A `Tensor` of type `int64` and shape `[batch_size, \n        \n\n        \n          \n                   num_true]`. The target classes.  Note that this format differs from \n        \n\n        \n          \n                   the `labels` argument of `nn.softmax_cross_entropy_with_logits`. \n        \n\n        \n          \n               inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward \n        \n\n        \n          \n                   activations of the input network. \n        \n\n        \n          \n               num_sampled: An `int`.  The number of classes to randomly sample per batch. \n        \n\n        \n          \n               num_classes: An `int`. The number of possible classes. \n        \n\n        \n          \n               num_true: An `int`.  The number of target classes per training example. \n        \n\n        \n          \n               sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`, \n        \n\n        \n          \n                   `sampled_expected_count`) returned by a `*_candidate_sampler` function. \n        \n\n        \n          \n                   (if None, we default to `log_uniform_candidate_sampler`) \n        \n\n        \n          \n               subtract_log_q: A `bool`.  whether to subtract the log expected count of \n        \n\n        \n          \n                   the labels in the sample to get the logits of the true labels. \n        \n\n        \n          \n                   Default is True.  Turn off for Negative Sampling. \n        \n\n        \n          \n               remove_accidental_hits:  A `bool`.  whether to remove \"accidental hits\" \n        \n\n        \n          \n                   where a sampled class equals one of the target classes.  Default is \n        \n\n        \n          \n                   False. \n        \n\n        \n          \n               partition_strategy: A string specifying the partitioning strategy, relevant \n        \n\n        \n          \n                   if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported. \n        \n\n        \n          \n                   Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details. \n        \n\n        \n          \n               name: A name for the operation (optional). \n        \n\n        \n          \n               seed: random seed for candidate sampling. Default to None, which doesn't set \n        \n\n        \n          \n                   the op-level random seed for candidate sampling. \n        \n\n        \n          \n             Returns: \n        \n\n        \n          \n               out_logits: `Tensor` object with shape \n        \n\n        \n          \n                   `[batch_size, num_true + num_sampled]`, for passing to either \n        \n\n        \n          \n                   `nn.sigmoid_cross_entropy_with_logits` (NCE) or \n        \n\n        \n          \n                   `nn.softmax_cross_entropy_with_logits` (sampled softmax). \n        \n\n        \n          \n               out_labels: A Tensor object with the same shape as `out_logits`. \n        \n\n        \n          \n             \"\"\" \n        \n\n        \n          \n            \n        \n\n        \n          \n             if isinstance(weights, variables.PartitionedVariable): \n        \n\n        \n          \n               weights = list(weights) \n        \n\n        \n          \n             if not isinstance(weights, list): \n        \n\n        \n          \n               weights = [weights] \n        \n\n        \n          \n            \n        \n\n        \n          \n             with ops.name_scope(name, \"compute_sampled_logits\", \n        \n\n        \n          \n                                 weights + [biases, inputs, labels]): \n        \n\n        \n          \n               if labels.dtype != dtypes.int64: \n        \n\n        \n          \n                 labels = math_ops.cast(labels, dtypes.int64) \n        \n\n        \n          \n               labels_flat = array_ops.reshape(labels, [-1]) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # Sample the negative labels. \n        \n\n        \n          \n               #   sampled shape: [num_sampled] tensor \n        \n\n        \n          \n               #   true_expected_count shape = [batch_size, 1] tensor \n        \n\n        \n          \n               #   sampled_expected_count shape = [num_sampled] tensor \n        \n\n        \n          \n               if sampled_values is None: \n        \n\n        \n          \n                 sampled_values = candidate_sampling_ops.log_uniform_candidate_sampler( \n        \n\n        \n          \n                     true_classes=labels, \n        \n\n        \n          \n                     num_true=num_true, \n        \n\n        \n          \n                     num_sampled=num_sampled, \n        \n\n        \n          \n                     unique=True, \n        \n\n        \n          \n                     range_max=num_classes, \n        \n\n        \n          \n                     seed=seed) \n        \n\n        \n          \n               # NOTE: pylint cannot tell that 'sampled_values' is a sequence \n        \n\n        \n          \n               # pylint: disable=unpacking-non-sequence \n        \n\n        \n          \n               sampled, true_expected_count, sampled_expected_count = ( \n        \n\n        \n          \n                   array_ops.stop_gradient(s) for s in sampled_values) \n        \n\n        \n          \n               # pylint: enable=unpacking-non-sequence \n        \n\n        \n          \n               sampled = math_ops.cast(sampled, dtypes.int64) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # labels_flat is a [batch_size * num_true] tensor \n        \n\n        \n          \n               # sampled is a [num_sampled] int tensor \n        \n\n        \n          \n               all_ids = array_ops.concat([labels_flat, sampled], 0) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # Retrieve the true weights and the logits of the sampled weights. \n        \n\n        \n          \n            \n        \n\n        \n          \n               # weights shape is [num_classes, dim] \n        \n\n        \n          \n               all_w = embedding_ops.embedding_lookup( \n        \n\n        \n          \n                   weights, all_ids, partition_strategy=partition_strategy) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # true_w shape is [batch_size * num_true, dim] \n        \n\n        \n          \n               true_w = array_ops.slice(all_w, [0, 0], \n        \n\n        \n          \n                                        array_ops.stack( \n        \n\n        \n          \n                                            [array_ops.shape(labels_flat)[0], -1])) \n        \n\n        \n          \n            \n        \n\n        \n          \n               sampled_w = array_ops.slice( \n        \n\n        \n          \n                   all_w, array_ops.stack([array_ops.shape(labels_flat)[0], 0]), [-1, -1]) \n        \n\n        \n          \n               # inputs has shape [batch_size, dim] \n        \n\n        \n          \n               # sampled_w has shape [num_sampled, dim] \n        \n\n        \n          \n               # Apply X*W', which yields [batch_size, num_sampled] \n        \n\n        \n          \n               sampled_logits = math_ops.matmul(inputs, sampled_w, transpose_b=True) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # Retrieve the true and sampled biases, compute the true logits, and \n        \n\n        \n          \n               # add the biases to the true and sampled logits. \n        \n\n        \n          \n               all_b = embedding_ops.embedding_lookup( \n        \n\n        \n          \n                   biases, all_ids, partition_strategy=partition_strategy) \n        \n\n        \n          \n               # true_b is a [batch_size * num_true] tensor \n        \n\n        \n          \n               # sampled_b is a [num_sampled] float tensor \n        \n\n        \n          \n               true_b = array_ops.slice(all_b, [0], array_ops.shape(labels_flat)) \n        \n\n        \n          \n               sampled_b = array_ops.slice(all_b, array_ops.shape(labels_flat), [-1]) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # inputs shape is [batch_size, dim] \n        \n\n        \n          \n               # true_w shape is [batch_size * num_true, dim] \n        \n\n        \n          \n               # row_wise_dots is [batch_size, num_true, dim] \n        \n\n        \n          \n               dim = array_ops.shape(true_w)[1:2] \n        \n\n        \n          \n               new_true_w_shape = array_ops.concat([[-1, num_true], dim], 0) \n        \n\n        \n          \n               row_wise_dots = math_ops.multiply( \n        \n\n        \n          \n                   array_ops.expand_dims(inputs, 1), \n        \n\n        \n          \n                   array_ops.reshape(true_w, new_true_w_shape)) \n        \n\n        \n          \n               # We want the row-wise dot plus biases which yields a \n        \n\n        \n          \n               # [batch_size, num_true] tensor of true_logits. \n        \n\n        \n          \n               dots_as_matrix = array_ops.reshape(row_wise_dots, \n        \n\n        \n          \n                                                  array_ops.concat([[-1], dim], 0)) \n        \n\n        \n          \n               true_logits = array_ops.reshape(_sum_rows(dots_as_matrix), [-1, num_true]) \n        \n\n        \n          \n               true_b = array_ops.reshape(true_b, [-1, num_true]) \n        \n\n        \n          \n               true_logits += true_b \n        \n\n        \n          \n               sampled_logits += sampled_b \n        \n\n        \n          \n            \n        \n\n        \n          \n               if remove_accidental_hits: \n        \n\n        \n          \n                 acc_hits = candidate_sampling_ops.compute_accidental_hits( \n        \n\n        \n          \n                     labels, sampled, num_true=num_true) \n        \n\n        \n          \n                 acc_indices, acc_ids, acc_weights = acc_hits \n        \n\n        \n          \n            \n        \n\n        \n          \n                 # This is how SparseToDense expects the indices. \n        \n\n        \n          \n                 acc_indices_2d = array_ops.reshape(acc_indices, [-1, 1]) \n        \n\n        \n          \n                 acc_ids_2d_int32 = array_ops.reshape( \n        \n\n        \n          \n                     math_ops.cast(acc_ids, dtypes.int32), [-1, 1]) \n        \n\n        \n          \n                 sparse_indices = array_ops.concat([acc_indices_2d, acc_ids_2d_int32], 1, \n        \n\n        \n          \n                                                   \"sparse_indices\") \n        \n\n        \n          \n                 # Create sampled_logits_shape = [batch_size, num_sampled] \n        \n\n        \n          \n                 sampled_logits_shape = array_ops.concat( \n        \n\n        \n          \n                     [array_ops.shape(labels)[:1], \n        \n\n        \n          \n                      array_ops.expand_dims(num_sampled, 0)], 0) \n        \n\n        \n          \n                 if sampled_logits.dtype != acc_weights.dtype: \n        \n\n        \n          \n                   acc_weights = math_ops.cast(acc_weights, sampled_logits.dtype) \n        \n\n        \n          \n                 sampled_logits += sparse_ops.sparse_to_dense( \n        \n\n        \n          \n                     sparse_indices, \n        \n\n        \n          \n                     sampled_logits_shape, \n        \n\n        \n          \n                     acc_weights, \n        \n\n        \n          \n                     default_value=0.0, \n        \n\n        \n          \n                     validate_indices=False) \n        \n\n        \n          \n            \n        \n\n        \n          \n               if subtract_log_q: \n        \n\n        \n          \n                 # Subtract log of Q(l), prior probability that l appears in sampled. \n        \n\n        \n          \n                 true_logits -= math_ops.log(true_expected_count) \n        \n\n        \n          \n                 sampled_logits -= math_ops.log(sampled_expected_count) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # Construct output logits and labels. The true labels/logits start at col 0. \n        \n\n        \n          \n               out_logits = array_ops.concat([true_logits, sampled_logits], 1) \n        \n\n        \n          \n            \n        \n\n        \n          \n               # true_logits is a float tensor, ones_like(true_logits) is a float \n        \n\n        \n          \n               # tensor of ones. We then divide by num_true to ensure the per-example \n        \n\n        \n          \n               # labels sum to 1.0, i.e. form a proper probability distribution. \n        \n\n        \n          \n               out_labels = array_ops.concat([ \n        \n\n        \n          \n                   array_ops.ones_like(true_logits) / num_true, \n        \n\n        \n          \n                   array_ops.zeros_like(sampled_logits) \n        \n\n        \n          \n               ], 1) \n        \n\n        \n          \n            \n        \n\n        \n          \n               return out_logits, out_labels \n        \n    \n  \n\n.\n_compute_sampled_logits takes as input:\n\nweights and biases of the final layer,\nthe output labels\nthe inputs to the final layer inputs\nthe sampled values of the output layer\na few other things\n\nand returns the logits and labels of only the requested sampled labels.\nOne of the first ops executed is \n  \n    \n      tensorflow/tensorflow/python/ops/nn_impl.py\n    \n    \n        Lines 1046 to 1047\n      in\n      8fb1284\n    \n    \n    \n    \n\n        \n          \n           sampled, true_expected_count, sampled_expected_count = ( \n        \n\n        \n          \n               array_ops.stop_gradient(s) for s in sampled_values) \n        \n    \n  \n\n\nThis line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.\nShouldn't the gradients be stopped from flowing back through the non-sampled values as opposed to the sampled values? Why are gradients being stopped at the sampled values?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 8 / cuDNN 6\r\n- **GPU model and memory**:\r\n4 x TITAN X (Pascal)\r\n\r\n### Describe the problem\r\n\r\nThe backbone of TensorFlow's sampled loss functions `nce_loss` and `sampled_softmax_loss` is a helper function called `_compute_sampled_logits`https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L961-L1139.\r\n\r\n`_compute_sampled_logits` takes as input:\r\n\r\n- weights and biases of the final layer,\r\n- the output labels\r\n- the inputs to the final layer inputs\r\n- the sampled values of the output layer\r\n- a few other things\r\n\r\nand returns the logits and labels of only the requested sampled labels.\r\n\r\nOne of the first ops executed is https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L1046-L1047 \r\n\r\nThis line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.\r\n\r\nShouldn't the gradients be stopped from flowing back through the _non-sampled values_ as opposed to the _sampled values_? Why are gradients being stopped at the sampled values? \r\n"}