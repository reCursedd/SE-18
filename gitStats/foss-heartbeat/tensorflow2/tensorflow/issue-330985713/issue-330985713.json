{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19896", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19896/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19896/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19896/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19896", "id": 330985713, "node_id": "MDU6SXNzdWUzMzA5ODU3MTM=", "number": 19896, "title": "How to Feed Batched Sequences of Images through Tensorflow conv2d", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-06-10T17:11:31Z", "updated_at": "2018-06-11T06:39:36Z", "closed_at": "2018-06-11T06:16:20Z", "author_association": "NONE", "body_html": "<p>This seems like a trivial question, but I've been unable to find the answer. Maybe I'm just bad at Googling, but I think the question is sufficiently common to justify an easily accessible answer.</p>\n<p>I have batched sequences of images of shape:</p>\n<p><code>[batch_size, number_of_frames, frame_height, frame_width, number_of_channels]</code></p>\n<p>and I would like to pass each frame through a few convolutional and pooling layers. However, <code>tf.layers.conv2d</code> accepts 4D inputs of shape:</p>\n<p><code>[batch_size, frame_height, frame_width, number_of_channels]</code></p>\n<p>My first attempt was to use tf.map_fn over axis=1, but I discovered that <a href=\"https://github.com/tensorflow/tensorflow/issues/3972\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3972/hovercard\">this function does not propagate gradients</a> - which isn't mentioned in the <a href=\"https://www.tensorflow.org/api_docs/python/tf/map_fn\" rel=\"nofollow\">documentation</a> (but should be!).</p>\n<p>My second attempt was to use <code>tf.unstack</code> over the first dimension and then use <code>tf.while_loop</code>. However, my batch_size and number_of_frames are dynamically determined (i.e. both are None), and <code>tf.unstack</code> raises <code>{ValueError} Cannot infer num from shape (?, ?, 30, 30, 3) if num is unspecified</code>. I tried specifying <code>num=tf.shape(observations)[1]</code>, but this raises <code>{TypeError} Expected int for argument 'num' not &lt;tf.Tensor 'A2C/infer/strided_slice:0' shape=() dtype=int32&gt;</code>.</p>\n<p>What is the proper way to feed batched sequences of images through <code>conv2d</code>?</p>\n<h3>System information</h3>\n<p>OS Platform and Distribution: Ubuntu 16.04.4 LTS<br>\nTensorFlow installed from binary.<br>\nTensorFlow version: 'v1.8.0-2159-gea9fb80'<br>\nPython version: 2.7.12</p>", "body_text": "This seems like a trivial question, but I've been unable to find the answer. Maybe I'm just bad at Googling, but I think the question is sufficiently common to justify an easily accessible answer.\nI have batched sequences of images of shape:\n[batch_size, number_of_frames, frame_height, frame_width, number_of_channels]\nand I would like to pass each frame through a few convolutional and pooling layers. However, tf.layers.conv2d accepts 4D inputs of shape:\n[batch_size, frame_height, frame_width, number_of_channels]\nMy first attempt was to use tf.map_fn over axis=1, but I discovered that this function does not propagate gradients - which isn't mentioned in the documentation (but should be!).\nMy second attempt was to use tf.unstack over the first dimension and then use tf.while_loop. However, my batch_size and number_of_frames are dynamically determined (i.e. both are None), and tf.unstack raises {ValueError} Cannot infer num from shape (?, ?, 30, 30, 3) if num is unspecified. I tried specifying num=tf.shape(observations)[1], but this raises {TypeError} Expected int for argument 'num' not <tf.Tensor 'A2C/infer/strided_slice:0' shape=() dtype=int32>.\nWhat is the proper way to feed batched sequences of images through conv2d?\nSystem information\nOS Platform and Distribution: Ubuntu 16.04.4 LTS\nTensorFlow installed from binary.\nTensorFlow version: 'v1.8.0-2159-gea9fb80'\nPython version: 2.7.12", "body": "This seems like a trivial question, but I've been unable to find the answer. Maybe I'm just bad at Googling, but I think the question is sufficiently common to justify an easily accessible answer.\r\n\r\nI have batched sequences of images of shape:\r\n\r\n`[batch_size, number_of_frames, frame_height, frame_width, number_of_channels]`\r\n\r\nand I would like to pass each frame through a few convolutional and pooling layers. However, `tf.layers.conv2d` accepts 4D inputs of shape:\r\n\r\n`[batch_size, frame_height, frame_width, number_of_channels]`\r\n\r\nMy first attempt was to use tf.map_fn over axis=1, but I discovered that [this function does not propagate gradients](https://github.com/tensorflow/tensorflow/issues/3972) - which isn't mentioned in the [documentation](https://www.tensorflow.org/api_docs/python/tf/map_fn) (but should be!).\r\n\r\nMy second attempt was to use `tf.unstack` over the first dimension and then use `tf.while_loop`. However, my batch_size and number_of_frames are dynamically determined (i.e. both are None), and `tf.unstack` raises `{ValueError} Cannot infer num from shape (?, ?, 30, 30, 3) if num is unspecified`. I tried specifying `num=tf.shape(observations)[1]`, but this raises `{TypeError} Expected int for argument 'num' not <tf.Tensor 'A2C/infer/strided_slice:0' shape=() dtype=int32>`.\r\n\r\nWhat is the proper way to feed batched sequences of images through `conv2d`?\r\n\r\n### System information\r\nOS Platform and Distribution: Ubuntu 16.04.4 LTS\r\nTensorFlow installed from binary.\r\nTensorFlow version: 'v1.8.0-2159-gea9fb80'\r\nPython version: 2.7.12\r\n"}