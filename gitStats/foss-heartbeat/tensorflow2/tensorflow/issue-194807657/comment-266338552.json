{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266338552", "html_url": "https://github.com/tensorflow/tensorflow/issues/6247#issuecomment-266338552", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6247", "id": 266338552, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjMzODU1Mg==", "user": {"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-12T04:01:37Z", "updated_at": "2016-12-12T04:01:37Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">TF runtime does not allocate gpu memory per op. Instead, it has it's own\nallocator, which grabs a big chunk (the pool) from cuda and never returns\nto cuda. All tensor objects are allocated within the pool.\n\nCombined w/ single compute stream configuration, one can prove the memory\nsafety in a similar way as people prove multi-threaded program's memory\nsafety properties, albeit much simpler for a single gpu cuda w/ only one\ncopy-in stream and one copy-out stream.\n\nConsider the following 3 operations in sequence:\n   x = add(a, b)\n   y = add(x, c)\n   z = add(y, d)\nassuming, a, b, c, d, x, y, and z are of the same shape, and x and y are\nnot used before the 3rd operation.\n\nIf we, TF runtime, know that they are run on the same gpu device and hence\none single gpu stream, the following are relevant events on cpu and gpu\nrespectively:\ncpu:\n  allocate x\n  dispatch kernel for x = add(a, b)\n  allocate y\n  dispatch kernel for y = add(x, c)\n  deallocate x\n  allocate z\n  dispatch kernel for z = add(y, d)\n  deallocate y\n\ngpu:\n  execute kernel for x = add(a, b)\n  execute kernel for y = add(x, b)\n  execute kernel for z = add(y, b)\n\nThe TF data flow graph guarantees that the events on cpu above happens one\nafter another. The single cuda stream semantics guarantees events on gpu\nabove happens one after another. dispatch kernel and execute kernel has the\nstrict happen-after relation. If you draw the happen-after edges of these\nevents, you can see their correctness.\n\nMore importantly, this allows us the TF runtime to reuse the memory release\nby \"deallocate x\" immediately to \"allocate z\".\n\n\n\"\"\"\nI know that all kernels launched on a stream are serialized. For a typical\nop's kernel's Compute method, the mode is as follows I think: first it gets\nthe input tensor of the kernel, then allocates the out tensor memory on gpu\nand at last launches the kernel (e.g. cuda) and after all the above things\ndone, the Compute() method returns. So the next Op's kernel's Compute\nmethod may be called at once. Tensor's TensorBuffer stores the pointer to\nthe location of the real tensor bytes and\n\"\"\"\nUp to this point, your understanding matches what TF runtime does.\n\n\"\"\"it can be remained in gpu if there is an tensor object or an\nTensorReference object remains in the program.\nHowever once a op's kernel is done, it's input will be cleaned and I\nhaven't see any Tensor or TensorReference object alive in the program since\nthe device's RequiresRecordingAccessedTensors() method returns false. So\nhow can this tensor's byte remains in gpu? In detail, does allocating\nmemory on gpu is queued on the stream that executes the kernel?\"\"\"\n\nThis section of your description doesn't chime well with TF runtime\nimplementation. Maybe the point that TF runtime has its own memory pool can\nhelp you understanding, I hope. Without such a pool, yes, you need to queue\nthe cuda allocation and deallocation calls on the appropriate stream and/or\nadds proper waiting events, which we think is a rather complex scheme to\nengineer when differently cuda libraries are used concurrently.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sun, Dec 11, 2016 at 5:41 PM cuiguoxin ***@***.***&gt; wrote:\n hi, <a class=\"user-mention\" href=\"https://github.com/michaelisard\">@michaelisard</a> &lt;<a href=\"https://github.com/michaelisard\">https://github.com/michaelisard</a>&gt; . I know that all\n kernels launched on a stream are serialized. For a typical op's kernel's\n Compute method, the mode is as follows I think: first it gets the input\n tensor of the kernel, then allocates the out tensor memory on gpu and at\n last launches the kernel (e.g. cuda) and after all the above things done,\n the Compute() method returns. So the next Op's kernel's Compute method may\n be called at once. Tensor's TensorBuffer stores the pointer to the location\n of the real tensor bytes and it can be remained in gpu if there is an\n tensor object or an TensorReference object remains in the program. However\n once a op's kernel is done, it's input will be cleaned and I haven't see\n any Tensor or TensorReference object alive in the program since the\n device's RequiresRecordingAccessedTensors() method returns false. So how\n can this tensor's byte remains in gpu? In detail, does allocating memory on\n gpu is queued on the stream that executes the kernel?\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"194807657\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6247\" href=\"https://github.com/tensorflow/tensorflow/issues/6247#issuecomment-266325602\">#6247 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AHk2biViME88SP5hGC-YxcD1vQ-xwj9fks5rHKZhgaJpZM4LJ0V4\">https://github.com/notifications/unsubscribe-auth/AHk2biViME88SP5hGC-YxcD1vQ-xwj9fks5rHKZhgaJpZM4LJ0V4</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "TF runtime does not allocate gpu memory per op. Instead, it has it's own\nallocator, which grabs a big chunk (the pool) from cuda and never returns\nto cuda. All tensor objects are allocated within the pool.\n\nCombined w/ single compute stream configuration, one can prove the memory\nsafety in a similar way as people prove multi-threaded program's memory\nsafety properties, albeit much simpler for a single gpu cuda w/ only one\ncopy-in stream and one copy-out stream.\n\nConsider the following 3 operations in sequence:\n   x = add(a, b)\n   y = add(x, c)\n   z = add(y, d)\nassuming, a, b, c, d, x, y, and z are of the same shape, and x and y are\nnot used before the 3rd operation.\n\nIf we, TF runtime, know that they are run on the same gpu device and hence\none single gpu stream, the following are relevant events on cpu and gpu\nrespectively:\ncpu:\n  allocate x\n  dispatch kernel for x = add(a, b)\n  allocate y\n  dispatch kernel for y = add(x, c)\n  deallocate x\n  allocate z\n  dispatch kernel for z = add(y, d)\n  deallocate y\n\ngpu:\n  execute kernel for x = add(a, b)\n  execute kernel for y = add(x, b)\n  execute kernel for z = add(y, b)\n\nThe TF data flow graph guarantees that the events on cpu above happens one\nafter another. The single cuda stream semantics guarantees events on gpu\nabove happens one after another. dispatch kernel and execute kernel has the\nstrict happen-after relation. If you draw the happen-after edges of these\nevents, you can see their correctness.\n\nMore importantly, this allows us the TF runtime to reuse the memory release\nby \"deallocate x\" immediately to \"allocate z\".\n\n\n\"\"\"\nI know that all kernels launched on a stream are serialized. For a typical\nop's kernel's Compute method, the mode is as follows I think: first it gets\nthe input tensor of the kernel, then allocates the out tensor memory on gpu\nand at last launches the kernel (e.g. cuda) and after all the above things\ndone, the Compute() method returns. So the next Op's kernel's Compute\nmethod may be called at once. Tensor's TensorBuffer stores the pointer to\nthe location of the real tensor bytes and\n\"\"\"\nUp to this point, your understanding matches what TF runtime does.\n\n\"\"\"it can be remained in gpu if there is an tensor object or an\nTensorReference object remains in the program.\nHowever once a op's kernel is done, it's input will be cleaned and I\nhaven't see any Tensor or TensorReference object alive in the program since\nthe device's RequiresRecordingAccessedTensors() method returns false. So\nhow can this tensor's byte remains in gpu? In detail, does allocating\nmemory on gpu is queued on the stream that executes the kernel?\"\"\"\n\nThis section of your description doesn't chime well with TF runtime\nimplementation. Maybe the point that TF runtime has its own memory pool can\nhelp you understanding, I hope. Without such a pool, yes, you need to queue\nthe cuda allocation and deallocation calls on the appropriate stream and/or\nadds proper waiting events, which we think is a rather complex scheme to\nengineer when differently cuda libraries are used concurrently.\n\u2026\nOn Sun, Dec 11, 2016 at 5:41 PM cuiguoxin ***@***.***> wrote:\n hi, @michaelisard <https://github.com/michaelisard> . I know that all\n kernels launched on a stream are serialized. For a typical op's kernel's\n Compute method, the mode is as follows I think: first it gets the input\n tensor of the kernel, then allocates the out tensor memory on gpu and at\n last launches the kernel (e.g. cuda) and after all the above things done,\n the Compute() method returns. So the next Op's kernel's Compute method may\n be called at once. Tensor's TensorBuffer stores the pointer to the location\n of the real tensor bytes and it can be remained in gpu if there is an\n tensor object or an TensorReference object remains in the program. However\n once a op's kernel is done, it's input will be cleaned and I haven't see\n any Tensor or TensorReference object alive in the program since the\n device's RequiresRecordingAccessedTensors() method returns false. So how\n can this tensor's byte remains in gpu? In detail, does allocating memory on\n gpu is queued on the stream that executes the kernel?\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n <#6247 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AHk2biViME88SP5hGC-YxcD1vQ-xwj9fks5rHKZhgaJpZM4LJ0V4>\n .", "body": "TF runtime does not allocate gpu memory per op. Instead, it has it's own\nallocator, which grabs a big chunk (the pool) from cuda and never returns\nto cuda. All tensor objects are allocated within the pool.\n\nCombined w/ single compute stream configuration, one can prove the memory\nsafety in a similar way as people prove multi-threaded program's memory\nsafety properties, albeit much simpler for a single gpu cuda w/ only one\ncopy-in stream and one copy-out stream.\n\nConsider the following 3 operations in sequence:\n   x = add(a, b)\n   y = add(x, c)\n   z = add(y, d)\nassuming, a, b, c, d, x, y, and z are of the same shape, and x and y are\nnot used before the 3rd operation.\n\nIf we, TF runtime, know that they are run on the same gpu device and hence\none single gpu stream, the following are relevant events on cpu and gpu\nrespectively:\ncpu:\n  allocate x\n  dispatch kernel for x = add(a, b)\n  allocate y\n  dispatch kernel for y = add(x, c)\n  deallocate x\n  allocate z\n  dispatch kernel for z = add(y, d)\n  deallocate y\n\ngpu:\n  execute kernel for x = add(a, b)\n  execute kernel for y = add(x, b)\n  execute kernel for z = add(y, b)\n\nThe TF data flow graph guarantees that the events on cpu above happens one\nafter another. The single cuda stream semantics guarantees events on gpu\nabove happens one after another. dispatch kernel and execute kernel has the\nstrict happen-after relation. If you draw the happen-after edges of these\nevents, you can see their correctness.\n\nMore importantly, this allows us the TF runtime to reuse the memory release\nby \"deallocate x\" immediately to \"allocate z\".\n\n\n\"\"\"\nI know that all kernels launched on a stream are serialized. For a typical\nop's kernel's Compute method, the mode is as follows I think: first it gets\nthe input tensor of the kernel, then allocates the out tensor memory on gpu\nand at last launches the kernel (e.g. cuda) and after all the above things\ndone, the Compute() method returns. So the next Op's kernel's Compute\nmethod may be called at once. Tensor's TensorBuffer stores the pointer to\nthe location of the real tensor bytes and\n\"\"\"\nUp to this point, your understanding matches what TF runtime does.\n\n\"\"\"it can be remained in gpu if there is an tensor object or an\nTensorReference object remains in the program.\nHowever once a op's kernel is done, it's input will be cleaned and I\nhaven't see any Tensor or TensorReference object alive in the program since\nthe device's RequiresRecordingAccessedTensors() method returns false. So\nhow can this tensor's byte remains in gpu? In detail, does allocating\nmemory on gpu is queued on the stream that executes the kernel?\"\"\"\n\nThis section of your description doesn't chime well with TF runtime\nimplementation. Maybe the point that TF runtime has its own memory pool can\nhelp you understanding, I hope. Without such a pool, yes, you need to queue\nthe cuda allocation and deallocation calls on the appropriate stream and/or\nadds proper waiting events, which we think is a rather complex scheme to\nengineer when differently cuda libraries are used concurrently.\n\n\n\nOn Sun, Dec 11, 2016 at 5:41 PM cuiguoxin <notifications@github.com> wrote:\n\n> hi, @michaelisard <https://github.com/michaelisard> . I know that all\n> kernels launched on a stream are serialized. For a typical op's kernel's\n> Compute method, the mode is as follows I think: first it gets the input\n> tensor of the kernel, then allocates the out tensor memory on gpu and at\n> last launches the kernel (e.g. cuda) and after all the above things done,\n> the Compute() method returns. So the next Op's kernel's Compute method may\n> be called at once. Tensor's TensorBuffer stores the pointer to the location\n> of the real tensor bytes and it can be remained in gpu if there is an\n> tensor object or an TensorReference object remains in the program. However\n> once a op's kernel is done, it's input will be cleaned and I haven't see\n> any Tensor or TensorReference object alive in the program since the\n> device's RequiresRecordingAccessedTensors() method returns false. So how\n> can this tensor's byte remains in gpu? In detail, does allocating memory on\n> gpu is queued on the stream that executes the kernel?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6247#issuecomment-266325602>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHk2biViME88SP5hGC-YxcD1vQ-xwj9fks5rHKZhgaJpZM4LJ0V4>\n> .\n>\n"}