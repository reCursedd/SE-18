{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266325602", "html_url": "https://github.com/tensorflow/tensorflow/issues/6247#issuecomment-266325602", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6247", "id": 266325602, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjMyNTYwMg==", "user": {"login": "cuiguoxin", "id": 8746710, "node_id": "MDQ6VXNlcjg3NDY3MTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8746710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cuiguoxin", "html_url": "https://github.com/cuiguoxin", "followers_url": "https://api.github.com/users/cuiguoxin/followers", "following_url": "https://api.github.com/users/cuiguoxin/following{/other_user}", "gists_url": "https://api.github.com/users/cuiguoxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/cuiguoxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cuiguoxin/subscriptions", "organizations_url": "https://api.github.com/users/cuiguoxin/orgs", "repos_url": "https://api.github.com/users/cuiguoxin/repos", "events_url": "https://api.github.com/users/cuiguoxin/events{/privacy}", "received_events_url": "https://api.github.com/users/cuiguoxin/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-12T01:41:17Z", "updated_at": "2016-12-12T02:07:42Z", "author_association": "NONE", "body_html": "<p>hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5376757\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/michaelisard\">@michaelisard</a> . I know that all kernels launched on a stream are serialized. For a typical op's kernel's Compute method, the mode is as follows I think: first it gets the input tensor of the kernel, then allocates the out tensor memory on gpu and at last launches the kernel (e.g. cuda) and after all the above things done, the Compute() method returns. So the next Op's kernel's Compute method may be called at once. Tensor's TensorBuffer stores the pointer to the location of the real tensor's bytes and the bytes can be remained in gpu if there is an tensor object or an TensorReference object remains in the program. However once a op's kernel is done, it's input will be cleaned and I haven't see any Tensor or TensorReference object alive in the program since the device's RequiresRecordingAccessedTensors() method returns false. So how can this tensor's byte remains in gpu? In detail, is allocating and deallocating memory on gpu queued on the stream that executes the kernel?</p>", "body_text": "hi, @michaelisard . I know that all kernels launched on a stream are serialized. For a typical op's kernel's Compute method, the mode is as follows I think: first it gets the input tensor of the kernel, then allocates the out tensor memory on gpu and at last launches the kernel (e.g. cuda) and after all the above things done, the Compute() method returns. So the next Op's kernel's Compute method may be called at once. Tensor's TensorBuffer stores the pointer to the location of the real tensor's bytes and the bytes can be remained in gpu if there is an tensor object or an TensorReference object remains in the program. However once a op's kernel is done, it's input will be cleaned and I haven't see any Tensor or TensorReference object alive in the program since the device's RequiresRecordingAccessedTensors() method returns false. So how can this tensor's byte remains in gpu? In detail, is allocating and deallocating memory on gpu queued on the stream that executes the kernel?", "body": "hi, @michaelisard . I know that all kernels launched on a stream are serialized. For a typical op's kernel's Compute method, the mode is as follows I think: first it gets the input tensor of the kernel, then allocates the out tensor memory on gpu and at last launches the kernel (e.g. cuda) and after all the above things done, the Compute() method returns. So the next Op's kernel's Compute method may be called at once. Tensor's TensorBuffer stores the pointer to the location of the real tensor's bytes and the bytes can be remained in gpu if there is an tensor object or an TensorReference object remains in the program. However once a op's kernel is done, it's input will be cleaned and I haven't see any Tensor or TensorReference object alive in the program since the device's RequiresRecordingAccessedTensors() method returns false. So how can this tensor's byte remains in gpu? In detail, is allocating and deallocating memory on gpu queued on the stream that executes the kernel?"}