{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6247", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6247/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6247/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6247/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6247", "id": 194807657, "node_id": "MDU6SXNzdWUxOTQ4MDc2NTc=", "number": 6247, "title": "How can a tensor remain in gpu when the kernel is launched but does not finish yet?", "user": {"login": "cuiguoxin", "id": 8746710, "node_id": "MDQ6VXNlcjg3NDY3MTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8746710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cuiguoxin", "html_url": "https://github.com/cuiguoxin", "followers_url": "https://api.github.com/users/cuiguoxin/followers", "following_url": "https://api.github.com/users/cuiguoxin/following{/other_user}", "gists_url": "https://api.github.com/users/cuiguoxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/cuiguoxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cuiguoxin/subscriptions", "organizations_url": "https://api.github.com/users/cuiguoxin/orgs", "repos_url": "https://api.github.com/users/cuiguoxin/repos", "events_url": "https://api.github.com/users/cuiguoxin/events{/privacy}", "received_events_url": "https://api.github.com/users/cuiguoxin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-12-11T02:18:13Z", "updated_at": "2016-12-12T13:21:01Z", "closed_at": "2016-12-12T13:21:01Z", "author_association": "NONE", "body_html": "<p>It confuse me a lot. I'm reading the stream executor part of tensorflow's source code. I find that once a graph is executed on a gpu device, each kernel of the graph's nodes just allocates the output memory and launches the cuda kernel on gpu on a stream by passing the input and output device memory. So the op kernel is done but the real execution of the kernel (e.g. cuda) may not be done on the gpu yet. Here is the question: how can tensorflow assure the input memory allocated on the gpu devices still live when the execution really happens on the gpu? I checked the executor.cc file and found that once a kernel is launched, the input tensor is destroyed. Can anyone show me what I miss? Thanks!</p>", "body_text": "It confuse me a lot. I'm reading the stream executor part of tensorflow's source code. I find that once a graph is executed on a gpu device, each kernel of the graph's nodes just allocates the output memory and launches the cuda kernel on gpu on a stream by passing the input and output device memory. So the op kernel is done but the real execution of the kernel (e.g. cuda) may not be done on the gpu yet. Here is the question: how can tensorflow assure the input memory allocated on the gpu devices still live when the execution really happens on the gpu? I checked the executor.cc file and found that once a kernel is launched, the input tensor is destroyed. Can anyone show me what I miss? Thanks!", "body": "It confuse me a lot. I'm reading the stream executor part of tensorflow's source code. I find that once a graph is executed on a gpu device, each kernel of the graph's nodes just allocates the output memory and launches the cuda kernel on gpu on a stream by passing the input and output device memory. So the op kernel is done but the real execution of the kernel (e.g. cuda) may not be done on the gpu yet. Here is the question: how can tensorflow assure the input memory allocated on the gpu devices still live when the execution really happens on the gpu? I checked the executor.cc file and found that once a kernel is launched, the input tensor is destroyed. Can anyone show me what I miss? Thanks!"}