{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266299418", "html_url": "https://github.com/tensorflow/tensorflow/issues/6247#issuecomment-266299418", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6247", "id": 266299418, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjI5OTQxOA==", "user": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-11T18:48:23Z", "updated_at": "2016-12-11T18:48:23Z", "author_association": "MEMBER", "body_html": "<p>When there is a single stream, all kernel launches on the GPU are serialized. So you can think of the GPU memory allocator in TensorFlow as \"simulating\" the memory usage of the actual kernels. It determines what memory is needed for kernel 1, then enqueues kernel 1 for launch, then \"frees\" the memory needed by kernel 1. When simulating what memory is needed for kernel 2, it can re-use any temporary memory used by kernel 1, before enqueueing kernel 2, because there is a single stream. This means that kernel 2 is guaranteed not to execute until after kernel 1 completes. As you observe, the memory allocator does not indicate what memory is actually in use on the stream, but it does indicate what memory is available to use for the next kernel to be enqueued, and that is the information that is needed when running the next Op.</p>\n<p>Does that make sense?</p>", "body_text": "When there is a single stream, all kernel launches on the GPU are serialized. So you can think of the GPU memory allocator in TensorFlow as \"simulating\" the memory usage of the actual kernels. It determines what memory is needed for kernel 1, then enqueues kernel 1 for launch, then \"frees\" the memory needed by kernel 1. When simulating what memory is needed for kernel 2, it can re-use any temporary memory used by kernel 1, before enqueueing kernel 2, because there is a single stream. This means that kernel 2 is guaranteed not to execute until after kernel 1 completes. As you observe, the memory allocator does not indicate what memory is actually in use on the stream, but it does indicate what memory is available to use for the next kernel to be enqueued, and that is the information that is needed when running the next Op.\nDoes that make sense?", "body": "When there is a single stream, all kernel launches on the GPU are serialized. So you can think of the GPU memory allocator in TensorFlow as \"simulating\" the memory usage of the actual kernels. It determines what memory is needed for kernel 1, then enqueues kernel 1 for launch, then \"frees\" the memory needed by kernel 1. When simulating what memory is needed for kernel 2, it can re-use any temporary memory used by kernel 1, before enqueueing kernel 2, because there is a single stream. This means that kernel 2 is guaranteed not to execute until after kernel 1 completes. As you observe, the memory allocator does not indicate what memory is actually in use on the stream, but it does indicate what memory is available to use for the next kernel to be enqueued, and that is the information that is needed when running the next Op.\r\n\r\nDoes that make sense?"}