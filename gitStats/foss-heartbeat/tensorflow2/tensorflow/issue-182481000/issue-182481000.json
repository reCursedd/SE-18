{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4909", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4909/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4909/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4909/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4909", "id": 182481000, "node_id": "MDU6SXNzdWUxODI0ODEwMDA=", "number": 4909, "title": "Empty array as loss function causes unhandled exception", "user": {"login": "tillahoffmann", "id": 966348, "node_id": "MDQ6VXNlcjk2NjM0OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/966348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tillahoffmann", "html_url": "https://github.com/tillahoffmann", "followers_url": "https://api.github.com/users/tillahoffmann/followers", "following_url": "https://api.github.com/users/tillahoffmann/following{/other_user}", "gists_url": "https://api.github.com/users/tillahoffmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/tillahoffmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tillahoffmann/subscriptions", "organizations_url": "https://api.github.com/users/tillahoffmann/orgs", "repos_url": "https://api.github.com/users/tillahoffmann/repos", "events_url": "https://api.github.com/users/tillahoffmann/events{/privacy}", "received_events_url": "https://api.github.com/users/tillahoffmann/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2016-10-12T09:44:31Z", "updated_at": "2017-06-16T17:31:32Z", "closed_at": "2017-06-16T17:31:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If the loss fed to an optimizer is an empty array and attempt is made to train on a GPU, an unhandled exception occurs that kills the python kernel. The logs reveal the following error message</p>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 4.89GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:423] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n</code></pre>\n<p>Code to reproduce the problem:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define a useless network</span>\n<span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> graph:\n    placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\n    <span class=\"pl-c1\">filter</span> <span class=\"pl-k\">=</span> tf.Variable(np.random.gamma(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, (<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)).astype(np.float32))\n    loss <span class=\"pl-k\">=</span> tf.nn.conv2d(placeholder, <span class=\"pl-c1\">filter</span>, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>)\n    optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer()\n    train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss)\n    init_op <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n\ngpu_options <span class=\"pl-k\">=</span> tf.GPUOptions(<span class=\"pl-v\">per_process_gpu_memory_fraction</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.3</span>)\nsession <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph, <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options))\nsession.run(init_op)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss<span class=\"pl-pds\">\"</span></span>, session.run(loss, {placeholder: x}))\n    session.run(train_op, {placeholder: x})\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>executed one training step<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This succeeds</span>\nrun(np.ones((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span>)))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This kills the kernel because the 'VALID' padding in the convolutional layer</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> leads to an empty array which the optimizer cannot handle</span>\nrun(np.ones((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">1</span>)))</pre></div>", "body_text": "If the loss fed to an optimizer is an empty array and attempt is made to train on a GPU, an unhandled exception occurs that kills the python kernel. The logs reveal the following error message\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 4.89GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:423] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n\nCode to reproduce the problem:\nimport tensorflow as tf\nimport numpy as np\n\n# Define a useless network\nwith tf.Graph().as_default() as graph:\n    placeholder = tf.placeholder(tf.float32)\n    filter = tf.Variable(np.random.gamma(1, 1, (10, 10, 1, 1)).astype(np.float32))\n    loss = tf.nn.conv2d(placeholder, filter, [1, 1, 1, 1], 'VALID')\n    optimizer = tf.train.AdamOptimizer()\n    train_op = optimizer.minimize(loss)\n    init_op = tf.initialize_all_variables()\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\nsession = tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\nsession.run(init_op)\n\n\ndef run(x):\n    print(\"loss\", session.run(loss, {placeholder: x}))\n    session.run(train_op, {placeholder: x})\n    print(\"executed one training step\")\n\n\n# This succeeds\nrun(np.ones((1, 10, 10, 1)))\n\n# This kills the kernel because the 'VALID' padding in the convolutional layer\n# leads to an empty array which the optimizer cannot handle\nrun(np.ones((1, 10, 9, 1)))", "body": "If the loss fed to an optimizer is an empty array and attempt is made to train on a GPU, an unhandled exception occurs that kills the python kernel. The logs reveal the following error message\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 4.89GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:423] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n```\n\nCode to reproduce the problem:\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\n# Define a useless network\nwith tf.Graph().as_default() as graph:\n    placeholder = tf.placeholder(tf.float32)\n    filter = tf.Variable(np.random.gamma(1, 1, (10, 10, 1, 1)).astype(np.float32))\n    loss = tf.nn.conv2d(placeholder, filter, [1, 1, 1, 1], 'VALID')\n    optimizer = tf.train.AdamOptimizer()\n    train_op = optimizer.minimize(loss)\n    init_op = tf.initialize_all_variables()\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\nsession = tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\nsession.run(init_op)\n\n\ndef run(x):\n    print(\"loss\", session.run(loss, {placeholder: x}))\n    session.run(train_op, {placeholder: x})\n    print(\"executed one training step\")\n\n\n# This succeeds\nrun(np.ones((1, 10, 10, 1)))\n\n# This kills the kernel because the 'VALID' padding in the convolutional layer\n# leads to an empty array which the optimizer cannot handle\nrun(np.ones((1, 10, 9, 1)))\n```\n"}