{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19433", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19433/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19433/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19433/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19433", "id": 324900370, "node_id": "MDU6SXNzdWUzMjQ5MDAzNzA=", "number": 19433, "title": "Tensorflow on iMX6 with Vivante GPU - OpenCL support enabled", "user": {"login": "karthee320", "id": 13103382, "node_id": "MDQ6VXNlcjEzMTAzMzgy", "avatar_url": "https://avatars3.githubusercontent.com/u/13103382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karthee320", "html_url": "https://github.com/karthee320", "followers_url": "https://api.github.com/users/karthee320/followers", "following_url": "https://api.github.com/users/karthee320/following{/other_user}", "gists_url": "https://api.github.com/users/karthee320/gists{/gist_id}", "starred_url": "https://api.github.com/users/karthee320/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karthee320/subscriptions", "organizations_url": "https://api.github.com/users/karthee320/orgs", "repos_url": "https://api.github.com/users/karthee320/repos", "events_url": "https://api.github.com/users/karthee320/events{/privacy}", "received_events_url": "https://api.github.com/users/karthee320/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173351, "node_id": "MDU6TGFiZWw0NzMxNzMzNTE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install", "name": "type:build/install", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-05-21T12:35:43Z", "updated_at": "2018-09-24T15:29:12Z", "closed_at": "2018-08-06T15:28:57Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Target OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux 4.9 Yocto</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source (Cross Compiling for arm)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.10.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: Vivante GPU - GC2000, OpenCL 1.1</li>\n<li><strong>Exact commands to reproduce</strong>:<br>\n$ ./configure<br>\nYou have bazel 0.10.1 installed.<br>\nPlease specify the location of python. [Default is /usr/bin/python]:</li>\n</ul>\n<p>Found possible Python library paths:<br>\n/opt/ros/indigo/lib/python2.7/dist-packages<br>\n/usr/local/lib/python2.7/dist-packages<br>\n/usr/lib/python2.7/dist-packages<br>\nPlease input the desired Python library path to use.  Default is [/opt/ros/indigo/lib/python2.7/dist-packages]<br>\n/usr/local/lib/python2.7/dist-packages<br>\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y<br>\njemalloc as malloc support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n<br>\nNo Google Cloud Platform support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n<br>\nNo Hadoop File System support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n<br>\nNo Amazon S3 File System support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with XLA JIT support? [y/N]: n<br>\nNo XLA JIT support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with GDR support? [y/N]: n<br>\nNo GDR support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with VERBS support? [y/N]: n<br>\nNo VERBS support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y<br>\nOpenCL SYCL support will be enabled for TensorFlow.</p>\n<p>Please specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]:</p>\n<p>Please specify which C compiler should be used as the hostC compiler. [Default is /usr/bin/gcc]:</p>\n<p>Do you wish to build TensorFlow with ComputeCPP support? [Y/n]: y<br>\nComputeCPP support will be enabled for TensorFlow.</p>\n<p>Please specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: /ComputeCpp-CE-0.7.0-Ubuntu-14.04-ARM_32</p>\n<p>Do you wish to build TensorFlow with CUDA support? [y/N]: n<br>\nNo CUDA support will be enabled for TensorFlow.</p>\n<p>Do you wish to build TensorFlow with MPI support? [y/N]: n<br>\nNo MPI support will be enabled for TensorFlow.</p>\n<p>Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -march=armv7-a</p>\n<p>Add \"--config=mkl\" to your bazel command to build with MKL support.<br>\nPlease note that MKL on MacOS or windows is still not supported.<br>\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.</p>\n<p>Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n<br>\nNot configuring the WORKSPACE for Android builds.</p>\n<p>Configuration finished</p>\n<p>$ bazel build --crosstool_top=//arm-compiler:toolchain --config=sycl --cpu=armeabi-v7a --config=opt -s tensorflow/examples/label_image/...  --cxxopt=\"-std=c++11\" --copt=\"-mfpu=neon\"</p>\n<p>I am currently trying to run a deep learning <strong>inference CNN network on embedded platforms in real-time using GPU</strong> with improved performance. Instead of creating my own inference engine, I thought of using existing frameworks in GPU mode itself to leverage the performance.</p>\n<p>After various study, I found that in the list of frameworks (Caffe, Tensorflow, MxNet, Torch) there are no resource on how to cross compile a particular framework for my arm architecture except few resources for Tensorflow. Though many frameworks suggest to use native compilation for building from source, I have not provided with the resources to build natively for my iMX6 quad [CPU : 4x Arm\u00ae Cortex\u00ae-A9 up to 1.2 GHz per core , GPU : Vivante GC2000] (yocto build) leaving cross compilation as the only option.</p>\n<p>I referred this link - <a href=\"http://www.morethantechnical.com/2018/03/08/cross-compile-latest-tensorflow-1-5-for-the-nvidia-jetson-tk1/\" rel=\"nofollow\">Cross compiling TF for Jetson TK1</a> and got successful in cross compiling Tensorflow cpu version. To benchmark, I tried to run Inception network and found CPU utilization is nearly 80% and the inference time is around 3.5 seconds. I need to make use of the <strong>vivante GPU (Embedded Profile : OpenCL 1.1)</strong> available in the board to reduce this utilization percentage and the inference time.</p>\n<p><strong>Since to use GPU,</strong></p>\n<p>(A) CUDA option is ruled out taking into the consideration I have no nvidia GPU's.</p>\n<p>(B) I tried to make use of the OpenCL capability <a href=\"https://github.com/tensorflow/tensorflow/issues/22\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/22/hovercard\">OpenCL support</a>. Using the similar way (like the CPU build) I tried to cross compile with OpenCL flags support this time, but failing miserably during the build.</p>\n<p><strong>Forum responses :</strong></p>\n<ol>\n<li>\n<p>They claim cross compiling TF with computecpp(codeplay sycl) is not possible. <a href=\"https://github.com/codeplaysoftware/computecpp-sdk/issues/68#issuecomment-348952794\" data-hovercard-type=\"issue\" data-hovercard-url=\"/codeplaysoftware/computecpp-sdk/issues/68/hovercard\">Codeplay forum</a></p>\n</li>\n<li>\n<p>Another implementation method with TriSycl claims the codes only use CPU, the option of using GPU is under research. <a href=\"http://%20https://github.com/triSYCL/triSYCL/pull/45\" rel=\"nofollow\">TriSycl forum</a></p>\n</li>\n</ol>\n<p><strong>Questions :</strong></p>\n<p>There is an another SYCL implementation available open source <a href=\"https://github.com/ProGTX/sycl-gtx\">Sycl ProGTX</a>.<br>\n-&gt; Whether Tensorflow  has an idea to integrate this implementation?<br>\n-&gt; Also is there any pre-requisite, my device driver version is only OpenCL 1.1? Is that enough to run the tensorflow code?</p>\n<p>Anyone attempted this and got success. Kindly guide me on how to proceed further to achieve my goal.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nHost OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTarget OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.9 Yocto\nTensorFlow installed from (source or binary): Source (Cross Compiling for arm)\nTensorFlow version (use command below): 1.5\nPython version: 2.7.12\nBazel version (if compiling from source): 0.10.1\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: NA\nGPU model and memory: Vivante GPU - GC2000, OpenCL 1.1\nExact commands to reproduce:\n$ ./configure\nYou have bazel 0.10.1 installed.\nPlease specify the location of python. [Default is /usr/bin/python]:\n\nFound possible Python library paths:\n/opt/ros/indigo/lib/python2.7/dist-packages\n/usr/local/lib/python2.7/dist-packages\n/usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/opt/ros/indigo/lib/python2.7/dist-packages]\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y\njemalloc as malloc support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\nNo Google Cloud Platform support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\nNo Hadoop File System support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\nNo Amazon S3 File System support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\nNo XLA JIT support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with GDR support? [y/N]: n\nNo GDR support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\nNo VERBS support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y\nOpenCL SYCL support will be enabled for TensorFlow.\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]:\nPlease specify which C compiler should be used as the hostC compiler. [Default is /usr/bin/gcc]:\nDo you wish to build TensorFlow with ComputeCPP support? [Y/n]: y\nComputeCPP support will be enabled for TensorFlow.\nPlease specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: /ComputeCpp-CE-0.7.0-Ubuntu-14.04-ARM_32\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\nNo CUDA support will be enabled for TensorFlow.\nDo you wish to build TensorFlow with MPI support? [y/N]: n\nNo MPI support will be enabled for TensorFlow.\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -march=armv7-a\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\nPlease note that MKL on MacOS or windows is still not supported.\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\nNot configuring the WORKSPACE for Android builds.\nConfiguration finished\n$ bazel build --crosstool_top=//arm-compiler:toolchain --config=sycl --cpu=armeabi-v7a --config=opt -s tensorflow/examples/label_image/...  --cxxopt=\"-std=c++11\" --copt=\"-mfpu=neon\"\nI am currently trying to run a deep learning inference CNN network on embedded platforms in real-time using GPU with improved performance. Instead of creating my own inference engine, I thought of using existing frameworks in GPU mode itself to leverage the performance.\nAfter various study, I found that in the list of frameworks (Caffe, Tensorflow, MxNet, Torch) there are no resource on how to cross compile a particular framework for my arm architecture except few resources for Tensorflow. Though many frameworks suggest to use native compilation for building from source, I have not provided with the resources to build natively for my iMX6 quad [CPU : 4x Arm\u00ae Cortex\u00ae-A9 up to 1.2 GHz per core , GPU : Vivante GC2000] (yocto build) leaving cross compilation as the only option.\nI referred this link - Cross compiling TF for Jetson TK1 and got successful in cross compiling Tensorflow cpu version. To benchmark, I tried to run Inception network and found CPU utilization is nearly 80% and the inference time is around 3.5 seconds. I need to make use of the vivante GPU (Embedded Profile : OpenCL 1.1) available in the board to reduce this utilization percentage and the inference time.\nSince to use GPU,\n(A) CUDA option is ruled out taking into the consideration I have no nvidia GPU's.\n(B) I tried to make use of the OpenCL capability OpenCL support. Using the similar way (like the CPU build) I tried to cross compile with OpenCL flags support this time, but failing miserably during the build.\nForum responses :\n\n\nThey claim cross compiling TF with computecpp(codeplay sycl) is not possible. Codeplay forum\n\n\nAnother implementation method with TriSycl claims the codes only use CPU, the option of using GPU is under research. TriSycl forum\n\n\nQuestions :\nThere is an another SYCL implementation available open source Sycl ProGTX.\n-> Whether Tensorflow  has an idea to integrate this implementation?\n-> Also is there any pre-requisite, my device driver version is only OpenCL 1.1? Is that enough to run the tensorflow code?\nAnyone attempted this and got success. Kindly guide me on how to proceed further to achieve my goal.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Target OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.9 Yocto\r\n- **TensorFlow installed from (source or binary)**: Source (Cross Compiling for arm)\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.10.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 \r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: Vivante GPU - GC2000, OpenCL 1.1\r\n- **Exact commands to reproduce**:\r\n$ ./configure\r\nYou have bazel 0.10.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\nFound possible Python library paths:\r\n  /opt/ros/indigo/lib/python2.7/dist-packages\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/opt/ros/indigo/lib/python2.7/dist-packages]\r\n/usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y\r\nOpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]: \r\n\r\nPlease specify which C compiler should be used as the hostC compiler. [Default is /usr/bin/gcc]: \r\n\r\nDo you wish to build TensorFlow with ComputeCPP support? [Y/n]: y\r\nComputeCPP support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: <Relative Location>/ComputeCpp-CE-0.7.0-Ubuntu-14.04-ARM_32\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -march=armv7-a\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nConfiguration finished\r\n\r\n\r\n$ bazel build --crosstool_top=//arm-compiler:toolchain --config=sycl --cpu=armeabi-v7a --config=opt -s tensorflow/examples/label_image/...  --cxxopt=\"-std=c++11\" --copt=\"-mfpu=neon\"\r\n\r\n\r\nI am currently trying to run a deep learning **inference CNN network on embedded platforms in real-time using GPU** with improved performance. Instead of creating my own inference engine, I thought of using existing frameworks in GPU mode itself to leverage the performance. \r\n\r\nAfter various study, I found that in the list of frameworks (Caffe, Tensorflow, MxNet, Torch) there are no resource on how to cross compile a particular framework for my arm architecture except few resources for Tensorflow. Though many frameworks suggest to use native compilation for building from source, I have not provided with the resources to build natively for my iMX6 quad [CPU : 4x Arm\u00ae Cortex\u00ae-A9 up to 1.2 GHz per core , GPU : Vivante GC2000] (yocto build) leaving cross compilation as the only option.\r\n\r\nI referred this link - [Cross compiling TF for Jetson TK1][1] and got successful in cross compiling Tensorflow cpu version. To benchmark, I tried to run Inception network and found CPU utilization is nearly 80% and the inference time is around 3.5 seconds. I need to make use of the **vivante GPU (Embedded Profile : OpenCL 1.1)** available in the board to reduce this utilization percentage and the inference time. \r\n\r\n**Since to use GPU,** \r\n\r\n(A) CUDA option is ruled out taking into the consideration I have no nvidia GPU's.\r\n\r\n(B) I tried to make use of the OpenCL capability [OpenCL support][2]. Using the similar way (like the CPU build) I tried to cross compile with OpenCL flags support this time, but failing miserably during the build. \r\n\r\n**Forum responses :** \r\n\r\n1. They claim cross compiling TF with computecpp(codeplay sycl) is not possible. [Codeplay forum][3]\r\n\r\n2. Another implementation method with TriSycl claims the codes only use CPU, the option of using GPU is under research. [TriSycl forum][4]\r\n\r\n**Questions :** \r\n\r\nThere is an another SYCL implementation available open source [Sycl ProGTX][5].\r\n-> Whether Tensorflow  has an idea to integrate this implementation? \r\n-> Also is there any pre-requisite, my device driver version is only OpenCL 1.1? Is that enough to run the tensorflow code?\r\n\r\nAnyone attempted this and got success. Kindly guide me on how to proceed further to achieve my goal.\r\n\r\n  [1]: http://www.morethantechnical.com/2018/03/08/cross-compile-latest-tensorflow-1-5-for-the-nvidia-jetson-tk1/\r\n  [2]: https://github.com/tensorflow/tensorflow/issues/22\r\n  [3]: https://github.com/codeplaysoftware/computecpp-sdk/issues/68#issuecomment-348952794\r\n  [4]: http://%20https://github.com/triSYCL/triSYCL/pull/45\r\n  [5]: https://github.com/ProGTX/sycl-gtx\r\n"}