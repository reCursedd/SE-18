{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/156898695", "pull_request_review_id": 83448840, "id": 156898695, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Njg5ODY5NQ==", "diff_hunk": "@@ -121,10 +121,69 @@ def tile_batch(t, multiplier, name=None):\n     return nest.map_structure(lambda t_: _tile_batch(t_, multiplier), t)\n \n \n+def gather_tree_from_array(t, parent_ids, sequence_length):\n+  \"\"\"Calculates the full beams for `TensorArray`s.\n+\n+  Args:\n+    t: A `TensorArray` of size `max_time` that contains `Tensor`s of shape\n+      `[batch_size, beam_width, depth]`.\n+    parent_ids: The parent ids of shape `[max_time, batch_size, beam_width]`.\n+    sequence_length: The sequence length of shape `[batch_size, beam_width]`.\n+\n+  Returns:\n+    A `TensorArray` of the same size and type as `t` and where beams are sorted\n+    in each `Tensor` according to `parent_ids`.\n+  \"\"\"\n+  max_time = array_ops.shape(parent_ids)[0]\n+  batch_size = array_ops.shape(parent_ids)[1]\n+  beam_width = array_ops.shape(parent_ids)[2]\n+\n+  # Generate beam ids that will be reordered by gather_tree.\n+  beam_ids = array_ops.expand_dims(\n+      array_ops.expand_dims(math_ops.range(beam_width), 0), 0)\n+  beam_ids = array_ops.tile(beam_ids, [max_time, batch_size, 1])\n+\n+  mask = array_ops.sequence_mask(\n+      sequence_length, maxlen=max_time, dtype=dtypes.int32)\n+  mask = array_ops.transpose(mask, perm=[2, 0, 1])\n+\n+  # Use beam_width + 1 to mark the end of beam.\n+  masked_beam_ids = (beam_ids * mask) + (1 - mask) * (beam_width + 1)\n+\n+  max_sequence_lengths = math_ops.to_int32(\n+      math_ops.reduce_max(sequence_length, axis=1))\n+  sorted_beam_ids = beam_search_ops.gather_tree(\n+      step_ids=masked_beam_ids,\n+      parent_ids=parent_ids,\n+      max_sequence_lengths=max_sequence_lengths,\n+      end_token=beam_width + 1)\n+\n+  # For out of range steps, simply copy the same beam.\n+  sorted_beam_ids = array_ops.where(\n+      math_ops.cast(mask, dtypes.bool), x=sorted_beam_ids, y=beam_ids)\n+\n+  # Gather from each tensor in t according to sorted_beam_ids.\n+  def _collect(collector, i):\n+    gathered = _tensor_gather_helper(\n+        gather_indices=sorted_beam_ids[i],\n+        gather_from=t.read(i),\n+        batch_size=batch_size,\n+        range_size=beam_width,\n+        gather_shape=[batch_size * beam_width, -1])\n+    return collector.write(i, gathered), i + 1\n+\n+  collected = tensor_array_ops.TensorArray(\n+      t.dtype, size=t.size(), dynamic_size=False)\n+  collected, _ = control_flow_ops.while_loop(\n+      lambda _, i: i < t.size(),", "path": "tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py", "position": null, "original_position": 58, "commit_id": "d3eb228f1db2d60caf380833684944ce12203634", "original_commit_id": "c50d47eda64e35dacf47d01e204a756be7fca9b7", "user": {"login": "guillaumekln", "id": 4805513, "node_id": "MDQ6VXNlcjQ4MDU1MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4805513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guillaumekln", "html_url": "https://github.com/guillaumekln", "followers_url": "https://api.github.com/users/guillaumekln/followers", "following_url": "https://api.github.com/users/guillaumekln/following{/other_user}", "gists_url": "https://api.github.com/users/guillaumekln/gists{/gist_id}", "starred_url": "https://api.github.com/users/guillaumekln/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guillaumekln/subscriptions", "organizations_url": "https://api.github.com/users/guillaumekln/orgs", "repos_url": "https://api.github.com/users/guillaumekln/repos", "events_url": "https://api.github.com/users/guillaumekln/events{/privacy}", "received_events_url": "https://api.github.com/users/guillaumekln/received_events", "type": "User", "site_admin": false}, "body": "I'm not sure to understand. This loop is only called at the end of the decoding in `finalize()` and iterates over `Tensor`s. As  `gather_tree` works on `step_ids`, here we gather a generated sequence of ids that we then use to reorder each element of the `TensorArray`.\r\n\r\nAm I missing something here?\r\n\r\nRegarding the test, when `has_attention` is `False` there is no `AttentionWrapper` and no `alignment_history` by extension.", "created_at": "2017-12-14T09:59:04Z", "updated_at": "2018-03-17T07:09:51Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13312#discussion_r156898695", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13312", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/156898695"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13312#discussion_r156898695"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13312"}}, "body_html": "<p>I'm not sure to understand. This loop is only called at the end of the decoding in <code>finalize()</code> and iterates over <code>Tensor</code>s. As  <code>gather_tree</code> works on <code>step_ids</code>, here we gather a generated sequence of ids that we then use to reorder each element of the <code>TensorArray</code>.</p>\n<p>Am I missing something here?</p>\n<p>Regarding the test, when <code>has_attention</code> is <code>False</code> there is no <code>AttentionWrapper</code> and no <code>alignment_history</code> by extension.</p>", "body_text": "I'm not sure to understand. This loop is only called at the end of the decoding in finalize() and iterates over Tensors. As  gather_tree works on step_ids, here we gather a generated sequence of ids that we then use to reorder each element of the TensorArray.\nAm I missing something here?\nRegarding the test, when has_attention is False there is no AttentionWrapper and no alignment_history by extension.", "in_reply_to_id": 156848356}