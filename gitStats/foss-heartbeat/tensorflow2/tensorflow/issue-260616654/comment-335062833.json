{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335062833", "html_url": "https://github.com/tensorflow/tensorflow/pull/13312#issuecomment-335062833", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13312", "id": 335062833, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTA2MjgzMw==", "user": {"login": "steven-hh-ding", "id": 8474647, "node_id": "MDQ6VXNlcjg0NzQ2NDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8474647?v=4", "gravatar_id": "", "url": "https://api.github.com/users/steven-hh-ding", "html_url": "https://github.com/steven-hh-ding", "followers_url": "https://api.github.com/users/steven-hh-ding/followers", "following_url": "https://api.github.com/users/steven-hh-ding/following{/other_user}", "gists_url": "https://api.github.com/users/steven-hh-ding/gists{/gist_id}", "starred_url": "https://api.github.com/users/steven-hh-ding/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/steven-hh-ding/subscriptions", "organizations_url": "https://api.github.com/users/steven-hh-ding/orgs", "repos_url": "https://api.github.com/users/steven-hh-ding/repos", "events_url": "https://api.github.com/users/steven-hh-ding/events{/privacy}", "received_events_url": "https://api.github.com/users/steven-hh-ding/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-09T03:56:24Z", "updated_at": "2017-10-09T22:30:40Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_gather_tree_for_array</span>(<span class=\"pl-smi\">t</span>, <span class=\"pl-smi\">parent_ids</span>, <span class=\"pl-smi\">sequence_length</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">  Convert tensor array that contains tensor with unsorted beams.</span>\n<span class=\"pl-s\">  Each element has shape [batch*beam, depth]</span>\n<span class=\"pl-s\">  Return sorted beam tensor array contains elements of shape</span>\n<span class=\"pl-s\">  [batch, beam, depth]</span>\n<span class=\"pl-s\">  Padding has a value of 0.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (time, batch, beam)</span>\n  time <span class=\"pl-k\">=</span> array_ops.shape(parent_ids)[<span class=\"pl-c1\">0</span>]\n  batch <span class=\"pl-k\">=</span> array_ops.shape(parent_ids)[<span class=\"pl-c1\">1</span>]\n  beam <span class=\"pl-k\">=</span> array_ops.shape(parent_ids)[<span class=\"pl-c1\">2</span>]\n  sorted_indx <span class=\"pl-k\">=</span> array_ops.expand_dims(\n      array_ops.expand_dims(math_ops.range(beam), <span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">0</span>)\n  sorted_indx <span class=\"pl-k\">=</span> array_ops.tile(sorted_indx, [time, batch, <span class=\"pl-c1\">1</span>])\n  sorted_beams <span class=\"pl-k\">=</span> beam_search_ops.gather_tree(\n      <span class=\"pl-v\">step_ids</span><span class=\"pl-k\">=</span>sorted_indx, <span class=\"pl-v\">parent_ids</span><span class=\"pl-k\">=</span>parent_ids,\n      <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_length)\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> all index of beam increase by 1. (-1 padding index becomes 0)</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 0 indicates a padding beam which has zero alignments.</span>\n  sorted_beams <span class=\"pl-k\">=</span> sorted_beams <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">collect</span>(<span class=\"pl-smi\">collector</span>, <span class=\"pl-smi\">i</span>):\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> concate a padding alignment (zeros) for each batch</span>\n      value <span class=\"pl-k\">=</span> array_ops.reshape(t.read(i), [batch, beam, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n      padding <span class=\"pl-k\">=</span> array_ops.zeros([batch, <span class=\"pl-c1\">1</span>, array_ops.shape(value)[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]])\n      value <span class=\"pl-k\">=</span> array_ops.concat([padding, value], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> collect value according to the sorted_beams</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> cannot use gather_helper as we increase the beam size by one</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> so the final_shape has a different beam size</span>\n      range_ <span class=\"pl-k\">=</span> array_ops.expand_dims(\n          math_ops.range(batch) <span class=\"pl-k\">*</span> (beam <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">1</span>)\n      gather_indices <span class=\"pl-k\">=</span> array_ops.reshape(sorted_beams[i] <span class=\"pl-k\">+</span> range_, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n      sorted_value <span class=\"pl-k\">=</span> array_ops.gather(\n          array_ops.reshape(\n              value, [batch <span class=\"pl-k\">*</span> (beam <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]),\n          gather_indices)\n      sorted_value <span class=\"pl-k\">=</span> array_ops.reshape(sorted_value, [batch, beam, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n\n      collector <span class=\"pl-k\">=</span> collector.write(i, sorted_value)\n      <span class=\"pl-k\">return</span> collector, i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n\n  collected <span class=\"pl-k\">=</span> tensor_array_ops.TensorArray(\n      <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>t.size(), <span class=\"pl-v\">dynamic_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtypes.float32)\n  collected, _ <span class=\"pl-k\">=</span> control_flow_ops.while_loop(\n      <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">_</span>, <span class=\"pl-smi\">i</span>: i <span class=\"pl-k\">&lt;</span> t.size(),\n      collect,\n      <span class=\"pl-v\">loop_vars</span><span class=\"pl-k\">=</span>(collected, <span class=\"pl-c1\">0</span>),\n      <span class=\"pl-v\">parallel_iterations</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n  <span class=\"pl-k\">return</span> collected</pre></div>\n<p>test code: (extended from testGatherTree)</p>\n<div class=\"highlight highlight-source-python\"><pre>step_ids <span class=\"pl-k\">=</span> _transpose_batch_time(\n    [[[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">6</span>], [<span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">9</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]], \n    [[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">6</span>], [<span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">9</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]]])\nparent_ids <span class=\"pl-k\">=</span> _transpose_batch_time(\n    [[[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]], \n    [[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]]])\nsequence_length <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>],[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>]]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> make a dummy alignment history, which is the tiled step_ids</span>\nstep_ids <span class=\"pl-k\">=</span> ops.convert_to_tensor(step_ids)\ntime <span class=\"pl-k\">=</span> array_ops.shape(step_ids)[<span class=\"pl-c1\">0</span>]\nbatch <span class=\"pl-k\">=</span> array_ops.shape(step_ids)[<span class=\"pl-c1\">1</span>]\nbeam <span class=\"pl-k\">=</span> array_ops.shape(step_ids)[<span class=\"pl-c1\">2</span>] \nalignment_length <span class=\"pl-k\">=</span> array_ops.constant(<span class=\"pl-c1\">10</span>)\nalignment_history <span class=\"pl-k\">=</span> array_ops.tile(\n    array_ops.expand_dims(step_ids,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>), \n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,alignment_length])\nalignment_history <span class=\"pl-k\">=</span> math_ops.cast(\n    alignment_history, dtypes.float32)\nalignment_history <span class=\"pl-k\">=</span> array_ops.reshape(\n    alignment_history, [time, batch, beam, alignment_length])\nalignment_history <span class=\"pl-k\">=</span> tensor_array_ops.TensorArray(\n    <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">dynamic_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, \n    <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtypes.float32).unstack(alignment_history)\n\nsorted_history <span class=\"pl-k\">=</span> _gather_tree_for_array(\n    alignment_history, parent_ids, sequence_length).stack()\n\nsorted_step_ids <span class=\"pl-k\">=</span> beam_search_ops.gather_tree(\n    <span class=\"pl-v\">step_ids</span><span class=\"pl-k\">=</span>step_ids, <span class=\"pl-v\">parent_ids</span><span class=\"pl-k\">=</span>parent_ids,\n    <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_length)\n\nsorted_step_ids <span class=\"pl-k\">=</span> array_ops.tile(\n    array_ops.expand_dims(sorted_step_ids,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>), \n    [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,alignment_length])\nsorted_step_ids <span class=\"pl-k\">=</span> math_ops.cast(\n    clip_ops.clip_by_value(\n        sorted_step_ids, \n        <span class=\"pl-c1\">0</span>, \n        math_ops.reduce_max(step_ids)),\n    dtypes.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the sorted history should be the same as the tiled sorted step id</span>\n<span class=\"pl-c1\">print</span>(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))</pre></div>\n<p>We also tested with beam_search_decoder and dynamic_decode. It looks fine (in the finalized func).</p>", "body_text": "def _gather_tree_for_array(t, parent_ids, sequence_length):\n  \"\"\"\n  Convert tensor array that contains tensor with unsorted beams.\n  Each element has shape [batch*beam, depth]\n  Return sorted beam tensor array contains elements of shape\n  [batch, beam, depth]\n  Padding has a value of 0.\n  \"\"\"\n  # (time, batch, beam)\n  time = array_ops.shape(parent_ids)[0]\n  batch = array_ops.shape(parent_ids)[1]\n  beam = array_ops.shape(parent_ids)[2]\n  sorted_indx = array_ops.expand_dims(\n      array_ops.expand_dims(math_ops.range(beam), 0), 0)\n  sorted_indx = array_ops.tile(sorted_indx, [time, batch, 1])\n  sorted_beams = beam_search_ops.gather_tree(\n      step_ids=sorted_indx, parent_ids=parent_ids,\n      sequence_length=sequence_length)\n  # all index of beam increase by 1. (-1 padding index becomes 0)\n  # 0 indicates a padding beam which has zero alignments.\n  sorted_beams = sorted_beams + 1\n\n  def collect(collector, i):\n      # concate a padding alignment (zeros) for each batch\n      value = array_ops.reshape(t.read(i), [batch, beam, -1])\n      padding = array_ops.zeros([batch, 1, array_ops.shape(value)[-1]])\n      value = array_ops.concat([padding, value], axis=1)\n\n      # collect value according to the sorted_beams\n      # cannot use gather_helper as we increase the beam size by one\n      # so the final_shape has a different beam size\n      range_ = array_ops.expand_dims(\n          math_ops.range(batch) * (beam + 1), 1)\n      gather_indices = array_ops.reshape(sorted_beams[i] + range_, [-1])\n      sorted_value = array_ops.gather(\n          array_ops.reshape(\n              value, [batch * (beam + 1), -1]),\n          gather_indices)\n      sorted_value = array_ops.reshape(sorted_value, [batch, beam, -1])\n\n      collector = collector.write(i, sorted_value)\n      return collector, i + 1\n\n  collected = tensor_array_ops.TensorArray(\n      size=t.size(), dynamic_size=True, dtype=dtypes.float32)\n  collected, _ = control_flow_ops.while_loop(\n      lambda _, i: i < t.size(),\n      collect,\n      loop_vars=(collected, 0),\n      parallel_iterations=1)\n  return collected\ntest code: (extended from testGatherTree)\nstep_ids = _transpose_batch_time(\n    [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]], \n    [[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\nparent_ids = _transpose_batch_time(\n    [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]], \n    [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]])\nsequence_length = [[3, 3, 3],[3, 1, 3]]\n\n# make a dummy alignment history, which is the tiled step_ids\nstep_ids = ops.convert_to_tensor(step_ids)\ntime = array_ops.shape(step_ids)[0]\nbatch = array_ops.shape(step_ids)[1]\nbeam = array_ops.shape(step_ids)[2] \nalignment_length = array_ops.constant(10)\nalignment_history = array_ops.tile(\n    array_ops.expand_dims(step_ids,-1), \n    [1,1,1,alignment_length])\nalignment_history = math_ops.cast(\n    alignment_history, dtypes.float32)\nalignment_history = array_ops.reshape(\n    alignment_history, [time, batch, beam, alignment_length])\nalignment_history = tensor_array_ops.TensorArray(\n    size=0, dynamic_size=True, \n    dtype=dtypes.float32).unstack(alignment_history)\n\nsorted_history = _gather_tree_for_array(\n    alignment_history, parent_ids, sequence_length).stack()\n\nsorted_step_ids = beam_search_ops.gather_tree(\n    step_ids=step_ids, parent_ids=parent_ids,\n    sequence_length=sequence_length)\n\nsorted_step_ids = array_ops.tile(\n    array_ops.expand_dims(sorted_step_ids,-1), \n    [1,1,1,alignment_length])\nsorted_step_ids = math_ops.cast(\n    clip_ops.clip_by_value(\n        sorted_step_ids, \n        0, \n        math_ops.reduce_max(step_ids)),\n    dtypes.float32)\n\n# the sorted history should be the same as the tiled sorted step id\nprint(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))\nWe also tested with beam_search_decoder and dynamic_decode. It looks fine (in the finalized func).", "body": "```python\r\ndef _gather_tree_for_array(t, parent_ids, sequence_length):\r\n  \"\"\"\r\n  Convert tensor array that contains tensor with unsorted beams.\r\n  Each element has shape [batch*beam, depth]\r\n  Return sorted beam tensor array contains elements of shape\r\n  [batch, beam, depth]\r\n  Padding has a value of 0.\r\n  \"\"\"\r\n  # (time, batch, beam)\r\n  time = array_ops.shape(parent_ids)[0]\r\n  batch = array_ops.shape(parent_ids)[1]\r\n  beam = array_ops.shape(parent_ids)[2]\r\n  sorted_indx = array_ops.expand_dims(\r\n      array_ops.expand_dims(math_ops.range(beam), 0), 0)\r\n  sorted_indx = array_ops.tile(sorted_indx, [time, batch, 1])\r\n  sorted_beams = beam_search_ops.gather_tree(\r\n      step_ids=sorted_indx, parent_ids=parent_ids,\r\n      sequence_length=sequence_length)\r\n  # all index of beam increase by 1. (-1 padding index becomes 0)\r\n  # 0 indicates a padding beam which has zero alignments.\r\n  sorted_beams = sorted_beams + 1\r\n\r\n  def collect(collector, i):\r\n      # concate a padding alignment (zeros) for each batch\r\n      value = array_ops.reshape(t.read(i), [batch, beam, -1])\r\n      padding = array_ops.zeros([batch, 1, array_ops.shape(value)[-1]])\r\n      value = array_ops.concat([padding, value], axis=1)\r\n\r\n      # collect value according to the sorted_beams\r\n      # cannot use gather_helper as we increase the beam size by one\r\n      # so the final_shape has a different beam size\r\n      range_ = array_ops.expand_dims(\r\n          math_ops.range(batch) * (beam + 1), 1)\r\n      gather_indices = array_ops.reshape(sorted_beams[i] + range_, [-1])\r\n      sorted_value = array_ops.gather(\r\n          array_ops.reshape(\r\n              value, [batch * (beam + 1), -1]),\r\n          gather_indices)\r\n      sorted_value = array_ops.reshape(sorted_value, [batch, beam, -1])\r\n\r\n      collector = collector.write(i, sorted_value)\r\n      return collector, i + 1\r\n\r\n  collected = tensor_array_ops.TensorArray(\r\n      size=t.size(), dynamic_size=True, dtype=dtypes.float32)\r\n  collected, _ = control_flow_ops.while_loop(\r\n      lambda _, i: i < t.size(),\r\n      collect,\r\n      loop_vars=(collected, 0),\r\n      parallel_iterations=1)\r\n  return collected\r\n```\r\n\r\ntest code: (extended from testGatherTree)\r\n\r\n```python\r\n\r\nstep_ids = _transpose_batch_time(\r\n    [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]], \r\n    [[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\r\nparent_ids = _transpose_batch_time(\r\n    [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]], \r\n    [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]])\r\nsequence_length = [[3, 3, 3],[3, 1, 3]]\r\n\r\n# make a dummy alignment history, which is the tiled step_ids\r\nstep_ids = ops.convert_to_tensor(step_ids)\r\ntime = array_ops.shape(step_ids)[0]\r\nbatch = array_ops.shape(step_ids)[1]\r\nbeam = array_ops.shape(step_ids)[2] \r\nalignment_length = array_ops.constant(10)\r\nalignment_history = array_ops.tile(\r\n    array_ops.expand_dims(step_ids,-1), \r\n    [1,1,1,alignment_length])\r\nalignment_history = math_ops.cast(\r\n    alignment_history, dtypes.float32)\r\nalignment_history = array_ops.reshape(\r\n    alignment_history, [time, batch, beam, alignment_length])\r\nalignment_history = tensor_array_ops.TensorArray(\r\n    size=0, dynamic_size=True, \r\n    dtype=dtypes.float32).unstack(alignment_history)\r\n\r\nsorted_history = _gather_tree_for_array(\r\n    alignment_history, parent_ids, sequence_length).stack()\r\n\r\nsorted_step_ids = beam_search_ops.gather_tree(\r\n    step_ids=step_ids, parent_ids=parent_ids,\r\n    sequence_length=sequence_length)\r\n\r\nsorted_step_ids = array_ops.tile(\r\n    array_ops.expand_dims(sorted_step_ids,-1), \r\n    [1,1,1,alignment_length])\r\nsorted_step_ids = math_ops.cast(\r\n    clip_ops.clip_by_value(\r\n        sorted_step_ids, \r\n        0, \r\n        math_ops.reduce_max(step_ids)),\r\n    dtypes.float32)\r\n\r\n# the sorted history should be the same as the tiled sorted step id\r\nprint(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))\r\n```\r\nWe also tested with beam_search_decoder and dynamic_decode. It looks fine (in the finalized func)."}