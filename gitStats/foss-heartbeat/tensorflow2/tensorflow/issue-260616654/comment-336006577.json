{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336006577", "html_url": "https://github.com/tensorflow/tensorflow/pull/13312#issuecomment-336006577", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13312", "id": 336006577, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjAwNjU3Nw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-12T02:57:41Z", "updated_at": "2017-10-12T02:57:41Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Steven, very nice!  Guillaume, are you interested in incorporating this\ninto your PR?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sun, Oct 8, 2017 at 8:57 PM, Steven Ding ***@***.***&gt; wrote:\n def _gather_tree_for_array(t, parent_ids, sequence_length):\n   \"\"\"  Convert tensor array that contains tensor with unsorted beams.  Each element has shape [batch*beam, depth]  Return sorted beam tensor array contains elements of shape  [batch, beam, depth]  Padding has a value of 0.  \"\"\"\n   # (time, batch, beam)\n   time = array_ops.shape(parent_ids)[0]\n   batch = array_ops.shape(parent_ids)[1]\n   beam = array_ops.shape(parent_ids)[2]\n   sorted_indx = array_ops.expand_dims(\n       array_ops.expand_dims(math_ops.range(beam), 0), 0)\n   sorted_indx = array_ops.tile(sorted_indx, [time, batch, 1])\n   sorted_beams = beam_search_ops.gather_tree(\n       step_ids=sorted_indx, parent_ids=parent_ids,\n       sequence_length=sequence_length)\n   # all index of beam increase by 1. (-1 padding index becomes 0)\n   # 0 indicates a padding beam which has zero alignments.\n   sorted_beams = sorted_beams + 1\n\n   def collect(collector, i):\n       # concate a padding alignment (zeros) for each batch\n       value = t.read(i)\n       padding = array_ops.zeros([batch, 1, array_ops.shape(value)[-1]])\n       value = array_ops.concat([padding, value], axis=1)\n\n       # collect value according to the sorted_beams\n       # cannot use gather_helper as we increase the beam size by one\n       # so the final_shape has a different beam size\n       range_ = array_ops.expand_dims(\n           math_ops.range(batch) * (beam + 1), 1)\n       gather_indices = array_ops.reshape(sorted_beams[i] + range_, [-1])\n       sorted_value = array_ops.gather(\n           array_ops.reshape(\n               value, [batch * (beam + 1), -1]),\n           gather_indices)\n       sorted_value = array_ops.reshape(sorted_value, [batch, beam, -1])\n\n       collector = collector.write(i, sorted_value)\n       return collector, i + 1\n\n   collected = tensor_array_ops.TensorArray(\n       size=t.size(), dynamic_size=True, dtype=dtypes.float32)\n   collected, _ = control_flow_ops.while_loop(\n       lambda _, i: i &lt; t.size(),\n       collect,\n       loop_vars=(collected, 0),\n       parallel_iterations=1)\n   return collected\n\n test code: (extended from testGatherTree)\n\n step_ids = _transpose_batch_time(\n     [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]],\n     [[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\n parent_ids = _transpose_batch_time(\n     [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]],\n     [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]])\n sequence_length = [[3, 3, 3],[3, 1, 3]]\n # make a dummy alignment history, which is the tiled step_ids\n step_ids = ops.convert_to_tensor(step_ids)\n alignment_length = array_ops.constant(10)\n alignment_history = tensor_array_ops.TensorArray(\n     size=0, dynamic_size=True, dtype=dtypes.float32)\n alignment_history = alignment_history.unstack(\n     math_ops.cast(array_ops.tile(array_ops.expand_dims(step_ids,-1), [1,1,1,alignment_length]),\n             dtypes.float32))\n\n sorted_history = _gather_tree_for_array(\n     alignment_history, parent_ids, sequence_length).stack()\n\n sorted_step_ids = beam_search_ops.gather_tree(\n     step_ids=step_ids, parent_ids=parent_ids,\n     sequence_length=sequence_length)\n\n sorted_step_ids = array_ops.tile(\n     array_ops.expand_dims(sorted_step_ids,-1),\n     [1,1,1,alignment_length])\n sorted_step_ids = math_ops.cast(\n     clip_ops.clip_by_value(\n         sorted_step_ids,\n         0,\n         math_ops.reduce_max(step_ids)),\n     dtypes.float32)\n # the sorted history should be the same as the tiled sorted step idprint(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"260616654\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13312\" href=\"https://github.com/tensorflow/tensorflow/pull/13312#issuecomment-335062833\">#13312 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim0RRKMp8tXCR7InNebWmvZmpKLn4ks5sqZmogaJpZM4PkNmd\">https://github.com/notifications/unsubscribe-auth/ABtim0RRKMp8tXCR7InNebWmvZmpKLn4ks5sqZmogaJpZM4PkNmd</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Steven, very nice!  Guillaume, are you interested in incorporating this\ninto your PR?\n\u2026\nOn Sun, Oct 8, 2017 at 8:57 PM, Steven Ding ***@***.***> wrote:\n def _gather_tree_for_array(t, parent_ids, sequence_length):\n   \"\"\"  Convert tensor array that contains tensor with unsorted beams.  Each element has shape [batch*beam, depth]  Return sorted beam tensor array contains elements of shape  [batch, beam, depth]  Padding has a value of 0.  \"\"\"\n   # (time, batch, beam)\n   time = array_ops.shape(parent_ids)[0]\n   batch = array_ops.shape(parent_ids)[1]\n   beam = array_ops.shape(parent_ids)[2]\n   sorted_indx = array_ops.expand_dims(\n       array_ops.expand_dims(math_ops.range(beam), 0), 0)\n   sorted_indx = array_ops.tile(sorted_indx, [time, batch, 1])\n   sorted_beams = beam_search_ops.gather_tree(\n       step_ids=sorted_indx, parent_ids=parent_ids,\n       sequence_length=sequence_length)\n   # all index of beam increase by 1. (-1 padding index becomes 0)\n   # 0 indicates a padding beam which has zero alignments.\n   sorted_beams = sorted_beams + 1\n\n   def collect(collector, i):\n       # concate a padding alignment (zeros) for each batch\n       value = t.read(i)\n       padding = array_ops.zeros([batch, 1, array_ops.shape(value)[-1]])\n       value = array_ops.concat([padding, value], axis=1)\n\n       # collect value according to the sorted_beams\n       # cannot use gather_helper as we increase the beam size by one\n       # so the final_shape has a different beam size\n       range_ = array_ops.expand_dims(\n           math_ops.range(batch) * (beam + 1), 1)\n       gather_indices = array_ops.reshape(sorted_beams[i] + range_, [-1])\n       sorted_value = array_ops.gather(\n           array_ops.reshape(\n               value, [batch * (beam + 1), -1]),\n           gather_indices)\n       sorted_value = array_ops.reshape(sorted_value, [batch, beam, -1])\n\n       collector = collector.write(i, sorted_value)\n       return collector, i + 1\n\n   collected = tensor_array_ops.TensorArray(\n       size=t.size(), dynamic_size=True, dtype=dtypes.float32)\n   collected, _ = control_flow_ops.while_loop(\n       lambda _, i: i < t.size(),\n       collect,\n       loop_vars=(collected, 0),\n       parallel_iterations=1)\n   return collected\n\n test code: (extended from testGatherTree)\n\n step_ids = _transpose_batch_time(\n     [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]],\n     [[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\n parent_ids = _transpose_batch_time(\n     [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]],\n     [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]])\n sequence_length = [[3, 3, 3],[3, 1, 3]]\n # make a dummy alignment history, which is the tiled step_ids\n step_ids = ops.convert_to_tensor(step_ids)\n alignment_length = array_ops.constant(10)\n alignment_history = tensor_array_ops.TensorArray(\n     size=0, dynamic_size=True, dtype=dtypes.float32)\n alignment_history = alignment_history.unstack(\n     math_ops.cast(array_ops.tile(array_ops.expand_dims(step_ids,-1), [1,1,1,alignment_length]),\n             dtypes.float32))\n\n sorted_history = _gather_tree_for_array(\n     alignment_history, parent_ids, sequence_length).stack()\n\n sorted_step_ids = beam_search_ops.gather_tree(\n     step_ids=step_ids, parent_ids=parent_ids,\n     sequence_length=sequence_length)\n\n sorted_step_ids = array_ops.tile(\n     array_ops.expand_dims(sorted_step_ids,-1),\n     [1,1,1,alignment_length])\n sorted_step_ids = math_ops.cast(\n     clip_ops.clip_by_value(\n         sorted_step_ids,\n         0,\n         math_ops.reduce_max(step_ids)),\n     dtypes.float32)\n # the sorted history should be the same as the tiled sorted step idprint(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n <#13312 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim0RRKMp8tXCR7InNebWmvZmpKLn4ks5sqZmogaJpZM4PkNmd>\n .", "body": "Steven, very nice!  Guillaume, are you interested in incorporating this\ninto your PR?\n\nOn Sun, Oct 8, 2017 at 8:57 PM, Steven Ding <notifications@github.com>\nwrote:\n\n> def _gather_tree_for_array(t, parent_ids, sequence_length):\n>   \"\"\"  Convert tensor array that contains tensor with unsorted beams.  Each element has shape [batch*beam, depth]  Return sorted beam tensor array contains elements of shape  [batch, beam, depth]  Padding has a value of 0.  \"\"\"\n>   # (time, batch, beam)\n>   time = array_ops.shape(parent_ids)[0]\n>   batch = array_ops.shape(parent_ids)[1]\n>   beam = array_ops.shape(parent_ids)[2]\n>   sorted_indx = array_ops.expand_dims(\n>       array_ops.expand_dims(math_ops.range(beam), 0), 0)\n>   sorted_indx = array_ops.tile(sorted_indx, [time, batch, 1])\n>   sorted_beams = beam_search_ops.gather_tree(\n>       step_ids=sorted_indx, parent_ids=parent_ids,\n>       sequence_length=sequence_length)\n>   # all index of beam increase by 1. (-1 padding index becomes 0)\n>   # 0 indicates a padding beam which has zero alignments.\n>   sorted_beams = sorted_beams + 1\n>\n>   def collect(collector, i):\n>       # concate a padding alignment (zeros) for each batch\n>       value = t.read(i)\n>       padding = array_ops.zeros([batch, 1, array_ops.shape(value)[-1]])\n>       value = array_ops.concat([padding, value], axis=1)\n>\n>       # collect value according to the sorted_beams\n>       # cannot use gather_helper as we increase the beam size by one\n>       # so the final_shape has a different beam size\n>       range_ = array_ops.expand_dims(\n>           math_ops.range(batch) * (beam + 1), 1)\n>       gather_indices = array_ops.reshape(sorted_beams[i] + range_, [-1])\n>       sorted_value = array_ops.gather(\n>           array_ops.reshape(\n>               value, [batch * (beam + 1), -1]),\n>           gather_indices)\n>       sorted_value = array_ops.reshape(sorted_value, [batch, beam, -1])\n>\n>       collector = collector.write(i, sorted_value)\n>       return collector, i + 1\n>\n>   collected = tensor_array_ops.TensorArray(\n>       size=t.size(), dynamic_size=True, dtype=dtypes.float32)\n>   collected, _ = control_flow_ops.while_loop(\n>       lambda _, i: i < t.size(),\n>       collect,\n>       loop_vars=(collected, 0),\n>       parallel_iterations=1)\n>   return collected\n>\n> test code: (extended from testGatherTree)\n>\n> step_ids = _transpose_batch_time(\n>     [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]],\n>     [[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\n> parent_ids = _transpose_batch_time(\n>     [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]],\n>     [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]])\n> sequence_length = [[3, 3, 3],[3, 1, 3]]\n> # make a dummy alignment history, which is the tiled step_ids\n> step_ids = ops.convert_to_tensor(step_ids)\n> alignment_length = array_ops.constant(10)\n> alignment_history = tensor_array_ops.TensorArray(\n>     size=0, dynamic_size=True, dtype=dtypes.float32)\n> alignment_history = alignment_history.unstack(\n>     math_ops.cast(array_ops.tile(array_ops.expand_dims(step_ids,-1), [1,1,1,alignment_length]),\n>             dtypes.float32))\n>\n> sorted_history = _gather_tree_for_array(\n>     alignment_history, parent_ids, sequence_length).stack()\n>\n> sorted_step_ids = beam_search_ops.gather_tree(\n>     step_ids=step_ids, parent_ids=parent_ids,\n>     sequence_length=sequence_length)\n>\n> sorted_step_ids = array_ops.tile(\n>     array_ops.expand_dims(sorted_step_ids,-1),\n>     [1,1,1,alignment_length])\n> sorted_step_ids = math_ops.cast(\n>     clip_ops.clip_by_value(\n>         sorted_step_ids,\n>         0,\n>         math_ops.reduce_max(step_ids)),\n>     dtypes.float32)\n> # the sorted history should be the same as the tiled sorted step idprint(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13312#issuecomment-335062833>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0RRKMp8tXCR7InNebWmvZmpKLn4ks5sqZmogaJpZM4PkNmd>\n> .\n>\n"}