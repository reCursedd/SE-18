{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/158392952", "pull_request_review_id": 85199928, "id": 158392952, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODM5Mjk1Mg==", "diff_hunk": "@@ -121,10 +121,69 @@ def tile_batch(t, multiplier, name=None):\n     return nest.map_structure(lambda t_: _tile_batch(t_, multiplier), t)\n \n \n+def gather_tree_from_array(t, parent_ids, sequence_length):\n+  \"\"\"Calculates the full beams for `TensorArray`s.\n+\n+  Args:\n+    t: A `TensorArray` of size `max_time` that contains `Tensor`s of shape\n+      `[batch_size, beam_width, depth]`.\n+    parent_ids: The parent ids of shape `[max_time, batch_size, beam_width]`.\n+    sequence_length: The sequence length of shape `[batch_size, beam_width]`.\n+\n+  Returns:\n+    A `TensorArray` of the same size and type as `t` and where beams are sorted\n+    in each `Tensor` according to `parent_ids`.\n+  \"\"\"\n+  max_time = array_ops.shape(parent_ids)[0]\n+  batch_size = array_ops.shape(parent_ids)[1]\n+  beam_width = array_ops.shape(parent_ids)[2]\n+\n+  # Generate beam ids that will be reordered by gather_tree.\n+  beam_ids = array_ops.expand_dims(\n+      array_ops.expand_dims(math_ops.range(beam_width), 0), 0)\n+  beam_ids = array_ops.tile(beam_ids, [max_time, batch_size, 1])\n+\n+  mask = array_ops.sequence_mask(\n+      sequence_length, maxlen=max_time, dtype=dtypes.int32)\n+  mask = array_ops.transpose(mask, perm=[2, 0, 1])\n+\n+  # Use beam_width + 1 to mark the end of beam.\n+  masked_beam_ids = (beam_ids * mask) + (1 - mask) * (beam_width + 1)\n+\n+  max_sequence_lengths = math_ops.to_int32(\n+      math_ops.reduce_max(sequence_length, axis=1))\n+  sorted_beam_ids = beam_search_ops.gather_tree(\n+      step_ids=masked_beam_ids,\n+      parent_ids=parent_ids,\n+      max_sequence_lengths=max_sequence_lengths,\n+      end_token=beam_width + 1)\n+\n+  # For out of range steps, simply copy the same beam.\n+  sorted_beam_ids = array_ops.where(\n+      math_ops.cast(mask, dtypes.bool), x=sorted_beam_ids, y=beam_ids)\n+\n+  # Gather from each tensor in t according to sorted_beam_ids.\n+  def _collect(collector, i):\n+    gathered = _tensor_gather_helper(\n+        gather_indices=sorted_beam_ids[i],\n+        gather_from=t.read(i),\n+        batch_size=batch_size,\n+        range_size=beam_width,\n+        gather_shape=[batch_size * beam_width, -1])\n+    return collector.write(i, gathered), i + 1\n+\n+  collected = tensor_array_ops.TensorArray(\n+      t.dtype, size=t.size(), dynamic_size=False)\n+  collected, _ = control_flow_ops.while_loop(\n+      lambda _, i: i < t.size(),", "path": "tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py", "position": null, "original_position": 58, "commit_id": "d3eb228f1db2d60caf380833684944ce12203634", "original_commit_id": "c50d47eda64e35dacf47d01e204a756be7fca9b7", "user": {"login": "steven-hh-ding", "id": 8474647, "node_id": "MDQ6VXNlcjg0NzQ2NDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8474647?v=4", "gravatar_id": "", "url": "https://api.github.com/users/steven-hh-ding", "html_url": "https://github.com/steven-hh-ding", "followers_url": "https://api.github.com/users/steven-hh-ding/followers", "following_url": "https://api.github.com/users/steven-hh-ding/following{/other_user}", "gists_url": "https://api.github.com/users/steven-hh-ding/gists{/gist_id}", "starred_url": "https://api.github.com/users/steven-hh-ding/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/steven-hh-ding/subscriptions", "organizations_url": "https://api.github.com/users/steven-hh-ding/orgs", "repos_url": "https://api.github.com/users/steven-hh-ding/repos", "events_url": "https://api.github.com/users/steven-hh-ding/events{/privacy}", "received_events_url": "https://api.github.com/users/steven-hh-ding/received_events", "type": "User", "site_admin": false}, "body": "if i remember the shapes here correctly:\r\n\r\n\r\n```python\r\ntime = 3\r\nbatch = 4\r\nbeam = 5\r\ndepth = 6\r\n\r\n# [time, batch, beam, depth]\r\na = tf.constant(np.arange(time * batch * beam * depth), shape=[time, batch, beam, depth], dtype=tf.int64)\r\n\r\n# [time, batch, beam] (reverse beam as example)\r\nbeam_ids = tf.constant(np.flip(np.arange(beam), 0), shape=[1,1,beam], dtype=tf.int64)\r\nbeam_ids = tf.tile(beam_ids, [3,4,1])\r\n\r\n# we need [time, batch, beam, 3]\r\n# assume beam_ids > -1\r\nt_bt_ind = tf.where(tf.ones_like(beam_ids) > -1)[:,:-1]\r\nt_bt_ind = tf.reshape(t_bt_ind, [time,batch,beam,-1])\r\nt_bt_be_ind = tf.concat([t_bt_ind, tf.expand_dims(beam_ids, -1)], -1)\r\n\r\n# gather\r\ntf.gather_nd(a, t_bt_be_ind)\r\n\r\n```\r\n\r\nPlease double-check. Another way is to combine range and tile; but it is hardly readable. We use this method a lot in our code (tf.where(constant)), it looks okay. ebrevdo may better comment on this. Thanks!", "created_at": "2017-12-21T22:28:45Z", "updated_at": "2018-03-17T07:09:51Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13312#discussion_r158392952", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13312", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/158392952"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13312#discussion_r158392952"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13312"}}, "body_html": "<p>if i remember the shapes here correctly:</p>\n<div class=\"highlight highlight-source-python\"><pre>time <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\nbatch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\nbeam <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\ndepth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">6</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [time, batch, beam, depth]</span>\na <span class=\"pl-k\">=</span> tf.constant(np.arange(time <span class=\"pl-k\">*</span> batch <span class=\"pl-k\">*</span> beam <span class=\"pl-k\">*</span> depth), <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[time, batch, beam, depth], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [time, batch, beam] (reverse beam as example)</span>\nbeam_ids <span class=\"pl-k\">=</span> tf.constant(np.flip(np.arange(beam), <span class=\"pl-c1\">0</span>), <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,beam], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)\nbeam_ids <span class=\"pl-k\">=</span> tf.tile(beam_ids, [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">1</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> we need [time, batch, beam, 3]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> assume beam_ids &gt; -1</span>\nt_bt_ind <span class=\"pl-k\">=</span> tf.where(tf.ones_like(beam_ids) <span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)[:,:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\nt_bt_ind <span class=\"pl-k\">=</span> tf.reshape(t_bt_ind, [time,batch,beam,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\nt_bt_be_ind <span class=\"pl-k\">=</span> tf.concat([t_bt_ind, tf.expand_dims(beam_ids, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> gather</span>\ntf.gather_nd(a, t_bt_be_ind)\n</pre></div>\n<p>Please double-check. Another way is to combine range and tile; but it is hardly readable. We use this method a lot in our code (tf.where(constant)), it looks okay. ebrevdo may better comment on this. Thanks!</p>", "body_text": "if i remember the shapes here correctly:\ntime = 3\nbatch = 4\nbeam = 5\ndepth = 6\n\n# [time, batch, beam, depth]\na = tf.constant(np.arange(time * batch * beam * depth), shape=[time, batch, beam, depth], dtype=tf.int64)\n\n# [time, batch, beam] (reverse beam as example)\nbeam_ids = tf.constant(np.flip(np.arange(beam), 0), shape=[1,1,beam], dtype=tf.int64)\nbeam_ids = tf.tile(beam_ids, [3,4,1])\n\n# we need [time, batch, beam, 3]\n# assume beam_ids > -1\nt_bt_ind = tf.where(tf.ones_like(beam_ids) > -1)[:,:-1]\nt_bt_ind = tf.reshape(t_bt_ind, [time,batch,beam,-1])\nt_bt_be_ind = tf.concat([t_bt_ind, tf.expand_dims(beam_ids, -1)], -1)\n\n# gather\ntf.gather_nd(a, t_bt_be_ind)\n\nPlease double-check. Another way is to combine range and tile; but it is hardly readable. We use this method a lot in our code (tf.where(constant)), it looks okay. ebrevdo may better comment on this. Thanks!", "in_reply_to_id": 156848356}