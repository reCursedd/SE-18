{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/174217827", "html_url": "https://github.com/tensorflow/tensorflow/issues/838#issuecomment-174217827", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/838", "id": 174217827, "node_id": "MDEyOklzc3VlQ29tbWVudDE3NDIxNzgyNw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-23T20:19:01Z", "updated_at": "2016-01-23T20:19:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>After the patch I get 30ms per step on my CPU (HP Z420) and 140ms per step on GPU (K40c). Note that the patch only pins 2 constant ops to the CPU. It looks like there's a \"Cast\" op which is part of input pipeline that ends up on GPU, so that slows things down a lot. In general I found it best to pin the whole input pipeline to CPU manually.</p>\n<p>However, even if you fixed this, this network is too small to benefit much from GPU. Using CPU-only version, the \"QueueDeqeueMany\" op takes 25ms out of 30ms total, so you have 5 ms spent on \"actual computation\", so even if you made the computation run 5x faster, it would have negligible effect on step size.</p>\n<p>third_party/tensorflow/models/image/mnist/convolutional is much better example for utilizing GPU, I get 8x speed-up when using GPU. Also, cifar10_train example has a better tuned input pipeline</p>\n<p>Regarding other questions:<br>\nif you turn on verbose logging in bazel, you'll see some messages every time tensors get copied between devices here: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L41\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L41</a></p>\n<p>One way to find if an op has GPU implementation is to search for \"REGISTER_KERNEL_BUILDER\" macros, like here <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc#L217\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc#L217</a></p>", "body_text": "After the patch I get 30ms per step on my CPU (HP Z420) and 140ms per step on GPU (K40c). Note that the patch only pins 2 constant ops to the CPU. It looks like there's a \"Cast\" op which is part of input pipeline that ends up on GPU, so that slows things down a lot. In general I found it best to pin the whole input pipeline to CPU manually.\nHowever, even if you fixed this, this network is too small to benefit much from GPU. Using CPU-only version, the \"QueueDeqeueMany\" op takes 25ms out of 30ms total, so you have 5 ms spent on \"actual computation\", so even if you made the computation run 5x faster, it would have negligible effect on step size.\nthird_party/tensorflow/models/image/mnist/convolutional is much better example for utilizing GPU, I get 8x speed-up when using GPU. Also, cifar10_train example has a better tuned input pipeline\nRegarding other questions:\nif you turn on verbose logging in bazel, you'll see some messages every time tensors get copied between devices here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L41\nOne way to find if an op has GPU implementation is to search for \"REGISTER_KERNEL_BUILDER\" macros, like here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc#L217", "body": "After the patch I get 30ms per step on my CPU (HP Z420) and 140ms per step on GPU (K40c). Note that the patch only pins 2 constant ops to the CPU. It looks like there's a \"Cast\" op which is part of input pipeline that ends up on GPU, so that slows things down a lot. In general I found it best to pin the whole input pipeline to CPU manually.\n\nHowever, even if you fixed this, this network is too small to benefit much from GPU. Using CPU-only version, the \"QueueDeqeueMany\" op takes 25ms out of 30ms total, so you have 5 ms spent on \"actual computation\", so even if you made the computation run 5x faster, it would have negligible effect on step size.\n\nthird_party/tensorflow/models/image/mnist/convolutional is much better example for utilizing GPU, I get 8x speed-up when using GPU. Also, cifar10_train example has a better tuned input pipeline\n\nRegarding other questions:\nif you turn on verbose logging in bazel, you'll see some messages every time tensors get copied between devices here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L41\n\nOne way to find if an op has GPU implementation is to search for \"REGISTER_KERNEL_BUILDER\" macros, like here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc#L217\n"}