{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309820582", "html_url": "https://github.com/tensorflow/tensorflow/issues/5193#issuecomment-309820582", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5193", "id": 309820582, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTgyMDU4Mg==", "user": {"login": "qingyangqing", "id": 25918282, "node_id": "MDQ6VXNlcjI1OTE4Mjgy", "avatar_url": "https://avatars1.githubusercontent.com/u/25918282?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qingyangqing", "html_url": "https://github.com/qingyangqing", "followers_url": "https://api.github.com/users/qingyangqing/followers", "following_url": "https://api.github.com/users/qingyangqing/following{/other_user}", "gists_url": "https://api.github.com/users/qingyangqing/gists{/gist_id}", "starred_url": "https://api.github.com/users/qingyangqing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qingyangqing/subscriptions", "organizations_url": "https://api.github.com/users/qingyangqing/orgs", "repos_url": "https://api.github.com/users/qingyangqing/repos", "events_url": "https://api.github.com/users/qingyangqing/events{/privacy}", "received_events_url": "https://api.github.com/users/qingyangqing/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-20T16:52:35Z", "updated_at": "2017-06-20T16:52:35Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> , I am also looking forward to having the support from tf to accelerate sparse matrix computations. sparse_matmul op is heavily used in my case, but this sparse op cannot compete with the normal matmul op on CPU with SSE support, not to say on GPU. I also find sparse_matmul op cannot run on GPU.</p>\n<p>I can say my motivation to have sparse matrix support. For neural network, many papers have proven neuron weights can be pruned, thus the matmul turns to be sparse matrix multiply, which gives speedup space and saves energy consumption.</p>", "body_text": "Hi @aselle , I am also looking forward to having the support from tf to accelerate sparse matrix computations. sparse_matmul op is heavily used in my case, but this sparse op cannot compete with the normal matmul op on CPU with SSE support, not to say on GPU. I also find sparse_matmul op cannot run on GPU.\nI can say my motivation to have sparse matrix support. For neural network, many papers have proven neuron weights can be pruned, thus the matmul turns to be sparse matrix multiply, which gives speedup space and saves energy consumption.", "body": "Hi @aselle , I am also looking forward to having the support from tf to accelerate sparse matrix computations. sparse_matmul op is heavily used in my case, but this sparse op cannot compete with the normal matmul op on CPU with SSE support, not to say on GPU. I also find sparse_matmul op cannot run on GPU. \r\n\r\nI can say my motivation to have sparse matrix support. For neural network, many papers have proven neuron weights can be pruned, thus the matmul turns to be sparse matrix multiply, which gives speedup space and saves energy consumption. "}