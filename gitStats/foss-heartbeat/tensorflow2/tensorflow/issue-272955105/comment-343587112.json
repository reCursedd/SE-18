{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/343587112", "html_url": "https://github.com/tensorflow/tensorflow/issues/14451#issuecomment-343587112", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14451", "id": 343587112, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzU4NzExMg==", "user": {"login": "kmkolasinski", "id": 10145864, "node_id": "MDQ6VXNlcjEwMTQ1ODY0", "avatar_url": "https://avatars3.githubusercontent.com/u/10145864?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kmkolasinski", "html_url": "https://github.com/kmkolasinski", "followers_url": "https://api.github.com/users/kmkolasinski/followers", "following_url": "https://api.github.com/users/kmkolasinski/following{/other_user}", "gists_url": "https://api.github.com/users/kmkolasinski/gists{/gist_id}", "starred_url": "https://api.github.com/users/kmkolasinski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kmkolasinski/subscriptions", "organizations_url": "https://api.github.com/users/kmkolasinski/orgs", "repos_url": "https://api.github.com/users/kmkolasinski/repos", "events_url": "https://api.github.com/users/kmkolasinski/events{/privacy}", "received_events_url": "https://api.github.com/users/kmkolasinski/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-10T21:10:35Z", "updated_at": "2017-11-10T21:11:50Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> , I've checked it, and it works perfectly <g-emoji class=\"g-emoji\" alias=\"+1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44d.png\">\ud83d\udc4d</g-emoji>  Thank you very much :)<br>\nHere is a sample code which I'm going to try in my image classification problem. I'm posting it here in case if someone will want to experiment with the method.  The code is oversampling low frequent classes and undersampling high frequent classes, where <code>class_target_prob</code> is just uniform distribution in my case. I wanted to check conclusions from recent manuscript <a href=\"https://arxiv.org/abs/1710.05381\" rel=\"nofollow\">A systematic study of the class imbalance problem in convolutional neural networks</a></p>\n<p>Here is the code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> sampling parameters</span>\noversampling_coef <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.9</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> if equal to 0 then oversample_classes() always returns 1</span>\nundersampling_coef <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.5</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> if equal to 0 then oversampling_filter() always returns True</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">oversample_classes</span>(<span class=\"pl-smi\">example</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Returns the number of copies of given example</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    class_prob <span class=\"pl-k\">=</span> example[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class_prob<span class=\"pl-pds\">'</span></span>]\n    class_target_prob <span class=\"pl-k\">=</span> example[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class_target_prob<span class=\"pl-pds\">'</span></span>]\n    prob_ratio <span class=\"pl-k\">=</span> tf.cast(class_target_prob<span class=\"pl-k\">/</span>class_prob, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> soften ratio is oversampling_coef==0 we recover original distribution</span>\n    prob_ratio <span class=\"pl-k\">=</span> prob_ratio <span class=\"pl-k\">**</span> oversampling_coef \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for classes with probability higher than class_target_prob we</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> want to return 1</span>\n    prob_ratio <span class=\"pl-k\">=</span> tf.maximum(prob_ratio, <span class=\"pl-c1\">1</span>) \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for low probability classes this number will be very large</span>\n    repeat_count <span class=\"pl-k\">=</span> tf.floor(prob_ratio)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> prob_ratio can be e.g 1.9 which means that there is still 90%</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> of change that we should return 2 instead of 1</span>\n    repeat_residual <span class=\"pl-k\">=</span> prob_ratio <span class=\"pl-k\">-</span> repeat_count <span class=\"pl-c\"><span class=\"pl-c\">#</span> a number between 0-1</span>\n    residual_acceptance <span class=\"pl-k\">=</span> tf.less_equal(\n                        tf.random_uniform([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32), repeat_residual\n    )\n    \n    residual_acceptance <span class=\"pl-k\">=</span> tf.cast(residual_acceptance, tf.int64)\n    repeat_count <span class=\"pl-k\">=</span> tf.cast(repeat_count, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)\n    \n    <span class=\"pl-k\">return</span> repeat_count <span class=\"pl-k\">+</span> residual_acceptance\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">undersampling_filter</span>(<span class=\"pl-smi\">example</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Computes if given example is rejected or not.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    class_prob <span class=\"pl-k\">=</span> example[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class_prob<span class=\"pl-pds\">'</span></span>]\n    class_target_prob <span class=\"pl-k\">=</span> example[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class_target_prob<span class=\"pl-pds\">'</span></span>]\n    prob_ratio <span class=\"pl-k\">=</span> tf.cast(class_target_prob<span class=\"pl-k\">/</span>class_prob, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    prob_ratio <span class=\"pl-k\">=</span> prob_ratio <span class=\"pl-k\">**</span> undersampling_coef\n    prob_ratio <span class=\"pl-k\">=</span> tf.minimum(prob_ratio, <span class=\"pl-c1\">1.0</span>)\n    \n    acceptance <span class=\"pl-k\">=</span> tf.less_equal(tf.random_uniform([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32), prob_ratio)\n\n    <span class=\"pl-k\">return</span> acceptance\n\n\ndataset <span class=\"pl-k\">=</span> dataset.flat_map(\n    <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: tf.data.Dataset.from_tensors(x).repeat(oversample_classes(x))\n)\n\ndataset <span class=\"pl-k\">=</span> dataset.filter(undersampling_filter)\n\ndataset <span class=\"pl-k\">=</span> dataset.repeat(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\ndataset <span class=\"pl-k\">=</span> dataset.shuffle(<span class=\"pl-c1\">2048</span>)\ndataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">32</span>)\n\nsess.run(tf.global_variables_initializer())\n\niterator <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator()\nnext_element <span class=\"pl-k\">=</span> iterator.get_next()</pre></div>\n<p>I think this issue can be closed now. Thanks again <g-emoji class=\"g-emoji\" alias=\"+1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44d.png\">\ud83d\udc4d</g-emoji></p>", "body_text": "Hi @mrry , I've checked it, and it works perfectly \ud83d\udc4d  Thank you very much :)\nHere is a sample code which I'm going to try in my image classification problem. I'm posting it here in case if someone will want to experiment with the method.  The code is oversampling low frequent classes and undersampling high frequent classes, where class_target_prob is just uniform distribution in my case. I wanted to check conclusions from recent manuscript A systematic study of the class imbalance problem in convolutional neural networks\nHere is the code:\n# sampling parameters\noversampling_coef = 0.9 # if equal to 0 then oversample_classes() always returns 1\nundersampling_coef = 0.5 # if equal to 0 then oversampling_filter() always returns True\n\ndef oversample_classes(example):\n    \"\"\"\n    Returns the number of copies of given example\n    \"\"\"\n    class_prob = example['class_prob']\n    class_target_prob = example['class_target_prob']\n    prob_ratio = tf.cast(class_target_prob/class_prob, dtype=tf.float32)\n    # soften ratio is oversampling_coef==0 we recover original distribution\n    prob_ratio = prob_ratio ** oversampling_coef \n    # for classes with probability higher than class_target_prob we\n    # want to return 1\n    prob_ratio = tf.maximum(prob_ratio, 1) \n    # for low probability classes this number will be very large\n    repeat_count = tf.floor(prob_ratio)\n    # prob_ratio can be e.g 1.9 which means that there is still 90%\n    # of change that we should return 2 instead of 1\n    repeat_residual = prob_ratio - repeat_count # a number between 0-1\n    residual_acceptance = tf.less_equal(\n                        tf.random_uniform([], dtype=tf.float32), repeat_residual\n    )\n    \n    residual_acceptance = tf.cast(residual_acceptance, tf.int64)\n    repeat_count = tf.cast(repeat_count, dtype=tf.int64)\n    \n    return repeat_count + residual_acceptance\n\n\ndef undersampling_filter(example):\n    \"\"\"\n    Computes if given example is rejected or not.\n    \"\"\"\n    class_prob = example['class_prob']\n    class_target_prob = example['class_target_prob']\n    prob_ratio = tf.cast(class_target_prob/class_prob, dtype=tf.float32)\n    prob_ratio = prob_ratio ** undersampling_coef\n    prob_ratio = tf.minimum(prob_ratio, 1.0)\n    \n    acceptance = tf.less_equal(tf.random_uniform([], dtype=tf.float32), prob_ratio)\n\n    return acceptance\n\n\ndataset = dataset.flat_map(\n    lambda x: tf.data.Dataset.from_tensors(x).repeat(oversample_classes(x))\n)\n\ndataset = dataset.filter(undersampling_filter)\n\ndataset = dataset.repeat(-1)\ndataset = dataset.shuffle(2048)\ndataset = dataset.batch(32)\n\nsess.run(tf.global_variables_initializer())\n\niterator = dataset.make_one_shot_iterator()\nnext_element = iterator.get_next()\nI think this issue can be closed now. Thanks again \ud83d\udc4d", "body": "Hi @mrry , I've checked it, and it works perfectly :+1:  Thank you very much :) \r\nHere is a sample code which I'm going to try in my image classification problem. I'm posting it here in case if someone will want to experiment with the method.  The code is oversampling low frequent classes and undersampling high frequent classes, where `class_target_prob` is just uniform distribution in my case. I wanted to check conclusions from recent manuscript [A systematic study of the class imbalance problem in convolutional neural networks](https://arxiv.org/abs/1710.05381)\r\n\r\nHere is the code:\r\n\r\n```python\r\n# sampling parameters\r\noversampling_coef = 0.9 # if equal to 0 then oversample_classes() always returns 1\r\nundersampling_coef = 0.5 # if equal to 0 then oversampling_filter() always returns True\r\n\r\ndef oversample_classes(example):\r\n    \"\"\"\r\n    Returns the number of copies of given example\r\n    \"\"\"\r\n    class_prob = example['class_prob']\r\n    class_target_prob = example['class_target_prob']\r\n    prob_ratio = tf.cast(class_target_prob/class_prob, dtype=tf.float32)\r\n    # soften ratio is oversampling_coef==0 we recover original distribution\r\n    prob_ratio = prob_ratio ** oversampling_coef \r\n    # for classes with probability higher than class_target_prob we\r\n    # want to return 1\r\n    prob_ratio = tf.maximum(prob_ratio, 1) \r\n    # for low probability classes this number will be very large\r\n    repeat_count = tf.floor(prob_ratio)\r\n    # prob_ratio can be e.g 1.9 which means that there is still 90%\r\n    # of change that we should return 2 instead of 1\r\n    repeat_residual = prob_ratio - repeat_count # a number between 0-1\r\n    residual_acceptance = tf.less_equal(\r\n                        tf.random_uniform([], dtype=tf.float32), repeat_residual\r\n    )\r\n    \r\n    residual_acceptance = tf.cast(residual_acceptance, tf.int64)\r\n    repeat_count = tf.cast(repeat_count, dtype=tf.int64)\r\n    \r\n    return repeat_count + residual_acceptance\r\n\r\n\r\ndef undersampling_filter(example):\r\n    \"\"\"\r\n    Computes if given example is rejected or not.\r\n    \"\"\"\r\n    class_prob = example['class_prob']\r\n    class_target_prob = example['class_target_prob']\r\n    prob_ratio = tf.cast(class_target_prob/class_prob, dtype=tf.float32)\r\n    prob_ratio = prob_ratio ** undersampling_coef\r\n    prob_ratio = tf.minimum(prob_ratio, 1.0)\r\n    \r\n    acceptance = tf.less_equal(tf.random_uniform([], dtype=tf.float32), prob_ratio)\r\n\r\n    return acceptance\r\n\r\n\r\ndataset = dataset.flat_map(\r\n    lambda x: tf.data.Dataset.from_tensors(x).repeat(oversample_classes(x))\r\n)\r\n\r\ndataset = dataset.filter(undersampling_filter)\r\n\r\ndataset = dataset.repeat(-1)\r\ndataset = dataset.shuffle(2048)\r\ndataset = dataset.batch(32)\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n```\r\n\r\nI think this issue can be closed now. Thanks again :+1: "}