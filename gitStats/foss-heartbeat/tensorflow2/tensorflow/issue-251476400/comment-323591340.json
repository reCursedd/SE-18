{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323591340", "html_url": "https://github.com/tensorflow/tensorflow/issues/12425#issuecomment-323591340", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12425", "id": 323591340, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzU5MTM0MA==", "user": {"login": "arita37", "id": 18707623, "node_id": "MDQ6VXNlcjE4NzA3NjIz", "avatar_url": "https://avatars3.githubusercontent.com/u/18707623?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arita37", "html_url": "https://github.com/arita37", "followers_url": "https://api.github.com/users/arita37/followers", "following_url": "https://api.github.com/users/arita37/following{/other_user}", "gists_url": "https://api.github.com/users/arita37/gists{/gist_id}", "starred_url": "https://api.github.com/users/arita37/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arita37/subscriptions", "organizations_url": "https://api.github.com/users/arita37/orgs", "repos_url": "https://api.github.com/users/arita37/repos", "events_url": "https://api.github.com/users/arita37/events{/privacy}", "received_events_url": "https://api.github.com/users/arita37/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-20T15:15:48Z", "updated_at": "2017-08-20T15:15:48Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Even this is GPU (TF GPu) is available,\nCPU will be used ?\n\nIn my case, only GPU seems being used for training.....\n\n\n\n\n\nOn 20 Aug 2017, at 23:58, Changho Hwang &lt;notifications@github.com&gt; wrote:\n\nTensorFlow uses every available CPUs as if they are unified all together as single CPU, which means that there is no such device like '/cpu:1' or '/cpu:2', only '/cpu:0'is legal. You cannot distinguish each CPU over TensorFlow as far as I know, and maybe, you don't need to in most cases. You can just write:\n\nwith tf.device('/cpu:0'):\n    for i in range(num_cpus):\n        outputs.append(tf.matmul(inputs[i], b))\nthen it will use all available CPUs automatically.\n\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.</div>", "body_text": "Even this is GPU (TF GPu) is available,\nCPU will be used ?\n\nIn my case, only GPU seems being used for training.....\n\n\n\n\n\nOn 20 Aug 2017, at 23:58, Changho Hwang <notifications@github.com> wrote:\n\nTensorFlow uses every available CPUs as if they are unified all together as single CPU, which means that there is no such device like '/cpu:1' or '/cpu:2', only '/cpu:0'is legal. You cannot distinguish each CPU over TensorFlow as far as I know, and maybe, you don't need to in most cases. You can just write:\n\nwith tf.device('/cpu:0'):\n    for i in range(num_cpus):\n        outputs.append(tf.matmul(inputs[i], b))\nthen it will use all available CPUs automatically.\n\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "body": "Even this is GPU (TF GPu) is available,\nCPU will be used ?\n\nIn my case, only GPU seems being used for training.....\n\n\n\n\n\nOn 20 Aug 2017, at 23:58, Changho Hwang <notifications@github.com> wrote:\n\nTensorFlow uses every available CPUs as if they are unified all together as single CPU, which means that there is no such device like '/cpu:1' or '/cpu:2', only '/cpu:0'is legal. You cannot distinguish each CPU over TensorFlow as far as I know, and maybe, you don't need to in most cases. You can just write:\n\nwith tf.device('/cpu:0'):\n    for i in range(num_cpus):\n        outputs.append(tf.matmul(inputs[i], b))\nthen it will use all available CPUs automatically.\n\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n"}