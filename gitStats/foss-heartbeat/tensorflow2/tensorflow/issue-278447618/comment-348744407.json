{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/348744407", "html_url": "https://github.com/tensorflow/tensorflow/issues/15038#issuecomment-348744407", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15038", "id": 348744407, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODc0NDQwNw==", "user": {"login": "gauss-clb", "id": 11674304, "node_id": "MDQ6VXNlcjExNjc0MzA0", "avatar_url": "https://avatars2.githubusercontent.com/u/11674304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gauss-clb", "html_url": "https://github.com/gauss-clb", "followers_url": "https://api.github.com/users/gauss-clb/followers", "following_url": "https://api.github.com/users/gauss-clb/following{/other_user}", "gists_url": "https://api.github.com/users/gauss-clb/gists{/gist_id}", "starred_url": "https://api.github.com/users/gauss-clb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gauss-clb/subscriptions", "organizations_url": "https://api.github.com/users/gauss-clb/orgs", "repos_url": "https://api.github.com/users/gauss-clb/repos", "events_url": "https://api.github.com/users/gauss-clb/events{/privacy}", "received_events_url": "https://api.github.com/users/gauss-clb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-03T06:45:54Z", "updated_at": "2017-12-03T07:30:11Z", "author_association": "NONE", "body_html": "<p>I just want to reproducing experiment of some paper, but I can't get good performance. I think it maybe the error of gradient due to <code>tf.float32</code>, such as <code>convolution</code> ops.  I wonder that small numerical error will accumulate and get bigger error, like butterfly effect.</p>\n<p>Like <a href=\"https://github.com/leongatys/DeepTextures\">DeepTextures</a>, the authors use <code>vgg19</code> of <code>caffe</code> to calculate gradient of loss and use <a href=\"https://github.com/leongatys/DeepTextures/blob/master/DeepImageSynthesis/ImageSyn.py#L54\">L-BFGS-B</a>(L-BFGS-B use float64 to optimize) of  <code>scipy.optimize</code> and get <a href=\"https://github.com/leongatys/DeepTextures/blob/master/Example.ipynb\">good performance</a>.</p>\n<p>But I try to use <code>vgg19</code> of <code>tensorflow.slim</code> pretrained on <code>imagenet</code> to calculate gardient and <code>L-BFGS-B</code>,  it converges so fast and the performance is so bad. Maybe I think the precision of gradient is not enough and it causes <code>L-BFGS-B</code> converge so fast.</p>\n<p>And I have tried to increase the <a href=\"https://github.com/leongatys/DeepTextures/blob/master/Example.ipynb\">weight</a>(tex_weights = [1e9,1e9,1e9,1e9,1e9]) of loss to <code>1e30</code> and use smaller learning rate, such as <code>1e-12</code>, so that gradient of loss becomes bigger and the effect of fractional part may become smaller.  And then I find that <code>L-BFGS-B</code> converges slower.  I'm not sure whether it is precision that causes the speed of convergence but I will try <code>tf.float64</code> if <code>tensorflow</code> support <code>tf.float64</code>.</p>", "body_text": "I just want to reproducing experiment of some paper, but I can't get good performance. I think it maybe the error of gradient due to tf.float32, such as convolution ops.  I wonder that small numerical error will accumulate and get bigger error, like butterfly effect.\nLike DeepTextures, the authors use vgg19 of caffe to calculate gradient of loss and use L-BFGS-B(L-BFGS-B use float64 to optimize) of  scipy.optimize and get good performance.\nBut I try to use vgg19 of tensorflow.slim pretrained on imagenet to calculate gardient and L-BFGS-B,  it converges so fast and the performance is so bad. Maybe I think the precision of gradient is not enough and it causes L-BFGS-B converge so fast.\nAnd I have tried to increase the weight(tex_weights = [1e9,1e9,1e9,1e9,1e9]) of loss to 1e30 and use smaller learning rate, such as 1e-12, so that gradient of loss becomes bigger and the effect of fractional part may become smaller.  And then I find that L-BFGS-B converges slower.  I'm not sure whether it is precision that causes the speed of convergence but I will try tf.float64 if tensorflow support tf.float64.", "body": "I just want to reproducing experiment of some paper, but I can't get good performance. I think it maybe the error of gradient due to `tf.float32`, such as `convolution` ops.  I wonder that small numerical error will accumulate and get bigger error, like butterfly effect.\r\n\r\nLike [DeepTextures](https://github.com/leongatys/DeepTextures), the authors use `vgg19` of `caffe` to calculate gradient of loss and use [L-BFGS-B](https://github.com/leongatys/DeepTextures/blob/master/DeepImageSynthesis/ImageSyn.py#L54)(L-BFGS-B use float64 to optimize) of  `scipy.optimize` and get [good performance](https://github.com/leongatys/DeepTextures/blob/master/Example.ipynb). \r\n\r\nBut I try to use `vgg19` of `tensorflow.slim` pretrained on `imagenet` to calculate gardient and `L-BFGS-B`,  it converges so fast and the performance is so bad. Maybe I think the precision of gradient is not enough and it causes `L-BFGS-B` converge so fast.\r\n\r\nAnd I have tried to increase the [weight](https://github.com/leongatys/DeepTextures/blob/master/Example.ipynb)(tex_weights = [1e9,1e9,1e9,1e9,1e9]) of loss to `1e30` and use smaller learning rate, such as `1e-12`, so that gradient of loss becomes bigger and the effect of fractional part may become smaller.  And then I find that `L-BFGS-B` converges slower.  I'm not sure whether it is precision that causes the speed of convergence but I will try `tf.float64` if `tensorflow` support `tf.float64`."}