{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404053923", "html_url": "https://github.com/tensorflow/tensorflow/issues/20613#issuecomment-404053923", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20613", "id": 404053923, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDA1MzkyMw==", "user": {"login": "jony0917", "id": 1837604, "node_id": "MDQ6VXNlcjE4Mzc2MDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1837604?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jony0917", "html_url": "https://github.com/jony0917", "followers_url": "https://api.github.com/users/jony0917/followers", "following_url": "https://api.github.com/users/jony0917/following{/other_user}", "gists_url": "https://api.github.com/users/jony0917/gists{/gist_id}", "starred_url": "https://api.github.com/users/jony0917/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jony0917/subscriptions", "organizations_url": "https://api.github.com/users/jony0917/orgs", "repos_url": "https://api.github.com/users/jony0917/repos", "events_url": "https://api.github.com/users/jony0917/events{/privacy}", "received_events_url": "https://api.github.com/users/jony0917/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-11T05:57:56Z", "updated_at": "2018-07-11T05:57:56Z", "author_association": "NONE", "body_html": "<p>This is not a bug, and yes , what is really output is dy_real/dx_real + i * dy_real/dx_imag for complex-valued functions y=f(x).</p>\n<p>I think the key to your question is the definition of gradient. let me explain it.</p>\n<ul>\n<li>for real-valued R -&gt; R funciton f:  y = f(x) , the gradient of f w.r.t x is grad f(x) = df/dx,</li>\n</ul>\n<p>Suppose what we aim to minimize a graph's output L, and L = z, z = z(y), y = y(x), we know that dz/dx = dz/dy * dy/dx,<br>\nso we can calculate gradient w.r.t any variant in this graph like this :</p>\n<pre><code>grad L(z) = dL/dz = 1\ngrad L(y) = dL/dy = dL/dz * dz/dy = grad L(z) * dz/dy\ngrad L(x) = dL/dx = dL/dy * dy/dx = grad L(y) * dy/dx\n</code></pre>\n<p>this is called the reversed calculation of gradients.</p>\n<p>But for complexed-valued function C -&gt; C: z = f(w),  if we define gradient f(w) = dz/dw, the chain rule can not apply directly.</p>\n<p>so, we change the definition:</p>\n<ul>\n<li>for complex-valued C -&gt; C function f: z = f(w), we define two gradient: the real gradient of f w.r.t w is grad_real f(w) = dz_real/dw_real + i * dz_real/dw_imag,<br>\nthe imaginary gradient of f w.r.t w is grad_imag f(w) = dz_imag/dw_real + i * dz_imag/dw_imag.</li>\n</ul>\n<p>Now, let's see how to calcualte the real gradient and the imag gradient.</p>\n<p>Similarly suppose we get a graph: L = z, z = z(y), y = y(x),  L,z,y,x are all complex variants. By the definition, we do it like this:</p>\n<pre><code>grad_real L(z) = (dz_real/dz_real + i * dz_real/dz_imag) = (1, 0)\ngrad_imag L(z) = (dz_imag/dz_real + i * dz_imag/dz_real) = (0, i)\n\ngrad_real L(y) = (dz_real/dy_real + i * dz_real/dy_imag) \n= (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag)\n\nAnd we know that for a analytic fucntion z = z(y) , wo get Cauchy\u2013Riemann equations:\ndz_real/dy_real = dz_imag/dy_imag\ndz_real/dy_imag = - dz_imag/dz_real\n\nand we know that :\ndz/dy = dz_real/dy_real + i * dz_imag/dz_real\n      = dz_real/dy_real - i * dz_real/dy_imag\n      = dz_imag/dy_imag + i * dz_imag/dy_real\n      = dz_imag/dy_imag - i * dz_real/dy_imag\n\nso, \ngrad_real L(y) = (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag) \n               = grad_real(z) * conjugate(dz_real/dy_real - i * dz_real/dy_imag)\n               = grad_real(z) * conjugate(dz/dy)\n\n\ngrad_imag L(y) = (dz_imag/dy_real + i * dz_imag/dy_imag) \n               = (0, i) * (dz_imag/dy_imag - i * dz_imag/dy_real)\n               = grad_imag(z) * conjugate(dz_imag/dy_imag + i * dz_imag/dy_real)\n               = grad_imag(z) * conjugate(dz/dy)\n\n\n\ngrad_real L(x) = dz_real/dx_real + i * dz_real/dx_imag\n= (dz_real/dy_real * dy_real/dx_real + dz_real/dy_imag * dy_imag/dx_real) + i * (dz_real/dy_real*dy_real/dx_imag + dz_real/dy_imag * dy_imag/dx_imag)\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  dz_real/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  i * dz_real/dy_imag * (dy_imag/dx_imag - i * dy_imag/dx_real)\n= dz_real/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_real/dy_imag * conjugate(dy_imag/dx_imag + i * dy_imag/dx_real)\n= dz_real/dy_real * conjugate(dy/dx) + i * dz_real/dy_imag * conjugate(dy/dx)\n= (dz_real/dy_real + i * dz_real/dy_imag) *  conjugate(dy/dx)\n= grad_real(y) * conjugate(dy/dx)\n\n\n\ngrad_real L(x) = dz_imag/dx_real + i * dz_imag/dx_imag\n= (dz_imag/dy_real * dy_real/dx_real + dz_imag/dy_imag * dy_imag/dx_real) + i * (dz_imag/dy_real * dy_real/dx_imag + dz_imag/dy_imag * dy_imag/dx_imag)\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + dz_imag/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + i * dz_imag/dy_imag * (dy_imag/dx_imag - dy_imag/dx_real)\n= dz_imag/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_imag/dy_imag * conjugate(dy_imag/dx_imag + dy_imag/dx_real)\n= dz_imag/dy_real * conjugate(dy/dx) + i * dz_imag/dy_imag * conjugate(dy/dx)\n= (dz_imag/dy_real + i * dz_imag/dy_imag) * conjugate(dy/dx)\n= grad_imag(y) * conjugate(dy/dx)\n</code></pre>\n<p>Summing up, for the graph: L = z, z = z(y), y = y(x), for any real variant t, we define L gradient w.r.t t is Grad L(t) = grad L(t), for any<br>\ncomplex variant t we define L gradient w.r.t t Grad L(t) = grad_real L(t); because for any real variant x, x = conjugate(x), now we can calculate Grad L w.r.t any variant by the same chain rules :</p>\n<pre><code>Grad L(z) = 1,\nGrad L(y) = Grad L(z) * conjugate(dz/dy)\nGrad L(x) = Grad L(y) * conjugate(dy/dx)\n</code></pre>", "body_text": "This is not a bug, and yes , what is really output is dy_real/dx_real + i * dy_real/dx_imag for complex-valued functions y=f(x).\nI think the key to your question is the definition of gradient. let me explain it.\n\nfor real-valued R -> R funciton f:  y = f(x) , the gradient of f w.r.t x is grad f(x) = df/dx,\n\nSuppose what we aim to minimize a graph's output L, and L = z, z = z(y), y = y(x), we know that dz/dx = dz/dy * dy/dx,\nso we can calculate gradient w.r.t any variant in this graph like this :\ngrad L(z) = dL/dz = 1\ngrad L(y) = dL/dy = dL/dz * dz/dy = grad L(z) * dz/dy\ngrad L(x) = dL/dx = dL/dy * dy/dx = grad L(y) * dy/dx\n\nthis is called the reversed calculation of gradients.\nBut for complexed-valued function C -> C: z = f(w),  if we define gradient f(w) = dz/dw, the chain rule can not apply directly.\nso, we change the definition:\n\nfor complex-valued C -> C function f: z = f(w), we define two gradient: the real gradient of f w.r.t w is grad_real f(w) = dz_real/dw_real + i * dz_real/dw_imag,\nthe imaginary gradient of f w.r.t w is grad_imag f(w) = dz_imag/dw_real + i * dz_imag/dw_imag.\n\nNow, let's see how to calcualte the real gradient and the imag gradient.\nSimilarly suppose we get a graph: L = z, z = z(y), y = y(x),  L,z,y,x are all complex variants. By the definition, we do it like this:\ngrad_real L(z) = (dz_real/dz_real + i * dz_real/dz_imag) = (1, 0)\ngrad_imag L(z) = (dz_imag/dz_real + i * dz_imag/dz_real) = (0, i)\n\ngrad_real L(y) = (dz_real/dy_real + i * dz_real/dy_imag) \n= (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag)\n\nAnd we know that for a analytic fucntion z = z(y) , wo get Cauchy\u2013Riemann equations:\ndz_real/dy_real = dz_imag/dy_imag\ndz_real/dy_imag = - dz_imag/dz_real\n\nand we know that :\ndz/dy = dz_real/dy_real + i * dz_imag/dz_real\n      = dz_real/dy_real - i * dz_real/dy_imag\n      = dz_imag/dy_imag + i * dz_imag/dy_real\n      = dz_imag/dy_imag - i * dz_real/dy_imag\n\nso, \ngrad_real L(y) = (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag) \n               = grad_real(z) * conjugate(dz_real/dy_real - i * dz_real/dy_imag)\n               = grad_real(z) * conjugate(dz/dy)\n\n\ngrad_imag L(y) = (dz_imag/dy_real + i * dz_imag/dy_imag) \n               = (0, i) * (dz_imag/dy_imag - i * dz_imag/dy_real)\n               = grad_imag(z) * conjugate(dz_imag/dy_imag + i * dz_imag/dy_real)\n               = grad_imag(z) * conjugate(dz/dy)\n\n\n\ngrad_real L(x) = dz_real/dx_real + i * dz_real/dx_imag\n= (dz_real/dy_real * dy_real/dx_real + dz_real/dy_imag * dy_imag/dx_real) + i * (dz_real/dy_real*dy_real/dx_imag + dz_real/dy_imag * dy_imag/dx_imag)\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  dz_real/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  i * dz_real/dy_imag * (dy_imag/dx_imag - i * dy_imag/dx_real)\n= dz_real/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_real/dy_imag * conjugate(dy_imag/dx_imag + i * dy_imag/dx_real)\n= dz_real/dy_real * conjugate(dy/dx) + i * dz_real/dy_imag * conjugate(dy/dx)\n= (dz_real/dy_real + i * dz_real/dy_imag) *  conjugate(dy/dx)\n= grad_real(y) * conjugate(dy/dx)\n\n\n\ngrad_real L(x) = dz_imag/dx_real + i * dz_imag/dx_imag\n= (dz_imag/dy_real * dy_real/dx_real + dz_imag/dy_imag * dy_imag/dx_real) + i * (dz_imag/dy_real * dy_real/dx_imag + dz_imag/dy_imag * dy_imag/dx_imag)\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + dz_imag/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + i * dz_imag/dy_imag * (dy_imag/dx_imag - dy_imag/dx_real)\n= dz_imag/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_imag/dy_imag * conjugate(dy_imag/dx_imag + dy_imag/dx_real)\n= dz_imag/dy_real * conjugate(dy/dx) + i * dz_imag/dy_imag * conjugate(dy/dx)\n= (dz_imag/dy_real + i * dz_imag/dy_imag) * conjugate(dy/dx)\n= grad_imag(y) * conjugate(dy/dx)\n\nSumming up, for the graph: L = z, z = z(y), y = y(x), for any real variant t, we define L gradient w.r.t t is Grad L(t) = grad L(t), for any\ncomplex variant t we define L gradient w.r.t t Grad L(t) = grad_real L(t); because for any real variant x, x = conjugate(x), now we can calculate Grad L w.r.t any variant by the same chain rules :\nGrad L(z) = 1,\nGrad L(y) = Grad L(z) * conjugate(dz/dy)\nGrad L(x) = Grad L(y) * conjugate(dy/dx)", "body": "This is not a bug, and yes , what is really output is dy_real/dx_real + i * dy_real/dx_imag for complex-valued functions y=f(x).\r\n\r\nI think the key to your question is the definition of gradient. let me explain it.\r\n\r\n* for real-valued R -> R funciton f:  y = f(x) , the gradient of f w.r.t x is grad f(x) = df/dx,  \r\n\r\nSuppose what we aim to minimize a graph's output L, and L = z, z = z(y), y = y(x), we know that dz/dx = dz/dy * dy/dx, \r\nso we can calculate gradient w.r.t any variant in this graph like this :\r\n\r\n```\r\ngrad L(z) = dL/dz = 1\r\ngrad L(y) = dL/dy = dL/dz * dz/dy = grad L(z) * dz/dy\r\ngrad L(x) = dL/dx = dL/dy * dy/dx = grad L(y) * dy/dx\r\n```\r\n\r\nthis is called the reversed calculation of gradients.\r\n\r\n\r\nBut for complexed-valued function C -> C: z = f(w),  if we define gradient f(w) = dz/dw, the chain rule can not apply directly.\r\n\r\nso, we change the definition:\r\n\r\n* for complex-valued C -> C function f: z = f(w), we define two gradient: the real gradient of f w.r.t w is grad_real f(w) = dz_real/dw_real + i * dz_real/dw_imag, \r\nthe imaginary gradient of f w.r.t w is grad_imag f(w) = dz_imag/dw_real + i * dz_imag/dw_imag.\r\n\r\n\r\nNow, let's see how to calcualte the real gradient and the imag gradient.\r\n\r\nSimilarly suppose we get a graph: L = z, z = z(y), y = y(x),  L,z,y,x are all complex variants. By the definition, we do it like this:\r\n\r\n```\r\ngrad_real L(z) = (dz_real/dz_real + i * dz_real/dz_imag) = (1, 0)\r\ngrad_imag L(z) = (dz_imag/dz_real + i * dz_imag/dz_real) = (0, i)\r\n\r\ngrad_real L(y) = (dz_real/dy_real + i * dz_real/dy_imag) \r\n= (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag)\r\n\r\nAnd we know that for a analytic fucntion z = z(y) , wo get Cauchy\u2013Riemann equations:\r\ndz_real/dy_real = dz_imag/dy_imag\r\ndz_real/dy_imag = - dz_imag/dz_real\r\n\r\nand we know that :\r\ndz/dy = dz_real/dy_real + i * dz_imag/dz_real\r\n      = dz_real/dy_real - i * dz_real/dy_imag\r\n      = dz_imag/dy_imag + i * dz_imag/dy_real\r\n      = dz_imag/dy_imag - i * dz_real/dy_imag\r\n\r\nso, \r\ngrad_real L(y) = (1, 0) * (dz_real/dy_real + i * dz_real/dy_imag) \r\n               = grad_real(z) * conjugate(dz_real/dy_real - i * dz_real/dy_imag)\r\n               = grad_real(z) * conjugate(dz/dy)\r\n\r\n\r\ngrad_imag L(y) = (dz_imag/dy_real + i * dz_imag/dy_imag) \r\n               = (0, i) * (dz_imag/dy_imag - i * dz_imag/dy_real)\r\n               = grad_imag(z) * conjugate(dz_imag/dy_imag + i * dz_imag/dy_real)\r\n               = grad_imag(z) * conjugate(dz/dy)\r\n\r\n\r\n\r\ngrad_real L(x) = dz_real/dx_real + i * dz_real/dx_imag\r\n= (dz_real/dy_real * dy_real/dx_real + dz_real/dy_imag * dy_imag/dx_real) + i * (dz_real/dy_real*dy_real/dx_imag + dz_real/dy_imag * dy_imag/dx_imag)\r\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  dz_real/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\r\n= dz_real/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) +  i * dz_real/dy_imag * (dy_imag/dx_imag - i * dy_imag/dx_real)\r\n= dz_real/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_real/dy_imag * conjugate(dy_imag/dx_imag + i * dy_imag/dx_real)\r\n= dz_real/dy_real * conjugate(dy/dx) + i * dz_real/dy_imag * conjugate(dy/dx)\r\n= (dz_real/dy_real + i * dz_real/dy_imag) *  conjugate(dy/dx)\r\n= grad_real(y) * conjugate(dy/dx)\r\n\r\n\r\n\r\ngrad_real L(x) = dz_imag/dx_real + i * dz_imag/dx_imag\r\n= (dz_imag/dy_real * dy_real/dx_real + dz_imag/dy_imag * dy_imag/dx_real) + i * (dz_imag/dy_real * dy_real/dx_imag + dz_imag/dy_imag * dy_imag/dx_imag)\r\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + dz_imag/dy_imag * (dy_imag/dx_real + i * dy_imag/dx_imag)\r\n= dz_imag/dy_real * (dy_real/dx_real + i * dy_real/dx_imag) + i * dz_imag/dy_imag * (dy_imag/dx_imag - dy_imag/dx_real)\r\n= dz_imag/dy_real * conjugate(dy_real/dx_real - i * dy_real/dx_imag) + i * dz_imag/dy_imag * conjugate(dy_imag/dx_imag + dy_imag/dx_real)\r\n= dz_imag/dy_real * conjugate(dy/dx) + i * dz_imag/dy_imag * conjugate(dy/dx)\r\n= (dz_imag/dy_real + i * dz_imag/dy_imag) * conjugate(dy/dx)\r\n= grad_imag(y) * conjugate(dy/dx)\r\n```\r\n\r\nSumming up, for the graph: L = z, z = z(y), y = y(x), for any real variant t, we define L gradient w.r.t t is Grad L(t) = grad L(t), for any \r\ncomplex variant t we define L gradient w.r.t t Grad L(t) = grad_real L(t); because for any real variant x, x = conjugate(x), now we can calculate Grad L w.r.t any variant by the same chain rules :\r\n\r\n```\r\nGrad L(z) = 1,\r\nGrad L(y) = Grad L(z) * conjugate(dz/dy)\r\nGrad L(x) = Grad L(y) * conjugate(dy/dx)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}