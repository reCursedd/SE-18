{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/391565833", "html_url": "https://github.com/tensorflow/tensorflow/issues/19447#issuecomment-391565833", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19447", "id": 391565833, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTU2NTgzMw==", "user": {"login": "xysmlx", "id": 7117752, "node_id": "MDQ6VXNlcjcxMTc3NTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/7117752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xysmlx", "html_url": "https://github.com/xysmlx", "followers_url": "https://api.github.com/users/xysmlx/followers", "following_url": "https://api.github.com/users/xysmlx/following{/other_user}", "gists_url": "https://api.github.com/users/xysmlx/gists{/gist_id}", "starred_url": "https://api.github.com/users/xysmlx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xysmlx/subscriptions", "organizations_url": "https://api.github.com/users/xysmlx/orgs", "repos_url": "https://api.github.com/users/xysmlx/repos", "events_url": "https://api.github.com/users/xysmlx/events{/privacy}", "received_events_url": "https://api.github.com/users/xysmlx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-24T02:17:41Z", "updated_at": "2018-05-24T02:19:30Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12690488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lw3259111\">@lw3259111</a> Have you confirmed that the tensorflow returns OOM when increases features to 90000? Tensorflow will allocate all of the GPU memory and manage them by itself in default. The GPU memory shown in nvidia-smi is useless.</p>\n<p>If tensorflow returns OOM, there are two ways:</p>\n<ol>\n<li>decrease the size of the batch_size</li>\n<li>split a batch into multiple samples and run forward propagation for each sample. Then, calculate the loss for each sample and sum them into one loss. Then, do backward propagation manually with this loss and tf.gradients/tf.optimizer.apply_gradients.</li>\n</ol>\n<p>Have you see whether features are dense or sparse?  If the features are sparse, it will be better to use sparse operator to process them.</p>", "body_text": "@lw3259111 Have you confirmed that the tensorflow returns OOM when increases features to 90000? Tensorflow will allocate all of the GPU memory and manage them by itself in default. The GPU memory shown in nvidia-smi is useless.\nIf tensorflow returns OOM, there are two ways:\n\ndecrease the size of the batch_size\nsplit a batch into multiple samples and run forward propagation for each sample. Then, calculate the loss for each sample and sum them into one loss. Then, do backward propagation manually with this loss and tf.gradients/tf.optimizer.apply_gradients.\n\nHave you see whether features are dense or sparse?  If the features are sparse, it will be better to use sparse operator to process them.", "body": "@lw3259111 Have you confirmed that the tensorflow returns OOM when increases features to 90000? Tensorflow will allocate all of the GPU memory and manage them by itself in default. The GPU memory shown in nvidia-smi is useless.\r\n\r\nIf tensorflow returns OOM, there are two ways:\r\n1. decrease the size of the batch_size\r\n2. split a batch into multiple samples and run forward propagation for each sample. Then, calculate the loss for each sample and sum them into one loss. Then, do backward propagation manually with this loss and tf.gradients/tf.optimizer.apply_gradients.\r\n\r\nHave you see whether features are dense or sparse?  If the features are sparse, it will be better to use sparse operator to process them."}