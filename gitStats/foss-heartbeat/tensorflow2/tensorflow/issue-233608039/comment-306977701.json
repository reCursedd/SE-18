{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/306977701", "html_url": "https://github.com/tensorflow/tensorflow/issues/10437#issuecomment-306977701", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10437", "id": 306977701, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjk3NzcwMQ==", "user": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-08T02:09:22Z", "updated_at": "2017-06-08T02:09:22Z", "author_association": "MEMBER", "body_html": "<p>I don't think we have any very specific thoughts on how this should best be done.</p>\n<p>There is a somewhat under-publicized mailing list for folks working on XLA development which might make a better forum for this sort of wide-ranging discussion than a Github issue:<br>\n<a href=\"https://groups.google.com/forum/#!forum/xla-dev\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/xla-dev</a></p>\n<p>To answer your specific question: I suspect the right thing to do is to have a separate AMDGPU XLA plugin, separate from the NVidia GPU XLA plugin. The two can share much or even most of the code (as do the current GPU and CPU backends, which share most of their LLVM logic, for example), but details like the target triple, or exactly which compiler passes to use, can differ between plugins.</p>\n<p>As I mentioned, we don't really have a lot of time ourselves to work on an additional backend, but we would be very happy if one did exist, so this is a \"contributions welcome\" situation.</p>", "body_text": "I don't think we have any very specific thoughts on how this should best be done.\nThere is a somewhat under-publicized mailing list for folks working on XLA development which might make a better forum for this sort of wide-ranging discussion than a Github issue:\nhttps://groups.google.com/forum/#!forum/xla-dev\nTo answer your specific question: I suspect the right thing to do is to have a separate AMDGPU XLA plugin, separate from the NVidia GPU XLA plugin. The two can share much or even most of the code (as do the current GPU and CPU backends, which share most of their LLVM logic, for example), but details like the target triple, or exactly which compiler passes to use, can differ between plugins.\nAs I mentioned, we don't really have a lot of time ourselves to work on an additional backend, but we would be very happy if one did exist, so this is a \"contributions welcome\" situation.", "body": "I don't think we have any very specific thoughts on how this should best be done.\r\n\r\nThere is a somewhat under-publicized mailing list for folks working on XLA development which might make a better forum for this sort of wide-ranging discussion than a Github issue:\r\nhttps://groups.google.com/forum/#!forum/xla-dev\r\n\r\nTo answer your specific question: I suspect the right thing to do is to have a separate AMDGPU XLA plugin, separate from the NVidia GPU XLA plugin. The two can share much or even most of the code (as do the current GPU and CPU backends, which share most of their LLVM logic, for example), but details like the target triple, or exactly which compiler passes to use, can differ between plugins. \r\n\r\nAs I mentioned, we don't really have a lot of time ourselves to work on an additional backend, but we would be very happy if one did exist, so this is a \"contributions welcome\" situation."}