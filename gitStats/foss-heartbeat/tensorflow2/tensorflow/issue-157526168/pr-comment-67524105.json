{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/67524105", "pull_request_review_id": null, "id": 67524105, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NTI0MTA1", "diff_hunk": "@@ -463,6 +463,113 @@ def bidirectional_rnn(cell_fw, cell_bw, inputs,\n   return (outputs, output_state_fw, output_state_bw)\n \n \n+def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,\n+                              initial_state_fw=None, initial_state_bw=None,\n+                              dtype=None, parallel_iterations=None,\n+                              swap_memory=False, time_major=False, scope=None):\n+  \"\"\"Creates a dynamic version of bidirectional recurrent neural network.\n+\n+  Similar to the unidirectional case above (rnn) but takes input and builds\n+  independent forward and backward RNNs. The input_size of forward and\n+  backward cell must match. The initial state for both directions is zero by\n+  default (but can be set optionally) and no intermediate states are ever\n+  returned -- the network is fully unrolled for the given (passed in)\n+  length(s) of the sequence(s) or completely unrolled if length(s) is not\n+  given.\n+\n+  Args:\n+    cell_fw: An instance of RNNCell, to be used for forward direction.\n+    cell_bw: An instance of RNNCell, to be used for backward direction.\n+    inputs: The RNN inputs.\n+      If time_major == False (default), this must be a tensor of shape:\n+        `[batch_size, max_time, input_size]`.\n+      If time_major == True, this must be a tensor of shape:\n+        `[max_time, batch_size, input_size]`.\n+      [batch_size, input_size].\n+    sequence_length: An int32/int64 vector, size `[batch_size]`,\n+      containing the actual lengths for each of the sequences.\n+    initial_state_fw: (optional) An initial state for the forward RNN.\n+      This must be a tensor of appropriate type and shape\n+      `[batch_size x cell_fw.state_size]`.\n+      If `cell_fw.state_size` is a tuple, this should be a tuple of\n+      tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.\n+    initial_state_bw: (optional) Same as for `initial_state_fw`, but using\n+      the corresponding properties of `cell_bw`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    dtype: (optional) The data type for the initial state.  Required if\n+      initial_state is not provided.\n+    sequence_length: An int32/int64 vector, size `[batch_size]`,\n+      containing the actual lengths for each of the sequences.\n+      either of the initial states are not provided.\n+    scope: VariableScope for the created subgraph; defaults to \"BiRNN\"\n+\n+  Returns:\n+    A tuple (outputs, output_states) where:\n+      outputs: A tuple of the forward and the backward rnn output `Tensor`.\n+        If time_major == False (default),\n+          each element will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, each element will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+        It returns a tuple instead of a single concatenated `Tensor`, unlike\n+        in the `bidirectional_rnn`. If the concatenated one is preferred,\n+        the forward and backward outputs can be concatenated as\n+        `tf.concat(2, outputs)`.\n+      output_states: A tuple of final states of the forward and the backward", "path": "tensorflow/python/ops/rnn.py", "position": null, "original_position": 71, "commit_id": "8daeec2604de8e0f545c6a1cd30088484ea2a623", "original_commit_id": "881507bcea9513755b28324d89942a90721aa13e", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "Good point re: bidirectional_rnn.  Let's just be consistent.  I'm fine with output_states.\n", "created_at": "2016-06-17T15:00:48Z", "updated_at": "2016-06-20T03:02:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2581#discussion_r67524105", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2581", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/67524105"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2581#discussion_r67524105"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2581"}}, "body_html": "<p>Good point re: bidirectional_rnn.  Let's just be consistent.  I'm fine with output_states.</p>", "body_text": "Good point re: bidirectional_rnn.  Let's just be consistent.  I'm fine with output_states."}