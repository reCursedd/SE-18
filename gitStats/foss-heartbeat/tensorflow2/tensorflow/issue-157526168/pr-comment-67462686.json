{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/67462686", "pull_request_review_id": null, "id": 67462686, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NDYyNjg2", "diff_hunk": "@@ -463,6 +463,113 @@ def bidirectional_rnn(cell_fw, cell_bw, inputs,\n   return (outputs, output_state_fw, output_state_bw)\n \n \n+def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,\n+                              initial_state_fw=None, initial_state_bw=None,\n+                              dtype=None, parallel_iterations=None,\n+                              swap_memory=False, time_major=False, scope=None):\n+  \"\"\"Creates a dynamic version of bidirectional recurrent neural network.\n+\n+  Similar to the unidirectional case above (rnn) but takes input and builds\n+  independent forward and backward RNNs. The input_size of forward and\n+  backward cell must match. The initial state for both directions is zero by\n+  default (but can be set optionally) and no intermediate states are ever\n+  returned -- the network is fully unrolled for the given (passed in)\n+  length(s) of the sequence(s) or completely unrolled if length(s) is not\n+  given.\n+\n+  Args:\n+    cell_fw: An instance of RNNCell, to be used for forward direction.\n+    cell_bw: An instance of RNNCell, to be used for backward direction.\n+    inputs: The RNN inputs.\n+      If time_major == False (default), this must be a tensor of shape:\n+        `[batch_size, max_time, input_size]`.\n+      If time_major == True, this must be a tensor of shape:\n+        `[max_time, batch_size, input_size]`.\n+      [batch_size, input_size].\n+    sequence_length: An int32/int64 vector, size `[batch_size]`,\n+      containing the actual lengths for each of the sequences.\n+    initial_state_fw: (optional) An initial state for the forward RNN.\n+      This must be a tensor of appropriate type and shape\n+      `[batch_size x cell_fw.state_size]`.\n+      If `cell_fw.state_size` is a tuple, this should be a tuple of\n+      tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.\n+    initial_state_bw: (optional) Same as for `initial_state_fw`, but using\n+      the corresponding properties of `cell_bw`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    dtype: (optional) The data type for the initial state.  Required if\n+      initial_state is not provided.\n+    sequence_length: An int32/int64 vector, size `[batch_size]`,\n+      containing the actual lengths for each of the sequences.\n+      either of the initial states are not provided.\n+    scope: VariableScope for the created subgraph; defaults to \"BiRNN\"\n+\n+  Returns:\n+    A tuple (outputs, output_states) where:\n+      outputs: A tuple of the forward and the backward rnn output `Tensor`.\n+        If time_major == False (default),\n+          each element will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, each element will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+        It returns a tuple instead of a single concatenated `Tensor`, unlike\n+        in the `bidirectional_rnn`. If the concatenated one is preferred,\n+        the forward and backward outputs can be concatenated as\n+        `tf.concat(2, outputs)`.\n+      output_states: A tuple of final states of the forward and the backward\n+        rnn.\n+\n+  Raises:\n+    TypeError: If `cell_fw` or `cell_bw` is not an instance of `RNNCell`.\n+  \"\"\"\n+\n+  if not isinstance(cell_fw, rnn_cell.RNNCell):\n+    raise TypeError(\"cell_fw must be an instance of RNNCell\")\n+  if not isinstance(cell_bw, rnn_cell.RNNCell):\n+    raise TypeError(\"cell_bw must be an instance of RNNCell\")\n+\n+  name = scope or \"BiRNN\"\n+  # Forward direction\n+  with vs.variable_scope(name + \"_FW\") as fw_scope:\n+    output_fw, output_state_fw = dynamic_rnn(\n+        cell_fw, inputs, sequence_length, initial_state_fw, dtype,\n+        parallel_iterations, swap_memory, time_major, scope=fw_scope)\n+  # Backward direction\n+  if not time_major:\n+    time_dim = 1\n+    batch_dim = 0\n+  else:\n+    time_dim = 0\n+    batch_dim = 1\n+  with vs.variable_scope(name + \"_BW\") as bw_scope:\n+    inputs_reverse = array_ops.reverse_sequence(\n+        inputs, sequence_length, time_dim, batch_dim)", "path": "tensorflow/python/ops/rnn.py", "position": null, "original_position": 98, "commit_id": "8daeec2604de8e0f545c6a1cd30088484ea2a623", "original_commit_id": "881507bcea9513755b28324d89942a90721aa13e", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "body": "In what cases should the keyword arguments be used?\nFor all parameters including those without default values, or for only parameters having default values?\n\n`dynamic_rnn(cell_fw, inputs, sequence_length=sequence_length, initial_state=initial_state_fw...)`\n`reverse_sequence(inputs, sequence_length, time_dim, batch_dim=batch_dim)`\n\nor\n`dynamic_rnn(cell=cell_fw, inputs=inputs, sequence_length=sequence_length, initial_state=initial_state_fw...)`\n`reverse_sequence(inputs=inputs, seq_lengths=sequence_length, seq_dim=time_dim, batch_dim=batch_dim)`\n", "created_at": "2016-06-17T05:39:09Z", "updated_at": "2016-06-20T03:02:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2581#discussion_r67462686", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2581", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/67462686"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2581#discussion_r67462686"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2581"}}, "body_html": "<p>In what cases should the keyword arguments be used?<br>\nFor all parameters including those without default values, or for only parameters having default values?</p>\n<p><code>dynamic_rnn(cell_fw, inputs, sequence_length=sequence_length, initial_state=initial_state_fw...)</code><br>\n<code>reverse_sequence(inputs, sequence_length, time_dim, batch_dim=batch_dim)</code></p>\n<p>or<br>\n<code>dynamic_rnn(cell=cell_fw, inputs=inputs, sequence_length=sequence_length, initial_state=initial_state_fw...)</code><br>\n<code>reverse_sequence(inputs=inputs, seq_lengths=sequence_length, seq_dim=time_dim, batch_dim=batch_dim)</code></p>", "body_text": "In what cases should the keyword arguments be used?\nFor all parameters including those without default values, or for only parameters having default values?\ndynamic_rnn(cell_fw, inputs, sequence_length=sequence_length, initial_state=initial_state_fw...)\nreverse_sequence(inputs, sequence_length, time_dim, batch_dim=batch_dim)\nor\ndynamic_rnn(cell=cell_fw, inputs=inputs, sequence_length=sequence_length, initial_state=initial_state_fw...)\nreverse_sequence(inputs=inputs, seq_lengths=sequence_length, seq_dim=time_dim, batch_dim=batch_dim)"}