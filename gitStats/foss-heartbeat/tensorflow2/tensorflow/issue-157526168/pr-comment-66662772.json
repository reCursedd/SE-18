{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/66662772", "pull_request_review_id": null, "id": 66662772, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NjYyNzcy", "diff_hunk": "@@ -463,6 +463,109 @@ def bidirectional_rnn(cell_fw, cell_bw, inputs,\n   return (outputs, output_state_fw, output_state_bw)\n \n \n+def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,\n+                              initial_state_fw=None, initial_state_bw=None,\n+                              dtype=None, parallel_iterations=None,\n+                              swap_memory=False, time_major=False, scope=None):\n+  \"\"\"Creates a bidirectional recurrent neural network, dynamic version.\n+\n+  Similar to the unidirectional case above (rnn) but takes input and builds\n+  independent forward and backward RNNs with the final forward and backward\n+  outputs depth-concatenated, such that the output will have the format\n+  [time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of\n+  forward and backward cell must match. The initial state for both directions\n+  is zero by default (but can be set optionally) and no intermediate states are\n+  ever returned -- the network is fully unrolled for the given (passed in)\n+  length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n+\n+  Args:\n+    cell_fw: An instance of RNNCell, to be used for forward direction.\n+    cell_bw: An instance of RNNCell, to be used for backward direction.\n+    inputs: The RNN inputs.\n+      If time_major == False (default), this must be a tensor of shape:\n+        `[batch_size, max_time, input_size]`.\n+      If time_major == True, this must be a tensor of shape:\n+        `[max_time, batch_size, input_size]`.\n+      [batch_size, input_size].\n+    sequence_length: An int32/int64 vector, size `[batch_size]`,\n+      containing the actual lengths for each of the sequences.\n+    initial_state_fw: (optional) An initial state for the forward RNN.\n+      This must be a tensor of appropriate type and shape\n+      `[batch_size x cell_fw.state_size]`.\n+      If `cell_fw.state_size` is a tuple, this should be a tuple of\n+      tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.\n+    initial_state_bw: (optional) Same as for `initial_state_fw`, but using\n+      the corresponding properties of `cell_bw`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    dtype: (optional) The data type for the initial state.  Required if\n+      initial_state is not provided.\n+    sequence_length: An int32/int64 vector, size `[batch_size]`,\n+      containing the actual lengths for each of the sequences.\n+      either of the initial states are not provided.\n+    scope: VariableScope for the created subgraph; defaults to \"BiRNN\"\n+\n+  Returns:\n+    A tuple (outputs, output_state_fw, output_state_bw) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      output_state_fw is the final state of the forward rnn.\n+      output_state_bw is the final state of the backward rnn.\n+\n+  Raises:\n+    TypeError: If `cell_fw` or `cell_bw` is not an instance of `RNNCell`.\n+    ValueError: If inputs is None or an empty list.\n+  \"\"\"\n+\n+  if not isinstance(cell_fw, rnn_cell.RNNCell):\n+    raise TypeError(\"cell_fw must be an instance of RNNCell\")\n+  if not isinstance(cell_bw, rnn_cell.RNNCell):\n+    raise TypeError(\"cell_bw must be an instance of RNNCell\")\n+\n+  name = scope or \"BiRNN\"\n+  # Forward direction\n+  with vs.variable_scope(name + \"_FW\") as fw_scope:\n+    output_fw, output_state_fw = dynamic_rnn(\n+        cell_fw, inputs, sequence_length, initial_state_fw, dtype,\n+        parallel_iterations, swap_memory, time_major, scope=fw_scope)\n+  # Backward direction\n+  if not time_major:\n+    time_dim = 1\n+    batch_dim = 0\n+  else:\n+    time_dim = 0\n+    batch_dim = 1\n+  with vs.variable_scope(name + \"_BW\") as bw_scope:\n+    inputs_reverse = array_ops.reverse_sequence(\n+        inputs, sequence_length, time_dim, batch_dim)\n+    tmp, output_state_bw = dynamic_rnn(\n+        cell_bw, inputs_reverse, sequence_length, initial_state_bw, dtype,\n+        parallel_iterations, swap_memory, time_major, scope=bw_scope)\n+  output_bw = array_ops.reverse_sequence(\n+      tmp, sequence_length, time_dim, batch_dim)\n+  # Concat each of the forward/backward outputs\n+  outputs = array_ops.concat(2, [output_fw, output_bw])\n+\n+  return (outputs, output_state_fw, output_state_bw)", "path": "tensorflow/python/ops/rnn.py", "position": null, "original_position": 104, "commit_id": "8daeec2604de8e0f545c6a1cd30088484ea2a623", "original_commit_id": "67736ab7bce7f8c5734d476e500a2a6d4325dadb", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "perhaps the output state should be a tuple as well?  first one containing fw, second one containing bw?\n", "created_at": "2016-06-10T18:46:31Z", "updated_at": "2016-06-20T03:02:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2581#discussion_r66662772", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2581", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/66662772"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2581#discussion_r66662772"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2581"}}, "body_html": "<p>perhaps the output state should be a tuple as well?  first one containing fw, second one containing bw?</p>", "body_text": "perhaps the output state should be a tuple as well?  first one containing fw, second one containing bw?"}