{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21875", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21875/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21875/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21875/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21875", "id": 354021528, "node_id": "MDU6SXNzdWUzNTQwMjE1Mjg=", "number": 21875, "title": "Incompatible shapes between op input and calculated input gradient for nn.conv3d_transpose", "user": {"login": "Bedrettin-Cetinkaya", "id": 28837795, "node_id": "MDQ6VXNlcjI4ODM3Nzk1", "avatar_url": "https://avatars0.githubusercontent.com/u/28837795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bedrettin-Cetinkaya", "html_url": "https://github.com/Bedrettin-Cetinkaya", "followers_url": "https://api.github.com/users/Bedrettin-Cetinkaya/followers", "following_url": "https://api.github.com/users/Bedrettin-Cetinkaya/following{/other_user}", "gists_url": "https://api.github.com/users/Bedrettin-Cetinkaya/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bedrettin-Cetinkaya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bedrettin-Cetinkaya/subscriptions", "organizations_url": "https://api.github.com/users/Bedrettin-Cetinkaya/orgs", "repos_url": "https://api.github.com/users/Bedrettin-Cetinkaya/repos", "events_url": "https://api.github.com/users/Bedrettin-Cetinkaya/events{/privacy}", "received_events_url": "https://api.github.com/users/Bedrettin-Cetinkaya/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-08-25T15:34:36Z", "updated_at": "2018-11-09T18:53:19Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>This is code:</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom utils import init_weight\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\n@tf.contrib.framework.add_arg_scope\ndef conv3d_transpose(inputs,\n                     num_outputs,\n                     kernel_size,\n                     weights,\n                     biases,\n                     stride,\n                     padding,\n                     activation_fn=tf.nn.relu,\n                     weights_initializer=tf.contrib.layers.xavier_initializer(),\n                     biases_initializer=tf.zeros_initializer(),\n                     reuse=None,\n                     trainable=True,\n                     scope=None):\n\n  with tf.variable_scope(\n      scope, 'Conv3d_transpose', [inputs], reuse=reuse):\n    dtype = inputs.dtype.base_dtype\n    kernel_d, kernel_h, kernel_w = kernel_size[0:3]\n    num_filters_in = inputs.get_shape()[4]\n    weights_shape = [kernel_d, kernel_h, kernel_w, num_outputs, num_filters_in]\n    \n    weights = tf.get_variable('weights',\n                              shape=weights_shape,\n                              dtype=dtype,\n                              initializer=tf.constant_initializer(weights),\n                              trainable=trainable)\n    \n    tf.contrib.framework.add_model_variable(weights)\n    \n    input_shape = inputs.get_shape().as_list()\n    batch_size = input_shape[0]\n    depth = input_shape[1]\n    height = input_shape[2]\n    width = input_shape[3]\n\n    def get_deconv_dim(dim_size, stride_size,pading,kernel):\n      if isinstance(dim_size, tf.Tensor):\n        sub = tf.subtract(dim_size,1)\n        dim_size = tf.multiply(dim_size,sub)\n        dim_size = tf.subtract(dim_size, 2*pading)\n        dim_size = tf.add(dim_size,kernel)\n        \n      elif dim_size is not None:\n        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel\n      return dim_size\n   \n    pad = 1\n    if padding == 'VALID':\n      pad = 0\n    out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])\n    out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])\n    out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])\n\n    out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]\n    outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,\n                                     [1, stride, stride, stride, 1],\n                                     padding)\n\n    biases = tf.get_variable('biases',\n                               shape=[num_outputs,],\n                               dtype=dtype,\n                               initializer=tf.constant_initializer(biases),\n                               trainable=trainable)\n    tf.contrib.framework.add_model_variable(biases)\n    outputs = tf.nn.bias_add(outputs, biases)\n    \n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return outputs\n\n\ndef model(identities, params, is_training):\n  \"\"\"Model transforming embedding to voxels.\"\"\"\n  del is_training  # Unused\n  f_dim = params.f_dim\n  with slim.arg_scope(\n      [slim.fully_connected, conv3d_transpose]):\n    w0 = init_weight(\"DecoderWeights/Layer1_13824_512_w.txt\",[13824,512],\"w\")\n    print(identities.get_shape().as_list())\n    h0 = slim.fully_connected(\n        identities, 3 * 3 * 3 * f_dim * 8, weights_initializer = tf.constant_initializer(w0), activation_fn=tf.nn.relu)\n    b0 = init_weight(\"DecoderWeights/Layer1_13824_b.txt\",[13824],\"b\")\n    b0 = tf.get_variable('db0',shape=[13824], initializer=tf.constant_initializer(b0),trainable=True)\n    h0 = tf.nn.bias_add(h0,b0)\n\n    h1 = tf.reshape(h0, [-1, 3, 3, 3, f_dim * 8])\n    print(h1.get_shape().as_list())\n    w1 = init_weight(\"DecoderWeights/Layer2_512_256_4_4_4_w.txt\",[512,256,4,4,4],\"w\")\n    b1 = init_weight(\"DecoderWeights/Layer2_256_b.txt\",[256],\"b\")\n    h1 = conv3d_transpose(\n        h1, f_dim * 4, [4, 4, 4],w1, b1,stride=1, padding= 'VALID',activation_fn=tf.nn.relu)\n    print(h1.get_shape().as_list())\n    w2 = init_weight(\"DecoderWeights/Layer3_256_96_5_5_5_w.txt\",[256,96,5,5,5],\"w\")\n    b2 = init_weight(\"DecoderWeights/Layer3_96_b.txt\",[96],\"b\")\n    h2 = conv3d_transpose(\n        h1, int(f_dim * 3 / 2), [5, 5, 5],w2 , b2, stride=2,padding ='VALID' ,activation_fn=tf.nn.relu)\n    print(h2.get_shape().as_list())\n    w3 = init_weight(\"DecoderWeights/Layer4_96_1_6_6_6_w.txt\",[96,1,6,6,6],\"w\")\n    b3 = init_weight(\"DecoderWeights/Layer4_1_b.txt\",[1],\"b\")\n    h3 = conv3d_transpose(\n        h2, 1, [6, 6, 6], w3,b3, stride=2, padding ='SAME' ,activation_fn=tf.nn.sigmoid)\n    print(h3.get_shape().as_list())\n  return h3\n</code></pre>\n<p>This is error:<br>\nTraceback (most recent call last):<br>\nFile \"train.py\", line 81, in <br>\nepsilon=1e-08).minimize(loss, global_step=global_step)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize<br>\ngrad_loss=grad_loss)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients<br>\ncolocate_gradients_with_ops=colocate_gradients_with_ops)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients<br>\n% (op.name, i, t_in.shape, in_grad.shape))<br>\nValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: decoder/Conv3d_transpose_2/conv3d_transpose.  Input index: 2. Original input shape: (6, 15, 15, 15, 96).  Calculated input gradient shape: (6, 16, 16, 16, 96)</p>\n<p>Tf version : 1.4.1</p>\n<p>I have 4 layers, 1 fc and 3 conv3d_transpose.</p>\n<p>Layer shape as follows:</p>\n<p>[6, 512]<br>\n[6, 3, 3, 3, 512]<br>\n[6, 6, 6, 6, 256]<br>\n[6, 15, 15, 15, 96]<br>\n[6, 32, 32, 32, 1]</p>\n<p>I calculated conv3d_tranpose output shape as follows:</p>\n<pre><code> def get_deconv_dim(dim_size, stride_size,pading,kernel):\n      if dim_size is not None:\n        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel\n      return dim_size\n   \n   pad = 1\n   if padding == 'VALID':\n     pad = 0\n   out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])\n   out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])\n   out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])\n   out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]\n   outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,\n                                     [1, stride, stride, stride, 1],\n                                     padding)\n</code></pre>\n<p>In the last conv3d_transpose layer, while its output is calculated as (6,15,15,15,96), input gradient is calculated as (6,16,16,16,96). Therefore, this leads to error.</p>\n<p>When i call last conv3d_transpose layer with padding=\"SAME\", it didn't gives error.</p>\n<p>edit:</p>\n<p>Environment:<br>\nOs: Ubuntu 16.04.3 LTS<br>\nbazel: N/a<br>\nTensorflow : 1.4.1 via pip install<br>\ncuda: 9.0.176<br>\nGPU: tesla k40, memory: 12gb<br>\ncommand: I uploaded only erroneous python script, just run \"python blabla.py\".  If you want, i can upload all python scripts.</p>", "body_text": "This is code:\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom utils import init_weight\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\n@tf.contrib.framework.add_arg_scope\ndef conv3d_transpose(inputs,\n                     num_outputs,\n                     kernel_size,\n                     weights,\n                     biases,\n                     stride,\n                     padding,\n                     activation_fn=tf.nn.relu,\n                     weights_initializer=tf.contrib.layers.xavier_initializer(),\n                     biases_initializer=tf.zeros_initializer(),\n                     reuse=None,\n                     trainable=True,\n                     scope=None):\n\n  with tf.variable_scope(\n      scope, 'Conv3d_transpose', [inputs], reuse=reuse):\n    dtype = inputs.dtype.base_dtype\n    kernel_d, kernel_h, kernel_w = kernel_size[0:3]\n    num_filters_in = inputs.get_shape()[4]\n    weights_shape = [kernel_d, kernel_h, kernel_w, num_outputs, num_filters_in]\n    \n    weights = tf.get_variable('weights',\n                              shape=weights_shape,\n                              dtype=dtype,\n                              initializer=tf.constant_initializer(weights),\n                              trainable=trainable)\n    \n    tf.contrib.framework.add_model_variable(weights)\n    \n    input_shape = inputs.get_shape().as_list()\n    batch_size = input_shape[0]\n    depth = input_shape[1]\n    height = input_shape[2]\n    width = input_shape[3]\n\n    def get_deconv_dim(dim_size, stride_size,pading,kernel):\n      if isinstance(dim_size, tf.Tensor):\n        sub = tf.subtract(dim_size,1)\n        dim_size = tf.multiply(dim_size,sub)\n        dim_size = tf.subtract(dim_size, 2*pading)\n        dim_size = tf.add(dim_size,kernel)\n        \n      elif dim_size is not None:\n        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel\n      return dim_size\n   \n    pad = 1\n    if padding == 'VALID':\n      pad = 0\n    out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])\n    out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])\n    out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])\n\n    out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]\n    outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,\n                                     [1, stride, stride, stride, 1],\n                                     padding)\n\n    biases = tf.get_variable('biases',\n                               shape=[num_outputs,],\n                               dtype=dtype,\n                               initializer=tf.constant_initializer(biases),\n                               trainable=trainable)\n    tf.contrib.framework.add_model_variable(biases)\n    outputs = tf.nn.bias_add(outputs, biases)\n    \n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return outputs\n\n\ndef model(identities, params, is_training):\n  \"\"\"Model transforming embedding to voxels.\"\"\"\n  del is_training  # Unused\n  f_dim = params.f_dim\n  with slim.arg_scope(\n      [slim.fully_connected, conv3d_transpose]):\n    w0 = init_weight(\"DecoderWeights/Layer1_13824_512_w.txt\",[13824,512],\"w\")\n    print(identities.get_shape().as_list())\n    h0 = slim.fully_connected(\n        identities, 3 * 3 * 3 * f_dim * 8, weights_initializer = tf.constant_initializer(w0), activation_fn=tf.nn.relu)\n    b0 = init_weight(\"DecoderWeights/Layer1_13824_b.txt\",[13824],\"b\")\n    b0 = tf.get_variable('db0',shape=[13824], initializer=tf.constant_initializer(b0),trainable=True)\n    h0 = tf.nn.bias_add(h0,b0)\n\n    h1 = tf.reshape(h0, [-1, 3, 3, 3, f_dim * 8])\n    print(h1.get_shape().as_list())\n    w1 = init_weight(\"DecoderWeights/Layer2_512_256_4_4_4_w.txt\",[512,256,4,4,4],\"w\")\n    b1 = init_weight(\"DecoderWeights/Layer2_256_b.txt\",[256],\"b\")\n    h1 = conv3d_transpose(\n        h1, f_dim * 4, [4, 4, 4],w1, b1,stride=1, padding= 'VALID',activation_fn=tf.nn.relu)\n    print(h1.get_shape().as_list())\n    w2 = init_weight(\"DecoderWeights/Layer3_256_96_5_5_5_w.txt\",[256,96,5,5,5],\"w\")\n    b2 = init_weight(\"DecoderWeights/Layer3_96_b.txt\",[96],\"b\")\n    h2 = conv3d_transpose(\n        h1, int(f_dim * 3 / 2), [5, 5, 5],w2 , b2, stride=2,padding ='VALID' ,activation_fn=tf.nn.relu)\n    print(h2.get_shape().as_list())\n    w3 = init_weight(\"DecoderWeights/Layer4_96_1_6_6_6_w.txt\",[96,1,6,6,6],\"w\")\n    b3 = init_weight(\"DecoderWeights/Layer4_1_b.txt\",[1],\"b\")\n    h3 = conv3d_transpose(\n        h2, 1, [6, 6, 6], w3,b3, stride=2, padding ='SAME' ,activation_fn=tf.nn.sigmoid)\n    print(h3.get_shape().as_list())\n  return h3\n\nThis is error:\nTraceback (most recent call last):\nFile \"train.py\", line 81, in \nepsilon=1e-08).minimize(loss, global_step=global_step)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize\ngrad_loss=grad_loss)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\ncolocate_gradients_with_ops=colocate_gradients_with_ops)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\n% (op.name, i, t_in.shape, in_grad.shape))\nValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: decoder/Conv3d_transpose_2/conv3d_transpose.  Input index: 2. Original input shape: (6, 15, 15, 15, 96).  Calculated input gradient shape: (6, 16, 16, 16, 96)\nTf version : 1.4.1\nI have 4 layers, 1 fc and 3 conv3d_transpose.\nLayer shape as follows:\n[6, 512]\n[6, 3, 3, 3, 512]\n[6, 6, 6, 6, 256]\n[6, 15, 15, 15, 96]\n[6, 32, 32, 32, 1]\nI calculated conv3d_tranpose output shape as follows:\n def get_deconv_dim(dim_size, stride_size,pading,kernel):\n      if dim_size is not None:\n        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel\n      return dim_size\n   \n   pad = 1\n   if padding == 'VALID':\n     pad = 0\n   out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])\n   out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])\n   out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])\n   out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]\n   outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,\n                                     [1, stride, stride, stride, 1],\n                                     padding)\n\nIn the last conv3d_transpose layer, while its output is calculated as (6,15,15,15,96), input gradient is calculated as (6,16,16,16,96). Therefore, this leads to error.\nWhen i call last conv3d_transpose layer with padding=\"SAME\", it didn't gives error.\nedit:\nEnvironment:\nOs: Ubuntu 16.04.3 LTS\nbazel: N/a\nTensorflow : 1.4.1 via pip install\ncuda: 9.0.176\nGPU: tesla k40, memory: 12gb\ncommand: I uploaded only erroneous python script, just run \"python blabla.py\".  If you want, i can upload all python scripts.", "body": "This is code:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom utils import init_weight\r\nimport tensorflow as tf\r\n\r\nslim = tf.contrib.slim\r\n\r\n\r\n@tf.contrib.framework.add_arg_scope\r\ndef conv3d_transpose(inputs,\r\n                     num_outputs,\r\n                     kernel_size,\r\n                     weights,\r\n                     biases,\r\n                     stride,\r\n                     padding,\r\n                     activation_fn=tf.nn.relu,\r\n                     weights_initializer=tf.contrib.layers.xavier_initializer(),\r\n                     biases_initializer=tf.zeros_initializer(),\r\n                     reuse=None,\r\n                     trainable=True,\r\n                     scope=None):\r\n\r\n  with tf.variable_scope(\r\n      scope, 'Conv3d_transpose', [inputs], reuse=reuse):\r\n    dtype = inputs.dtype.base_dtype\r\n    kernel_d, kernel_h, kernel_w = kernel_size[0:3]\r\n    num_filters_in = inputs.get_shape()[4]\r\n    weights_shape = [kernel_d, kernel_h, kernel_w, num_outputs, num_filters_in]\r\n    \r\n    weights = tf.get_variable('weights',\r\n                              shape=weights_shape,\r\n                              dtype=dtype,\r\n                              initializer=tf.constant_initializer(weights),\r\n                              trainable=trainable)\r\n    \r\n    tf.contrib.framework.add_model_variable(weights)\r\n    \r\n    input_shape = inputs.get_shape().as_list()\r\n    batch_size = input_shape[0]\r\n    depth = input_shape[1]\r\n    height = input_shape[2]\r\n    width = input_shape[3]\r\n\r\n    def get_deconv_dim(dim_size, stride_size,pading,kernel):\r\n      if isinstance(dim_size, tf.Tensor):\r\n        sub = tf.subtract(dim_size,1)\r\n        dim_size = tf.multiply(dim_size,sub)\r\n        dim_size = tf.subtract(dim_size, 2*pading)\r\n        dim_size = tf.add(dim_size,kernel)\r\n        \r\n      elif dim_size is not None:\r\n        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel\r\n      return dim_size\r\n   \r\n    pad = 1\r\n    if padding == 'VALID':\r\n      pad = 0\r\n    out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])\r\n    out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])\r\n    out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])\r\n\r\n    out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]\r\n    outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,\r\n                                     [1, stride, stride, stride, 1],\r\n                                     padding)\r\n\r\n    biases = tf.get_variable('biases',\r\n                               shape=[num_outputs,],\r\n                               dtype=dtype,\r\n                               initializer=tf.constant_initializer(biases),\r\n                               trainable=trainable)\r\n    tf.contrib.framework.add_model_variable(biases)\r\n    outputs = tf.nn.bias_add(outputs, biases)\r\n    \r\n    if activation_fn:\r\n      outputs = activation_fn(outputs)\r\n    return outputs\r\n\r\n\r\ndef model(identities, params, is_training):\r\n  \"\"\"Model transforming embedding to voxels.\"\"\"\r\n  del is_training  # Unused\r\n  f_dim = params.f_dim\r\n  with slim.arg_scope(\r\n      [slim.fully_connected, conv3d_transpose]):\r\n    w0 = init_weight(\"DecoderWeights/Layer1_13824_512_w.txt\",[13824,512],\"w\")\r\n    print(identities.get_shape().as_list())\r\n    h0 = slim.fully_connected(\r\n        identities, 3 * 3 * 3 * f_dim * 8, weights_initializer = tf.constant_initializer(w0), activation_fn=tf.nn.relu)\r\n    b0 = init_weight(\"DecoderWeights/Layer1_13824_b.txt\",[13824],\"b\")\r\n    b0 = tf.get_variable('db0',shape=[13824], initializer=tf.constant_initializer(b0),trainable=True)\r\n    h0 = tf.nn.bias_add(h0,b0)\r\n\r\n    h1 = tf.reshape(h0, [-1, 3, 3, 3, f_dim * 8])\r\n    print(h1.get_shape().as_list())\r\n    w1 = init_weight(\"DecoderWeights/Layer2_512_256_4_4_4_w.txt\",[512,256,4,4,4],\"w\")\r\n    b1 = init_weight(\"DecoderWeights/Layer2_256_b.txt\",[256],\"b\")\r\n    h1 = conv3d_transpose(\r\n        h1, f_dim * 4, [4, 4, 4],w1, b1,stride=1, padding= 'VALID',activation_fn=tf.nn.relu)\r\n    print(h1.get_shape().as_list())\r\n    w2 = init_weight(\"DecoderWeights/Layer3_256_96_5_5_5_w.txt\",[256,96,5,5,5],\"w\")\r\n    b2 = init_weight(\"DecoderWeights/Layer3_96_b.txt\",[96],\"b\")\r\n    h2 = conv3d_transpose(\r\n        h1, int(f_dim * 3 / 2), [5, 5, 5],w2 , b2, stride=2,padding ='VALID' ,activation_fn=tf.nn.relu)\r\n    print(h2.get_shape().as_list())\r\n    w3 = init_weight(\"DecoderWeights/Layer4_96_1_6_6_6_w.txt\",[96,1,6,6,6],\"w\")\r\n    b3 = init_weight(\"DecoderWeights/Layer4_1_b.txt\",[1],\"b\")\r\n    h3 = conv3d_transpose(\r\n        h2, 1, [6, 6, 6], w3,b3, stride=2, padding ='SAME' ,activation_fn=tf.nn.sigmoid)\r\n    print(h3.get_shape().as_list())\r\n  return h3\r\n```\r\n\r\nThis is error:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    epsilon=1e-08).minimize(loss, global_step=global_step)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\r\n    % (op.name, i, t_in.shape, in_grad.shape))\r\nValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: decoder/Conv3d_transpose_2/conv3d_transpose.  Input index: 2. Original input shape: (6, 15, 15, 15, 96).  Calculated input gradient shape: (6, 16, 16, 16, 96)\r\n\r\nTf version : 1.4.1\r\n\r\nI have 4 layers, 1 fc and 3 conv3d_transpose.\r\n\r\nLayer shape as follows:\r\n\r\n[6, 512]\r\n[6, 3, 3, 3, 512]\r\n[6, 6, 6, 6, 256]\r\n[6, 15, 15, 15, 96]\r\n[6, 32, 32, 32, 1]\r\n\r\nI calculated conv3d_tranpose output shape as follows:\r\n```\r\n def get_deconv_dim(dim_size, stride_size,pading,kernel):\r\n      if dim_size is not None:\r\n        dim_size  = (dim_size -1)*stride_size - 2*pading + kernel\r\n      return dim_size\r\n   \r\n   pad = 1\r\n   if padding == 'VALID':\r\n     pad = 0\r\n   out_depth = get_deconv_dim(depth, stride,pad,weights.get_shape().as_list()[0])\r\n   out_height = get_deconv_dim(height, stride,pad,weights.get_shape().as_list()[1])\r\n   out_width = get_deconv_dim(width, stride,pad,weights.get_shape().as_list()[2])\r\n   out_shape = [batch_size, out_depth, out_height, out_width, num_outputs]\r\n   outputs = tf.nn.conv3d_transpose(inputs, weights, out_shape,\r\n                                     [1, stride, stride, stride, 1],\r\n                                     padding)\r\n```\r\nIn the last conv3d_transpose layer, while its output is calculated as (6,15,15,15,96), input gradient is calculated as (6,16,16,16,96). Therefore, this leads to error.\r\n\r\nWhen i call last conv3d_transpose layer with padding=\"SAME\", it didn't gives error.\r\n\r\nedit:\r\n\r\nEnvironment:\r\nOs: Ubuntu 16.04.3 LTS\r\nbazel: N/a\r\nTensorflow : 1.4.1 via pip install\r\ncuda: 9.0.176\r\nGPU: tesla k40, memory: 12gb\r\ncommand: I uploaded only erroneous python script, just run \"python blabla.py\".  If you want, i can upload all python scripts."}