{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5099", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5099/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5099/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5099/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5099", "id": 184331190, "node_id": "MDU6SXNzdWUxODQzMzExOTA=", "number": 5099, "title": "lookup_embedding returning 0 on missing index", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2016-10-20T20:23:26Z", "updated_at": "2018-02-08T00:00:23Z", "closed_at": "2018-02-08T00:00:23Z", "author_association": "NONE", "body_html": "<p>If I create a variable with the first dimension equal to k and I use it for looking up vectors with lookup_embeddings, if I input an invalid index (higher than k or lower than 0) the lookup just returns 0s.<br>\nIn the code below I create a 10x2 embedding matrix e, so that each embedding is of dimension 2. i then lookup for a number that is the step number. This has no use, it's just for showing the issue. Obviously for the first 10 steps the value I fetch is a random embedding of dimension 2, after the tenth iteration, I start getting all 0s.</p>\n<p>I think this should be documented. I'm not sure if it is a good default or not, probably it is, but at least there should be a warning or something similar telling that you are trying to access an index that is not there. It would be really useful for debugging. I would have saved few hours of work if I noticed this before.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 16.04 64bit<br>\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1<br>\nTensorflow version: 0.10.0 compiled for cuda 8.0 (but it is not the issue)</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nnum_embedding = 10\nembedding_size = 2\n\nx = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]])\ny = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1])\n\ngraph = tf.Graph()\nwith graph.as_default():\n    x_p = tf.placeholder(tf.float32, shape=[None, 2])\n    e_p = tf.placeholder(tf.int64, shape=[])\n\n    y_p = tf.placeholder(tf.float32, shape=[None])\n    y_pe = tf.expand_dims(y_p, 1)\n\n    w = tf.Variable(tf.random_normal([2, 1]))\n    b = tf.Variable(tf.random_normal([1]))\n    e = tf.Variable(tf.random_normal([num_embedding, embedding_size]))\n\n    embed = tf.nn.embedding_lookup(e, e_p)\n\n    logits = tf.matmul(x_p, w) + b + tf.reduce_sum(embed)\n\n    xe = tf.nn.sigmoid_cross_entropy_with_logits(logits, y_pe)\n    loss = tf.reduce_mean(xe)\n\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.nn.sigmoid(logits) &gt; 0.5, tf.float32), y_pe), tf.float32))\n    optimizer = tf.train.AdamOptimizer(0.5).minimize(loss)\n\n    #Dangling nodes in the computational graph\n    z_p = tf.placeholder(tf.float32, shape=[None, 2])\n    u = tf.Variable(tf.random_normal([2, 1]))\n    l = tf.matmul(z_p, u)\n\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    for step in range(201):\n        if step % 100 == 0:\n            save_path = saver.save(session, \"lc_{:04d}.ckpt\".format(step))\n        _, loss_val, acc_val, embed_val = session.run(\n            [optimizer, loss, accuracy, embed],\n            feed_dict={x_p: x, e_p: step, y_p: y})\n        print(\"step {step} - loss: {loss_val:.6f}, acc: {acc_val:.4f}\".format(step=step, loss_val=loss_val, acc_val=acc_val))\n        print(\"embed_val: {}\".format(embed_val))\n\n</code></pre>", "body_text": "If I create a variable with the first dimension equal to k and I use it for looking up vectors with lookup_embeddings, if I input an invalid index (higher than k or lower than 0) the lookup just returns 0s.\nIn the code below I create a 10x2 embedding matrix e, so that each embedding is of dimension 2. i then lookup for a number that is the step number. This has no use, it's just for showing the issue. Obviously for the first 10 steps the value I fetch is a random embedding of dimension 2, after the tenth iteration, I start getting all 0s.\nI think this should be documented. I'm not sure if it is a good default or not, probably it is, but at least there should be a warning or something similar telling that you are trying to access an index that is not there. It would be really useful for debugging. I would have saved few hours of work if I noticed this before.\nEnvironment info\nOperating System: Ubuntu 16.04 64bit\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1\nTensorflow version: 0.10.0 compiled for cuda 8.0 (but it is not the issue)\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport numpy as np\nimport tensorflow as tf\n\nnum_embedding = 10\nembedding_size = 2\n\nx = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]])\ny = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1])\n\ngraph = tf.Graph()\nwith graph.as_default():\n    x_p = tf.placeholder(tf.float32, shape=[None, 2])\n    e_p = tf.placeholder(tf.int64, shape=[])\n\n    y_p = tf.placeholder(tf.float32, shape=[None])\n    y_pe = tf.expand_dims(y_p, 1)\n\n    w = tf.Variable(tf.random_normal([2, 1]))\n    b = tf.Variable(tf.random_normal([1]))\n    e = tf.Variable(tf.random_normal([num_embedding, embedding_size]))\n\n    embed = tf.nn.embedding_lookup(e, e_p)\n\n    logits = tf.matmul(x_p, w) + b + tf.reduce_sum(embed)\n\n    xe = tf.nn.sigmoid_cross_entropy_with_logits(logits, y_pe)\n    loss = tf.reduce_mean(xe)\n\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.nn.sigmoid(logits) > 0.5, tf.float32), y_pe), tf.float32))\n    optimizer = tf.train.AdamOptimizer(0.5).minimize(loss)\n\n    #Dangling nodes in the computational graph\n    z_p = tf.placeholder(tf.float32, shape=[None, 2])\n    u = tf.Variable(tf.random_normal([2, 1]))\n    l = tf.matmul(z_p, u)\n\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    for step in range(201):\n        if step % 100 == 0:\n            save_path = saver.save(session, \"lc_{:04d}.ckpt\".format(step))\n        _, loss_val, acc_val, embed_val = session.run(\n            [optimizer, loss, accuracy, embed],\n            feed_dict={x_p: x, e_p: step, y_p: y})\n        print(\"step {step} - loss: {loss_val:.6f}, acc: {acc_val:.4f}\".format(step=step, loss_val=loss_val, acc_val=acc_val))\n        print(\"embed_val: {}\".format(embed_val))", "body": "If I create a variable with the first dimension equal to k and I use it for looking up vectors with lookup_embeddings, if I input an invalid index (higher than k or lower than 0) the lookup just returns 0s.\nIn the code below I create a 10x2 embedding matrix e, so that each embedding is of dimension 2. i then lookup for a number that is the step number. This has no use, it's just for showing the issue. Obviously for the first 10 steps the value I fetch is a random embedding of dimension 2, after the tenth iteration, I start getting all 0s.\n\nI think this should be documented. I'm not sure if it is a good default or not, probably it is, but at least there should be a warning or something similar telling that you are trying to access an index that is not there. It would be really useful for debugging. I would have saved few hours of work if I noticed this before.\n### Environment info\n\nOperating System: Ubuntu 16.04 64bit\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1\nTensorflow version: 0.10.0 compiled for cuda 8.0 (but it is not the issue)\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\nimport numpy as np\nimport tensorflow as tf\n\nnum_embedding = 10\nembedding_size = 2\n\nx = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]])\ny = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1])\n\ngraph = tf.Graph()\nwith graph.as_default():\n    x_p = tf.placeholder(tf.float32, shape=[None, 2])\n    e_p = tf.placeholder(tf.int64, shape=[])\n\n    y_p = tf.placeholder(tf.float32, shape=[None])\n    y_pe = tf.expand_dims(y_p, 1)\n\n    w = tf.Variable(tf.random_normal([2, 1]))\n    b = tf.Variable(tf.random_normal([1]))\n    e = tf.Variable(tf.random_normal([num_embedding, embedding_size]))\n\n    embed = tf.nn.embedding_lookup(e, e_p)\n\n    logits = tf.matmul(x_p, w) + b + tf.reduce_sum(embed)\n\n    xe = tf.nn.sigmoid_cross_entropy_with_logits(logits, y_pe)\n    loss = tf.reduce_mean(xe)\n\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.nn.sigmoid(logits) > 0.5, tf.float32), y_pe), tf.float32))\n    optimizer = tf.train.AdamOptimizer(0.5).minimize(loss)\n\n    #Dangling nodes in the computational graph\n    z_p = tf.placeholder(tf.float32, shape=[None, 2])\n    u = tf.Variable(tf.random_normal([2, 1]))\n    l = tf.matmul(z_p, u)\n\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    for step in range(201):\n        if step % 100 == 0:\n            save_path = saver.save(session, \"lc_{:04d}.ckpt\".format(step))\n        _, loss_val, acc_val, embed_val = session.run(\n            [optimizer, loss, accuracy, embed],\n            feed_dict={x_p: x, e_p: step, y_p: y})\n        print(\"step {step} - loss: {loss_val:.6f}, acc: {acc_val:.4f}\".format(step=step, loss_val=loss_val, acc_val=acc_val))\n        print(\"embed_val: {}\".format(embed_val))\n\n```\n"}