{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/384153013", "html_url": "https://github.com/tensorflow/tensorflow/issues/18708#issuecomment-384153013", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18708", "id": 384153013, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDE1MzAxMw==", "user": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-25T03:53:01Z", "updated_at": "2018-04-25T03:53:01Z", "author_association": "MEMBER", "body_html": "<p>Sorry that I hadn't looked carefully enough at your example.</p>\n<p>The reason for the counterintuitive behavior is that you aren't feeding either of the placeholders in your example. You are feeding 'a' in the outer scope, which is returned by the tf.cond and is I guess a Merge node (you can verify this by printing its name and looking at the graph to find it).</p>\n<p>You could feed the placeholders e.g., by capturing them while the lambdas execute, something like:</p>\n<pre><code>      p_list = []\n\n      def fnTrue():\n        a      = tf.placeholder(tf.float32, shape=[5, 1], name=\"column\")\n        p_list.append(a)\n        return a\n\n    def fnFalse():\n        a      = tf.placeholder(tf.float32, shape=[1, 5], name=\"row\")\n        p_list.append(a)\n        return a\n ...\n\n    feed_dict = {\n        p_list[0]    : plTrue,\n        p_list[1]    : plFalse,\n        pred         : False\n    }\n</code></pre>\n<p>I think if you do that you should get the shape error from the placeholders.</p>\n<p>Instead you are feeding the Merge, which (as you observe) is only constrained to have rank 2. The most precise shape constraint on the Merge is \"either [5,1] or [1,5]\" which our shape inference system is not rich enough to encode, and I don't think it's realistic to make it encode arbitrary expressions like that, so it's probably going to stay at this level of imprecision.</p>\n<p>I wonder from your example if you think that feeding False to p should constrain the Merge to have shape [1,5]. Unfortunately it doesn't, because of the way that graph pruning works in TensorFlow. The algorithm is to look at all the fetches (in this case a) and work backwards from them only as far as necessary to evaluate them given the supplied feeds (in this case a and p). Since a can be computed given a, the pruned graph that is evaluated by the session.run() call is just the singleton node a, and the value of p is ignored. You should also know that the pruning algorithm is based on the identity of the feed and fetch nodes, not the values sent to the feeds, so in the case I gave where I fed the placeholders it would be necessary to feed both placeholders in every run: the set of feeds required to evaluate the graph is determined before the value of p is known, so it has to include both branches.</p>\n<p>There is no doubt that the algorithm for determining which nodes are evaluated given a set of feeds and fetches has confused a lot of people. At this point, however, it's probably not going to change given the amount of code that has been built, so I hope that you can solve your problem based on this explanation of the graph pruning process.</p>", "body_text": "Sorry that I hadn't looked carefully enough at your example.\nThe reason for the counterintuitive behavior is that you aren't feeding either of the placeholders in your example. You are feeding 'a' in the outer scope, which is returned by the tf.cond and is I guess a Merge node (you can verify this by printing its name and looking at the graph to find it).\nYou could feed the placeholders e.g., by capturing them while the lambdas execute, something like:\n      p_list = []\n\n      def fnTrue():\n        a      = tf.placeholder(tf.float32, shape=[5, 1], name=\"column\")\n        p_list.append(a)\n        return a\n\n    def fnFalse():\n        a      = tf.placeholder(tf.float32, shape=[1, 5], name=\"row\")\n        p_list.append(a)\n        return a\n ...\n\n    feed_dict = {\n        p_list[0]    : plTrue,\n        p_list[1]    : plFalse,\n        pred         : False\n    }\n\nI think if you do that you should get the shape error from the placeholders.\nInstead you are feeding the Merge, which (as you observe) is only constrained to have rank 2. The most precise shape constraint on the Merge is \"either [5,1] or [1,5]\" which our shape inference system is not rich enough to encode, and I don't think it's realistic to make it encode arbitrary expressions like that, so it's probably going to stay at this level of imprecision.\nI wonder from your example if you think that feeding False to p should constrain the Merge to have shape [1,5]. Unfortunately it doesn't, because of the way that graph pruning works in TensorFlow. The algorithm is to look at all the fetches (in this case a) and work backwards from them only as far as necessary to evaluate them given the supplied feeds (in this case a and p). Since a can be computed given a, the pruned graph that is evaluated by the session.run() call is just the singleton node a, and the value of p is ignored. You should also know that the pruning algorithm is based on the identity of the feed and fetch nodes, not the values sent to the feeds, so in the case I gave where I fed the placeholders it would be necessary to feed both placeholders in every run: the set of feeds required to evaluate the graph is determined before the value of p is known, so it has to include both branches.\nThere is no doubt that the algorithm for determining which nodes are evaluated given a set of feeds and fetches has confused a lot of people. At this point, however, it's probably not going to change given the amount of code that has been built, so I hope that you can solve your problem based on this explanation of the graph pruning process.", "body": "Sorry that I hadn't looked carefully enough at your example.\r\n\r\nThe reason for the counterintuitive behavior is that you aren't feeding either of the placeholders in your example. You are feeding 'a' in the outer scope, which is returned by the tf.cond and is I guess a Merge node (you can verify this by printing its name and looking at the graph to find it).\r\n\r\nYou could feed the placeholders e.g., by capturing them while the lambdas execute, something like:\r\n\r\n```\r\n      p_list = []\r\n\r\n      def fnTrue():\r\n        a      = tf.placeholder(tf.float32, shape=[5, 1], name=\"column\")\r\n        p_list.append(a)\r\n        return a\r\n\r\n    def fnFalse():\r\n        a      = tf.placeholder(tf.float32, shape=[1, 5], name=\"row\")\r\n        p_list.append(a)\r\n        return a\r\n ...\r\n\r\n    feed_dict = {\r\n        p_list[0]    : plTrue,\r\n        p_list[1]    : plFalse,\r\n        pred         : False\r\n    }\r\n```\r\n\r\nI think if you do that you should get the shape error from the placeholders.\r\n\r\nInstead you are feeding the Merge, which (as you observe) is only constrained to have rank 2. The most precise shape constraint on the Merge is \"either [5,1] or [1,5]\" which our shape inference system is not rich enough to encode, and I don't think it's realistic to make it encode arbitrary expressions like that, so it's probably going to stay at this level of imprecision.\r\n\r\nI wonder from your example if you think that feeding False to p should constrain the Merge to have shape [1,5]. Unfortunately it doesn't, because of the way that graph pruning works in TensorFlow. The algorithm is to look at all the fetches (in this case a) and work backwards from them only as far as necessary to evaluate them given the supplied feeds (in this case a and p). Since a can be computed given a, the pruned graph that is evaluated by the session.run() call is just the singleton node a, and the value of p is ignored. You should also know that the pruning algorithm is based on the identity of the feed and fetch nodes, not the values sent to the feeds, so in the case I gave where I fed the placeholders it would be necessary to feed both placeholders in every run: the set of feeds required to evaluate the graph is determined before the value of p is known, so it has to include both branches.\r\n\r\nThere is no doubt that the algorithm for determining which nodes are evaluated given a set of feeds and fetches has confused a lot of people. At this point, however, it's probably not going to change given the amount of code that has been built, so I hope that you can solve your problem based on this explanation of the graph pruning process."}