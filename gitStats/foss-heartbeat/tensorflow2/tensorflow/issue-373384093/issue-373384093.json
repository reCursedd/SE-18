{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23210", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23210/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23210/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23210/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23210", "id": 373384093, "node_id": "MDU6SXNzdWUzNzMzODQwOTM=", "number": 23210, "title": "Different inference time on my computer and server with the same gpu", "user": {"login": "ningningG", "id": 17879317, "node_id": "MDQ6VXNlcjE3ODc5MzE3", "avatar_url": "https://avatars2.githubusercontent.com/u/17879317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ningningG", "html_url": "https://github.com/ningningG", "followers_url": "https://api.github.com/users/ningningG/followers", "following_url": "https://api.github.com/users/ningningG/following{/other_user}", "gists_url": "https://api.github.com/users/ningningG/gists{/gist_id}", "starred_url": "https://api.github.com/users/ningningG/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ningningG/subscriptions", "organizations_url": "https://api.github.com/users/ningningG/orgs", "repos_url": "https://api.github.com/users/ningningG/repos", "events_url": "https://api.github.com/users/ningningG/events{/privacy}", "received_events_url": "https://api.github.com/users/ningningG/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547538, "node_id": "MDU6TGFiZWwxMDk3NTQ3NTM4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:gpu", "name": "comp:gpu", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-24T09:17:59Z", "updated_at": "2018-10-26T06:48:49Z", "closed_at": "2018-10-26T06:48:49Z", "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>\n<p>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</p>\n</li>\n<li>\n<p>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):<br>\nWin10</p>\n</li>\n<li>\n<p>TensorFlow installed from (source or binary):<br>\nanaconda3</p>\n</li>\n<li>\n<p>TensorFlow version (use command below):<br>\ntensorflow-gpu1.11.0</p>\n</li>\n<li>\n<p>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A</p>\n</li>\n<li>\n<p>Python version:<br>\npython3.6.6</p>\n</li>\n<li>\n<p>Bazel version (if compiling from source): N/A</p>\n</li>\n<li>\n<p>GCC/Compiler version (if compiling from source): N/A</p>\n</li>\n<li>\n<p>CUDA/cuDNN version:<br>\nCUDA9.0</p>\n</li>\n<li>\n<p>GPU model and memory:<br>\nNVIDIA GeForce GTX 1080 Ti  11264MiB</p>\n</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nI have trained a 2 classified ResNet18\uff0c when I use it for inference on different machine, the time is as follow:<br>\nabout 2.0s on my conputer\uff0c<br>\nabout 1.6s on my workmate's computer (win10,  tensorflow-gpu of version 1.9, python 3.6.6, CUDA9.0,  NVIDIA GeForce GTX 1080 Ti 11264MiB)\uff0c<br>\nabout 0.65s on a server (Ubuntu16.04, tensorflow-gpu of version 1.9, python 3.6,2, CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB),<br>\nabout 0.22s on another server (Ubuntu16.04,  tensorflow-gpu of version 1.8, python 2.7.13,  CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB).<br>\nThe code is exactly the same, only change is the model path and data path. Why there is so huge difference?</p>\n<p><strong>Code to reproduce the issue</strong><br>\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<pre><code>model_path = 'D:/proj_tf/models_pb/slice_cor.pb'\nimage_path = 'D:/proj_tf/data/'\nfiles_in = ['0205_256_06.raw', '0205_256_07.raw', '0205_256_08.raw']\ndata_image = np.zeros((3, 256, 256, 3), dtype=np.float32)\nfor index in range(3):\n    image_file = os.path.join(image_path, files_in[index])\n    data_tmp = np.fromfile(image_file, dtype=np.float32)\n    data_tmp.shape = 256, 256\n    min_value = data_tmp.min()\n    max_value = data_tmp.max()\n    data_tmp -= min_value\n    data_tmp /= (max_value - min_value)\n    for i in range(3):\n        data_image[index, :, :, i] = data_tmp\n    \nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.7\nsess = tf.Session(config=config)\nwith gfile.FastGFile(model_path, 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    sess.graph.as_default()\n    tf.import_graph_def(graph_def, name=\"\")\n    \n    sess.run(tf.global_variables_initializer())\n    inp = sess.graph.get_tensor_by_name('data:0')\n    out = sess.graph.get_tensor_by_name('softmax:0')\n    \n    start_time = time.time()\n    pred_slice = sess.run(out, {inp: data_image})\n    end_time = time.time()\n    \n    print(\"time for inference : \", end_time - start_time)\n    sess.close()   \n</code></pre>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nWin10\n\n\nTensorFlow installed from (source or binary):\nanaconda3\n\n\nTensorFlow version (use command below):\ntensorflow-gpu1.11.0\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\n\n\nPython version:\npython3.6.6\n\n\nBazel version (if compiling from source): N/A\n\n\nGCC/Compiler version (if compiling from source): N/A\n\n\nCUDA/cuDNN version:\nCUDA9.0\n\n\nGPU model and memory:\nNVIDIA GeForce GTX 1080 Ti  11264MiB\n\n\nDescribe the current behavior\nI have trained a 2 classified ResNet18\uff0c when I use it for inference on different machine, the time is as follow:\nabout 2.0s on my conputer\uff0c\nabout 1.6s on my workmate's computer (win10,  tensorflow-gpu of version 1.9, python 3.6.6, CUDA9.0,  NVIDIA GeForce GTX 1080 Ti 11264MiB)\uff0c\nabout 0.65s on a server (Ubuntu16.04, tensorflow-gpu of version 1.9, python 3.6,2, CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB),\nabout 0.22s on another server (Ubuntu16.04,  tensorflow-gpu of version 1.8, python 2.7.13,  CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB).\nThe code is exactly the same, only change is the model path and data path. Why there is so huge difference?\nCode to reproduce the issue\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\nmodel_path = 'D:/proj_tf/models_pb/slice_cor.pb'\nimage_path = 'D:/proj_tf/data/'\nfiles_in = ['0205_256_06.raw', '0205_256_07.raw', '0205_256_08.raw']\ndata_image = np.zeros((3, 256, 256, 3), dtype=np.float32)\nfor index in range(3):\n    image_file = os.path.join(image_path, files_in[index])\n    data_tmp = np.fromfile(image_file, dtype=np.float32)\n    data_tmp.shape = 256, 256\n    min_value = data_tmp.min()\n    max_value = data_tmp.max()\n    data_tmp -= min_value\n    data_tmp /= (max_value - min_value)\n    for i in range(3):\n        data_image[index, :, :, i] = data_tmp\n    \nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.7\nsess = tf.Session(config=config)\nwith gfile.FastGFile(model_path, 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    sess.graph.as_default()\n    tf.import_graph_def(graph_def, name=\"\")\n    \n    sess.run(tf.global_variables_initializer())\n    inp = sess.graph.get_tensor_by_name('data:0')\n    out = sess.graph.get_tensor_by_name('softmax:0')\n    \n    start_time = time.time()\n    pred_slice = sess.run(out, {inp: data_image})\n    end_time = time.time()\n    \n    print(\"time for inference : \", end_time - start_time)\n    sess.close()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWin10\r\n\r\n- TensorFlow installed from (source or binary):\r\nanaconda3\r\n\r\n- TensorFlow version (use command below):\r\ntensorflow-gpu1.11.0\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n\r\n- Python version:\r\npython3.6.6\r\n\r\n- Bazel version (if compiling from source): N/A\r\n\r\n- GCC/Compiler version (if compiling from source): N/A\r\n\r\n- CUDA/cuDNN version:\r\nCUDA9.0\r\n\r\n- GPU model and memory:\r\nNVIDIA GeForce GTX 1080 Ti  11264MiB\r\n\r\n**Describe the current behavior**\r\nI have trained a 2 classified ResNet18\uff0c when I use it for inference on different machine, the time is as follow:\r\nabout 2.0s on my conputer\uff0c\r\nabout 1.6s on my workmate's computer (win10,  tensorflow-gpu of version 1.9, python 3.6.6, CUDA9.0,  NVIDIA GeForce GTX 1080 Ti 11264MiB)\uff0c \r\nabout 0.65s on a server (Ubuntu16.04, tensorflow-gpu of version 1.9, python 3.6,2, CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB), \r\nabout 0.22s on another server (Ubuntu16.04,  tensorflow-gpu of version 1.8, python 2.7.13,  CUDA9.0, NVIDIA GeForce GTX 1080 Ti 11171MiB). \r\nThe code is exactly the same, only change is the model path and data path. Why there is so huge difference?\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nmodel_path = 'D:/proj_tf/models_pb/slice_cor.pb'\r\nimage_path = 'D:/proj_tf/data/'\r\nfiles_in = ['0205_256_06.raw', '0205_256_07.raw', '0205_256_08.raw']\r\ndata_image = np.zeros((3, 256, 256, 3), dtype=np.float32)\r\nfor index in range(3):\r\n    image_file = os.path.join(image_path, files_in[index])\r\n    data_tmp = np.fromfile(image_file, dtype=np.float32)\r\n    data_tmp.shape = 256, 256\r\n    min_value = data_tmp.min()\r\n    max_value = data_tmp.max()\r\n    data_tmp -= min_value\r\n    data_tmp /= (max_value - min_value)\r\n    for i in range(3):\r\n        data_image[index, :, :, i] = data_tmp\r\n    \r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.7\r\nsess = tf.Session(config=config)\r\nwith gfile.FastGFile(model_path, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    sess.graph.as_default()\r\n    tf.import_graph_def(graph_def, name=\"\")\r\n    \r\n    sess.run(tf.global_variables_initializer())\r\n    inp = sess.graph.get_tensor_by_name('data:0')\r\n    out = sess.graph.get_tensor_by_name('softmax:0')\r\n    \r\n    start_time = time.time()\r\n    pred_slice = sess.run(out, {inp: data_image})\r\n    end_time = time.time()\r\n    \r\n    print(\"time for inference : \", end_time - start_time)\r\n    sess.close()   \r\n```\r\n\r\n"}