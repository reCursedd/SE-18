{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13154", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13154/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13154/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13154/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13154", "id": 258828231, "node_id": "MDU6SXNzdWUyNTg4MjgyMzE=", "number": 13154, "title": "BeamSearchDecoder should support an AttentionWrapper cell with alignment_history enabled", "user": {"login": "guillaumekln", "id": 4805513, "node_id": "MDQ6VXNlcjQ4MDU1MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4805513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guillaumekln", "html_url": "https://github.com/guillaumekln", "followers_url": "https://api.github.com/users/guillaumekln/followers", "following_url": "https://api.github.com/users/guillaumekln/following{/other_user}", "gists_url": "https://api.github.com/users/guillaumekln/gists{/gist_id}", "starred_url": "https://api.github.com/users/guillaumekln/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guillaumekln/subscriptions", "organizations_url": "https://api.github.com/users/guillaumekln/orgs", "repos_url": "https://api.github.com/users/guillaumekln/repos", "events_url": "https://api.github.com/users/guillaumekln/events{/privacy}", "received_events_url": "https://api.github.com/users/guillaumekln/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-09-19T13:51:06Z", "updated_at": "2018-03-18T22:18:47Z", "closed_at": "2018-03-18T22:18:47Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: yes</li>\n<li><strong>OS Platform and Distribution</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from</strong>: binary</li>\n<li><strong>TensorFlow version</strong>: 1.3.0</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Exact command to reproduce</strong>: see the code snippet below.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Currently, setting <code>tf.contrib.seq2seq.AttentionWrapper</code>'s <code>alignment_history</code> argument to <code>True</code> and using this cell in a <code>tf.contrib.seq2seq.BeamSearchDecoder</code> does not work for 2 reasons:</p>\n<ol>\n<li>In this configuration, the <code>tf.contrib.seq2seq.AttentionWrapper.state_size</code> property is invalid as it does not have the same structure as <code>zero_state</code> (see the code below). The <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L193\">decoder state initialization</a> is failing because of this.</li>\n<li><code>tf.contrib.seq2seq.BeamSearchDecoder</code> <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L123\">raises an error</a> when the state contains a <code>TensorArray</code>, which is the type currently used to gather alignments.</li>\n</ol>\n<p>I believe this configuration should be supported as it is a standard use case for sequence to sequence models.</p>\n<p>To address both of these limitations, it seems this <code>alignment_history</code> could be a <code>Tensor</code> on which alignments are repeatedly concatenated. Would it work?</p>\n<h3>Source code / logs</h3>\n<p>This code sample reproduces 1. which is the error directly visible when using this configuration.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\nnum_units <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\nmemory <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>, num_units))\n\nattention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.LuongAttention(\n  num_units,\n  memory)\n\ncell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n  tf.contrib.rnn.LSTMCell(num_units),\n  attention_mechanism,\n  <span class=\"pl-v\">alignment_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set this to False to make it work.</span>\n\ntf.contrib.framework.nest.assert_same_structure(\n  cell.zero_state(batch_size, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32),\n  cell.state_size)</pre></div>\n<p>It exits with this error:</p>\n<pre lang=\"text\"><code>Traceback (most recent call last):\n  File \"&lt;file&gt;\", line 19, in &lt;module&gt;\n    cell.state_size)\n  File \"&lt;dir&gt;/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 199, in assert_same_structure\n    % (len_nest1, nest1, len_nest2, nest2))\nValueError: The two structures don't have the same number of elements.\n\nFirst structure (6 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=&lt;tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(2, 10) dtype=float32&gt;, h=&lt;tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(2, 10) dtype=float32&gt;), attention=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(2, 10) dtype=float32&gt;, time=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32&gt;, alignments=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(2, ?) dtype=float32&gt;, alignment_history=&lt;tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fbd6326ec50&gt;)\n\nSecond structure (5 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=10, h=10), attention=10, time=TensorShape([]), alignments=&lt;tf.Tensor 'LuongAttention/strided_slice_2:0' shape=() dtype=int32&gt;, alignment_history=())\n</code></pre>", "body_text": "System information\n\nHave I written custom code: yes\nOS Platform and Distribution: Ubuntu 16.04\nTensorFlow installed from: binary\nTensorFlow version: 1.3.0\nPython version: 2.7\nExact command to reproduce: see the code snippet below.\n\nDescribe the problem\nCurrently, setting tf.contrib.seq2seq.AttentionWrapper's alignment_history argument to True and using this cell in a tf.contrib.seq2seq.BeamSearchDecoder does not work for 2 reasons:\n\nIn this configuration, the tf.contrib.seq2seq.AttentionWrapper.state_size property is invalid as it does not have the same structure as zero_state (see the code below). The decoder state initialization is failing because of this.\ntf.contrib.seq2seq.BeamSearchDecoder raises an error when the state contains a TensorArray, which is the type currently used to gather alignments.\n\nI believe this configuration should be supported as it is a standard use case for sequence to sequence models.\nTo address both of these limitations, it seems this alignment_history could be a Tensor on which alignments are repeatedly concatenated. Would it work?\nSource code / logs\nThis code sample reproduces 1. which is the error directly visible when using this configuration.\nimport tensorflow as tf\n\nbatch_size = 2\nnum_units = 10\n\nmemory = tf.placeholder(tf.float32, shape=(None, None, num_units))\n\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n  num_units,\n  memory)\n\ncell = tf.contrib.seq2seq.AttentionWrapper(\n  tf.contrib.rnn.LSTMCell(num_units),\n  attention_mechanism,\n  alignment_history=True) # Set this to False to make it work.\n\ntf.contrib.framework.nest.assert_same_structure(\n  cell.zero_state(batch_size, dtype=tf.float32),\n  cell.state_size)\nIt exits with this error:\nTraceback (most recent call last):\n  File \"<file>\", line 19, in <module>\n    cell.state_size)\n  File \"<dir>/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 199, in assert_same_structure\n    % (len_nest1, nest1, len_nest2, nest2))\nValueError: The two structures don't have the same number of elements.\n\nFirst structure (6 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(2, 10) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(2, 10) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(2, 10) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(2, ?) dtype=float32>, alignment_history=<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fbd6326ec50>)\n\nSecond structure (5 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=10, h=10), attention=10, time=TensorShape([]), alignments=<tf.Tensor 'LuongAttention/strided_slice_2:0' shape=() dtype=int32>, alignment_history=())", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.3.0\r\n- **Python version**: 2.7\r\n- **Exact command to reproduce**: see the code snippet below.\r\n\r\n### Describe the problem\r\n\r\nCurrently, setting `tf.contrib.seq2seq.AttentionWrapper`'s `alignment_history` argument to `True` and using this cell in a `tf.contrib.seq2seq.BeamSearchDecoder` does not work for 2 reasons:\r\n\r\n1. In this configuration, the `tf.contrib.seq2seq.AttentionWrapper.state_size` property is invalid as it does not have the same structure as `zero_state` (see the code below). The [decoder state initialization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L193) is failing because of this.\r\n2. `tf.contrib.seq2seq.BeamSearchDecoder` [raises an error](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L123) when the state contains a `TensorArray`, which is the type currently used to gather alignments.\r\n\r\nI believe this configuration should be supported as it is a standard use case for sequence to sequence models.\r\n\r\nTo address both of these limitations, it seems this `alignment_history` could be a `Tensor` on which alignments are repeatedly concatenated. Would it work?\r\n\r\n### Source code / logs\r\n\r\nThis code sample reproduces 1. which is the error directly visible when using this configuration.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nbatch_size = 2\r\nnum_units = 10\r\n\r\nmemory = tf.placeholder(tf.float32, shape=(None, None, num_units))\r\n\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n  num_units,\r\n  memory)\r\n\r\ncell = tf.contrib.seq2seq.AttentionWrapper(\r\n  tf.contrib.rnn.LSTMCell(num_units),\r\n  attention_mechanism,\r\n  alignment_history=True) # Set this to False to make it work.\r\n\r\ntf.contrib.framework.nest.assert_same_structure(\r\n  cell.zero_state(batch_size, dtype=tf.float32),\r\n  cell.state_size)\r\n```\r\n\r\nIt exits with this error:\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"<file>\", line 19, in <module>\r\n    cell.state_size)\r\n  File \"<dir>/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 199, in assert_same_structure\r\n    % (len_nest1, nest1, len_nest2, nest2))\r\nValueError: The two structures don't have the same number of elements.\r\n\r\nFirst structure (6 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(2, 10) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(2, 10) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(2, 10) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(2, ?) dtype=float32>, alignment_history=<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fbd6326ec50>)\r\n\r\nSecond structure (5 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=10, h=10), attention=10, time=TensorShape([]), alignments=<tf.Tensor 'LuongAttention/strided_slice_2:0' shape=() dtype=int32>, alignment_history=())\r\n```"}