{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7996", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7996/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7996/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7996/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7996", "id": 211291630, "node_id": "MDU6SXNzdWUyMTEyOTE2MzA=", "number": 7996, "title": "Cant  implement Miking Human", "user": {"login": "danlutan", "id": 26005896, "node_id": "MDQ6VXNlcjI2MDA1ODk2", "avatar_url": "https://avatars3.githubusercontent.com/u/26005896?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danlutan", "html_url": "https://github.com/danlutan", "followers_url": "https://api.github.com/users/danlutan/followers", "following_url": "https://api.github.com/users/danlutan/following{/other_user}", "gists_url": "https://api.github.com/users/danlutan/gists{/gist_id}", "starred_url": "https://api.github.com/users/danlutan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danlutan/subscriptions", "organizations_url": "https://api.github.com/users/danlutan/orgs", "repos_url": "https://api.github.com/users/danlutan/repos", "events_url": "https://api.github.com/users/danlutan/events{/privacy}", "received_events_url": "https://api.github.com/users/danlutan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-02T05:14:16Z", "updated_at": "2017-03-02T23:31:48Z", "closed_at": "2017-03-02T23:31:48Z", "author_association": "NONE", "body_html": "<p>I want to implement a neural architecture from  A Neural Architecture Mimicking Humans<br>\nEnd-to-End for Natural Language Inference . In keras , I can get accuracy  86% . But the same  architecture in tensorflow , just get 76% . Here is the main code \uff1a<br>\n`class miki(BaseModel):</p>\n<pre><code>def build(self):\n    params = self.params\n    batch_size = params.batch_size\n    max_length = params.max_length\n    nr_hidden = params.nr_hidden\n    keep_dr=params.dr\n\n\n    ids1 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='premise')  # none l1\n    ids2 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='hypoyhesis')\n    lable = tf.placeholder(tf.float32, shape=[batch_size, 3], name='lable')\n    train_dr = tf.placeholder(tf.bool)\n\n\n    oov_W = np.load(open(os.path.join('glove/', 'oov_W.weights'), 'rb'))\n    oov_W = oov_W.astype('float32')\n    unchanged_W = np.load(open(os.path.join('glove/', 'unchanged_W.weights'), 'rb'))\n    unchanged_W = unchanged_W.astype('float32')\n    embedding = np.concatenate((oov_W, unchanged_W), axis=0)\n    W = tf.get_variable(name=\"W\", shape=embedding.shape, initializer=tf.constant_initializer(embedding),\n                        trainable=False)\n\n    def he_nomal(fan_in):\n        s = np.sqrt(2. / fan_in)\n        seed = np.random.randint(10e8)\n        return tf.random_normal_initializer(0.0, s, dtype=tf.float32, seed=seed)\n\n    with tf.variable_scope('embedding1'):\n        embed1 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\n        em_reshape1=tf.reshape(embed1,[-1,nr_hidden])\n        network_en1 = tl.layers.InputLayer(inputs=em_reshape1, name='em_layer1-1')\n        network_en1 = tl.layers.DropoutLayer(network_en1, is_fix=True, keep=keep_dr, name='emdrop1-1',is_train=train_dr)\n        network_en1 = tl.layers.DenseLayer(network_en1, n_units=nr_hidden,\n                                       act=tf.nn.relu, name='emrelu2-1')\n        embed1_out=network_en1.outputs\n        embed1_re=tf.reshape(embed1_out,[-1,max_length,nr_hidden])\n\n\n    with tf.variable_scope('embedding2'):\n        embed2 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\n        em_reshape2=tf.reshape(embed2,[-1,nr_hidden])\n        network_en2 = tl.layers.InputLayer(inputs=em_reshape2, name='em_layer2-1')\n        network_en2 = tl.layers.DropoutLayer(network_en2, is_fix=True, keep=keep_dr, name='emdrop2-1',is_train=train_dr)\n        network_en2 = tl.layers.DenseLayer(network_en2, n_units=nr_hidden,\n                                       act=tf.nn.relu, name='emrelu1-1')\n        embed2_out=network_en2.outputs\n        embed2_re=tf.reshape(embed2_out,[-1,max_length,nr_hidden])\n\n\n    _seq_len = tf.fill(tf.expand_dims(batch_size, 0),\n                       tf.constant(max_length, dtype=tf.int32))\n\n    with tf.variable_scope('ecode1'):\n        fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        h, _ = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed1_re, sequence_length=(_seq_len), dtype=tf.float32)\n        encode1 = tf.concat(2, h)  # none l 2h\n\n    with tf.variable_scope('ecode2'):\n        fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        h, _ = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed2_re, sequence_length=(_seq_len), dtype=tf.float32)\n        encode2 = tf.concat(2, h)  # none l 2h\n\n    with tf.variable_scope('atte_layer'):\n\n        encode2_tr = tf.transpose(encode2, perm=[0, 2, 1])  # none 2h l2\n        attention = tf.batch_matmul(encode1, encode2_tr)  # none l1 l2\n        e = tf.exp(attention - tf.reduce_max(attention, 2, keep_dims=True))\n        s = tf.reduce_sum(e, 2, keep_dims=True)  #none l1 1\n        am_att = e / s  #none l1 l2\n        aligh_attention = tf.batch_matmul(am_att, encode2)  # none l1 2h\n        concat = tf.concat(2, [aligh_attention, encode1])  # none l1 4h\n        concat_reshape=tf.reshape(concat,[-1,4*nr_hidden])    #none*l1 4h\n\n\n    with tf.variable_scope('task1_operator'):\n        network_task1=tl.layers.InputLayer(concat_reshape, name='task_layer1-1')\n        network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-1',is_train=train_dr,is_fix=True)\n        network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-1')\n        network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-2',is_train=train_dr,is_fix=True)\n        network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-2')\n        task1 = network_task1.outputs  # none*l 2h\n        task1_re = tf.reshape(task1, [-1, max_length, 2 * nr_hidden])  #none l h\n\n    with tf.variable_scope('task2_operator'):\n        network_task2=tl.layers.InputLayer(concat_reshape, name='task_layer2-1')\n        network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-1',is_train=train_dr,is_fix=True)\n        network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-1')\n        network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-2',is_train=train_dr,is_fix=True)\n        network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-2')\n        task2 = network_task2.outputs  # none*l 2h\n        task2_re = tf.reshape(task2, [-1, max_length, 2 * nr_hidden])  #none l h\n\n    with tf.variable_scope('task3_operator'):\n        network_task3=tl.layers.InputLayer(concat_reshape, name='task_layer3-1')\n        network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-1',is_train=train_dr,is_fix=True)\n        network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-1')\n        network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-2',is_train=train_dr,is_fix=True)\n        network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-2')\n        task3 = network_task3.outputs  # none*l 2h\n        task3_re = tf.reshape(task3, [-1, max_length, 2 * nr_hidden])  #none l h\n\n    with tf.variable_scope('task4_operator'):\n        network_task4=tl.layers.InputLayer(concat_reshape, name='task_layer4-1')\n        network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-1',is_train=train_dr,is_fix=True)\n        network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-1')\n        network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-2',is_train=train_dr,is_fix=True)\n        network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-2')\n        task4 = network_task4.outputs  # none*l 2h\n        task4_re = tf.reshape(task4, [-1, max_length, 2 * nr_hidden])  #none l h\n\n\n    with tf.variable_scope('gate_operator'):\n        network_gate= tl.layers.InputLayer(concat_reshape, name='gate_layer')\n        network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop1',is_train=train_dr)\n        network_gate = tl.layers.DenseLayer(network_gate, n_units=2 * nr_hidden,\n                                            act=tf.nn.relu, name='relu_gate1')\n        network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop2',is_train=train_dr)\n        network_gate = tl.layers.DenseLayer(network_gate, n_units=4,\n                                            act=tf.nn.softmax, name='relu_gate2')\n        gate = network_gate.outputs\n        gate_re = tf.reshape(gate, [-1, max_length, 4])  # none l 2\n\n    def repeat(gate_i):\n\n        gate_a = gate_i  # none l\n        gate_b = tf.tile(gate_a, [1, 2 * nr_hidden])\n        gate_c = tf.reshape(gate_b, [-1, 2 * nr_hidden, max_length])\n        gate_d = tf.transpose(gate_c, perm=[0, 2, 1])\n        return gate_d  # none l 2h\n\n    def Out(t1,t2,t3,t4,gate):\n        gate_shuffle = tf.transpose(gate, perm=[0, 2, 1])  # none 4 l\n\n        g1 = repeat(gate_shuffle[:, 0, :])  # none l\n        g2 = repeat(gate_shuffle[:, 1, :])\n        g3 = repeat(gate_shuffle[:, 2, :])  # none l\n        g4 = repeat(gate_shuffle[:, 3, :])\n\n        O1=g1*t1\n        O2 = g2 * t2\n        O3=g3*t3\n        O4 = g4 * t4\n\n        out = O1 + O2 +O3+O4\n        return out\n\n    task_output = Out(task1_re,task2_re,task3_re,task4_re,gate_re)  # none l1 2h\n\n\n    with tf.variable_scope('aggregate'):\n        fwd_lstm = tf.nn.rnn_cell.BasicLSTMCell(2 * nr_hidden)\n        x_output, x_state = tf.nn.dynamic_rnn(cell=fwd_lstm, inputs=task_output, dtype=tf.float32,sequence_length=(_seq_len))\n\n        composable = x_output[:,-1, :]  # none  2h\n\n    with tf.variable_scope('Entaiment'):\n        network3 = tl.layers.InputLayer(composable, name='layer_entalment1')\n        network3 = tl.layers.DropoutLayer(network3, keep=keep_dr, name='entaidrop3',is_train=train_dr,is_fix=True)\n        network3 = tl.layers.DenseLayer(network3, n_units=2 * nr_hidden,\n                                        act=tf.nn.tanh, name='layer_entaiment2')\n        network3 = tl.layers.DenseLayer(network3, n_units=3,\n                                        act=tf.nn.softmax, name='entai_relu3.2')\n        entaiment = network3.outputs\n</code></pre>\n<p>with tf.name_scope('Loss'):<br>\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(lable * tf.log(entaiment), reduction_indices=[1]))</p>\n<pre><code>        loss = cross_entropy\n\n    with tf.variable_scope('Accuracy'):\n        predicts = tf.cast(tf.argmax(entaiment, 1), 'int32')   #entaiment none 3\n        lable_one = tf.cast(tf.argmax(lable, 1), 'int32')\n        corrects = tf.equal(predicts, lable_one)\n        num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n        accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n\n    optimizer = tf.train.AdamOptimizer(params.learning_rate)\n    opt_op = optimizer.minimize(loss, global_step=self.global_step)\n</code></pre>\n<p>`</p>", "body_text": "I want to implement a neural architecture from  A Neural Architecture Mimicking Humans\nEnd-to-End for Natural Language Inference . In keras , I can get accuracy  86% . But the same  architecture in tensorflow , just get 76% . Here is the main code \uff1a\n`class miki(BaseModel):\ndef build(self):\n    params = self.params\n    batch_size = params.batch_size\n    max_length = params.max_length\n    nr_hidden = params.nr_hidden\n    keep_dr=params.dr\n\n\n    ids1 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='premise')  # none l1\n    ids2 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='hypoyhesis')\n    lable = tf.placeholder(tf.float32, shape=[batch_size, 3], name='lable')\n    train_dr = tf.placeholder(tf.bool)\n\n\n    oov_W = np.load(open(os.path.join('glove/', 'oov_W.weights'), 'rb'))\n    oov_W = oov_W.astype('float32')\n    unchanged_W = np.load(open(os.path.join('glove/', 'unchanged_W.weights'), 'rb'))\n    unchanged_W = unchanged_W.astype('float32')\n    embedding = np.concatenate((oov_W, unchanged_W), axis=0)\n    W = tf.get_variable(name=\"W\", shape=embedding.shape, initializer=tf.constant_initializer(embedding),\n                        trainable=False)\n\n    def he_nomal(fan_in):\n        s = np.sqrt(2. / fan_in)\n        seed = np.random.randint(10e8)\n        return tf.random_normal_initializer(0.0, s, dtype=tf.float32, seed=seed)\n\n    with tf.variable_scope('embedding1'):\n        embed1 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\n        em_reshape1=tf.reshape(embed1,[-1,nr_hidden])\n        network_en1 = tl.layers.InputLayer(inputs=em_reshape1, name='em_layer1-1')\n        network_en1 = tl.layers.DropoutLayer(network_en1, is_fix=True, keep=keep_dr, name='emdrop1-1',is_train=train_dr)\n        network_en1 = tl.layers.DenseLayer(network_en1, n_units=nr_hidden,\n                                       act=tf.nn.relu, name='emrelu2-1')\n        embed1_out=network_en1.outputs\n        embed1_re=tf.reshape(embed1_out,[-1,max_length,nr_hidden])\n\n\n    with tf.variable_scope('embedding2'):\n        embed2 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\n        em_reshape2=tf.reshape(embed2,[-1,nr_hidden])\n        network_en2 = tl.layers.InputLayer(inputs=em_reshape2, name='em_layer2-1')\n        network_en2 = tl.layers.DropoutLayer(network_en2, is_fix=True, keep=keep_dr, name='emdrop2-1',is_train=train_dr)\n        network_en2 = tl.layers.DenseLayer(network_en2, n_units=nr_hidden,\n                                       act=tf.nn.relu, name='emrelu1-1')\n        embed2_out=network_en2.outputs\n        embed2_re=tf.reshape(embed2_out,[-1,max_length,nr_hidden])\n\n\n    _seq_len = tf.fill(tf.expand_dims(batch_size, 0),\n                       tf.constant(max_length, dtype=tf.int32))\n\n    with tf.variable_scope('ecode1'):\n        fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        h, _ = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed1_re, sequence_length=(_seq_len), dtype=tf.float32)\n        encode1 = tf.concat(2, h)  # none l 2h\n\n    with tf.variable_scope('ecode2'):\n        fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\n        h, _ = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed2_re, sequence_length=(_seq_len), dtype=tf.float32)\n        encode2 = tf.concat(2, h)  # none l 2h\n\n    with tf.variable_scope('atte_layer'):\n\n        encode2_tr = tf.transpose(encode2, perm=[0, 2, 1])  # none 2h l2\n        attention = tf.batch_matmul(encode1, encode2_tr)  # none l1 l2\n        e = tf.exp(attention - tf.reduce_max(attention, 2, keep_dims=True))\n        s = tf.reduce_sum(e, 2, keep_dims=True)  #none l1 1\n        am_att = e / s  #none l1 l2\n        aligh_attention = tf.batch_matmul(am_att, encode2)  # none l1 2h\n        concat = tf.concat(2, [aligh_attention, encode1])  # none l1 4h\n        concat_reshape=tf.reshape(concat,[-1,4*nr_hidden])    #none*l1 4h\n\n\n    with tf.variable_scope('task1_operator'):\n        network_task1=tl.layers.InputLayer(concat_reshape, name='task_layer1-1')\n        network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-1',is_train=train_dr,is_fix=True)\n        network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-1')\n        network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-2',is_train=train_dr,is_fix=True)\n        network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-2')\n        task1 = network_task1.outputs  # none*l 2h\n        task1_re = tf.reshape(task1, [-1, max_length, 2 * nr_hidden])  #none l h\n\n    with tf.variable_scope('task2_operator'):\n        network_task2=tl.layers.InputLayer(concat_reshape, name='task_layer2-1')\n        network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-1',is_train=train_dr,is_fix=True)\n        network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-1')\n        network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-2',is_train=train_dr,is_fix=True)\n        network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-2')\n        task2 = network_task2.outputs  # none*l 2h\n        task2_re = tf.reshape(task2, [-1, max_length, 2 * nr_hidden])  #none l h\n\n    with tf.variable_scope('task3_operator'):\n        network_task3=tl.layers.InputLayer(concat_reshape, name='task_layer3-1')\n        network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-1',is_train=train_dr,is_fix=True)\n        network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-1')\n        network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-2',is_train=train_dr,is_fix=True)\n        network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-2')\n        task3 = network_task3.outputs  # none*l 2h\n        task3_re = tf.reshape(task3, [-1, max_length, 2 * nr_hidden])  #none l h\n\n    with tf.variable_scope('task4_operator'):\n        network_task4=tl.layers.InputLayer(concat_reshape, name='task_layer4-1')\n        network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-1',is_train=train_dr,is_fix=True)\n        network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-1')\n        network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-2',is_train=train_dr,is_fix=True)\n        network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-2')\n        task4 = network_task4.outputs  # none*l 2h\n        task4_re = tf.reshape(task4, [-1, max_length, 2 * nr_hidden])  #none l h\n\n\n    with tf.variable_scope('gate_operator'):\n        network_gate= tl.layers.InputLayer(concat_reshape, name='gate_layer')\n        network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop1',is_train=train_dr)\n        network_gate = tl.layers.DenseLayer(network_gate, n_units=2 * nr_hidden,\n                                            act=tf.nn.relu, name='relu_gate1')\n        network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop2',is_train=train_dr)\n        network_gate = tl.layers.DenseLayer(network_gate, n_units=4,\n                                            act=tf.nn.softmax, name='relu_gate2')\n        gate = network_gate.outputs\n        gate_re = tf.reshape(gate, [-1, max_length, 4])  # none l 2\n\n    def repeat(gate_i):\n\n        gate_a = gate_i  # none l\n        gate_b = tf.tile(gate_a, [1, 2 * nr_hidden])\n        gate_c = tf.reshape(gate_b, [-1, 2 * nr_hidden, max_length])\n        gate_d = tf.transpose(gate_c, perm=[0, 2, 1])\n        return gate_d  # none l 2h\n\n    def Out(t1,t2,t3,t4,gate):\n        gate_shuffle = tf.transpose(gate, perm=[0, 2, 1])  # none 4 l\n\n        g1 = repeat(gate_shuffle[:, 0, :])  # none l\n        g2 = repeat(gate_shuffle[:, 1, :])\n        g3 = repeat(gate_shuffle[:, 2, :])  # none l\n        g4 = repeat(gate_shuffle[:, 3, :])\n\n        O1=g1*t1\n        O2 = g2 * t2\n        O3=g3*t3\n        O4 = g4 * t4\n\n        out = O1 + O2 +O3+O4\n        return out\n\n    task_output = Out(task1_re,task2_re,task3_re,task4_re,gate_re)  # none l1 2h\n\n\n    with tf.variable_scope('aggregate'):\n        fwd_lstm = tf.nn.rnn_cell.BasicLSTMCell(2 * nr_hidden)\n        x_output, x_state = tf.nn.dynamic_rnn(cell=fwd_lstm, inputs=task_output, dtype=tf.float32,sequence_length=(_seq_len))\n\n        composable = x_output[:,-1, :]  # none  2h\n\n    with tf.variable_scope('Entaiment'):\n        network3 = tl.layers.InputLayer(composable, name='layer_entalment1')\n        network3 = tl.layers.DropoutLayer(network3, keep=keep_dr, name='entaidrop3',is_train=train_dr,is_fix=True)\n        network3 = tl.layers.DenseLayer(network3, n_units=2 * nr_hidden,\n                                        act=tf.nn.tanh, name='layer_entaiment2')\n        network3 = tl.layers.DenseLayer(network3, n_units=3,\n                                        act=tf.nn.softmax, name='entai_relu3.2')\n        entaiment = network3.outputs\n\nwith tf.name_scope('Loss'):\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(lable * tf.log(entaiment), reduction_indices=[1]))\n        loss = cross_entropy\n\n    with tf.variable_scope('Accuracy'):\n        predicts = tf.cast(tf.argmax(entaiment, 1), 'int32')   #entaiment none 3\n        lable_one = tf.cast(tf.argmax(lable, 1), 'int32')\n        corrects = tf.equal(predicts, lable_one)\n        num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n        accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n\n    optimizer = tf.train.AdamOptimizer(params.learning_rate)\n    opt_op = optimizer.minimize(loss, global_step=self.global_step)\n\n`", "body": "I want to implement a neural architecture from  A Neural Architecture Mimicking Humans\r\nEnd-to-End for Natural Language Inference . In keras , I can get accuracy  86% . But the same  architecture in tensorflow , just get 76% . Here is the main code \uff1a\r\n`class miki(BaseModel):\r\n\r\n    def build(self):\r\n        params = self.params\r\n        batch_size = params.batch_size\r\n        max_length = params.max_length\r\n        nr_hidden = params.nr_hidden\r\n        keep_dr=params.dr\r\n\r\n\r\n        ids1 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='premise')  # none l1\r\n        ids2 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='hypoyhesis')\r\n        lable = tf.placeholder(tf.float32, shape=[batch_size, 3], name='lable')\r\n        train_dr = tf.placeholder(tf.bool)\r\n\r\n\r\n        oov_W = np.load(open(os.path.join('glove/', 'oov_W.weights'), 'rb'))\r\n        oov_W = oov_W.astype('float32')\r\n        unchanged_W = np.load(open(os.path.join('glove/', 'unchanged_W.weights'), 'rb'))\r\n        unchanged_W = unchanged_W.astype('float32')\r\n        embedding = np.concatenate((oov_W, unchanged_W), axis=0)\r\n        W = tf.get_variable(name=\"W\", shape=embedding.shape, initializer=tf.constant_initializer(embedding),\r\n                            trainable=False)\r\n\r\n        def he_nomal(fan_in):\r\n            s = np.sqrt(2. / fan_in)\r\n            seed = np.random.randint(10e8)\r\n            return tf.random_normal_initializer(0.0, s, dtype=tf.float32, seed=seed)\r\n\r\n        with tf.variable_scope('embedding1'):\r\n            embed1 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\r\n            em_reshape1=tf.reshape(embed1,[-1,nr_hidden])\r\n            network_en1 = tl.layers.InputLayer(inputs=em_reshape1, name='em_layer1-1')\r\n            network_en1 = tl.layers.DropoutLayer(network_en1, is_fix=True, keep=keep_dr, name='emdrop1-1',is_train=train_dr)\r\n            network_en1 = tl.layers.DenseLayer(network_en1, n_units=nr_hidden,\r\n                                           act=tf.nn.relu, name='emrelu2-1')\r\n            embed1_out=network_en1.outputs\r\n            embed1_re=tf.reshape(embed1_out,[-1,max_length,nr_hidden])\r\n\r\n\r\n        with tf.variable_scope('embedding2'):\r\n            embed2 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\r\n            em_reshape2=tf.reshape(embed2,[-1,nr_hidden])\r\n            network_en2 = tl.layers.InputLayer(inputs=em_reshape2, name='em_layer2-1')\r\n            network_en2 = tl.layers.DropoutLayer(network_en2, is_fix=True, keep=keep_dr, name='emdrop2-1',is_train=train_dr)\r\n            network_en2 = tl.layers.DenseLayer(network_en2, n_units=nr_hidden,\r\n                                           act=tf.nn.relu, name='emrelu1-1')\r\n            embed2_out=network_en2.outputs\r\n            embed2_re=tf.reshape(embed2_out,[-1,max_length,nr_hidden])\r\n\r\n\r\n        _seq_len = tf.fill(tf.expand_dims(batch_size, 0),\r\n                           tf.constant(max_length, dtype=tf.int32))\r\n\r\n        with tf.variable_scope('ecode1'):\r\n            fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            h, _ = tf.nn.bidirectional_dynamic_rnn(\r\n                cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed1_re, sequence_length=(_seq_len), dtype=tf.float32)\r\n            encode1 = tf.concat(2, h)  # none l 2h\r\n\r\n        with tf.variable_scope('ecode2'):\r\n            fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            h, _ = tf.nn.bidirectional_dynamic_rnn(\r\n                cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed2_re, sequence_length=(_seq_len), dtype=tf.float32)\r\n            encode2 = tf.concat(2, h)  # none l 2h\r\n\r\n        with tf.variable_scope('atte_layer'):\r\n\r\n            encode2_tr = tf.transpose(encode2, perm=[0, 2, 1])  # none 2h l2\r\n            attention = tf.batch_matmul(encode1, encode2_tr)  # none l1 l2\r\n            e = tf.exp(attention - tf.reduce_max(attention, 2, keep_dims=True))\r\n            s = tf.reduce_sum(e, 2, keep_dims=True)  #none l1 1\r\n            am_att = e / s  #none l1 l2\r\n            aligh_attention = tf.batch_matmul(am_att, encode2)  # none l1 2h\r\n            concat = tf.concat(2, [aligh_attention, encode1])  # none l1 4h\r\n            concat_reshape=tf.reshape(concat,[-1,4*nr_hidden])    #none*l1 4h\r\n\r\n\r\n        with tf.variable_scope('task1_operator'):\r\n            network_task1=tl.layers.InputLayer(concat_reshape, name='task_layer1-1')\r\n            network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-1',is_train=train_dr,is_fix=True)\r\n            network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-1')\r\n            network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-2',is_train=train_dr,is_fix=True)\r\n            network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-2')\r\n            task1 = network_task1.outputs  # none*l 2h\r\n            task1_re = tf.reshape(task1, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n        with tf.variable_scope('task2_operator'):\r\n            network_task2=tl.layers.InputLayer(concat_reshape, name='task_layer2-1')\r\n            network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-1',is_train=train_dr,is_fix=True)\r\n            network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-1')\r\n            network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-2',is_train=train_dr,is_fix=True)\r\n            network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-2')\r\n            task2 = network_task2.outputs  # none*l 2h\r\n            task2_re = tf.reshape(task2, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n        with tf.variable_scope('task3_operator'):\r\n            network_task3=tl.layers.InputLayer(concat_reshape, name='task_layer3-1')\r\n            network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-1',is_train=train_dr,is_fix=True)\r\n            network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-1')\r\n            network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-2',is_train=train_dr,is_fix=True)\r\n            network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-2')\r\n            task3 = network_task3.outputs  # none*l 2h\r\n            task3_re = tf.reshape(task3, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n        with tf.variable_scope('task4_operator'):\r\n            network_task4=tl.layers.InputLayer(concat_reshape, name='task_layer4-1')\r\n            network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-1',is_train=train_dr,is_fix=True)\r\n            network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-1')\r\n            network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-2',is_train=train_dr,is_fix=True)\r\n            network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-2')\r\n            task4 = network_task4.outputs  # none*l 2h\r\n            task4_re = tf.reshape(task4, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n\r\n        with tf.variable_scope('gate_operator'):\r\n            network_gate= tl.layers.InputLayer(concat_reshape, name='gate_layer')\r\n            network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop1',is_train=train_dr)\r\n            network_gate = tl.layers.DenseLayer(network_gate, n_units=2 * nr_hidden,\r\n                                                act=tf.nn.relu, name='relu_gate1')\r\n            network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop2',is_train=train_dr)\r\n            network_gate = tl.layers.DenseLayer(network_gate, n_units=4,\r\n                                                act=tf.nn.softmax, name='relu_gate2')\r\n            gate = network_gate.outputs\r\n            gate_re = tf.reshape(gate, [-1, max_length, 4])  # none l 2\r\n\r\n        def repeat(gate_i):\r\n\r\n            gate_a = gate_i  # none l\r\n            gate_b = tf.tile(gate_a, [1, 2 * nr_hidden])\r\n            gate_c = tf.reshape(gate_b, [-1, 2 * nr_hidden, max_length])\r\n            gate_d = tf.transpose(gate_c, perm=[0, 2, 1])\r\n            return gate_d  # none l 2h\r\n\r\n        def Out(t1,t2,t3,t4,gate):\r\n            gate_shuffle = tf.transpose(gate, perm=[0, 2, 1])  # none 4 l\r\n\r\n            g1 = repeat(gate_shuffle[:, 0, :])  # none l\r\n            g2 = repeat(gate_shuffle[:, 1, :])\r\n            g3 = repeat(gate_shuffle[:, 2, :])  # none l\r\n            g4 = repeat(gate_shuffle[:, 3, :])\r\n\r\n            O1=g1*t1\r\n            O2 = g2 * t2\r\n            O3=g3*t3\r\n            O4 = g4 * t4\r\n\r\n            out = O1 + O2 +O3+O4\r\n            return out\r\n\r\n        task_output = Out(task1_re,task2_re,task3_re,task4_re,gate_re)  # none l1 2h\r\n\r\n\r\n        with tf.variable_scope('aggregate'):\r\n            fwd_lstm = tf.nn.rnn_cell.BasicLSTMCell(2 * nr_hidden)\r\n            x_output, x_state = tf.nn.dynamic_rnn(cell=fwd_lstm, inputs=task_output, dtype=tf.float32,sequence_length=(_seq_len))\r\n\r\n            composable = x_output[:,-1, :]  # none  2h\r\n\r\n        with tf.variable_scope('Entaiment'):\r\n            network3 = tl.layers.InputLayer(composable, name='layer_entalment1')\r\n            network3 = tl.layers.DropoutLayer(network3, keep=keep_dr, name='entaidrop3',is_train=train_dr,is_fix=True)\r\n            network3 = tl.layers.DenseLayer(network3, n_units=2 * nr_hidden,\r\n                                            act=tf.nn.tanh, name='layer_entaiment2')\r\n            network3 = tl.layers.DenseLayer(network3, n_units=3,\r\n                                            act=tf.nn.softmax, name='entai_relu3.2')\r\n            entaiment = network3.outputs\r\nwith tf.name_scope('Loss'):\r\n            cross_entropy = tf.reduce_mean(-tf.reduce_sum(lable * tf.log(entaiment), reduction_indices=[1]))\r\n\r\n            loss = cross_entropy\r\n\r\n        with tf.variable_scope('Accuracy'):\r\n            predicts = tf.cast(tf.argmax(entaiment, 1), 'int32')   #entaiment none 3\r\n            lable_one = tf.cast(tf.argmax(lable, 1), 'int32')\r\n            corrects = tf.equal(predicts, lable_one)\r\n            num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\r\n            accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\r\n\r\n        optimizer = tf.train.AdamOptimizer(params.learning_rate)\r\n        opt_op = optimizer.minimize(loss, global_step=self.global_step)\r\n`"}