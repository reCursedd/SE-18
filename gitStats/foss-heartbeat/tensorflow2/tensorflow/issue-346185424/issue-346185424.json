{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21277", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21277/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21277/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21277/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21277", "id": 346185424, "node_id": "MDU6SXNzdWUzNDYxODU0MjQ=", "number": 21277, "title": "Using TensorFlow's Datasets API causes process to hang in session destructor", "user": {"login": "nfergu", "id": 1291583, "node_id": "MDQ6VXNlcjEyOTE1ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nfergu", "html_url": "https://github.com/nfergu", "followers_url": "https://api.github.com/users/nfergu/followers", "following_url": "https://api.github.com/users/nfergu/following{/other_user}", "gists_url": "https://api.github.com/users/nfergu/gists{/gist_id}", "starred_url": "https://api.github.com/users/nfergu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nfergu/subscriptions", "organizations_url": "https://api.github.com/users/nfergu/orgs", "repos_url": "https://api.github.com/users/nfergu/repos", "events_url": "https://api.github.com/users/nfergu/events{/privacy}", "received_events_url": "https://api.github.com/users/nfergu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2018-07-31T13:23:01Z", "updated_at": "2018-08-09T17:59:36Z", "closed_at": "2018-08-09T17:59:36Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n</ul>\n<p>Yes</p>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n</ul>\n<p>MacOS High Sierra (10.13.1), though we've also seen this happen on Linux as well, we believe.</p>\n<ul>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n</ul>\n<p>Source (but happens with the binary version as well)</p>\n<ul>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n</ul>\n<p>v1.8.0-0-g93bc2e2072 1.8.0</p>\n<ul>\n<li><strong>Python version</strong>:</li>\n</ul>\n<p>Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)</p>\n<ul>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n</ul>\n<p>0.10.1</p>\n<ul>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n</ul>\n<p>Apple LLVM version 9.0.0 (clang-900.0.39.2)</p>\n<ul>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n</ul>\n<p>N/A</p>\n<ul>\n<li><strong>GPU model and memory</strong>:</li>\n</ul>\n<p>N/A</p>\n<ul>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>Unfortunately the issue isn't that easy to reproduce without running our application (I haven't managed to produce a smaller test case).</p>\n<h3>Describe the problem</h3>\n<p><strong>Summary:</strong></p>\n<p>We are using TensorFlow's Datasets API. More specifically, we're using <code>tf.data.Dataset.from_generator</code> to create a dataset based on a generator function.</p>\n<p>When Python comes to garbage collect our <code>tf.Session</code> object its destructor makes a call into TensorFlow to delete the session (<code>tf_session.TF_DeleteSession</code>). This call hangs because it's trying to execute a <code>tf.py_func</code> function, but cannot acquire Python's global interpreter lock. The function its trying to execute appears to be the \"finalize\" function from our dataset.</p>\n<p>This looks to me like a bug in TensorFlow, as (my understanding is) that we shouldn't be able to write code that causes this to happen. Although it's clearly a consequence of our specific use of TensorFlow I can't see that we're doing anything in our application that we shouldn't be.</p>\n<p><strong>More Details:</strong></p>\n<p>When our <code>tf.Session</code> object is garbage collected in Python, its destructor (<code>__del__</code> method) hangs indefinitely. The problem appears to be this call in <code>BaseSession</code>:</p>\n<pre><code>tf_session.TF_DeleteSession(self._session)\n</code></pre>\n<p>Running lldb shows the following stack trace:</p>\n<pre><code>* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\n  * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;) + 18\n    frame #3: 0x000000011279a63b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283\n    frame #4: 0x0000000112796eb7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423\n    frame #5: 0x0000000112797621 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49\n    frame #6: 0x00000001090810e3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67\n    frame #7: 0x0000000109d4d809 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector&lt;tensorflow::Tensor, std::__1::allocator&lt;tensorflow::Tensor&gt; &gt; const&amp;, std::__1::vector&lt;tensorflow::Tensor, std::__1::allocator&lt;tensorflow::Tensor&gt; &gt;*) + 649\n    frame #8: 0x0000000109cffa21 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97\n    frame #9: 0x0000000109cffb8e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14\n    frame #10: 0x0000000109cfd669 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105\n    frame #11: 0x0000000109cfd6de _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14\n    frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43\n    frame #13: 0x0000000109d0a579 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169\n    frame #14: 0x0000000109d0a5fe _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14\n    frame #15: 0x000000011226db4d libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, unsigned long long, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 301\n    frame #16: 0x000000011226dd50 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::type_index, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 192\n    frame #17: 0x0000000109d0c558 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104\n    frame #18: 0x0000000109d0c71e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14\n    frame #19: 0x00000001122670ff libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63\n    frame #20: 0x0000000112267ffd libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 205\n    frame #21: 0x000000010b880b42 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546\n    frame #22: 0x000000010b88108e _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14\n    frame #23: 0x000000010935dfd3 _pywrap_tensorflow_internal.so`TF_DeleteSession + 931\n    frame #24: 0x0000000109006e5a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122\n    frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568\n    frame #26: 0x00000001008443e4 Python`call_function + 612\n    frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\n    frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828\n    frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356\n    frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208\n    frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180\n    frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121\n    frame #33: 0x000000010089b18a Python`collect + 1418\n    frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99\n    frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119\n    frame #36: 0x000000010087b0e0 Python`Py_Exit + 16\n    frame #37: 0x000000010087ef4c Python`handle_system_exit + 252\n    frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437\n    frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125\n    frame #40: 0x00000001008992a4 Python`Py_Main + 1812\n    frame #41: 0x0000000100000dfe Python\n    frame #42: 0x0000000100000c34 Python\n</code></pre>\n<p>It appears that the session's destructor is waiting for an op to complete. The culprit seems to be <code>PyFuncOp</code>, which doesn't get past this line:</p>\n<pre><code>py_threadstate = PyGILState_Ensure();\n</code></pre>\n<p>So it looks like this op is trying to acquire the GIL but can't. My assumption is that this py_func is the \"finalize\" function for the dataset (from <code>_GeneratorDataset</code>).</p>\n<p>My assumption is that when Python calls <code>tf_session.TF_DeleteSession(self._session)</code> that the GIL should be released, and so <code>PyFuncOp</code> should then be able to acquire it again. Indeed, when I write an isolated test to try and reproduce this I don't see this problem, and the GIL is acquired successfully.</p>\n<p>Unfortunately, as I mention, I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario, but I haven't been able to isolate exactly what it is about this scenario that causes the problem.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nYes\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\nMacOS High Sierra (10.13.1), though we've also seen this happen on Linux as well, we believe.\n\nTensorFlow installed from (source or binary):\n\nSource (but happens with the binary version as well)\n\nTensorFlow version (use command below):\n\nv1.8.0-0-g93bc2e2072 1.8.0\n\nPython version:\n\nPython 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)\n\nBazel version (if compiling from source):\n\n0.10.1\n\nGCC/Compiler version (if compiling from source):\n\nApple LLVM version 9.0.0 (clang-900.0.39.2)\n\nCUDA/cuDNN version:\n\nN/A\n\nGPU model and memory:\n\nN/A\n\nExact command to reproduce:\n\nUnfortunately the issue isn't that easy to reproduce without running our application (I haven't managed to produce a smaller test case).\nDescribe the problem\nSummary:\nWe are using TensorFlow's Datasets API. More specifically, we're using tf.data.Dataset.from_generator to create a dataset based on a generator function.\nWhen Python comes to garbage collect our tf.Session object its destructor makes a call into TensorFlow to delete the session (tf_session.TF_DeleteSession). This call hangs because it's trying to execute a tf.py_func function, but cannot acquire Python's global interpreter lock. The function its trying to execute appears to be the \"finalize\" function from our dataset.\nThis looks to me like a bug in TensorFlow, as (my understanding is) that we shouldn't be able to write code that causes this to happen. Although it's clearly a consequence of our specific use of TensorFlow I can't see that we're doing anything in our application that we shouldn't be.\nMore Details:\nWhen our tf.Session object is garbage collected in Python, its destructor (__del__ method) hangs indefinitely. The problem appears to be this call in BaseSession:\ntf_session.TF_DeleteSession(self._session)\n\nRunning lldb shows the following stack trace:\n* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\n  * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\n    frame #3: 0x000000011279a63b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283\n    frame #4: 0x0000000112796eb7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423\n    frame #5: 0x0000000112797621 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49\n    frame #6: 0x00000001090810e3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67\n    frame #7: 0x0000000109d4d809 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 649\n    frame #8: 0x0000000109cffa21 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97\n    frame #9: 0x0000000109cffb8e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14\n    frame #10: 0x0000000109cfd669 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105\n    frame #11: 0x0000000109cfd6de _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14\n    frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43\n    frame #13: 0x0000000109d0a579 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169\n    frame #14: 0x0000000109d0a5fe _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14\n    frame #15: 0x000000011226db4d libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 301\n    frame #16: 0x000000011226dd50 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::type_index, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 192\n    frame #17: 0x0000000109d0c558 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104\n    frame #18: 0x0000000109d0c71e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14\n    frame #19: 0x00000001122670ff libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63\n    frame #20: 0x0000000112267ffd libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 205\n    frame #21: 0x000000010b880b42 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546\n    frame #22: 0x000000010b88108e _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14\n    frame #23: 0x000000010935dfd3 _pywrap_tensorflow_internal.so`TF_DeleteSession + 931\n    frame #24: 0x0000000109006e5a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122\n    frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568\n    frame #26: 0x00000001008443e4 Python`call_function + 612\n    frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\n    frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828\n    frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356\n    frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208\n    frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180\n    frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121\n    frame #33: 0x000000010089b18a Python`collect + 1418\n    frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99\n    frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119\n    frame #36: 0x000000010087b0e0 Python`Py_Exit + 16\n    frame #37: 0x000000010087ef4c Python`handle_system_exit + 252\n    frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437\n    frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125\n    frame #40: 0x00000001008992a4 Python`Py_Main + 1812\n    frame #41: 0x0000000100000dfe Python\n    frame #42: 0x0000000100000c34 Python\n\nIt appears that the session's destructor is waiting for an op to complete. The culprit seems to be PyFuncOp, which doesn't get past this line:\npy_threadstate = PyGILState_Ensure();\n\nSo it looks like this op is trying to acquire the GIL but can't. My assumption is that this py_func is the \"finalize\" function for the dataset (from _GeneratorDataset).\nMy assumption is that when Python calls tf_session.TF_DeleteSession(self._session) that the GIL should be released, and so PyFuncOp should then be able to acquire it again. Indeed, when I write an isolated test to try and reproduce this I don't see this problem, and the GIL is acquired successfully.\nUnfortunately, as I mention, I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario, but I haven't been able to isolate exactly what it is about this scenario that causes the problem.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nMacOS High Sierra (10.13.1), though we've also seen this happen on Linux as well, we believe.\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nSource (but happens with the binary version as well)\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n\r\n- **Python version**:\r\n\r\nPython 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n0.10.1\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nN/A\r\n\r\n- **GPU model and memory**:\r\n\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\n\r\nUnfortunately the issue isn't that easy to reproduce without running our application (I haven't managed to produce a smaller test case).\r\n\r\n### Describe the problem\r\n\r\n**Summary:**\r\n\r\nWe are using TensorFlow's Datasets API. More specifically, we're using `tf.data.Dataset.from_generator` to create a dataset based on a generator function.\r\n\r\nWhen Python comes to garbage collect our `tf.Session` object its destructor makes a call into TensorFlow to delete the session (`tf_session.TF_DeleteSession`). This call hangs because it's trying to execute a `tf.py_func` function, but cannot acquire Python's global interpreter lock. The function its trying to execute appears to be the \"finalize\" function from our dataset.\r\n\r\nThis looks to me like a bug in TensorFlow, as (my understanding is) that we shouldn't be able to write code that causes this to happen. Although it's clearly a consequence of our specific use of TensorFlow I can't see that we're doing anything in our application that we shouldn't be.\r\n\r\n**More Details:**\r\n\r\nWhen our `tf.Session` object is garbage collected in Python, its destructor (`__del__` method) hangs indefinitely. The problem appears to be this call in `BaseSession`:\r\n\r\n    tf_session.TF_DeleteSession(self._session)\r\n\r\nRunning lldb shows the following stack trace:\r\n\r\n    * thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\r\n      * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n        frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n        frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n        frame #3: 0x000000011279a63b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283\r\n        frame #4: 0x0000000112796eb7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423\r\n        frame #5: 0x0000000112797621 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49\r\n        frame #6: 0x00000001090810e3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67\r\n        frame #7: 0x0000000109d4d809 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 649\r\n        frame #8: 0x0000000109cffa21 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97\r\n        frame #9: 0x0000000109cffb8e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14\r\n        frame #10: 0x0000000109cfd669 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105\r\n        frame #11: 0x0000000109cfd6de _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14\r\n        frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43\r\n        frame #13: 0x0000000109d0a579 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169\r\n        frame #14: 0x0000000109d0a5fe _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14\r\n        frame #15: 0x000000011226db4d libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 301\r\n        frame #16: 0x000000011226dd50 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::type_index, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 192\r\n        frame #17: 0x0000000109d0c558 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104\r\n        frame #18: 0x0000000109d0c71e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14\r\n        frame #19: 0x00000001122670ff libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63\r\n        frame #20: 0x0000000112267ffd libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 205\r\n        frame #21: 0x000000010b880b42 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546\r\n        frame #22: 0x000000010b88108e _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14\r\n        frame #23: 0x000000010935dfd3 _pywrap_tensorflow_internal.so`TF_DeleteSession + 931\r\n        frame #24: 0x0000000109006e5a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122\r\n        frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568\r\n        frame #26: 0x00000001008443e4 Python`call_function + 612\r\n        frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n        frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828\r\n        frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356\r\n        frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208\r\n        frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180\r\n        frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121\r\n        frame #33: 0x000000010089b18a Python`collect + 1418\r\n        frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99\r\n        frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119\r\n        frame #36: 0x000000010087b0e0 Python`Py_Exit + 16\r\n        frame #37: 0x000000010087ef4c Python`handle_system_exit + 252\r\n        frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437\r\n        frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125\r\n        frame #40: 0x00000001008992a4 Python`Py_Main + 1812\r\n        frame #41: 0x0000000100000dfe Python\r\n        frame #42: 0x0000000100000c34 Python\r\n\r\nIt appears that the session's destructor is waiting for an op to complete. The culprit seems to be `PyFuncOp`, which doesn't get past this line:\r\n\r\n    py_threadstate = PyGILState_Ensure();\r\n\r\nSo it looks like this op is trying to acquire the GIL but can't. My assumption is that this py_func is the \"finalize\" function for the dataset (from `_GeneratorDataset`).\r\n\r\nMy assumption is that when Python calls `tf_session.TF_DeleteSession(self._session)` that the GIL should be released, and so `PyFuncOp` should then be able to acquire it again. Indeed, when I write an isolated test to try and reproduce this I don't see this problem, and the GIL is acquired successfully.\r\n\r\nUnfortunately, as I mention, I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario, but I haven't been able to isolate exactly what it is about this scenario that causes the problem."}