{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/409272053", "html_url": "https://github.com/tensorflow/tensorflow/issues/21277#issuecomment-409272053", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21277", "id": 409272053, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTI3MjA1Mw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T15:54:35Z", "updated_at": "2018-07-31T15:54:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hmm, I think the Python thread in <code>multiprocessing/connection.py</code> (0x00007000025b4000) maps to native thread 7, which is in <code>_Py_read()</code> and that drops the GIL as well.</p>\n<p>As far as I can tell though, none of the native threads is blocked trying to acquire the GIL:</p>\n<ul>\n<li>Thread 1 is waiting on a <code>tensorflow::Notification</code> for the end of the finialize function.</li>\n<li>Thread 2 isn't TF-related.</li>\n<li>Thread 3 is ???</li>\n<li>Thread 4 is a Python thread that's waiting on a Python lock. I don't think this is TF-related.</li>\n<li>Threads 5, 6, 8, and 9 are idle TF/Eigen threadpool threads that are waiting for work. I'm a little surprised to see 4 of them rather than <code>inter_op_parallelism + intra_op_parallelism = 2</code> threads, but they don't seem to be doing anything concerning.</li>\n<li>Thread 7 is (probably) Python thread 0x00007000025b4000, blocked on a multiprocessing queue.</li>\n</ul>\n<p>It would be interesting to know if the PyFunc op in finalization actually started and/or finished. One way to do this is to set the environment variable <code>TF_CPP_MIN_VLOG_LEVEL=1</code> (which triggers very verbose logging, including each op invocation and completion). Could you try that and capture the part of the log that is produced <em>after</em> the <code>tf.Session</code> destructor begins?</p>", "body_text": "Hmm, I think the Python thread in multiprocessing/connection.py (0x00007000025b4000) maps to native thread 7, which is in _Py_read() and that drops the GIL as well.\nAs far as I can tell though, none of the native threads is blocked trying to acquire the GIL:\n\nThread 1 is waiting on a tensorflow::Notification for the end of the finialize function.\nThread 2 isn't TF-related.\nThread 3 is ???\nThread 4 is a Python thread that's waiting on a Python lock. I don't think this is TF-related.\nThreads 5, 6, 8, and 9 are idle TF/Eigen threadpool threads that are waiting for work. I'm a little surprised to see 4 of them rather than inter_op_parallelism + intra_op_parallelism = 2 threads, but they don't seem to be doing anything concerning.\nThread 7 is (probably) Python thread 0x00007000025b4000, blocked on a multiprocessing queue.\n\nIt would be interesting to know if the PyFunc op in finalization actually started and/or finished. One way to do this is to set the environment variable TF_CPP_MIN_VLOG_LEVEL=1 (which triggers very verbose logging, including each op invocation and completion). Could you try that and capture the part of the log that is produced after the tf.Session destructor begins?", "body": "Hmm, I think the Python thread in `multiprocessing/connection.py` (0x00007000025b4000) maps to native thread 7, which is in `_Py_read()` and that drops the GIL as well. \r\n\r\nAs far as I can tell though, none of the native threads is blocked trying to acquire the GIL:\r\n* Thread 1 is waiting on a `tensorflow::Notification` for the end of the finialize function.\r\n* Thread 2 isn't TF-related.\r\n* Thread 3 is ???\r\n* Thread 4 is a Python thread that's waiting on a Python lock. I don't think this is TF-related.\r\n* Threads 5, 6, 8, and 9 are idle TF/Eigen threadpool threads that are waiting for work. I'm a little surprised to see 4 of them rather than `inter_op_parallelism + intra_op_parallelism = 2` threads, but they don't seem to be doing anything concerning.\r\n* Thread 7 is (probably) Python thread 0x00007000025b4000, blocked on a multiprocessing queue.\r\n\r\nIt would be interesting to know if the PyFunc op in finalization actually started and/or finished. One way to do this is to set the environment variable `TF_CPP_MIN_VLOG_LEVEL=1` (which triggers very verbose logging, including each op invocation and completion). Could you try that and capture the part of the log that is produced *after* the `tf.Session` destructor begins?"}