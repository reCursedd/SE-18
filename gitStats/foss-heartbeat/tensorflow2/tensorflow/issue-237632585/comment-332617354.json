{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/332617354", "html_url": "https://github.com/tensorflow/tensorflow/issues/10961#issuecomment-332617354", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10961", "id": 332617354, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjYxNzM1NA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-27T18:41:20Z", "updated_at": "2017-09-27T18:41:20Z", "author_association": "MEMBER", "body_html": "<p>I'm copying a section from the logs reported by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7975891\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Vargeel\">@Vargeel</a> above, which I think may be important.<br>\nA difficulty is that when things start going wrong within a GPU execution, it can easily result in secondary errors that distract attention from the primary cause.  In this case it may be that the original problem is running out of memory on the GPU.   A similar problem was noticed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"224823993\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9489\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9489/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/9489\">#9489</a>.</p>\n<p>2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available. cudaCheckError() failed : invalid resource handle 2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED 2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1</p>", "body_text": "I'm copying a section from the logs reported by @Vargeel above, which I think may be important.\nA difficulty is that when things start going wrong within a GPU execution, it can easily result in secondary errors that distract attention from the primary cause.  In this case it may be that the original problem is running out of memory on the GPU.   A similar problem was noticed in #9489.\n2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available. cudaCheckError() failed : invalid resource handle 2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED 2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1", "body": "I'm copying a section from the logs reported by @vargeel above, which I think may be important.\r\nA difficulty is that when things start going wrong within a GPU execution, it can easily result in secondary errors that distract attention from the primary cause.  In this case it may be that the original problem is running out of memory on the GPU.   A similar problem was noticed in #9489.   \r\n\r\n2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available. cudaCheckError() failed : invalid resource handle 2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED 2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1"}