{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10961", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10961/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10961/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10961/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10961", "id": 237632585, "node_id": "MDU6SXNzdWUyMzc2MzI1ODU=", "number": 10961, "title": "cudaCheckError() failed on any gpu apart from gpu:0", "user": {"login": "Vargeel", "id": 7975891, "node_id": "MDQ6VXNlcjc5NzU4OTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7975891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Vargeel", "html_url": "https://github.com/Vargeel", "followers_url": "https://api.github.com/users/Vargeel/followers", "following_url": "https://api.github.com/users/Vargeel/following{/other_user}", "gists_url": "https://api.github.com/users/Vargeel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Vargeel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Vargeel/subscriptions", "organizations_url": "https://api.github.com/users/Vargeel/orgs", "repos_url": "https://api.github.com/users/Vargeel/repos", "events_url": "https://api.github.com/users/Vargeel/events{/privacy}", "received_events_url": "https://api.github.com/users/Vargeel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-06-21T18:59:30Z", "updated_at": "2018-01-04T21:59:50Z", "closed_at": "2018-01-04T21:59:50Z", "author_association": "NONE", "body_html": "<p>I am experiencing a strange issue (similar to  <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"224823993\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9489\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9489/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/9489\">#9489</a>) on both a 4 GPUs and a 2 GPUs machine.</p>\n<p>Basically I have a tensorflow model that trains and performs very well on one GPU, I now want to distribute the training phase by using multiple GPUs at once. I followed the CIFAR10 example to parallelize the model but keep getting errors a few iterations into the training process. I get</p>\n<pre><code>\"cudaCheckError() failed : invalid resource handle\n2017-06-21 19:50:28.168726: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:639] failed to record completion event; therefore, failed to create inter-stream dependency\" \n</code></pre>\n<p>most times but I have also seen shape errors on reshape tensors (while the graph is already running) and segmentation errors.</p>\n<p>I have been playing around with configurations and so far I have noticed that the crash only happens if gpu:1 is being called. I tried using only gpu:1 and encountered the same problem. On the other end, the same script using with tf.device('gpu:0') and only this gpu works fine.</p>\n<p>What I tried.<br>\nI reinstalled tensorflow from sources, set up and tested another machine with more GPUs to no avail.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/1092625/test_out.txt\">test_out.txt</a></p>\n<p>other errors that have occurred :<br>\n<code>iter: 1 / 70000, total loss: 2.5591 / 2.5591, rpn_loss_cls: 0.6961, rpn_loss_box: 0.0025, loss_cls: 1.8603, loss_box: 0.0003, lr: 0.001000 speed: 4.586s / iter iter: 2 / 70000, total loss: 1.3976 / 1.3976, rpn_loss_cls: 0.6869, rpn_loss_box: 0.0078, loss_cls: 0.7021, loss_box: 0.0007, lr: 0.001000 speed: 2.638s / iter iter: 3 / 70000, total loss: 1.0764 / 1.0764, rpn_loss_cls: 0.6849, rpn_loss_box: 0.0251, loss_cls: 0.2764, loss_box: 0.0900, lr: 0.001000 speed: 1.999s / iter iter: 4 / 70000, total loss: 0.9941 / 0.9941, rpn_loss_cls: 0.6804, rpn_loss_box: 0.0118, loss_cls: 0.2407, loss_box: 0.0611, lr: 0.001000 speed: 1.673s / iter cudaCheckError() failed : invalid resource handle 2017-06-22 10:07:47.528133: F tensorflow/stream_executor/cuda/cuda_driver.cc:312] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context-&gt;context()) (0 vs. 4) ./experiments/scripts/faster_rcnn_end2end.sh: line 58: 30171 Aborted                 (core dumped) python ./tools/train_net.py --device ${DEV} --number_of_devices ${NUM_DEVICES} --weights data/pretrain_model/VGG_imagenet.npy --imdb ${TRAIN_IMDB} --iters ${ITERS} --cfg experiments/cfgs/faster_rcnn_end2end.yml --network VGGnet_train ${EXTRA_ARGS} </code></p>\n<p><code>2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available. cudaCheckError() failed : invalid resource handle 2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED 2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1 </code><br>\n`2017-06-22 10:10:58.935801: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.<br>\niter: 1 / 70000, total loss: 2.8609 / 2.8609, rpn_loss_cls: 0.6944, rpn_loss_box: 0.0092, loss_cls: 2.1569, loss_box: 0.0004, lr: 0.001000<br>\nspeed: 4.461s / iter<br>\niter: 2 / 70000, total loss: 1.4497 / 1.4497, rpn_loss_cls: 0.6778, rpn_loss_box: 0.0152, loss_cls: 0.7555, loss_box: 0.0012, lr: 0.001000<br>\nspeed: 2.556s / iter<br>\niter: 3 / 70000, total loss: 1.5660 / 1.5660, rpn_loss_cls: 0.7197, rpn_loss_box: 0.0325, loss_cls: 0.6074, loss_box: 0.2063, lr: 0.001000<br>\nspeed: 1.921s / iter<br>\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990<br>\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]<br>\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990<br>\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]<br>\n2017-06-22 10:11:03.655687: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990<br>\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]<br>\n2017-06-22 10:11:03.655692: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990<br>\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]<br>\nTraceback (most recent call last):<br>\nFile \"./tools/train_net.py\", line 100, in <br>\nmax_iters=args.max_iters)<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net<br>\nsw.train_model(sess, max_iters)<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 281, in train_model<br>\nrun_metadata=run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 896, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1108, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1261, in _do_run<br>\noptions, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1280, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990<br>\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]</p>\n<p>Caused by op u'tower_1/Reshape', defined at:<br>\nFile \"./tools/train_net.py\", line 100, in <br>\nmax_iters=args.max_iters)<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net<br>\nsw.train_model(sess, max_iters)<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 208, in train_model<br>\ncross_entropy_t, loss_box_t, rpn_cross_entropy_t, rpn_loss_box_t = self.tower_loss(i)<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 148, in tower_loss<br>\noutputs_roi_data, outputs_cls_score, outputs_bbox_pred = self.net.inference(gpu_id)<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/VGGnet_train.py\", line 89, in inference<br>\n.reshape_layer(2, name='rpn_cls_score_reshape'.format(i))<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 26, in layer_decorated<br>\nlayer_output = op(self, layer_input, <em>args, **kwargs)<br>\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 217, in reshape_layer<br>\nint(d),tf.cast(tf.cast(input_shape[1],tf.float32)</em>(tf.cast(input_shape[3],tf.float32)/tf.cast(d,tf.float32)),tf.int32),input_shape[2]]),[0,2,3,1],name=name))<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2472, in reshape<br>\nname=name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op<br>\nop_def=op_def)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2528, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1203, in <strong>init</strong><br>\nself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access</p>\n<p>InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990<br>\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]<br>\n`</p>", "body_text": "I am experiencing a strange issue (similar to  #9489) on both a 4 GPUs and a 2 GPUs machine.\nBasically I have a tensorflow model that trains and performs very well on one GPU, I now want to distribute the training phase by using multiple GPUs at once. I followed the CIFAR10 example to parallelize the model but keep getting errors a few iterations into the training process. I get\n\"cudaCheckError() failed : invalid resource handle\n2017-06-21 19:50:28.168726: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:639] failed to record completion event; therefore, failed to create inter-stream dependency\" \n\nmost times but I have also seen shape errors on reshape tensors (while the graph is already running) and segmentation errors.\nI have been playing around with configurations and so far I have noticed that the crash only happens if gpu:1 is being called. I tried using only gpu:1 and encountered the same problem. On the other end, the same script using with tf.device('gpu:0') and only this gpu works fine.\nWhat I tried.\nI reinstalled tensorflow from sources, set up and tested another machine with more GPUs to no avail.\ntest_out.txt\nother errors that have occurred :\niter: 1 / 70000, total loss: 2.5591 / 2.5591, rpn_loss_cls: 0.6961, rpn_loss_box: 0.0025, loss_cls: 1.8603, loss_box: 0.0003, lr: 0.001000 speed: 4.586s / iter iter: 2 / 70000, total loss: 1.3976 / 1.3976, rpn_loss_cls: 0.6869, rpn_loss_box: 0.0078, loss_cls: 0.7021, loss_box: 0.0007, lr: 0.001000 speed: 2.638s / iter iter: 3 / 70000, total loss: 1.0764 / 1.0764, rpn_loss_cls: 0.6849, rpn_loss_box: 0.0251, loss_cls: 0.2764, loss_box: 0.0900, lr: 0.001000 speed: 1.999s / iter iter: 4 / 70000, total loss: 0.9941 / 0.9941, rpn_loss_cls: 0.6804, rpn_loss_box: 0.0118, loss_cls: 0.2407, loss_box: 0.0611, lr: 0.001000 speed: 1.673s / iter cudaCheckError() failed : invalid resource handle 2017-06-22 10:07:47.528133: F tensorflow/stream_executor/cuda/cuda_driver.cc:312] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4) ./experiments/scripts/faster_rcnn_end2end.sh: line 58: 30171 Aborted                 (core dumped) python ./tools/train_net.py --device ${DEV} --number_of_devices ${NUM_DEVICES} --weights data/pretrain_model/VGG_imagenet.npy --imdb ${TRAIN_IMDB} --iters ${ITERS} --cfg experiments/cfgs/faster_rcnn_end2end.yml --network VGGnet_train ${EXTRA_ARGS} \n2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available. cudaCheckError() failed : invalid resource handle 2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED 2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1 \n`2017-06-22 10:10:58.935801: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\niter: 1 / 70000, total loss: 2.8609 / 2.8609, rpn_loss_cls: 0.6944, rpn_loss_box: 0.0092, loss_cls: 2.1569, loss_box: 0.0004, lr: 0.001000\nspeed: 4.461s / iter\niter: 2 / 70000, total loss: 1.4497 / 1.4497, rpn_loss_cls: 0.6778, rpn_loss_box: 0.0152, loss_cls: 0.7555, loss_box: 0.0012, lr: 0.001000\nspeed: 2.556s / iter\niter: 3 / 70000, total loss: 1.5660 / 1.5660, rpn_loss_cls: 0.7197, rpn_loss_box: 0.0325, loss_cls: 0.6074, loss_box: 0.2063, lr: 0.001000\nspeed: 1.921s / iter\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\n2017-06-22 10:11:03.655687: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\n2017-06-22 10:11:03.655692: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\nTraceback (most recent call last):\nFile \"./tools/train_net.py\", line 100, in \nmax_iters=args.max_iters)\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net\nsw.train_model(sess, max_iters)\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 281, in train_model\nrun_metadata=run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 896, in run\nrun_metadata_ptr)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1108, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1261, in _do_run\noptions, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1280, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\nCaused by op u'tower_1/Reshape', defined at:\nFile \"./tools/train_net.py\", line 100, in \nmax_iters=args.max_iters)\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net\nsw.train_model(sess, max_iters)\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 208, in train_model\ncross_entropy_t, loss_box_t, rpn_cross_entropy_t, rpn_loss_box_t = self.tower_loss(i)\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 148, in tower_loss\noutputs_roi_data, outputs_cls_score, outputs_bbox_pred = self.net.inference(gpu_id)\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/VGGnet_train.py\", line 89, in inference\n.reshape_layer(2, name='rpn_cls_score_reshape'.format(i))\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 26, in layer_decorated\nlayer_output = op(self, layer_input, args, **kwargs)\nFile \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 217, in reshape_layer\nint(d),tf.cast(tf.cast(input_shape[1],tf.float32)(tf.cast(input_shape[3],tf.float32)/tf.cast(d,tf.float32)),tf.int32),input_shape[2]]),[0,2,3,1],name=name))\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2472, in reshape\nname=name)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\nop_def=op_def)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2528, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1203, in init\nself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\n[[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\n`", "body": "I am experiencing a strange issue (similar to  #9489) on both a 4 GPUs and a 2 GPUs machine. \r\n\r\nBasically I have a tensorflow model that trains and performs very well on one GPU, I now want to distribute the training phase by using multiple GPUs at once. I followed the CIFAR10 example to parallelize the model but keep getting errors a few iterations into the training process. I get \r\n```\r\n\"cudaCheckError() failed : invalid resource handle\r\n2017-06-21 19:50:28.168726: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:639] failed to record completion event; therefore, failed to create inter-stream dependency\" \r\n```\r\nmost times but I have also seen shape errors on reshape tensors (while the graph is already running) and segmentation errors. \r\n\r\nI have been playing around with configurations and so far I have noticed that the crash only happens if gpu:1 is being called. I tried using only gpu:1 and encountered the same problem. On the other end, the same script using with tf.device('gpu:0') and only this gpu works fine.\r\n\r\nWhat I tried. \r\nI reinstalled tensorflow from sources, set up and tested another machine with more GPUs to no avail.\r\n\r\n[test_out.txt](https://github.com/tensorflow/tensorflow/files/1092625/test_out.txt)\r\n\r\nother errors that have occurred : \r\n`iter: 1 / 70000, total loss: 2.5591 / 2.5591, rpn_loss_cls: 0.6961, rpn_loss_box: 0.0025, loss_cls: 1.8603, loss_box: 0.0003, lr: 0.001000\r\nspeed: 4.586s / iter\r\niter: 2 / 70000, total loss: 1.3976 / 1.3976, rpn_loss_cls: 0.6869, rpn_loss_box: 0.0078, loss_cls: 0.7021, loss_box: 0.0007, lr: 0.001000\r\nspeed: 2.638s / iter\r\niter: 3 / 70000, total loss: 1.0764 / 1.0764, rpn_loss_cls: 0.6849, rpn_loss_box: 0.0251, loss_cls: 0.2764, loss_box: 0.0900, lr: 0.001000\r\nspeed: 1.999s / iter\r\niter: 4 / 70000, total loss: 0.9941 / 0.9941, rpn_loss_cls: 0.6804, rpn_loss_box: 0.0118, loss_cls: 0.2407, loss_box: 0.0611, lr: 0.001000\r\nspeed: 1.673s / iter\r\ncudaCheckError() failed : invalid resource handle\r\n2017-06-22 10:07:47.528133: F tensorflow/stream_executor/cuda/cuda_driver.cc:312] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4)\r\n./experiments/scripts/faster_rcnn_end2end.sh: line 58: 30171 Aborted                 (core dumped) python ./tools/train_net.py --device ${DEV} --number_of_devices ${NUM_DEVICES} --weights data/pretrain_model/VGG_imagenet.npy --imdb ${TRAIN_IMDB} --iters ${ITERS} --cfg experiments/cfgs/faster_rcnn_end2end.yml --network VGGnet_train ${EXTRA_ARGS}\r\n`\r\n\r\n`2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\ncudaCheckError() failed : invalid resource handle\r\n2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED\r\n2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n`\r\n`2017-06-22 10:10:58.935801: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\niter: 1 / 70000, total loss: 2.8609 / 2.8609, rpn_loss_cls: 0.6944, rpn_loss_box: 0.0092, loss_cls: 2.1569, loss_box: 0.0004, lr: 0.001000\r\nspeed: 4.461s / iter\r\niter: 2 / 70000, total loss: 1.4497 / 1.4497, rpn_loss_cls: 0.6778, rpn_loss_box: 0.0152, loss_cls: 0.7555, loss_box: 0.0012, lr: 0.001000\r\nspeed: 2.556s / iter\r\niter: 3 / 70000, total loss: 1.5660 / 1.5660, rpn_loss_cls: 0.7197, rpn_loss_box: 0.0325, loss_cls: 0.6074, loss_box: 0.2063, lr: 0.001000\r\nspeed: 1.921s / iter\r\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n2017-06-22 10:11:03.655687: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n2017-06-22 10:11:03.655692: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\nTraceback (most recent call last):\r\n  File \"./tools/train_net.py\", line 100, in <module>\r\n    max_iters=args.max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net\r\n    sw.train_model(sess, max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 281, in train_model\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 896, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1108, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1261, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1280, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n\r\nCaused by op u'tower_1/Reshape', defined at:\r\n  File \"./tools/train_net.py\", line 100, in <module>\r\n    max_iters=args.max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net\r\n    sw.train_model(sess, max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 208, in train_model\r\n    cross_entropy_t, loss_box_t, rpn_cross_entropy_t, rpn_loss_box_t = self.tower_loss(i)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 148, in tower_loss\r\n    outputs_roi_data, outputs_cls_score, outputs_bbox_pred = self.net.inference(gpu_id)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/VGGnet_train.py\", line 89, in inference\r\n    .reshape_layer(2, name='rpn_cls_score_reshape'.format(i))\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 26, in layer_decorated\r\n    layer_output = op(self, layer_input, *args, **kwargs)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 217, in reshape_layer\r\n    int(d),tf.cast(tf.cast(input_shape[1],tf.float32)*(tf.cast(input_shape[3],tf.float32)/tf.cast(d,tf.float32)),tf.int32),input_shape[2]]),[0,2,3,1],name=name))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2472, in reshape\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2528, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1203, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n`\r\n\r\n\r\n"}