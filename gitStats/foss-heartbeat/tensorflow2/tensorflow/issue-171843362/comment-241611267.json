{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241611267", "html_url": "https://github.com/tensorflow/tensorflow/issues/3895#issuecomment-241611267", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3895", "id": 241611267, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTYxMTI2Nw==", "user": {"login": "guozhizou", "id": 7837436, "node_id": "MDQ6VXNlcjc4Mzc0MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7837436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guozhizou", "html_url": "https://github.com/guozhizou", "followers_url": "https://api.github.com/users/guozhizou/followers", "following_url": "https://api.github.com/users/guozhizou/following{/other_user}", "gists_url": "https://api.github.com/users/guozhizou/gists{/gist_id}", "starred_url": "https://api.github.com/users/guozhizou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guozhizou/subscriptions", "organizations_url": "https://api.github.com/users/guozhizou/orgs", "repos_url": "https://api.github.com/users/guozhizou/repos", "events_url": "https://api.github.com/users/guozhizou/events{/privacy}", "received_events_url": "https://api.github.com/users/guozhizou/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-23T02:42:51Z", "updated_at": "2016-08-23T02:42:51Z", "author_association": "NONE", "body_html": "<p>Thanks for your help in advance.</p>\n<p>Tensorflow version is v0.9,  the running code as follows:</p>\n<pre><code>def build_multilayer_lstm_graph_with_dynamic_rnn(state_size=100, num_classes=vocab_size, batch_size=32,\n                                                 num_steps=200, num_layers=3, learning_rate=1e-4):\n    reset_graph()\n\n    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n\n    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n\n    # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n\n    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n    init_state = cell.zero_state(batch_size, tf.float32)\n    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n\n    with tf.variable_scope('softmax'):\n        W = tf.get_variable('W', [state_size, num_classes])\n        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n\n    # reshape rnn_outputs and y so we can get the logits in a single matmul\n    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n    y_reshaped = tf.reshape(y, [-1])\n    logits = tf.matmul(rnn_outputs, W) + b\n    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n    return dict(x=x, y=y, init_state=init_state, final_state=final_state, total_loss=total_loss, train_step=train_step)\n</code></pre>\n<p>The problem happens when I set  state_is_tuple=True.</p>", "body_text": "Thanks for your help in advance.\nTensorflow version is v0.9,  the running code as follows:\ndef build_multilayer_lstm_graph_with_dynamic_rnn(state_size=100, num_classes=vocab_size, batch_size=32,\n                                                 num_steps=200, num_layers=3, learning_rate=1e-4):\n    reset_graph()\n\n    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n\n    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n\n    # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n\n    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n    init_state = cell.zero_state(batch_size, tf.float32)\n    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n\n    with tf.variable_scope('softmax'):\n        W = tf.get_variable('W', [state_size, num_classes])\n        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n\n    # reshape rnn_outputs and y so we can get the logits in a single matmul\n    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n    y_reshaped = tf.reshape(y, [-1])\n    logits = tf.matmul(rnn_outputs, W) + b\n    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n    return dict(x=x, y=y, init_state=init_state, final_state=final_state, total_loss=total_loss, train_step=train_step)\n\nThe problem happens when I set  state_is_tuple=True.", "body": "Thanks for your help in advance.\n\nTensorflow version is v0.9,  the running code as follows:\n\n```\ndef build_multilayer_lstm_graph_with_dynamic_rnn(state_size=100, num_classes=vocab_size, batch_size=32,\n                                                 num_steps=200, num_layers=3, learning_rate=1e-4):\n    reset_graph()\n\n    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n\n    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n\n    # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n\n    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n    init_state = cell.zero_state(batch_size, tf.float32)\n    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n\n    with tf.variable_scope('softmax'):\n        W = tf.get_variable('W', [state_size, num_classes])\n        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n\n    # reshape rnn_outputs and y so we can get the logits in a single matmul\n    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n    y_reshaped = tf.reshape(y, [-1])\n    logits = tf.matmul(rnn_outputs, W) + b\n    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n    return dict(x=x, y=y, init_state=init_state, final_state=final_state, total_loss=total_loss, train_step=train_step)\n```\n\nThe problem happens when I set  state_is_tuple=True.\n"}