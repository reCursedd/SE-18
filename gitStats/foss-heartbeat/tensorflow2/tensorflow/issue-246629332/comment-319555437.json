{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319555437", "html_url": "https://github.com/tensorflow/tensorflow/issues/11896#issuecomment-319555437", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11896", "id": 319555437, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTU1NTQzNw==", "user": {"login": "qinglintian", "id": 14145160, "node_id": "MDQ6VXNlcjE0MTQ1MTYw", "avatar_url": "https://avatars2.githubusercontent.com/u/14145160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qinglintian", "html_url": "https://github.com/qinglintian", "followers_url": "https://api.github.com/users/qinglintian/followers", "following_url": "https://api.github.com/users/qinglintian/following{/other_user}", "gists_url": "https://api.github.com/users/qinglintian/gists{/gist_id}", "starred_url": "https://api.github.com/users/qinglintian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qinglintian/subscriptions", "organizations_url": "https://api.github.com/users/qinglintian/orgs", "repos_url": "https://api.github.com/users/qinglintian/repos", "events_url": "https://api.github.com/users/qinglintian/events{/privacy}", "received_events_url": "https://api.github.com/users/qinglintian/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-02T03:09:12Z", "updated_at": "2017-08-02T03:09:12Z", "author_association": "NONE", "body_html": "<p>Thank you the info provided above. It seems that dynamic scaling of the cluster is not supported yet. Is there any clear plan for supporting that?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> Correct me if my understanding is wrong. What you mentioned for async parameter server is applicable for data parallelism? When more workers come online, the worker simply build same graph and request data to training the model and transfer updates to parameter server. Then, the effectively batchsize of the cluster increases so we can have larger learning rate as described in <a href=\"https://research.fb.com/publications/imagenet1kin1h/\" rel=\"nofollow\">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a>. Do we have a mechanism for dynamically adjust hyperparamters?</p>\n<p>BTW, in <a href=\"https://arxiv.org/pdf/1603.04467.pdf\" rel=\"nofollow\">TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems</a>, it also mentioned parameter replication among servers for server fault tolerant. I suppose that is also not supported in current version of TensorFlow?</p>", "body_text": "Thank you the info provided above. It seems that dynamic scaling of the cluster is not supported yet. Is there any clear plan for supporting that?\n@yaroslavvb Correct me if my understanding is wrong. What you mentioned for async parameter server is applicable for data parallelism? When more workers come online, the worker simply build same graph and request data to training the model and transfer updates to parameter server. Then, the effectively batchsize of the cluster increases so we can have larger learning rate as described in Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. Do we have a mechanism for dynamically adjust hyperparamters?\nBTW, in TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems, it also mentioned parameter replication among servers for server fault tolerant. I suppose that is also not supported in current version of TensorFlow?", "body": "Thank you the info provided above. It seems that dynamic scaling of the cluster is not supported yet. Is there any clear plan for supporting that?\r\n\r\n@yaroslavvb Correct me if my understanding is wrong. What you mentioned for async parameter server is applicable for data parallelism? When more workers come online, the worker simply build same graph and request data to training the model and transfer updates to parameter server. Then, the effectively batchsize of the cluster increases so we can have larger learning rate as described in [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://research.fb.com/publications/imagenet1kin1h/). Do we have a mechanism for dynamically adjust hyperparamters?\r\n\r\nBTW, in [TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems](https://arxiv.org/pdf/1603.04467.pdf), it also mentioned parameter replication among servers for server fault tolerant. I suppose that is also not supported in current version of TensorFlow?"}