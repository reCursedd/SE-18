{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/412342106", "html_url": "https://github.com/tensorflow/tensorflow/issues/11896#issuecomment-412342106", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11896", "id": 412342106, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjM0MjEwNg==", "user": {"login": "kzhang28", "id": 15720392, "node_id": "MDQ6VXNlcjE1NzIwMzky", "avatar_url": "https://avatars3.githubusercontent.com/u/15720392?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kzhang28", "html_url": "https://github.com/kzhang28", "followers_url": "https://api.github.com/users/kzhang28/followers", "following_url": "https://api.github.com/users/kzhang28/following{/other_user}", "gists_url": "https://api.github.com/users/kzhang28/gists{/gist_id}", "starred_url": "https://api.github.com/users/kzhang28/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kzhang28/subscriptions", "organizations_url": "https://api.github.com/users/kzhang28/orgs", "repos_url": "https://api.github.com/users/kzhang28/repos", "events_url": "https://api.github.com/users/kzhang28/events{/privacy}", "received_events_url": "https://api.github.com/users/kzhang28/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-12T13:13:52Z", "updated_at": "2018-08-12T13:14:25Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2613663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/byronyi\">@byronyi</a></p>\n<p>Following your approach: training ---&gt;restart training from checkpoint with a different ClusterSpec</p>\n<blockquote>\n<p>1 Prepare for a scaling by writing a checkpoint to external storage<br>\n2 Terminate current session<br>\n3 Re-do the partitioning of graph with respect to the new cluster_def<br>\n4 Create a new session and recover from the checkpoint</p>\n</blockquote>\n<p>I have experimented with training my model using synchronous between-graph replicas. After terminating the training process(say 2ps + 8 workers), the training process can be restarted from the last checkpoint and proceed with a different number of worker (say 2ps+10workers).  I was wondering if it is possible to adjust the number of ps . If yes, could you illustrate how to scale the number of parameter server nodes ? Thank you!</p>", "body_text": "Hi @byronyi\nFollowing your approach: training --->restart training from checkpoint with a different ClusterSpec\n\n1 Prepare for a scaling by writing a checkpoint to external storage\n2 Terminate current session\n3 Re-do the partitioning of graph with respect to the new cluster_def\n4 Create a new session and recover from the checkpoint\n\nI have experimented with training my model using synchronous between-graph replicas. After terminating the training process(say 2ps + 8 workers), the training process can be restarted from the last checkpoint and proceed with a different number of worker (say 2ps+10workers).  I was wondering if it is possible to adjust the number of ps . If yes, could you illustrate how to scale the number of parameter server nodes ? Thank you!", "body": "Hi @byronyi \r\n\r\nFollowing your approach: training --->restart training from checkpoint with a different ClusterSpec\r\n> 1 Prepare for a scaling by writing a checkpoint to external storage\r\n 2 Terminate current session\r\n3 Re-do the partitioning of graph with respect to the new cluster_def\r\n4 Create a new session and recover from the checkpoint\r\n\r\n I have experimented with training my model using synchronous between-graph replicas. After terminating the training process(say 2ps + 8 workers), the training process can be restarted from the last checkpoint and proceed with a different number of worker (say 2ps+10workers).  I was wondering if it is possible to adjust the number of ps . If yes, could you illustrate how to scale the number of parameter server nodes ? Thank you! "}