{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11896", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11896/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11896/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11896/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11896", "id": 246629332, "node_id": "MDU6SXNzdWUyNDY2MjkzMzI=", "number": 11896, "title": "[Feature request] Dynamically add new machines in distributed TensorFlow", "user": {"login": "qinglintian", "id": 14145160, "node_id": "MDQ6VXNlcjE0MTQ1MTYw", "avatar_url": "https://avatars2.githubusercontent.com/u/14145160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qinglintian", "html_url": "https://github.com/qinglintian", "followers_url": "https://api.github.com/users/qinglintian/followers", "following_url": "https://api.github.com/users/qinglintian/following{/other_user}", "gists_url": "https://api.github.com/users/qinglintian/gists{/gist_id}", "starred_url": "https://api.github.com/users/qinglintian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qinglintian/subscriptions", "organizations_url": "https://api.github.com/users/qinglintian/orgs", "repos_url": "https://api.github.com/users/qinglintian/repos", "events_url": "https://api.github.com/users/qinglintian/events{/privacy}", "received_events_url": "https://api.github.com/users/qinglintian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2017-07-31T02:27:05Z", "updated_at": "2018-08-12T13:14:25Z", "closed_at": "2018-02-03T01:33:05Z", "author_association": "NONE", "body_html": "<p>I'm not sure this has been raised before. I did some search on Google and haven't found relevant stuff. If it do exist, please direct me there. Thank you.</p>\n<p>I'm currently experimenting with distributed TensorFlow. When building a distributed cluster, all machines in the cluster should be fed into tf.train.Server as parameters. That is, the disitributed cluster configuration is defined when building the computation graph. Like the example provided in <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/imagenet_distributed_train.py\">https://github.com/tensorflow/models/blob/master/inception/inception/imagenet_distributed_train.py</a>.</p>\n<p>But I have also read papers about robust distributed cluster that it would be nice if the framework support dynamically adding or removing machines if the cluster get larger or some machine goes down.</p>\n<p>Is this doable in current version of TensorFlow. If so, is there an example to implement this? If not, is there plans for this?</p>", "body_text": "I'm not sure this has been raised before. I did some search on Google and haven't found relevant stuff. If it do exist, please direct me there. Thank you.\nI'm currently experimenting with distributed TensorFlow. When building a distributed cluster, all machines in the cluster should be fed into tf.train.Server as parameters. That is, the disitributed cluster configuration is defined when building the computation graph. Like the example provided in https://github.com/tensorflow/models/blob/master/inception/inception/imagenet_distributed_train.py.\nBut I have also read papers about robust distributed cluster that it would be nice if the framework support dynamically adding or removing machines if the cluster get larger or some machine goes down.\nIs this doable in current version of TensorFlow. If so, is there an example to implement this? If not, is there plans for this?", "body": "I'm not sure this has been raised before. I did some search on Google and haven't found relevant stuff. If it do exist, please direct me there. Thank you.\r\n\r\nI'm currently experimenting with distributed TensorFlow. When building a distributed cluster, all machines in the cluster should be fed into tf.train.Server as parameters. That is, the disitributed cluster configuration is defined when building the computation graph. Like the example provided in https://github.com/tensorflow/models/blob/master/inception/inception/imagenet_distributed_train.py.\r\n\r\nBut I have also read papers about robust distributed cluster that it would be nice if the framework support dynamically adding or removing machines if the cluster get larger or some machine goes down.\r\n\r\nIs this doable in current version of TensorFlow. If so, is there an example to implement this? If not, is there plans for this?"}