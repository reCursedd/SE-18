{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14942", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14942/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14942/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14942/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14942", "id": 277394871, "node_id": "MDU6SXNzdWUyNzczOTQ4NzE=", "number": 14942, "title": "tensorflow 1.4 is 8 times slower than tensorflow 1.3 when read data", "user": {"login": "WenmuZhou", "id": 12406017, "node_id": "MDQ6VXNlcjEyNDA2MDE3", "avatar_url": "https://avatars2.githubusercontent.com/u/12406017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WenmuZhou", "html_url": "https://github.com/WenmuZhou", "followers_url": "https://api.github.com/users/WenmuZhou/followers", "following_url": "https://api.github.com/users/WenmuZhou/following{/other_user}", "gists_url": "https://api.github.com/users/WenmuZhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/WenmuZhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WenmuZhou/subscriptions", "organizations_url": "https://api.github.com/users/WenmuZhou/orgs", "repos_url": "https://api.github.com/users/WenmuZhou/repos", "events_url": "https://api.github.com/users/WenmuZhou/events{/privacy}", "received_events_url": "https://api.github.com/users/WenmuZhou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 36, "created_at": "2017-11-28T14:01:08Z", "updated_at": "2018-02-20T18:40:35Z", "closed_at": "2018-02-20T18:40:09Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: python wheel</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4 and 1.3</li>\n<li><strong>Python version</strong>: 3.6.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: None</li>\n<li><strong>GPU model and memory</strong>: None</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>when I run tensorflow1.4 script using estimator, the script is 8 times slower than tensorflow 1.3</p>\n<h3>Source code / logs</h3>\n<p><code>main</code> script</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n__author__ <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>zj<span class=\"pl-pds\">'</span></span>\n\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">try</span>:\n    <span class=\"pl-k\">import</span> better_exceptions\n<span class=\"pl-k\">except</span> <span class=\"pl-c1\">ImportError</span>:\n    <span class=\"pl-k\">pass</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> src.model_ori <span class=\"pl-k\">import</span> crnn_fn\n<span class=\"pl-k\">from</span> src.data_handler <span class=\"pl-k\">import</span> data_loader\n<span class=\"pl-k\">from</span> src.config <span class=\"pl-k\">import</span> Params, Alphabet\n<span class=\"pl-k\">from</span> src.input_utils <span class=\"pl-k\">import</span> input_fn\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">unused_argv</span>):\n    models_path <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.input_model_dir\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(models_path):\n        <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">FileNotFoundError</span>\n\n    models_list <span class=\"pl-k\">=</span> [os.path.join(models_path, x[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">5</span>]) <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> os.listdir(models_path) <span class=\"pl-k\">if</span> x.endswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.meta<span class=\"pl-pds\">'</span></span>)]\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(<span class=\"pl-c1\">FLAGS</span>.output_model_dir):\n        os.makedirs(<span class=\"pl-c1\">FLAGS</span>.output_model_dir)\n\n    parameters <span class=\"pl-k\">=</span> Params(<span class=\"pl-v\">eval_batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n                        <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">304</span>),\n                        <span class=\"pl-v\">digits_only</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                        <span class=\"pl-v\">alphabet</span><span class=\"pl-k\">=</span>Alphabet.<span class=\"pl-c1\">CHINESECHAR_LETTERS_DIGITS_EXTENDED</span>,\n                        <span class=\"pl-v\">alphabet_decoding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">image_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                        <span class=\"pl-v\">csv_delimiter</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">csv_files_eval</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.csv_files_eval,\n                        <span class=\"pl-v\">output_model_dir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.output_model_dir,\n                        <span class=\"pl-v\">gpu</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.gpu\n                        )\n\n    model_params <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Params<span class=\"pl-pds\">'</span></span>: parameters,\n    }\n\n    os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> parameters.gpu\n    config_sess <span class=\"pl-k\">=</span> tf.ConfigProto()\n    config_sess.gpu_options.per_process_gpu_memory_fraction <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.6</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Config estimator</span>\n    est_config <span class=\"pl-k\">=</span> tf.estimator.RunConfig()\n    est_config <span class=\"pl-k\">=</span> est_config.replace(<span class=\"pl-v\">session_config</span><span class=\"pl-k\">=</span>config_sess,\n                                    <span class=\"pl-v\">save_summary_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n                                    <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span>parameters.output_model_dir)\n\n    estimator <span class=\"pl-k\">=</span> tf.estimator.Estimator(<span class=\"pl-v\">model_fn</span><span class=\"pl-k\">=</span>crnn_fn,\n                                       <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>model_params,\n                                       <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>est_config,\n                                       <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span>parameters.output_model_dir,\n                                       )\n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(<span class=\"pl-c1\">FLAGS</span>.output_file, <span class=\"pl-v\">encoding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>utf-8<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> save_file:\n            <span class=\"pl-k\">for</span> model <span class=\"pl-k\">in</span> models_list:\n                start <span class=\"pl-k\">=</span> time.time()\n                \n                eval_results <span class=\"pl-k\">=</span> estimator.evaluate(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>data_loader(<span class=\"pl-v\">csv_filename</span><span class=\"pl-k\">=</span>parameters.csv_files_eval,\n                                                                       <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>parameters,\n                                                                       <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>parameters.eval_batch_size,\n                                                                       <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n                                                  <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,\n                                                  <span class=\"pl-v\">checkpoint_path</span><span class=\"pl-k\">=</span>model)\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>time:<span class=\"pl-pds\">'</span></span>,time.time() <span class=\"pl-k\">-</span> start)\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model: <span class=\"pl-c1\">%s</span> Evaluation results: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (model, <span class=\"pl-c1\">str</span>(eval_results)))\n                save_file.write(model <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(eval_results) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">except</span> <span class=\"pl-c1\">KeyboardInterrupt</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Interrupted<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-fe<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--csv_files_eval<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">required</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>CSV filename for evaluation<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">nargs</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>E:/val1.csv<span class=\"pl-pds\">'</span></span>])\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-o<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--output_model_dir<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">required</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Directory for output<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>models_vgg_100K_no_eval<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-m<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--input_model_dir<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">required</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Directory for output<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_test<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-g<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--gpu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>GPU 0,1 or '' <span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>0<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-of<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--output_file<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">required</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>123.txt<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>the log output file<span class=\"pl-pds\">\"</span></span>)\n\n    tf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">DEBUG</span>)\n    <span class=\"pl-c1\">FLAGS</span>, unparsed <span class=\"pl-k\">=</span> parser.parse_known_args()\n    tf.app.run(<span class=\"pl-v\">main</span><span class=\"pl-k\">=</span>main, <span class=\"pl-v\">argv</span><span class=\"pl-k\">=</span>[sys.argv[<span class=\"pl-c1\">0</span>]] <span class=\"pl-k\">+</span> unparsed)\n</pre></div>\n<p><code>data_loader</code> script</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> .config <span class=\"pl-k\">import</span> Params, <span class=\"pl-c1\">CONST</span>\n<span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Tuple\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">data_loader</span>(<span class=\"pl-smi\">csv_filename</span>: <span class=\"pl-c1\">str</span>, <span class=\"pl-smi\">params</span>: Params, <span class=\"pl-smi\">batch_size</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, <span class=\"pl-smi\">data_augmentation</span>: <span class=\"pl-c1\">bool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>,\n                <span class=\"pl-smi\">num_epochs</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">image_summaries</span>: <span class=\"pl-c1\">bool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">input_fn</span>():\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Choose case one csv file or list of csv files</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(csv_filename, <span class=\"pl-c1\">list</span>):\n            filename_queue <span class=\"pl-k\">=</span> tf.train.string_input_producer([csv_filename], <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span>num_epochs,\n                                                            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>filename_queue<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">isinstance</span>(csv_filename, <span class=\"pl-c1\">list</span>):\n            filename_queue <span class=\"pl-k\">=</span> tf.train.string_input_producer(csv_filename, <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span>num_epochs, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>filename_queue<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Skip lines that have already been processed</span>\n        reader <span class=\"pl-k\">=</span> tf.TextLineReader(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>CSV_Reader<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">skip_header_lines</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        key, value <span class=\"pl-k\">=</span> reader.read(filename_queue, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>file_reading_op<span class=\"pl-pds\">'</span></span>)\n\n        default_line <span class=\"pl-k\">=</span> [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>None<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>None<span class=\"pl-pds\">'</span></span>]]\n        path, label <span class=\"pl-k\">=</span> tf.decode_csv(value, <span class=\"pl-v\">record_defaults</span><span class=\"pl-k\">=</span>default_line, <span class=\"pl-v\">field_delim</span><span class=\"pl-k\">=</span>params.csv_delimiter,\n                                    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>csv_reading_op<span class=\"pl-pds\">'</span></span>)\n\n        image, img_width <span class=\"pl-k\">=</span> image_reading(path, <span class=\"pl-v\">resized_size</span><span class=\"pl-k\">=</span>params.input_shape, <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>params,\n                                         <span class=\"pl-v\">data_augmentation</span><span class=\"pl-k\">=</span>data_augmentation, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n        to_batch <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>images<span class=\"pl-pds\">'</span></span>: image, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>images_widths<span class=\"pl-pds\">'</span></span>: img_width, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>filenames<span class=\"pl-pds\">'</span></span>: path, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>: label}\n        prepared_batch <span class=\"pl-k\">=</span> tf.train.shuffle_batch(to_batch,\n                                                <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n                                                <span class=\"pl-v\">min_after_dequeue</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">500</span>,\n                                                <span class=\"pl-v\">num_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">15</span>, <span class=\"pl-v\">capacity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4000</span>,\n                                                <span class=\"pl-v\">allow_smaller_final_batch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                                <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>prepared_batch_queue<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-k\">if</span> image_summaries:\n            tf.summary.image(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input/image<span class=\"pl-pds\">'</span></span>, prepared_batch.get(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>images<span class=\"pl-pds\">'</span></span>), <span class=\"pl-v\">max_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        tf.summary.text(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input/labels<span class=\"pl-pds\">'</span></span>, prepared_batch.get(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>)[:<span class=\"pl-c1\">10</span>])\n        tf.summary.text(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input/widths<span class=\"pl-pds\">'</span></span>, tf.as_string(prepared_batch.get(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>images_widths<span class=\"pl-pds\">'</span></span>)))\n\n        <span class=\"pl-k\">return</span> prepared_batch, prepared_batch.get(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">return</span> input_fn\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">image_reading</span>(<span class=\"pl-smi\">path</span>: <span class=\"pl-c1\">str</span>, <span class=\"pl-smi\">params</span>: Params, <span class=\"pl-smi\">resized_size</span>: Tuple[<span class=\"pl-c1\">int</span>, <span class=\"pl-c1\">int</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">data_augmentation</span>: <span class=\"pl-c1\">bool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>,\n                  <span class=\"pl-smi\">padding</span>: <span class=\"pl-c1\">bool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Read image</span>\n    image_content <span class=\"pl-k\">=</span> tf.read_file(path, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>image_reader<span class=\"pl-pds\">'</span></span>)\n    image <span class=\"pl-k\">=</span> tf.cond(tf.equal(tf.string_split([path], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>.<span class=\"pl-pds\">'</span></span>).values[<span class=\"pl-c1\">1</span>], tf.constant(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>jpg<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.string)),\n                    <span class=\"pl-v\">true_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: tf.image.decode_jpeg(image_content, <span class=\"pl-v\">channels</span><span class=\"pl-k\">=</span>params.image_channels,\n                                                         <span class=\"pl-v\">try_recover_truncated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),  <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span> channels = 3 ?</span>\n                    <span class=\"pl-v\">false_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: tf.image.decode_png(image_content, <span class=\"pl-v\">channels</span><span class=\"pl-k\">=</span>params.image_channels),\n                    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>image_decoding<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Data augmentation</span>\n    <span class=\"pl-k\">if</span> data_augmentation:\n        image <span class=\"pl-k\">=</span> augment_data(image)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Padding</span>\n    <span class=\"pl-k\">if</span> padding:\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>padding<span class=\"pl-pds\">'</span></span>):\n            image, img_width <span class=\"pl-k\">=</span> padding_inputs_width(image, resized_size, <span class=\"pl-v\">increment</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">CONST</span>.<span class=\"pl-c1\">DIMENSION_REDUCTION_W_POOLING</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Resize</span>\n    <span class=\"pl-k\">else</span>:\n        image <span class=\"pl-k\">=</span> tf.image.resize_images(image, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>resized_size)\n        img_width <span class=\"pl-k\">=</span> tf.shape(image)[<span class=\"pl-c1\">1</span>]\n\n    <span class=\"pl-k\">with</span> tf.control_dependencies([tf.assert_equal(image.shape[:<span class=\"pl-c1\">2</span>], resized_size)]):\n        <span class=\"pl-k\">return</span> image, img_width\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">random_rotation</span>(<span class=\"pl-smi\">img</span>: tf.Tensor, <span class=\"pl-smi\">max_rotation</span>: <span class=\"pl-c1\">float</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>, <span class=\"pl-smi\">crop</span>: <span class=\"pl-c1\">bool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>) -&gt; tf.Tensor:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> from SeguinBe</span>\n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>RandomRotation<span class=\"pl-pds\">'</span></span>):\n        rotation <span class=\"pl-k\">=</span> tf.random_uniform([], <span class=\"pl-k\">-</span>max_rotation, max_rotation)\n        rotated_image <span class=\"pl-k\">=</span> tf.contrib.image.rotate(img, rotation, <span class=\"pl-v\">interpolation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>BILINEAR<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">if</span> crop:\n            rotation <span class=\"pl-k\">=</span> tf.abs(rotation)\n            original_shape <span class=\"pl-k\">=</span> tf.shape(rotated_image)[:<span class=\"pl-c1\">2</span>]\n            h, w <span class=\"pl-k\">=</span> original_shape[<span class=\"pl-c1\">0</span>], original_shape[<span class=\"pl-c1\">1</span>]\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> see https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders for formulae</span>\n            old_l, old_s <span class=\"pl-k\">=</span> tf.cond(h <span class=\"pl-k\">&gt;</span> w, <span class=\"pl-k\">lambda</span>: [h, w], <span class=\"pl-k\">lambda</span>: [w, h])\n            old_l, old_s <span class=\"pl-k\">=</span> tf.cast(old_l, tf.float32), tf.cast(old_s, tf.float32)\n            new_l <span class=\"pl-k\">=</span> (old_l <span class=\"pl-k\">*</span> tf.cos(rotation) <span class=\"pl-k\">-</span> old_s <span class=\"pl-k\">*</span> tf.sin(rotation)) <span class=\"pl-k\">/</span> tf.cos(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> rotation)\n            new_s <span class=\"pl-k\">=</span> (old_s <span class=\"pl-k\">-</span> tf.sin(rotation) <span class=\"pl-k\">*</span> new_l) <span class=\"pl-k\">/</span> tf.cos(rotation)\n            new_h, new_w <span class=\"pl-k\">=</span> tf.cond(h <span class=\"pl-k\">&gt;</span> w, <span class=\"pl-k\">lambda</span>: [new_l, new_s], <span class=\"pl-k\">lambda</span>: [new_s, new_l])\n            new_h, new_w <span class=\"pl-k\">=</span> tf.cast(new_h, tf.int32), tf.cast(new_w, tf.int32)\n            bb_begin <span class=\"pl-k\">=</span> tf.cast(tf.ceil((h <span class=\"pl-k\">-</span> new_h) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>), tf.int32), tf.cast(tf.ceil((w <span class=\"pl-k\">-</span> new_w) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>), tf.int32)\n            rotated_image_crop <span class=\"pl-k\">=</span> rotated_image[bb_begin[<span class=\"pl-c1\">0</span>]:h <span class=\"pl-k\">-</span> bb_begin[<span class=\"pl-c1\">0</span>], bb_begin[<span class=\"pl-c1\">1</span>]:w <span class=\"pl-k\">-</span> bb_begin[<span class=\"pl-c1\">1</span>], :]\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> If crop removes the entire image, keep the original image</span>\n            rotated_image <span class=\"pl-k\">=</span> tf.cond(tf.equal(tf.size(rotated_image_crop), <span class=\"pl-c1\">0</span>),\n                                    <span class=\"pl-v\">true_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: img,\n                                    <span class=\"pl-v\">false_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: rotated_image_crop)\n\n        <span class=\"pl-k\">return</span> rotated_image\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">random_padding</span>(<span class=\"pl-smi\">image</span>: tf.Tensor, <span class=\"pl-smi\">max_pad_w</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>, <span class=\"pl-smi\">max_pad_h</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>) -&gt; tf.Tensor:\n    w_pad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(np.random.randint(<span class=\"pl-c1\">0</span>, max_pad_w, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>]))\n    h_pad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(np.random.randint(<span class=\"pl-c1\">0</span>, max_pad_h, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>]))\n    paddings <span class=\"pl-k\">=</span> [h_pad, w_pad, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]]\n\n    <span class=\"pl-k\">return</span> tf.pad(image, paddings, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>REFLECT<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>random_padding<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">augment_data</span>(<span class=\"pl-smi\">image</span>: tf.Tensor) -&gt; tf.Tensor:\n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>DataAugmentation<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Random padding</span>\n        image <span class=\"pl-k\">=</span> random_padding(image)\n\n        image <span class=\"pl-k\">=</span> tf.image.random_brightness(image, <span class=\"pl-v\">max_delta</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n        image <span class=\"pl-k\">=</span> tf.image.random_contrast(image, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">1.5</span>)\n        image <span class=\"pl-k\">=</span> random_rotation(image, <span class=\"pl-c1\">0.05</span>, <span class=\"pl-v\">crop</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n        <span class=\"pl-k\">if</span> image.shape[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">3</span>:\n            image <span class=\"pl-k\">=</span> tf.image.random_hue(image, <span class=\"pl-c1\">0.2</span>)\n            image <span class=\"pl-k\">=</span> tf.image.random_saturation(image, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">1.5</span>)\n\n        <span class=\"pl-k\">return</span> image\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">padding_inputs_width</span>(<span class=\"pl-smi\">image</span>: tf.Tensor, <span class=\"pl-smi\">target_shape</span>: Tuple[<span class=\"pl-c1\">int</span>, <span class=\"pl-c1\">int</span>], <span class=\"pl-smi\">increment</span>: <span class=\"pl-c1\">int</span>) -&gt; Tuple[\n    tf.Tensor, tf.Tensor]:\n    target_ratio <span class=\"pl-k\">=</span> target_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">/</span> target_shape[<span class=\"pl-c1\">0</span>]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute ratio to keep the same ratio in new image and get the size of padding</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> necessary to have the final desired shape</span>\n    shape <span class=\"pl-k\">=</span> tf.shape(image)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> \u8ba1\u7b97\u5bbd\u9ad8\u6bd4</span>\n    ratio <span class=\"pl-k\">=</span> tf.divide(shape[<span class=\"pl-c1\">1</span>], shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>ratio<span class=\"pl-pds\">'</span></span>)\n\n    new_h <span class=\"pl-k\">=</span> target_shape[<span class=\"pl-c1\">0</span>]\n    new_w <span class=\"pl-k\">=</span> tf.cast(tf.round((ratio <span class=\"pl-k\">*</span> new_h) <span class=\"pl-k\">/</span> increment) <span class=\"pl-k\">*</span> increment, tf.int32)\n    f1 <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span>: (new_w, ratio)\n    f2 <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span>: (new_h, tf.constant(<span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float64))\n    new_w, ratio <span class=\"pl-k\">=</span> tf.case({tf.greater(new_w, <span class=\"pl-c1\">0</span>): f1,\n                            tf.less_equal(new_w, <span class=\"pl-c1\">0</span>): f2},\n                           <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span>f1, <span class=\"pl-v\">exclusive</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    target_w <span class=\"pl-k\">=</span> target_shape[<span class=\"pl-c1\">1</span>]\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Definitions for cases</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">pad_fn</span>():\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mirror_padding<span class=\"pl-pds\">'</span></span>):\n            pad <span class=\"pl-k\">=</span> tf.subtract(target_w, new_w)\n\n            img_resized <span class=\"pl-k\">=</span> tf.image.resize_images(image, [new_h, new_w])\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Padding to have the desired width</span>\n            paddings <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, pad], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]]\n            pad_image <span class=\"pl-k\">=</span> tf.pad(img_resized, paddings, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SYMMETRIC<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set manually the shape</span>\n            pad_image.set_shape([target_shape[<span class=\"pl-c1\">0</span>], target_shape[<span class=\"pl-c1\">1</span>], img_resized.get_shape()[<span class=\"pl-c1\">2</span>]])\n\n            <span class=\"pl-k\">return</span> pad_image, (new_h, new_w)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">replicate_fn</span>():\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>replication_padding<span class=\"pl-pds\">'</span></span>):\n            img_resized <span class=\"pl-k\">=</span> tf.image.resize_images(image, [new_h, new_w])\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> If one symmetry is not enough to have a full width</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Count number of replications needed</span>\n            n_replication <span class=\"pl-k\">=</span> tf.cast(tf.ceil(target_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">/</span> new_w), tf.int32)\n            img_replicated <span class=\"pl-k\">=</span> tf.tile(img_resized, tf.stack([<span class=\"pl-c1\">1</span>, n_replication, <span class=\"pl-c1\">1</span>]))\n            pad_image <span class=\"pl-k\">=</span> tf.image.crop_to_bounding_box(<span class=\"pl-v\">image</span><span class=\"pl-k\">=</span>img_replicated, <span class=\"pl-v\">offset_height</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">offset_width</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>,\n                                                      <span class=\"pl-v\">target_height</span><span class=\"pl-k\">=</span>target_shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">target_width</span><span class=\"pl-k\">=</span>target_shape[<span class=\"pl-c1\">1</span>])\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set manually the shape</span>\n            pad_image.set_shape([target_shape[<span class=\"pl-c1\">0</span>], target_shape[<span class=\"pl-c1\">1</span>], img_resized.get_shape()[<span class=\"pl-c1\">2</span>]])\n\n            <span class=\"pl-k\">return</span> pad_image, (new_h, new_w)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">simple_resize</span>():\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>simple_resize<span class=\"pl-pds\">'</span></span>):\n            img_resized <span class=\"pl-k\">=</span> tf.image.resize_images(image, target_shape)\n\n            img_resized.set_shape([target_shape[<span class=\"pl-c1\">0</span>], target_shape[<span class=\"pl-c1\">1</span>], img_resized.get_shape()[<span class=\"pl-c1\">2</span>]])\n\n            <span class=\"pl-k\">return</span> img_resized, target_shape\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> 3 cases</span>\n    pad_image, (new_h, new_w) <span class=\"pl-k\">=</span> tf.case(\n        {  <span class=\"pl-c\"><span class=\"pl-c\">#</span> case 1 : new_w &gt;= target_w</span>\n            tf.logical_and(tf.greater_equal(ratio, target_ratio),\n                           tf.greater_equal(new_w, target_w)): simple_resize,\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> case 2 : new_w &gt;= target_w/2 &amp; new_w &lt; target_w &amp; ratio &lt; target_ratio</span>\n            tf.logical_and(tf.less(ratio, target_ratio),\n                           tf.logical_and(tf.greater_equal(new_w, tf.cast(tf.divide(target_w, <span class=\"pl-c1\">2</span>), tf.int32)),\n                                          tf.less(new_w, target_w))): pad_fn,\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> case 3 : new_w &lt; target_w/2 &amp; new_w &lt; target_w &amp; ratio &lt; target_ratio</span>\n            tf.logical_and(tf.less(ratio, target_ratio),\n                           tf.logical_and(tf.less(new_w, target_w),\n                                          tf.less(new_w, tf.cast(tf.divide(target_w, <span class=\"pl-c1\">2</span>), tf.int32)))): replicate_fn\n        },\n        <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span>simple_resize, <span class=\"pl-v\">exclusive</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    <span class=\"pl-k\">return</span> pad_image, new_w  <span class=\"pl-c\"><span class=\"pl-c\">#</span> new_w = image width used for computing sequence lengths</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">preprocess_image_for_prediction</span>(<span class=\"pl-smi\">fixed_height</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>, <span class=\"pl-smi\">min_width</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Input function to use when exporting the model for making predictions (see estimator.export_savedmodel)</span>\n<span class=\"pl-s\">    :param fixed_height: height of the input image after resizing</span>\n<span class=\"pl-s\">    :param min_width: minimum width of image after resizing</span>\n<span class=\"pl-s\">    :return:</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">serving_input_fn</span>():\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> define placeholder for input image</span>\n        image <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">1</span>])\n\n        shape <span class=\"pl-k\">=</span> tf.shape(image)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assert shape is h x w x c with c = 1</span>\n\n        ratio <span class=\"pl-k\">=</span> tf.divide(shape[<span class=\"pl-c1\">1</span>], shape[<span class=\"pl-c1\">0</span>])\n        increment <span class=\"pl-k\">=</span> <span class=\"pl-c1\">CONST</span>.<span class=\"pl-c1\">DIMENSION_REDUCTION_W_POOLING</span>\n        new_width <span class=\"pl-k\">=</span> tf.cast(tf.round((ratio <span class=\"pl-k\">*</span> fixed_height) <span class=\"pl-k\">/</span> increment) <span class=\"pl-k\">*</span> increment, tf.int32)\n\n        resized_image <span class=\"pl-k\">=</span> tf.cond(new_width <span class=\"pl-k\">&lt;</span> tf.constant(min_width, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32),\n                                <span class=\"pl-v\">true_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: tf.image.resize_images(image, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(fixed_height, min_width)),\n                                <span class=\"pl-v\">false_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: tf.image.resize_images(image, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(fixed_height, new_width))\n                                )\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Features to serve</span>\n        features <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>images<span class=\"pl-pds\">'</span></span>: resized_image[<span class=\"pl-c1\">None</span>],  <span class=\"pl-c\"><span class=\"pl-c\">#</span> cast to 1 x h x w x c</span>\n                    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>images_widths<span class=\"pl-pds\">'</span></span>: new_width[<span class=\"pl-c1\">None</span>]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> cast to tensor</span>\n                    }\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Inputs received</span>\n        receiver_inputs <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>images<span class=\"pl-pds\">'</span></span>: image}\n\n        <span class=\"pl-k\">return</span> tf.estimator.export.ServingInputReceiver(features, receiver_inputs)\n\n    <span class=\"pl-k\">return</span> serving_input_fn</pre></div>\n<p>log<br>\ntensorflow1.4</p>\n<div class=\"highlight highlight-source-shell\"><pre>INFO:tensorflow:Using config: {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>_model_dir<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>models_vgg_100K_no_eval<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_tf_random_seed<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_summary_steps<span class=\"pl-pds\">'</span></span>: 100, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_steps<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_secs<span class=\"pl-pds\">'</span></span>: 600, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_session_config<span class=\"pl-pds\">'</span></span>: gpu_options {\n  per_process_gpu_memory_fraction: 0.6\n}\n, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_max<span class=\"pl-pds\">'</span></span>: 5, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_every_n_hours<span class=\"pl-pds\">'</span></span>: 10000, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_log_step_count_steps<span class=\"pl-pds\">'</span></span>: 100, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_service<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_cluster_spec<span class=\"pl-pds\">'</span></span>: <span class=\"pl-k\">&lt;</span>tensorflow.python.training.server_lib.ClusterSpec object at 0x000002BAAA7A<span class=\"pl-k\">6780&gt;</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_task_type<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_task_id<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_master<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_is_chief<span class=\"pl-pds\">'</span></span>: True, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_num_ps_replicas<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_num_worker_replicas<span class=\"pl-pds\">'</span></span>: 1}\nINFO:tensorflow:Starting evaluation at 2017-11-28-12:21:42\nINFO:tensorflow:Restoring parameters from model_test<span class=\"pl-cce\">\\m</span>odel.ckpt-54692\n2017-11-28 20:22:04.720980: I C:<span class=\"pl-cce\">\\t</span>f_jenkins<span class=\"pl-cce\">\\h</span>ome<span class=\"pl-cce\">\\w</span>orkspace<span class=\"pl-cce\">\\r</span>el-win<span class=\"pl-cce\">\\M\\w</span>indows<span class=\"pl-cce\">\\P</span>Y<span class=\"pl-cce\">\\3</span>6<span class=\"pl-cce\">\\t</span>ensorflow<span class=\"pl-cce\">\\c</span>ore<span class=\"pl-cce\">\\k</span>ernels<span class=\"pl-cce\">\\l</span>ogging_ops.cc:79] <span class=\"pl-k\">*</span> Loss <span class=\"pl-c1\">:</span> [0.236689657]\nINFO:tensorflow:Evaluation [1/3]\n2017-11-28 20:28:32.360331: I C:<span class=\"pl-cce\">\\t</span>f_jenkins<span class=\"pl-cce\">\\h</span>ome<span class=\"pl-cce\">\\w</span>orkspace<span class=\"pl-cce\">\\r</span>el-win<span class=\"pl-cce\">\\M\\w</span>indows<span class=\"pl-cce\">\\P</span>Y<span class=\"pl-cce\">\\3</span>6<span class=\"pl-cce\">\\t</span>ensorflow<span class=\"pl-cce\">\\c</span>ore<span class=\"pl-cce\">\\k</span>ernels<span class=\"pl-cce\">\\l</span>ogging_ops.cc:79] <span class=\"pl-k\">*</span> Loss <span class=\"pl-c1\">:</span> [0.238805175]\nINFO:tensorflow:Evaluation [2/3]\n2017-11-28 20:35:41.020994: I C:<span class=\"pl-cce\">\\t</span>f_jenkins<span class=\"pl-cce\">\\h</span>ome<span class=\"pl-cce\">\\w</span>orkspace<span class=\"pl-cce\">\\r</span>el-win<span class=\"pl-cce\">\\M\\w</span>indows<span class=\"pl-cce\">\\P</span>Y<span class=\"pl-cce\">\\3</span>6<span class=\"pl-cce\">\\t</span>ensorflow<span class=\"pl-cce\">\\c</span>ore<span class=\"pl-cce\">\\k</span>ernels<span class=\"pl-cce\">\\l</span>ogging_ops.cc:79] <span class=\"pl-k\">*</span> Loss <span class=\"pl-c1\">:</span> [0.237995088]\nINFO:tensorflow:Evaluation [3/3]\nINFO:tensorflow:Finished evaluation at 2017-11-28-12:43:21\nINFO:tensorflow:Saving dict <span class=\"pl-k\">for</span> global step 54692: eval/CER = 0.0108218, eval/accuracy = 0.929688, global_step = 54692, loss = 0.23783\ntime:1306.1133954524994\nmodel: model_test<span class=\"pl-cce\">\\m</span>odel.ckpt-54692 Evaluation results: {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval/CER<span class=\"pl-pds\">'</span></span>: 0.01082176, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval/accuracy<span class=\"pl-pds\">'</span></span>: 0.9296875, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>: 0.23782997, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_step<span class=\"pl-pds\">'</span></span>: 54692}</pre></div>\n<p>tensorflow 1.3</p>\n<div class=\"highlight highlight-source-shell\"><pre>INFO:tensorflow:Using config: {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>_model_dir<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>models_vgg_100K_no_eval<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_tf_random_seed<span class=\"pl-pds\">'</span></span>: 1, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_summary_steps<span class=\"pl-pds\">'</span></span>: 100, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_secs<span class=\"pl-pds\">'</span></span>: 600, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_steps<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_session_config<span class=\"pl-pds\">'</span></span>: gpu_options {\n  per_process_gpu_memory_fraction: 0.6\n}\n, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_max<span class=\"pl-pds\">'</span></span>: 5, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_every_n_hours<span class=\"pl-pds\">'</span></span>: 10000, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_log_step_count_steps<span class=\"pl-pds\">'</span></span>: 100}\nINFO:tensorflow:Starting evaluation at 2017-11-28-12:49:50\nINFO:tensorflow:Restoring parameters from model_test<span class=\"pl-cce\">\\m</span>odel.ckpt-54692\n2017-11-28 20:50:12.841210: I C:<span class=\"pl-cce\">\\t</span>f_jenkins<span class=\"pl-cce\">\\h</span>ome<span class=\"pl-cce\">\\w</span>orkspace<span class=\"pl-cce\">\\r</span>el-win<span class=\"pl-cce\">\\M\\w</span>indows<span class=\"pl-cce\">\\P</span>Y<span class=\"pl-cce\">\\3</span>6<span class=\"pl-cce\">\\t</span>ensorflow<span class=\"pl-cce\">\\c</span>ore<span class=\"pl-cce\">\\k</span>ernels<span class=\"pl-cce\">\\l</span>ogging_ops.cc:79] <span class=\"pl-k\">*</span> Loss <span class=\"pl-c1\">:</span> [0.17519826]\nINFO:tensorflow:Evaluation [1/3]\n2017-11-28 20:51:03.366275: I C:<span class=\"pl-cce\">\\t</span>f_jenkins<span class=\"pl-cce\">\\h</span>ome<span class=\"pl-cce\">\\w</span>orkspace<span class=\"pl-cce\">\\r</span>el-win<span class=\"pl-cce\">\\M\\w</span>indows<span class=\"pl-cce\">\\P</span>Y<span class=\"pl-cce\">\\3</span>6<span class=\"pl-cce\">\\t</span>ensorflow<span class=\"pl-cce\">\\c</span>ore<span class=\"pl-cce\">\\k</span>ernels<span class=\"pl-cce\">\\l</span>ogging_ops.cc:79] <span class=\"pl-k\">*</span> Loss <span class=\"pl-c1\">:</span> [0.2987892]\nINFO:tensorflow:Evaluation [2/3]\n2017-11-28 20:51:49.843030: I C:<span class=\"pl-cce\">\\t</span>f_jenkins<span class=\"pl-cce\">\\h</span>ome<span class=\"pl-cce\">\\w</span>orkspace<span class=\"pl-cce\">\\r</span>el-win<span class=\"pl-cce\">\\M\\w</span>indows<span class=\"pl-cce\">\\P</span>Y<span class=\"pl-cce\">\\3</span>6<span class=\"pl-cce\">\\t</span>ensorflow<span class=\"pl-cce\">\\c</span>ore<span class=\"pl-cce\">\\k</span>ernels<span class=\"pl-cce\">\\l</span>ogging_ops.cc:79] <span class=\"pl-k\">*</span> Loss <span class=\"pl-c1\">:</span> [0.20660429]\nINFO:tensorflow:Evaluation [3/3]\nINFO:tensorflow:Finished evaluation at 2017-11-28-12:52:19\nINFO:tensorflow:Saving dict <span class=\"pl-k\">for</span> global step 54692: eval/CER = 0.01188, eval/accuracy = 0.924479, global_step = 54692, loss = 0.226864\ntime:157.26274514198303\nmodel: model_test<span class=\"pl-cce\">\\m</span>odel.ckpt-54692 Evaluation results: {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval/CER<span class=\"pl-pds\">'</span></span>: 0.011879961, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval/accuracy<span class=\"pl-pds\">'</span></span>: 0.92447919, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>: 0.22686392, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_step<span class=\"pl-pds\">'</span></span>: 54692}</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary): python wheel\nTensorFlow version (use command below): 1.4 and 1.3\nPython version: 3.6.1\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: None\nGPU model and memory: None\nExact command to reproduce:\n\nwhen I run tensorflow1.4 script using estimator, the script is 8 times slower than tensorflow 1.3\nSource code / logs\nmain script\n#!/usr/bin/env python\n__author__ = 'zj'\n\nimport argparse\nimport os\nimport sys\nimport numpy as np\nimport time\ntry:\n    import better_exceptions\nexcept ImportError:\n    pass\nimport tensorflow as tf\nfrom src.model_ori import crnn_fn\nfrom src.data_handler import data_loader\nfrom src.config import Params, Alphabet\nfrom src.input_utils import input_fn\n\n\ndef main(unused_argv):\n    models_path = FLAGS.input_model_dir\n    if not os.path.exists(models_path):\n        assert FileNotFoundError\n\n    models_list = [os.path.join(models_path, x[:-5]) for x in os.listdir(models_path) if x.endswith('.meta')]\n\n    if not os.path.exists(FLAGS.output_model_dir):\n        os.makedirs(FLAGS.output_model_dir)\n\n    parameters = Params(eval_batch_size=128,\n                        input_shape=(32, 304),\n                        digits_only=False,\n                        alphabet=Alphabet.CHINESECHAR_LETTERS_DIGITS_EXTENDED,\n                        alphabet_decoding='same',\n                        image_channels=1,\n                        csv_delimiter=' ',\n                        csv_files_eval=FLAGS.csv_files_eval,\n                        output_model_dir=FLAGS.output_model_dir,\n                        gpu=FLAGS.gpu\n                        )\n\n    model_params = {\n        'Params': parameters,\n    }\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = parameters.gpu\n    config_sess = tf.ConfigProto()\n    config_sess.gpu_options.per_process_gpu_memory_fraction = 0.6\n\n    # Config estimator\n    est_config = tf.estimator.RunConfig()\n    est_config = est_config.replace(session_config=config_sess,\n                                    save_summary_steps=100,\n                                    model_dir=parameters.output_model_dir)\n\n    estimator = tf.estimator.Estimator(model_fn=crnn_fn,\n                                       params=model_params,\n                                       config=est_config,\n                                       model_dir=parameters.output_model_dir,\n                                       )\n    try:\n        with open(FLAGS.output_file, encoding='utf-8', mode='w') as save_file:\n            for model in models_list:\n                start = time.time()\n                \n                eval_results = estimator.evaluate(input_fn=data_loader(csv_filename=parameters.csv_files_eval,\n                                                                       params=parameters,\n                                                                       batch_size=parameters.eval_batch_size,\n                                                                       num_epochs=1),\n                                                  steps=3,\n                                                  checkpoint_path=model)\n                print('time:',time.time() - start)\n                print('model: %s Evaluation results: %s' % (model, str(eval_results)))\n                save_file.write(model + ' ' + str(eval_results) + '\\n')\n\n    except KeyboardInterrupt:\n        print('Interrupted')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-fe', '--csv_files_eval', required=False, type=str, help='CSV filename for evaluation',\n                        nargs='*', default=['E:/val1.csv'])\n    parser.add_argument('-o', '--output_model_dir', required=False, type=str,\n                        help='Directory for output', default='models_vgg_100K_no_eval')\n    parser.add_argument('-m', '--input_model_dir', required=False, type=str,\n                        help='Directory for output', default='model_test')\n    parser.add_argument('-g', '--gpu', type=str, help=\"GPU 0,1 or '' \", default='0')\n    parser.add_argument('-of', '--output_file', required=False, type=str, default='123.txt', help=\"the log output file\")\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    FLAGS, unparsed = parser.parse_known_args()\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\ndata_loader script\n#!/usr/bin/env python\nimport tensorflow as tf\nimport numpy as np\nfrom .config import Params, CONST\nfrom typing import Tuple\n\n\ndef data_loader(csv_filename: str, params: Params, batch_size: int = 128, data_augmentation: bool = False,\n                num_epochs: int = None, image_summaries: bool = False):\n    def input_fn():\n        # Choose case one csv file or list of csv files\n        if not isinstance(csv_filename, list):\n            filename_queue = tf.train.string_input_producer([csv_filename], num_epochs=num_epochs,\n                                                            name='filename_queue')\n        elif isinstance(csv_filename, list):\n            filename_queue = tf.train.string_input_producer(csv_filename, num_epochs=num_epochs, name='filename_queue')\n\n        # Skip lines that have already been processed\n        reader = tf.TextLineReader(name='CSV_Reader', skip_header_lines=0)\n        key, value = reader.read(filename_queue, name='file_reading_op')\n\n        default_line = [['None'], ['None']]\n        path, label = tf.decode_csv(value, record_defaults=default_line, field_delim=params.csv_delimiter,\n                                    name='csv_reading_op')\n\n        image, img_width = image_reading(path, resized_size=params.input_shape, params=params,\n                                         data_augmentation=data_augmentation, padding=True)\n\n        to_batch = {'images': image, 'images_widths': img_width, 'filenames': path, 'labels': label}\n        prepared_batch = tf.train.shuffle_batch(to_batch,\n                                                batch_size=batch_size,\n                                                min_after_dequeue=500,\n                                                num_threads=15, capacity=4000,\n                                                allow_smaller_final_batch=False,\n                                                name='prepared_batch_queue')\n\n        if image_summaries:\n            tf.summary.image('input/image', prepared_batch.get('images'), max_outputs=1)\n        tf.summary.text('input/labels', prepared_batch.get('labels')[:10])\n        tf.summary.text('input/widths', tf.as_string(prepared_batch.get('images_widths')))\n\n        return prepared_batch, prepared_batch.get('labels')\n\n    return input_fn\n\n\ndef image_reading(path: str, params: Params, resized_size: Tuple[int, int] = None, data_augmentation: bool = False,\n                  padding: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:\n    # Read image\n    image_content = tf.read_file(path, name='image_reader')\n    image = tf.cond(tf.equal(tf.string_split([path], '.').values[1], tf.constant('jpg', dtype=tf.string)),\n                    true_fn=lambda: tf.image.decode_jpeg(image_content, channels=params.image_channels,\n                                                         try_recover_truncated=True),  # TODO channels = 3 ?\n                    false_fn=lambda: tf.image.decode_png(image_content, channels=params.image_channels),\n                    name='image_decoding')\n\n    # Data augmentation\n    if data_augmentation:\n        image = augment_data(image)\n\n    # Padding\n    if padding:\n        with tf.name_scope('padding'):\n            image, img_width = padding_inputs_width(image, resized_size, increment=CONST.DIMENSION_REDUCTION_W_POOLING)\n    # Resize\n    else:\n        image = tf.image.resize_images(image, size=resized_size)\n        img_width = tf.shape(image)[1]\n\n    with tf.control_dependencies([tf.assert_equal(image.shape[:2], resized_size)]):\n        return image, img_width\n\n\ndef random_rotation(img: tf.Tensor, max_rotation: float = 0.1, crop: bool = True) -> tf.Tensor:  # from SeguinBe\n    with tf.name_scope('RandomRotation'):\n        rotation = tf.random_uniform([], -max_rotation, max_rotation)\n        rotated_image = tf.contrib.image.rotate(img, rotation, interpolation='BILINEAR')\n        if crop:\n            rotation = tf.abs(rotation)\n            original_shape = tf.shape(rotated_image)[:2]\n            h, w = original_shape[0], original_shape[1]\n            # see https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders for formulae\n            old_l, old_s = tf.cond(h > w, lambda: [h, w], lambda: [w, h])\n            old_l, old_s = tf.cast(old_l, tf.float32), tf.cast(old_s, tf.float32)\n            new_l = (old_l * tf.cos(rotation) - old_s * tf.sin(rotation)) / tf.cos(2 * rotation)\n            new_s = (old_s - tf.sin(rotation) * new_l) / tf.cos(rotation)\n            new_h, new_w = tf.cond(h > w, lambda: [new_l, new_s], lambda: [new_s, new_l])\n            new_h, new_w = tf.cast(new_h, tf.int32), tf.cast(new_w, tf.int32)\n            bb_begin = tf.cast(tf.ceil((h - new_h) / 2), tf.int32), tf.cast(tf.ceil((w - new_w) / 2), tf.int32)\n            rotated_image_crop = rotated_image[bb_begin[0]:h - bb_begin[0], bb_begin[1]:w - bb_begin[1], :]\n\n            # If crop removes the entire image, keep the original image\n            rotated_image = tf.cond(tf.equal(tf.size(rotated_image_crop), 0),\n                                    true_fn=lambda: img,\n                                    false_fn=lambda: rotated_image_crop)\n\n        return rotated_image\n\n\ndef random_padding(image: tf.Tensor, max_pad_w: int = 5, max_pad_h: int = 10) -> tf.Tensor:\n    w_pad = list(np.random.randint(0, max_pad_w, size=[2]))\n    h_pad = list(np.random.randint(0, max_pad_h, size=[2]))\n    paddings = [h_pad, w_pad, [0, 0]]\n\n    return tf.pad(image, paddings, mode='REFLECT', name='random_padding')\n\n\ndef augment_data(image: tf.Tensor) -> tf.Tensor:\n    with tf.name_scope('DataAugmentation'):\n        # Random padding\n        image = random_padding(image)\n\n        image = tf.image.random_brightness(image, max_delta=0.1)\n        image = tf.image.random_contrast(image, 0.5, 1.5)\n        image = random_rotation(image, 0.05, crop=True)\n\n        if image.shape[-1] >= 3:\n            image = tf.image.random_hue(image, 0.2)\n            image = tf.image.random_saturation(image, 0.5, 1.5)\n\n        return image\n\n\ndef padding_inputs_width(image: tf.Tensor, target_shape: Tuple[int, int], increment: int) -> Tuple[\n    tf.Tensor, tf.Tensor]:\n    target_ratio = target_shape[1] / target_shape[0]\n    # Compute ratio to keep the same ratio in new image and get the size of padding\n    # necessary to have the final desired shape\n    shape = tf.shape(image)\n    # \u8ba1\u7b97\u5bbd\u9ad8\u6bd4\n    ratio = tf.divide(shape[1], shape[0], name='ratio')\n\n    new_h = target_shape[0]\n    new_w = tf.cast(tf.round((ratio * new_h) / increment) * increment, tf.int32)\n    f1 = lambda: (new_w, ratio)\n    f2 = lambda: (new_h, tf.constant(1.0, dtype=tf.float64))\n    new_w, ratio = tf.case({tf.greater(new_w, 0): f1,\n                            tf.less_equal(new_w, 0): f2},\n                           default=f1, exclusive=True)\n    target_w = target_shape[1]\n\n    # Definitions for cases\n    def pad_fn():\n        with tf.name_scope('mirror_padding'):\n            pad = tf.subtract(target_w, new_w)\n\n            img_resized = tf.image.resize_images(image, [new_h, new_w])\n\n            # Padding to have the desired width\n            paddings = [[0, 0], [0, pad], [0, 0]]\n            pad_image = tf.pad(img_resized, paddings, mode='SYMMETRIC', name=None)\n\n            # Set manually the shape\n            pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\n\n            return pad_image, (new_h, new_w)\n\n    def replicate_fn():\n        with tf.name_scope('replication_padding'):\n            img_resized = tf.image.resize_images(image, [new_h, new_w])\n\n            # If one symmetry is not enough to have a full width\n            # Count number of replications needed\n            n_replication = tf.cast(tf.ceil(target_shape[1] / new_w), tf.int32)\n            img_replicated = tf.tile(img_resized, tf.stack([1, n_replication, 1]))\n            pad_image = tf.image.crop_to_bounding_box(image=img_replicated, offset_height=0, offset_width=0,\n                                                      target_height=target_shape[0], target_width=target_shape[1])\n\n            # Set manually the shape\n            pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\n\n            return pad_image, (new_h, new_w)\n\n    def simple_resize():\n        with tf.name_scope('simple_resize'):\n            img_resized = tf.image.resize_images(image, target_shape)\n\n            img_resized.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\n\n            return img_resized, target_shape\n\n    # 3 cases\n    pad_image, (new_h, new_w) = tf.case(\n        {  # case 1 : new_w >= target_w\n            tf.logical_and(tf.greater_equal(ratio, target_ratio),\n                           tf.greater_equal(new_w, target_w)): simple_resize,\n            # case 2 : new_w >= target_w/2 & new_w < target_w & ratio < target_ratio\n            tf.logical_and(tf.less(ratio, target_ratio),\n                           tf.logical_and(tf.greater_equal(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)),\n                                          tf.less(new_w, target_w))): pad_fn,\n            # case 3 : new_w < target_w/2 & new_w < target_w & ratio < target_ratio\n            tf.logical_and(tf.less(ratio, target_ratio),\n                           tf.logical_and(tf.less(new_w, target_w),\n                                          tf.less(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)))): replicate_fn\n        },\n        default=simple_resize, exclusive=True)\n\n    return pad_image, new_w  # new_w = image width used for computing sequence lengths\n\n\ndef preprocess_image_for_prediction(fixed_height: int = 32, min_width: int = 8):\n    \"\"\"\n    Input function to use when exporting the model for making predictions (see estimator.export_savedmodel)\n    :param fixed_height: height of the input image after resizing\n    :param min_width: minimum width of image after resizing\n    :return:\n    \"\"\"\n\n    def serving_input_fn():\n        # define placeholder for input image\n        image = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])\n\n        shape = tf.shape(image)\n        # Assert shape is h x w x c with c = 1\n\n        ratio = tf.divide(shape[1], shape[0])\n        increment = CONST.DIMENSION_REDUCTION_W_POOLING\n        new_width = tf.cast(tf.round((ratio * fixed_height) / increment) * increment, tf.int32)\n\n        resized_image = tf.cond(new_width < tf.constant(min_width, dtype=tf.int32),\n                                true_fn=lambda: tf.image.resize_images(image, size=(fixed_height, min_width)),\n                                false_fn=lambda: tf.image.resize_images(image, size=(fixed_height, new_width))\n                                )\n\n        # Features to serve\n        features = {'images': resized_image[None],  # cast to 1 x h x w x c\n                    'images_widths': new_width[None]  # cast to tensor\n                    }\n\n        # Inputs received\n        receiver_inputs = {'images': image}\n\n        return tf.estimator.export.ServingInputReceiver(features, receiver_inputs)\n\n    return serving_input_fn\nlog\ntensorflow1.4\nINFO:tensorflow:Using config: {'_model_dir': 'models_vgg_100K_no_eval', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n  per_process_gpu_memory_fraction: 0.6\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002BAAA7A6780>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Starting evaluation at 2017-11-28-12:21:42\nINFO:tensorflow:Restoring parameters from model_test\\model.ckpt-54692\n2017-11-28 20:22:04.720980: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.236689657]\nINFO:tensorflow:Evaluation [1/3]\n2017-11-28 20:28:32.360331: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.238805175]\nINFO:tensorflow:Evaluation [2/3]\n2017-11-28 20:35:41.020994: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.237995088]\nINFO:tensorflow:Evaluation [3/3]\nINFO:tensorflow:Finished evaluation at 2017-11-28-12:43:21\nINFO:tensorflow:Saving dict for global step 54692: eval/CER = 0.0108218, eval/accuracy = 0.929688, global_step = 54692, loss = 0.23783\ntime:1306.1133954524994\nmodel: model_test\\model.ckpt-54692 Evaluation results: {'eval/CER': 0.01082176, 'eval/accuracy': 0.9296875, 'loss': 0.23782997, 'global_step': 54692}\ntensorflow 1.3\nINFO:tensorflow:Using config: {'_model_dir': 'models_vgg_100K_no_eval', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': gpu_options {\n  per_process_gpu_memory_fraction: 0.6\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\nINFO:tensorflow:Starting evaluation at 2017-11-28-12:49:50\nINFO:tensorflow:Restoring parameters from model_test\\model.ckpt-54692\n2017-11-28 20:50:12.841210: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.17519826]\nINFO:tensorflow:Evaluation [1/3]\n2017-11-28 20:51:03.366275: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.2987892]\nINFO:tensorflow:Evaluation [2/3]\n2017-11-28 20:51:49.843030: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.20660429]\nINFO:tensorflow:Evaluation [3/3]\nINFO:tensorflow:Finished evaluation at 2017-11-28-12:52:19\nINFO:tensorflow:Saving dict for global step 54692: eval/CER = 0.01188, eval/accuracy = 0.924479, global_step = 54692, loss = 0.226864\ntime:157.26274514198303\nmodel: model_test\\model.ckpt-54692 Evaluation results: {'eval/CER': 0.011879961, 'eval/accuracy': 0.92447919, 'loss': 0.22686392, 'global_step': 54692}", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**: python wheel\r\n- **TensorFlow version (use command below)**: 1.4 and 1.3\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:\r\n\r\nwhen I run tensorflow1.4 script using estimator, the script is 8 times slower than tensorflow 1.3\r\n\r\n### Source code / logs\r\n`main` script\r\n```python\r\n#!/usr/bin/env python\r\n__author__ = 'zj'\r\n\r\nimport argparse\r\nimport os\r\nimport sys\r\nimport numpy as np\r\nimport time\r\ntry:\r\n    import better_exceptions\r\nexcept ImportError:\r\n    pass\r\nimport tensorflow as tf\r\nfrom src.model_ori import crnn_fn\r\nfrom src.data_handler import data_loader\r\nfrom src.config import Params, Alphabet\r\nfrom src.input_utils import input_fn\r\n\r\n\r\ndef main(unused_argv):\r\n    models_path = FLAGS.input_model_dir\r\n    if not os.path.exists(models_path):\r\n        assert FileNotFoundError\r\n\r\n    models_list = [os.path.join(models_path, x[:-5]) for x in os.listdir(models_path) if x.endswith('.meta')]\r\n\r\n    if not os.path.exists(FLAGS.output_model_dir):\r\n        os.makedirs(FLAGS.output_model_dir)\r\n\r\n    parameters = Params(eval_batch_size=128,\r\n                        input_shape=(32, 304),\r\n                        digits_only=False,\r\n                        alphabet=Alphabet.CHINESECHAR_LETTERS_DIGITS_EXTENDED,\r\n                        alphabet_decoding='same',\r\n                        image_channels=1,\r\n                        csv_delimiter=' ',\r\n                        csv_files_eval=FLAGS.csv_files_eval,\r\n                        output_model_dir=FLAGS.output_model_dir,\r\n                        gpu=FLAGS.gpu\r\n                        )\r\n\r\n    model_params = {\r\n        'Params': parameters,\r\n    }\r\n\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = parameters.gpu\r\n    config_sess = tf.ConfigProto()\r\n    config_sess.gpu_options.per_process_gpu_memory_fraction = 0.6\r\n\r\n    # Config estimator\r\n    est_config = tf.estimator.RunConfig()\r\n    est_config = est_config.replace(session_config=config_sess,\r\n                                    save_summary_steps=100,\r\n                                    model_dir=parameters.output_model_dir)\r\n\r\n    estimator = tf.estimator.Estimator(model_fn=crnn_fn,\r\n                                       params=model_params,\r\n                                       config=est_config,\r\n                                       model_dir=parameters.output_model_dir,\r\n                                       )\r\n    try:\r\n        with open(FLAGS.output_file, encoding='utf-8', mode='w') as save_file:\r\n            for model in models_list:\r\n                start = time.time()\r\n                \r\n                eval_results = estimator.evaluate(input_fn=data_loader(csv_filename=parameters.csv_files_eval,\r\n                                                                       params=parameters,\r\n                                                                       batch_size=parameters.eval_batch_size,\r\n                                                                       num_epochs=1),\r\n                                                  steps=3,\r\n                                                  checkpoint_path=model)\r\n                print('time:',time.time() - start)\r\n                print('model: %s Evaluation results: %s' % (model, str(eval_results)))\r\n                save_file.write(model + ' ' + str(eval_results) + '\\n')\r\n\r\n    except KeyboardInterrupt:\r\n        print('Interrupted')\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('-fe', '--csv_files_eval', required=False, type=str, help='CSV filename for evaluation',\r\n                        nargs='*', default=['E:/val1.csv'])\r\n    parser.add_argument('-o', '--output_model_dir', required=False, type=str,\r\n                        help='Directory for output', default='models_vgg_100K_no_eval')\r\n    parser.add_argument('-m', '--input_model_dir', required=False, type=str,\r\n                        help='Directory for output', default='model_test')\r\n    parser.add_argument('-g', '--gpu', type=str, help=\"GPU 0,1 or '' \", default='0')\r\n    parser.add_argument('-of', '--output_file', required=False, type=str, default='123.txt', help=\"the log output file\")\r\n\r\n    tf.logging.set_verbosity(tf.logging.DEBUG)\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\r\n```\r\n\r\n`data_loader` script\r\n```python\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom .config import Params, CONST\r\nfrom typing import Tuple\r\n\r\n\r\ndef data_loader(csv_filename: str, params: Params, batch_size: int = 128, data_augmentation: bool = False,\r\n                num_epochs: int = None, image_summaries: bool = False):\r\n    def input_fn():\r\n        # Choose case one csv file or list of csv files\r\n        if not isinstance(csv_filename, list):\r\n            filename_queue = tf.train.string_input_producer([csv_filename], num_epochs=num_epochs,\r\n                                                            name='filename_queue')\r\n        elif isinstance(csv_filename, list):\r\n            filename_queue = tf.train.string_input_producer(csv_filename, num_epochs=num_epochs, name='filename_queue')\r\n\r\n        # Skip lines that have already been processed\r\n        reader = tf.TextLineReader(name='CSV_Reader', skip_header_lines=0)\r\n        key, value = reader.read(filename_queue, name='file_reading_op')\r\n\r\n        default_line = [['None'], ['None']]\r\n        path, label = tf.decode_csv(value, record_defaults=default_line, field_delim=params.csv_delimiter,\r\n                                    name='csv_reading_op')\r\n\r\n        image, img_width = image_reading(path, resized_size=params.input_shape, params=params,\r\n                                         data_augmentation=data_augmentation, padding=True)\r\n\r\n        to_batch = {'images': image, 'images_widths': img_width, 'filenames': path, 'labels': label}\r\n        prepared_batch = tf.train.shuffle_batch(to_batch,\r\n                                                batch_size=batch_size,\r\n                                                min_after_dequeue=500,\r\n                                                num_threads=15, capacity=4000,\r\n                                                allow_smaller_final_batch=False,\r\n                                                name='prepared_batch_queue')\r\n\r\n        if image_summaries:\r\n            tf.summary.image('input/image', prepared_batch.get('images'), max_outputs=1)\r\n        tf.summary.text('input/labels', prepared_batch.get('labels')[:10])\r\n        tf.summary.text('input/widths', tf.as_string(prepared_batch.get('images_widths')))\r\n\r\n        return prepared_batch, prepared_batch.get('labels')\r\n\r\n    return input_fn\r\n\r\n\r\ndef image_reading(path: str, params: Params, resized_size: Tuple[int, int] = None, data_augmentation: bool = False,\r\n                  padding: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:\r\n    # Read image\r\n    image_content = tf.read_file(path, name='image_reader')\r\n    image = tf.cond(tf.equal(tf.string_split([path], '.').values[1], tf.constant('jpg', dtype=tf.string)),\r\n                    true_fn=lambda: tf.image.decode_jpeg(image_content, channels=params.image_channels,\r\n                                                         try_recover_truncated=True),  # TODO channels = 3 ?\r\n                    false_fn=lambda: tf.image.decode_png(image_content, channels=params.image_channels),\r\n                    name='image_decoding')\r\n\r\n    # Data augmentation\r\n    if data_augmentation:\r\n        image = augment_data(image)\r\n\r\n    # Padding\r\n    if padding:\r\n        with tf.name_scope('padding'):\r\n            image, img_width = padding_inputs_width(image, resized_size, increment=CONST.DIMENSION_REDUCTION_W_POOLING)\r\n    # Resize\r\n    else:\r\n        image = tf.image.resize_images(image, size=resized_size)\r\n        img_width = tf.shape(image)[1]\r\n\r\n    with tf.control_dependencies([tf.assert_equal(image.shape[:2], resized_size)]):\r\n        return image, img_width\r\n\r\n\r\ndef random_rotation(img: tf.Tensor, max_rotation: float = 0.1, crop: bool = True) -> tf.Tensor:  # from SeguinBe\r\n    with tf.name_scope('RandomRotation'):\r\n        rotation = tf.random_uniform([], -max_rotation, max_rotation)\r\n        rotated_image = tf.contrib.image.rotate(img, rotation, interpolation='BILINEAR')\r\n        if crop:\r\n            rotation = tf.abs(rotation)\r\n            original_shape = tf.shape(rotated_image)[:2]\r\n            h, w = original_shape[0], original_shape[1]\r\n            # see https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders for formulae\r\n            old_l, old_s = tf.cond(h > w, lambda: [h, w], lambda: [w, h])\r\n            old_l, old_s = tf.cast(old_l, tf.float32), tf.cast(old_s, tf.float32)\r\n            new_l = (old_l * tf.cos(rotation) - old_s * tf.sin(rotation)) / tf.cos(2 * rotation)\r\n            new_s = (old_s - tf.sin(rotation) * new_l) / tf.cos(rotation)\r\n            new_h, new_w = tf.cond(h > w, lambda: [new_l, new_s], lambda: [new_s, new_l])\r\n            new_h, new_w = tf.cast(new_h, tf.int32), tf.cast(new_w, tf.int32)\r\n            bb_begin = tf.cast(tf.ceil((h - new_h) / 2), tf.int32), tf.cast(tf.ceil((w - new_w) / 2), tf.int32)\r\n            rotated_image_crop = rotated_image[bb_begin[0]:h - bb_begin[0], bb_begin[1]:w - bb_begin[1], :]\r\n\r\n            # If crop removes the entire image, keep the original image\r\n            rotated_image = tf.cond(tf.equal(tf.size(rotated_image_crop), 0),\r\n                                    true_fn=lambda: img,\r\n                                    false_fn=lambda: rotated_image_crop)\r\n\r\n        return rotated_image\r\n\r\n\r\ndef random_padding(image: tf.Tensor, max_pad_w: int = 5, max_pad_h: int = 10) -> tf.Tensor:\r\n    w_pad = list(np.random.randint(0, max_pad_w, size=[2]))\r\n    h_pad = list(np.random.randint(0, max_pad_h, size=[2]))\r\n    paddings = [h_pad, w_pad, [0, 0]]\r\n\r\n    return tf.pad(image, paddings, mode='REFLECT', name='random_padding')\r\n\r\n\r\ndef augment_data(image: tf.Tensor) -> tf.Tensor:\r\n    with tf.name_scope('DataAugmentation'):\r\n        # Random padding\r\n        image = random_padding(image)\r\n\r\n        image = tf.image.random_brightness(image, max_delta=0.1)\r\n        image = tf.image.random_contrast(image, 0.5, 1.5)\r\n        image = random_rotation(image, 0.05, crop=True)\r\n\r\n        if image.shape[-1] >= 3:\r\n            image = tf.image.random_hue(image, 0.2)\r\n            image = tf.image.random_saturation(image, 0.5, 1.5)\r\n\r\n        return image\r\n\r\n\r\ndef padding_inputs_width(image: tf.Tensor, target_shape: Tuple[int, int], increment: int) -> Tuple[\r\n    tf.Tensor, tf.Tensor]:\r\n    target_ratio = target_shape[1] / target_shape[0]\r\n    # Compute ratio to keep the same ratio in new image and get the size of padding\r\n    # necessary to have the final desired shape\r\n    shape = tf.shape(image)\r\n    # \u8ba1\u7b97\u5bbd\u9ad8\u6bd4\r\n    ratio = tf.divide(shape[1], shape[0], name='ratio')\r\n\r\n    new_h = target_shape[0]\r\n    new_w = tf.cast(tf.round((ratio * new_h) / increment) * increment, tf.int32)\r\n    f1 = lambda: (new_w, ratio)\r\n    f2 = lambda: (new_h, tf.constant(1.0, dtype=tf.float64))\r\n    new_w, ratio = tf.case({tf.greater(new_w, 0): f1,\r\n                            tf.less_equal(new_w, 0): f2},\r\n                           default=f1, exclusive=True)\r\n    target_w = target_shape[1]\r\n\r\n    # Definitions for cases\r\n    def pad_fn():\r\n        with tf.name_scope('mirror_padding'):\r\n            pad = tf.subtract(target_w, new_w)\r\n\r\n            img_resized = tf.image.resize_images(image, [new_h, new_w])\r\n\r\n            # Padding to have the desired width\r\n            paddings = [[0, 0], [0, pad], [0, 0]]\r\n            pad_image = tf.pad(img_resized, paddings, mode='SYMMETRIC', name=None)\r\n\r\n            # Set manually the shape\r\n            pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\r\n\r\n            return pad_image, (new_h, new_w)\r\n\r\n    def replicate_fn():\r\n        with tf.name_scope('replication_padding'):\r\n            img_resized = tf.image.resize_images(image, [new_h, new_w])\r\n\r\n            # If one symmetry is not enough to have a full width\r\n            # Count number of replications needed\r\n            n_replication = tf.cast(tf.ceil(target_shape[1] / new_w), tf.int32)\r\n            img_replicated = tf.tile(img_resized, tf.stack([1, n_replication, 1]))\r\n            pad_image = tf.image.crop_to_bounding_box(image=img_replicated, offset_height=0, offset_width=0,\r\n                                                      target_height=target_shape[0], target_width=target_shape[1])\r\n\r\n            # Set manually the shape\r\n            pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\r\n\r\n            return pad_image, (new_h, new_w)\r\n\r\n    def simple_resize():\r\n        with tf.name_scope('simple_resize'):\r\n            img_resized = tf.image.resize_images(image, target_shape)\r\n\r\n            img_resized.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])\r\n\r\n            return img_resized, target_shape\r\n\r\n    # 3 cases\r\n    pad_image, (new_h, new_w) = tf.case(\r\n        {  # case 1 : new_w >= target_w\r\n            tf.logical_and(tf.greater_equal(ratio, target_ratio),\r\n                           tf.greater_equal(new_w, target_w)): simple_resize,\r\n            # case 2 : new_w >= target_w/2 & new_w < target_w & ratio < target_ratio\r\n            tf.logical_and(tf.less(ratio, target_ratio),\r\n                           tf.logical_and(tf.greater_equal(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)),\r\n                                          tf.less(new_w, target_w))): pad_fn,\r\n            # case 3 : new_w < target_w/2 & new_w < target_w & ratio < target_ratio\r\n            tf.logical_and(tf.less(ratio, target_ratio),\r\n                           tf.logical_and(tf.less(new_w, target_w),\r\n                                          tf.less(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)))): replicate_fn\r\n        },\r\n        default=simple_resize, exclusive=True)\r\n\r\n    return pad_image, new_w  # new_w = image width used for computing sequence lengths\r\n\r\n\r\ndef preprocess_image_for_prediction(fixed_height: int = 32, min_width: int = 8):\r\n    \"\"\"\r\n    Input function to use when exporting the model for making predictions (see estimator.export_savedmodel)\r\n    :param fixed_height: height of the input image after resizing\r\n    :param min_width: minimum width of image after resizing\r\n    :return:\r\n    \"\"\"\r\n\r\n    def serving_input_fn():\r\n        # define placeholder for input image\r\n        image = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])\r\n\r\n        shape = tf.shape(image)\r\n        # Assert shape is h x w x c with c = 1\r\n\r\n        ratio = tf.divide(shape[1], shape[0])\r\n        increment = CONST.DIMENSION_REDUCTION_W_POOLING\r\n        new_width = tf.cast(tf.round((ratio * fixed_height) / increment) * increment, tf.int32)\r\n\r\n        resized_image = tf.cond(new_width < tf.constant(min_width, dtype=tf.int32),\r\n                                true_fn=lambda: tf.image.resize_images(image, size=(fixed_height, min_width)),\r\n                                false_fn=lambda: tf.image.resize_images(image, size=(fixed_height, new_width))\r\n                                )\r\n\r\n        # Features to serve\r\n        features = {'images': resized_image[None],  # cast to 1 x h x w x c\r\n                    'images_widths': new_width[None]  # cast to tensor\r\n                    }\r\n\r\n        # Inputs received\r\n        receiver_inputs = {'images': image}\r\n\r\n        return tf.estimator.export.ServingInputReceiver(features, receiver_inputs)\r\n\r\n    return serving_input_fn\r\n```\r\n\r\nlog\r\ntensorflow1.4\r\n```sh\r\nINFO:tensorflow:Using config: {'_model_dir': 'models_vgg_100K_no_eval', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 0.6\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002BAAA7A6780>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Starting evaluation at 2017-11-28-12:21:42\r\nINFO:tensorflow:Restoring parameters from model_test\\model.ckpt-54692\r\n2017-11-28 20:22:04.720980: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.236689657]\r\nINFO:tensorflow:Evaluation [1/3]\r\n2017-11-28 20:28:32.360331: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.238805175]\r\nINFO:tensorflow:Evaluation [2/3]\r\n2017-11-28 20:35:41.020994: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.237995088]\r\nINFO:tensorflow:Evaluation [3/3]\r\nINFO:tensorflow:Finished evaluation at 2017-11-28-12:43:21\r\nINFO:tensorflow:Saving dict for global step 54692: eval/CER = 0.0108218, eval/accuracy = 0.929688, global_step = 54692, loss = 0.23783\r\ntime:1306.1133954524994\r\nmodel: model_test\\model.ckpt-54692 Evaluation results: {'eval/CER': 0.01082176, 'eval/accuracy': 0.9296875, 'loss': 0.23782997, 'global_step': 54692}\r\n```\r\n\r\ntensorflow 1.3\r\n```sh\r\nINFO:tensorflow:Using config: {'_model_dir': 'models_vgg_100K_no_eval', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 0.6\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\r\nINFO:tensorflow:Starting evaluation at 2017-11-28-12:49:50\r\nINFO:tensorflow:Restoring parameters from model_test\\model.ckpt-54692\r\n2017-11-28 20:50:12.841210: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.17519826]\r\nINFO:tensorflow:Evaluation [1/3]\r\n2017-11-28 20:51:03.366275: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.2987892]\r\nINFO:tensorflow:Evaluation [2/3]\r\n2017-11-28 20:51:49.843030: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\kernels\\logging_ops.cc:79] * Loss : [0.20660429]\r\nINFO:tensorflow:Evaluation [3/3]\r\nINFO:tensorflow:Finished evaluation at 2017-11-28-12:52:19\r\nINFO:tensorflow:Saving dict for global step 54692: eval/CER = 0.01188, eval/accuracy = 0.924479, global_step = 54692, loss = 0.226864\r\ntime:157.26274514198303\r\nmodel: model_test\\model.ckpt-54692 Evaluation results: {'eval/CER': 0.011879961, 'eval/accuracy': 0.92447919, 'loss': 0.22686392, 'global_step': 54692}\r\n```"}