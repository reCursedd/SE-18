{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23386", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23386/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23386/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23386/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23386", "id": 375656191, "node_id": "MDU6SXNzdWUzNzU2NTYxOTE=", "number": 23386, "title": "InvalidArgumentError: Invalid name: An op that loads optimization parameters into HBM for embedding. (ConfigureTPUEmbeddingHost)", "user": {"login": "neopostmodern", "id": 786840, "node_id": "MDQ6VXNlcjc4Njg0MA==", "avatar_url": "https://avatars0.githubusercontent.com/u/786840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neopostmodern", "html_url": "https://github.com/neopostmodern", "followers_url": "https://api.github.com/users/neopostmodern/followers", "following_url": "https://api.github.com/users/neopostmodern/following{/other_user}", "gists_url": "https://api.github.com/users/neopostmodern/gists{/gist_id}", "starred_url": "https://api.github.com/users/neopostmodern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neopostmodern/subscriptions", "organizations_url": "https://api.github.com/users/neopostmodern/orgs", "repos_url": "https://api.github.com/users/neopostmodern/repos", "events_url": "https://api.github.com/users/neopostmodern/events{/privacy}", "received_events_url": "https://api.github.com/users/neopostmodern/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097546578, "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras", "name": "comp:keras", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-10-30T19:29:17Z", "updated_at": "2018-11-21T18:58:35Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No</li>\n<li>TensorFlow installed from (source or binary): Source</li>\n<li>TensorFlow version: 1.12.0-rc0</li>\n<li>Python version: 3.6.7rc1</li>\n<li>Bazel version (if compiling from source): 0.19.0</li>\n<li>GCC/Compiler version (if compiling from source): 7.3.0</li>\n<li>CUDA/cuDNN version: 10.0.130 / 7.3.1.20</li>\n<li>GPU model and memory: Nvidia M1000M / ~4GB</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nPyPI packages for tensorflow-gpu didn't work for me on Ubuntu 18.10, but I (somehow) managed to compile the latest Tensorflow. I then tried to run some code that I was working on before (and which used to run fine) and got the following error:</p>\n<pre><code>\nUsing TensorFlow backend.\nTensorflow: 1.12.0-rc0\nTraceback (most recent call last):\n  File \"/home/neopostmodern/.PyCharm2018.1/config/scratches/scratch_3.py\", line 10, in &lt;module&gt;\n    model.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/sequential.py\", line 165, in add\n    layer(x)\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 532, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py\", line 425, in build\n    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 40, in &lt;module&gt;\n    from tensorflow.contrib import distribute\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in &lt;module&gt;\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 29, in &lt;module&gt;\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in &lt;module&gt;\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in &lt;module&gt;\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\n    ret = load_library.load_op_library(path)\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n (Did you use CamelCase?); in OpDef: name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { i: -1 } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" has_minimum: true minimum: -1 } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { s: \"\" } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } summary: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" is_stateful: true\n</code></pre>\n<p><strong>Describe the expected behavior</strong><br>\nIt should run, as it did before.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nReduced to the minimum:</p>\n<pre><code>import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import CuDNNLSTM\n\ntf.set_random_seed(42)\n\nprint(f'Tensorflow: {tf.__version__}')\n\nmodel = Sequential()\nmodel.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\nTensorFlow installed from (source or binary): Source\nTensorFlow version: 1.12.0-rc0\nPython version: 3.6.7rc1\nBazel version (if compiling from source): 0.19.0\nGCC/Compiler version (if compiling from source): 7.3.0\nCUDA/cuDNN version: 10.0.130 / 7.3.1.20\nGPU model and memory: Nvidia M1000M / ~4GB\n\nDescribe the current behavior\nPyPI packages for tensorflow-gpu didn't work for me on Ubuntu 18.10, but I (somehow) managed to compile the latest Tensorflow. I then tried to run some code that I was working on before (and which used to run fine) and got the following error:\n\nUsing TensorFlow backend.\nTensorflow: 1.12.0-rc0\nTraceback (most recent call last):\n  File \"/home/neopostmodern/.PyCharm2018.1/config/scratches/scratch_3.py\", line 10, in <module>\n    model.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/sequential.py\", line 165, in add\n    layer(x)\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 532, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py\", line 425, in build\n    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\n    from tensorflow.contrib import distribute\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 29, in <module>\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\n    ret = load_library.load_op_library(path)\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n (Did you use CamelCase?); in OpDef: name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { i: -1 } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" has_minimum: true minimum: -1 } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { s: \"\" } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } summary: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" is_stateful: true\n\nDescribe the expected behavior\nIt should run, as it did before.\nCode to reproduce the issue\nReduced to the minimum:\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import CuDNNLSTM\n\ntf.set_random_seed(42)\n\nprint(f'Tensorflow: {tf.__version__}')\n\nmodel = Sequential()\nmodel.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12.0-rc0\r\n- Python version: 3.6.7rc1\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0.130 / 7.3.1.20\r\n- GPU model and memory: Nvidia M1000M / ~4GB\r\n\r\n**Describe the current behavior**\r\nPyPI packages for tensorflow-gpu didn't work for me on Ubuntu 18.10, but I (somehow) managed to compile the latest Tensorflow. I then tried to run some code that I was working on before (and which used to run fine) and got the following error:\r\n```\r\n\r\nUsing TensorFlow backend.\r\nTensorflow: 1.12.0-rc0\r\nTraceback (most recent call last):\r\n  File \"/home/neopostmodern/.PyCharm2018.1/config/scratches/scratch_3.py\", line 10, in <module>\r\n    model.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/sequential.py\", line 165, in add\r\n    layer(x)\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 532, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 431, in __call__\r\n    self.build(unpack_singleton(input_shapes))\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/keras/layers/cudnn_recurrent.py\", line 425, in build\r\n    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 29, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\r\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/home/neopostmodern/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.\r\n\r\nparameters: A tensor containing the initial embedding table parameters to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\r\nlookups using the Adagrad optimization algorithm.\r\ntable_name: Name of this table; must match a name in the\r\n  TPUEmbeddingConfiguration proto (overrides table_id).\r\nnum_shards: Number of shards into which the embedding tables are divided.\r\nshard_id: Identifier of shard for this operation.\r\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\r\n  (deprecated).\r\n (Did you use CamelCase?); in OpDef: name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } input_arg { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: DT_FLOAT type_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" number_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type_list_attr: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { i: -1 } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" has_minimum: true minimum: -1 } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" default_value { s: \"\" } description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } attr { name: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" type: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" } summary: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" description: \"\\nAn op that loads optimization parameters into HBM for embedding. Must be\\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\\nembedding table configuration. For example, this op is used to install\\nparameters that are loaded from a checkpoint before a training loop is\\nexecuted.\\n\\nparameters: A tensor containing the initial embedding table parameters to use in embedding\\nlookups using the Adagrad optimization algorithm.\\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\\nlookups using the Adagrad optimization algorithm.\\ntable_name: Name of this table; must match a name in the\\n  TPUEmbeddingConfiguration proto (overrides table_id).\\nnum_shards: Number of shards into which the embedding tables are divided.\\nshard_id: Identifier of shard for this operation.\\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\\n  (deprecated).\\n\" is_stateful: true\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should run, as it did before.\r\n\r\n**Code to reproduce the issue**\r\nReduced to the minimum:\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import CuDNNLSTM\r\n\r\ntf.set_random_seed(42)\r\n\r\nprint(f'Tensorflow: {tf.__version__}')\r\n\r\nmodel = Sequential()\r\nmodel.add(CuDNNLSTM(64, input_shape=(10, 39), return_sequences=True))\r\n```\r\n"}