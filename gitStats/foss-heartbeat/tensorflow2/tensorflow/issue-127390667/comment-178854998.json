{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/178854998", "html_url": "https://github.com/tensorflow/tensorflow/issues/808#issuecomment-178854998", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/808", "id": 178854998, "node_id": "MDEyOklzc3VlQ29tbWVudDE3ODg1NDk5OA==", "user": {"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-02T22:14:02Z", "updated_at": "2016-02-02T22:20:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here are the debugging steps that I recently went through to get TensorFlow working (kind of) with GPU and Docker. (I say \"kind of\" because there are still some GPU-related bugs in Tensorflow, which caused some test failures and will likely cause some user-code errors in that regard as well).</p>\n<p>See:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"130439666\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/952\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/952/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/952\">#952</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"130514566\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/953\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/953/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/953\">#953</a></p>\n<p>That said, here are the things you want to check on:</p>\n<ol>\n<li>\n<p>On the host, outside Docker, you have NVIDIA driver installed. If you have it installed, the binaries \"nvidia-smi\" and \"nvidia-debugdump\" ought to be available on the host. Make sure that the following two commands list your GPU:<br>\n<code>nvidia-smi</code><br>\n<code>nvidia-debugdump -l</code></p>\n</li>\n<li>\n<p>On the host, the output of nvidia-smi tells you the version of the NVIDIA driver installed. It needs to be recent enough for your GPU. For example, on my machine, version 340 doesn't work, but version 352 does.</p>\n</li>\n<li>\n<p>On the host, get the CUDA sample code and compile the deviceQuery binary:<br>\n<a href=\"http://docs.nvidia.com/cuda/cuda-samples/#axzz3z3C3lhk1\" rel=\"nofollow\">http://docs.nvidia.com/cuda/cuda-samples/#axzz3z3C3lhk1</a></p>\n</li>\n</ol>\n<p>For this you'll need to install the CUDA toolkit, which includes the nvcc compiler and supporting libraries.<br>\n<a href=\"https://developer.nvidia.com/cuda-downloads\" rel=\"nofollow\">https://developer.nvidia.com/cuda-downloads</a></p>\n<p>Once the deviceQuery binary is compiled, try to run it<br>\n<code>./deviceQuery</code></p>\n<p>If it fails, don't panic, just try<br>\n<code>sudo ./deviceQuery</code></p>\n<p>There are some file permissions issues related to NVIDIA devices in /dev that requires you to do sudo like the above for each boot cycle, for detailed dicussion, see:<br>\n<a href=\"https://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/\" rel=\"nofollow\">https://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/</a></p>\n<ol start=\"4\">\n<li>After step 3, CUDA GPU is all set on the host. Let's now look inside Docker. Make sure that you install the same CUDA Toolkit as listed in step 3. TensorFlow additionally requires CUDA DNN (cudnn) libraries which you can also get from the CUDA website, after a somewhat time consuming user approval process.</li>\n</ol>\n<p>After this make sure that the following files are present in your docker, as they will be used by (the current version of) TensorFlow:</p>\n<pre><code>/usr/local/cuda/include/cudnn.h \n/usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so.6.5/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so/libcudnn.so.6.5\n</code></pre>\n<p>It is okay for NVIDIA driver to be unavailable inside the Docker, even though the CUDA Toolkit is required inside it.</p>\n<ol start=\"5\">\n<li>Before you can try to start your Docker container, make sure you use docker flags to map a few devices so the the NVIDIA devices are available under /dev, inside the container:</li>\n</ol>\n<pre><code>\"--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl\"\n</code></pre>\n<p>Also, map a bunch of lib files, to make sure that cuda library files are visible inside the container</p>\n<pre><code>\"-v /usr/lib/x86_64-linux-gnu/libcudadevrt.a:/usr/lib/x86_64-linux-gnu/libcudadevrt.a\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so:/usr/lib/x86_64-linux-gnu/libcudart.so\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22\" \"-v /usr/lib/x86_64-linux-gnu/libcudart_static.a:/usr/lib/x86_64-linux-gnu/libcudart_static.a\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.352.63:/usr/lib/x86_64-linux-gnu/libcuda.so.352.63\"\n</code></pre>\n<p>Now the NVIDIA docker container should be ready to run TensorFlow with GPU.</p>", "body_text": "Here are the debugging steps that I recently went through to get TensorFlow working (kind of) with GPU and Docker. (I say \"kind of\" because there are still some GPU-related bugs in Tensorflow, which caused some test failures and will likely cause some user-code errors in that regard as well).\nSee:\n#952\n#953\nThat said, here are the things you want to check on:\n\n\nOn the host, outside Docker, you have NVIDIA driver installed. If you have it installed, the binaries \"nvidia-smi\" and \"nvidia-debugdump\" ought to be available on the host. Make sure that the following two commands list your GPU:\nnvidia-smi\nnvidia-debugdump -l\n\n\nOn the host, the output of nvidia-smi tells you the version of the NVIDIA driver installed. It needs to be recent enough for your GPU. For example, on my machine, version 340 doesn't work, but version 352 does.\n\n\nOn the host, get the CUDA sample code and compile the deviceQuery binary:\nhttp://docs.nvidia.com/cuda/cuda-samples/#axzz3z3C3lhk1\n\n\nFor this you'll need to install the CUDA toolkit, which includes the nvcc compiler and supporting libraries.\nhttps://developer.nvidia.com/cuda-downloads\nOnce the deviceQuery binary is compiled, try to run it\n./deviceQuery\nIf it fails, don't panic, just try\nsudo ./deviceQuery\nThere are some file permissions issues related to NVIDIA devices in /dev that requires you to do sudo like the above for each boot cycle, for detailed dicussion, see:\nhttps://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/\n\nAfter step 3, CUDA GPU is all set on the host. Let's now look inside Docker. Make sure that you install the same CUDA Toolkit as listed in step 3. TensorFlow additionally requires CUDA DNN (cudnn) libraries which you can also get from the CUDA website, after a somewhat time consuming user approval process.\n\nAfter this make sure that the following files are present in your docker, as they will be used by (the current version of) TensorFlow:\n/usr/local/cuda/include/cudnn.h \n/usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so.6.5/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so/libcudnn.so.6.5\n\nIt is okay for NVIDIA driver to be unavailable inside the Docker, even though the CUDA Toolkit is required inside it.\n\nBefore you can try to start your Docker container, make sure you use docker flags to map a few devices so the the NVIDIA devices are available under /dev, inside the container:\n\n\"--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl\"\n\nAlso, map a bunch of lib files, to make sure that cuda library files are visible inside the container\n\"-v /usr/lib/x86_64-linux-gnu/libcudadevrt.a:/usr/lib/x86_64-linux-gnu/libcudadevrt.a\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so:/usr/lib/x86_64-linux-gnu/libcudart.so\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22\" \"-v /usr/lib/x86_64-linux-gnu/libcudart_static.a:/usr/lib/x86_64-linux-gnu/libcudart_static.a\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.352.63:/usr/lib/x86_64-linux-gnu/libcuda.so.352.63\"\n\nNow the NVIDIA docker container should be ready to run TensorFlow with GPU.", "body": "Here are the debugging steps that I recently went through to get TensorFlow working (kind of) with GPU and Docker. (I say \"kind of\" because there are still some GPU-related bugs in Tensorflow, which caused some test failures and will likely cause some user-code errors in that regard as well). \n\nSee: \nhttps://github.com/tensorflow/tensorflow/issues/952\nhttps://github.com/tensorflow/tensorflow/issues/953\n\nThat said, here are the things you want to check on: \n\n1) On the host, outside Docker, you have NVIDIA driver installed. If you have it installed, the binaries \"nvidia-smi\" and \"nvidia-debugdump\" ought to be available on the host. Make sure that the following two commands list your GPU: \n`nvidia-smi`\n`nvidia-debugdump -l`\n\n2) On the host, the output of nvidia-smi tells you the version of the NVIDIA driver installed. It needs to be recent enough for your GPU. For example, on my machine, version 340 doesn't work, but version 352 does. \n\n3) On the host, get the CUDA sample code and compile the deviceQuery binary:\nhttp://docs.nvidia.com/cuda/cuda-samples/#axzz3z3C3lhk1\n\nFor this you'll need to install the CUDA toolkit, which includes the nvcc compiler and supporting libraries. \nhttps://developer.nvidia.com/cuda-downloads\n\nOnce the deviceQuery binary is compiled, try to run it\n`./deviceQuery`\n\nIf it fails, don't panic, just try\n`sudo ./deviceQuery`\n\nThere are some file permissions issues related to NVIDIA devices in /dev that requires you to do sudo like the above for each boot cycle, for detailed dicussion, see: \nhttps://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/\n\n4) After step 3, CUDA GPU is all set on the host. Let's now look inside Docker. Make sure that you install the same CUDA Toolkit as listed in step 3. TensorFlow additionally requires CUDA DNN (cudnn) libraries which you can also get from the CUDA website, after a somewhat time consuming user approval process. \n\nAfter this make sure that the following files are present in your docker, as they will be used by (the current version of) TensorFlow:\n\n```\n/usr/local/cuda/include/cudnn.h \n/usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so.6.5/libcudnn.so.6.5.48 \n/usr/local/cuda/lib64/libcudnn.so/libcudnn.so.6.5\n```\n\nIt is okay for NVIDIA driver to be unavailable inside the Docker, even though the CUDA Toolkit is required inside it.\n\n5) Before you can try to start your Docker container, make sure you use docker flags to map a few devices so the the NVIDIA devices are available under /dev, inside the container:\n\n```\n\"--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl\"\n```\n\nAlso, map a bunch of lib files, to make sure that cuda library files are visible inside the container\n\n```\n\"-v /usr/lib/x86_64-linux-gnu/libcudadevrt.a:/usr/lib/x86_64-linux-gnu/libcudadevrt.a\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so:/usr/lib/x86_64-linux-gnu/libcudart.so\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5\" \"-v /usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22:/usr/lib/x86_64-linux-gnu/libcudart.so.5.5.22\" \"-v /usr/lib/x86_64-linux-gnu/libcudart_static.a:/usr/lib/x86_64-linux-gnu/libcudart_static.a\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1\" \"-v /usr/lib/x86_64-linux-gnu/libcuda.so.352.63:/usr/lib/x86_64-linux-gnu/libcuda.so.352.63\"\n```\n\nNow the NVIDIA docker container should be ready to run TensorFlow with GPU. \n"}