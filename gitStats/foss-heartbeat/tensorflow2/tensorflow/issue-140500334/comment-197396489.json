{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/197396489", "html_url": "https://github.com/tensorflow/tensorflow/issues/1482#issuecomment-197396489", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1482", "id": 197396489, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NzM5NjQ4OQ==", "user": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-16T15:58:50Z", "updated_at": "2016-03-16T15:58:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>You could use fold to achieve what you wanted to do:</p>\n<pre><code>x = tf.placeholder(tf.float32, shape=[None])\ninit = tf.initialize_all_variables()\nsum = tf.foldl(lambda a, e: a + e, x, initializer=0.0)\nwith tf.Session() as sess:\n  sess.run(init)\n  for i in range(100):\n    length = np.random.randint(0,10)\n    a = np.random.randint(0, 10, length)\n  print sess.run(sum,feed_dict={x:a})\n</code></pre>\n<p>If you want to split/unpack on a tensor with dynamic shape and do some computation on each subtensor, the combination of TensorArray and While() is the way to go.</p>", "body_text": "You could use fold to achieve what you wanted to do:\nx = tf.placeholder(tf.float32, shape=[None])\ninit = tf.initialize_all_variables()\nsum = tf.foldl(lambda a, e: a + e, x, initializer=0.0)\nwith tf.Session() as sess:\n  sess.run(init)\n  for i in range(100):\n    length = np.random.randint(0,10)\n    a = np.random.randint(0, 10, length)\n  print sess.run(sum,feed_dict={x:a})\n\nIf you want to split/unpack on a tensor with dynamic shape and do some computation on each subtensor, the combination of TensorArray and While() is the way to go.", "body": "You could use fold to achieve what you wanted to do:\n\n```\nx = tf.placeholder(tf.float32, shape=[None])\ninit = tf.initialize_all_variables()\nsum = tf.foldl(lambda a, e: a + e, x, initializer=0.0)\nwith tf.Session() as sess:\n  sess.run(init)\n  for i in range(100):\n    length = np.random.randint(0,10)\n    a = np.random.randint(0, 10, length)\n  print sess.run(sum,feed_dict={x:a})\n```\n\nIf you want to split/unpack on a tensor with dynamic shape and do some computation on each subtensor, the combination of TensorArray and While() is the way to go.\n"}