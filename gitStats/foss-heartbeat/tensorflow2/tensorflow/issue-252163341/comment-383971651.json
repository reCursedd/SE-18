{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/383971651", "html_url": "https://github.com/tensorflow/tensorflow/issues/12514#issuecomment-383971651", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12514", "id": 383971651, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Mzk3MTY1MQ==", "user": {"login": "woodshop", "id": 4654379, "node_id": "MDQ6VXNlcjQ2NTQzNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4654379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/woodshop", "html_url": "https://github.com/woodshop", "followers_url": "https://api.github.com/users/woodshop/followers", "following_url": "https://api.github.com/users/woodshop/following{/other_user}", "gists_url": "https://api.github.com/users/woodshop/gists{/gist_id}", "starred_url": "https://api.github.com/users/woodshop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/woodshop/subscriptions", "organizations_url": "https://api.github.com/users/woodshop/orgs", "repos_url": "https://api.github.com/users/woodshop/repos", "events_url": "https://api.github.com/users/woodshop/events{/privacy}", "received_events_url": "https://api.github.com/users/woodshop/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-24T15:18:17Z", "updated_at": "2018-04-24T15:23:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16907534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rmlarsen\">@rmlarsen</a> Any timeline on when this might be looked at?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22173987\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/brianwa84\">@brianwa84</a> - you might be interested in this</p>\n<p>For others that run into this issue, you can use <code>gradient_override_map</code>. Ex <strong>(please note that this has not been unit tested)</strong>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> array_ops\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> math_ops\n<span class=\"pl-k\">from</span> tensorflow.python.framework <span class=\"pl-k\">import</span> ops\n<span class=\"pl-k\">from</span> tensorflow.python.framework <span class=\"pl-k\">import</span> dtypes\n<span class=\"pl-k\">from</span> tensorflow.python.ops.math_grad <span class=\"pl-k\">import</span> _safe_shape_div\n\n<span class=\"pl-en\">@tf.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ModifiedProdGrad<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_ModifiedProdGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gradient for Prod.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The gradient can be expressed by dividing the product by each entry of the</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> input tensor, but this approach can't deal with zeros in the input.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Here, we avoid this problem by composing the output as a product of two</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cumprod operations.</span>\n\n    input_shape <span class=\"pl-k\">=</span> array_ops.shape(op.inputs[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reshape reduction indices for the case where the parameter is a scalar</span>\n    reduction_indices <span class=\"pl-k\">=</span> array_ops.reshape(op.inputs[<span class=\"pl-c1\">1</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Expand grad to full input shape</span>\n    output_shape_kept_dims <span class=\"pl-k\">=</span> math_ops.reduced_shape(input_shape, op.inputs[<span class=\"pl-c1\">1</span>])\n    tile_scaling <span class=\"pl-k\">=</span> _safe_shape_div(input_shape, output_shape_kept_dims)\n    grad <span class=\"pl-k\">=</span> array_ops.reshape(grad, output_shape_kept_dims)\n    grad <span class=\"pl-k\">=</span> array_ops.tile(grad, tile_scaling)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Pack all reduced dimensions into a single one, so we can perform the</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cumprod ops. If the reduction dims list is empty, it defaults to float32,</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> so we need to cast here.  We put all the shape-related ops on CPU to avoid</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> copying back and forth, and since listdiff is CPU only.</span>\n    <span class=\"pl-k\">with</span> ops.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/cpu:0<span class=\"pl-pds\">\"</span></span>):\n        rank <span class=\"pl-k\">=</span> array_ops.rank(op.inputs[<span class=\"pl-c1\">0</span>])\n        reduction_indices <span class=\"pl-k\">=</span> (reduction_indices <span class=\"pl-k\">+</span> rank) <span class=\"pl-k\">%</span> rank\n        reduced <span class=\"pl-k\">=</span> math_ops.cast(reduction_indices, dtypes.int32)\n        idx <span class=\"pl-k\">=</span> math_ops.range(<span class=\"pl-c1\">0</span>, rank)\n        other, _ <span class=\"pl-k\">=</span> array_ops.setdiff1d(idx, reduced)\n        perm <span class=\"pl-k\">=</span> array_ops.concat([reduced, other], <span class=\"pl-c1\">0</span>)\n        reduced_num <span class=\"pl-k\">=</span> math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\n        other_num <span class=\"pl-k\">=</span> math_ops.reduce_prod(array_ops.gather(input_shape, other))\n    permuted <span class=\"pl-k\">=</span> array_ops.transpose(op.inputs[<span class=\"pl-c1\">0</span>], perm)\n    permuted_shape <span class=\"pl-k\">=</span> array_ops.shape(permuted)\n    reshaped <span class=\"pl-k\">=</span> array_ops.reshape(permuted, (reduced_num, other_num))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Calculate product, leaving out the current entry</span>\n    left <span class=\"pl-k\">=</span> math_ops.cumprod(reshaped, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">exclusive</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    right <span class=\"pl-k\">=</span> math_ops.cumprod(reshaped, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">exclusive</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">reverse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    y <span class=\"pl-k\">=</span> array_ops.reshape(tf.conj(left) <span class=\"pl-k\">*</span> tf.conj(right), permuted_shape)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Invert the transpose and reshape operations.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make sure to set the statically known shape information through a reshape.</span>\n    out <span class=\"pl-k\">=</span> grad <span class=\"pl-k\">*</span> array_ops.transpose(y, array_ops.invert_permutation(perm))\n    <span class=\"pl-k\">return</span> array_ops.reshape(out, input_shape), <span class=\"pl-c1\">None</span></pre></div>\n<p>With TF gradient:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> g:\n    x <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">1.0</span>)\n    E <span class=\"pl-k\">=</span> tf.real(tf.reduce_prod(tf.complex( [x,x], [<span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>x,<span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>x] )))\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.variables_initializer([x]))\n        <span class=\"pl-c1\">print</span>(sess.run(tf.gradients(E,x)))\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> [<span class=\"pl-c1\">10.0</span>]</pre></div>\n<p>With modified gradient:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> g:\n    <span class=\"pl-k\">with</span> g.gradient_override_map({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Prod<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ModifiedProdGrad<span class=\"pl-pds\">\"</span></span>}):\n        x <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">1.0</span>)\n        E <span class=\"pl-k\">=</span> tf.real(tf.reduce_prod(tf.complex( [x,x], [<span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>x,<span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>x] )))\n        <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n            sess.run(tf.variables_initializer([x]))\n            <span class=\"pl-c1\">print</span>(sess.run(tf.gradients(E,x)))\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> [<span class=\"pl-k\">-</span><span class=\"pl-c1\">6.0</span>]</pre></div>", "body_text": "@rmlarsen Any timeline on when this might be looked at?\n@brianwa84 - you might be interested in this\nFor others that run into this issue, you can use gradient_override_map. Ex (please note that this has not been unit tested):\nimport tensorflow as tf\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops.math_grad import _safe_shape_div\n\n@tf.RegisterGradient(\"ModifiedProdGrad\")\ndef _ModifiedProdGrad(op, grad):\n    \"\"\"Gradient for Prod.\"\"\"\n    # The gradient can be expressed by dividing the product by each entry of the\n    # input tensor, but this approach can't deal with zeros in the input.\n    # Here, we avoid this problem by composing the output as a product of two\n    # cumprod operations.\n\n    input_shape = array_ops.shape(op.inputs[0])\n    # Reshape reduction indices for the case where the parameter is a scalar\n    reduction_indices = array_ops.reshape(op.inputs[1], [-1])\n\n    # Expand grad to full input shape\n    output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\n    tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\n    grad = array_ops.reshape(grad, output_shape_kept_dims)\n    grad = array_ops.tile(grad, tile_scaling)\n\n    # Pack all reduced dimensions into a single one, so we can perform the\n    # cumprod ops. If the reduction dims list is empty, it defaults to float32,\n    # so we need to cast here.  We put all the shape-related ops on CPU to avoid\n    # copying back and forth, and since listdiff is CPU only.\n    with ops.device(\"/cpu:0\"):\n        rank = array_ops.rank(op.inputs[0])\n        reduction_indices = (reduction_indices + rank) % rank\n        reduced = math_ops.cast(reduction_indices, dtypes.int32)\n        idx = math_ops.range(0, rank)\n        other, _ = array_ops.setdiff1d(idx, reduced)\n        perm = array_ops.concat([reduced, other], 0)\n        reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\n        other_num = math_ops.reduce_prod(array_ops.gather(input_shape, other))\n    permuted = array_ops.transpose(op.inputs[0], perm)\n    permuted_shape = array_ops.shape(permuted)\n    reshaped = array_ops.reshape(permuted, (reduced_num, other_num))\n\n    # Calculate product, leaving out the current entry\n    left = math_ops.cumprod(reshaped, axis=0, exclusive=True)\n    right = math_ops.cumprod(reshaped, axis=0, exclusive=True, reverse=True)\n    y = array_ops.reshape(tf.conj(left) * tf.conj(right), permuted_shape)\n\n    # Invert the transpose and reshape operations.\n    # Make sure to set the statically known shape information through a reshape.\n    out = grad * array_ops.transpose(y, array_ops.invert_permutation(perm))\n    return array_ops.reshape(out, input_shape), None\nWith TF gradient:\nwith tf.Graph().as_default() as g:\n    x = tf.Variable(1.0)\n    E = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))\n    with tf.Session() as sess:\n        sess.run(tf.variables_initializer([x]))\n        print(sess.run(tf.gradients(E,x)))\n\n>>> [10.0]\nWith modified gradient:\nwith tf.Graph().as_default() as g:\n    with g.gradient_override_map({\"Prod\": \"ModifiedProdGrad\"}):\n        x = tf.Variable(1.0)\n        E = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))\n        with tf.Session() as sess:\n            sess.run(tf.variables_initializer([x]))\n            print(sess.run(tf.gradients(E,x)))\n\n>>> [-6.0]", "body": "@rmlarsen Any timeline on when this might be looked at?\r\n@brianwa84 - you might be interested in this \r\n\r\nFor others that run into this issue, you can use `gradient_override_map`. Ex **(please note that this has not been unit tested)**:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops.math_grad import _safe_shape_div\r\n\r\n@tf.RegisterGradient(\"ModifiedProdGrad\")\r\ndef _ModifiedProdGrad(op, grad):\r\n    \"\"\"Gradient for Prod.\"\"\"\r\n    # The gradient can be expressed by dividing the product by each entry of the\r\n    # input tensor, but this approach can't deal with zeros in the input.\r\n    # Here, we avoid this problem by composing the output as a product of two\r\n    # cumprod operations.\r\n\r\n    input_shape = array_ops.shape(op.inputs[0])\r\n    # Reshape reduction indices for the case where the parameter is a scalar\r\n    reduction_indices = array_ops.reshape(op.inputs[1], [-1])\r\n\r\n    # Expand grad to full input shape\r\n    output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\r\n    tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\r\n    grad = array_ops.reshape(grad, output_shape_kept_dims)\r\n    grad = array_ops.tile(grad, tile_scaling)\r\n\r\n    # Pack all reduced dimensions into a single one, so we can perform the\r\n    # cumprod ops. If the reduction dims list is empty, it defaults to float32,\r\n    # so we need to cast here.  We put all the shape-related ops on CPU to avoid\r\n    # copying back and forth, and since listdiff is CPU only.\r\n    with ops.device(\"/cpu:0\"):\r\n        rank = array_ops.rank(op.inputs[0])\r\n        reduction_indices = (reduction_indices + rank) % rank\r\n        reduced = math_ops.cast(reduction_indices, dtypes.int32)\r\n        idx = math_ops.range(0, rank)\r\n        other, _ = array_ops.setdiff1d(idx, reduced)\r\n        perm = array_ops.concat([reduced, other], 0)\r\n        reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\r\n        other_num = math_ops.reduce_prod(array_ops.gather(input_shape, other))\r\n    permuted = array_ops.transpose(op.inputs[0], perm)\r\n    permuted_shape = array_ops.shape(permuted)\r\n    reshaped = array_ops.reshape(permuted, (reduced_num, other_num))\r\n\r\n    # Calculate product, leaving out the current entry\r\n    left = math_ops.cumprod(reshaped, axis=0, exclusive=True)\r\n    right = math_ops.cumprod(reshaped, axis=0, exclusive=True, reverse=True)\r\n    y = array_ops.reshape(tf.conj(left) * tf.conj(right), permuted_shape)\r\n\r\n    # Invert the transpose and reshape operations.\r\n    # Make sure to set the statically known shape information through a reshape.\r\n    out = grad * array_ops.transpose(y, array_ops.invert_permutation(perm))\r\n    return array_ops.reshape(out, input_shape), None\r\n```\r\n\r\nWith TF gradient:\r\n```python\r\nwith tf.Graph().as_default() as g:\r\n    x = tf.Variable(1.0)\r\n    E = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))\r\n    with tf.Session() as sess:\r\n        sess.run(tf.variables_initializer([x]))\r\n        print(sess.run(tf.gradients(E,x)))\r\n\r\n>>> [10.0]\r\n```\r\n\r\nWith modified gradient:\r\n```python\r\nwith tf.Graph().as_default() as g:\r\n    with g.gradient_override_map({\"Prod\": \"ModifiedProdGrad\"}):\r\n        x = tf.Variable(1.0)\r\n        E = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))\r\n        with tf.Session() as sess:\r\n            sess.run(tf.variables_initializer([x]))\r\n            print(sess.run(tf.gradients(E,x)))\r\n\r\n>>> [-6.0]\r\n```\r\n"}