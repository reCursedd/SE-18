{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331726533", "html_url": "https://github.com/tensorflow/tensorflow/issues/8770#issuecomment-331726533", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8770", "id": 331726533, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTcyNjUzMw==", "user": {"login": "yongtang", "id": 6932348, "node_id": "MDQ6VXNlcjY5MzIzNDg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6932348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongtang", "html_url": "https://github.com/yongtang", "followers_url": "https://api.github.com/users/yongtang/followers", "following_url": "https://api.github.com/users/yongtang/following{/other_user}", "gists_url": "https://api.github.com/users/yongtang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongtang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongtang/subscriptions", "organizations_url": "https://api.github.com/users/yongtang/orgs", "repos_url": "https://api.github.com/users/yongtang/repos", "events_url": "https://api.github.com/users/yongtang/events{/privacy}", "received_events_url": "https://api.github.com/users/yongtang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-24T17:45:59Z", "updated_at": "2017-09-24T17:45:59Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=14728438\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/NickPoon\">@NickPoon</a> I think you may invoke the reshape() from within <code>def _sampled_loss(labels, logits)</code>?</p>\n<p>See the code below. I think that will resolve the issue without modifying the existing code, and not causing other incompatibilities with different loss functions.</p>\n<pre><code>def _sampled_loss(labels, logits):\n    labels = tf.cast(labels, tf.int64)\n    labels = tf.reshape(labels, [-1, 1])\n    logits = tf.cast(logits, tf.float32)\n    return tf.cast(\n                    tf.nn.sampled_softmax_loss(\n                        proj_w,\n                        proj_b,\n                        labels,\n                        logits,\n                        num_sampled=20,\n                        num_classes=vocab_size),\n                    tf.float32)\n\nsoftmax_loss_f = _sampled_loss\n\n</code></pre>", "body_text": "@NickPoon I think you may invoke the reshape() from within def _sampled_loss(labels, logits)?\nSee the code below. I think that will resolve the issue without modifying the existing code, and not causing other incompatibilities with different loss functions.\ndef _sampled_loss(labels, logits):\n    labels = tf.cast(labels, tf.int64)\n    labels = tf.reshape(labels, [-1, 1])\n    logits = tf.cast(logits, tf.float32)\n    return tf.cast(\n                    tf.nn.sampled_softmax_loss(\n                        proj_w,\n                        proj_b,\n                        labels,\n                        logits,\n                        num_sampled=20,\n                        num_classes=vocab_size),\n                    tf.float32)\n\nsoftmax_loss_f = _sampled_loss", "body": "@NickPoon I think you may invoke the reshape() from within `def _sampled_loss(labels, logits)`?\r\n\r\nSee the code below. I think that will resolve the issue without modifying the existing code, and not causing other incompatibilities with different loss functions.\r\n\r\n```\r\ndef _sampled_loss(labels, logits):\r\n    labels = tf.cast(labels, tf.int64)\r\n    labels = tf.reshape(labels, [-1, 1])\r\n    logits = tf.cast(logits, tf.float32)\r\n    return tf.cast(\r\n                    tf.nn.sampled_softmax_loss(\r\n                        proj_w,\r\n                        proj_b,\r\n                        labels,\r\n                        logits,\r\n                        num_sampled=20,\r\n                        num_classes=vocab_size),\r\n                    tf.float32)\r\n\r\nsoftmax_loss_f = _sampled_loss\r\n\r\n```"}