{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16917", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16917/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16917/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16917/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16917", "id": 296129266, "node_id": "MDU6SXNzdWUyOTYxMjkyNjY=", "number": 16917, "title": "ValueError: No attr named '_XlaCompile' in name: \"while/attention/cond/fw/CudnnRNN/Enter\" and AttributeError: 'NoneType' object has no attribute 'back_prop'", "user": {"login": "burglarhobbit", "id": 15987266, "node_id": "MDQ6VXNlcjE1OTg3MjY2", "avatar_url": "https://avatars2.githubusercontent.com/u/15987266?v=4", "gravatar_id": "", "url": "https://api.github.com/users/burglarhobbit", "html_url": "https://github.com/burglarhobbit", "followers_url": "https://api.github.com/users/burglarhobbit/followers", "following_url": "https://api.github.com/users/burglarhobbit/following{/other_user}", "gists_url": "https://api.github.com/users/burglarhobbit/gists{/gist_id}", "starred_url": "https://api.github.com/users/burglarhobbit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/burglarhobbit/subscriptions", "organizations_url": "https://api.github.com/users/burglarhobbit/orgs", "repos_url": "https://api.github.com/users/burglarhobbit/repos", "events_url": "https://api.github.com/users/burglarhobbit/events{/privacy}", "received_events_url": "https://api.github.com/users/burglarhobbit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-02-10T20:04:00Z", "updated_at": "2018-02-23T09:27:26Z", "closed_at": "2018-02-23T09:27:26Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from</strong>: binary</li>\n<li><strong>TensorFlow version</strong>: 1.4.1</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version:</strong> Not compiled from source</li>\n<li><strong>GCC/Compiler version</strong>: Not compiled from source</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080 (8GB x 4)</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<p>I have a similar issue faced in <a href=\"https://github.com/tensorflow/tensorflow/issues/12420\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/12420/hovercard\">this thread</a> since I've started using tf.while_loop and the error is causing on <code>grads = self.opt.compute_gradients(self.loss)</code></p>\n<p>I'm initializing a class for gru layers outside the <code>tf.while_loop</code> since I can't initialize variables within the <code>tf.while_loop</code> without using <code>tf.get_variable</code> and then use the <code>__call__</code> of the class variable multiple times to use it within the <code>tf.while_loop</code> fn body. The sample codes are provided as per below:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\",\n line 348, in _MaybeCompile\n    xla_compile = op.get_attr(\"_XlaCompile\")\n  File \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line\n 2003, in get_attr\n    raise ValueError(\"No attr named '\" + name + \"' in \" + str(self._node_def))\nValueError: No attr named '_XlaCompile' in name: \"while/attention/cond/fw/CudnnRNN/Enter\"\nop: \"Enter\"\ninput: \"Variable_14/read\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"frame_name\"\n  value {\n    s: \"while/while_context\"\n  }\n}\nattr {\n  key: \"is_constant\"\n  value {\n    b: true\n  }\n}\nattr {\n  key: \"parallel_iterations\"\n  value {\n    i: 10\n  }\n}\n\n# also, error in the same line while handling the above error:\n\nAttributeError: 'NoneType' object has no attribute 'back_prop'\n</code></pre>\n<p>The Cudnn_gru custom class:</p>\n<pre><code>class cudnn_gru:\n\tdef __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=None):\n\t\tself.num_layers = num_layers\n\t\tself.grus = []\n\t\tself.params = []\n\t\tself.inits = []\n\t\tself.dropout_mask = []\n\t\tfor layer in range(num_layers):\n\t\t\tinput_size_ = input_size if layer == 0 else 2 * num_units\n\t\t\tgru_fw = tf.contrib.cudnn_rnn.CudnnGRU(\n\t\t\t\tnum_layers=1, num_units=num_units, input_size=input_size_)\n\t\t\tgru_bw = tf.contrib.cudnn_rnn.CudnnGRU(\n\t\t\t\tnum_layers=1, num_units=num_units, input_size=input_size_)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;Error over the following 4 lines for initializing within the while_loop\n\t\t\tparam_fw = tf.Variable(tf.random_uniform(\n\t\t\t\t[gru_fw.params_size()], -0.1, 0.1), validate_shape=False)\n\t\t\tparam_bw = tf.Variable(tf.random_uniform(\n\t\t\t\t[gru_bw.params_size()], -0.1, 0.1), validate_shape=False)\n\t\t\tinit_fw = tf.Variable(tf.zeros([1, batch_size, num_units]))\n\t\t\tinit_bw = tf.Variable(tf.zeros([1, batch_size, num_units]))\n\t\t\tmask_fw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n\t\t\t\t\t\t\t  keep_prob=keep_prob, is_train=is_train, mode=None)\n\t\t\tmask_bw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n\t\t\t\t\t\t\t  keep_prob=keep_prob, is_train=is_train, mode=None)\n\t\t\tself.grus.append((gru_fw, gru_bw, ))\n\t\t\tself.params.append((param_fw, param_bw, ))\n\t\t\tself.inits.append((init_fw, init_bw, ))\n\t\t\tself.dropout_mask.append((mask_fw, mask_bw, ))\n\n\tdef __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n\t\toutputs = [tf.transpose(inputs, [1, 0, 2])]\n\t\tfor layer in range(self.num_layers):\n\t\t\tgru_fw, gru_bw = self.grus[layer]\n\t\t\tparam_fw, param_bw = self.params[layer]\n\t\t\tinit_fw, init_bw = self.inits[layer]\n\t\t\tmask_fw, mask_bw = self.dropout_mask[layer]\n\t\t\twith tf.variable_scope(\"fw\"):\n\t\t\t\tout_fw, _ = gru_fw(outputs[-1] * mask_fw, init_fw, param_fw)\n\t\t\twith tf.variable_scope(\"bw\"):\n\t\t\t\tinputs_bw = tf.reverse_sequence(\n\t\t\t\t\toutputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n\t\t\t\tout_bw, _ = gru_bw(inputs_bw, init_bw, param_bw)\n\t\t\t\tout_bw = tf.reverse_sequence(\n\t\t\t\t\tout_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n\t\t\toutputs.append(tf.concat([out_fw, out_bw], axis=2))\n\t\tif concat_layers:\n\t\t\tres = tf.concat(outputs[1:], axis=2)\n\t\telse:\n\t\t\tres = outputs[-1]\n\t\tres = tf.transpose(res, [1, 0, 2])\n\t\treturn res\n</code></pre>\n<p>and the model:</p>\n<pre><code>class Model(object):\n        def __init__(...):\n            ....\n            ....\n            self.ready()\n            if trainable:\n\t\t\tself.lr = tf.get_variable(\n\t\t\t\"lr\", shape=[], dtype=tf.float32, trainable=False)\n\t\t\tself.opt = tf.train.AdadeltaOptimizer(\n\t\t\t\tlearning_rate=self.lr, epsilon=1e-6)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;Compile time ERROR over this line:\n                \tgrads = self.opt.compute_gradients(self.loss)\n\t\t\tgradients, variables = zip(*grads)\n\t\t\tcapped_grads, _ = tf.clip_by_global_norm(gradients, config.grad_clip)\n\t\t\tself.train_op = self.opt.apply_gradients(zip(capped_grads, variables), global_step=self.global_step)\n\n\tdef get_vP(self,i):\n\t\t....\n\t        ....\n\t\twith tf.variable_scope(\"encoding\"):\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; def f1():\n                # used to initialize the cudnn_gru class over here,\n                # but shifted outside the tf.while_loop\n                # due to initializing errors in tf.Variable in cudnn_gru __init__\n                return self.rnn1(c_emb, seq_len=self.c_len)\n            def f2():\n                return self.rnn1(c_emb, seq_len=self.c_len)\n            c = tf.cond(tf.equal(i, zero), f1, f2)\n            q = self.rnn1(q_emb, seq_len=self.q_len)\n            self.q_enc = q\n        with tf.variable_scope(\"attention\"):\n                qc_att = dot_attention(c, q, mask=self.q_mask, hidden=d,\n                    keep_prob=config.keep_prob, is_train=self.is_train,\n\t\tname_scope=\"attention_layer\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; def f3():\n                # same situation as f1()\n                return self.rnn2(qc_att, seq_len=self.c_len)\n            def f4():\n\t\t\t\treturn self.rnn2(qc_att, seq_len=self.c_len)\n            att = tf.cond(tf.equal(self.i, zero), f3, f4)\n            def f5():\n                return att\n            def f6():\n                return tf.concat([att, att], axis=1)\n            self.att_vP = tf.cond(tf.equal(i, zero), f5, f6)\n\n            return tf.add(i,tf.constant(1, dtype=tf.int64))\n\t\n\tdef condition(self,i):\n\t\treturn tf.less(i, self.para_count[0])\n\t\n\tdef ready(self):\n\t\tconfig = self.config\n\t\tN, PL, QL, CL, d, dc, dg = config.batch_size, self.c_maxlen, self.q_maxlen, \\\n\t\t\tconfig.char_limit, config.hidden, config.char_dim, config.char_hidden\n\t\tgru = cudnn_gru if config.use_cudnn else native_gru\n\n\t\tself.cell_fw = tf.contrib.rnn.GRUCell(dg)\n\t\tself.cell_bw = tf.contrib.rnn.GRUCell(dg)\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;# initializing here instead of within tf.while_loop body\n\t\tself.rnn1 = gru(num_layers=3, num_units=150, batch_size=N, input_size=500,\\\n\t\t\tkeep_prob=config.keep_prob, is_train=self.is_train)\n\t\tself.rnn2 = gru(num_layers=1, num_units=d, batch_size=N, input_size=1800,\\\n\t\t\tkeep_prob=config.keep_prob, is_train=self.is_train)\n\t\t\n\t\tresult = tf.while_loop(self.condition, self.get_vP, loop_vars=[self.i])\n\n                ....\n                ....\n</code></pre>", "body_text": "System information\n\nHave I written custom code: Yes\nOS Platform and Distribution: Ubuntu 16.04\nTensorFlow installed from: binary\nTensorFlow version: 1.4.1\nPython version: 3.5.2\nBazel version: Not compiled from source\nGCC/Compiler version: Not compiled from source\nCUDA/cuDNN version: 8.0\nGPU model and memory: GeForce GTX 1080 (8GB x 4)\nExact command to reproduce: N/A\n\nI have a similar issue faced in this thread since I've started using tf.while_loop and the error is causing on grads = self.opt.compute_gradients(self.loss)\nI'm initializing a class for gru layers outside the tf.while_loop since I can't initialize variables within the tf.while_loop without using tf.get_variable and then use the __call__ of the class variable multiple times to use it within the tf.while_loop fn body. The sample codes are provided as per below:\nTraceback (most recent call last):\n  File \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\",\n line 348, in _MaybeCompile\n    xla_compile = op.get_attr(\"_XlaCompile\")\n  File \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line\n 2003, in get_attr\n    raise ValueError(\"No attr named '\" + name + \"' in \" + str(self._node_def))\nValueError: No attr named '_XlaCompile' in name: \"while/attention/cond/fw/CudnnRNN/Enter\"\nop: \"Enter\"\ninput: \"Variable_14/read\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"frame_name\"\n  value {\n    s: \"while/while_context\"\n  }\n}\nattr {\n  key: \"is_constant\"\n  value {\n    b: true\n  }\n}\nattr {\n  key: \"parallel_iterations\"\n  value {\n    i: 10\n  }\n}\n\n# also, error in the same line while handling the above error:\n\nAttributeError: 'NoneType' object has no attribute 'back_prop'\n\nThe Cudnn_gru custom class:\nclass cudnn_gru:\n\tdef __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=None):\n\t\tself.num_layers = num_layers\n\t\tself.grus = []\n\t\tself.params = []\n\t\tself.inits = []\n\t\tself.dropout_mask = []\n\t\tfor layer in range(num_layers):\n\t\t\tinput_size_ = input_size if layer == 0 else 2 * num_units\n\t\t\tgru_fw = tf.contrib.cudnn_rnn.CudnnGRU(\n\t\t\t\tnum_layers=1, num_units=num_units, input_size=input_size_)\n\t\t\tgru_bw = tf.contrib.cudnn_rnn.CudnnGRU(\n\t\t\t\tnum_layers=1, num_units=num_units, input_size=input_size_)\n>>>>>>>>>>>Error over the following 4 lines for initializing within the while_loop\n\t\t\tparam_fw = tf.Variable(tf.random_uniform(\n\t\t\t\t[gru_fw.params_size()], -0.1, 0.1), validate_shape=False)\n\t\t\tparam_bw = tf.Variable(tf.random_uniform(\n\t\t\t\t[gru_bw.params_size()], -0.1, 0.1), validate_shape=False)\n\t\t\tinit_fw = tf.Variable(tf.zeros([1, batch_size, num_units]))\n\t\t\tinit_bw = tf.Variable(tf.zeros([1, batch_size, num_units]))\n\t\t\tmask_fw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n\t\t\t\t\t\t\t  keep_prob=keep_prob, is_train=is_train, mode=None)\n\t\t\tmask_bw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n\t\t\t\t\t\t\t  keep_prob=keep_prob, is_train=is_train, mode=None)\n\t\t\tself.grus.append((gru_fw, gru_bw, ))\n\t\t\tself.params.append((param_fw, param_bw, ))\n\t\t\tself.inits.append((init_fw, init_bw, ))\n\t\t\tself.dropout_mask.append((mask_fw, mask_bw, ))\n\n\tdef __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n\t\toutputs = [tf.transpose(inputs, [1, 0, 2])]\n\t\tfor layer in range(self.num_layers):\n\t\t\tgru_fw, gru_bw = self.grus[layer]\n\t\t\tparam_fw, param_bw = self.params[layer]\n\t\t\tinit_fw, init_bw = self.inits[layer]\n\t\t\tmask_fw, mask_bw = self.dropout_mask[layer]\n\t\t\twith tf.variable_scope(\"fw\"):\n\t\t\t\tout_fw, _ = gru_fw(outputs[-1] * mask_fw, init_fw, param_fw)\n\t\t\twith tf.variable_scope(\"bw\"):\n\t\t\t\tinputs_bw = tf.reverse_sequence(\n\t\t\t\t\toutputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n\t\t\t\tout_bw, _ = gru_bw(inputs_bw, init_bw, param_bw)\n\t\t\t\tout_bw = tf.reverse_sequence(\n\t\t\t\t\tout_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n\t\t\toutputs.append(tf.concat([out_fw, out_bw], axis=2))\n\t\tif concat_layers:\n\t\t\tres = tf.concat(outputs[1:], axis=2)\n\t\telse:\n\t\t\tres = outputs[-1]\n\t\tres = tf.transpose(res, [1, 0, 2])\n\t\treturn res\n\nand the model:\nclass Model(object):\n        def __init__(...):\n            ....\n            ....\n            self.ready()\n            if trainable:\n\t\t\tself.lr = tf.get_variable(\n\t\t\t\"lr\", shape=[], dtype=tf.float32, trainable=False)\n\t\t\tself.opt = tf.train.AdadeltaOptimizer(\n\t\t\t\tlearning_rate=self.lr, epsilon=1e-6)\n>>>>>>>>>>>Compile time ERROR over this line:\n                \tgrads = self.opt.compute_gradients(self.loss)\n\t\t\tgradients, variables = zip(*grads)\n\t\t\tcapped_grads, _ = tf.clip_by_global_norm(gradients, config.grad_clip)\n\t\t\tself.train_op = self.opt.apply_gradients(zip(capped_grads, variables), global_step=self.global_step)\n\n\tdef get_vP(self,i):\n\t\t....\n\t        ....\n\t\twith tf.variable_scope(\"encoding\"):\n>>>>>>>>>>> def f1():\n                # used to initialize the cudnn_gru class over here,\n                # but shifted outside the tf.while_loop\n                # due to initializing errors in tf.Variable in cudnn_gru __init__\n                return self.rnn1(c_emb, seq_len=self.c_len)\n            def f2():\n                return self.rnn1(c_emb, seq_len=self.c_len)\n            c = tf.cond(tf.equal(i, zero), f1, f2)\n            q = self.rnn1(q_emb, seq_len=self.q_len)\n            self.q_enc = q\n        with tf.variable_scope(\"attention\"):\n                qc_att = dot_attention(c, q, mask=self.q_mask, hidden=d,\n                    keep_prob=config.keep_prob, is_train=self.is_train,\n\t\tname_scope=\"attention_layer\")\n>>>>>>>>>>> def f3():\n                # same situation as f1()\n                return self.rnn2(qc_att, seq_len=self.c_len)\n            def f4():\n\t\t\t\treturn self.rnn2(qc_att, seq_len=self.c_len)\n            att = tf.cond(tf.equal(self.i, zero), f3, f4)\n            def f5():\n                return att\n            def f6():\n                return tf.concat([att, att], axis=1)\n            self.att_vP = tf.cond(tf.equal(i, zero), f5, f6)\n\n            return tf.add(i,tf.constant(1, dtype=tf.int64))\n\t\n\tdef condition(self,i):\n\t\treturn tf.less(i, self.para_count[0])\n\t\n\tdef ready(self):\n\t\tconfig = self.config\n\t\tN, PL, QL, CL, d, dc, dg = config.batch_size, self.c_maxlen, self.q_maxlen, \\\n\t\t\tconfig.char_limit, config.hidden, config.char_dim, config.char_hidden\n\t\tgru = cudnn_gru if config.use_cudnn else native_gru\n\n\t\tself.cell_fw = tf.contrib.rnn.GRUCell(dg)\n\t\tself.cell_bw = tf.contrib.rnn.GRUCell(dg)\n\n>>>>>>>># initializing here instead of within tf.while_loop body\n\t\tself.rnn1 = gru(num_layers=3, num_units=150, batch_size=N, input_size=500,\\\n\t\t\tkeep_prob=config.keep_prob, is_train=self.is_train)\n\t\tself.rnn2 = gru(num_layers=1, num_units=d, batch_size=N, input_size=1800,\\\n\t\t\tkeep_prob=config.keep_prob, is_train=self.is_train)\n\t\t\n\t\tresult = tf.while_loop(self.condition, self.get_vP, loop_vars=[self.i])\n\n                ....\n                ....", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.4.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version:** Not compiled from source\r\n- **GCC/Compiler version**: Not compiled from source\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GeForce GTX 1080 (8GB x 4)\r\n- **Exact command to reproduce**: N/A\r\n\r\nI have a similar issue faced in [this thread](https://github.com/tensorflow/tensorflow/issues/12420) since I've started using tf.while_loop and the error is causing on `grads = self.opt.compute_gradients(self.loss)`\r\n\r\nI'm initializing a class for gru layers outside the `tf.while_loop` since I can't initialize variables within the `tf.while_loop` without using `tf.get_variable` and then use the `__call__` of the class variable multiple times to use it within the `tf.while_loop` fn body. The sample codes are provided as per below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\",\r\n line 348, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line\r\n 2003, in get_attr\r\n    raise ValueError(\"No attr named '\" + name + \"' in \" + str(self._node_def))\r\nValueError: No attr named '_XlaCompile' in name: \"while/attention/cond/fw/CudnnRNN/Enter\"\r\nop: \"Enter\"\r\ninput: \"Variable_14/read\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"frame_name\"\r\n  value {\r\n    s: \"while/while_context\"\r\n  }\r\n}\r\nattr {\r\n  key: \"is_constant\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nattr {\r\n  key: \"parallel_iterations\"\r\n  value {\r\n    i: 10\r\n  }\r\n}\r\n\r\n# also, error in the same line while handling the above error:\r\n\r\nAttributeError: 'NoneType' object has no attribute 'back_prop'\r\n```\r\n\r\nThe Cudnn_gru custom class:\r\n```\r\nclass cudnn_gru:\r\n\tdef __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=None):\r\n\t\tself.num_layers = num_layers\r\n\t\tself.grus = []\r\n\t\tself.params = []\r\n\t\tself.inits = []\r\n\t\tself.dropout_mask = []\r\n\t\tfor layer in range(num_layers):\r\n\t\t\tinput_size_ = input_size if layer == 0 else 2 * num_units\r\n\t\t\tgru_fw = tf.contrib.cudnn_rnn.CudnnGRU(\r\n\t\t\t\tnum_layers=1, num_units=num_units, input_size=input_size_)\r\n\t\t\tgru_bw = tf.contrib.cudnn_rnn.CudnnGRU(\r\n\t\t\t\tnum_layers=1, num_units=num_units, input_size=input_size_)\r\n>>>>>>>>>>>Error over the following 4 lines for initializing within the while_loop\r\n\t\t\tparam_fw = tf.Variable(tf.random_uniform(\r\n\t\t\t\t[gru_fw.params_size()], -0.1, 0.1), validate_shape=False)\r\n\t\t\tparam_bw = tf.Variable(tf.random_uniform(\r\n\t\t\t\t[gru_bw.params_size()], -0.1, 0.1), validate_shape=False)\r\n\t\t\tinit_fw = tf.Variable(tf.zeros([1, batch_size, num_units]))\r\n\t\t\tinit_bw = tf.Variable(tf.zeros([1, batch_size, num_units]))\r\n\t\t\tmask_fw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\r\n\t\t\t\t\t\t\t  keep_prob=keep_prob, is_train=is_train, mode=None)\r\n\t\t\tmask_bw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\r\n\t\t\t\t\t\t\t  keep_prob=keep_prob, is_train=is_train, mode=None)\r\n\t\t\tself.grus.append((gru_fw, gru_bw, ))\r\n\t\t\tself.params.append((param_fw, param_bw, ))\r\n\t\t\tself.inits.append((init_fw, init_bw, ))\r\n\t\t\tself.dropout_mask.append((mask_fw, mask_bw, ))\r\n\r\n\tdef __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\r\n\t\toutputs = [tf.transpose(inputs, [1, 0, 2])]\r\n\t\tfor layer in range(self.num_layers):\r\n\t\t\tgru_fw, gru_bw = self.grus[layer]\r\n\t\t\tparam_fw, param_bw = self.params[layer]\r\n\t\t\tinit_fw, init_bw = self.inits[layer]\r\n\t\t\tmask_fw, mask_bw = self.dropout_mask[layer]\r\n\t\t\twith tf.variable_scope(\"fw\"):\r\n\t\t\t\tout_fw, _ = gru_fw(outputs[-1] * mask_fw, init_fw, param_fw)\r\n\t\t\twith tf.variable_scope(\"bw\"):\r\n\t\t\t\tinputs_bw = tf.reverse_sequence(\r\n\t\t\t\t\toutputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\r\n\t\t\t\tout_bw, _ = gru_bw(inputs_bw, init_bw, param_bw)\r\n\t\t\t\tout_bw = tf.reverse_sequence(\r\n\t\t\t\t\tout_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\r\n\t\t\toutputs.append(tf.concat([out_fw, out_bw], axis=2))\r\n\t\tif concat_layers:\r\n\t\t\tres = tf.concat(outputs[1:], axis=2)\r\n\t\telse:\r\n\t\t\tres = outputs[-1]\r\n\t\tres = tf.transpose(res, [1, 0, 2])\r\n\t\treturn res\r\n```\r\nand the model:\r\n```\r\nclass Model(object):\r\n        def __init__(...):\r\n            ....\r\n            ....\r\n            self.ready()\r\n            if trainable:\r\n\t\t\tself.lr = tf.get_variable(\r\n\t\t\t\"lr\", shape=[], dtype=tf.float32, trainable=False)\r\n\t\t\tself.opt = tf.train.AdadeltaOptimizer(\r\n\t\t\t\tlearning_rate=self.lr, epsilon=1e-6)\r\n>>>>>>>>>>>Compile time ERROR over this line:\r\n                \tgrads = self.opt.compute_gradients(self.loss)\r\n\t\t\tgradients, variables = zip(*grads)\r\n\t\t\tcapped_grads, _ = tf.clip_by_global_norm(gradients, config.grad_clip)\r\n\t\t\tself.train_op = self.opt.apply_gradients(zip(capped_grads, variables), global_step=self.global_step)\r\n\r\n\tdef get_vP(self,i):\r\n\t\t....\r\n\t        ....\r\n\t\twith tf.variable_scope(\"encoding\"):\r\n>>>>>>>>>>> def f1():\r\n                # used to initialize the cudnn_gru class over here,\r\n                # but shifted outside the tf.while_loop\r\n                # due to initializing errors in tf.Variable in cudnn_gru __init__\r\n                return self.rnn1(c_emb, seq_len=self.c_len)\r\n            def f2():\r\n                return self.rnn1(c_emb, seq_len=self.c_len)\r\n            c = tf.cond(tf.equal(i, zero), f1, f2)\r\n            q = self.rnn1(q_emb, seq_len=self.q_len)\r\n            self.q_enc = q\r\n        with tf.variable_scope(\"attention\"):\r\n                qc_att = dot_attention(c, q, mask=self.q_mask, hidden=d,\r\n                    keep_prob=config.keep_prob, is_train=self.is_train,\r\n\t\tname_scope=\"attention_layer\")\r\n>>>>>>>>>>> def f3():\r\n                # same situation as f1()\r\n                return self.rnn2(qc_att, seq_len=self.c_len)\r\n            def f4():\r\n\t\t\t\treturn self.rnn2(qc_att, seq_len=self.c_len)\r\n            att = tf.cond(tf.equal(self.i, zero), f3, f4)\r\n            def f5():\r\n                return att\r\n            def f6():\r\n                return tf.concat([att, att], axis=1)\r\n            self.att_vP = tf.cond(tf.equal(i, zero), f5, f6)\r\n\r\n            return tf.add(i,tf.constant(1, dtype=tf.int64))\r\n\t\r\n\tdef condition(self,i):\r\n\t\treturn tf.less(i, self.para_count[0])\r\n\t\r\n\tdef ready(self):\r\n\t\tconfig = self.config\r\n\t\tN, PL, QL, CL, d, dc, dg = config.batch_size, self.c_maxlen, self.q_maxlen, \\\r\n\t\t\tconfig.char_limit, config.hidden, config.char_dim, config.char_hidden\r\n\t\tgru = cudnn_gru if config.use_cudnn else native_gru\r\n\r\n\t\tself.cell_fw = tf.contrib.rnn.GRUCell(dg)\r\n\t\tself.cell_bw = tf.contrib.rnn.GRUCell(dg)\r\n\r\n>>>>>>>># initializing here instead of within tf.while_loop body\r\n\t\tself.rnn1 = gru(num_layers=3, num_units=150, batch_size=N, input_size=500,\\\r\n\t\t\tkeep_prob=config.keep_prob, is_train=self.is_train)\r\n\t\tself.rnn2 = gru(num_layers=1, num_units=d, batch_size=N, input_size=1800,\\\r\n\t\t\tkeep_prob=config.keep_prob, is_train=self.is_train)\r\n\t\t\r\n\t\tresult = tf.while_loop(self.condition, self.get_vP, loop_vars=[self.i])\r\n\r\n                ....\r\n                ...."}