{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293915757", "html_url": "https://github.com/tensorflow/tensorflow/issues/5221#issuecomment-293915757", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5221", "id": 293915757, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzkxNTc1Nw==", "user": {"login": "MInner", "id": 5229267, "node_id": "MDQ6VXNlcjUyMjkyNjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5229267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MInner", "html_url": "https://github.com/MInner", "followers_url": "https://api.github.com/users/MInner/followers", "following_url": "https://api.github.com/users/MInner/following{/other_user}", "gists_url": "https://api.github.com/users/MInner/gists{/gist_id}", "starred_url": "https://api.github.com/users/MInner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MInner/subscriptions", "organizations_url": "https://api.github.com/users/MInner/orgs", "repos_url": "https://api.github.com/users/MInner/repos", "events_url": "https://api.github.com/users/MInner/events{/privacy}", "received_events_url": "https://api.github.com/users/MInner/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-13T14:43:08Z", "updated_at": "2017-04-13T14:45:17Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a> yes, and other random errors like shape mismatch in the middle of training; people in <code>seq2seq</code> issue tracker (referenced above) suggest that this might be related to race condition during concurrent training and evaluation in <code>contrib.learn.experiment.train_and_evaluate</code>. I also sometimes observe weird errors (not related to memory allocation) if I try running two processes concurrently even on different GPUs, and often if I run two processes on same GPU.</p>", "body_text": "@drpngx yes, and other random errors like shape mismatch in the middle of training; people in seq2seq issue tracker (referenced above) suggest that this might be related to race condition during concurrent training and evaluation in contrib.learn.experiment.train_and_evaluate. I also sometimes observe weird errors (not related to memory allocation) if I try running two processes concurrently even on different GPUs, and often if I run two processes on same GPU.", "body": "@drpngx yes, and other random errors like shape mismatch in the middle of training; people in `seq2seq` issue tracker (referenced above) suggest that this might be related to race condition during concurrent training and evaluation in `contrib.learn.experiment.train_and_evaluate`. I also sometimes observe weird errors (not related to memory allocation) if I try running two processes concurrently even on different GPUs, and often if I run two processes on same GPU."}