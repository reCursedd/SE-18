{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310769359", "html_url": "https://github.com/tensorflow/tensorflow/issues/11013#issuecomment-310769359", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11013", "id": 310769359, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDc2OTM1OQ==", "user": {"login": "dagarcia-nvidia", "id": 29103856, "node_id": "MDQ6VXNlcjI5MTAzODU2", "avatar_url": "https://avatars3.githubusercontent.com/u/29103856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dagarcia-nvidia", "html_url": "https://github.com/dagarcia-nvidia", "followers_url": "https://api.github.com/users/dagarcia-nvidia/followers", "following_url": "https://api.github.com/users/dagarcia-nvidia/following{/other_user}", "gists_url": "https://api.github.com/users/dagarcia-nvidia/gists{/gist_id}", "starred_url": "https://api.github.com/users/dagarcia-nvidia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dagarcia-nvidia/subscriptions", "organizations_url": "https://api.github.com/users/dagarcia-nvidia/orgs", "repos_url": "https://api.github.com/users/dagarcia-nvidia/repos", "events_url": "https://api.github.com/users/dagarcia-nvidia/events{/privacy}", "received_events_url": "https://api.github.com/users/dagarcia-nvidia/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-23T20:48:08Z", "updated_at": "2017-06-23T20:48:08Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1184671\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xiejw\">@xiejw</a>,</p>\n<p>Setting shuffle=true in a tf.RandomShuffleQueue would hide the problem a bit, but not eliminate it, because the \"random\" part of a random queue occurs <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_shuffle_queue_op.cc#L116\">when dequeuing</a>. Since queues have a limited size, and the state is reset on every training lap, the model would still not train with the whole dataset.</p>\n<p>It seems like the way to fix this properly is by not regenerating the graph from scratch on every training / testing lap. The graphs should be generated only once and then reused.</p>", "body_text": "@xiejw,\nSetting shuffle=true in a tf.RandomShuffleQueue would hide the problem a bit, but not eliminate it, because the \"random\" part of a random queue occurs when dequeuing. Since queues have a limited size, and the state is reset on every training lap, the model would still not train with the whole dataset.\nIt seems like the way to fix this properly is by not regenerating the graph from scratch on every training / testing lap. The graphs should be generated only once and then reused.", "body": "@xiejw,\r\n\r\nSetting shuffle=true in a tf.RandomShuffleQueue would hide the problem a bit, but not eliminate it, because the \"random\" part of a random queue occurs [when dequeuing](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_shuffle_queue_op.cc#L116). Since queues have a limited size, and the state is reset on every training lap, the model would still not train with the whole dataset.\r\n\r\nIt seems like the way to fix this properly is by not regenerating the graph from scratch on every training / testing lap. The graphs should be generated only once and then reused."}