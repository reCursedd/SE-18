{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11013", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11013/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11013/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11013/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11013", "id": 238158296, "node_id": "MDU6SXNzdWUyMzgxNTgyOTY=", "number": 11013, "title": "Bug: experiment.py continuous_train_and_eval() leads to overfitting", "user": {"login": "dagarcia-nvidia", "id": 29103856, "node_id": "MDQ6VXNlcjI5MTAzODU2", "avatar_url": "https://avatars3.githubusercontent.com/u/29103856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dagarcia-nvidia", "html_url": "https://github.com/dagarcia-nvidia", "followers_url": "https://api.github.com/users/dagarcia-nvidia/followers", "following_url": "https://api.github.com/users/dagarcia-nvidia/following{/other_user}", "gists_url": "https://api.github.com/users/dagarcia-nvidia/gists{/gist_id}", "starred_url": "https://api.github.com/users/dagarcia-nvidia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dagarcia-nvidia/subscriptions", "organizations_url": "https://api.github.com/users/dagarcia-nvidia/orgs", "repos_url": "https://api.github.com/users/dagarcia-nvidia/repos", "events_url": "https://api.github.com/users/dagarcia-nvidia/events{/privacy}", "received_events_url": "https://api.github.com/users/dagarcia-nvidia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-06-23T14:26:06Z", "updated_at": "2017-12-20T19:19:22Z", "closed_at": "2017-12-20T19:19:22Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: YES</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.0.1-4-gbd6743e-dirty 1.0.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Our team has found what we believe is a critical bug in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L506\">experiment.py continuous_train_and_eval()</a> that pretty much guarantees any model trained with this method will lead to overfitting.</p>\n<p>The problem is that every time it alternates from evaluating to training again, it calls <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L569\">self._call_train()</a>, which eventually makes it to <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L950\">estimator.py _train_model()</a>.</p>\n<p>This function first <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L952\">creates a brand new graph</a> then <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L956\">sets up the input pipelines</a>. This necessarily means that the input pipelines are reset from scratch on every training lap, and since a training lap is typically much shorter than an epoch, this means the model is only trained with a small portion of the whole dataset.</p>\n<p>Originally we raised this as an issue in <a href=\"https://github.com/google/seq2seq/issues/262\" data-hovercard-type=\"issue\" data-hovercard-url=\"/google/seq2seq/issues/262/hovercard\">Google's tf-seq2seq</a>, but as far as we can tell now the bug probably belongs into Tensorflow instead, as we can't see a simple way to modify the tf-seq2seq code to avoid this issue from arising.</p>\n<p>Thank you for looking into this matter.</p>\n<h3>Source code / logs</h3>\n<p>N/A</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): v1.0.1-4-gbd6743e-dirty 1.0.1\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nOur team has found what we believe is a critical bug in experiment.py continuous_train_and_eval() that pretty much guarantees any model trained with this method will lead to overfitting.\nThe problem is that every time it alternates from evaluating to training again, it calls self._call_train(), which eventually makes it to estimator.py _train_model().\nThis function first creates a brand new graph then sets up the input pipelines. This necessarily means that the input pipelines are reset from scratch on every training lap, and since a training lap is typically much shorter than an epoch, this means the model is only trained with a small portion of the whole dataset.\nOriginally we raised this as an issue in Google's tf-seq2seq, but as far as we can tell now the bug probably belongs into Tensorflow instead, as we can't see a simple way to modify the tf-seq2seq code to avoid this issue from arising.\nThank you for looking into this matter.\nSource code / logs\nN/A", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.0.1-4-gbd6743e-dirty 1.0.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nOur team has found what we believe is a critical bug in [experiment.py continuous_train_and_eval()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L506) that pretty much guarantees any model trained with this method will lead to overfitting.\r\n\r\nThe problem is that every time it alternates from evaluating to training again, it calls [self._call_train()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L569), which eventually makes it to [estimator.py _train_model()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L950).\r\n\r\nThis function first [creates a brand new graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L952) then [sets up the input pipelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L956). This necessarily means that the input pipelines are reset from scratch on every training lap, and since a training lap is typically much shorter than an epoch, this means the model is only trained with a small portion of the whole dataset.\r\n\r\nOriginally we raised this as an issue in [Google's tf-seq2seq](https://github.com/google/seq2seq/issues/262), but as far as we can tell now the bug probably belongs into Tensorflow instead, as we can't see a simple way to modify the tf-seq2seq code to avoid this issue from arising.\r\n\r\nThank you for looking into this matter.\r\n\r\n### Source code / logs\r\nN/A"}