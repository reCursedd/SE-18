{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17949", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17949/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17949/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17949/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17949", "id": 308000576, "node_id": "MDU6SXNzdWUzMDgwMDA1NzY=", "number": 17949, "title": "incorrect description of num_sampled parameter in tf.nn.nce_loss() function: num_sampled is number of negative examples per 1 positive example (NOT per batch)", "user": {"login": "denkuzin", "id": 17611021, "node_id": "MDQ6VXNlcjE3NjExMDIx", "avatar_url": "https://avatars1.githubusercontent.com/u/17611021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/denkuzin", "html_url": "https://github.com/denkuzin", "followers_url": "https://api.github.com/users/denkuzin/followers", "following_url": "https://api.github.com/users/denkuzin/following{/other_user}", "gists_url": "https://api.github.com/users/denkuzin/gists{/gist_id}", "starred_url": "https://api.github.com/users/denkuzin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/denkuzin/subscriptions", "organizations_url": "https://api.github.com/users/denkuzin/orgs", "repos_url": "https://api.github.com/users/denkuzin/repos", "events_url": "https://api.github.com/users/denkuzin/events{/privacy}", "received_events_url": "https://api.github.com/users/denkuzin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "MarkDaoust", "id": 1414837, "node_id": "MDQ6VXNlcjE0MTQ4Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1414837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkDaoust", "html_url": "https://github.com/MarkDaoust", "followers_url": "https://api.github.com/users/MarkDaoust/followers", "following_url": "https://api.github.com/users/MarkDaoust/following{/other_user}", "gists_url": "https://api.github.com/users/MarkDaoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkDaoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkDaoust/subscriptions", "organizations_url": "https://api.github.com/users/MarkDaoust/orgs", "repos_url": "https://api.github.com/users/MarkDaoust/repos", "events_url": "https://api.github.com/users/MarkDaoust/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkDaoust/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "MarkDaoust", "id": 1414837, "node_id": "MDQ6VXNlcjE0MTQ4Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1414837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkDaoust", "html_url": "https://github.com/MarkDaoust", "followers_url": "https://api.github.com/users/MarkDaoust/followers", "following_url": "https://api.github.com/users/MarkDaoust/following{/other_user}", "gists_url": "https://api.github.com/users/MarkDaoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkDaoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkDaoust/subscriptions", "organizations_url": "https://api.github.com/users/MarkDaoust/orgs", "repos_url": "https://api.github.com/users/MarkDaoust/repos", "events_url": "https://api.github.com/users/MarkDaoust/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkDaoust/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-03-23T11:54:29Z", "updated_at": "2018-08-20T19:51:01Z", "closed_at": "2018-08-20T19:44:47Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>Looks like parameter <code>num_sampled</code> in  <code>tf.nn.nce_loss</code> function defines the number of negative examples per a positive example but not per a batch as described in tensorflow documentation (<a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss</a>)</p>\n<p>(see the next code)</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n# `_compute_sampled_logits` is invoked in nce_loss to generate negative sample and calculate logits\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\n\nembedding_size = 10\nwords_number = 300\nbatch_size = 3\nnum_sampled = 3\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Input data.\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n\n    with tf.device('/cpu:0'):\n        embeddings = tf.Variable(\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n        nce_weights = tf.Variable(\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\n        nce_biases = tf.Variable(tf.zeros([words_number]))\n        \n    logits, labels = _compute_sampled_logits(\n                       weights=nce_weights,\n                       biases=nce_biases,\n                       inputs=embed,\n                       labels=train_labels,\n                       num_true=1,\n                       num_sampled=num_sampled,\n                       num_classes=words_number,\n                       remove_accidental_hits = False)\n    init = tf.global_variables_initializer()\n\n\nsession = tf.InteractiveSession(graph=graph)\ninit.run(session=session)\n\nbatch_inputs = np.array([0,1,2], dtype=np.int32)\nbatch_labels = np.array([[3],[4],[5]], dtype=np.int32)\n\nfeed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\nlogits_val, labels_val = session.run([logits, labels], feed_dict=feed_dict)\n\nprint (\"logits_val = {}\".format(logits_val))\nprint (\"labels_val = {}\".format(labels_val))\n\n</code></pre>\n<p>As a result, <code>_compute_sampled_logits</code> function generated <code>num_sampled</code> examples per <strong>1 positive example</strong>:</p>\n<pre><code>logits_val = [[ -8.18727493   2.02518415  14.18676853   0.51900673]\n [ -8.97232056   5.60003376   4.52866602   3.68161726]\n [ -0.36226368  -5.84330416  -3.39891291   5.58423615]]\nlabels_val = [[ 1.  0.  0.  0.]\n [ 1.  0.  0.  0.]\n [ 1.  0.  0.  0.]]\n</code></pre>", "body_text": "Hi all,\nLooks like parameter num_sampled in  tf.nn.nce_loss function defines the number of negative examples per a positive example but not per a batch as described in tensorflow documentation (https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)\n(see the next code)\nimport tensorflow as tf\nimport numpy as np\n# `_compute_sampled_logits` is invoked in nce_loss to generate negative sample and calculate logits\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\n\nembedding_size = 10\nwords_number = 300\nbatch_size = 3\nnum_sampled = 3\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Input data.\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n\n    with tf.device('/cpu:0'):\n        embeddings = tf.Variable(\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n        nce_weights = tf.Variable(\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\n        nce_biases = tf.Variable(tf.zeros([words_number]))\n        \n    logits, labels = _compute_sampled_logits(\n                       weights=nce_weights,\n                       biases=nce_biases,\n                       inputs=embed,\n                       labels=train_labels,\n                       num_true=1,\n                       num_sampled=num_sampled,\n                       num_classes=words_number,\n                       remove_accidental_hits = False)\n    init = tf.global_variables_initializer()\n\n\nsession = tf.InteractiveSession(graph=graph)\ninit.run(session=session)\n\nbatch_inputs = np.array([0,1,2], dtype=np.int32)\nbatch_labels = np.array([[3],[4],[5]], dtype=np.int32)\n\nfeed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\nlogits_val, labels_val = session.run([logits, labels], feed_dict=feed_dict)\n\nprint (\"logits_val = {}\".format(logits_val))\nprint (\"labels_val = {}\".format(labels_val))\n\n\nAs a result, _compute_sampled_logits function generated num_sampled examples per 1 positive example:\nlogits_val = [[ -8.18727493   2.02518415  14.18676853   0.51900673]\n [ -8.97232056   5.60003376   4.52866602   3.68161726]\n [ -0.36226368  -5.84330416  -3.39891291   5.58423615]]\nlabels_val = [[ 1.  0.  0.  0.]\n [ 1.  0.  0.  0.]\n [ 1.  0.  0.  0.]]", "body": "Hi all,\r\n\r\nLooks like parameter `num_sampled` in  `tf.nn.nce_loss` function defines the number of negative examples per a positive example but not per a batch as described in tensorflow documentation (https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) \r\n\r\n(see the next code)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# `_compute_sampled_logits` is invoked in nce_loss to generate negative sample and calculate logits\r\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\r\n\r\nembedding_size = 10\r\nwords_number = 300\r\nbatch_size = 3\r\nnum_sampled = 3\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    # Input data.\r\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\r\n    train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\r\n\r\n    with tf.device('/cpu:0'):\r\n        embeddings = tf.Variable(\r\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\r\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\r\n        nce_weights = tf.Variable(\r\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\r\n        nce_biases = tf.Variable(tf.zeros([words_number]))\r\n        \r\n    logits, labels = _compute_sampled_logits(\r\n                       weights=nce_weights,\r\n                       biases=nce_biases,\r\n                       inputs=embed,\r\n                       labels=train_labels,\r\n                       num_true=1,\r\n                       num_sampled=num_sampled,\r\n                       num_classes=words_number,\r\n                       remove_accidental_hits = False)\r\n    init = tf.global_variables_initializer()\r\n\r\n\r\nsession = tf.InteractiveSession(graph=graph)\r\ninit.run(session=session)\r\n\r\nbatch_inputs = np.array([0,1,2], dtype=np.int32)\r\nbatch_labels = np.array([[3],[4],[5]], dtype=np.int32)\r\n\r\nfeed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\r\nlogits_val, labels_val = session.run([logits, labels], feed_dict=feed_dict)\r\n\r\nprint (\"logits_val = {}\".format(logits_val))\r\nprint (\"labels_val = {}\".format(labels_val))\r\n\r\n```\r\n\r\nAs a result, `_compute_sampled_logits` function generated `num_sampled` examples per **1 positive example**:\r\n```\r\nlogits_val = [[ -8.18727493   2.02518415  14.18676853   0.51900673]\r\n [ -8.97232056   5.60003376   4.52866602   3.68161726]\r\n [ -0.36226368  -5.84330416  -3.39891291   5.58423615]]\r\nlabels_val = [[ 1.  0.  0.  0.]\r\n [ 1.  0.  0.  0.]\r\n [ 1.  0.  0.  0.]]\r\n```"}