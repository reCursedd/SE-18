{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106750403", "pull_request_review_id": 27693632, "id": 106750403, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNjc1MDQwMw==", "diff_hunk": "@@ -0,0 +1,324 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/nn_ops.cc.\n+#ifdef INTEL_MKL\n+\n+#define USE_EIGEN_TENSOR\n+#define EIGEN_USE_THREADS\n+#include <algorithm>\n+#include <vector>\n+#include \"tensorflow/core/framework/numeric_op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_slice.h\"\n+#include \"tensorflow/core/kernels/conv_2d.h\"\n+#include \"tensorflow/core/kernels/conv_grad_ops.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/gtl/array_slice.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/util/padding.h\"\n+#include \"tensorflow/core/util/tensor_format.h\"\n+#include \"tensorflow/core/util/use_cudnn.h\"\n+#include \"tensorflow/core/util/work_sharder.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"third_party/mkl/include/mkl_dnn.h\"\n+#include \"third_party/mkl/include/mkl_dnn_types.h\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename Device, class T>\n+class MklConv2DCustomBackpropInputOp : public OpKernel {\n+ public:\n+  ~MklConv2DCustomBackpropInputOp() {}\n+  explicit MklConv2DCustomBackpropInputOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    string dataformat;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &dataformat));\n+    OP_REQUIRES(context, FormatFromString(dataformat, &data_format),\n+                errors::InvalidArgument(\"Invalid data format\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides));\n+    int stride_n = GetTensorDim(strides, data_format, 'N');\n+    int stride_c = GetTensorDim(strides, data_format, 'C');\n+    OP_REQUIRES(\n+        context, (stride_n == 1 && stride_c == 1),\n+        errors::InvalidArgument(\"Current implementation does not yet support \"\n+                                \"strides in the batch and depth dimensions.\"));\n+\n+    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding));\n+  }\n+\n+typedef struct {\n+  int in_dims;\n+  size_t in_sizes[4];\n+  size_t in_strides[4];\n+  size_t out_sizes[4];\n+  size_t out_strides[4];\n+  int input_offset[2];\n+  size_t filter_size[4];\n+  size_t filter_stride[4];\n+  size_t conv_strides[2];\n+  MklShape mkl_filter_shape, mkl_outback_shape;\n+} MklConvBackInputOpParams;\n+\n+  void MklCreateInputLayouts(OpKernelContext* context) {\n+    bool filter_in_mkl_format = mkl_params.mkl_filter_shape.IsMklTensor();\n+    bool outback_in_mkl_format = mkl_params.mkl_outback_shape.IsMklTensor();\n+    if (filter_in_mkl_format) {\n+      mkl_lt_filter = (dnnLayout_t)mkl_params.mkl_filter_shape.GetCurLayout();\n+    } else {\n+      CHECK_EQ(dnnLayoutCreate_F32(&mkl_lt_filter, mkl_params.in_dims,\n+          mkl_params.filter_size,\n+          mkl_params.filter_stride), E_SUCCESS);\n+    }\n+\n+    if (outback_in_mkl_format) {\n+      mkl_lt_outbackprop =\n+          (dnnLayout_t)mkl_params.mkl_outback_shape.GetCurLayout();\n+    } else {\n+      CHECK_EQ(dnnLayoutCreate_F32(&mkl_lt_outbackprop, mkl_params.in_dims,\n+          mkl_params.out_sizes, mkl_params.out_strides), E_SUCCESS);\n+    }\n+  }\n+\n+  void MklPrepareConvolutionInputs(OpKernelContext* context,\n+      Tensor* mkl_tmp_outbackprop_buf_tensor,\n+      Tensor* mkl_tmp_filter_buf_tensor) {\n+    dnnPrimitive_t mkl_convert_filter = nullptr,\n+                   mkl_convert_outbackprop = nullptr;\n+    void *mkl_filter_buf = nullptr, *mkl_outbackprop_buf = nullptr;\n+    dnnLayout_t mkl_lt_filter_internal = nullptr,\n+                mkl_lt_outbackprop_internal = nullptr;\n+    CHECK_EQ(dnnLayoutCreateFromPrimitive_F32(\n+        &mkl_lt_filter_internal, mkl_convolutionbwdata,\n+        dnnResourceFilter), E_SUCCESS);\n+\n+    const Tensor& filter = MklGetInput(context, 1);\n+\n+    CHECK_EQ(dnnLayoutCreateFromPrimitive_F32(&mkl_lt_outbackprop_internal,\n+                                         mkl_convolutionbwdata,\n+                                         dnnResourceDiffDst), E_SUCCESS);\n+     if (!dnnLayoutCompare_F32(mkl_lt_filter_internal, mkl_lt_filter)) {\n+      // Create conversion primitive\n+      CHECK_EQ(dnnConversionCreate_F32(&mkl_convert_filter, mkl_lt_filter,\n+                                  mkl_lt_filter_internal), E_SUCCESS);\n+\n+      AllocTmpBuffer(context, mkl_tmp_filter_buf_tensor,\n+                     mkl_lt_filter_internal, &mkl_filter_buf);\n+      CHECK_EQ(dnnConversionExecute_F32(\n+          mkl_convert_filter,\n+          static_cast<void*>(const_cast<T*>(filter.flat<T>().data())),\n+          mkl_filter_buf), E_SUCCESS);\n+\n+      // Assign filter buf to resources[] for convolution.\n+      conv_res[dnnResourceFilter] = mkl_filter_buf;\n+      dnnDelete_F32(mkl_convert_filter);\n+    } else {\n+      // If we do not need any layout conversion for filter, then\n+      // we direclty assign input filter to resources[].\n+      conv_res[dnnResourceFilter] =\n+          static_cast<void*>(const_cast<T*>(filter.flat<T>().data()));\n+    }\n+    dnnLayoutDelete_F32(mkl_lt_filter_internal);\n+    const Tensor& out_backprop = MklGetInput(context, 2);\n+    // --\n+    // We do similar steps as above for outputbackprop.\n+    if (!dnnLayoutCompare_F32(mkl_lt_outbackprop_internal,\n+                              mkl_lt_outbackprop)) {\n+      CHECK_EQ(dnnConversionCreate_F32(&mkl_convert_outbackprop,\n+          mkl_lt_outbackprop, mkl_lt_outbackprop_internal), E_SUCCESS);\n+      AllocTmpBuffer(context, mkl_tmp_outbackprop_buf_tensor,\n+                     mkl_lt_outbackprop_internal, &mkl_outbackprop_buf);\n+\n+      CHECK_EQ(dnnConversionExecute_F32(\n+          mkl_convert_outbackprop,\n+          static_cast<void*>(const_cast<T*>(out_backprop.flat<T>().data())),\n+          mkl_outbackprop_buf), E_SUCCESS);\n+\n+      conv_res[dnnResourceDiffDst] = mkl_outbackprop_buf;\n+      dnnDelete_F32(mkl_convert_outbackprop);\n+    } else {\n+      conv_res[dnnResourceDiffDst] =\n+          static_cast<void*>(const_cast<T*>(out_backprop.flat<T>().data()));\n+    }\n+    dnnLayoutDelete_F32(mkl_lt_outbackprop_internal);\n+  }\n+\n+  void MklCleanup() {\n+    bool filter_in_mkl_format = mkl_params.mkl_filter_shape.IsMklTensor();\n+    bool outback_in_mkl_format = mkl_params.mkl_outback_shape.IsMklTensor();\n+    if (!filter_in_mkl_format) dnnLayoutDelete_F32(mkl_lt_filter);\n+    if (!outback_in_mkl_format) dnnLayoutDelete_F32(mkl_lt_outbackprop);\n+    dnnDelete_F32(mkl_convolutionbwdata);\n+  }\n+  void Compute(OpKernelContext* context) override {\n+    const Tensor& input = MklGetInput(context, 0);\n+    const Tensor& filter = MklGetInput(context, 1);\n+\n+    GetMklShape(context, 1, &(mkl_params.mkl_filter_shape));\n+    bool filter_in_mkl_format = mkl_params.mkl_filter_shape.IsMklTensor();\n+\n+    const Tensor& out_backprop = MklGetInput(context, 2);\n+    GetMklShape(context, 2, &(mkl_params.mkl_outback_shape));\n+    bool outback_in_mkl_format = mkl_params.mkl_outback_shape.IsMklTensor();\n+\n+    TensorShape input_shape, filter_shape, outback_shape;\n+\n+    // Generate input shape.\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(input.shape()),\n+        errors::InvalidArgument(\n+            \"Conv2DBackpropInput: input_sizes input must be 1-dim, not \",\n+            input.dims()));\n+    OP_REQUIRES_OK(\n+        context, TensorShapeUtils::MakeShape(input.vec<int32>(), &input_shape));\n+\n+    // Generate shape for filter prop if input is in MKL format.\n+    if (filter_in_mkl_format) {\n+      OP_REQUIRES(context, mkl_params.mkl_filter_shape.GetDimension() == 4,\n+                  errors::InvalidArgument(\n+                      \"Conv2DCustomBackpropInput: size must be 4-dim\"));\n+\n+      MklSizesToTFSizes(context, data_format, mkl_params.mkl_filter_shape,\n+                        &filter_shape);\n+    } else {\n+      filter_shape = filter.shape();\n+    }\n+\n+    // Generate shape for outback prop if input is in MKL format.\n+    if (outback_in_mkl_format) {\n+      OP_REQUIRES(context, mkl_params.mkl_outback_shape.GetDimension() == 4,\n+                  errors::InvalidArgument(\n+                      \"Conv2DCustomBackpropInput: size must be 4-dim\"));\n+\n+      MklSizesToTFSizes(context, data_format, mkl_params.mkl_outback_shape,\n+                        &outback_shape);\n+    } else {\n+      outback_shape = out_backprop.shape();\n+    }\n+\n+    Conv2DBackpropDimensions dims;\n+    OP_REQUIRES_OK(context,\n+                   Conv2DBackpropComputeDimensions(\n+                       \"Conv2DCustomBackpropInput\", input_shape, filter_shape,\n+                       outback_shape, strides, padding, data_format, &dims));\n+\n+    int64 pad_top, pad_bottom;\n+    int64 pad_left, pad_right;\n+    OP_REQUIRES_OK(context, GetWindowedOutputSizeVerbose(\n+                                dims.rows.input_size, dims.rows.filter_size,\n+                                dims.rows.stride, padding,\n+                                &dims.rows.output_size, &pad_top, &pad_bottom));\n+    OP_REQUIRES_OK(context, GetWindowedOutputSizeVerbose(\n+                                dims.cols.input_size, dims.cols.filter_size,\n+                                dims.cols.stride, padding,\n+                                &dims.cols.output_size, &pad_left, &pad_right));\n+\n+    mkl_params.in_dims = 4;\n+\n+    mkl_params.in_sizes[0] = static_cast<size_t>(dims.cols.input_size);", "path": "tensorflow/core/kernels/mkl_conv_grad_input_ops.cc", "position": 255, "original_position": 238, "commit_id": "3c2192fc868ddc99a3f39fec14a497c156499b88", "original_commit_id": "d7c6d240ece8313bda0a1b3bf50925aafe6470b8", "user": {"login": "andydavis1", "id": 15696327, "node_id": "MDQ6VXNlcjE1Njk2MzI3", "avatar_url": "https://avatars0.githubusercontent.com/u/15696327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andydavis1", "html_url": "https://github.com/andydavis1", "followers_url": "https://api.github.com/users/andydavis1/followers", "following_url": "https://api.github.com/users/andydavis1/following{/other_user}", "gists_url": "https://api.github.com/users/andydavis1/gists{/gist_id}", "starred_url": "https://api.github.com/users/andydavis1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andydavis1/subscriptions", "organizations_url": "https://api.github.com/users/andydavis1/orgs", "repos_url": "https://api.github.com/users/andydavis1/repos", "events_url": "https://api.github.com/users/andydavis1/events{/privacy}", "received_events_url": "https://api.github.com/users/andydavis1/received_events", "type": "User", "site_admin": false}, "body": "Maybe break this up with line spaces between param types for readability:\r\n\r\n    mkl_params.input_offset[0] = static_cast<int>(-pad_left);\ufffc\r\n    mkl_params.input_offset[1] = static_cast<int>(-pad_top);\r\n\r\n\ufffc    mkl_params.conv_strides[0] = static_cast<size_t>(dims.cols.stride);\r\n\ufffc    mkl_params.conv_strides[1] = static_cast<size_t>(dims.rows.stride);\r\n", "created_at": "2017-03-17T21:36:24Z", "updated_at": "2017-03-22T22:13:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8184#discussion_r106750403", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8184", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106750403"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8184#discussion_r106750403"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8184"}}, "body_html": "<p>Maybe break this up with line spaces between param types for readability:</p>\n<pre><code>mkl_params.input_offset[0] = static_cast&lt;int&gt;(-pad_left);\ufffc\nmkl_params.input_offset[1] = static_cast&lt;int&gt;(-pad_top);\n</code></pre>\n<p>\ufffc    mkl_params.conv_strides[0] = static_cast&lt;size_t&gt;(dims.cols.stride);<br>\n\ufffc    mkl_params.conv_strides[1] = static_cast&lt;size_t&gt;(dims.rows.stride);</p>", "body_text": "Maybe break this up with line spaces between param types for readability:\nmkl_params.input_offset[0] = static_cast<int>(-pad_left);\ufffc\nmkl_params.input_offset[1] = static_cast<int>(-pad_top);\n\n\ufffc    mkl_params.conv_strides[0] = static_cast<size_t>(dims.cols.stride);\n\ufffc    mkl_params.conv_strides[1] = static_cast<size_t>(dims.rows.stride);"}