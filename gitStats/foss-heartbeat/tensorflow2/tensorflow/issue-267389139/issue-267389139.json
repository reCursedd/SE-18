{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13879", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13879/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13879/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13879/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13879", "id": 267389139, "node_id": "MDU6SXNzdWUyNjczODkxMzk=", "number": 13879, "title": "Variable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed.", "user": {"login": "shuvayanti", "id": 32708885, "node_id": "MDQ6VXNlcjMyNzA4ODg1", "avatar_url": "https://avatars3.githubusercontent.com/u/32708885?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shuvayanti", "html_url": "https://github.com/shuvayanti", "followers_url": "https://api.github.com/users/shuvayanti/followers", "following_url": "https://api.github.com/users/shuvayanti/following{/other_user}", "gists_url": "https://api.github.com/users/shuvayanti/gists{/gist_id}", "starred_url": "https://api.github.com/users/shuvayanti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shuvayanti/subscriptions", "organizations_url": "https://api.github.com/users/shuvayanti/orgs", "repos_url": "https://api.github.com/users/shuvayanti/repos", "events_url": "https://api.github.com/users/shuvayanti/events{/privacy}", "received_events_url": "https://api.github.com/users/shuvayanti/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-21T15:20:17Z", "updated_at": "2017-10-21T19:19:24Z", "closed_at": "2017-10-21T19:19:24Z", "author_association": "NONE", "body_html": "<p>Hello,<br>\nI wrote this code:</p>\n<p>def rnn_inputs(FLAGS, input_data):</p>\n<pre><code>with tf.variable_scope('rnn_inputs', reuse=True):\n    W_input = tf.get_variable(\"W_input\",\n        [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\n\n# &lt;num_examples, seq_len, num_hidden_units&gt;\nembeddings = tf.nn.embedding_lookup(W_input, input_data)\n\nreturn embeddings\n</code></pre>\n<p>def rnn_softmax(FLAGS, outputs):<br>\nwith tf.variable_scope('rnn_softmax', reuse=True):<br>\nW_softmax = tf.get_variable(\"W_softmax\",<br>\n[FLAGS.num_hidden_units, FLAGS.num_classes])<br>\nb_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])</p>\n<pre><code>logits = tf.matmul(outputs, W_softmax) + b_softmax\n\nreturn logits\n</code></pre>\n<p>class model(object):</p>\n<pre><code>def __init__(self, FLAGS):\n\n    # Placeholders\n    self.inputs_X = tf.placeholder(tf.int32,\n        shape=[None, None], name='inputs_X')\n    self.targets_y = tf.placeholder(tf.float32,\n        shape=[None, None], name='targets_y')\n    self.seq_lens = tf.placeholder(tf.int32,\n        shape=[None, ], name='seq_lens')\n    self.dropout = tf.placeholder(tf.float32)\n\n    # RNN cell\n    stacked_cell = rnn_cell(FLAGS, self.dropout)\n\n    # Inputs to RNN\n    with tf.variable_scope('rnn_inputs',reuse=True):\n        W_input = tf.get_variable(\"W_input\",\n            [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\n\n    inputs = rnn_inputs(FLAGS, self.inputs_X)\n    #initial_state = stacked_cell.zero_state(FLAGS.batch_size, tf.float32)\n\n    # Outputs from RNN\n    all_outputs, state = tf.nn.dynamic_rnn(cell=stacked_cell, inputs=inputs,\n        sequence_length=self.seq_lens, dtype=tf.float32)\n\n    # state has the last RELEVANT output automatically since we fed in seq_len\n    # [0] because state is a tuple with a tensor inside it\n    outputs = state[0]\n\n    # Process RNN outputs\n    with tf.variable_scope('rnn_softmax',reuse=True):\n        W_softmax = tf.get_variable(\"W_softmax\",\n            [FLAGS.num_hidden_units, FLAGS.num_classes])\n        b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])\n\n    # Logits\n    logits = rnn_softmax(FLAGS, outputs)\n    probabilities = tf.nn.softmax(logits)\n    self.accuracy = tf.equal(tf.argmax(\n        self.targets_y,1), tf.argmax(logits,1))\n\n    # Loss\n    self.loss = tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.targets_y))\n\n    # Optimization\n    self.lr = tf.Variable(0.0, trainable=False)\n    trainable_vars = tf.trainable_variables()\n    # clip the gradient to avoid vanishing or blowing up gradients\n    grads, _ = tf.clip_by_global_norm(\n        tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)\n    optimizer = tf.train.AdamOptimizer(self.lr)\n    self.train_optimizer = optimizer.apply_gradients(\n        zip(grads, trainable_vars))\n\n    # Below are values we will use for sampling (generating the sentiment\n    # after each word.)\n\n    # this is taking all the ouputs for the first input sequence\n    # (only 1 input sequence since we are sampling)\n    sampling_outputs = all_outputs[0]\n\n    # Logits\n    sampling_logits = rnn_softmax(FLAGS, sampling_outputs)\n    self.sampling_probabilities = tf.nn.softmax(sampling_logits)\n\n    # Components for model saving\n    self.global_step = tf.Variable(0, trainable=False)\n    self.saver = tf.train.Saver(tf.all_variables())\n\ndef step(self, sess, batch_X, batch_seq_lens, batch_y=None, dropout=0.0,\n    forward_only=True, sampling=False):\n\n    input_feed = {self.inputs_X: batch_X,\n                  self.targets_y: batch_y,\n                  self.seq_lens: batch_seq_lens,\n                  self.dropout: dropout}\n\n    if forward_only:\n        if not sampling:\n            output_feed = [self.loss,\n                           self.accuracy]\n        elif sampling:\n            input_feed = {self.inputs_X: batch_X,\n                          self.seq_lens: batch_seq_lens,\n                          self.dropout: dropout}\n            output_feed = [self.sampling_probabilities]\n    else: # training\n        output_feed = [self.train_optimizer,\n                       self.loss,\n                       self.accuracy]\n\n\n    outputs = sess.run(output_feed, input_feed)\n\n    if forward_only:\n        if not sampling:\n            return outputs[0], outputs[1]\n        elif sampling:\n            return outputs[0]\n    else: # training\n        return outputs[0], outputs[1], outputs[2]\n</code></pre>\n<p><strong>But I faced this error while training:</strong></p>\n<p>ValueError                                Traceback (most recent call last)<br>\n in ()<br>\n----&gt; 1 train()</p>\n<p> in train()<br>\n9<br>\n10         # Load old model or create new one<br>\n---&gt; 11         model = create_model(sess, FLAGS)<br>\n12<br>\n13         # Train results</p>\n<p> in create_model(sess, FLAGS)<br>\n1 def create_model(sess, FLAGS):<br>\n2<br>\n----&gt; 3     text_model = model(FLAGS)<br>\n4<br>\n5     ckpt = tf.train.get_checkpoint_state(FLAGS.ckpt_dir)</p>\n<p> in <strong>init</strong>(self, FLAGS)<br>\n18         with tf.variable_scope('rnn_inputs',reuse=True):<br>\n19             W_input = tf.get_variable(\"W_input\",<br>\n---&gt; 20                 [FLAGS.en_vocab_size, FLAGS.num_hidden_units])<br>\n21<br>\n22         inputs = rnn_inputs(FLAGS, self.inputs_X)</p>\n<p>C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)<br>\n1063       collections=collections, caching_device=caching_device,<br>\n1064       partitioner=partitioner, validate_shape=validate_shape,<br>\n-&gt; 1065       use_resource=use_resource, custom_getter=custom_getter)<br>\n1066 get_variable_or_local_docstring = (<br>\n1067     \"\"\"%s</p>\n<p>C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)<br>\n960           collections=collections, caching_device=caching_device,<br>\n961           partitioner=partitioner, validate_shape=validate_shape,<br>\n--&gt; 962           use_resource=use_resource, custom_getter=custom_getter)<br>\n963<br>\n964   def _get_partitioned_variable(self,</p>\n<p>C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)<br>\n365           reuse=reuse, trainable=trainable, collections=collections,<br>\n366           caching_device=caching_device, partitioner=partitioner,<br>\n--&gt; 367           validate_shape=validate_shape, use_resource=use_resource)<br>\n368<br>\n369   def _get_partitioned_variable(</p>\n<p>C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)<br>\n350           trainable=trainable, collections=collections,<br>\n351           caching_device=caching_device, validate_shape=validate_shape,<br>\n--&gt; 352           use_resource=use_resource)<br>\n353<br>\n354     if custom_getter is not None:</p>\n<p>C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)<br>\n680       raise ValueError(\"Variable %s does not exist, or was not created with \"<br>\n681                        \"tf.get_variable(). Did you mean to set reuse=None in \"<br>\n--&gt; 682                        \"VarScope?\" % name)<br>\n683     if not shape.is_fully_defined() and not initializing_from_value:<br>\n684       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"</p>\n<p><strong>ValueError:</strong> Variable rnn_inputs/W_input does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?</p>\n<p><strong>So,I set reuse=None, but it showed another error:</strong></p>\n<p><strong>ValueError</strong>: Variable rnn_inputs/W_input already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:</p>\n<p>File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in <strong>init</strong><br>\nself._traceback = _extract_stack()<br>\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op<br>\nop_def=op_def)</p>\n<p><strong>I again set back reuse = True, which should be the case,but this is error this time:</strong></p>\n<p>Variable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:</p>\n<p>File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in <strong>init</strong><br>\nself._traceback = _extract_stack()<br>\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op<br>\nop_def=op_def)</p>\n<p>Can anybody help me with this?</p>", "body_text": "Hello,\nI wrote this code:\ndef rnn_inputs(FLAGS, input_data):\nwith tf.variable_scope('rnn_inputs', reuse=True):\n    W_input = tf.get_variable(\"W_input\",\n        [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\n\n# <num_examples, seq_len, num_hidden_units>\nembeddings = tf.nn.embedding_lookup(W_input, input_data)\n\nreturn embeddings\n\ndef rnn_softmax(FLAGS, outputs):\nwith tf.variable_scope('rnn_softmax', reuse=True):\nW_softmax = tf.get_variable(\"W_softmax\",\n[FLAGS.num_hidden_units, FLAGS.num_classes])\nb_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])\nlogits = tf.matmul(outputs, W_softmax) + b_softmax\n\nreturn logits\n\nclass model(object):\ndef __init__(self, FLAGS):\n\n    # Placeholders\n    self.inputs_X = tf.placeholder(tf.int32,\n        shape=[None, None], name='inputs_X')\n    self.targets_y = tf.placeholder(tf.float32,\n        shape=[None, None], name='targets_y')\n    self.seq_lens = tf.placeholder(tf.int32,\n        shape=[None, ], name='seq_lens')\n    self.dropout = tf.placeholder(tf.float32)\n\n    # RNN cell\n    stacked_cell = rnn_cell(FLAGS, self.dropout)\n\n    # Inputs to RNN\n    with tf.variable_scope('rnn_inputs',reuse=True):\n        W_input = tf.get_variable(\"W_input\",\n            [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\n\n    inputs = rnn_inputs(FLAGS, self.inputs_X)\n    #initial_state = stacked_cell.zero_state(FLAGS.batch_size, tf.float32)\n\n    # Outputs from RNN\n    all_outputs, state = tf.nn.dynamic_rnn(cell=stacked_cell, inputs=inputs,\n        sequence_length=self.seq_lens, dtype=tf.float32)\n\n    # state has the last RELEVANT output automatically since we fed in seq_len\n    # [0] because state is a tuple with a tensor inside it\n    outputs = state[0]\n\n    # Process RNN outputs\n    with tf.variable_scope('rnn_softmax',reuse=True):\n        W_softmax = tf.get_variable(\"W_softmax\",\n            [FLAGS.num_hidden_units, FLAGS.num_classes])\n        b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])\n\n    # Logits\n    logits = rnn_softmax(FLAGS, outputs)\n    probabilities = tf.nn.softmax(logits)\n    self.accuracy = tf.equal(tf.argmax(\n        self.targets_y,1), tf.argmax(logits,1))\n\n    # Loss\n    self.loss = tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.targets_y))\n\n    # Optimization\n    self.lr = tf.Variable(0.0, trainable=False)\n    trainable_vars = tf.trainable_variables()\n    # clip the gradient to avoid vanishing or blowing up gradients\n    grads, _ = tf.clip_by_global_norm(\n        tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)\n    optimizer = tf.train.AdamOptimizer(self.lr)\n    self.train_optimizer = optimizer.apply_gradients(\n        zip(grads, trainable_vars))\n\n    # Below are values we will use for sampling (generating the sentiment\n    # after each word.)\n\n    # this is taking all the ouputs for the first input sequence\n    # (only 1 input sequence since we are sampling)\n    sampling_outputs = all_outputs[0]\n\n    # Logits\n    sampling_logits = rnn_softmax(FLAGS, sampling_outputs)\n    self.sampling_probabilities = tf.nn.softmax(sampling_logits)\n\n    # Components for model saving\n    self.global_step = tf.Variable(0, trainable=False)\n    self.saver = tf.train.Saver(tf.all_variables())\n\ndef step(self, sess, batch_X, batch_seq_lens, batch_y=None, dropout=0.0,\n    forward_only=True, sampling=False):\n\n    input_feed = {self.inputs_X: batch_X,\n                  self.targets_y: batch_y,\n                  self.seq_lens: batch_seq_lens,\n                  self.dropout: dropout}\n\n    if forward_only:\n        if not sampling:\n            output_feed = [self.loss,\n                           self.accuracy]\n        elif sampling:\n            input_feed = {self.inputs_X: batch_X,\n                          self.seq_lens: batch_seq_lens,\n                          self.dropout: dropout}\n            output_feed = [self.sampling_probabilities]\n    else: # training\n        output_feed = [self.train_optimizer,\n                       self.loss,\n                       self.accuracy]\n\n\n    outputs = sess.run(output_feed, input_feed)\n\n    if forward_only:\n        if not sampling:\n            return outputs[0], outputs[1]\n        elif sampling:\n            return outputs[0]\n    else: # training\n        return outputs[0], outputs[1], outputs[2]\n\nBut I faced this error while training:\nValueError                                Traceback (most recent call last)\n in ()\n----> 1 train()\n in train()\n9\n10         # Load old model or create new one\n---> 11         model = create_model(sess, FLAGS)\n12\n13         # Train results\n in create_model(sess, FLAGS)\n1 def create_model(sess, FLAGS):\n2\n----> 3     text_model = model(FLAGS)\n4\n5     ckpt = tf.train.get_checkpoint_state(FLAGS.ckpt_dir)\n in init(self, FLAGS)\n18         with tf.variable_scope('rnn_inputs',reuse=True):\n19             W_input = tf.get_variable(\"W_input\",\n---> 20                 [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\n21\n22         inputs = rnn_inputs(FLAGS, self.inputs_X)\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n1063       collections=collections, caching_device=caching_device,\n1064       partitioner=partitioner, validate_shape=validate_shape,\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\n1066 get_variable_or_local_docstring = (\n1067     \"\"\"%s\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n960           collections=collections, caching_device=caching_device,\n961           partitioner=partitioner, validate_shape=validate_shape,\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\n963\n964   def _get_partitioned_variable(self,\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n365           reuse=reuse, trainable=trainable, collections=collections,\n366           caching_device=caching_device, partitioner=partitioner,\n--> 367           validate_shape=validate_shape, use_resource=use_resource)\n368\n369   def _get_partitioned_variable(\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\n350           trainable=trainable, collections=collections,\n351           caching_device=caching_device, validate_shape=validate_shape,\n--> 352           use_resource=use_resource)\n353\n354     if custom_getter is not None:\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\n680       raise ValueError(\"Variable %s does not exist, or was not created with \"\n681                        \"tf.get_variable(). Did you mean to set reuse=None in \"\n--> 682                        \"VarScope?\" % name)\n683     if not shape.is_fully_defined() and not initializing_from_value:\n684       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\nValueError: Variable rnn_inputs/W_input does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\nSo,I set reuse=None, but it showed another error:\nValueError: Variable rnn_inputs/W_input already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in init\nself._traceback = _extract_stack()\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\nop_def=op_def)\nI again set back reuse = True, which should be the case,but this is error this time:\nVariable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in init\nself._traceback = _extract_stack()\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\nop_def=op_def)\nCan anybody help me with this?", "body": "Hello,\r\nI wrote this code:\r\n\r\ndef rnn_inputs(FLAGS, input_data):\r\n\r\n    with tf.variable_scope('rnn_inputs', reuse=True):\r\n        W_input = tf.get_variable(\"W_input\",\r\n            [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\r\n\r\n    # <num_examples, seq_len, num_hidden_units>\r\n    embeddings = tf.nn.embedding_lookup(W_input, input_data)\r\n\r\n    return embeddings\r\n\r\ndef rnn_softmax(FLAGS, outputs):\r\n    with tf.variable_scope('rnn_softmax', reuse=True):\r\n        W_softmax = tf.get_variable(\"W_softmax\",\r\n            [FLAGS.num_hidden_units, FLAGS.num_classes])\r\n        b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])\r\n\r\n    logits = tf.matmul(outputs, W_softmax) + b_softmax\r\n\r\n    return logits\r\n\r\nclass model(object):\r\n\r\n    def __init__(self, FLAGS):\r\n\r\n        # Placeholders\r\n        self.inputs_X = tf.placeholder(tf.int32,\r\n            shape=[None, None], name='inputs_X')\r\n        self.targets_y = tf.placeholder(tf.float32,\r\n            shape=[None, None], name='targets_y')\r\n        self.seq_lens = tf.placeholder(tf.int32,\r\n            shape=[None, ], name='seq_lens')\r\n        self.dropout = tf.placeholder(tf.float32)\r\n\r\n        # RNN cell\r\n        stacked_cell = rnn_cell(FLAGS, self.dropout)\r\n\r\n        # Inputs to RNN\r\n        with tf.variable_scope('rnn_inputs',reuse=True):\r\n            W_input = tf.get_variable(\"W_input\",\r\n                [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\r\n\r\n        inputs = rnn_inputs(FLAGS, self.inputs_X)\r\n        #initial_state = stacked_cell.zero_state(FLAGS.batch_size, tf.float32)\r\n\r\n        # Outputs from RNN\r\n        all_outputs, state = tf.nn.dynamic_rnn(cell=stacked_cell, inputs=inputs,\r\n            sequence_length=self.seq_lens, dtype=tf.float32)\r\n\r\n        # state has the last RELEVANT output automatically since we fed in seq_len\r\n        # [0] because state is a tuple with a tensor inside it\r\n        outputs = state[0]\r\n\r\n        # Process RNN outputs\r\n        with tf.variable_scope('rnn_softmax',reuse=True):\r\n            W_softmax = tf.get_variable(\"W_softmax\",\r\n                [FLAGS.num_hidden_units, FLAGS.num_classes])\r\n            b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])\r\n\r\n        # Logits\r\n        logits = rnn_softmax(FLAGS, outputs)\r\n        probabilities = tf.nn.softmax(logits)\r\n        self.accuracy = tf.equal(tf.argmax(\r\n            self.targets_y,1), tf.argmax(logits,1))\r\n\r\n        # Loss\r\n        self.loss = tf.reduce_mean(\r\n            tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.targets_y))\r\n\r\n        # Optimization\r\n        self.lr = tf.Variable(0.0, trainable=False)\r\n        trainable_vars = tf.trainable_variables()\r\n        # clip the gradient to avoid vanishing or blowing up gradients\r\n        grads, _ = tf.clip_by_global_norm(\r\n            tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)\r\n        optimizer = tf.train.AdamOptimizer(self.lr)\r\n        self.train_optimizer = optimizer.apply_gradients(\r\n            zip(grads, trainable_vars))\r\n\r\n        # Below are values we will use for sampling (generating the sentiment\r\n        # after each word.)\r\n\r\n        # this is taking all the ouputs for the first input sequence\r\n        # (only 1 input sequence since we are sampling)\r\n        sampling_outputs = all_outputs[0]\r\n\r\n        # Logits\r\n        sampling_logits = rnn_softmax(FLAGS, sampling_outputs)\r\n        self.sampling_probabilities = tf.nn.softmax(sampling_logits)\r\n\r\n        # Components for model saving\r\n        self.global_step = tf.Variable(0, trainable=False)\r\n        self.saver = tf.train.Saver(tf.all_variables())\r\n\r\n    def step(self, sess, batch_X, batch_seq_lens, batch_y=None, dropout=0.0,\r\n        forward_only=True, sampling=False):\r\n\r\n        input_feed = {self.inputs_X: batch_X,\r\n                      self.targets_y: batch_y,\r\n                      self.seq_lens: batch_seq_lens,\r\n                      self.dropout: dropout}\r\n\r\n        if forward_only:\r\n            if not sampling:\r\n                output_feed = [self.loss,\r\n                               self.accuracy]\r\n            elif sampling:\r\n                input_feed = {self.inputs_X: batch_X,\r\n                              self.seq_lens: batch_seq_lens,\r\n                              self.dropout: dropout}\r\n                output_feed = [self.sampling_probabilities]\r\n        else: # training\r\n            output_feed = [self.train_optimizer,\r\n                           self.loss,\r\n                           self.accuracy]\r\n\r\n\r\n        outputs = sess.run(output_feed, input_feed)\r\n\r\n        if forward_only:\r\n            if not sampling:\r\n                return outputs[0], outputs[1]\r\n            elif sampling:\r\n                return outputs[0]\r\n        else: # training\r\n            return outputs[0], outputs[1], outputs[2]\r\n\r\n**But I faced this error while training:**\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-19-93fd337a0d5c> in <module>()\r\n----> 1 train()\r\n\r\n<ipython-input-18-62be6fa1e73e> in train()\r\n      9 \r\n     10         # Load old model or create new one\r\n---> 11         model = create_model(sess, FLAGS)\r\n     12 \r\n     13         # Train results\r\n\r\n<ipython-input-17-0c9c27ad52d3> in create_model(sess, FLAGS)\r\n      1 def create_model(sess, FLAGS):\r\n      2 \r\n----> 3     text_model = model(FLAGS)\r\n      4 \r\n      5     ckpt = tf.train.get_checkpoint_state(FLAGS.ckpt_dir)\r\n\r\n<ipython-input-15-bd33cb4f9d34> in __init__(self, FLAGS)\r\n     18         with tf.variable_scope('rnn_inputs',reuse=True):\r\n     19             W_input = tf.get_variable(\"W_input\",\r\n---> 20                 [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\r\n     21 \r\n     22         inputs = rnn_inputs(FLAGS, self.inputs_X)\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n   1063       collections=collections, caching_device=caching_device,\r\n   1064       partitioner=partitioner, validate_shape=validate_shape,\r\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\r\n   1066 get_variable_or_local_docstring = (\r\n   1067     \"\"\"%s\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    960           collections=collections, caching_device=caching_device,\r\n    961           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\r\n    963 \r\n    964   def _get_partitioned_variable(self,\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    365           reuse=reuse, trainable=trainable, collections=collections,\r\n    366           caching_device=caching_device, partitioner=partitioner,\r\n--> 367           validate_shape=validate_shape, use_resource=use_resource)\r\n    368 \r\n    369   def _get_partitioned_variable(\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\r\n    350           trainable=trainable, collections=collections,\r\n    351           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 352           use_resource=use_resource)\r\n    353 \r\n    354     if custom_getter is not None:\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\r\n    680       raise ValueError(\"Variable %s does not exist, or was not created with \"\r\n    681                        \"tf.get_variable(). Did you mean to set reuse=None in \"\r\n--> 682                        \"VarScope?\" % name)\r\n    683     if not shape.is_fully_defined() and not initializing_from_value:\r\n    684       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\r\n\r\n**ValueError:** Variable rnn_inputs/W_input does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\n**So,I set reuse=None, but it showed another error:**\r\n\r\n**ValueError**: Variable rnn_inputs/W_input already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n\r\n**I again set back reuse = True, which should be the case,but this is error this time:**\r\n\r\nVariable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n\r\nCan anybody help me with this?"}