{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264544202", "html_url": "https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264544202", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6019", "id": 264544202, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDU0NDIwMg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-02T19:42:32Z", "updated_at": "2016-12-02T19:42:32Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">What happens if you run with tcmalloc? There's been some memory leaks\nreported earlier which disappear when you do tcmalloc (as specified here\n&lt;<a href=\"http://goog-perftools.sourceforge.net/doc/tcmalloc.html\">http://goog-perftools.sourceforge.net/doc/tcmalloc.html</a>&gt;)\nAlso, you can see which ops are allocating memory by doing something like\nthis\n\nrun_metadata = tf.RunMetadata()\nsess.run([C.op],\n             options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n             run_metadata=run_metadata)\nprint run_metadata\n\nOr more visually in Chrome Trace Viewer (see\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"146958443\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1824\" href=\"https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659\">#1824 (comment)</a>)\n\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\nwith open('timeline.ctf.json', 'w') as trace_file:\n  trace_file.write(trace.generate_chrome_trace_format(show_memory=True))</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Dec 2, 2016 at 10:56 AM, Pepe Mandioca ***@***.***&gt; wrote:\n Further testing with a really simple network: conv(4) - pool(2x2) - fc(10)\n (that's 4 filters for the convolutional layer) full code available in\n <a href=\"http://pastebin.com/Zh14pS3C\">http://pastebin.com/Zh14pS3C</a>.\n I removed the training, accuracy evaluation, and most layers from the net,\n while also reducing the filters for the convolutional layer and the relu.\n For the whole training dataset this still requires *6.5gb* of memory.\n There's a roughly 1gb decrease if I evaluate with 10000 less training\n examples, 2gb for 20000, and so on. Is this normal?\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"192908142\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6019\" href=\"https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264533170\">#6019 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHPtgmRHA9TlLuX2xxFdyyzookUQOks5rEGnkgaJpZM4LBp_t\">https://github.com/notifications/unsubscribe-auth/AABaHPtgmRHA9TlLuX2xxFdyyzookUQOks5rEGnkgaJpZM4LBp_t</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "What happens if you run with tcmalloc? There's been some memory leaks\nreported earlier which disappear when you do tcmalloc (as specified here\n<http://goog-perftools.sourceforge.net/doc/tcmalloc.html>)\nAlso, you can see which ops are allocating memory by doing something like\nthis\n\nrun_metadata = tf.RunMetadata()\nsess.run([C.op],\n             options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n             run_metadata=run_metadata)\nprint run_metadata\n\nOr more visually in Chrome Trace Viewer (see\n#1824 (comment))\n\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\nwith open('timeline.ctf.json', 'w') as trace_file:\n  trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n\u2026\nOn Fri, Dec 2, 2016 at 10:56 AM, Pepe Mandioca ***@***.***> wrote:\n Further testing with a really simple network: conv(4) - pool(2x2) - fc(10)\n (that's 4 filters for the convolutional layer) full code available in\n http://pastebin.com/Zh14pS3C.\n I removed the training, accuracy evaluation, and most layers from the net,\n while also reducing the filters for the convolutional layer and the relu.\n For the whole training dataset this still requires *6.5gb* of memory.\n There's a roughly 1gb decrease if I evaluate with 10000 less training\n examples, 2gb for 20000, and so on. Is this normal?\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n <#6019 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AABaHPtgmRHA9TlLuX2xxFdyyzookUQOks5rEGnkgaJpZM4LBp_t>\n .", "body": "What happens if you run with tcmalloc? There's been some memory leaks\nreported earlier which disappear when you do tcmalloc (as specified here\n<http://goog-perftools.sourceforge.net/doc/tcmalloc.html>)\nAlso, you can see which ops are allocating memory by doing something like\nthis\n\nrun_metadata = tf.RunMetadata()\nsess.run([C.op],\n             options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n             run_metadata=run_metadata)\nprint run_metadata\n\nOr more visually in Chrome Trace Viewer (see\nhttps://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659)\n\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\nwith open('timeline.ctf.json', 'w') as trace_file:\n  trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n\n\nOn Fri, Dec 2, 2016 at 10:56 AM, Pepe Mandioca <notifications@github.com>\nwrote:\n\n> Further testing with a really simple network: conv(4) - pool(2x2) - fc(10)\n> (that's 4 filters for the convolutional layer) full code available in\n> http://pastebin.com/Zh14pS3C.\n> I removed the training, accuracy evaluation, and most layers from the net,\n> while also reducing the filters for the convolutional layer and the relu.\n> For the whole training dataset this still requires *6.5gb* of memory.\n> There's a roughly 1gb decrease if I evaluate with 10000 less training\n> examples, 2gb for 20000, and so on. Is this normal?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264533170>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHPtgmRHA9TlLuX2xxFdyyzookUQOks5rEGnkgaJpZM4LBp_t>\n> .\n>\n"}