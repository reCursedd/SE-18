{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264524252", "html_url": "https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264524252", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6019", "id": 264524252, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDUyNDI1Mg==", "user": {"login": "facundoq", "id": 1576711, "node_id": "MDQ6VXNlcjE1NzY3MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1576711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facundoq", "html_url": "https://github.com/facundoq", "followers_url": "https://api.github.com/users/facundoq/followers", "following_url": "https://api.github.com/users/facundoq/following{/other_user}", "gists_url": "https://api.github.com/users/facundoq/gists{/gist_id}", "starred_url": "https://api.github.com/users/facundoq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facundoq/subscriptions", "organizations_url": "https://api.github.com/users/facundoq/orgs", "repos_url": "https://api.github.com/users/facundoq/repos", "events_url": "https://api.github.com/users/facundoq/events{/privacy}", "received_events_url": "https://api.github.com/users/facundoq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-02T18:21:15Z", "updated_at": "2016-12-02T18:21:15Z", "author_association": "NONE", "body_html": "<p>Indeed limiting the memory of the process avoids the OOM. TF is trying to use about 11gbs of memory to evaluate the entire training set. While this can be workaround quite easily (evaluating the accuracy in smaller batches), isn't 11gbs a bit overmuch? Is it keeping the activations of every layer even when not doing a train_step.run()? If the activations are not saved for the backward pass, no more than 1.3gb+160mb should be used by the forward pass. Is there any way to avoid storing the activations?</p>", "body_text": "Indeed limiting the memory of the process avoids the OOM. TF is trying to use about 11gbs of memory to evaluate the entire training set. While this can be workaround quite easily (evaluating the accuracy in smaller batches), isn't 11gbs a bit overmuch? Is it keeping the activations of every layer even when not doing a train_step.run()? If the activations are not saved for the backward pass, no more than 1.3gb+160mb should be used by the forward pass. Is there any way to avoid storing the activations?", "body": "Indeed limiting the memory of the process avoids the OOM. TF is trying to use about 11gbs of memory to evaluate the entire training set. While this can be workaround quite easily (evaluating the accuracy in smaller batches), isn't 11gbs a bit overmuch? Is it keeping the activations of every layer even when not doing a train_step.run()? If the activations are not saved for the backward pass, no more than 1.3gb+160mb should be used by the forward pass. Is there any way to avoid storing the activations?"}