{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264326894", "html_url": "https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264326894", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6019", "id": 264326894, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDMyNjg5NA==", "user": {"login": "facundoq", "id": 1576711, "node_id": "MDQ6VXNlcjE1NzY3MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1576711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facundoq", "html_url": "https://github.com/facundoq", "followers_url": "https://api.github.com/users/facundoq/followers", "following_url": "https://api.github.com/users/facundoq/following{/other_user}", "gists_url": "https://api.github.com/users/facundoq/gists{/gist_id}", "starred_url": "https://api.github.com/users/facundoq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facundoq/subscriptions", "organizations_url": "https://api.github.com/users/facundoq/orgs", "repos_url": "https://api.github.com/users/facundoq/repos", "events_url": "https://api.github.com/users/facundoq/events{/privacy}", "received_events_url": "https://api.github.com/users/facundoq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-01T23:23:46Z", "updated_at": "2016-12-01T23:23:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a> Thanks, I guessed as much, but just before the freeze I still see 3gbs free of ram; maybe a big chunk gets allocated just before the freeze and causes it. Also, I thought having no swap would cause an out-of-memory error, my bad, so maybe that could be it.  I'll check tomorrow from the computer at work if limiting the memory for the process helps to avoid the freeze.</p>\n<p>Nonetheless, the entire training set is 160mb if in float32. The output of the convolutional layers are 1.3gb and 650mb respectively. 214mb for the first fully connected and just 2 megs for the output, for a total of ~2.5gb total. The memory required for the weights is negligible. It seems weird the activations of the intermediate layers would take that much space.</p>", "body_text": "@poxvoculi Thanks, I guessed as much, but just before the freeze I still see 3gbs free of ram; maybe a big chunk gets allocated just before the freeze and causes it. Also, I thought having no swap would cause an out-of-memory error, my bad, so maybe that could be it.  I'll check tomorrow from the computer at work if limiting the memory for the process helps to avoid the freeze.\nNonetheless, the entire training set is 160mb if in float32. The output of the convolutional layers are 1.3gb and 650mb respectively. 214mb for the first fully connected and just 2 megs for the output, for a total of ~2.5gb total. The memory required for the weights is negligible. It seems weird the activations of the intermediate layers would take that much space.", "body": "@poxvoculi Thanks, I guessed as much, but just before the freeze I still see 3gbs free of ram; maybe a big chunk gets allocated just before the freeze and causes it. Also, I thought having no swap would cause an out-of-memory error, my bad, so maybe that could be it.  I'll check tomorrow from the computer at work if limiting the memory for the process helps to avoid the freeze.  \r\n\r\nNonetheless, the entire training set is 160mb if in float32. The output of the convolutional layers are 1.3gb and 650mb respectively. 214mb for the first fully connected and just 2 megs for the output, for a total of ~2.5gb total. The memory required for the weights is negligible. It seems weird the activations of the intermediate layers would take that much space. "}