{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/268037463", "html_url": "https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-268037463", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6019", "id": 268037463, "node_id": "MDEyOklzc3VlQ29tbWVudDI2ODAzNzQ2Mw==", "user": {"login": "facundoq", "id": 1576711, "node_id": "MDQ6VXNlcjE1NzY3MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1576711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facundoq", "html_url": "https://github.com/facundoq", "followers_url": "https://api.github.com/users/facundoq/followers", "following_url": "https://api.github.com/users/facundoq/following{/other_user}", "gists_url": "https://api.github.com/users/facundoq/gists{/gist_id}", "starred_url": "https://api.github.com/users/facundoq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facundoq/subscriptions", "organizations_url": "https://api.github.com/users/facundoq/orgs", "repos_url": "https://api.github.com/users/facundoq/repos", "events_url": "https://api.github.com/users/facundoq/events{/privacy}", "received_events_url": "https://api.github.com/users/facundoq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-19T18:18:35Z", "updated_at": "2016-12-19T18:18:35Z", "author_association": "NONE", "body_html": "<p>Yes, a fused conv+add could store the activation A(b_conv1), and do A(b_conv1)-b_conv1 in the backward pass (again, in-place) to obtain A(h_conv1) again! That would surely help for big models even with small batch sizes.</p>\n<p>Anyhow, in this case I guess the sane thing to do is to use a tf input queue that would automatically batch stuff or calculate the full training set accuracy by feeding small batches which is easy since it's a reduction op.</p>", "body_text": "Yes, a fused conv+add could store the activation A(b_conv1), and do A(b_conv1)-b_conv1 in the backward pass (again, in-place) to obtain A(h_conv1) again! That would surely help for big models even with small batch sizes.\nAnyhow, in this case I guess the sane thing to do is to use a tf input queue that would automatically batch stuff or calculate the full training set accuracy by feeding small batches which is easy since it's a reduction op.", "body": "Yes, a fused conv+add could store the activation A(b_conv1), and do A(b_conv1)-b_conv1 in the backward pass (again, in-place) to obtain A(h_conv1) again! That would surely help for big models even with small batch sizes. \r\n\r\nAnyhow, in this case I guess the sane thing to do is to use a tf input queue that would automatically batch stuff or calculate the full training set accuracy by feeding small batches which is easy since it's a reduction op."}