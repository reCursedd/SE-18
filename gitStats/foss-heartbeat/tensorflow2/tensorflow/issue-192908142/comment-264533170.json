{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264533170", "html_url": "https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264533170", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6019", "id": 264533170, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDUzMzE3MA==", "user": {"login": "facundoq", "id": 1576711, "node_id": "MDQ6VXNlcjE1NzY3MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1576711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facundoq", "html_url": "https://github.com/facundoq", "followers_url": "https://api.github.com/users/facundoq/followers", "following_url": "https://api.github.com/users/facundoq/following{/other_user}", "gists_url": "https://api.github.com/users/facundoq/gists{/gist_id}", "starred_url": "https://api.github.com/users/facundoq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facundoq/subscriptions", "organizations_url": "https://api.github.com/users/facundoq/orgs", "repos_url": "https://api.github.com/users/facundoq/repos", "events_url": "https://api.github.com/users/facundoq/events{/privacy}", "received_events_url": "https://api.github.com/users/facundoq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-02T18:56:35Z", "updated_at": "2016-12-02T18:57:17Z", "author_association": "NONE", "body_html": "<p>Further testing with a really simple network: conv(4) - pool(2x2) - fc(10) (that's 4 filters for the convolutional layer) full code available in <a href=\"http://pastebin.com/Zh14pS3C\" rel=\"nofollow\">http://pastebin.com/Zh14pS3C</a>.<br>\nI removed the training, accuracy evaluation, and most layers from the net, while also reducing the filters for the convolutional layer and the relu. For the whole training dataset this still requires <strong>6.5gb</strong> of memory. There's a 1gb decrease if I evaluate with 10000 less training examples, 2gb for 20000, and so on. Is this normal?</p>", "body_text": "Further testing with a really simple network: conv(4) - pool(2x2) - fc(10) (that's 4 filters for the convolutional layer) full code available in http://pastebin.com/Zh14pS3C.\nI removed the training, accuracy evaluation, and most layers from the net, while also reducing the filters for the convolutional layer and the relu. For the whole training dataset this still requires 6.5gb of memory. There's a 1gb decrease if I evaluate with 10000 less training examples, 2gb for 20000, and so on. Is this normal?", "body": "Further testing with a really simple network: conv(4) - pool(2x2) - fc(10) (that's 4 filters for the convolutional layer) full code available in http://pastebin.com/Zh14pS3C.\r\nI removed the training, accuracy evaluation, and most layers from the net, while also reducing the filters for the convolutional layer and the relu. For the whole training dataset this still requires **6.5gb** of memory. There's a 1gb decrease if I evaluate with 10000 less training examples, 2gb for 20000, and so on. Is this normal?"}