{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/268027443", "html_url": "https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-268027443", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6019", "id": 268027443, "node_id": "MDEyOklzc3VlQ29tbWVudDI2ODAyNzQ0Mw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-19T17:37:25Z", "updated_at": "2016-12-19T17:37:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Actually if you look at <a href=\"https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb\">memory timeline</a> you'll see that activations for <code>h_conv1</code> are discarded as soon as <code>h_conv1+b_conv1</code> completes. This is because it's eval pass, so they are not needed for derivatives. If <code>add</code> could be done in place this would essentially lower the peak usage from 12GB to 6GB, and that's perhaps what the upcoming XLA framework could do. However, it would be trickier to lower usage for the backward pass as well since <code>h_conv1</code> activations are needed until much later -- you would need an implementation of fused <code>conv + add</code> op, and a corresponding gradient.</p>", "body_text": "Actually if you look at memory timeline you'll see that activations for h_conv1 are discarded as soon as h_conv1+b_conv1 completes. This is because it's eval pass, so they are not needed for derivatives. If add could be done in place this would essentially lower the peak usage from 12GB to 6GB, and that's perhaps what the upcoming XLA framework could do. However, it would be trickier to lower usage for the backward pass as well since h_conv1 activations are needed until much later -- you would need an implementation of fused conv + add op, and a corresponding gradient.", "body": "Actually if you look at [memory timeline](https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb) you'll see that activations for `h_conv1` are discarded as soon as `h_conv1+b_conv1` completes. This is because it's eval pass, so they are not needed for derivatives. If `add` could be done in place this would essentially lower the peak usage from 12GB to 6GB, and that's perhaps what the upcoming XLA framework could do. However, it would be trickier to lower usage for the backward pass as well since `h_conv1` activations are needed until much later -- you would need an implementation of fused `conv + add` op, and a corresponding gradient."}