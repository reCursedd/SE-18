{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264912948", "html_url": "https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264912948", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6019", "id": 264912948, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDkxMjk0OA==", "user": {"login": "facundoq", "id": 1576711, "node_id": "MDQ6VXNlcjE1NzY3MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1576711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facundoq", "html_url": "https://github.com/facundoq", "followers_url": "https://api.github.com/users/facundoq/followers", "following_url": "https://api.github.com/users/facundoq/following{/other_user}", "gists_url": "https://api.github.com/users/facundoq/gists{/gist_id}", "starred_url": "https://api.github.com/users/facundoq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facundoq/subscriptions", "organizations_url": "https://api.github.com/users/facundoq/orgs", "repos_url": "https://api.github.com/users/facundoq/repos", "events_url": "https://api.github.com/users/facundoq/events{/privacy}", "received_events_url": "https://api.github.com/users/facundoq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-05T17:07:41Z", "updated_at": "2016-12-05T17:09:14Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> Running with tcmalloc decreased the memory usage slightly but not entirely.</p>\n<p>I tried the tracer but it seems there's only cpu use info.</p>\n<p>Here's the output from <code>print(run_metadata)</code>, <a href=\"http://pastebin.com/tXwB7Lfn\" rel=\"nofollow\">http://pastebin.com/tXwB7Lfn</a> when testing with <strong>4000</strong> training examples (so that the run() completes).</p>\n<p>The relevant? bits from the memory subsection:<br>\nThe Conv2D allocates <code>total_bytes: 25088000</code> (~25mb) (28<em>28</em>4000<em>4</em>2)<br>\nThe Add allocates <code>total_bytes: 25088000</code> (~25mb) (28<em>28</em>4000<em>4</em>2)<br>\nThe MaxPool allocates <code>total_bytes: 6272000</code> (~6mb) (14<em>14</em>4000<em>4</em>2)<br>\nThe MatMul allocates <code>total_bytes: 160000</code> (~160k) (10<em>4000</em>4)<br>\nThe Add_1 allocates <code>total_bytes: 160000</code> (~160k) (10<em>4000</em>4)<br>\nThe SoftmaxCrossEntropyWithLogits allocates <code>total_bytes: 192000</code> (~192k) (10<em>4000</em>4+4000*8)</p>\n<p>There are also a lot of  \"Shape\", \"Slice\", most of which allocate just 4, 8, 16 or 200 bytes. Other nodes of the sort allocate bytes 15680, 15680, 12544000, 6272000, 160000, 160000, but all of these add up to at most 20mb. So if with 4000 samples the \"use\" is less than 100mb, with 55000 it would be about 1350mb or 1.35gb.</p>\n<p>Note: Are the 'total bytes' and 'requested_bytes' added up? if so the conv, add and maxpool layers occupy twice as much.</p>", "body_text": "@yaroslavvb Running with tcmalloc decreased the memory usage slightly but not entirely.\nI tried the tracer but it seems there's only cpu use info.\nHere's the output from print(run_metadata), http://pastebin.com/tXwB7Lfn when testing with 4000 training examples (so that the run() completes).\nThe relevant? bits from the memory subsection:\nThe Conv2D allocates total_bytes: 25088000 (~25mb) (2828400042)\nThe Add allocates total_bytes: 25088000 (~25mb) (2828400042)\nThe MaxPool allocates total_bytes: 6272000 (~6mb) (1414400042)\nThe MatMul allocates total_bytes: 160000 (~160k) (1040004)\nThe Add_1 allocates total_bytes: 160000 (~160k) (1040004)\nThe SoftmaxCrossEntropyWithLogits allocates total_bytes: 192000 (~192k) (1040004+4000*8)\nThere are also a lot of  \"Shape\", \"Slice\", most of which allocate just 4, 8, 16 or 200 bytes. Other nodes of the sort allocate bytes 15680, 15680, 12544000, 6272000, 160000, 160000, but all of these add up to at most 20mb. So if with 4000 samples the \"use\" is less than 100mb, with 55000 it would be about 1350mb or 1.35gb.\nNote: Are the 'total bytes' and 'requested_bytes' added up? if so the conv, add and maxpool layers occupy twice as much.", "body": "@yaroslavvb Running with tcmalloc decreased the memory usage slightly but not entirely.\r\n\r\nI tried the tracer but it seems there's only cpu use info.\r\n\r\nHere's the output from `print(run_metadata)`, http://pastebin.com/tXwB7Lfn when testing with **4000** training examples (so that the run() completes).\r\n\r\nThe relevant? bits from the memory subsection:\r\nThe Conv2D allocates `total_bytes: 25088000` (~25mb) (28*28*4000*4*2) \r\nThe Add allocates `total_bytes: 25088000` (~25mb) (28*28*4000*4*2)\r\nThe MaxPool allocates `total_bytes: 6272000` (~6mb) (14*14*4000*4*2)\r\nThe MatMul allocates `total_bytes: 160000` (~160k) (10*4000*4)\r\nThe Add_1 allocates `total_bytes: 160000` (~160k) (10*4000*4)\r\nThe SoftmaxCrossEntropyWithLogits allocates `total_bytes: 192000` (~192k) (10*4000*4+4000*8)\r\n\r\nThere are also a lot of  \"Shape\", \"Slice\", most of which allocate just 4, 8, 16 or 200 bytes. Other nodes of the sort allocate bytes 15680, 15680, 12544000, 6272000, 160000, 160000, but all of these add up to at most 20mb. So if with 4000 samples the \"use\" is less than 100mb, with 55000 it would be about 1350mb or 1.35gb. \r\n\r\nNote: Are the 'total bytes' and 'requested_bytes' added up? if so the conv, add and maxpool layers occupy twice as much. \r\n"}