{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289732269", "html_url": "https://github.com/tensorflow/tensorflow/pull/8662#issuecomment-289732269", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8662", "id": 289732269, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTczMjI2OQ==", "user": {"login": "Itaydal", "id": 5824376, "node_id": "MDQ6VXNlcjU4MjQzNzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/5824376?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Itaydal", "html_url": "https://github.com/Itaydal", "followers_url": "https://api.github.com/users/Itaydal/followers", "following_url": "https://api.github.com/users/Itaydal/following{/other_user}", "gists_url": "https://api.github.com/users/Itaydal/gists{/gist_id}", "starred_url": "https://api.github.com/users/Itaydal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Itaydal/subscriptions", "organizations_url": "https://api.github.com/users/Itaydal/orgs", "repos_url": "https://api.github.com/users/Itaydal/repos", "events_url": "https://api.github.com/users/Itaydal/events{/privacy}", "received_events_url": "https://api.github.com/users/Itaydal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-28T10:46:29Z", "updated_at": "2017-03-28T10:46:29Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a>, great contribution! just ran into this issue myself few days ago.<br>\nI've compiled fork with you'r fix, and I think I've found some strange problem,<br>\nWhen i try to use tf.self_adjoint_eig over a large tensor of type tf.float64 (large is [2e7, 4, 4]), the work gets distributed between my 8 CPU cores.<br>\nBut when do the same with tensor of type tf.complex64 (with the same values) tf uses only single CPU core.<br>\nWhat could be the cause of this problem?</p>\n<p>Test code:</p>\n<pre><code>def eig_test(H, w):\n    H_tens = tf.constant(H[:, :, np.newaxis], dtype=tf.float64) \n    w_tens = tf.constant(w, dtype=tf.float64)\n    H_tens = tf.multiply(w_tens, H_tens)\n    H_tens = tf.transpose(H_tens, perm=[2, 0, 1])\n    \n    (e_tens, v_tens) = tf.self_adjoint_eig(H_tens)\n    \n    sess = tf.Session()\n    return sess.run(e_tens)\n\nH = np.array([[1, 2, 3, 4], [2, 1, 5, 6], [3, 5, 1, 7], [4, 6, 7, 1]])\nw = np.random.rand(20000000)\n    \nstart_time = time()\nres = eig_test(H, w)\nprint 'duration', time() - start_time\n</code></pre>\n<p>Test results:<br>\nwith type tf.complex64 - test duration was 45 sec<br>\nwith type tf.float64 - test duration was 10 sec</p>", "body_text": "Hi @martinwicke, great contribution! just ran into this issue myself few days ago.\nI've compiled fork with you'r fix, and I think I've found some strange problem,\nWhen i try to use tf.self_adjoint_eig over a large tensor of type tf.float64 (large is [2e7, 4, 4]), the work gets distributed between my 8 CPU cores.\nBut when do the same with tensor of type tf.complex64 (with the same values) tf uses only single CPU core.\nWhat could be the cause of this problem?\nTest code:\ndef eig_test(H, w):\n    H_tens = tf.constant(H[:, :, np.newaxis], dtype=tf.float64) \n    w_tens = tf.constant(w, dtype=tf.float64)\n    H_tens = tf.multiply(w_tens, H_tens)\n    H_tens = tf.transpose(H_tens, perm=[2, 0, 1])\n    \n    (e_tens, v_tens) = tf.self_adjoint_eig(H_tens)\n    \n    sess = tf.Session()\n    return sess.run(e_tens)\n\nH = np.array([[1, 2, 3, 4], [2, 1, 5, 6], [3, 5, 1, 7], [4, 6, 7, 1]])\nw = np.random.rand(20000000)\n    \nstart_time = time()\nres = eig_test(H, w)\nprint 'duration', time() - start_time\n\nTest results:\nwith type tf.complex64 - test duration was 45 sec\nwith type tf.float64 - test duration was 10 sec", "body": "Hi @martinwicke, great contribution! just ran into this issue myself few days ago.\r\nI've compiled fork with you'r fix, and I think I've found some strange problem,\r\nWhen i try to use tf.self_adjoint_eig over a large tensor of type tf.float64 (large is [2e7, 4, 4]), the work gets distributed between my 8 CPU cores. \r\nBut when do the same with tensor of type tf.complex64 (with the same values) tf uses only single CPU core.\r\nWhat could be the cause of this problem?\r\n\r\nTest code:\r\n\r\n```\r\ndef eig_test(H, w):\r\n    H_tens = tf.constant(H[:, :, np.newaxis], dtype=tf.float64) \r\n    w_tens = tf.constant(w, dtype=tf.float64)\r\n    H_tens = tf.multiply(w_tens, H_tens)\r\n    H_tens = tf.transpose(H_tens, perm=[2, 0, 1])\r\n    \r\n    (e_tens, v_tens) = tf.self_adjoint_eig(H_tens)\r\n    \r\n    sess = tf.Session()\r\n    return sess.run(e_tens)\r\n\r\nH = np.array([[1, 2, 3, 4], [2, 1, 5, 6], [3, 5, 1, 7], [4, 6, 7, 1]])\r\nw = np.random.rand(20000000)\r\n    \r\nstart_time = time()\r\nres = eig_test(H, w)\r\nprint 'duration', time() - start_time\r\n```\r\n\r\nTest results:\r\nwith type tf.complex64 - test duration was 45 sec\r\nwith type tf.float64 - test duration was 10 sec"}