{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6858", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6858/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6858/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6858/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6858", "id": 200852474, "node_id": "MDU6SXNzdWUyMDA4NTI0NzQ=", "number": 6858, "title": "Support for mixed precision gradients", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-01-15T06:35:30Z", "updated_at": "2017-01-17T23:04:25Z", "closed_at": "2017-01-17T23:04:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>To save memory while keeping precision it's useful to be allow functions that store activations as fp16, but compute gradients in fp32. We are working on custom ops that compute gradients in mixed mode, but it requires hacks on Python gradient type checks side to working. The feature request is to relax these checks.</p>\n<p>Here's an example computation graph with mixed precision gradient. Forward propagation is done in fp16, and backprop is in fp32 as in the graph below<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/21962339/52c99a62-dad8-11e6-99a8-d034abd6c1af.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/21962339/52c99a62-dad8-11e6-99a8-d034abd6c1af.png\" alt=\"mixed-grads\" style=\"max-width:100%;\"></a></p>\n<p>Here's a toy example that creates such a g</p>\n<p>Graph:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.framework import function\n\n@function.Defun(tf.float16, tf.float32)\ndef custom_grad(x, grad):\n    return 2*tf.cast(x, tf.float32)*grad\n\n@function.Defun(tf.float16, grad_func=custom_grad)\ndef custom(x):\n    return tf.square(x)\n\nx = tf.Variable(1., dtype=tf.float16)\n\n# override cast to keep first backprop in fp32 \nwith tf.get_default_graph().gradient_override_map({\"Cast\": \"Identity\"}):\n    loss = tf.cast(custom(x, name=\"mycustom_apply\"), tf.float32)\n    \ngradient = tf.gradients(loss, x)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(gradient)\n</code></pre>\n<p>Currently the example above crashes because various places in code assumes that grad function output matches type of incoming activation.</p>\n<p>in particular:</p>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/cb17d1b0e2b581b5da1d9597b7929ba764749d38/tensorflow/python/ops/gradients_impl.py#L260\">tensorflow/python/ops/gradients_impl.py</a>, line 264, in _VerifyGeneratedGradients: enforces that gradient output type is the same as activation input type</li>\n</ul>", "body_text": "To save memory while keeping precision it's useful to be allow functions that store activations as fp16, but compute gradients in fp32. We are working on custom ops that compute gradients in mixed mode, but it requires hacks on Python gradient type checks side to working. The feature request is to relax these checks.\nHere's an example computation graph with mixed precision gradient. Forward propagation is done in fp16, and backprop is in fp32 as in the graph below\n\nHere's a toy example that creates such a g\nGraph:\nimport tensorflow as tf\nfrom tensorflow.python.framework import function\n\n@function.Defun(tf.float16, tf.float32)\ndef custom_grad(x, grad):\n    return 2*tf.cast(x, tf.float32)*grad\n\n@function.Defun(tf.float16, grad_func=custom_grad)\ndef custom(x):\n    return tf.square(x)\n\nx = tf.Variable(1., dtype=tf.float16)\n\n# override cast to keep first backprop in fp32 \nwith tf.get_default_graph().gradient_override_map({\"Cast\": \"Identity\"}):\n    loss = tf.cast(custom(x, name=\"mycustom_apply\"), tf.float32)\n    \ngradient = tf.gradients(loss, x)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(gradient)\n\nCurrently the example above crashes because various places in code assumes that grad function output matches type of incoming activation.\nin particular:\n\ntensorflow/python/ops/gradients_impl.py, line 264, in _VerifyGeneratedGradients: enforces that gradient output type is the same as activation input type", "body": "To save memory while keeping precision it's useful to be allow functions that store activations as fp16, but compute gradients in fp32. We are working on custom ops that compute gradients in mixed mode, but it requires hacks on Python gradient type checks side to working. The feature request is to relax these checks.\r\n\r\nHere's an example computation graph with mixed precision gradient. Forward propagation is done in fp16, and backprop is in fp32 as in the graph below \r\n![mixed-grads](https://cloud.githubusercontent.com/assets/23068/21962339/52c99a62-dad8-11e6-99a8-d034abd6c1af.png)\r\n\r\n\r\nHere's a toy example that creates such a g\r\n\r\nGraph:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import function\r\n\r\n@function.Defun(tf.float16, tf.float32)\r\ndef custom_grad(x, grad):\r\n    return 2*tf.cast(x, tf.float32)*grad\r\n\r\n@function.Defun(tf.float16, grad_func=custom_grad)\r\ndef custom(x):\r\n    return tf.square(x)\r\n\r\nx = tf.Variable(1., dtype=tf.float16)\r\n\r\n# override cast to keep first backprop in fp32 \r\nwith tf.get_default_graph().gradient_override_map({\"Cast\": \"Identity\"}):\r\n    loss = tf.cast(custom(x, name=\"mycustom_apply\"), tf.float32)\r\n    \r\ngradient = tf.gradients(loss, x)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(gradient)\r\n```\r\n\r\nCurrently the example above crashes because various places in code assumes that grad function output matches type of incoming activation.\r\n\r\nin particular:\r\n\r\n- [tensorflow/python/ops/gradients_impl.py](https://github.com/tensorflow/tensorflow/blob/cb17d1b0e2b581b5da1d9597b7929ba764749d38/tensorflow/python/ops/gradients_impl.py#L260), line 264, in _VerifyGeneratedGradients: enforces that gradient output type is the same as activation input type"}