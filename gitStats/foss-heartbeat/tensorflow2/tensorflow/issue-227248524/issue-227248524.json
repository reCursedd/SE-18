{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9779", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9779/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9779/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9779/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9779", "id": 227248524, "node_id": "MDU6SXNzdWUyMjcyNDg1MjQ=", "number": 9779, "title": "memory leak when implement rnn attention decoder", "user": {"login": "liusiye", "id": 12574263, "node_id": "MDQ6VXNlcjEyNTc0MjYz", "avatar_url": "https://avatars1.githubusercontent.com/u/12574263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liusiye", "html_url": "https://github.com/liusiye", "followers_url": "https://api.github.com/users/liusiye/followers", "following_url": "https://api.github.com/users/liusiye/following{/other_user}", "gists_url": "https://api.github.com/users/liusiye/gists{/gist_id}", "starred_url": "https://api.github.com/users/liusiye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liusiye/subscriptions", "organizations_url": "https://api.github.com/users/liusiye/orgs", "repos_url": "https://api.github.com/users/liusiye/repos", "events_url": "https://api.github.com/users/liusiye/events{/privacy}", "received_events_url": "https://api.github.com/users/liusiye/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-05-09T04:42:12Z", "updated_at": "2018-11-22T18:51:47Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>== cat /etc/issue ===============================================<br>\nLinux quad 4.4.0-72-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116118515\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/93\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/93/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/93\">#93</a>-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux<br>\nVERSION=\"16.04 LTS (Xenial Xerus)\"<br>\nVERSION_ID=\"16.04\"</p>\n<p>== are we in docker =============================================<br>\nNo</p>\n<p>== compiler =====================================================<br>\nc++ (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4<br>\nCopyright (C) 2015 Free Software Foundation, Inc.<br>\nThis is free software; see the source for copying conditions.  There is NO<br>\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>\n<p>== uname -a =====================================================<br>\nLinux quad 4.4.0-72-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116118515\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/93\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/93/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/93\">#93</a>-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</p>\n<p>== check pips ===================================================<br>\nnumpy (1.12.1)<br>\nprotobuf (3.3.0)<br>\ntensorflow-gpu (1.1.0)</p>\n<p>== check for virtualenv =========================================<br>\nTrue</p>\n<p>== tensorflow import ============================================<br>\ntf.VERSION = 1.1.0<br>\ntf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5<br>\ntf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5<br>\nSanity check: array([1], dtype=int32)</p>\n<p>== env ==========================================================<br>\nLD_LIBRARY_PATH is unset<br>\nDYLD_LIBRARY_PATH is unset</p>\n<p>== nvidia-smi ===================================================<br>\nTue May  9 12:16:54 2017<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |<br>\n| 22%   47C    P0    76W / 250W |      0MiB / 12205MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |<br>\n| 22%   60C    P2   129W / 250W |  11713MiB / 12207MiB |     80%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |<br>\n| 22%   49C    P0    83W / 250W |      0MiB / 12207MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |<br>\n| 24%   63C    P2   117W / 250W |  11713MiB / 12207MiB |     70%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID  Type  Process name                               Usage      |<br>\n|=============================================================================|<br>\n|    1     21558    C   python3                                      11709MiB |<br>\n|    3     21346    C   python3                                      11709MiB |<br>\n+-----------------------------------------------------------------------------+</p>\n<p>== cuda libs  ===================================================<br>\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7<br>\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7<br>\n/usr/local/cuda-7.5/lib64/libcudart_static.a<br>\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18<br>\n/usr/local/cuda-7.5/lib/libcudart_static.a<br>\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7<br>\n/usr/local/cuda-8.0/lib64/libcudart_static.a<br>\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27</p>\n<h3>Describe the problem</h3>\n<p>When we try to implement a complex network which contain a rnn attention decoder, It will consume all the memory after several days. I extract the decoder in a test file, the memory still grow in a slower speed. also found that if change softmax to sigmoid, memory doesn't leak.</p>\n<h3>Source code / logs</h3>\n<p>test code:</p>\n<pre><code># __author__ = \"liusiye\"\n# -*- coding: utf-8 -*-\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell, MultiRNNCell\nfrom os import getpid\nimport psutil\nimport gc\nimport tensorflow as tf\nimport numpy as np\n\n\nprocess = psutil.Process(getpid())\nB, T, H = 20, 60, 256\nlayer_num = 4\n\ndef apply_attention(encoding, rnn_output):\n    ''' encoding: [t, b, h1]\n        rnn_output: [b, h2]\n    '''\n    T, B, H1 = encoding.get_shape().as_list()\n    _, H2 = rnn_output.get_shape().as_list()\n    with tf.variable_scope('attention'):\n        w_encoder = tf.get_variable(\n            name='W_encoder',\n            shape=[H1, H1],\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\n        w_decoder = tf.get_variable(\n            name='W_decoder',\n            shape=[H2, H1],\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\n        w_attention = tf.get_variable(\n            name='W_attention',\n            shape=[H1, 1],\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\n    r_decoder = tf.matmul(rnn_output, w_decoder)  # [b, h1]\n\n    r_encoder = tf.matmul(tf.reshape(encoding, [-1, H1]), w_encoder)\n    r_encoder = tf.reshape(r_encoder, [T, B, H1])\n    # [t, b, h] -&gt; [t * b, h] -&gt; [t, b, h]\n\n    r_attention = tf.tanh(r_encoder + r_decoder)  # [t, b, h1]\n    attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)\n    attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)\n    #attention = tf.nn.sigmoid(tf.reshape(attention, [T, B]))\n    encoding = tf.transpose(encoding, perm=[2, 0, 1])  # [t, b, h1]-&gt;[h1, t, b]\n\n    context = tf.reduce_sum(encoding * attention, axis=1)  # [h1, b]\n    return tf.transpose(context)  # [b, h1]\n\ndef rnn_attention_decoder_test():\n    encoding = tf.get_variable(name='encoding', shape=[T, B, H], dtype=tf.float32)\n    rnn_outputs = []  # t * [b, h]\n    scope = tf.get_variable_scope()\n\n    zero_input = tf.constant(0, shape=[B, H], dtype=tf.float32)\n    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(H) for i in range(layer_num)], state_is_tuple=True)\n    state = cell.zero_state(B, tf.float32)\n    with tf.variable_scope(scope) as outer_scope:\n        for t in range(T):#T):\n            attention_input = zero_input\n            rnn_input = apply_attention(encoding, attention_input)\n            rnn_output, state = cell(rnn_input, state)\n            rnn_outputs.append(rnn_output)\n            outer_scope.reuse_variables()\n    return tf.stack(rnn_outputs, axis=1), state  # t * [b, h] -&gt; [b, t, h]\n\nwith tf.Session() as sess, tf.variable_scope('model'):\n    with tf.variable_scope('model', reuse=None):\n        tensor_lists_test = rnn_attention_decoder_test()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n    sess.run(init_op)\n    sess.graph.finalize()\n    for step in range(100000):\n        after = process.memory_percent()\n        if step &gt; 0:\n            print(\"MEMORY CHANGE %.7f -&gt; %.7f\" % (before, after))\n        before = process.memory_percent()\n        sess.run(tensor_lists_test)\n        gc.collect()\n</code></pre>\n<p>log:</p>\n<pre><code>2017-05-09 09:27:55.435870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435903: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435909: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435913: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.732204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\npciBusID 0000:09:00.0\nTotal memory: 11.92GiB\nFree memory: 11.81GiB\n2017-05-09 09:27:55.732232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-09 09:27:55.732238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-09 09:27:55.732247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nMEMORY CHANGE 1.1297021 -&gt; 1.3666840\nMEMORY CHANGE 1.3666840 -&gt; 1.3666840\nMEMORY CHANGE 1.3666840 -&gt; 1.3697929\nMEMORY CHANGE 1.3697929 -&gt; 1.3697929\nMEMORY CHANGE 1.3697929 -&gt; 1.3729200\nMEMORY CHANGE 1.3729200 -&gt; 1.3733208\nMEMORY CHANGE 1.3733208 -&gt; 1.3733208\nMEMORY CHANGE 1.3733208 -&gt; 1.3737215\nMEMORY CHANGE 1.3737215 -&gt; 1.3768487\nMEMORY CHANGE 1.3768487 -&gt; 1.3799758\nMEMORY CHANGE 1.3799758 -&gt; 1.3803644\nMEMORY CHANGE 1.3803644 -&gt; 1.3834976\nMEMORY CHANGE 1.3834976 -&gt; 1.3834976\nMEMORY CHANGE 1.3834976 -&gt; 1.3838862\nMEMORY CHANGE 1.3838862 -&gt; 1.3838862\nMEMORY CHANGE 1.3838862 -&gt; 1.3842870\nMEMORY CHANGE 1.3842870 -&gt; 1.3846878\nMEMORY CHANGE 1.3846878 -&gt; 1.3850885\nMEMORY CHANGE 1.3850885 -&gt; 1.3850885\nMEMORY CHANGE 1.3850885 -&gt; 1.3850885\nMEMORY CHANGE 1.3850885 -&gt; 1.3850885\nMEMORY CHANGE 1.3850885 -&gt; 1.3850885\nMEMORY CHANGE 1.3850885 -&gt; 1.3854771\nMEMORY CHANGE 1.3854771 -&gt; 1.3854771\nMEMORY CHANGE 1.3854771 -&gt; 1.3854771\nMEMORY CHANGE 1.3854771 -&gt; 1.3858779\nMEMORY CHANGE 1.3858779 -&gt; 1.3858779\nMEMORY CHANGE 1.3858779 -&gt; 1.3858779\nMEMORY CHANGE 1.3858779 -&gt; 1.3858779\nMEMORY CHANGE 1.3858779 -&gt; 1.3858779\nMEMORY CHANGE 1.3858779 -&gt; 1.3858779\nMEMORY CHANGE 1.3858779 -&gt; 1.3862665\nMEMORY CHANGE 1.3862665 -&gt; 1.3866673\nMEMORY CHANGE 1.3866673 -&gt; 1.3866673\nMEMORY CHANGE 1.3866673 -&gt; 1.3870680\nMEMORY CHANGE 1.3870680 -&gt; 1.3870680\nMEMORY CHANGE 1.3870680 -&gt; 1.3870680\nMEMORY CHANGE 1.3870680 -&gt; 1.3870680\nMEMORY CHANGE 1.3870680 -&gt; 1.3870680\nMEMORY CHANGE 1.3870680 -&gt; 1.3870680\nMEMORY CHANGE 1.3870680 -&gt; 1.3870680\nMEMORY CHANGE 1.3870680 -&gt; 1.3874688\nMEMORY CHANGE 1.3874688 -&gt; 1.3874688\nMEMORY CHANGE 1.3874688 -&gt; 1.3878695\nMEMORY CHANGE 1.3878695 -&gt; 1.3878695\nMEMORY CHANGE 1.3878695 -&gt; 1.3882581\nMEMORY CHANGE 1.3882581 -&gt; 1.3882581\nMEMORY CHANGE 1.3882581 -&gt; 1.3886589\nMEMORY CHANGE 1.3886589 -&gt; 1.3886589\nMEMORY CHANGE 1.3886589 -&gt; 1.3890597\nMEMORY CHANGE 1.3890597 -&gt; 1.3894604\nMEMORY CHANGE 1.3894604 -&gt; 1.3894604\nMEMORY CHANGE 1.3894604 -&gt; 1.3898612\nMEMORY CHANGE 1.3898612 -&gt; 1.3898612\nMEMORY CHANGE 1.3898612 -&gt; 1.3902619\nMEMORY CHANGE 1.3902619 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3906627\nMEMORY CHANGE 1.3906627 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3937898\nMEMORY CHANGE 1.3937898 -&gt; 1.3941906\nMEMORY CHANGE 1.3941906 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3945913\nMEMORY CHANGE 1.3945913 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3949921\nMEMORY CHANGE 1.3949921 -&gt; 1.3953929\nMEMORY CHANGE 1.3953929 -&gt; 1.3953929\nMEMORY CHANGE 1.3953929 -&gt; 1.3953929\nMEMORY CHANGE 1.3953929 -&gt; 1.3953929\nMEMORY CHANGE 1.3953929 -&gt; 1.3953929\nMEMORY CHANGE 1.3953929 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3957936\nMEMORY CHANGE 1.3957936 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3961944\nMEMORY CHANGE 1.3961944 -&gt; 1.3965951\nMEMORY CHANGE 1.3965951 -&gt; 1.3965951\nMEMORY CHANGE 1.3965951 -&gt; 1.3965951\nMEMORY CHANGE 1.3965951 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3969959\nMEMORY CHANGE 1.3969959 -&gt; 1.3973967\nMEMORY CHANGE 1.3973967 -&gt; 1.3973967\nMEMORY CHANGE 1.3973967 -&gt; 1.3973967\nMEMORY CHANGE 1.3973967 -&gt; 1.3973967\nMEMORY CHANGE 1.3973967 -&gt; 1.3977974\nMEMORY CHANGE 1.3977974 -&gt; 1.3977974\n</code></pre>", "body_text": "System information\n== cat /etc/issue ===============================================\nLinux quad 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\n== are we in docker =============================================\nNo\n== compiler =====================================================\nc++ (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n== uname -a =====================================================\nLinux quad 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n== check pips ===================================================\nnumpy (1.12.1)\nprotobuf (3.3.0)\ntensorflow-gpu (1.1.0)\n== check for virtualenv =========================================\nTrue\n== tensorflow import ============================================\ntf.VERSION = 1.1.0\ntf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5\ntf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5\nSanity check: array([1], dtype=int32)\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n== nvidia-smi ===================================================\nTue May  9 12:16:54 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |\n| 22%   47C    P0    76W / 250W |      0MiB / 12205MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |\n| 22%   60C    P2   129W / 250W |  11713MiB / 12207MiB |     80%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   49C    P0    83W / 250W |      0MiB / 12207MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |\n| 24%   63C    P2   117W / 250W |  11713MiB / 12207MiB |     70%      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    1     21558    C   python3                                      11709MiB |\n|    3     21346    C   python3                                      11709MiB |\n+-----------------------------------------------------------------------------+\n== cuda libs  ===================================================\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\n/usr/local/cuda-7.5/lib64/libcudart_static.a\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib/libcudart_static.a\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/lib64/libcudart_static.a\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\nDescribe the problem\nWhen we try to implement a complex network which contain a rnn attention decoder, It will consume all the memory after several days. I extract the decoder in a test file, the memory still grow in a slower speed. also found that if change softmax to sigmoid, memory doesn't leak.\nSource code / logs\ntest code:\n# __author__ = \"liusiye\"\n# -*- coding: utf-8 -*-\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell, MultiRNNCell\nfrom os import getpid\nimport psutil\nimport gc\nimport tensorflow as tf\nimport numpy as np\n\n\nprocess = psutil.Process(getpid())\nB, T, H = 20, 60, 256\nlayer_num = 4\n\ndef apply_attention(encoding, rnn_output):\n    ''' encoding: [t, b, h1]\n        rnn_output: [b, h2]\n    '''\n    T, B, H1 = encoding.get_shape().as_list()\n    _, H2 = rnn_output.get_shape().as_list()\n    with tf.variable_scope('attention'):\n        w_encoder = tf.get_variable(\n            name='W_encoder',\n            shape=[H1, H1],\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\n        w_decoder = tf.get_variable(\n            name='W_decoder',\n            shape=[H2, H1],\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\n        w_attention = tf.get_variable(\n            name='W_attention',\n            shape=[H1, 1],\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\n    r_decoder = tf.matmul(rnn_output, w_decoder)  # [b, h1]\n\n    r_encoder = tf.matmul(tf.reshape(encoding, [-1, H1]), w_encoder)\n    r_encoder = tf.reshape(r_encoder, [T, B, H1])\n    # [t, b, h] -> [t * b, h] -> [t, b, h]\n\n    r_attention = tf.tanh(r_encoder + r_decoder)  # [t, b, h1]\n    attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)\n    attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)\n    #attention = tf.nn.sigmoid(tf.reshape(attention, [T, B]))\n    encoding = tf.transpose(encoding, perm=[2, 0, 1])  # [t, b, h1]->[h1, t, b]\n\n    context = tf.reduce_sum(encoding * attention, axis=1)  # [h1, b]\n    return tf.transpose(context)  # [b, h1]\n\ndef rnn_attention_decoder_test():\n    encoding = tf.get_variable(name='encoding', shape=[T, B, H], dtype=tf.float32)\n    rnn_outputs = []  # t * [b, h]\n    scope = tf.get_variable_scope()\n\n    zero_input = tf.constant(0, shape=[B, H], dtype=tf.float32)\n    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(H) for i in range(layer_num)], state_is_tuple=True)\n    state = cell.zero_state(B, tf.float32)\n    with tf.variable_scope(scope) as outer_scope:\n        for t in range(T):#T):\n            attention_input = zero_input\n            rnn_input = apply_attention(encoding, attention_input)\n            rnn_output, state = cell(rnn_input, state)\n            rnn_outputs.append(rnn_output)\n            outer_scope.reuse_variables()\n    return tf.stack(rnn_outputs, axis=1), state  # t * [b, h] -> [b, t, h]\n\nwith tf.Session() as sess, tf.variable_scope('model'):\n    with tf.variable_scope('model', reuse=None):\n        tensor_lists_test = rnn_attention_decoder_test()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n    sess.run(init_op)\n    sess.graph.finalize()\n    for step in range(100000):\n        after = process.memory_percent()\n        if step > 0:\n            print(\"MEMORY CHANGE %.7f -> %.7f\" % (before, after))\n        before = process.memory_percent()\n        sess.run(tensor_lists_test)\n        gc.collect()\n\nlog:\n2017-05-09 09:27:55.435870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435903: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435909: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435913: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.435916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-09 09:27:55.732204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\npciBusID 0000:09:00.0\nTotal memory: 11.92GiB\nFree memory: 11.81GiB\n2017-05-09 09:27:55.732232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-09 09:27:55.732238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-09 09:27:55.732247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nMEMORY CHANGE 1.1297021 -> 1.3666840\nMEMORY CHANGE 1.3666840 -> 1.3666840\nMEMORY CHANGE 1.3666840 -> 1.3697929\nMEMORY CHANGE 1.3697929 -> 1.3697929\nMEMORY CHANGE 1.3697929 -> 1.3729200\nMEMORY CHANGE 1.3729200 -> 1.3733208\nMEMORY CHANGE 1.3733208 -> 1.3733208\nMEMORY CHANGE 1.3733208 -> 1.3737215\nMEMORY CHANGE 1.3737215 -> 1.3768487\nMEMORY CHANGE 1.3768487 -> 1.3799758\nMEMORY CHANGE 1.3799758 -> 1.3803644\nMEMORY CHANGE 1.3803644 -> 1.3834976\nMEMORY CHANGE 1.3834976 -> 1.3834976\nMEMORY CHANGE 1.3834976 -> 1.3838862\nMEMORY CHANGE 1.3838862 -> 1.3838862\nMEMORY CHANGE 1.3838862 -> 1.3842870\nMEMORY CHANGE 1.3842870 -> 1.3846878\nMEMORY CHANGE 1.3846878 -> 1.3850885\nMEMORY CHANGE 1.3850885 -> 1.3850885\nMEMORY CHANGE 1.3850885 -> 1.3850885\nMEMORY CHANGE 1.3850885 -> 1.3850885\nMEMORY CHANGE 1.3850885 -> 1.3850885\nMEMORY CHANGE 1.3850885 -> 1.3854771\nMEMORY CHANGE 1.3854771 -> 1.3854771\nMEMORY CHANGE 1.3854771 -> 1.3854771\nMEMORY CHANGE 1.3854771 -> 1.3858779\nMEMORY CHANGE 1.3858779 -> 1.3858779\nMEMORY CHANGE 1.3858779 -> 1.3858779\nMEMORY CHANGE 1.3858779 -> 1.3858779\nMEMORY CHANGE 1.3858779 -> 1.3858779\nMEMORY CHANGE 1.3858779 -> 1.3858779\nMEMORY CHANGE 1.3858779 -> 1.3862665\nMEMORY CHANGE 1.3862665 -> 1.3866673\nMEMORY CHANGE 1.3866673 -> 1.3866673\nMEMORY CHANGE 1.3866673 -> 1.3870680\nMEMORY CHANGE 1.3870680 -> 1.3870680\nMEMORY CHANGE 1.3870680 -> 1.3870680\nMEMORY CHANGE 1.3870680 -> 1.3870680\nMEMORY CHANGE 1.3870680 -> 1.3870680\nMEMORY CHANGE 1.3870680 -> 1.3870680\nMEMORY CHANGE 1.3870680 -> 1.3870680\nMEMORY CHANGE 1.3870680 -> 1.3874688\nMEMORY CHANGE 1.3874688 -> 1.3874688\nMEMORY CHANGE 1.3874688 -> 1.3878695\nMEMORY CHANGE 1.3878695 -> 1.3878695\nMEMORY CHANGE 1.3878695 -> 1.3882581\nMEMORY CHANGE 1.3882581 -> 1.3882581\nMEMORY CHANGE 1.3882581 -> 1.3886589\nMEMORY CHANGE 1.3886589 -> 1.3886589\nMEMORY CHANGE 1.3886589 -> 1.3890597\nMEMORY CHANGE 1.3890597 -> 1.3894604\nMEMORY CHANGE 1.3894604 -> 1.3894604\nMEMORY CHANGE 1.3894604 -> 1.3898612\nMEMORY CHANGE 1.3898612 -> 1.3898612\nMEMORY CHANGE 1.3898612 -> 1.3902619\nMEMORY CHANGE 1.3902619 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3906627\nMEMORY CHANGE 1.3906627 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3937898\nMEMORY CHANGE 1.3937898 -> 1.3941906\nMEMORY CHANGE 1.3941906 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3945913\nMEMORY CHANGE 1.3945913 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3949921\nMEMORY CHANGE 1.3949921 -> 1.3953929\nMEMORY CHANGE 1.3953929 -> 1.3953929\nMEMORY CHANGE 1.3953929 -> 1.3953929\nMEMORY CHANGE 1.3953929 -> 1.3953929\nMEMORY CHANGE 1.3953929 -> 1.3953929\nMEMORY CHANGE 1.3953929 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3957936\nMEMORY CHANGE 1.3957936 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3961944\nMEMORY CHANGE 1.3961944 -> 1.3965951\nMEMORY CHANGE 1.3965951 -> 1.3965951\nMEMORY CHANGE 1.3965951 -> 1.3965951\nMEMORY CHANGE 1.3965951 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3969959\nMEMORY CHANGE 1.3969959 -> 1.3973967\nMEMORY CHANGE 1.3973967 -> 1.3973967\nMEMORY CHANGE 1.3973967 -> 1.3973967\nMEMORY CHANGE 1.3973967 -> 1.3973967\nMEMORY CHANGE 1.3973967 -> 1.3977974\nMEMORY CHANGE 1.3977974 -> 1.3977974", "body": "### System information\r\n\r\n== cat /etc/issue ===============================================\r\nLinux quad 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux quad 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.3.0)\r\ntensorflow-gpu (1.1.0)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.1.0\r\ntf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\ntf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue May  9 12:16:54 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |\r\n| 22%   47C    P0    76W / 250W |      0MiB / 12205MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |\r\n| 22%   60C    P2   129W / 250W |  11713MiB / 12207MiB |     80%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\r\n| 22%   49C    P0    83W / 250W |      0MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |\r\n| 24%   63C    P2   117W / 250W |  11713MiB / 12207MiB |     70%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    1     21558    C   python3                                      11709MiB |\r\n|    3     21346    C   python3                                      11709MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\r\n\r\n\r\n### Describe the problem\r\nWhen we try to implement a complex network which contain a rnn attention decoder, It will consume all the memory after several days. I extract the decoder in a test file, the memory still grow in a slower speed. also found that if change softmax to sigmoid, memory doesn't leak.\r\n\r\n### Source code / logs\r\ntest code:\r\n```\r\n# __author__ = \"liusiye\"\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn import GRUCell, MultiRNNCell\r\nfrom os import getpid\r\nimport psutil\r\nimport gc\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nprocess = psutil.Process(getpid())\r\nB, T, H = 20, 60, 256\r\nlayer_num = 4\r\n\r\ndef apply_attention(encoding, rnn_output):\r\n    ''' encoding: [t, b, h1]\r\n        rnn_output: [b, h2]\r\n    '''\r\n    T, B, H1 = encoding.get_shape().as_list()\r\n    _, H2 = rnn_output.get_shape().as_list()\r\n    with tf.variable_scope('attention'):\r\n        w_encoder = tf.get_variable(\r\n            name='W_encoder',\r\n            shape=[H1, H1],\r\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\r\n        w_decoder = tf.get_variable(\r\n            name='W_decoder',\r\n            shape=[H2, H1],\r\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\r\n        w_attention = tf.get_variable(\r\n            name='W_attention',\r\n            shape=[H1, 1],\r\n            initializer=tf.random_uniform_initializer(-0.01, 0.01))\r\n    r_decoder = tf.matmul(rnn_output, w_decoder)  # [b, h1]\r\n\r\n    r_encoder = tf.matmul(tf.reshape(encoding, [-1, H1]), w_encoder)\r\n    r_encoder = tf.reshape(r_encoder, [T, B, H1])\r\n    # [t, b, h] -> [t * b, h] -> [t, b, h]\r\n\r\n    r_attention = tf.tanh(r_encoder + r_decoder)  # [t, b, h1]\r\n    attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)\r\n    attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)\r\n    #attention = tf.nn.sigmoid(tf.reshape(attention, [T, B]))\r\n    encoding = tf.transpose(encoding, perm=[2, 0, 1])  # [t, b, h1]->[h1, t, b]\r\n\r\n    context = tf.reduce_sum(encoding * attention, axis=1)  # [h1, b]\r\n    return tf.transpose(context)  # [b, h1]\r\n\r\ndef rnn_attention_decoder_test():\r\n    encoding = tf.get_variable(name='encoding', shape=[T, B, H], dtype=tf.float32)\r\n    rnn_outputs = []  # t * [b, h]\r\n    scope = tf.get_variable_scope()\r\n\r\n    zero_input = tf.constant(0, shape=[B, H], dtype=tf.float32)\r\n    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(H) for i in range(layer_num)], state_is_tuple=True)\r\n    state = cell.zero_state(B, tf.float32)\r\n    with tf.variable_scope(scope) as outer_scope:\r\n        for t in range(T):#T):\r\n            attention_input = zero_input\r\n            rnn_input = apply_attention(encoding, attention_input)\r\n            rnn_output, state = cell(rnn_input, state)\r\n            rnn_outputs.append(rnn_output)\r\n            outer_scope.reuse_variables()\r\n    return tf.stack(rnn_outputs, axis=1), state  # t * [b, h] -> [b, t, h]\r\n\r\nwith tf.Session() as sess, tf.variable_scope('model'):\r\n    with tf.variable_scope('model', reuse=None):\r\n        tensor_lists_test = rnn_attention_decoder_test()\r\n    init_op = tf.group(\r\n        tf.global_variables_initializer(),\r\n        tf.local_variables_initializer()\r\n    )\r\n    sess.run(init_op)\r\n    sess.graph.finalize()\r\n    for step in range(100000):\r\n        after = process.memory_percent()\r\n        if step > 0:\r\n            print(\"MEMORY CHANGE %.7f -> %.7f\" % (before, after))\r\n        before = process.memory_percent()\r\n        sess.run(tensor_lists_test)\r\n        gc.collect()\r\n```\r\n\r\nlog:\r\n```\r\n2017-05-09 09:27:55.435870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-09 09:27:55.435903: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-09 09:27:55.435909: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-09 09:27:55.435913: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-09 09:27:55.435916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-09 09:27:55.732204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\r\npciBusID 0000:09:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.81GiB\r\n2017-05-09 09:27:55.732232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-09 09:27:55.732238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-09 09:27:55.732247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\r\nMEMORY CHANGE 1.1297021 -> 1.3666840\r\nMEMORY CHANGE 1.3666840 -> 1.3666840\r\nMEMORY CHANGE 1.3666840 -> 1.3697929\r\nMEMORY CHANGE 1.3697929 -> 1.3697929\r\nMEMORY CHANGE 1.3697929 -> 1.3729200\r\nMEMORY CHANGE 1.3729200 -> 1.3733208\r\nMEMORY CHANGE 1.3733208 -> 1.3733208\r\nMEMORY CHANGE 1.3733208 -> 1.3737215\r\nMEMORY CHANGE 1.3737215 -> 1.3768487\r\nMEMORY CHANGE 1.3768487 -> 1.3799758\r\nMEMORY CHANGE 1.3799758 -> 1.3803644\r\nMEMORY CHANGE 1.3803644 -> 1.3834976\r\nMEMORY CHANGE 1.3834976 -> 1.3834976\r\nMEMORY CHANGE 1.3834976 -> 1.3838862\r\nMEMORY CHANGE 1.3838862 -> 1.3838862\r\nMEMORY CHANGE 1.3838862 -> 1.3842870\r\nMEMORY CHANGE 1.3842870 -> 1.3846878\r\nMEMORY CHANGE 1.3846878 -> 1.3850885\r\nMEMORY CHANGE 1.3850885 -> 1.3850885\r\nMEMORY CHANGE 1.3850885 -> 1.3850885\r\nMEMORY CHANGE 1.3850885 -> 1.3850885\r\nMEMORY CHANGE 1.3850885 -> 1.3850885\r\nMEMORY CHANGE 1.3850885 -> 1.3854771\r\nMEMORY CHANGE 1.3854771 -> 1.3854771\r\nMEMORY CHANGE 1.3854771 -> 1.3854771\r\nMEMORY CHANGE 1.3854771 -> 1.3858779\r\nMEMORY CHANGE 1.3858779 -> 1.3858779\r\nMEMORY CHANGE 1.3858779 -> 1.3858779\r\nMEMORY CHANGE 1.3858779 -> 1.3858779\r\nMEMORY CHANGE 1.3858779 -> 1.3858779\r\nMEMORY CHANGE 1.3858779 -> 1.3858779\r\nMEMORY CHANGE 1.3858779 -> 1.3862665\r\nMEMORY CHANGE 1.3862665 -> 1.3866673\r\nMEMORY CHANGE 1.3866673 -> 1.3866673\r\nMEMORY CHANGE 1.3866673 -> 1.3870680\r\nMEMORY CHANGE 1.3870680 -> 1.3870680\r\nMEMORY CHANGE 1.3870680 -> 1.3870680\r\nMEMORY CHANGE 1.3870680 -> 1.3870680\r\nMEMORY CHANGE 1.3870680 -> 1.3870680\r\nMEMORY CHANGE 1.3870680 -> 1.3870680\r\nMEMORY CHANGE 1.3870680 -> 1.3870680\r\nMEMORY CHANGE 1.3870680 -> 1.3874688\r\nMEMORY CHANGE 1.3874688 -> 1.3874688\r\nMEMORY CHANGE 1.3874688 -> 1.3878695\r\nMEMORY CHANGE 1.3878695 -> 1.3878695\r\nMEMORY CHANGE 1.3878695 -> 1.3882581\r\nMEMORY CHANGE 1.3882581 -> 1.3882581\r\nMEMORY CHANGE 1.3882581 -> 1.3886589\r\nMEMORY CHANGE 1.3886589 -> 1.3886589\r\nMEMORY CHANGE 1.3886589 -> 1.3890597\r\nMEMORY CHANGE 1.3890597 -> 1.3894604\r\nMEMORY CHANGE 1.3894604 -> 1.3894604\r\nMEMORY CHANGE 1.3894604 -> 1.3898612\r\nMEMORY CHANGE 1.3898612 -> 1.3898612\r\nMEMORY CHANGE 1.3898612 -> 1.3902619\r\nMEMORY CHANGE 1.3902619 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3906627\r\nMEMORY CHANGE 1.3906627 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3937898\r\nMEMORY CHANGE 1.3937898 -> 1.3941906\r\nMEMORY CHANGE 1.3941906 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3945913\r\nMEMORY CHANGE 1.3945913 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3949921\r\nMEMORY CHANGE 1.3949921 -> 1.3953929\r\nMEMORY CHANGE 1.3953929 -> 1.3953929\r\nMEMORY CHANGE 1.3953929 -> 1.3953929\r\nMEMORY CHANGE 1.3953929 -> 1.3953929\r\nMEMORY CHANGE 1.3953929 -> 1.3953929\r\nMEMORY CHANGE 1.3953929 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3957936\r\nMEMORY CHANGE 1.3957936 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3961944\r\nMEMORY CHANGE 1.3961944 -> 1.3965951\r\nMEMORY CHANGE 1.3965951 -> 1.3965951\r\nMEMORY CHANGE 1.3965951 -> 1.3965951\r\nMEMORY CHANGE 1.3965951 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3969959\r\nMEMORY CHANGE 1.3969959 -> 1.3973967\r\nMEMORY CHANGE 1.3973967 -> 1.3973967\r\nMEMORY CHANGE 1.3973967 -> 1.3973967\r\nMEMORY CHANGE 1.3973967 -> 1.3973967\r\nMEMORY CHANGE 1.3973967 -> 1.3977974\r\nMEMORY CHANGE 1.3977974 -> 1.3977974\r\n```"}