{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300062628", "html_url": "https://github.com/tensorflow/tensorflow/issues/9779#issuecomment-300062628", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9779", "id": 300062628, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDA2MjYyOA==", "user": {"login": "wenjunpku", "id": 22230272, "node_id": "MDQ6VXNlcjIyMjMwMjcy", "avatar_url": "https://avatars3.githubusercontent.com/u/22230272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenjunpku", "html_url": "https://github.com/wenjunpku", "followers_url": "https://api.github.com/users/wenjunpku/followers", "following_url": "https://api.github.com/users/wenjunpku/following{/other_user}", "gists_url": "https://api.github.com/users/wenjunpku/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenjunpku/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenjunpku/subscriptions", "organizations_url": "https://api.github.com/users/wenjunpku/orgs", "repos_url": "https://api.github.com/users/wenjunpku/repos", "events_url": "https://api.github.com/users/wenjunpku/events{/privacy}", "received_events_url": "https://api.github.com/users/wenjunpku/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-09T05:08:47Z", "updated_at": "2017-05-09T05:08:47Z", "author_association": "NONE", "body_html": "<p>by the way , if we implement softmax like this, the problem doesn't exists.</p>\n<h1>original softmax in TF</h1>\n<pre><code>attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)\n</code></pre>\n<h1>mannul softmax</h1>\n<pre><code>attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)\nattention = tf.reshape(attention, [T, B])\nattention_exp = tf.exp(attention)  # [T ,B]\nattention_sum = tf.reduce_sum(tf.exp(attention), axis=0)  # [B]\nattention = attention_exp / attention_sum  # [T, B]\n</code></pre>", "body_text": "by the way , if we implement softmax like this, the problem doesn't exists.\noriginal softmax in TF\nattention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)\n\nmannul softmax\nattention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)\nattention = tf.reshape(attention, [T, B])\nattention_exp = tf.exp(attention)  # [T ,B]\nattention_sum = tf.reduce_sum(tf.exp(attention), axis=0)  # [B]\nattention = attention_exp / attention_sum  # [T, B]", "body": "by the way , if we implement softmax like this, the problem doesn't exists.\r\n# original softmax in TF\r\n    attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)\r\n\r\n# mannul softmax\r\n    attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)\r\n    attention = tf.reshape(attention, [T, B])\r\n    attention_exp = tf.exp(attention)  # [T ,B]\r\n    attention_sum = tf.reduce_sum(tf.exp(attention), axis=0)  # [B]\r\n    attention = attention_exp / attention_sum  # [T, B]"}