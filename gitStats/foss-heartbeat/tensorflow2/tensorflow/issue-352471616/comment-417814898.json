{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417814898", "html_url": "https://github.com/tensorflow/tensorflow/issues/21756#issuecomment-417814898", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21756", "id": 417814898, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzgxNDg5OA==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-31T23:43:20Z", "updated_at": "2018-08-31T23:43:20Z", "author_association": "MEMBER", "body_html": "<p>I'm preparing a PR which removes \"list\" from the documentation, fixing the issue you saw there.</p>\n<p>In your last example the correct way to do this is</p>\n<pre><code>@tf.custom_gradient\ndef loss_func(x):\n    w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\n                        initializer=tf.constant_initializer([10]), use_resource=True)\n\n    def grad(dy, variables=None):\n        return dy, [dy for v in variables]\n\n    return tf.multiply(x, w), grad\n</code></pre>\n<p>as in, the second return value when variables is not None should be a list with one element per variable in variables. I'll clarify the documentation there too.</p>", "body_text": "I'm preparing a PR which removes \"list\" from the documentation, fixing the issue you saw there.\nIn your last example the correct way to do this is\n@tf.custom_gradient\ndef loss_func(x):\n    w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\n                        initializer=tf.constant_initializer([10]), use_resource=True)\n\n    def grad(dy, variables=None):\n        return dy, [dy for v in variables]\n\n    return tf.multiply(x, w), grad\n\nas in, the second return value when variables is not None should be a list with one element per variable in variables. I'll clarify the documentation there too.", "body": "I'm preparing a PR which removes \"list\" from the documentation, fixing the issue you saw there.\r\n\r\nIn your last example the correct way to do this is\r\n\r\n    @tf.custom_gradient\r\n    def loss_func(x):\r\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\r\n                            initializer=tf.constant_initializer([10]), use_resource=True)\r\n\r\n        def grad(dy, variables=None):\r\n            return dy, [dy for v in variables]\r\n\r\n        return tf.multiply(x, w), grad\r\n\r\n\r\nas in, the second return value when variables is not None should be a list with one element per variable in variables. I'll clarify the documentation there too."}