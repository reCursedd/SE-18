{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21756", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21756/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21756/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21756/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21756", "id": 352471616, "node_id": "MDU6SXNzdWUzNTI0NzE2MTY=", "number": 21756, "title": "input and output of @tf.custom_gradient", "user": {"login": "huangbiubiu", "id": 9823705, "node_id": "MDQ6VXNlcjk4MjM3MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/9823705?v=4", "gravatar_id": "", "url": "https://api.github.com/users/huangbiubiu", "html_url": "https://github.com/huangbiubiu", "followers_url": "https://api.github.com/users/huangbiubiu/followers", "following_url": "https://api.github.com/users/huangbiubiu/following{/other_user}", "gists_url": "https://api.github.com/users/huangbiubiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/huangbiubiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/huangbiubiu/subscriptions", "organizations_url": "https://api.github.com/users/huangbiubiu/orgs", "repos_url": "https://api.github.com/users/huangbiubiu/repos", "events_url": "https://api.github.com/users/huangbiubiu/events{/privacy}", "received_events_url": "https://api.github.com/users/huangbiubiu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-08-21T10:06:12Z", "updated_at": "2018-09-11T14:37:36Z", "closed_at": "2018-09-01T00:28:43Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: TensorFlow 1.10</li>\n<li><strong>Python version</strong>: Python 3.6.5 by Anaconda</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0/ cuDNN 7.1</li>\n<li><strong>GPU model and memory</strong>: NVIDIA GeForce GTX 1080Ti 11G</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am confusing about the input and output of <a href=\"https://www.tensorflow.org/api_docs/python/tf/custom_gradient\" rel=\"nofollow\">tf.custom_gradient</a>.</p>\n<h4>Input</h4>\n<p>In <a href=\"https://www.tensorflow.org/api_docs/python/tf/custom_gradient\" rel=\"nofollow\">doc</a>, it says:</p>\n<p><code>x</code> is a <code>Tensor</code> or sequence of <code>Tensor</code> inputs to the function. But with multiple inputs, instead of taking a sequence of <code>Tensor</code>s, function <code>f</code> takes <code>N</code> positional arguments. I think this is a mistake in documentation. A sequence of <code>Tensor</code>s can't be passed to <code>f</code> which can be reproduced by code below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">self_define_op_multiple_inputs</span>():\n    <span class=\"pl-en\">@tf.custom_gradient</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">loss_func</span>(<span class=\"pl-smi\">input_</span>):\n        x <span class=\"pl-k\">=</span> input_[<span class=\"pl-c1\">0</span>]\n        label <span class=\"pl-k\">=</span> input_[<span class=\"pl-c1\">2</span>]\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">grad</span>(<span class=\"pl-smi\">dy</span>):\n            <span class=\"pl-k\">return</span> [dy, dy]\n\n        <span class=\"pl-k\">return</span> x <span class=\"pl-k\">-</span> label, grad\n\n    x <span class=\"pl-k\">=</span> tf.range(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    y <span class=\"pl-k\">=</span> tf.range(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n\n    loss <span class=\"pl-k\">=</span> loss_func([x, y])\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    self_define_op_multiple_inputs()</pre></div>\n<p>It will try to convert <code>[x, y]</code> to a single <code>Tensor</code> and raises a error:</p>\n<pre><code>/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nTraceback (most recent call last):\n  File \"/home/hyh/projects/benchmark/test.py\", line 280, in &lt;module&gt;\n    self_define_op_multiple_inputs()\n  File \"/home/hyh/projects/benchmark/test.py\", line 276, in self_define_op_multiple_inputs\n    loss = loss_func([x, y])\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 111, in decorated\n    return _graph_mode_decorator(f, *args, **kwargs)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in _graph_mode_decorator\n    args = [ops.convert_to_tensor(x) for x in args]\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in &lt;listcomp&gt;\n    args = [ops.convert_to_tensor(x) for x in args]\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 998, in convert_to_tensor\n    as_ref=False)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 961, in _autopacking_conversion_function\n    return _autopacking_helper(v, inferred_dtype, name or \"packed\")\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 903, in _autopacking_helper\n    elem))\nTypeError: Cannot convert a list containing a tensor of dtype &lt;dtype: 'int32'&gt; to &lt;dtype: 'float32'&gt; (Tensor is: &lt;tf.Tensor 'range_1:0' shape=(10,) dtype=int32&gt;)\n</code></pre>\n<p>While change to positional arguments can fix the bug:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@tf.custom_gradient</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">loss_func</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">label</span>):\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">grad</span>(<span class=\"pl-smi\">dy</span>):\n            <span class=\"pl-k\">return</span> [dy, dy]</pre></div>\n<p>Related discussion can be found at <a href=\"https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs\" rel=\"nofollow\">https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs</a>.</p>\n<h4>Output</h4>\n<p>This is the problem about the output of <code>grad_fn</code>.<br>\nIn doc, <code>grad_vars</code> is a <code>list&lt;Tensor&gt;</code>  with the derivatives of <code>Tensor</code>s in <code>y</code> with respect to the variables, and signature is <code>g(*grad_ys, variables=None)</code>.</p>\n<ol>\n<li>Is <code>variables</code> is original <code>variables</code> or the gradient of <code>variables</code> like <code>grad_ys</code>?</li>\n<li>Return <code>grad_vars</code> as a <code>list&lt;Tensor&gt;</code> will raise an error:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">self_define_op_multiple_inputs</span>():\n    <span class=\"pl-en\">@tf.custom_gradient</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">loss_func</span>(<span class=\"pl-smi\">x</span>):\n        w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>margin_inner_product_layer/W<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n                            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer([<span class=\"pl-c1\">10</span>]), <span class=\"pl-v\">use_resource</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">grad</span>(<span class=\"pl-smi\">dy</span>, <span class=\"pl-smi\">variables</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n            <span class=\"pl-k\">return</span> dy, [variables]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> just for testing</span>\n\n        <span class=\"pl-k\">return</span> tf.multiply(x, w), grad\n\n    x <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">5</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,))\n\n    loss <span class=\"pl-k\">=</span> loss_func(x)\n    dl <span class=\"pl-k\">=</span> tf.gradients(loss, x)\n\n    <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n        derivative <span class=\"pl-k\">=</span> sess.run(dl)\n        <span class=\"pl-c1\">print</span>(derivative)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    self_define_op_multiple_inputs()\n</pre></div>\n<p>It seems like it handles <code>grad_vars</code> as a <code>Tensor</code>:</p>\n<pre><code>/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nTraceback (most recent call last):\n  File \"/home/hyh/projects/benchmark/test.py\", line 259, in &lt;module&gt;\n    self_define_op_multiple_inputs()\n  File \"/home/hyh/projects/benchmark/test.py\", line 251, in self_define_op_multiple_inputs\n    dl = tf.gradients(loss, x)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 795, in _GradientsHelper\n    _LogOpGradients(op, out_grads, in_grads)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in _LogOpGradients\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in &lt;listcomp&gt;\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\nAttributeError: 'list' object has no attribute 'name'\n</code></pre>\n<p>Change <code>grad_vars</code> to <code>Tensor</code> doesn't work either:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">self_define_op_multiple_inputs</span>():\n    <span class=\"pl-en\">@tf.custom_gradient</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">loss_func</span>(<span class=\"pl-smi\">x</span>):\n        w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>margin_inner_product_layer/W<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n                            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer([<span class=\"pl-c1\">10</span>]), <span class=\"pl-v\">use_resource</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">grad</span>(<span class=\"pl-smi\">dy</span>, <span class=\"pl-smi\">variables</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n            <span class=\"pl-k\">return</span> dy, variables  <span class=\"pl-c\"><span class=\"pl-c\">#</span> just for testing</span>\n\n        <span class=\"pl-k\">return</span> tf.multiply(x, w), grad\n\n    x <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">5</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,))\n\n    loss <span class=\"pl-k\">=</span> loss_func(x)\n    dl <span class=\"pl-k\">=</span> tf.gradients(loss, x)\n\n    <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n        derivative <span class=\"pl-k\">=</span> sess.run(dl)\n        <span class=\"pl-c1\">print</span>(derivative)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    self_define_op_multiple_inputs()</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): pip\nTensorFlow version (use command below): TensorFlow 1.10\nPython version: Python 3.6.5 by Anaconda\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: CUDA 9.0/ cuDNN 7.1\nGPU model and memory: NVIDIA GeForce GTX 1080Ti 11G\nExact command to reproduce: N/A\n\nDescribe the problem\nI am confusing about the input and output of tf.custom_gradient.\nInput\nIn doc, it says:\nx is a Tensor or sequence of Tensor inputs to the function. But with multiple inputs, instead of taking a sequence of Tensors, function f takes N positional arguments. I think this is a mistake in documentation. A sequence of Tensors can't be passed to f which can be reproduced by code below:\ndef self_define_op_multiple_inputs():\n    @tf.custom_gradient\n    def loss_func(input_):\n        x = input_[0]\n        label = input_[2]\n\n        def grad(dy):\n            return [dy, dy]\n\n        return x - label, grad\n\n    x = tf.range(10, dtype=tf.float32)\n    y = tf.range(10, dtype=tf.int32)\n\n    loss = loss_func([x, y])\n\n\nif __name__ == '__main__':\n    self_define_op_multiple_inputs()\nIt will try to convert [x, y] to a single Tensor and raises a error:\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nTraceback (most recent call last):\n  File \"/home/hyh/projects/benchmark/test.py\", line 280, in <module>\n    self_define_op_multiple_inputs()\n  File \"/home/hyh/projects/benchmark/test.py\", line 276, in self_define_op_multiple_inputs\n    loss = loss_func([x, y])\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 111, in decorated\n    return _graph_mode_decorator(f, *args, **kwargs)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in _graph_mode_decorator\n    args = [ops.convert_to_tensor(x) for x in args]\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in <listcomp>\n    args = [ops.convert_to_tensor(x) for x in args]\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 998, in convert_to_tensor\n    as_ref=False)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 961, in _autopacking_conversion_function\n    return _autopacking_helper(v, inferred_dtype, name or \"packed\")\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 903, in _autopacking_helper\n    elem))\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'range_1:0' shape=(10,) dtype=int32>)\n\nWhile change to positional arguments can fix the bug:\n@tf.custom_gradient\n    def loss_func(x, label):\n\n        def grad(dy):\n            return [dy, dy]\nRelated discussion can be found at https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs.\nOutput\nThis is the problem about the output of grad_fn.\nIn doc, grad_vars is a list<Tensor>  with the derivatives of Tensors in y with respect to the variables, and signature is g(*grad_ys, variables=None).\n\nIs variables is original variables or the gradient of variables like grad_ys?\nReturn grad_vars as a list<Tensor> will raise an error:\n\ndef self_define_op_multiple_inputs():\n    @tf.custom_gradient\n    def loss_func(x):\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\n                            initializer=tf.constant_initializer([10]), use_resource=True)\n\n        def grad(dy, variables=None):\n            return dy, [variables]  # just for testing\n\n        return tf.multiply(x, w), grad\n\n    x = tf.constant([5], dtype=tf.float32, shape=(1,))\n\n    loss = loss_func(x)\n    dl = tf.gradients(loss, x)\n\n    with tf.Session(config=config) as sess:\n        derivative = sess.run(dl)\n        print(derivative)\n\n\nif __name__ == '__main__':\n    self_define_op_multiple_inputs()\n\nIt seems like it handles grad_vars as a Tensor:\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nTraceback (most recent call last):\n  File \"/home/hyh/projects/benchmark/test.py\", line 259, in <module>\n    self_define_op_multiple_inputs()\n  File \"/home/hyh/projects/benchmark/test.py\", line 251, in self_define_op_multiple_inputs\n    dl = tf.gradients(loss, x)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 795, in _GradientsHelper\n    _LogOpGradients(op, out_grads, in_grads)\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in _LogOpGradients\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in <listcomp>\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\nAttributeError: 'list' object has no attribute 'name'\n\nChange grad_vars to Tensor doesn't work either:\ndef self_define_op_multiple_inputs():\n    @tf.custom_gradient\n    def loss_func(x):\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\n                            initializer=tf.constant_initializer([10]), use_resource=True)\n\n        def grad(dy, variables=None):\n            return dy, variables  # just for testing\n\n        return tf.multiply(x, w), grad\n\n    x = tf.constant([5], dtype=tf.float32, shape=(1,))\n\n    loss = loss_func(x)\n    dl = tf.gradients(loss, x)\n\n    with tf.Session(config=config) as sess:\n        derivative = sess.run(dl)\n        print(derivative)\n\n\nif __name__ == '__main__':\n    self_define_op_multiple_inputs()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: TensorFlow 1.10\r\n- **Python version**: Python 3.6.5 by Anaconda\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0/ cuDNN 7.1\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1080Ti 11G\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\nI am confusing about the input and output of [tf.custom_gradient](https://www.tensorflow.org/api_docs/python/tf/custom_gradient).\r\n\r\n#### Input\r\n\r\nIn [doc](https://www.tensorflow.org/api_docs/python/tf/custom_gradient), it says:\r\n\r\n `x` is a `Tensor` or sequence of `Tensor` inputs to the function. But with multiple inputs, instead of taking a sequence of `Tensor`s, function `f` takes `N` positional arguments. I think this is a mistake in documentation. A sequence of `Tensor`s can't be passed to `f` which can be reproduced by code below:\r\n```python3\r\ndef self_define_op_multiple_inputs():\r\n    @tf.custom_gradient\r\n    def loss_func(input_):\r\n        x = input_[0]\r\n        label = input_[2]\r\n\r\n        def grad(dy):\r\n            return [dy, dy]\r\n\r\n        return x - label, grad\r\n\r\n    x = tf.range(10, dtype=tf.float32)\r\n    y = tf.range(10, dtype=tf.int32)\r\n\r\n    loss = loss_func([x, y])\r\n\r\n\r\nif __name__ == '__main__':\r\n    self_define_op_multiple_inputs()\r\n```\r\nIt will try to convert `[x, y]` to a single `Tensor` and raises a error:\r\n```\r\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 280, in <module>\r\n    self_define_op_multiple_inputs()\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 276, in self_define_op_multiple_inputs\r\n    loss = loss_func([x, y])\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 111, in decorated\r\n    return _graph_mode_decorator(f, *args, **kwargs)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in _graph_mode_decorator\r\n    args = [ops.convert_to_tensor(x) for x in args]\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in <listcomp>\r\n    args = [ops.convert_to_tensor(x) for x in args]\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 998, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 961, in _autopacking_conversion_function\r\n    return _autopacking_helper(v, inferred_dtype, name or \"packed\")\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 903, in _autopacking_helper\r\n    elem))\r\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'range_1:0' shape=(10,) dtype=int32>)\r\n```\r\nWhile change to positional arguments can fix the bug:\r\n\r\n```python3\r\n@tf.custom_gradient\r\n    def loss_func(x, label):\r\n\r\n        def grad(dy):\r\n            return [dy, dy]\r\n```\r\n\r\n\r\nRelated discussion can be found at [https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs](https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs).\r\n\r\n\r\n#### Output\r\nThis is the problem about the output of `grad_fn`.\r\nIn doc, `grad_vars` is a `list<Tensor>`  with the derivatives of `Tensor`s in `y` with respect to the variables, and signature is `g(*grad_ys, variables=None)`. \r\n1. Is `variables` is original `variables` or the gradient of `variables` like `grad_ys`?\r\n2. Return `grad_vars` as a `list<Tensor>` will raise an error:\r\n```python3\r\n\r\ndef self_define_op_multiple_inputs():\r\n    @tf.custom_gradient\r\n    def loss_func(x):\r\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\r\n                            initializer=tf.constant_initializer([10]), use_resource=True)\r\n\r\n        def grad(dy, variables=None):\r\n            return dy, [variables]  # just for testing\r\n\r\n        return tf.multiply(x, w), grad\r\n\r\n    x = tf.constant([5], dtype=tf.float32, shape=(1,))\r\n\r\n    loss = loss_func(x)\r\n    dl = tf.gradients(loss, x)\r\n\r\n    with tf.Session(config=config) as sess:\r\n        derivative = sess.run(dl)\r\n        print(derivative)\r\n\r\n\r\nif __name__ == '__main__':\r\n    self_define_op_multiple_inputs()\r\n\r\n```\r\n\r\nIt seems like it handles `grad_vars` as a `Tensor`:\r\n\r\n```\r\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 259, in <module>\r\n    self_define_op_multiple_inputs()\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 251, in self_define_op_multiple_inputs\r\n    dl = tf.gradients(loss, x)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 795, in _GradientsHelper\r\n    _LogOpGradients(op, out_grads, in_grads)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in _LogOpGradients\r\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in <listcomp>\r\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\r\nAttributeError: 'list' object has no attribute 'name'\r\n```\r\nChange `grad_vars` to `Tensor` doesn't work either:\r\n```python3\r\n\r\ndef self_define_op_multiple_inputs():\r\n    @tf.custom_gradient\r\n    def loss_func(x):\r\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\r\n                            initializer=tf.constant_initializer([10]), use_resource=True)\r\n\r\n        def grad(dy, variables=None):\r\n            return dy, variables  # just for testing\r\n\r\n        return tf.multiply(x, w), grad\r\n\r\n    x = tf.constant([5], dtype=tf.float32, shape=(1,))\r\n\r\n    loss = loss_func(x)\r\n    dl = tf.gradients(loss, x)\r\n\r\n    with tf.Session(config=config) as sess:\r\n        derivative = sess.run(dl)\r\n        print(derivative)\r\n\r\n\r\nif __name__ == '__main__':\r\n    self_define_op_multiple_inputs()\r\n```\r\n"}