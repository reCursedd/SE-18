{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/420231669", "html_url": "https://github.com/tensorflow/tensorflow/issues/21756#issuecomment-420231669", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21756", "id": 420231669, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDIzMTY2OQ==", "user": {"login": "huangbiubiu", "id": 9823705, "node_id": "MDQ6VXNlcjk4MjM3MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/9823705?v=4", "gravatar_id": "", "url": "https://api.github.com/users/huangbiubiu", "html_url": "https://github.com/huangbiubiu", "followers_url": "https://api.github.com/users/huangbiubiu/followers", "following_url": "https://api.github.com/users/huangbiubiu/following{/other_user}", "gists_url": "https://api.github.com/users/huangbiubiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/huangbiubiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/huangbiubiu/subscriptions", "organizations_url": "https://api.github.com/users/huangbiubiu/orgs", "repos_url": "https://api.github.com/users/huangbiubiu/repos", "events_url": "https://api.github.com/users/huangbiubiu/events{/privacy}", "received_events_url": "https://api.github.com/users/huangbiubiu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-11T10:49:43Z", "updated_at": "2018-09-11T10:49:43Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> To be more clear, for the second parameter (and the second return value), <code>grad_fn</code> accepts <em>original</em> variables (not the <em>gradient</em> of variables) and return the  <em>gradient</em> of variables, is that correct?</p>\n<p>If that's correct, why not <code>grad_fn</code> accepts <em>gradient</em> of variables instead of <em>original</em> variables ( in order to be consistent with <code>grad_ys</code> (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually.</p>", "body_text": "@alextp To be more clear, for the second parameter (and the second return value), grad_fn accepts original variables (not the gradient of variables) and return the  gradient of variables, is that correct?\nIf that's correct, why not grad_fn accepts gradient of variables instead of original variables ( in order to be consistent with grad_ys (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually.", "body": "@alextp To be more clear, for the second parameter (and the second return value), `grad_fn` accepts *original* variables (not the *gradient* of variables) and return the  *gradient* of variables, is that correct?\r\n\r\nIf that's correct, why not `grad_fn` accepts *gradient* of variables instead of *original* variables ( in order to be consistent with `grad_ys` (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually."}