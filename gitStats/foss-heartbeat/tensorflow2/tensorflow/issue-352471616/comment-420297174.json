{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/420297174", "html_url": "https://github.com/tensorflow/tensorflow/issues/21756#issuecomment-420297174", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21756", "id": 420297174, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDI5NzE3NA==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-11T14:37:36Z", "updated_at": "2018-09-11T14:37:36Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">grad_ys is the \"downstream\" gradient of the outputs of your function; since\nthe variables are not outputs there is no gradient already computed wrt\nthem. If you want you can call tf.gradients or use the tf.GradientTape\nyourself to compute the gradient wrt the variables to then modify it, but\nwe don't force you to do that since it would waste computation in eager\nexecution.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Tue, Sep 11, 2018 at 3:56 AM Huang Yuheng ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/alextp\">@alextp</a> &lt;<a href=\"https://github.com/alextp\">https://github.com/alextp</a>&gt; To be more clear, for the second\n parameter (and the second return value), grad_fn accepts *original*\n variables (not the *gradient* of variables) and return the *gradient* of\n variables, is that correct?\n\n If that's correct, why not grad_fn accepts *gradient* of variables\n instead of *original* variables ( in order to be consistent with grad_ys\n (the first parameter))? In that case, we can use the derivates of variables\n by automatic differentiation instead of writing the derivates of variables\n manually.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"352471616\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/21756\" href=\"https://github.com/tensorflow/tensorflow/issues/21756#issuecomment-420231669\">#21756 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxetzCj348nk4KLUjm3FAJNNXJqR3ks5uZ5b1gaJpZM4WFeuE\">https://github.com/notifications/unsubscribe-auth/AAATxetzCj348nk4KLUjm3FAJNNXJqR3ks5uZ5b1gaJpZM4WFeuE</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "grad_ys is the \"downstream\" gradient of the outputs of your function; since\nthe variables are not outputs there is no gradient already computed wrt\nthem. If you want you can call tf.gradients or use the tf.GradientTape\nyourself to compute the gradient wrt the variables to then modify it, but\nwe don't force you to do that since it would waste computation in eager\nexecution.\n\u2026\nOn Tue, Sep 11, 2018 at 3:56 AM Huang Yuheng ***@***.***> wrote:\n @alextp <https://github.com/alextp> To be more clear, for the second\n parameter (and the second return value), grad_fn accepts *original*\n variables (not the *gradient* of variables) and return the *gradient* of\n variables, is that correct?\n\n If that's correct, why not grad_fn accepts *gradient* of variables\n instead of *original* variables ( in order to be consistent with grad_ys\n (the first parameter))? In that case, we can use the derivates of variables\n by automatic differentiation instead of writing the derivates of variables\n manually.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#21756 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxetzCj348nk4KLUjm3FAJNNXJqR3ks5uZ5b1gaJpZM4WFeuE>\n .\n\n\n-- \n - Alex", "body": "grad_ys is the \"downstream\" gradient of the outputs of your function; since\nthe variables are not outputs there is no gradient already computed wrt\nthem. If you want you can call tf.gradients or use the tf.GradientTape\nyourself to compute the gradient wrt the variables to then modify it, but\nwe don't force you to do that since it would waste computation in eager\nexecution.\n\nOn Tue, Sep 11, 2018 at 3:56 AM Huang Yuheng <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> To be more clear, for the second\n> parameter (and the second return value), grad_fn accepts *original*\n> variables (not the *gradient* of variables) and return the *gradient* of\n> variables, is that correct?\n>\n> If that's correct, why not grad_fn accepts *gradient* of variables\n> instead of *original* variables ( in order to be consistent with grad_ys\n> (the first parameter))? In that case, we can use the derivates of variables\n> by automatic differentiation instead of writing the derivates of variables\n> manually.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21756#issuecomment-420231669>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxetzCj348nk4KLUjm3FAJNNXJqR3ks5uZ5b1gaJpZM4WFeuE>\n> .\n>\n\n\n-- \n - Alex\n"}