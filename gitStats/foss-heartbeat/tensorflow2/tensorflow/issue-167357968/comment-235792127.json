{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235792127", "html_url": "https://github.com/tensorflow/tensorflow/issues/3492#issuecomment-235792127", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3492", "id": 235792127, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTc5MjEyNw==", "user": {"login": "gpapan", "id": 6232317, "node_id": "MDQ6VXNlcjYyMzIzMTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6232317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gpapan", "html_url": "https://github.com/gpapan", "followers_url": "https://api.github.com/users/gpapan/followers", "following_url": "https://api.github.com/users/gpapan/following{/other_user}", "gists_url": "https://api.github.com/users/gpapan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gpapan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gpapan/subscriptions", "organizations_url": "https://api.github.com/users/gpapan/orgs", "repos_url": "https://api.github.com/users/gpapan/repos", "events_url": "https://api.github.com/users/gpapan/events{/privacy}", "received_events_url": "https://api.github.com/users/gpapan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-28T03:35:40Z", "updated_at": "2016-07-28T03:35:40Z", "author_association": "NONE", "body_html": "<p>(1) We do not have immediate plans to add support for 3D atrous/dilated convolution in TF. For the 2-D case we already have <code>tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)</code>.</p>\n<p>(2) You can adapt the existing <a href=\"https://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/python/ops/nn_ops.py\">tf.nn.atrous_conv2d</a> implementation to perform 2D atrous/dilated pooling by replacing the call to <code>conv2d</code> with a call to <code>avg_pool</code> or <code>max_pool</code>.</p>\n<p>An alternative to achieve a similar effect (i.e., pooling over a larger area) is to just use standard pooling with enlarged kernel size:</p>\n<pre><code> ksize_eff = ksize + (ksize - 1) * (rate - 1)\n</code></pre>\n<p>Note that average or max pooling act on each channel separately, which means that they are fast even for large kernel size. They also have no learned parameters, which means that there is no risk for over-fitting. These factors combined imply that kernel upsampling is less appealing for pooling than it is for convolution.</p>", "body_text": "(1) We do not have immediate plans to add support for 3D atrous/dilated convolution in TF. For the 2-D case we already have tf.nn.atrous_conv2d(value, filters, rate, padding, name=None).\n(2) You can adapt the existing tf.nn.atrous_conv2d implementation to perform 2D atrous/dilated pooling by replacing the call to conv2d with a call to avg_pool or max_pool.\nAn alternative to achieve a similar effect (i.e., pooling over a larger area) is to just use standard pooling with enlarged kernel size:\n ksize_eff = ksize + (ksize - 1) * (rate - 1)\n\nNote that average or max pooling act on each channel separately, which means that they are fast even for large kernel size. They also have no learned parameters, which means that there is no risk for over-fitting. These factors combined imply that kernel upsampling is less appealing for pooling than it is for convolution.", "body": "(1) We do not have immediate plans to add support for 3D atrous/dilated convolution in TF. For the 2-D case we already have `tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)`.\n\n(2) You can adapt the existing [tf.nn.atrous_conv2d](https://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/python/ops/nn_ops.py) implementation to perform 2D atrous/dilated pooling by replacing the call to `conv2d` with a call to `avg_pool` or `max_pool`.\n\nAn alternative to achieve a similar effect (i.e., pooling over a larger area) is to just use standard pooling with enlarged kernel size:\n\n```\n ksize_eff = ksize + (ksize - 1) * (rate - 1)\n```\n\nNote that average or max pooling act on each channel separately, which means that they are fast even for large kernel size. They also have no learned parameters, which means that there is no risk for over-fitting. These factors combined imply that kernel upsampling is less appealing for pooling than it is for convolution.\n"}