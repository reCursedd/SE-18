{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19892", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19892/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19892/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19892/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19892", "id": 330979333, "node_id": "MDU6SXNzdWUzMzA5NzkzMzM=", "number": 19892, "title": "distributed estrimator hang forever.", "user": {"login": "cklsoft", "id": 3956357, "node_id": "MDQ6VXNlcjM5NTYzNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/3956357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cklsoft", "html_url": "https://github.com/cklsoft", "followers_url": "https://api.github.com/users/cklsoft/followers", "following_url": "https://api.github.com/users/cklsoft/following{/other_user}", "gists_url": "https://api.github.com/users/cklsoft/gists{/gist_id}", "starred_url": "https://api.github.com/users/cklsoft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cklsoft/subscriptions", "organizations_url": "https://api.github.com/users/cklsoft/orgs", "repos_url": "https://api.github.com/users/cklsoft/repos", "events_url": "https://api.github.com/users/cklsoft/events{/privacy}", "received_events_url": "https://api.github.com/users/cklsoft/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-06-10T15:47:25Z", "updated_at": "2018-07-04T05:45:58Z", "closed_at": "2018-07-04T05:45:58Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:  binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>:  2.7.13</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>I ran the mnist code with estimator, It's ok when running in single process, but hang when running in 3 processes for distributed mode. The code (PS) is shown below. Notice that, worker should change the task.type in TF_CONFIG to 'worker', and chief should change to 'chief'.</p>\n<h3>Source code / logs</h3>\n<p>import tensorflow as tf</p>\n<p>import numpy as np</p>\n<p>import pandas as pd</p>\n<p>from sklearn.model_selection import train_test_split</p>\n<p>import matplotlib.pyplot as plt</p>\n<p>import matplotlib.cm as cm</p>\n<p>tf.logging.set_verbosity(tf.logging.INFO)</p>\n<p>def load2():<br>\ntrain = pd.read_csv('/tmp/train.csv')</p>\n<pre><code>test = pd.read_csv('/tmp/test.csv')\n\nlabels = train['label']\n\nimages = train.iloc[:, 1:]\n\nimage_size = 28\n\ntrain_ds, valid_ds, train_labels, valid_labels = train_test_split(images, labels, test_size=0.33, random_state=42)\n\nprint len(train_ds)\nprint len(valid_ds)\nprint len(train_labels)\nprint len(valid_labels)\n\ndata = images\n\ntrain_ds, train_labels = reformat(train_ds, train_labels)\nvalid_ds, valid_labels = reformat(valid_ds, valid_labels)\ntest_ds, test_labels = reformat(test, train_labels[:len(test)])\n\nprint('Training set', train_ds.shape, train_labels.shape)\nprint('Validation set', valid_ds.shape, valid_labels.shape)\nprint('Test set', test_ds.shape, test_labels.shape)\n</code></pre>\n<p>def reformat(dataset, labels):<br>\nnum_channels = 1<br>\nnum_labels = 10<br>\ndataset = dataset.values.reshape(<br>\n(-1, image_size, image_size, num_channels)).astype(np.float32)<br>\nlabels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)<br>\nreturn dataset, labels</p>\n<p>def cnn_model_fn(features, labels, mode):<br>\ninput_layer = tf.reshape(features['x'], [-1, 28, 28, 1])<br>\nconv1 = tf.layers.conv2d(<br>\ninputs=input_layer,<br>\nfilters=32,<br>\nkernel_size=[5, 5],<br>\npadding=\"same\",<br>\nactivation=tf.nn.relu<br>\n)</p>\n<pre><code>pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)  # 14*14*32\n\nconv2 = tf.layers.conv2d(\n    inputs=pool1,\n    filters=64,\n    kernel_size=[5, 5],\n    padding='same',\n    activation=tf.nn.relu\n)\n\npool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\npool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\ndense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\ndropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\nlogits = tf.layers.dense(inputs=dropout, units=10)\nprint '####', logits\n\npredictions = {\n    'classes': tf.argmax(logits, axis=1),\n    'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n}\n\nif mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\nif mode == tf.estimator.ModeKeys.TRAIN:\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step()\n    )\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n</code></pre>\n<p>def main(argvs):<br>\nmnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")<br>\ntrain_data = mnist.train.images<br>\nprint train_data.shape<br>\ntrain_labels = np.asarray(mnist.train.labels, dtype=np.int32)<br>\neval_data = mnist.test.images<br>\neval_labels = np.asarray(mnist.test.labels, dtype=np.int32)<br>\nmnist_classifier = tf.estimator.Estimator(<br>\nmodel_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")<br>\n# Set up logging for predictions<br>\n# Log the values in the \"Softmax\" tensor with label \"probabilities\"<br>\ntensors_to_log = {\"probabilities\": \"softmax_tensor\"}<br>\nlogging_hook = tf.train.LoggingTensorHook(<br>\ntensors=tensors_to_log, every_n_iter=50)<br>\nprint 'labels: ', train_labels<br>\n# Train the model<br>\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(<br>\nx={\"x\": train_data},<br>\ny=train_labels,<br>\nbatch_size=100,<br>\nnum_epochs=None,<br>\nshuffle=True)<br>\nmnist_classifier.train(<br>\ninput_fn=train_input_fn,<br>\nsteps=20000,<br>\nhooks=[logging_hook])<br>\neval_input_fn = tf.estimator.inputs.numpy_input_fn(<br>\nx={\"x\": eval_data},<br>\ny=eval_labels,<br>\nnum_epochs=1,<br>\nshuffle=False)</p>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nimport os,json<br>\nos.environ['TF_CONFIG']=json.dumps({<br>\n\"cluster\": {<br>\n\"ps\": [<br>\n\"127.0.0.1:34567\"<br>\n],<br>\n\"chief\": [<br>\n\"127.0.0.1:34568\"<br>\n],<br>\n\"worker\": [<br>\n\"127.0.0.1:34569\"<br>\n]<br>\n},<br>\n\"task\": {<br>\n\"index\": 0,<br>\n\"type\": \"ps\" # optional: chief, ps, worker<br>\n}<br>\n})<br>\ntf.app.run()`</p>\n<p>WARNING:tensorflow:From ps_cnn_mnist.py:94: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease use tf.data.<br>\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/<strong>init</strong>.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.<br>\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.<br>\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease write your own downloading logic.<br>\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease use tf.data to implement this functionality.<br>\nExtracting MNIST-data/train-images-idx3-ubyte.gz<br>\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease use tf.data to implement this functionality.<br>\nExtracting MNIST-data/train-labels-idx1-ubyte.gz<br>\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz<br>\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz<br>\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: <strong>init</strong> (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.<br>\n(55000, 784)<br>\nINFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'127.0.0.1:34567'], u'chief': [u'127.0.0.1:34568'], u'worker': [u'127.0.0.1:34569']}, u'task': {u'index': 0, u'type': u'ps'}}<br>\nINFO:tensorflow:Using default config.<br>\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff7169d49d0&gt;, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://127.0.0.1:34567', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/mnist_convnet_model', '_global_id_in_cluster': 2, '_save_summary_steps': 100}<br>\nlabels:  [7 3 4 ... 5 6 8]<br>\nINFO:tensorflow:Calling model_fn.</p>\n<h4>Tensor(\"dense_1/BiasAdd:0\", shape=(100, 10), dtype=float32, device=/job:ps/task:0)</h4>\n<p>INFO:tensorflow:Done calling model_fn.<br>\nINFO:tensorflow:Create CheckpointSaverHook.<br>\nINFO:tensorflow:Graph was finalized.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux\nTensorFlow installed from (source or binary):  binary\nTensorFlow version (use command below): 1.8.0\nPython version:  2.7.13\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nI ran the mnist code with estimator, It's ok when running in single process, but hang when running in 3 processes for distributed mode. The code (PS) is shown below. Notice that, worker should change the task.type in TF_CONFIG to 'worker', and chief should change to 'chief'.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\ntf.logging.set_verbosity(tf.logging.INFO)\ndef load2():\ntrain = pd.read_csv('/tmp/train.csv')\ntest = pd.read_csv('/tmp/test.csv')\n\nlabels = train['label']\n\nimages = train.iloc[:, 1:]\n\nimage_size = 28\n\ntrain_ds, valid_ds, train_labels, valid_labels = train_test_split(images, labels, test_size=0.33, random_state=42)\n\nprint len(train_ds)\nprint len(valid_ds)\nprint len(train_labels)\nprint len(valid_labels)\n\ndata = images\n\ntrain_ds, train_labels = reformat(train_ds, train_labels)\nvalid_ds, valid_labels = reformat(valid_ds, valid_labels)\ntest_ds, test_labels = reformat(test, train_labels[:len(test)])\n\nprint('Training set', train_ds.shape, train_labels.shape)\nprint('Validation set', valid_ds.shape, valid_labels.shape)\nprint('Test set', test_ds.shape, test_labels.shape)\n\ndef reformat(dataset, labels):\nnum_channels = 1\nnum_labels = 10\ndataset = dataset.values.reshape(\n(-1, image_size, image_size, num_channels)).astype(np.float32)\nlabels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\nreturn dataset, labels\ndef cnn_model_fn(features, labels, mode):\ninput_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\nconv1 = tf.layers.conv2d(\ninputs=input_layer,\nfilters=32,\nkernel_size=[5, 5],\npadding=\"same\",\nactivation=tf.nn.relu\n)\npool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)  # 14*14*32\n\nconv2 = tf.layers.conv2d(\n    inputs=pool1,\n    filters=64,\n    kernel_size=[5, 5],\n    padding='same',\n    activation=tf.nn.relu\n)\n\npool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\npool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\ndense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\ndropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\nlogits = tf.layers.dense(inputs=dropout, units=10)\nprint '####', logits\n\npredictions = {\n    'classes': tf.argmax(logits, axis=1),\n    'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n}\n\nif mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\nif mode == tf.estimator.ModeKeys.TRAIN:\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step()\n    )\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\ndef main(argvs):\nmnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\ntrain_data = mnist.train.images\nprint train_data.shape\ntrain_labels = np.asarray(mnist.train.labels, dtype=np.int32)\neval_data = mnist.test.images\neval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\nmnist_classifier = tf.estimator.Estimator(\nmodel_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n# Set up logging for predictions\n# Log the values in the \"Softmax\" tensor with label \"probabilities\"\ntensors_to_log = {\"probabilities\": \"softmax_tensor\"}\nlogging_hook = tf.train.LoggingTensorHook(\ntensors=tensors_to_log, every_n_iter=50)\nprint 'labels: ', train_labels\n# Train the model\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\nx={\"x\": train_data},\ny=train_labels,\nbatch_size=100,\nnum_epochs=None,\nshuffle=True)\nmnist_classifier.train(\ninput_fn=train_input_fn,\nsteps=20000,\nhooks=[logging_hook])\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\nx={\"x\": eval_data},\ny=eval_labels,\nnum_epochs=1,\nshuffle=False)\nif name == 'main':\nimport os,json\nos.environ['TF_CONFIG']=json.dumps({\n\"cluster\": {\n\"ps\": [\n\"127.0.0.1:34567\"\n],\n\"chief\": [\n\"127.0.0.1:34568\"\n],\n\"worker\": [\n\"127.0.0.1:34569\"\n]\n},\n\"task\": {\n\"index\": 0,\n\"type\": \"ps\" # optional: chief, ps, worker\n}\n})\ntf.app.run()`\nWARNING:tensorflow:From ps_cnn_mnist.py:94: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data.\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/init.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST-data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: init (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n(55000, 784)\nINFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'127.0.0.1:34567'], u'chief': [u'127.0.0.1:34568'], u'worker': [u'127.0.0.1:34569']}, u'task': {u'index': 0, u'type': u'ps'}}\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff7169d49d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://127.0.0.1:34567', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/mnist_convnet_model', '_global_id_in_cluster': 2, '_save_summary_steps': 100}\nlabels:  [7 3 4 ... 5 6 8]\nINFO:tensorflow:Calling model_fn.\nTensor(\"dense_1/BiasAdd:0\", shape=(100, 10), dtype=float32, device=/job:ps/task:0)\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux\r\n- **TensorFlow installed from (source or binary)**:  binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI ran the mnist code with estimator, It's ok when running in single process, but hang when running in 3 processes for distributed mode. The code (PS) is shown below. Notice that, worker should change the task.type in TF_CONFIG to 'worker', and chief should change to 'chief'.\r\n\r\n\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\n\r\nimport pandas as pd\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nimport matplotlib.cm as cm\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef load2():\r\n    train = pd.read_csv('/tmp/train.csv')\r\n\r\n    test = pd.read_csv('/tmp/test.csv')\r\n\r\n    labels = train['label']\r\n\r\n    images = train.iloc[:, 1:]\r\n\r\n    image_size = 28\r\n\r\n    train_ds, valid_ds, train_labels, valid_labels = train_test_split(images, labels, test_size=0.33, random_state=42)\r\n\r\n    print len(train_ds)\r\n    print len(valid_ds)\r\n    print len(train_labels)\r\n    print len(valid_labels)\r\n\r\n    data = images\r\n\r\n    train_ds, train_labels = reformat(train_ds, train_labels)\r\n    valid_ds, valid_labels = reformat(valid_ds, valid_labels)\r\n    test_ds, test_labels = reformat(test, train_labels[:len(test)])\r\n\r\n    print('Training set', train_ds.shape, train_labels.shape)\r\n    print('Validation set', valid_ds.shape, valid_labels.shape)\r\n    print('Test set', test_ds.shape, test_labels.shape)\r\n\r\n\r\ndef reformat(dataset, labels):\r\n    num_channels = 1\r\n    num_labels = 10\r\n    dataset = dataset.values.reshape(\r\n        (-1, image_size, image_size, num_channels)).astype(np.float32)\r\n    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\r\n    return dataset, labels\r\n\r\n\r\ndef cnn_model_fn(features, labels, mode):\r\n    input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\r\n    conv1 = tf.layers.conv2d(\r\n        inputs=input_layer,\r\n        filters=32,\r\n        kernel_size=[5, 5],\r\n        padding=\"same\",\r\n        activation=tf.nn.relu\r\n    )\r\n\r\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)  # 14*14*32\r\n\r\n    conv2 = tf.layers.conv2d(\r\n        inputs=pool1,\r\n        filters=64,\r\n        kernel_size=[5, 5],\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n\r\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n\r\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n    logits = tf.layers.dense(inputs=dropout, units=10)\r\n    print '####', logits\r\n\r\n    predictions = {\r\n        'classes': tf.argmax(logits, axis=1),\r\n        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n    }\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n        train_op = optimizer.minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step()\r\n        )\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\ndef main(argvs):\r\n    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n    train_data = mnist.train.images\r\n    print train_data.shape\r\n    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\n    eval_data = mnist.test.images\r\n    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\r\n    mnist_classifier = tf.estimator.Estimator(\r\n        model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\r\n    # Set up logging for predictions\r\n    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\r\n    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\r\n    logging_hook = tf.train.LoggingTensorHook(\r\n        tensors=tensors_to_log, every_n_iter=50)\r\n    print 'labels: ', train_labels\r\n    # Train the model\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": train_data},\r\n        y=train_labels,\r\n        batch_size=100,\r\n        num_epochs=None,\r\n        shuffle=True)\r\n    mnist_classifier.train(\r\n        input_fn=train_input_fn,\r\n        steps=20000,\r\n        hooks=[logging_hook])\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": eval_data},\r\n        y=eval_labels,\r\n        num_epochs=1,\r\n        shuffle=False)\r\n\r\n\r\nif __name__ == '__main__':\r\n  import os,json\r\n  os.environ['TF_CONFIG']=json.dumps({\r\n      \"cluster\": {\r\n        \"ps\": [\r\n          \"127.0.0.1:34567\"\r\n        ],\r\n        \"chief\": [\r\n          \"127.0.0.1:34568\"\r\n        ],\r\n        \"worker\": [\r\n          \"127.0.0.1:34569\"\r\n        ]\r\n      },\r\n      \"task\": {\r\n        \"index\": 0,\r\n        \"type\": \"ps\" # optional: chief, ps, worker\r\n      }\r\n  })\r\n  tf.app.run()`\r\n\r\n\r\n\r\n\r\nWARNING:tensorflow:From ps_cnn_mnist.py:94: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n(55000, 784)\r\nINFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'127.0.0.1:34567'], u'chief': [u'127.0.0.1:34568'], u'worker': [u'127.0.0.1:34569']}, u'task': {u'index': 0, u'type': u'ps'}}\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff7169d49d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://127.0.0.1:34567', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/mnist_convnet_model', '_global_id_in_cluster': 2, '_save_summary_steps': 100}\r\nlabels:  [7 3 4 ... 5 6 8]\r\nINFO:tensorflow:Calling model_fn.\r\n#### Tensor(\"dense_1/BiasAdd:0\", shape=(100, 10), dtype=float32, device=/job:ps/task:0)\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized."}