{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413756334", "html_url": "https://github.com/tensorflow/tensorflow/issues/21600#issuecomment-413756334", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21600", "id": 413756334, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzc1NjMzNA==", "user": {"login": "alapha23", "id": 23239892, "node_id": "MDQ6VXNlcjIzMjM5ODky", "avatar_url": "https://avatars2.githubusercontent.com/u/23239892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alapha23", "html_url": "https://github.com/alapha23", "followers_url": "https://api.github.com/users/alapha23/followers", "following_url": "https://api.github.com/users/alapha23/following{/other_user}", "gists_url": "https://api.github.com/users/alapha23/gists{/gist_id}", "starred_url": "https://api.github.com/users/alapha23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alapha23/subscriptions", "organizations_url": "https://api.github.com/users/alapha23/orgs", "repos_url": "https://api.github.com/users/alapha23/repos", "events_url": "https://api.github.com/users/alapha23/events{/privacy}", "received_events_url": "https://api.github.com/users/alapha23/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T04:46:39Z", "updated_at": "2018-08-17T04:46:39Z", "author_association": "NONE", "body_html": "<p>Hi!<br>\nI used a script to confirm that CUDA unified memory cannot grow on demand.<br>\nFor your information, this is <a href=\"https://github.com/alapha23/playground/blob/master/cat_dog_tf.py\">the script I used</a></p>\n<p>Furthermore, I found the GPU memory is poorly utilized when <code>per_process_gpu_memory_fraction</code> is set to a large value, e.g. 200.</p>\n<p>In my case, with <code>batch_size</code> 512 and <code>per_process_gpu_memory_fraction</code> 200, around 4G gpu memory is utilized out of 16G, while around 360G of RAM is occupied. GPU memory seems to be taken by the model but the gpu memory usage is not large(4G/16G).</p>\n<p>Then I tried to know why we are not fully utilizing GPU memory and <code>bfc_allocator</code> might be the reason.<br>\nFrom what I am seeing, the implementation of <code>bfc_allocator</code> could be improved ---  to be aware of the memory swapping and hence improve performance.</p>\n<p>In brief,</p>\n<ul>\n<li>Make UVM grow on demand</li>\n<li>Improve bfc_allocator with respect of UVM</li>\n</ul>\n<p>Are they already implemented?<br>\nIf not, I believe they should be helpful and intriguing features we could contribute to.</p>\n<p>What do you think?</p>\n<p>Thank you in advance!</p>", "body_text": "Hi!\nI used a script to confirm that CUDA unified memory cannot grow on demand.\nFor your information, this is the script I used\nFurthermore, I found the GPU memory is poorly utilized when per_process_gpu_memory_fraction is set to a large value, e.g. 200.\nIn my case, with batch_size 512 and per_process_gpu_memory_fraction 200, around 4G gpu memory is utilized out of 16G, while around 360G of RAM is occupied. GPU memory seems to be taken by the model but the gpu memory usage is not large(4G/16G).\nThen I tried to know why we are not fully utilizing GPU memory and bfc_allocator might be the reason.\nFrom what I am seeing, the implementation of bfc_allocator could be improved ---  to be aware of the memory swapping and hence improve performance.\nIn brief,\n\nMake UVM grow on demand\nImprove bfc_allocator with respect of UVM\n\nAre they already implemented?\nIf not, I believe they should be helpful and intriguing features we could contribute to.\nWhat do you think?\nThank you in advance!", "body": "Hi!\r\nI used a script to confirm that CUDA unified memory cannot grow on demand. \r\nFor your information, this is [the script I used](https://github.com/alapha23/playground/blob/master/cat_dog_tf.py)\r\n\r\nFurthermore, I found the GPU memory is poorly utilized when `per_process_gpu_memory_fraction` is set to a large value, e.g. 200. \r\n\r\nIn my case, with `batch_size` 512 and `per_process_gpu_memory_fraction` 200, around 4G gpu memory is utilized out of 16G, while around 360G of RAM is occupied. GPU memory seems to be taken by the model but the gpu memory usage is not large(4G/16G). \r\n\r\nThen I tried to know why we are not fully utilizing GPU memory and `bfc_allocator` might be the reason. \r\nFrom what I am seeing, the implementation of `bfc_allocator` could be improved ---  to be aware of the memory swapping and hence improve performance. \r\n\r\nIn brief, \r\n* Make UVM grow on demand\r\n* Improve bfc_allocator with respect of UVM\r\n\r\nAre they already implemented?\r\nIf not, I believe they should be helpful and intriguing features we could contribute to. \r\n\r\nWhat do you think?\r\n\r\nThank you in advance!"}