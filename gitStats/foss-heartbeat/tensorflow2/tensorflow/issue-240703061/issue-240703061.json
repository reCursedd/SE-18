{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11301", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11301/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11301/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11301/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11301", "id": 240703061, "node_id": "MDU6SXNzdWUyNDA3MDMwNjE=", "number": 11301, "title": "Blas SGEMM launch failed", "user": {"login": "pradyumnanpk", "id": 16674201, "node_id": "MDQ6VXNlcjE2Njc0MjAx", "avatar_url": "https://avatars1.githubusercontent.com/u/16674201?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pradyumnanpk", "html_url": "https://github.com/pradyumnanpk", "followers_url": "https://api.github.com/users/pradyumnanpk/followers", "following_url": "https://api.github.com/users/pradyumnanpk/following{/other_user}", "gists_url": "https://api.github.com/users/pradyumnanpk/gists{/gist_id}", "starred_url": "https://api.github.com/users/pradyumnanpk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pradyumnanpk/subscriptions", "organizations_url": "https://api.github.com/users/pradyumnanpk/orgs", "repos_url": "https://api.github.com/users/pradyumnanpk/repos", "events_url": "https://api.github.com/users/pradyumnanpk/events{/privacy}", "received_events_url": "https://api.github.com/users/pradyumnanpk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-07-05T16:15:15Z", "updated_at": "2018-10-24T21:29:02Z", "closed_at": "2018-10-24T21:29:01Z", "author_association": "NONE", "body_html": "<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: I am using Resnet code from tensorflow models with some modifications.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Fedora 24</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.2.0-rc2-21-g12f033d', '1.2.0')</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda_8.0.61-cudnnv5</li>\n<li><strong>GPU model and memory</strong>: Nvidia Titan X, 12 GB</li>\n</ul>\n<p>I am training a Resnet model on my own data from scratch. After successfully running for 65700 steps, it crashed with the following error:</p>\n<p>2017-06-30 16:27:50.302438: E tensorflow/stream_executor/cuda/cuda_blas.cc:543] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED<br>\nTraceback (most recent call last):<br>\nFile \"resnet_main.py\", line 178, in <br>\nif z%100 == 0:<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run<br>\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))<br>\nFile \"resnet_main.py\", line 171, in main<br>\ntf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)<br>\nFile \"resnet_main.py\", line 90, in train<br>\nmon_sess.run(model.train_op)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 505, in run<br>\nrun_metadata=run_metadata)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 842, in run<br>\nrun_metadata=run_metadata)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run<br>\nreturn self._sess.run(*args, **kwargs)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 952, in run<br>\nrun_metadata=run_metadata)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run<br>\nreturn self._sess.run(*args, **kwargs)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run<br>\nrun_metadata_ptr)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run<br>\nfeed_dict_string, options, run_metadata)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run<br>\ntarget_list, options, run_metadata)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=12544, n=1024, k=256<br>\n[[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]<br>\n[[Node: train_step/update/_6858 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"</a>]]</p>\n<p>Caused by op u'unit_3_2/sub3/conv3/Conv2D', defined at:<br>\nFile \"resnet_main.py\", line 178, in <br>\nif z%100 == 0:<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run<br>\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))<br>\nFile \"resnet_main.py\", line 171, in main<br>\ntf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)<br>\nFile \"resnet_main.py\", line 31, in train<br>\nmodel.build_graph()<br>\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 42, in build_graph<br>\nself._build_model()<br>\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 95, in _build_model<br>\nx = res_func(x, filters[3], filters[3], self._stride_arr(1), False)<br>\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 245, in _bottleneck_residual<br>\nx = self._conv('conv3', x, 1, out_filter / 4, out_filter, [1, 1, 1, 1])<br>\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 273, in _conv<br>\nreturn tf.nn.conv2d(x, kernel, strides, padding='SAME')<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 399, in conv2d<br>\ndata_format=data_format, name=name)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op<br>\nop_def=op_def)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in <strong>init</strong><br>\nself._traceback = _extract_stack()</p>\n<p>InternalError (see above for traceback): Blas SGEMM launch failed : m=12544, n=1024, k=256<br>\n[[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]<br>\n[[Node: train_step/update/_6858 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"</a>]]</p>\n<p>2017-06-30 16:27:55.680213: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x564291240800: CUDA_ERROR_MISALIGNED_ADDRESS</p>\n<p>On another machine with exact configuration, it is still running successfully (425,000 steps as of now). I don't understand why it crashed on one machine after running for 65,700 steps.</p>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I am using Resnet code from tensorflow models with some modifications.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 24\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): ('v1.2.0-rc2-21-g12f033d', '1.2.0')\nPython version: 2.7\nCUDA/cuDNN version: cuda_8.0.61-cudnnv5\nGPU model and memory: Nvidia Titan X, 12 GB\n\nI am training a Resnet model on my own data from scratch. After successfully running for 65700 steps, it crashed with the following error:\n2017-06-30 16:27:50.302438: E tensorflow/stream_executor/cuda/cuda_blas.cc:543] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED\nTraceback (most recent call last):\nFile \"resnet_main.py\", line 178, in \nif z%100 == 0:\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"resnet_main.py\", line 171, in main\ntf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\nFile \"resnet_main.py\", line 90, in train\nmon_sess.run(model.train_op)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 505, in run\nrun_metadata=run_metadata)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 842, in run\nrun_metadata=run_metadata)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run\nreturn self._sess.run(*args, **kwargs)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\nrun_metadata=run_metadata)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run\nreturn self._sess.run(*args, **kwargs)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\nrun_metadata_ptr)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\nfeed_dict_string, options, run_metadata)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\ntarget_list, options, run_metadata)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=12544, n=1024, k=256\n[[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]\n[[Node: train_step/update/_6858 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nCaused by op u'unit_3_2/sub3/conv3/Conv2D', defined at:\nFile \"resnet_main.py\", line 178, in \nif z%100 == 0:\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n_sys.exit(main(_sys.argv[:1] + flags_passthrough))\nFile \"resnet_main.py\", line 171, in main\ntf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\nFile \"resnet_main.py\", line 31, in train\nmodel.build_graph()\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 42, in build_graph\nself._build_model()\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 95, in _build_model\nx = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 245, in _bottleneck_residual\nx = self._conv('conv3', x, 1, out_filter / 4, out_filter, [1, 1, 1, 1])\nFile \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 273, in _conv\nreturn tf.nn.conv2d(x, kernel, strides, padding='SAME')\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 399, in conv2d\ndata_format=data_format, name=name)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\nop_def=op_def)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in init\nself._traceback = _extract_stack()\nInternalError (see above for traceback): Blas SGEMM launch failed : m=12544, n=1024, k=256\n[[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]\n[[Node: train_step/update/_6858 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2017-06-30 16:27:55.680213: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x564291240800: CUDA_ERROR_MISALIGNED_ADDRESS\nOn another machine with exact configuration, it is still running successfully (425,000 steps as of now). I don't understand why it crashed on one machine after running for 65,700 steps.", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am using Resnet code from tensorflow models with some modifications.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 24\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**: cuda_8.0.61-cudnnv5\r\n- **GPU model and memory**: Nvidia Titan X, 12 GB\r\n\r\nI am training a Resnet model on my own data from scratch. After successfully running for 65700 steps, it crashed with the following error:\r\n\r\n2017-06-30 16:27:50.302438: E tensorflow/stream_executor/cuda/cuda_blas.cc:543] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED\r\nTraceback (most recent call last):\r\n  File \"resnet_main.py\", line 178, in <module>\r\n    if z%100 == 0:\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"resnet_main.py\", line 171, in main\r\n    tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\r\n  File \"resnet_main.py\", line 90, in train\r\n    mon_sess.run(model.train_op)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 505, in run\r\n    run_metadata=run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 842, in run\r\n    run_metadata=run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    run_metadata=run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=12544, n=1024, k=256\r\n\t [[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]\r\n\t [[Node: train_step/update/_6858 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'unit_3_2/sub3/conv3/Conv2D', defined at:\r\n  File \"resnet_main.py\", line 178, in <module>\r\n    if z%100 == 0:\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"resnet_main.py\", line 171, in main\r\n    tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\r\n  File \"resnet_main.py\", line 31, in train\r\n    model.build_graph()\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 42, in build_graph\r\n    self._build_model()\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 95, in _build_model\r\n    x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 245, in _bottleneck_residual\r\n    x = self._conv('conv3', x, 1, out_filter / 4, out_filter, [1, 1, 1, 1])\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 273, in _conv\r\n    return tf.nn.conv2d(x, kernel, strides, padding='SAME')\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 399, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Blas SGEMM launch failed : m=12544, n=1024, k=256\r\n\t [[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]\r\n\t [[Node: train_step/update/_6858 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n2017-06-30 16:27:55.680213: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x564291240800: CUDA_ERROR_MISALIGNED_ADDRESS\r\n\r\nOn another machine with exact configuration, it is still running successfully (425,000 steps as of now). I don't understand why it crashed on one machine after running for 65,700 steps."}