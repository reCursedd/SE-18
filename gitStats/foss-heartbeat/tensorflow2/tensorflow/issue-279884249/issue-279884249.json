{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15165", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15165/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15165/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15165/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15165", "id": 279884249, "node_id": "MDU6SXNzdWUyNzk4ODQyNDk=", "number": 15165, "title": "LSTM layer in consistent with tf.keras v2.0.8-tf and keras 2.1.2", "user": {"login": "jolespin", "id": 9061708, "node_id": "MDQ6VXNlcjkwNjE3MDg=", "avatar_url": "https://avatars1.githubusercontent.com/u/9061708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jolespin", "html_url": "https://github.com/jolespin", "followers_url": "https://api.github.com/users/jolespin/followers", "following_url": "https://api.github.com/users/jolespin/following{/other_user}", "gists_url": "https://api.github.com/users/jolespin/gists{/gist_id}", "starred_url": "https://api.github.com/users/jolespin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jolespin/subscriptions", "organizations_url": "https://api.github.com/users/jolespin/orgs", "repos_url": "https://api.github.com/users/jolespin/repos", "events_url": "https://api.github.com/users/jolespin/events{/privacy}", "received_events_url": "https://api.github.com/users/jolespin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-12-06T19:51:34Z", "updated_at": "2018-02-08T16:59:09Z", "closed_at": "2018-02-08T16:59:09Z", "author_association": "NONE", "body_html": "<p>It looks like there are some inconsistencies with the output shape of the LSTM layer.</p>\n<p>Running the following code does not produce an error in <code>keras 2.1.2</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>model <span class=\"pl-k\">=</span> Sequential()\n\nconv_layer <span class=\"pl-k\">=</span> Conv1D(<span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">320</span>,\n                    <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">26</span>,\n                    <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                    <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>valid<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1000</span>,<span class=\"pl-c1\">4</span>))\n\nmodel.add(conv_layer)\nmodel.add(MaxPooling1D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">13</span>,\n                       <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">13</span>))\n\nmodel.add(LSTM(<span class=\"pl-c1\">320</span>, <span class=\"pl-v\">return_sequences</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\nmodel.add(Flatten())\nmodel.add(Dense(<span class=\"pl-c1\">925</span>,\n                <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Dense(<span class=\"pl-c1\">919</span>,\n                <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>))\n\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>binary_crossentropy<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>rmsprop<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])\n\n_________________________________________________________________\nLayer (<span class=\"pl-c1\">type</span>)                 Output Shape              Param <span class=\"pl-c\"><span class=\"pl-c\">#</span>   </span>\n<span class=\"pl-k\">================================================================</span><span class=\"pl-k\">=</span>\nconv1d_1 (Conv1D)            (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">975</span>, <span class=\"pl-c1\">320</span>)          <span class=\"pl-c1\">33600</span>     \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">75</span>, <span class=\"pl-c1\">320</span>)           <span class=\"pl-c1\">0</span>         \n_________________________________________________________________\nlstm_1 (<span class=\"pl-c1\">LSTM</span>)                (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">75</span>, <span class=\"pl-c1\">320</span>)           <span class=\"pl-c1\">820480</span>    \n_________________________________________________________________\nflatten_1 (Flatten)          (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">24000</span>)             <span class=\"pl-c1\">0</span>         \n_________________________________________________________________\ndense_1 (Dense)              (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">925</span>)               <span class=\"pl-c1\">22200925</span>  \n_________________________________________________________________\ndense_2 (Dense)              (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">919</span>)               <span class=\"pl-c1\">850994</span>    \n<span class=\"pl-k\">================================================================</span><span class=\"pl-k\">=</span>\nTotal params: <span class=\"pl-c1\">23</span>,<span class=\"pl-c1\">905</span>,<span class=\"pl-c1\">999</span>\nTrainable params: <span class=\"pl-c1\">23</span>,<span class=\"pl-c1\">905</span>,<span class=\"pl-c1\">999</span>\nNon<span class=\"pl-k\">-</span>trainable params: <span class=\"pl-c1\">0</span>\n_________________________________________________________________</pre></div>\n<p>but produces this error in <code>keras v2.0.8-tf</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">389</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">956501e5fb90</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n     <span class=\"pl-c1\">16</span> model.add(Flatten())\n     <span class=\"pl-c1\">17</span> model.add(Dense(<span class=\"pl-c1\">925</span>,\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">18</span>                 <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\n     <span class=\"pl-c1\">19</span> model.add(Dense(<span class=\"pl-c1\">919</span>,\n     <span class=\"pl-c1\">20</span>                 <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>))\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>keras<span class=\"pl-k\">/</span>_impl<span class=\"pl-k\">/</span>keras<span class=\"pl-k\">/</span>models.py <span class=\"pl-k\">in</span> add(<span class=\"pl-c1\">self</span>, layer)\n    <span class=\"pl-c1\">499</span>           output_tensors<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.outputs)\n    <span class=\"pl-c1\">500</span>     <span class=\"pl-k\">else</span>:\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">501</span>       output_tensor <span class=\"pl-k\">=</span> layer(<span class=\"pl-c1\">self</span>.outputs[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-c1\">502</span>       <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(output_tensor, <span class=\"pl-c1\">list</span>):\n    <span class=\"pl-c1\">503</span>         <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">TypeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>All layers in a Sequential model <span class=\"pl-pds\">'</span></span>\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>keras<span class=\"pl-k\">/</span>_impl<span class=\"pl-k\">/</span>keras<span class=\"pl-k\">/</span>engine<span class=\"pl-k\">/</span>topology.py <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-c1\">self</span>, inputs, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">250</span>     <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    251     # Actually call the layer (optionally building it).</span>\n<span class=\"pl-s\">--&gt; 252     output = super(Layer, self).__call__(inputs, **kwargs)</span>\n<span class=\"pl-s\">    253 </span>\n<span class=\"pl-s\">    254     # Update learning phase info.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)</span>\n<span class=\"pl-s\">    557           input_shapes = [x.get_shape() for x in input_list]</span>\n<span class=\"pl-s\">    558           if len(input_shapes) == 1:</span>\n<span class=\"pl-s\">--&gt; 559             self.build(input_shapes[0])</span>\n<span class=\"pl-s\">    560           else:</span>\n<span class=\"pl-s\">    561             self.build(input_shapes)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/core.py in build(self, input_shape)</span>\n<span class=\"pl-s\">    125     input_shape = tensor_shape.TensorShape(input_shape)</span>\n<span class=\"pl-s\">    126     if input_shape[-1].value is None:</span>\n<span class=\"pl-s\">--&gt; 127       raise ValueError('The last dimension of the inputs to `Dense` '</span>\n<span class=\"pl-s\">    128                        'should be defined. Found `None`.')</span>\n<span class=\"pl-s\">    129     self.input_spec = base.InputSpec(min_ndim=2,</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.</span></pre></div>\n<p>If I keep return_sequences = True and remove Flatten() after the LSTM I get the following:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_49 (Conv1D)           (None, 975, 320)          33600     \n_________________________________________________________________\nmax_pooling1d_49 (MaxPooling (None, 75, 320)           0         \n_________________________________________________________________\nlstm_33 (LSTM)               (None, None, 320)         820480    \n_________________________________________________________________\ndense_92 (Dense)             (None, None, 925)         296925    \n_________________________________________________________________\ndense_93 (Dense)             (None, None, 919)         850994    \n=================================================================\nTotal params: 2,001,999\nTrainable params: 2,001,999\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n<p>More on the discussion in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"276927599\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/uci-cbcl/DanQ/issues/9\" data-hovercard-type=\"issue\" data-hovercard-url=\"/uci-cbcl/DanQ/issues/9/hovercard?comment_id=348377899&amp;comment_type=issue_comment\" href=\"https://github.com/uci-cbcl/DanQ/issues/9#issuecomment-348377899\">uci-cbcl/DanQ#9 (comment)</a></p>", "body_text": "It looks like there are some inconsistencies with the output shape of the LSTM layer.\nRunning the following code does not produce an error in keras 2.1.2:\nmodel = Sequential()\n\nconv_layer = Conv1D(filters=320,\n                    kernel_size=26,\n                    strides=1,\n                    padding='valid',\n                    activation='relu',\n                    input_shape=(1000,4))\n\nmodel.add(conv_layer)\nmodel.add(MaxPooling1D(pool_size=13,\n                       strides=13))\n\nmodel.add(LSTM(320, return_sequences=True))\n\nmodel.add(Flatten())\nmodel.add(Dense(925,\n                activation='relu'))\nmodel.add(Dense(919,\n                activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_1 (Conv1D)            (None, 975, 320)          33600     \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 75, 320)           0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 75, 320)           820480    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 24000)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 925)               22200925  \n_________________________________________________________________\ndense_2 (Dense)              (None, 919)               850994    \n=================================================================\nTotal params: 23,905,999\nTrainable params: 23,905,999\nNon-trainable params: 0\n_________________________________________________________________\nbut produces this error in keras v2.0.8-tf:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-389-956501e5fb90> in <module>()\n     16 model.add(Flatten())\n     17 model.add(Dense(925,\n---> 18                 activation='relu'))\n     19 model.add(Dense(919,\n     20                 activation='sigmoid'))\n\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py in add(self, layer)\n    499           output_tensors=self.outputs)\n    500     else:\n--> 501       output_tensor = layer(self.outputs[0])\n    502       if isinstance(output_tensor, list):\n    503         raise TypeError('All layers in a Sequential model '\n\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in __call__(self, inputs, **kwargs)\n    250     \"\"\"\n    251     # Actually call the layer (optionally building it).\n--> 252     output = super(Layer, self).__call__(inputs, **kwargs)\n    253 \n    254     # Update learning phase info.\n\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    557           input_shapes = [x.get_shape() for x in input_list]\n    558           if len(input_shapes) == 1:\n--> 559             self.build(input_shapes[0])\n    560           else:\n    561             self.build(input_shapes)\n\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/core.py in build(self, input_shape)\n    125     input_shape = tensor_shape.TensorShape(input_shape)\n    126     if input_shape[-1].value is None:\n--> 127       raise ValueError('The last dimension of the inputs to `Dense` '\n    128                        'should be defined. Found `None`.')\n    129     self.input_spec = base.InputSpec(min_ndim=2,\n\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\nIf I keep return_sequences = True and remove Flatten() after the LSTM I get the following:\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_49 (Conv1D)           (None, 975, 320)          33600     \n_________________________________________________________________\nmax_pooling1d_49 (MaxPooling (None, 75, 320)           0         \n_________________________________________________________________\nlstm_33 (LSTM)               (None, None, 320)         820480    \n_________________________________________________________________\ndense_92 (Dense)             (None, None, 925)         296925    \n_________________________________________________________________\ndense_93 (Dense)             (None, None, 919)         850994    \n=================================================================\nTotal params: 2,001,999\nTrainable params: 2,001,999\nNon-trainable params: 0\n_________________________________________________________________\n\nMore on the discussion in uci-cbcl/DanQ#9 (comment)", "body": "It looks like there are some inconsistencies with the output shape of the LSTM layer. \r\n\r\nRunning the following code does not produce an error in `keras 2.1.2`:\r\n```python\r\nmodel = Sequential()\r\n\r\nconv_layer = Conv1D(filters=320,\r\n                    kernel_size=26,\r\n                    strides=1,\r\n                    padding='valid',\r\n                    activation='relu',\r\n                    input_shape=(1000,4))\r\n\r\nmodel.add(conv_layer)\r\nmodel.add(MaxPooling1D(pool_size=13,\r\n                       strides=13))\r\n\r\nmodel.add(LSTM(320, return_sequences=True))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(925,\r\n                activation='relu'))\r\nmodel.add(Dense(919,\r\n                activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv1d_1 (Conv1D)            (None, 975, 320)          33600     \r\n_________________________________________________________________\r\nmax_pooling1d_1 (MaxPooling1 (None, 75, 320)           0         \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 75, 320)           820480    \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 24000)             0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 925)               22200925  \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 919)               850994    \r\n=================================================================\r\nTotal params: 23,905,999\r\nTrainable params: 23,905,999\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nbut produces this error in `keras v2.0.8-tf`:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-389-956501e5fb90> in <module>()\r\n     16 model.add(Flatten())\r\n     17 model.add(Dense(925,\r\n---> 18                 activation='relu'))\r\n     19 model.add(Dense(919,\r\n     20                 activation='sigmoid'))\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py in add(self, layer)\r\n    499           output_tensors=self.outputs)\r\n    500     else:\r\n--> 501       output_tensor = layer(self.outputs[0])\r\n    502       if isinstance(output_tensor, list):\r\n    503         raise TypeError('All layers in a Sequential model '\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in __call__(self, inputs, **kwargs)\r\n    250     \"\"\"\r\n    251     # Actually call the layer (optionally building it).\r\n--> 252     output = super(Layer, self).__call__(inputs, **kwargs)\r\n    253 \r\n    254     # Update learning phase info.\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    557           input_shapes = [x.get_shape() for x in input_list]\r\n    558           if len(input_shapes) == 1:\r\n--> 559             self.build(input_shapes[0])\r\n    560           else:\r\n    561             self.build(input_shapes)\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/core.py in build(self, input_shape)\r\n    125     input_shape = tensor_shape.TensorShape(input_shape)\r\n    126     if input_shape[-1].value is None:\r\n--> 127       raise ValueError('The last dimension of the inputs to `Dense` '\r\n    128                        'should be defined. Found `None`.')\r\n    129     self.input_spec = base.InputSpec(min_ndim=2,\r\n\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```\r\n\r\nIf I keep return_sequences = True and remove Flatten() after the LSTM I get the following:\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv1d_49 (Conv1D)           (None, 975, 320)          33600     \r\n_________________________________________________________________\r\nmax_pooling1d_49 (MaxPooling (None, 75, 320)           0         \r\n_________________________________________________________________\r\nlstm_33 (LSTM)               (None, None, 320)         820480    \r\n_________________________________________________________________\r\ndense_92 (Dense)             (None, None, 925)         296925    \r\n_________________________________________________________________\r\ndense_93 (Dense)             (None, None, 919)         850994    \r\n=================================================================\r\nTotal params: 2,001,999\r\nTrainable params: 2,001,999\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\nMore on the discussion in https://github.com/uci-cbcl/DanQ/issues/9#issuecomment-348377899"}