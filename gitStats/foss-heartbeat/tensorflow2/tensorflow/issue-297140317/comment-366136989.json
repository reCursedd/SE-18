{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366136989", "html_url": "https://github.com/tensorflow/tensorflow/issues/17013#issuecomment-366136989", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17013", "id": 366136989, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjEzNjk4OQ==", "user": {"login": "gabrielrlp", "id": 29979626, "node_id": "MDQ6VXNlcjI5OTc5NjI2", "avatar_url": "https://avatars3.githubusercontent.com/u/29979626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gabrielrlp", "html_url": "https://github.com/gabrielrlp", "followers_url": "https://api.github.com/users/gabrielrlp/followers", "following_url": "https://api.github.com/users/gabrielrlp/following{/other_user}", "gists_url": "https://api.github.com/users/gabrielrlp/gists{/gist_id}", "starred_url": "https://api.github.com/users/gabrielrlp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gabrielrlp/subscriptions", "organizations_url": "https://api.github.com/users/gabrielrlp/orgs", "repos_url": "https://api.github.com/users/gabrielrlp/repos", "events_url": "https://api.github.com/users/gabrielrlp/events{/privacy}", "received_events_url": "https://api.github.com/users/gabrielrlp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-16T03:45:21Z", "updated_at": "2018-02-16T03:45:21Z", "author_association": "NONE", "body_html": "<p>I am using <code>armeabi-v7a</code> for real device and <code>x86_64</code> for emulated device. But I created the library for both of them. Yes, I repeated every step for each one with the correct flag <code>--cpu=armeabi-v7a</code> and <code>--cpu=x86_64</code>.</p>\n<p><a href=\"https://medium.com/@daj/how-to-shrink-the-tensorflow-android-inference-library-cb698facf758\" rel=\"nofollow\">On your blog</a>,</p>\n<blockquote>\n<p>Copy your new <code>libtensorflow_inference.so</code> file into your project, putting it in the correct folder for the architecture that you built, e.g. <code>app/src/main/jniLibs/armeabi-v7a/</code>.</p>\n</blockquote>\n<p>In my case: <code>jniLibs/armeabi-v7a</code> and <code>jniLibs/x86_64</code></p>\n<p>Yeah, We have already use a lot of debug <g-emoji class=\"g-emoji\" alias=\"smile\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f604.png\">\ud83d\ude04</g-emoji></p>\n<p>I think the only thing I did not try was to download your project and test it.</p>", "body_text": "I am using armeabi-v7a for real device and x86_64 for emulated device. But I created the library for both of them. Yes, I repeated every step for each one with the correct flag --cpu=armeabi-v7a and --cpu=x86_64.\nOn your blog,\n\nCopy your new libtensorflow_inference.so file into your project, putting it in the correct folder for the architecture that you built, e.g. app/src/main/jniLibs/armeabi-v7a/.\n\nIn my case: jniLibs/armeabi-v7a and jniLibs/x86_64\nYeah, We have already use a lot of debug \ud83d\ude04\nI think the only thing I did not try was to download your project and test it.", "body": "I am using `armeabi-v7a` for real device and `x86_64` for emulated device. But I created the library for both of them. Yes, I repeated every step for each one with the correct flag `--cpu=armeabi-v7a` and `--cpu=x86_64`.\r\n\r\n[On your blog](https://medium.com/@daj/how-to-shrink-the-tensorflow-android-inference-library-cb698facf758),\r\n\r\n> Copy your new `libtensorflow_inference.so` file into your project, putting it in the correct folder for the architecture that you built, e.g. `app/src/main/jniLibs/armeabi-v7a/`.\r\n\r\nIn my case: `jniLibs/armeabi-v7a` and `jniLibs/x86_64`\r\n\r\nYeah, We have already use a lot of debug :smile:\r\n\r\nI think the only thing I did not try was to download your project and test it."}