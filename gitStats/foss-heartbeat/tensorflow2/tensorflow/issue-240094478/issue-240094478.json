{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11236", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11236/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11236/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11236/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11236", "id": 240094478, "node_id": "MDU6SXNzdWUyNDAwOTQ0Nzg=", "number": 11236, "title": "Cannot use AdamOptimizer in C++ when running graph defined from Python", "user": {"login": "fferroni", "id": 16327442, "node_id": "MDQ6VXNlcjE2MzI3NDQy", "avatar_url": "https://avatars1.githubusercontent.com/u/16327442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fferroni", "html_url": "https://github.com/fferroni", "followers_url": "https://api.github.com/users/fferroni/followers", "following_url": "https://api.github.com/users/fferroni/following{/other_user}", "gists_url": "https://api.github.com/users/fferroni/gists{/gist_id}", "starred_url": "https://api.github.com/users/fferroni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fferroni/subscriptions", "organizations_url": "https://api.github.com/users/fferroni/orgs", "repos_url": "https://api.github.com/users/fferroni/repos", "events_url": "https://api.github.com/users/fferroni/events{/privacy}", "received_events_url": "https://api.github.com/users/fferroni/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-03T07:58:49Z", "updated_at": "2017-11-07T08:57:38Z", "closed_at": "2017-07-19T11:55:55Z", "author_association": "NONE", "body_html": "<p>Running Ubuntu 14.04, with r1.2 Tensorflow</p>\n<p>I have defined a graph in Python</p>\n<pre><code>features = 784\noutput_dim = 10\ngraph_name = \"graph.pb\"\ngraph_folder = \"/home/fran/Repositories/tensorflow/bazel-bin/tensorflow/test\"\ninput_node_name = \"input\"\noutput_node_name = \"output\"\n\nwith tf.Session() as session:\n    x = tf.placeholder(tf.float32, [None, features], name=\"input\")\n    W = tf.Variable(tf.zeros([features, output_dim]), dtype=tf.float32, name=\"Weight\")\n    b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32, name=\"bias\")\n    y_pred = tf.nn.softmax(tf.matmul(x, W)+b, name=output_node_name)\n\n    y_true = tf.placeholder(tf.float32, [None, 10], name=\"y_true\")\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]), name=\"loss\")\n    optimizer = tf.train.AdamOptimizer(0.00001)\n    train_step = optimizer.minimize(cross_entropy, name=\"train\")\n    init = tf.variables_initializer(tf.global_variables(), name='init_all_vars_op')\n    saver = tf.train.Saver()\n    tf.train.write_graph(session.graph_def, graph_folder, graph_name, as_text=False)\n</code></pre>\n<p>If I run this graph in a C++ program, where I load the graph, initialize the variables and feed X and Y tensors to \"train\", I get the following error (works fine with other optimizers like GradientDescent or RMSPropOptimizer):</p>\n<pre><code>Training.\nStep: 0; Loss: 9.25913\n2017-07-03 09:54:43.264264: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op&lt;name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -&gt; out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false&gt;; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\n2017-07-03 09:54:43.264468: F tensorflow/driving_school/training.cc:102] Non-OK-status: session-&gt;Run(inputs, {}, {\"train\"}, nullptr) status: Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op&lt;name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -&gt; out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false&gt;; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\nAborted (core dumped)\n</code></pre>\n<p>Is this a bug?</p>\n<p>For reference, the code I use to load the graph definition and train is the following:</p>\n<pre><code>#include \"tensorflow/core/public/session.h\"\n#include \"tensorflow/core/platform/env.h\"\n\nusing namespace tensorflow;\n\nint main(int argc, char* argv[]) {\n\n    Session* session;\n    std::cout &lt;&lt; \"Initializing Tensorflow session.\" &lt;&lt; std::endl;\n    TF_CHECK_OK(NewSession(SessionOptions(), &amp;session));\n\n    // Read in the protobuf graph we previously exported\n    // (The path is relative to the cwd. Keep this in mind\n    // when using `bazel run` since the cwd isn't where you call\n    // `bazel run` but from inside a temp folder.)\n    GraphDef graph_def;\n    std::cout &lt;&lt; \"Reading protobuf graph.\" &lt;&lt; std::endl;\n    TF_CHECK_OK(ReadBinaryProto(Env::Default(), \"graph.pb\", &amp;graph_def));\n\n    std::cout &lt;&lt; \"Loading graph in Tensorflow session.\" &lt;&lt; std::endl;\n    TF_CHECK_OK(session-&gt;Create(graph_def));\n\n    int batch_size = 64;\n    int input_feature_len = 784;\n    int output_feature_len = 10;\n    int training_steps = 1000;\n    int save_interval = 100;\n    std::string save_path = \"/home/fran/Repositories/tensorflow/tensorflow/test/\";\n\n    std::cout &lt;&lt; \"Initializing Tensorflow I/O tensors.\" &lt;&lt; std::endl;\n    Tensor X(DT_FLOAT, TensorShape({batch_size, input_feature_len}));\n    Tensor Y(DT_FLOAT, TensorShape({batch_size, output_feature_len}));\n\n    // Initialize our variables for training\n    std::cout &lt;&lt; \"Initializing Tensorflow variables.\" &lt;&lt; std::endl;\n    TF_CHECK_OK(session-&gt;Run({}, {}, {\"init_all_vars_op\"}, nullptr));\n\n    auto _XTensor = X.matrix&lt;float&gt;();\n    auto _YTensor = Y.matrix&lt;float&gt;();\n    _XTensor.setRandom();\n    _YTensor.setRandom();\n\n    // Assuming graph contains single input X, with node name \"input\"\n    // and single ground truth placeholder, with node name \"y_true\"\n    std::vector&lt;std::pair&lt;string, Tensor&gt;&gt; inputs = {\n            { \"input\", X },\n            { \"y_true\", Y }\n    };\n\n    // for checkpoint generation\n    Tensor model_string(DT_STRING, TensorShape( { 1, 1 } ) );\n\n    // Initialize output pointer for loss metric.\n    // Assuming graph contains single loss, with node name \"loss\"\n    // and single training node, with node name \"train\"\n    std::cout &lt;&lt; \"Training.\" &lt;&lt; std::endl;\n    std::vector&lt;Tensor&gt; output;\n    for (int i=0; i&lt;training_steps; ++i) {\n        TF_CHECK_OK(session-&gt;Run(inputs, {\"loss\"}, {}, &amp;output)); // Get loss (for monitoring)\n        float loss = output[0].scalar&lt;float&gt;()(0);\n        std::cout &lt;&lt; \"Step: \" &lt;&lt; i &lt;&lt; \"; Loss: \" &lt;&lt; loss &lt;&lt; std::endl;\n        TF_CHECK_OK(session-&gt;Run(inputs, {}, {\"train\"}, nullptr)); // Train\n        output.clear();\n\n        // to save checkpoint feed name of checkpoint file as saver_def.filename_tensor_name\n        // and fetch the value of saver_def.save_tensor_name, as per graph definition script.\n        if (i % save_interval == 0){\n            model_string.matrix&lt; std::string &gt;()( 0, 0 ) = save_path + \"model_checkpoint_step\" + std::to_string(i) + \".ckpt\";\n            TF_CHECK_OK(session-&gt;Run({{\"save/Const:0\", model_string}}, {}, {\"save/control_dependency\"}, nullptr));\n        };\n    };\n    std::cout &lt;&lt; \"Training complete.\" &lt;&lt; std::endl;\n    \n    std::cout &lt;&lt; \"Closing Tensorflow session.\" &lt;&lt; std::endl;\n    session-&gt;Close();\n    delete session;\n    return 0;\n}\n</code></pre>", "body_text": "Running Ubuntu 14.04, with r1.2 Tensorflow\nI have defined a graph in Python\nfeatures = 784\noutput_dim = 10\ngraph_name = \"graph.pb\"\ngraph_folder = \"/home/fran/Repositories/tensorflow/bazel-bin/tensorflow/test\"\ninput_node_name = \"input\"\noutput_node_name = \"output\"\n\nwith tf.Session() as session:\n    x = tf.placeholder(tf.float32, [None, features], name=\"input\")\n    W = tf.Variable(tf.zeros([features, output_dim]), dtype=tf.float32, name=\"Weight\")\n    b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32, name=\"bias\")\n    y_pred = tf.nn.softmax(tf.matmul(x, W)+b, name=output_node_name)\n\n    y_true = tf.placeholder(tf.float32, [None, 10], name=\"y_true\")\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]), name=\"loss\")\n    optimizer = tf.train.AdamOptimizer(0.00001)\n    train_step = optimizer.minimize(cross_entropy, name=\"train\")\n    init = tf.variables_initializer(tf.global_variables(), name='init_all_vars_op')\n    saver = tf.train.Saver()\n    tf.train.write_graph(session.graph_def, graph_folder, graph_name, as_text=False)\n\nIf I run this graph in a C++ program, where I load the graph, initialize the variables and feed X and Y tensors to \"train\", I get the following error (works fine with other optimizers like GradientDescent or RMSPropOptimizer):\nTraining.\nStep: 0; Loss: 9.25913\n2017-07-03 09:54:43.264264: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\n2017-07-03 09:54:43.264468: F tensorflow/driving_school/training.cc:102] Non-OK-status: session->Run(inputs, {}, {\"train\"}, nullptr) status: Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\nAborted (core dumped)\n\nIs this a bug?\nFor reference, the code I use to load the graph definition and train is the following:\n#include \"tensorflow/core/public/session.h\"\n#include \"tensorflow/core/platform/env.h\"\n\nusing namespace tensorflow;\n\nint main(int argc, char* argv[]) {\n\n    Session* session;\n    std::cout << \"Initializing Tensorflow session.\" << std::endl;\n    TF_CHECK_OK(NewSession(SessionOptions(), &session));\n\n    // Read in the protobuf graph we previously exported\n    // (The path is relative to the cwd. Keep this in mind\n    // when using `bazel run` since the cwd isn't where you call\n    // `bazel run` but from inside a temp folder.)\n    GraphDef graph_def;\n    std::cout << \"Reading protobuf graph.\" << std::endl;\n    TF_CHECK_OK(ReadBinaryProto(Env::Default(), \"graph.pb\", &graph_def));\n\n    std::cout << \"Loading graph in Tensorflow session.\" << std::endl;\n    TF_CHECK_OK(session->Create(graph_def));\n\n    int batch_size = 64;\n    int input_feature_len = 784;\n    int output_feature_len = 10;\n    int training_steps = 1000;\n    int save_interval = 100;\n    std::string save_path = \"/home/fran/Repositories/tensorflow/tensorflow/test/\";\n\n    std::cout << \"Initializing Tensorflow I/O tensors.\" << std::endl;\n    Tensor X(DT_FLOAT, TensorShape({batch_size, input_feature_len}));\n    Tensor Y(DT_FLOAT, TensorShape({batch_size, output_feature_len}));\n\n    // Initialize our variables for training\n    std::cout << \"Initializing Tensorflow variables.\" << std::endl;\n    TF_CHECK_OK(session->Run({}, {}, {\"init_all_vars_op\"}, nullptr));\n\n    auto _XTensor = X.matrix<float>();\n    auto _YTensor = Y.matrix<float>();\n    _XTensor.setRandom();\n    _YTensor.setRandom();\n\n    // Assuming graph contains single input X, with node name \"input\"\n    // and single ground truth placeholder, with node name \"y_true\"\n    std::vector<std::pair<string, Tensor>> inputs = {\n            { \"input\", X },\n            { \"y_true\", Y }\n    };\n\n    // for checkpoint generation\n    Tensor model_string(DT_STRING, TensorShape( { 1, 1 } ) );\n\n    // Initialize output pointer for loss metric.\n    // Assuming graph contains single loss, with node name \"loss\"\n    // and single training node, with node name \"train\"\n    std::cout << \"Training.\" << std::endl;\n    std::vector<Tensor> output;\n    for (int i=0; i<training_steps; ++i) {\n        TF_CHECK_OK(session->Run(inputs, {\"loss\"}, {}, &output)); // Get loss (for monitoring)\n        float loss = output[0].scalar<float>()(0);\n        std::cout << \"Step: \" << i << \"; Loss: \" << loss << std::endl;\n        TF_CHECK_OK(session->Run(inputs, {}, {\"train\"}, nullptr)); // Train\n        output.clear();\n\n        // to save checkpoint feed name of checkpoint file as saver_def.filename_tensor_name\n        // and fetch the value of saver_def.save_tensor_name, as per graph definition script.\n        if (i % save_interval == 0){\n            model_string.matrix< std::string >()( 0, 0 ) = save_path + \"model_checkpoint_step\" + std::to_string(i) + \".ckpt\";\n            TF_CHECK_OK(session->Run({{\"save/Const:0\", model_string}}, {}, {\"save/control_dependency\"}, nullptr));\n        };\n    };\n    std::cout << \"Training complete.\" << std::endl;\n    \n    std::cout << \"Closing Tensorflow session.\" << std::endl;\n    session->Close();\n    delete session;\n    return 0;\n}", "body": "Running Ubuntu 14.04, with r1.2 Tensorflow\r\n\r\nI have defined a graph in Python\r\n```\r\nfeatures = 784\r\noutput_dim = 10\r\ngraph_name = \"graph.pb\"\r\ngraph_folder = \"/home/fran/Repositories/tensorflow/bazel-bin/tensorflow/test\"\r\ninput_node_name = \"input\"\r\noutput_node_name = \"output\"\r\n\r\nwith tf.Session() as session:\r\n    x = tf.placeholder(tf.float32, [None, features], name=\"input\")\r\n    W = tf.Variable(tf.zeros([features, output_dim]), dtype=tf.float32, name=\"Weight\")\r\n    b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32, name=\"bias\")\r\n    y_pred = tf.nn.softmax(tf.matmul(x, W)+b, name=output_node_name)\r\n\r\n    y_true = tf.placeholder(tf.float32, [None, 10], name=\"y_true\")\r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]), name=\"loss\")\r\n    optimizer = tf.train.AdamOptimizer(0.00001)\r\n    train_step = optimizer.minimize(cross_entropy, name=\"train\")\r\n    init = tf.variables_initializer(tf.global_variables(), name='init_all_vars_op')\r\n    saver = tf.train.Saver()\r\n    tf.train.write_graph(session.graph_def, graph_folder, graph_name, as_text=False)\r\n```\r\n\r\nIf I run this graph in a C++ program, where I load the graph, initialize the variables and feed X and Y tensors to \"train\", I get the following error (works fine with other optimizers like GradientDescent or RMSPropOptimizer):\r\n```\r\nTraining.\r\nStep: 0; Loss: 9.25913\r\n2017-07-03 09:54:43.264264: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\r\n2017-07-03 09:54:43.264468: F tensorflow/driving_school/training.cc:102] Non-OK-status: session->Run(inputs, {}, {\"train\"}, nullptr) status: Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\r\nAborted (core dumped)\r\n```\r\nIs this a bug?\r\n\r\nFor reference, the code I use to load the graph definition and train is the following:\r\n```\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nint main(int argc, char* argv[]) {\r\n\r\n    Session* session;\r\n    std::cout << \"Initializing Tensorflow session.\" << std::endl;\r\n    TF_CHECK_OK(NewSession(SessionOptions(), &session));\r\n\r\n    // Read in the protobuf graph we previously exported\r\n    // (The path is relative to the cwd. Keep this in mind\r\n    // when using `bazel run` since the cwd isn't where you call\r\n    // `bazel run` but from inside a temp folder.)\r\n    GraphDef graph_def;\r\n    std::cout << \"Reading protobuf graph.\" << std::endl;\r\n    TF_CHECK_OK(ReadBinaryProto(Env::Default(), \"graph.pb\", &graph_def));\r\n\r\n    std::cout << \"Loading graph in Tensorflow session.\" << std::endl;\r\n    TF_CHECK_OK(session->Create(graph_def));\r\n\r\n    int batch_size = 64;\r\n    int input_feature_len = 784;\r\n    int output_feature_len = 10;\r\n    int training_steps = 1000;\r\n    int save_interval = 100;\r\n    std::string save_path = \"/home/fran/Repositories/tensorflow/tensorflow/test/\";\r\n\r\n    std::cout << \"Initializing Tensorflow I/O tensors.\" << std::endl;\r\n    Tensor X(DT_FLOAT, TensorShape({batch_size, input_feature_len}));\r\n    Tensor Y(DT_FLOAT, TensorShape({batch_size, output_feature_len}));\r\n\r\n    // Initialize our variables for training\r\n    std::cout << \"Initializing Tensorflow variables.\" << std::endl;\r\n    TF_CHECK_OK(session->Run({}, {}, {\"init_all_vars_op\"}, nullptr));\r\n\r\n    auto _XTensor = X.matrix<float>();\r\n    auto _YTensor = Y.matrix<float>();\r\n    _XTensor.setRandom();\r\n    _YTensor.setRandom();\r\n\r\n    // Assuming graph contains single input X, with node name \"input\"\r\n    // and single ground truth placeholder, with node name \"y_true\"\r\n    std::vector<std::pair<string, Tensor>> inputs = {\r\n            { \"input\", X },\r\n            { \"y_true\", Y }\r\n    };\r\n\r\n    // for checkpoint generation\r\n    Tensor model_string(DT_STRING, TensorShape( { 1, 1 } ) );\r\n\r\n    // Initialize output pointer for loss metric.\r\n    // Assuming graph contains single loss, with node name \"loss\"\r\n    // and single training node, with node name \"train\"\r\n    std::cout << \"Training.\" << std::endl;\r\n    std::vector<Tensor> output;\r\n    for (int i=0; i<training_steps; ++i) {\r\n        TF_CHECK_OK(session->Run(inputs, {\"loss\"}, {}, &output)); // Get loss (for monitoring)\r\n        float loss = output[0].scalar<float>()(0);\r\n        std::cout << \"Step: \" << i << \"; Loss: \" << loss << std::endl;\r\n        TF_CHECK_OK(session->Run(inputs, {}, {\"train\"}, nullptr)); // Train\r\n        output.clear();\r\n\r\n        // to save checkpoint feed name of checkpoint file as saver_def.filename_tensor_name\r\n        // and fetch the value of saver_def.save_tensor_name, as per graph definition script.\r\n        if (i % save_interval == 0){\r\n            model_string.matrix< std::string >()( 0, 0 ) = save_path + \"model_checkpoint_step\" + std::to_string(i) + \".ckpt\";\r\n            TF_CHECK_OK(session->Run({{\"save/Const:0\", model_string}}, {}, {\"save/control_dependency\"}, nullptr));\r\n        };\r\n    };\r\n    std::cout << \"Training complete.\" << std::endl;\r\n    \r\n    std::cout << \"Closing Tensorflow session.\" << std::endl;\r\n    session->Close();\r\n    delete session;\r\n    return 0;\r\n}\r\n```\r\n"}