{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289423028", "html_url": "https://github.com/tensorflow/tensorflow/issues/6011#issuecomment-289423028", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6011", "id": 289423028, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTQyMzAyOA==", "user": {"login": "luizgh", "id": 1473080, "node_id": "MDQ6VXNlcjE0NzMwODA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1473080?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luizgh", "html_url": "https://github.com/luizgh", "followers_url": "https://api.github.com/users/luizgh/followers", "following_url": "https://api.github.com/users/luizgh/following{/other_user}", "gists_url": "https://api.github.com/users/luizgh/gists{/gist_id}", "starred_url": "https://api.github.com/users/luizgh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luizgh/subscriptions", "organizations_url": "https://api.github.com/users/luizgh/orgs", "repos_url": "https://api.github.com/users/luizgh/repos", "events_url": "https://api.github.com/users/luizgh/events{/privacy}", "received_events_url": "https://api.github.com/users/luizgh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-27T11:10:20Z", "updated_at": "2017-03-27T11:10:20Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23121591\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Evan-Gao\">@Evan-Gao</a>,</p>\n<p>Theano will backpropagate only through the part of the input tensor that was selected in the slice operation. I ran a quick experiment and Tensorflow does the same:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\ninputs = tf.placeholder(dtype=tf.float32, shape=(2,3,3))\nwhich_channel = tf.placeholder(dtype=tf.int32)\nsliced = inputs[which_channel]\ncost = tf.norm(sliced)\ngrad = tf.gradients(cost, inputs)\n\nsess = tf.Session()\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:0}))\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:1}))\n</code></pre>\n<p>This code uses a symbolic variable (\"which_channel\") to slice the input, and considers the cost as the norm of the sliced tensor. We can see that backpropagation will only affect the part of the input that was selected in the slice operation: when we select the channel 0, it shows a gradient different than 0 for the first channel, with zeros in the gradient for the second channel, and vice versa when we select which_channel=1</p>", "body_text": "Hi @Evan-Gao,\nTheano will backpropagate only through the part of the input tensor that was selected in the slice operation. I ran a quick experiment and Tensorflow does the same:\nimport numpy as np\nimport tensorflow as tf\n\ninputs = tf.placeholder(dtype=tf.float32, shape=(2,3,3))\nwhich_channel = tf.placeholder(dtype=tf.int32)\nsliced = inputs[which_channel]\ncost = tf.norm(sliced)\ngrad = tf.gradients(cost, inputs)\n\nsess = tf.Session()\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:0}))\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:1}))\n\nThis code uses a symbolic variable (\"which_channel\") to slice the input, and considers the cost as the norm of the sliced tensor. We can see that backpropagation will only affect the part of the input that was selected in the slice operation: when we select the channel 0, it shows a gradient different than 0 for the first channel, with zeros in the gradient for the second channel, and vice versa when we select which_channel=1", "body": "Hi @Evan-Gao,\r\n\r\nTheano will backpropagate only through the part of the input tensor that was selected in the slice operation. I ran a quick experiment and Tensorflow does the same:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninputs = tf.placeholder(dtype=tf.float32, shape=(2,3,3))\r\nwhich_channel = tf.placeholder(dtype=tf.int32)\r\nsliced = inputs[which_channel]\r\ncost = tf.norm(sliced)\r\ngrad = tf.gradients(cost, inputs)\r\n\r\nsess = tf.Session()\r\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:0}))\r\nprint(sess.run(grad, feed_dict={inputs: np.ones((2,3,3)), which_channel:1}))\r\n```\r\n\r\nThis code uses a symbolic variable (\"which_channel\") to slice the input, and considers the cost as the norm of the sliced tensor. We can see that backpropagation will only affect the part of the input that was selected in the slice operation: when we select the channel 0, it shows a gradient different than 0 for the first channel, with zeros in the gradient for the second channel, and vice versa when we select which_channel=1\r\n\r\n"}