{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/334839834", "html_url": "https://github.com/tensorflow/tensorflow/issues/13532#issuecomment-334839834", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13532", "id": 334839834, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDgzOTgzNA==", "user": {"login": "martin-gorner", "id": 959847, "node_id": "MDQ6VXNlcjk1OTg0Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/959847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martin-gorner", "html_url": "https://github.com/martin-gorner", "followers_url": "https://api.github.com/users/martin-gorner/followers", "following_url": "https://api.github.com/users/martin-gorner/following{/other_user}", "gists_url": "https://api.github.com/users/martin-gorner/gists{/gist_id}", "starred_url": "https://api.github.com/users/martin-gorner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martin-gorner/subscriptions", "organizations_url": "https://api.github.com/users/martin-gorner/orgs", "repos_url": "https://api.github.com/users/martin-gorner/repos", "events_url": "https://api.github.com/users/martin-gorner/events{/privacy}", "received_events_url": "https://api.github.com/users/martin-gorner/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-06T18:50:32Z", "updated_at": "2017-10-06T18:50:32Z", "author_association": "NONE", "body_html": "<p>Ok, the general question is: how do you generate a dataset from a list of very large images by cutting them up in pieces in a case when 1) you cannot load multiple very large images in memory at once and 2) the collection of pieces resulting from one image will not fit in memory either.</p>\n<p>If the answer is \"you cannot\", then this is a feature request :-)</p>", "body_text": "Ok, the general question is: how do you generate a dataset from a list of very large images by cutting them up in pieces in a case when 1) you cannot load multiple very large images in memory at once and 2) the collection of pieces resulting from one image will not fit in memory either.\nIf the answer is \"you cannot\", then this is a feature request :-)", "body": "Ok, the general question is: how do you generate a dataset from a list of very large images by cutting them up in pieces in a case when 1) you cannot load multiple very large images in memory at once and 2) the collection of pieces resulting from one image will not fit in memory either.\r\n\r\nIf the answer is \"you cannot\", then this is a feature request :-)"}