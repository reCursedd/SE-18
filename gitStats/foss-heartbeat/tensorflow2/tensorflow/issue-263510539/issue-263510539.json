{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13532", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13532/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13532/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13532/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13532", "id": 263510539, "node_id": "MDU6SXNzdWUyNjM1MTA1Mzk=", "number": 13532, "title": "tf.contrib.data.Dataset generated by slicing and dicing very large images", "user": {"login": "martin-gorner", "id": 959847, "node_id": "MDQ6VXNlcjk1OTg0Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/959847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martin-gorner", "html_url": "https://github.com/martin-gorner", "followers_url": "https://api.github.com/users/martin-gorner/followers", "following_url": "https://api.github.com/users/martin-gorner/following{/other_user}", "gists_url": "https://api.github.com/users/martin-gorner/gists{/gist_id}", "starred_url": "https://api.github.com/users/martin-gorner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martin-gorner/subscriptions", "organizations_url": "https://api.github.com/users/martin-gorner/orgs", "repos_url": "https://api.github.com/users/martin-gorner/repos", "events_url": "https://api.github.com/users/martin-gorner/events{/privacy}", "received_events_url": "https://api.github.com/users/martin-gorner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-10-06T16:59:22Z", "updated_at": "2017-10-11T00:44:57Z", "closed_at": "2017-10-11T00:44:57Z", "author_association": "NONE", "body_html": "<p>Hi<br>\n(writing here as requested by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> for further tf.contrib.data feature requests)</p>\n<p>I would like to create a Dataset by cutting up and preprocessing very large images. I did this:</p>\n<pre><code>dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\ndataset = dataset.flat_map(load_cut_up_and_process)\n</code></pre>\n<p>This goes out of memory because my function load_cut_up_and_process creates too many pieces from one image, all in memory. If I try to make a function that returns fewer pieces for an image and then want to call it repeatedly on the same image to get more, how can I achieve that with Dataset, without replicating the huge image in memory? The only thing I can think of is:</p>\n<pre><code>dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\ndataset = dataset.flat_map(load_and_replicate_each_image) # Dataset is [im1, im1, im1, im2, im2, im2, im3, im3, im3, ...]\ndataset = dataset.flat_map(cut_up_and_process_gently)\n</code></pre>\n<p>Now the second step goes out of memory because the implementation of load_and_replicate_each_image necessarily involves a tensor like [im, im, im] and multiple copies of the image will not fit. I also thought of this:</p>\n<pre><code>dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\ndataset = dataset.flat_map(replicate_each_filename) # Dataset is [imfname1, imfname1, imfname1, imfname2, imfname2, imfname2, imfname3, imfname3, imfname3, ...]\ndataset = dataset.flat_map(load_cut_up_and_process_gently)\n</code></pre>\n<p>Which works, does not go out of memory, but now I am loading the same huge image multiple times in a row which is slow.</p>\n<p>Any ideas ?</p>", "body_text": "Hi\n(writing here as requested by @mrry for further tf.contrib.data feature requests)\nI would like to create a Dataset by cutting up and preprocessing very large images. I did this:\ndataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\ndataset = dataset.flat_map(load_cut_up_and_process)\n\nThis goes out of memory because my function load_cut_up_and_process creates too many pieces from one image, all in memory. If I try to make a function that returns fewer pieces for an image and then want to call it repeatedly on the same image to get more, how can I achieve that with Dataset, without replicating the huge image in memory? The only thing I can think of is:\ndataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\ndataset = dataset.flat_map(load_and_replicate_each_image) # Dataset is [im1, im1, im1, im2, im2, im2, im3, im3, im3, ...]\ndataset = dataset.flat_map(cut_up_and_process_gently)\n\nNow the second step goes out of memory because the implementation of load_and_replicate_each_image necessarily involves a tensor like [im, im, im] and multiple copies of the image will not fit. I also thought of this:\ndataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\ndataset = dataset.flat_map(replicate_each_filename) # Dataset is [imfname1, imfname1, imfname1, imfname2, imfname2, imfname2, imfname3, imfname3, imfname3, ...]\ndataset = dataset.flat_map(load_cut_up_and_process_gently)\n\nWhich works, does not go out of memory, but now I am loading the same huge image multiple times in a row which is slow.\nAny ideas ?", "body": "Hi\r\n(writing here as requested by @mrry for further tf.contrib.data feature requests)\r\n\r\nI would like to create a Dataset by cutting up and preprocessing very large images. I did this:\r\n\r\n```\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\r\ndataset = dataset.flat_map(load_cut_up_and_process)\r\n```\r\n\r\nThis goes out of memory because my function load_cut_up_and_process creates too many pieces from one image, all in memory. If I try to make a function that returns fewer pieces for an image and then want to call it repeatedly on the same image to get more, how can I achieve that with Dataset, without replicating the huge image in memory? The only thing I can think of is:\r\n\r\n```\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\r\ndataset = dataset.flat_map(load_and_replicate_each_image) # Dataset is [im1, im1, im1, im2, im2, im2, im3, im3, im3, ...]\r\ndataset = dataset.flat_map(cut_up_and_process_gently)\r\n```\r\n\r\nNow the second step goes out of memory because the implementation of load_and_replicate_each_image necessarily involves a tensor like [im, im, im] and multiple copies of the image will not fit. I also thought of this:\r\n\r\n```\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))\r\ndataset = dataset.flat_map(replicate_each_filename) # Dataset is [imfname1, imfname1, imfname1, imfname2, imfname2, imfname2, imfname3, imfname3, imfname3, ...]\r\ndataset = dataset.flat_map(load_cut_up_and_process_gently)\r\n```\r\n\r\nWhich works, does not go out of memory, but now I am loading the same huge image multiple times in a row which is slow.\r\n\r\nAny ideas ?"}