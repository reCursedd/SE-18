{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/412527556", "html_url": "https://github.com/tensorflow/tensorflow/issues/11904#issuecomment-412527556", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11904", "id": 412527556, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjUyNzU1Ng==", "user": {"login": "vikmary", "id": 17520043, "node_id": "MDQ6VXNlcjE3NTIwMDQz", "avatar_url": "https://avatars0.githubusercontent.com/u/17520043?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vikmary", "html_url": "https://github.com/vikmary", "followers_url": "https://api.github.com/users/vikmary/followers", "following_url": "https://api.github.com/users/vikmary/following{/other_user}", "gists_url": "https://api.github.com/users/vikmary/gists{/gist_id}", "starred_url": "https://api.github.com/users/vikmary/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vikmary/subscriptions", "organizations_url": "https://api.github.com/users/vikmary/orgs", "repos_url": "https://api.github.com/users/vikmary/repos", "events_url": "https://api.github.com/users/vikmary/events{/privacy}", "received_events_url": "https://api.github.com/users/vikmary/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-13T13:58:58Z", "updated_at": "2018-08-13T13:58:58Z", "author_association": "NONE", "body_html": "<p>This is the minimal fix of code by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16261331\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhedongzheng\">@zhedongzheng</a> that makes parameters of attention mechanism shared for train and predict decoders:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.layers.core <span class=\"pl-k\">import</span> Dense\n\n\n<span class=\"pl-c1\">BEAM_WIDTH</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> INPUTS</span>\nX <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">None</span>])\nY <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">None</span>])\nX_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>])\nY_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>])\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ENCODER         </span>\nencoder_out, encoder_state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>), \n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.contrib.layers.embed_sequence(X, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len,\n    <span class=\"pl-v\">dtype</span> <span class=\"pl-k\">=</span> tf.float32)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER COMPONENTS</span>\nY_vocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\ndecoder_embedding <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([Y_vocab_size, <span class=\"pl-c1\">128</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>))\nprojection_layer <span class=\"pl-k\">=</span> Dense(Y_vocab_size)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ATTENTION (TRAINING)</span>\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shared_attention_mechanism<span class=\"pl-pds\">'</span></span>):\n    attention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.LuongAttention(\n        <span class=\"pl-v\">num_units</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, \n        <span class=\"pl-v\">memory</span> <span class=\"pl-k\">=</span> encoder_out,\n        <span class=\"pl-v\">memory_sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len)\n\ndecoder_cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">attention_mechanism</span> <span class=\"pl-k\">=</span> attention_mechanism,\n    <span class=\"pl-v\">attention_layer_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER (TRAINING)</span>\ntraining_helper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(\n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(decoder_embedding, Y),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> Y_seq_len,\n    <span class=\"pl-v\">time_major</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\ntraining_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">helper</span> <span class=\"pl-k\">=</span> training_helper,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> decoder_cell.zero_state(<span class=\"pl-c1\">BATCH_SIZE</span>,tf.float32).clone(<span class=\"pl-v\">cell_state</span><span class=\"pl-k\">=</span>encoder_state),\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>decode_with_shared_attention<span class=\"pl-pds\">'</span></span>):\n    training_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n        <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> training_decoder,\n        <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>,\n        <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> tf.reduce_max(Y_seq_len))\ntraining_logits <span class=\"pl-k\">=</span> training_decoder_output.rnn_output\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> BEAM SEARCH TILE</span>\nencoder_out <span class=\"pl-k\">=</span> tf.contrib.seq2seq.tile_batch(encoder_out, <span class=\"pl-v\">multiplier</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BEAM_WIDTH</span>)\nX_seq_len <span class=\"pl-k\">=</span> tf.contrib.seq2seq.tile_batch(X_seq_len, <span class=\"pl-v\">multiplier</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BEAM_WIDTH</span>)\nencoder_state <span class=\"pl-k\">=</span> tf.contrib.seq2seq.tile_batch(encoder_state, <span class=\"pl-v\">multiplier</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BEAM_WIDTH</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ATTENTION (PREDICTING)</span>\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shared_attention_mechanism<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    attention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.LuongAttention(\n        <span class=\"pl-v\">num_units</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, \n        <span class=\"pl-v\">memory</span> <span class=\"pl-k\">=</span> encoder_out,\n        <span class=\"pl-v\">memory_sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len)\n\ndecoder_cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">attention_mechanism</span> <span class=\"pl-k\">=</span> attention_mechanism,\n    <span class=\"pl-v\">attention_layer_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER (PREDICTING)</span>\npredicting_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BeamSearchDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">embedding</span> <span class=\"pl-k\">=</span> decoder_embedding,\n    <span class=\"pl-v\">start_tokens</span> <span class=\"pl-k\">=</span> tf.tile(tf.constant([<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32), [<span class=\"pl-c1\">BATCH_SIZE</span>]),\n    <span class=\"pl-v\">end_token</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> decoder_cell.zero_state(<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">BEAM_WIDTH</span>,tf.float32).clone(<span class=\"pl-v\">cell_state</span><span class=\"pl-k\">=</span>encoder_state),\n    <span class=\"pl-v\">beam_width</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">BEAM_WIDTH</span>,\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer,\n    <span class=\"pl-v\">length_penalty_weight</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>decode_with_shared_attention<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    predicting_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n        <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> predicting_decoder,\n        <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>,\n        <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> tf.reduce_max(Y_seq_len))\npredicting_logits <span class=\"pl-k\">=</span> predicting_decoder_output.predicted_ids[:, :, <span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>successful<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>To check sharing of trainable parameters execute the following code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> tf.trainable_variables():\n    <span class=\"pl-c1\">print</span>(var)</pre></div>\n<p>For the code above it will output:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>EmbedSequence/embeddings:0<span class=\"pl-pds\">'</span></span> shape=(10000, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn/basic_lstm_cell/kernel:0<span class=\"pl-pds\">'</span></span> shape=(256, 512) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn/basic_lstm_cell/bias:0<span class=\"pl-pds\">'</span></span> shape=(512,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Variable:0<span class=\"pl-pds\">'</span></span> shape=(10000, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>shared_attention_mechanism/memory_layer/kernel:0<span class=\"pl-pds\">'</span></span> shape=(128, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/kernel:0<span class=\"pl-pds\">'</span></span> shape=(384, 512) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/bias:0<span class=\"pl-pds\">'</span></span> shape=(512,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decode_with_shared_attention/decoder/attention_wrapper/attention_layer/kernel:0<span class=\"pl-pds\">'</span></span> shape=(256, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decode_with_shared_attention/decoder/dense/kernel:0<span class=\"pl-pds\">'</span></span> shape=(128, 10000) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decode_with_shared_attention/decoder/dense/bias:0<span class=\"pl-pds\">'</span></span> shape=(10000,) dtype=float32_ref<span class=\"pl-k\">&gt;</span></pre></div>\n<p>But without wrapping of <code>tf.contrib.seq2seq.LuongAttention</code> and <code>tf.contrib.seq2seq.BeamSearchDecoder</code> into variable scopes it outputs:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>EmbedSequence/embeddings:0<span class=\"pl-pds\">'</span></span> shape=(10000, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn/basic_lstm_cell/kernel:0<span class=\"pl-pds\">'</span></span> shape=(256, 512) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn/basic_lstm_cell/bias:0<span class=\"pl-pds\">'</span></span> shape=(512,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Variable:0<span class=\"pl-pds\">'</span></span> shape=(10000, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>memory_layer/kernel:0<span class=\"pl-pds\">'</span></span> shape=(128, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder/attention_wrapper/basic_lstm_cell/kernel:0<span class=\"pl-pds\">'</span></span> shape=(384, 512) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder/attention_wrapper/basic_lstm_cell/bias:0<span class=\"pl-pds\">'</span></span> shape=(512,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder/attention_wrapper/attention_layer/kernel:0<span class=\"pl-pds\">'</span></span> shape=(256, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder/dense/kernel:0<span class=\"pl-pds\">'</span></span> shape=(128, 10000) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder/dense/bias:0<span class=\"pl-pds\">'</span></span> shape=(10000,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>memory_layer_1/kernel:0<span class=\"pl-pds\">'</span></span> shape=(128, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder_1/attention_wrapper/basic_lstm_cell/kernel:0<span class=\"pl-pds\">'</span></span> shape=(384, 512) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder_1/attention_wrapper/basic_lstm_cell/bias:0<span class=\"pl-pds\">'</span></span> shape=(512,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>decoder_1/attention_wrapper/attention_layer/kernel:0<span class=\"pl-pds\">'</span></span> shape=(256, 128) dtype=float32_ref<span class=\"pl-k\">&gt;</span></pre></div>\n<p>where there are extra memory_layer_1/ and decoder_1/attention_wrapper/ being trained.</p>", "body_text": "This is the minimal fix of code by @zhedongzheng that makes parameters of attention mechanism shared for train and predict decoders:\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\n\n\nBEAM_WIDTH = 5\nBATCH_SIZE = 128\n\n\n# INPUTS\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\n\n\n# ENCODER         \nencoder_out, encoder_state = tf.nn.dynamic_rnn(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\n    sequence_length = X_seq_len,\n    dtype = tf.float32)\n\n\n# DECODER COMPONENTS\nY_vocab_size = 10000\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\nprojection_layer = Dense(Y_vocab_size)\n\n\n# ATTENTION (TRAINING)\nwith tf.variable_scope('shared_attention_mechanism'):\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n        num_units = 128, \n        memory = encoder_out,\n        memory_sequence_length = X_seq_len)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\n    attention_mechanism = attention_mechanism,\n    attention_layer_size = 128)\n\n\n# DECODER (TRAINING)\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\n    sequence_length = Y_seq_len,\n    time_major = False)\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\n    cell = decoder_cell,\n    helper = training_helper,\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\n    output_layer = projection_layer)\nwith tf.variable_scope('decode_with_shared_attention'):\n    training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n        decoder = training_decoder,\n        impute_finished = True,\n        maximum_iterations = tf.reduce_max(Y_seq_len))\ntraining_logits = training_decoder_output.rnn_output\n\n\n# BEAM SEARCH TILE\nencoder_out = tf.contrib.seq2seq.tile_batch(encoder_out, multiplier=BEAM_WIDTH)\nX_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\nencoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=BEAM_WIDTH)\n\n\n# ATTENTION (PREDICTING)\nwith tf.variable_scope('shared_attention_mechanism', reuse=True):\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n        num_units = 128, \n        memory = encoder_out,\n        memory_sequence_length = X_seq_len)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\n    attention_mechanism = attention_mechanism,\n    attention_layer_size = 128)\n\n\n# DECODER (PREDICTING)\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n    cell = decoder_cell,\n    embedding = decoder_embedding,\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\n    end_token = 2,\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(cell_state=encoder_state),\n    beam_width = BEAM_WIDTH,\n    output_layer = projection_layer,\n    length_penalty_weight = 0.0)\nwith tf.variable_scope('decode_with_shared_attention', reuse=True):\n    predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n        decoder = predicting_decoder,\n        impute_finished = False,\n        maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\n\nprint('successful')\nTo check sharing of trainable parameters execute the following code:\nfor var in tf.trainable_variables():\n    print(var)\nFor the code above it will output:\n<tf.Variable 'EmbedSequence/embeddings:0' shape=(10000, 128) dtype=float32_ref>\n<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\n<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n<tf.Variable 'Variable:0' shape=(10000, 128) dtype=float32_ref>\n<tf.Variable 'shared_attention_mechanism/memory_layer/kernel:0' shape=(128, 128) dtype=float32_ref>\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\n<tf.Variable 'decode_with_shared_attention/decoder/dense/kernel:0' shape=(128, 10000) dtype=float32_ref>\n<tf.Variable 'decode_with_shared_attention/decoder/dense/bias:0' shape=(10000,) dtype=float32_ref>\nBut without wrapping of tf.contrib.seq2seq.LuongAttention and tf.contrib.seq2seq.BeamSearchDecoder into variable scopes it outputs:\n<tf.Variable 'EmbedSequence/embeddings:0' shape=(10000, 128) dtype=float32_ref>\n<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\n<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n<tf.Variable 'Variable:0' shape=(10000, 128) dtype=float32_ref>\n<tf.Variable 'memory_layer/kernel:0' shape=(128, 128) dtype=float32_ref>\n<tf.Variable 'decoder/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\n<tf.Variable 'decoder/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n<tf.Variable 'decoder/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\n<tf.Variable 'decoder/dense/kernel:0' shape=(128, 10000) dtype=float32_ref>\n<tf.Variable 'decoder/dense/bias:0' shape=(10000,) dtype=float32_ref>\n<tf.Variable 'memory_layer_1/kernel:0' shape=(128, 128) dtype=float32_ref>\n<tf.Variable 'decoder_1/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\n<tf.Variable 'decoder_1/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n<tf.Variable 'decoder_1/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\nwhere there are extra memory_layer_1/ and decoder_1/attention_wrapper/ being trained.", "body": "This is the minimal fix of code by @zhedongzheng that makes parameters of attention mechanism shared for train and predict decoders:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n\r\nBEAM_WIDTH = 5\r\nBATCH_SIZE = 128\r\n\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\n\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\n\r\n# ATTENTION (TRAINING)\r\nwith tf.variable_scope('shared_attention_mechanism'):\r\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n        num_units = 128, \r\n        memory = encoder_out,\r\n        memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (TRAINING)\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\r\n    output_layer = projection_layer)\r\nwith tf.variable_scope('decode_with_shared_attention'):\r\n    training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n        decoder = training_decoder,\r\n        impute_finished = True,\r\n        maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n\r\n# BEAM SEARCH TILE\r\nencoder_out = tf.contrib.seq2seq.tile_batch(encoder_out, multiplier=BEAM_WIDTH)\r\nX_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\r\nencoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=BEAM_WIDTH)\r\n\r\n\r\n# ATTENTION (PREDICTING)\r\nwith tf.variable_scope('shared_attention_mechanism', reuse=True):\r\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n        num_units = 128, \r\n        memory = encoder_out,\r\n        memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (PREDICTING)\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\r\n    end_token = 2,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(cell_state=encoder_state),\r\n    beam_width = BEAM_WIDTH,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\nwith tf.variable_scope('decode_with_shared_attention', reuse=True):\r\n    predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n        decoder = predicting_decoder,\r\n        impute_finished = False,\r\n        maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\r\n\r\nprint('successful')\r\n```\r\n\r\nTo check sharing of trainable parameters execute the following code:\r\n```python\r\nfor var in tf.trainable_variables():\r\n    print(var)\r\n```\r\n\r\nFor the code above it will output:\r\n```bash\r\n<tf.Variable 'EmbedSequence/embeddings:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'Variable:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'shared_attention_mechanism/memory_layer/kernel:0' shape=(128, 128) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/dense/kernel:0' shape=(128, 10000) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/dense/bias:0' shape=(10000,) dtype=float32_ref>\r\n```\r\n But without wrapping of `tf.contrib.seq2seq.LuongAttention` and `tf.contrib.seq2seq.BeamSearchDecoder` into variable scopes it outputs:\r\n```bash\r\n<tf.Variable 'EmbedSequence/embeddings:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'Variable:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'memory_layer/kernel:0' shape=(128, 128) dtype=float32_ref>\r\n<tf.Variable 'decoder/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\r\n<tf.Variable 'decoder/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'decoder/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\r\n<tf.Variable 'decoder/dense/kernel:0' shape=(128, 10000) dtype=float32_ref>\r\n<tf.Variable 'decoder/dense/bias:0' shape=(10000,) dtype=float32_ref>\r\n<tf.Variable 'memory_layer_1/kernel:0' shape=(128, 128) dtype=float32_ref>\r\n<tf.Variable 'decoder_1/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\r\n<tf.Variable 'decoder_1/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'decoder_1/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\r\n```\r\n where there are extra memory_layer_1/ and decoder_1/attention_wrapper/ being trained.\r\n"}