{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319553148", "html_url": "https://github.com/tensorflow/tensorflow/issues/11904#issuecomment-319553148", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11904", "id": 319553148, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTU1MzE0OA==", "user": {"login": "zhedongzheng", "id": 16261331, "node_id": "MDQ6VXNlcjE2MjYxMzMx", "avatar_url": "https://avatars2.githubusercontent.com/u/16261331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhedongzheng", "html_url": "https://github.com/zhedongzheng", "followers_url": "https://api.github.com/users/zhedongzheng/followers", "following_url": "https://api.github.com/users/zhedongzheng/following{/other_user}", "gists_url": "https://api.github.com/users/zhedongzheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhedongzheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhedongzheng/subscriptions", "organizations_url": "https://api.github.com/users/zhedongzheng/orgs", "repos_url": "https://api.github.com/users/zhedongzheng/repos", "events_url": "https://api.github.com/users/zhedongzheng/events{/privacy}", "received_events_url": "https://api.github.com/users/zhedongzheng/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-02T02:50:18Z", "updated_at": "2017-08-02T03:47:47Z", "author_association": "NONE", "body_html": "<p>I finally get the graph successfully compiled by the following code,  I will further test the session running and then close the issue if solved</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.layers.core <span class=\"pl-k\">import</span> Dense\n\n\n<span class=\"pl-c1\">BEAM_WIDTH</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> INPUTS</span>\nX <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">None</span>])\nY <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">None</span>])\nX_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>])\nY_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>])\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ENCODER         </span>\nencoder_out, encoder_state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>), \n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.contrib.layers.embed_sequence(X, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len,\n    <span class=\"pl-v\">dtype</span> <span class=\"pl-k\">=</span> tf.float32)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER COMPONENTS</span>\nY_vocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\ndecoder_embedding <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([Y_vocab_size, <span class=\"pl-c1\">128</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>))\nprojection_layer <span class=\"pl-k\">=</span> Dense(Y_vocab_size)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ATTENTION (TRAINING)</span>\nattention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.LuongAttention(\n    <span class=\"pl-v\">num_units</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, \n    <span class=\"pl-v\">memory</span> <span class=\"pl-k\">=</span> encoder_out,\n    <span class=\"pl-v\">memory_sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len)\n\ndecoder_cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">attention_mechanism</span> <span class=\"pl-k\">=</span> attention_mechanism,\n    <span class=\"pl-v\">attention_layer_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER (TRAINING)</span>\ntraining_helper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(\n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(decoder_embedding, Y),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> Y_seq_len,\n    <span class=\"pl-v\">time_major</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\ntraining_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">helper</span> <span class=\"pl-k\">=</span> training_helper,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> decoder_cell.zero_state(<span class=\"pl-c1\">BATCH_SIZE</span>,tf.float32).clone(<span class=\"pl-v\">cell_state</span><span class=\"pl-k\">=</span>encoder_state),\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer)\ntraining_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> training_decoder,\n    <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> tf.reduce_max(Y_seq_len))\ntraining_logits <span class=\"pl-k\">=</span> training_decoder_output.rnn_output\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> BEAM SEARCH TILE</span>\nencoder_out <span class=\"pl-k\">=</span> tf.contrib.seq2seq.tile_batch(encoder_out, <span class=\"pl-v\">multiplier</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BEAM_WIDTH</span>)\nX_seq_len <span class=\"pl-k\">=</span> tf.contrib.seq2seq.tile_batch(X_seq_len, <span class=\"pl-v\">multiplier</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BEAM_WIDTH</span>)\nencoder_state <span class=\"pl-k\">=</span> tf.contrib.seq2seq.tile_batch(encoder_state, <span class=\"pl-v\">multiplier</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BEAM_WIDTH</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ATTENTION (PREDICTING)</span>\nattention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.LuongAttention(\n    <span class=\"pl-v\">num_units</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, \n    <span class=\"pl-v\">memory</span> <span class=\"pl-k\">=</span> encoder_out,\n    <span class=\"pl-v\">memory_sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len)\n\ndecoder_cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">attention_mechanism</span> <span class=\"pl-k\">=</span> attention_mechanism,\n    <span class=\"pl-v\">attention_layer_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER (PREDICTING)</span>\npredicting_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BeamSearchDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">embedding</span> <span class=\"pl-k\">=</span> decoder_embedding,\n    <span class=\"pl-v\">start_tokens</span> <span class=\"pl-k\">=</span> tf.tile(tf.constant([<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32), [<span class=\"pl-c1\">BATCH_SIZE</span>]),\n    <span class=\"pl-v\">end_token</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> decoder_cell.zero_state(<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">BEAM_WIDTH</span>,tf.float32).clone(<span class=\"pl-v\">cell_state</span><span class=\"pl-k\">=</span>encoder_state),\n    <span class=\"pl-v\">beam_width</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">BEAM_WIDTH</span>,\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer,\n    <span class=\"pl-v\">length_penalty_weight</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>)\npredicting_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> predicting_decoder,\n    <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>,\n    <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> tf.reduce_max(Y_seq_len))\npredicting_logits <span class=\"pl-k\">=</span> predicting_decoder_output.predicted_ids[:, :, <span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>successful<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "I finally get the graph successfully compiled by the following code,  I will further test the session running and then close the issue if solved\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\n\n\nBEAM_WIDTH = 5\nBATCH_SIZE = 128\n\n\n# INPUTS\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\n\n\n# ENCODER         \nencoder_out, encoder_state = tf.nn.dynamic_rnn(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\n    sequence_length = X_seq_len,\n    dtype = tf.float32)\n\n\n# DECODER COMPONENTS\nY_vocab_size = 10000\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\nprojection_layer = Dense(Y_vocab_size)\n\n\n# ATTENTION (TRAINING)\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n    num_units = 128, \n    memory = encoder_out,\n    memory_sequence_length = X_seq_len)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\n    attention_mechanism = attention_mechanism,\n    attention_layer_size = 128)\n\n\n# DECODER (TRAINING)\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\n    sequence_length = Y_seq_len,\n    time_major = False)\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\n    cell = decoder_cell,\n    helper = training_helper,\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\n    output_layer = projection_layer)\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder = training_decoder,\n    impute_finished = True,\n    maximum_iterations = tf.reduce_max(Y_seq_len))\ntraining_logits = training_decoder_output.rnn_output\n\n\n# BEAM SEARCH TILE\nencoder_out = tf.contrib.seq2seq.tile_batch(encoder_out, multiplier=BEAM_WIDTH)\nX_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\nencoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=BEAM_WIDTH)\n\n\n# ATTENTION (PREDICTING)\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n    num_units = 128, \n    memory = encoder_out,\n    memory_sequence_length = X_seq_len)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\n    attention_mechanism = attention_mechanism,\n    attention_layer_size = 128)\n\n\n# DECODER (PREDICTING)\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n    cell = decoder_cell,\n    embedding = decoder_embedding,\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\n    end_token = 2,\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(cell_state=encoder_state),\n    beam_width = BEAM_WIDTH,\n    output_layer = projection_layer,\n    length_penalty_weight = 0.0)\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder = predicting_decoder,\n    impute_finished = False,\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\n\nprint('successful')", "body": "I finally get the graph successfully compiled by the following code,  I will further test the session running and then close the issue if solved\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n\r\nBEAM_WIDTH = 5\r\nBATCH_SIZE = 128\r\n\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\n\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\n\r\n# ATTENTION (TRAINING)\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (TRAINING)\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n\r\n# BEAM SEARCH TILE\r\nencoder_out = tf.contrib.seq2seq.tile_batch(encoder_out, multiplier=BEAM_WIDTH)\r\nX_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\r\nencoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=BEAM_WIDTH)\r\n\r\n\r\n# ATTENTION (PREDICTING)\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (PREDICTING)\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\r\n    end_token = 2,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(cell_state=encoder_state),\r\n    beam_width = BEAM_WIDTH,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = predicting_decoder,\r\n    impute_finished = False,\r\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\r\n\r\nprint('successful')\r\n```"}