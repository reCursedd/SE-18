{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11904", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11904/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11904/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11904/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11904", "id": 246701488, "node_id": "MDU6SXNzdWUyNDY3MDE0ODg=", "number": 11904, "title": "Unclear about how to integrate AttentionWrapper with BeamSearchDecoder", "user": {"login": "zhedongzheng", "id": 16261331, "node_id": "MDQ6VXNlcjE2MjYxMzMx", "avatar_url": "https://avatars2.githubusercontent.com/u/16261331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhedongzheng", "html_url": "https://github.com/zhedongzheng", "followers_url": "https://api.github.com/users/zhedongzheng/followers", "following_url": "https://api.github.com/users/zhedongzheng/following{/other_user}", "gists_url": "https://api.github.com/users/zhedongzheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhedongzheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhedongzheng/subscriptions", "organizations_url": "https://api.github.com/users/zhedongzheng/orgs", "repos_url": "https://api.github.com/users/zhedongzheng/repos", "events_url": "https://api.github.com/users/zhedongzheng/events{/privacy}", "received_events_url": "https://api.github.com/users/zhedongzheng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2017-07-31T09:46:55Z", "updated_at": "2018-08-13T13:58:58Z", "closed_at": "2017-08-02T03:47:28Z", "author_association": "NONE", "body_html": "<p>TF Version: 1.2.1</p>\n<p>I cannot find information about how to integrate AttentionWrapper with BeamSearchDecoder on website / nmt tutorial, in particular how to feed the beam-search-tiled (<code>tf.contrib.seq2seq.tile_batch</code>) states to the attention-cloned (<code>zero_state(...).clone(...)</code>) states.</p>\n<p>In my attempt, there seems to be an inconsistency between the requirement of the <code>batch_size</code> of the zero states generated by <code>AttentionWrapper</code>  in the training stage and predicting stage. In training stage, initial state of decoder requires batch size, however in predicting stage it requires (batch_size * beam_width).<br>\nthe minimal code to demonstrate this problem is:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.layers.core <span class=\"pl-k\">import</span> Dense\n\n<span class=\"pl-c1\">BEAM_WIDTH</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> INPUTS</span>\nX <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">None</span>])\nY <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">None</span>])\nX_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>])\nY_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">BATCH_SIZE</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ENCODER         </span>\nencoder_out, encoder_state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>), \n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.contrib.layers.embed_sequence(X, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len,\n    <span class=\"pl-v\">dtype</span> <span class=\"pl-k\">=</span> tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ATTENTION</span>\nattention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.LuongAttention(\n    <span class=\"pl-v\">num_units</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, \n    <span class=\"pl-v\">memory</span> <span class=\"pl-k\">=</span> encoder_out,\n    <span class=\"pl-v\">memory_sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len)\n\ndecoder_cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">attention_mechanism</span> <span class=\"pl-k\">=</span> attention_mechanism,\n    <span class=\"pl-v\">attention_layer_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER COMPONENTS</span>\nY_vocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\ndecoder_embedding <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([Y_vocab_size, <span class=\"pl-c1\">128</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>))\nprojection_layer <span class=\"pl-k\">=</span> Dense(Y_vocab_size)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> TRAINING DECODER</span>\ntraining_helper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(\n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(decoder_embedding, Y),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> Y_seq_len,\n    <span class=\"pl-v\">time_major</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\ntraining_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">helper</span> <span class=\"pl-k\">=</span> training_helper,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> decoder_cell.zero_state(<span class=\"pl-c1\">BATCH_SIZE</span>,tf.float32).clone(<span class=\"pl-v\">cell_state</span><span class=\"pl-k\">=</span>encoder_state),\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer)\ntraining_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> training_decoder,\n    <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> tf.reduce_max(Y_seq_len))\ntraining_logits <span class=\"pl-k\">=</span> training_decoder_output.rnn_output\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> PREDICTING_DECODER</span>\npredicting_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BeamSearchDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">embedding</span> <span class=\"pl-k\">=</span> decoder_embedding,\n    <span class=\"pl-v\">start_tokens</span> <span class=\"pl-k\">=</span> tf.tile(tf.constant([<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32), [<span class=\"pl-c1\">BATCH_SIZE</span>]),\n    <span class=\"pl-v\">end_token</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> decoder_cell.zero_state(<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">BEAM_WIDTH</span>,tf.float32).clone(\n                    <span class=\"pl-v\">cell_state</span><span class=\"pl-k\">=</span>tf.contrib.seq2seq.tile_batch(encoder_state, <span class=\"pl-c1\">BEAM_WIDTH</span>)),\n    <span class=\"pl-v\">beam_width</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">BEAM_WIDTH</span>,\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer,\n    <span class=\"pl-v\">length_penalty_weight</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>)\npredicting_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> predicting_decoder,\n    <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>,\n    <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> tf.reduce_max(Y_seq_len))\npredicting_logits <span class=\"pl-k\">=</span> predicting_decoder_output.predicted_ids[:, :, <span class=\"pl-c1\">0</span>]</pre></div>\n<p>the error message is:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 58, in &lt;module&gt;\n    initial_state = decoder_cell.zero_state(BATCH_SIZE*BEAM_WIDTH,tf.float32).clone(\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 659, in zero_state\n    message=error_message)]):\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 317, in assert_equal\n    _assert_static(condition_static, data)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 100, in _assert_static\n    raise ValueError('\\n'.join(data_static))\nValueError: When calling zero_state of AttentionWrapper attention_wrapper: Non-matching batch sizes between the memory (encoder output) and the requested batch size.  Are you using the BeamSearchDecoder?  If so, make sure your encoder output has been tiled to beam_width via tf.contrib.seq2seq.tile_batch, and the batch_size= argument passed to zero_state is batch_size * beam_width.\nCondition x == y did not hold element-wise:\nx (AttentionWrapperZeroState_1/assert_equal/x:0) = \n640\ny (AttentionWrapperZeroState_1/assert_equal/y:0) = \n128\n</code></pre>", "body_text": "TF Version: 1.2.1\nI cannot find information about how to integrate AttentionWrapper with BeamSearchDecoder on website / nmt tutorial, in particular how to feed the beam-search-tiled (tf.contrib.seq2seq.tile_batch) states to the attention-cloned (zero_state(...).clone(...)) states.\nIn my attempt, there seems to be an inconsistency between the requirement of the batch_size of the zero states generated by AttentionWrapper  in the training stage and predicting stage. In training stage, initial state of decoder requires batch size, however in predicting stage it requires (batch_size * beam_width).\nthe minimal code to demonstrate this problem is:\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\n\nBEAM_WIDTH = 5\nBATCH_SIZE = 128\n\n# INPUTS\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\n\n# ENCODER         \nencoder_out, encoder_state = tf.nn.dynamic_rnn(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\n    sequence_length = X_seq_len,\n    dtype = tf.float32)\n\n# ATTENTION\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n    num_units = 128, \n    memory = encoder_out,\n    memory_sequence_length = X_seq_len)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\n    attention_mechanism = attention_mechanism,\n    attention_layer_size = 128)\n\n# DECODER COMPONENTS\nY_vocab_size = 10000\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\nprojection_layer = Dense(Y_vocab_size)\n\n# TRAINING DECODER\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\n    sequence_length = Y_seq_len,\n    time_major = False)\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\n    cell = decoder_cell,\n    helper = training_helper,\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\n    output_layer = projection_layer)\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder = training_decoder,\n    impute_finished = True,\n    maximum_iterations = tf.reduce_max(Y_seq_len))\ntraining_logits = training_decoder_output.rnn_output\n\n# PREDICTING_DECODER\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n    cell = decoder_cell,\n    embedding = decoder_embedding,\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\n    end_token = 2,\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(\n                    cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, BEAM_WIDTH)),\n    beam_width = BEAM_WIDTH,\n    output_layer = projection_layer,\n    length_penalty_weight = 0.0)\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder = predicting_decoder,\n    impute_finished = False,\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\nthe error message is:\nTraceback (most recent call last):\n  File \"test.py\", line 58, in <module>\n    initial_state = decoder_cell.zero_state(BATCH_SIZE*BEAM_WIDTH,tf.float32).clone(\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 659, in zero_state\n    message=error_message)]):\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 317, in assert_equal\n    _assert_static(condition_static, data)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 100, in _assert_static\n    raise ValueError('\\n'.join(data_static))\nValueError: When calling zero_state of AttentionWrapper attention_wrapper: Non-matching batch sizes between the memory (encoder output) and the requested batch size.  Are you using the BeamSearchDecoder?  If so, make sure your encoder output has been tiled to beam_width via tf.contrib.seq2seq.tile_batch, and the batch_size= argument passed to zero_state is batch_size * beam_width.\nCondition x == y did not hold element-wise:\nx (AttentionWrapperZeroState_1/assert_equal/x:0) = \n640\ny (AttentionWrapperZeroState_1/assert_equal/y:0) = \n128", "body": "TF Version: 1.2.1\r\n\r\nI cannot find information about how to integrate AttentionWrapper with BeamSearchDecoder on website / nmt tutorial, in particular how to feed the beam-search-tiled (```tf.contrib.seq2seq.tile_batch```) states to the attention-cloned (```zero_state(...).clone(...)```) states.\r\n\r\nIn my attempt, there seems to be an inconsistency between the requirement of the ```batch_size``` of the zero states generated by ```AttentionWrapper```  in the training stage and predicting stage. In training stage, initial state of decoder requires batch size, however in predicting stage it requires (batch_size * beam_width).\r\nthe minimal code to demonstrate this problem is:\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\nBEAM_WIDTH = 5\r\nBATCH_SIZE = 128\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n# ATTENTION\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\n# TRAINING DECODER\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n# PREDICTING_DECODER\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\r\n    end_token = 2,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(\r\n                    cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, BEAM_WIDTH)),\r\n    beam_width = BEAM_WIDTH,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = predicting_decoder,\r\n    impute_finished = False,\r\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\r\n```\r\nthe error message is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 58, in <module>\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE*BEAM_WIDTH,tf.float32).clone(\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 659, in zero_state\r\n    message=error_message)]):\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 317, in assert_equal\r\n    _assert_static(condition_static, data)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 100, in _assert_static\r\n    raise ValueError('\\n'.join(data_static))\r\nValueError: When calling zero_state of AttentionWrapper attention_wrapper: Non-matching batch sizes between the memory (encoder output) and the requested batch size.  Are you using the BeamSearchDecoder?  If so, make sure your encoder output has been tiled to beam_width via tf.contrib.seq2seq.tile_batch, and the batch_size= argument passed to zero_state is batch_size * beam_width.\r\nCondition x == y did not hold element-wise:\r\nx (AttentionWrapperZeroState_1/assert_equal/x:0) = \r\n640\r\ny (AttentionWrapperZeroState_1/assert_equal/y:0) = \r\n128\r\n```"}