{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/105832294", "pull_request_review_id": 26728937, "id": 105832294, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNTgzMjI5NA==", "diff_hunk": "@@ -20,9 +20,119 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/kernels/bounds_check.h\"\n \n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/kernels/cuda_device_array.h\"\n+#endif  // GOOGLE_CUDA\n+\n namespace tensorflow {\n \n-template <class T>\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename Device, typename T>\n+struct LaunchDynamicStitchOp;\n+\n+template <typename T>\n+struct LaunchDynamicStitchOp<CPUDevice, T> {\n+  static void launch(OpKernelContext* c, const int slice_size,\n+                     const int first_dim_size, OpInputList indices_inputs,\n+                     OpInputList data_inputs, Tensor* merged) {\n+    auto merged_flat = merged->flat_outer_dims<T>();\n+    for (int input_num = 0; input_num < indices_inputs.size(); input_num++) {\n+      const Tensor& indices = indices_inputs[input_num];\n+      auto indices_vec = indices.flat<int32>();\n+      const Tensor& data = data_inputs[input_num];\n+      auto data_flat =\n+          data.shaped<T, 2>({indices_vec.dimension(0), slice_size});\n+\n+      if (DataTypeCanUseMemcpy(DataTypeToEnum<T>::v())) {\n+        T* merged_base = &merged_flat(0, 0);\n+        const T* data_base = &data_flat(0, 0);\n+        const size_t slice_bytes = slice_size * sizeof(T);\n+        for (int i = 0; i < indices_vec.size(); i++) {\n+          int32 index = internal::SubtleMustCopy(indices_vec(i));\n+          OP_REQUIRES(\n+              c, FastBoundsCheck(index, first_dim_size),\n+              errors::InvalidArgument(\"indices[\", i, \"] is out of range\"));\n+          memcpy(merged_base + index * slice_size, data_base + i * slice_size,\n+                 slice_bytes);\n+        }\n+      } else {\n+        Eigen::DSizes<Eigen::DenseIndex, 2> sizes(1, slice_size);\n+        for (int i = 0; i < indices_vec.size(); i++) {\n+          // Copy slice data[i] to merged[indices[i]]\n+          Eigen::DSizes<Eigen::DenseIndex, 2> data_indices(i, 0);\n+          int32 index = internal::SubtleMustCopy(indices_vec(i));\n+          OP_REQUIRES(\n+              c, FastBoundsCheck(index, first_dim_size),\n+              errors::InvalidArgument(\"indices[\", i, \"] is out of range\"));\n+          Eigen::DSizes<Eigen::DenseIndex, 2> merged_indices(index, 0);\n+          merged_flat.slice(merged_indices, sizes) =\n+              data_flat.slice(data_indices, sizes);\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+#if GOOGLE_CUDA\n+\n+template <typename T>\n+struct DynamicStitchGPULaunch {\n+  static void Run(const GPUDevice& d, const int slice_size,\n+                  const int first_dim_size,\n+                  const CudaDeviceArrayStruct<int32>& indices_flat,\n+                  const CudaDeviceArrayStruct<const T*>& data_slice_ptrs,\n+                  T* output);\n+};\n+\n+template <typename T>\n+struct LaunchDynamicStitchOp<GPUDevice, T> {\n+  static void launch(OpKernelContext* c, const int slice_size,\n+                     const int first_dim_size, OpInputList indices_inputs,\n+                     OpInputList data_inputs, Tensor* merged) {\n+    // createt two arrays that will be sent to CUDA device\n+    // one used as pointers to output's row id\n+    CudaDeviceArrayOnHost<int32> indices_flat(c, first_dim_size);\n+    // another used as pointers to the pointers of the head of each data_slice\n+    CudaDeviceArrayOnHost<const T*> data_slice_ptrs(c, first_dim_size);\n+    OP_REQUIRES_OK(c, indices_flat.Init());\n+    OP_REQUIRES_OK(c, data_slice_ptrs.Init());\n+\n+    int ptr_num = 0;\n+    for (int input_num = 0; input_num < indices_inputs.size(); input_num++) {\n+      auto indices_vec = indices_inputs[input_num].flat<int32>();\n+      auto base_ptr = data_inputs[input_num].template flat<T>().data();\n+      for (int ind_num = 0; ind_num < indices_vec.size(); ind_num++) {\n+        indices_flat.Set(ptr_num, indices_vec(ind_num));", "path": "tensorflow/core/kernels/dynamic_stitch_op.cc", "position": 89, "original_position": 89, "commit_id": "c472fe3228621d40e58126f4ea820c8fb65ca319", "original_commit_id": "c472fe3228621d40e58126f4ea820c8fb65ca319", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "body": "Yes, I should actually just replace `ptr_num` with the value of `indices_vec(ind_num)`, so when collision happens the new index value will naturally overwrite the old one, which would be the same as how current implementation handles collisions, is my understanding correct? \r\n\r\nHowever, if we end up taking this approach, we would also need to account for missing indices on `indices_flat`. To do this, I plan on adding 1 to all the values of output index then subtract it back on the GPU kernel, so if the threads on CUDA kernel encounter a 0 on `output_id` they can just skip the read and write. \r\n\r\nUnfortunately, I think `CudaDeviceArrayOnHost` is not zero initialized, so I would need to do it manually. Is there a standard way to zero-initialize a temporary tensor? or do I just use normal for-loop to achieve this? ", "created_at": "2017-03-14T05:57:12Z", "updated_at": "2017-03-14T05:57:12Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8260#discussion_r105832294", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8260", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/105832294"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8260#discussion_r105832294"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8260"}}, "body_html": "<p>Yes, I should actually just replace <code>ptr_num</code> with the value of <code>indices_vec(ind_num)</code>, so when collision happens the new index value will naturally overwrite the old one, which would be the same as how current implementation handles collisions, is my understanding correct?</p>\n<p>However, if we end up taking this approach, we would also need to account for missing indices on <code>indices_flat</code>. To do this, I plan on adding 1 to all the values of output index then subtract it back on the GPU kernel, so if the threads on CUDA kernel encounter a 0 on <code>output_id</code> they can just skip the read and write.</p>\n<p>Unfortunately, I think <code>CudaDeviceArrayOnHost</code> is not zero initialized, so I would need to do it manually. Is there a standard way to zero-initialize a temporary tensor? or do I just use normal for-loop to achieve this?</p>", "body_text": "Yes, I should actually just replace ptr_num with the value of indices_vec(ind_num), so when collision happens the new index value will naturally overwrite the old one, which would be the same as how current implementation handles collisions, is my understanding correct?\nHowever, if we end up taking this approach, we would also need to account for missing indices on indices_flat. To do this, I plan on adding 1 to all the values of output index then subtract it back on the GPU kernel, so if the threads on CUDA kernel encounter a 0 on output_id they can just skip the read and write.\nUnfortunately, I think CudaDeviceArrayOnHost is not zero initialized, so I would need to do it manually. Is there a standard way to zero-initialize a temporary tensor? or do I just use normal for-loop to achieve this?", "in_reply_to_id": 105453509}