{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6659", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6659/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6659/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6659/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6659", "id": 198887650, "node_id": "MDU6SXNzdWUxOTg4ODc2NTA=", "number": 6659, "title": "Is it possible to implement Sparse Cross Entropy or Sparse Softmax Cross Entropy with Smooth Threshold to deal with large loss?", "user": {"login": "Syndrome777", "id": 6788909, "node_id": "MDQ6VXNlcjY3ODg5MDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/6788909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Syndrome777", "html_url": "https://github.com/Syndrome777", "followers_url": "https://api.github.com/users/Syndrome777/followers", "following_url": "https://api.github.com/users/Syndrome777/following{/other_user}", "gists_url": "https://api.github.com/users/Syndrome777/gists{/gist_id}", "starred_url": "https://api.github.com/users/Syndrome777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Syndrome777/subscriptions", "organizations_url": "https://api.github.com/users/Syndrome777/orgs", "repos_url": "https://api.github.com/users/Syndrome777/repos", "events_url": "https://api.github.com/users/Syndrome777/events{/privacy}", "received_events_url": "https://api.github.com/users/Syndrome777/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-01-05T06:47:30Z", "updated_at": "2017-01-06T18:10:12Z", "closed_at": "2017-01-06T18:10:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I use <code>sparse_softmax_cross_entropy_with_logits</code> in Seq2Seq task with large vocabulary.<br>\nBut sometime I got a large loss, such as 1000. It's too large for optimization. So I think to add a smooth threshold in sparse_softmax_cross_entropy_with_logits may deal with this problem.</p>\n<div class=\"highlight highlight-source-python\"><pre>sm <span class=\"pl-k\">=</span> tf.nn.softmax(logit)\nsm <span class=\"pl-k\">=</span> sm <span class=\"pl-k\">+</span> threshold\nce <span class=\"pl-k\">=</span> sparse_cross_entropy(sm, target)</pre></div>\n<p>BTW, I clip <code>logit</code> for smoothness now. But I'm not sure if it's a good idea.</p>\n<div class=\"highlight highlight-source-python\"><pre>logit <span class=\"pl-k\">=</span> clip(logit, <span class=\"pl-k\">-</span>threshold, threshold)</pre></div>\n<p>Does anyone have better solution to deal with large loss or will implement Sparse Cross Entropy?<br>\nThanks so much!</p>", "body_text": "I use sparse_softmax_cross_entropy_with_logits in Seq2Seq task with large vocabulary.\nBut sometime I got a large loss, such as 1000. It's too large for optimization. So I think to add a smooth threshold in sparse_softmax_cross_entropy_with_logits may deal with this problem.\nsm = tf.nn.softmax(logit)\nsm = sm + threshold\nce = sparse_cross_entropy(sm, target)\nBTW, I clip logit for smoothness now. But I'm not sure if it's a good idea.\nlogit = clip(logit, -threshold, threshold)\nDoes anyone have better solution to deal with large loss or will implement Sparse Cross Entropy?\nThanks so much!", "body": "I use `sparse_softmax_cross_entropy_with_logits` in Seq2Seq task with large vocabulary.\r\nBut sometime I got a large loss, such as 1000. It's too large for optimization. So I think to add a smooth threshold in sparse_softmax_cross_entropy_with_logits may deal with this problem.\r\n```python\r\nsm = tf.nn.softmax(logit)\r\nsm = sm + threshold\r\nce = sparse_cross_entropy(sm, target)\r\n```\r\nBTW, I clip `logit` for smoothness now. But I'm not sure if it's a good idea.\r\n```python\r\nlogit = clip(logit, -threshold, threshold)\r\n```\r\nDoes anyone have better solution to deal with large loss or will implement Sparse Cross Entropy?\r\nThanks so much!\r\n\r\n"}