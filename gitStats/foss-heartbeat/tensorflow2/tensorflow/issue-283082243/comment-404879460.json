{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404879460", "html_url": "https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-404879460", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15465", "id": 404879460, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDg3OTQ2MA==", "user": {"login": "tvercaut", "id": 1614505, "node_id": "MDQ6VXNlcjE2MTQ1MDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/1614505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tvercaut", "html_url": "https://github.com/tvercaut", "followers_url": "https://api.github.com/users/tvercaut/followers", "following_url": "https://api.github.com/users/tvercaut/following{/other_user}", "gists_url": "https://api.github.com/users/tvercaut/gists{/gist_id}", "starred_url": "https://api.github.com/users/tvercaut/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tvercaut/subscriptions", "organizations_url": "https://api.github.com/users/tvercaut/orgs", "repos_url": "https://api.github.com/users/tvercaut/repos", "events_url": "https://api.github.com/users/tvercaut/events{/privacy}", "received_events_url": "https://api.github.com/users/tvercaut/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-13T16:10:31Z", "updated_at": "2018-07-13T16:10:31Z", "author_association": "NONE", "body_html": "<p>I did some further experimentation and while I am still not 100% confident I get all the indexing right, this seems more consistent:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MatrixExponential<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_expm_grad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> We want the backward-mode gradient (left multiplication).</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Let X be the NxN input matrix.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Let J(X) be the the N^2xN^2 complete Jacobian matrix of expm at X.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Let Y be the NxN previous gradient in the backward AD (left multiplication)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> We have</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> unvec( ( vec(Y)^T . J(X) )^T )</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   = unvec( J(X)^T . vec(Y) )</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   = unvec( J(X^T) . vec(Y) )</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> where the last part (if I am not mistaken) holds in the case of the</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> exponential and other matrix power series.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> It can be seen that this is now the forward-mode derivative</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (right multiplication) applied to the Jacobian of the transpose.</span>\n    grad_func <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>: scipy.linalg.expm_frechet(x, y, <span class=\"pl-v\">compute_expm</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-k\">return</span> tf.py_func(grad_func, [tf.transpose(op.inputs[<span class=\"pl-c1\">0</span>]), grad], tf.float64) <span class=\"pl-c\"><span class=\"pl-c\">#</span> List of one Tensor, since we have one input</span></pre></div>\n<p>The updated \"tests\" are in the same gist: <a href=\"https://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda\">https://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda</a></p>\n<p>It'd be great to have some proper unit tests at some point.</p>", "body_text": "I did some further experimentation and while I am still not 100% confident I get all the indexing right, this seems more consistent:\n@ops.RegisterGradient(\"MatrixExponential\")\ndef _expm_grad(op, grad):\n    # We want the backward-mode gradient (left multiplication).\n    # Let X be the NxN input matrix.\n    # Let J(X) be the the N^2xN^2 complete Jacobian matrix of expm at X.\n    # Let Y be the NxN previous gradient in the backward AD (left multiplication)\n    # We have\n    # unvec( ( vec(Y)^T . J(X) )^T )\n    #   = unvec( J(X)^T . vec(Y) )\n    #   = unvec( J(X^T) . vec(Y) )\n    # where the last part (if I am not mistaken) holds in the case of the\n    # exponential and other matrix power series.\n    # It can be seen that this is now the forward-mode derivative\n    # (right multiplication) applied to the Jacobian of the transpose.\n    grad_func = lambda x, y: scipy.linalg.expm_frechet(x, y, compute_expm=False)\n    return tf.py_func(grad_func, [tf.transpose(op.inputs[0]), grad], tf.float64) # List of one Tensor, since we have one input\nThe updated \"tests\" are in the same gist: https://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda\nIt'd be great to have some proper unit tests at some point.", "body": "I did some further experimentation and while I am still not 100% confident I get all the indexing right, this seems more consistent:\r\n```python\r\n@ops.RegisterGradient(\"MatrixExponential\")\r\ndef _expm_grad(op, grad):\r\n    # We want the backward-mode gradient (left multiplication).\r\n    # Let X be the NxN input matrix.\r\n    # Let J(X) be the the N^2xN^2 complete Jacobian matrix of expm at X.\r\n    # Let Y be the NxN previous gradient in the backward AD (left multiplication)\r\n    # We have\r\n    # unvec( ( vec(Y)^T . J(X) )^T )\r\n    #   = unvec( J(X)^T . vec(Y) )\r\n    #   = unvec( J(X^T) . vec(Y) )\r\n    # where the last part (if I am not mistaken) holds in the case of the\r\n    # exponential and other matrix power series.\r\n    # It can be seen that this is now the forward-mode derivative\r\n    # (right multiplication) applied to the Jacobian of the transpose.\r\n    grad_func = lambda x, y: scipy.linalg.expm_frechet(x, y, compute_expm=False)\r\n    return tf.py_func(grad_func, [tf.transpose(op.inputs[0]), grad], tf.float64) # List of one Tensor, since we have one input\r\n```\r\n\r\nThe updated \"tests\" are in the same gist: https://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda\r\n\r\nIt'd be great to have some proper unit tests at some point."}