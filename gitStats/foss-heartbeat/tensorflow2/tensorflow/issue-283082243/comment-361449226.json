{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361449226", "html_url": "https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-361449226", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15465", "id": 361449226, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQ0OTIyNg==", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T01:46:19Z", "updated_at": "2018-01-30T01:46:19Z", "author_association": "MEMBER", "body_html": "<p>I think modifying Eigen to enable automatic differentiation is probably not feasible in this case, since the matrix exponentiation code is neither in Eigen core, nor in the tensor library originally co-developed with TensorFlow. I think a more viable approach would be to derive the gradients mathematically and implementing that in TensorFlow, possibly as a custom C++ gradient op (or preferably in Python using existing primitives/ops).</p>", "body_text": "I think modifying Eigen to enable automatic differentiation is probably not feasible in this case, since the matrix exponentiation code is neither in Eigen core, nor in the tensor library originally co-developed with TensorFlow. I think a more viable approach would be to derive the gradients mathematically and implementing that in TensorFlow, possibly as a custom C++ gradient op (or preferably in Python using existing primitives/ops).", "body": "I think modifying Eigen to enable automatic differentiation is probably not feasible in this case, since the matrix exponentiation code is neither in Eigen core, nor in the tensor library originally co-developed with TensorFlow. I think a more viable approach would be to derive the gradients mathematically and implementing that in TensorFlow, possibly as a custom C++ gradient op (or preferably in Python using existing primitives/ops)."}