{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404657288", "html_url": "https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-404657288", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15465", "id": 404657288, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDY1NzI4OA==", "user": {"login": "tvercaut", "id": 1614505, "node_id": "MDQ6VXNlcjE2MTQ1MDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/1614505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tvercaut", "html_url": "https://github.com/tvercaut", "followers_url": "https://api.github.com/users/tvercaut/followers", "following_url": "https://api.github.com/users/tvercaut/following{/other_user}", "gists_url": "https://api.github.com/users/tvercaut/gists{/gist_id}", "starred_url": "https://api.github.com/users/tvercaut/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tvercaut/subscriptions", "organizations_url": "https://api.github.com/users/tvercaut/orgs", "repos_url": "https://api.github.com/users/tvercaut/repos", "events_url": "https://api.github.com/users/tvercaut/events{/privacy}", "received_events_url": "https://api.github.com/users/tvercaut/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T21:31:43Z", "updated_at": "2018-07-12T21:33:13Z", "author_association": "NONE", "body_html": "<p>In case this is helpful for others, I used this as an excuse to learn about custom ops in tensorflow. It looks like getting a pure python version is only a couple of lines:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MatrixExponential<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_expm_grad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> If I got it right we would need a left multiplication with the</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> output gradient but since the expm_frechet function assumes</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> column major vectorisation while tensorflow assumes row major vectorisation</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> we can just do a right multiplication...</span>\n    out_expm, new_grad <span class=\"pl-k\">=</span> tf.py_func(scipy.linalg.expm_frechet, [op.inputs[<span class=\"pl-c1\">0</span>], grad], tf.float64)\n    <span class=\"pl-k\">return</span> [tf.transpose(new_grad)] <span class=\"pl-c\"><span class=\"pl-c\">#</span> List of one Tensor, since we have one input</span></pre></div>\n<p>I am not 100% confident about the row major complication so additional eyeballs with better understanding of the internals of TF would be great. A short runnable example using this code can be found here:<br>\n<a href=\"https://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda\">https://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda</a></p>", "body_text": "In case this is helpful for others, I used this as an excuse to learn about custom ops in tensorflow. It looks like getting a pure python version is only a couple of lines:\n@ops.RegisterGradient(\"MatrixExponential\")\ndef _expm_grad(op, grad):\n    # If I got it right we would need a left multiplication with the\n    # output gradient but since the expm_frechet function assumes\n    # column major vectorisation while tensorflow assumes row major vectorisation\n    # we can just do a right multiplication...\n    out_expm, new_grad = tf.py_func(scipy.linalg.expm_frechet, [op.inputs[0], grad], tf.float64)\n    return [tf.transpose(new_grad)] # List of one Tensor, since we have one input\nI am not 100% confident about the row major complication so additional eyeballs with better understanding of the internals of TF would be great. A short runnable example using this code can be found here:\nhttps://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda", "body": "In case this is helpful for others, I used this as an excuse to learn about custom ops in tensorflow. It looks like getting a pure python version is only a couple of lines:\r\n```python\r\n@ops.RegisterGradient(\"MatrixExponential\")\r\ndef _expm_grad(op, grad):\r\n    # If I got it right we would need a left multiplication with the\r\n    # output gradient but since the expm_frechet function assumes\r\n    # column major vectorisation while tensorflow assumes row major vectorisation\r\n    # we can just do a right multiplication...\r\n    out_expm, new_grad = tf.py_func(scipy.linalg.expm_frechet, [op.inputs[0], grad], tf.float64)\r\n    return [tf.transpose(new_grad)] # List of one Tensor, since we have one input\r\n```\r\n\r\nI am not 100% confident about the row major complication so additional eyeballs with better understanding of the internals of TF would be great. A short runnable example using this code can be found here:\r\nhttps://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda"}