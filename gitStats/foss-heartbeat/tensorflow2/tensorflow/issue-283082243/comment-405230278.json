{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/405230278", "html_url": "https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-405230278", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15465", "id": 405230278, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTIzMDI3OA==", "user": {"login": "kmcnaught", "id": 12151404, "node_id": "MDQ6VXNlcjEyMTUxNDA0", "avatar_url": "https://avatars2.githubusercontent.com/u/12151404?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kmcnaught", "html_url": "https://github.com/kmcnaught", "followers_url": "https://api.github.com/users/kmcnaught/followers", "following_url": "https://api.github.com/users/kmcnaught/following{/other_user}", "gists_url": "https://api.github.com/users/kmcnaught/gists{/gist_id}", "starred_url": "https://api.github.com/users/kmcnaught/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kmcnaught/subscriptions", "organizations_url": "https://api.github.com/users/kmcnaught/orgs", "repos_url": "https://api.github.com/users/kmcnaught/repos", "events_url": "https://api.github.com/users/kmcnaught/events{/privacy}", "received_events_url": "https://api.github.com/users/kmcnaught/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-16T12:23:56Z", "updated_at": "2018-07-16T12:23:56Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1614505\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tvercaut\">@tvercaut</a> I haven't thought through your math/indexing but I've empirically tested your gradient against <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=143368\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dpfau\">@dpfau</a>'s suggestion, which uses existing differentiable tf ops but only works for symmetric matrices (so we can test your method for symmetric matrices).</p>\n<p>The gradients seem to agree with your gist tests (modulo the note below). See my gist, based on yours:<br>\n<a href=\"https://gist.github.com/kmcnaught/e499cf89b76fa05b9bfb75f5b419404b\">https://gist.github.com/kmcnaught/e499cf89b76fa05b9bfb75f5b419404b</a></p>\n<p>Note that <code>self_adjoint_eig</code> uses only the lower triangular part of the input matrix, so the resulting gradients are either double or zero on the off-diagonals, compared to your version which distributes them on both triangles. The two are equivalent in the case we have where the input is constrained to be symmetric.</p>", "body_text": "@tvercaut I haven't thought through your math/indexing but I've empirically tested your gradient against @dpfau's suggestion, which uses existing differentiable tf ops but only works for symmetric matrices (so we can test your method for symmetric matrices).\nThe gradients seem to agree with your gist tests (modulo the note below). See my gist, based on yours:\nhttps://gist.github.com/kmcnaught/e499cf89b76fa05b9bfb75f5b419404b\nNote that self_adjoint_eig uses only the lower triangular part of the input matrix, so the resulting gradients are either double or zero on the off-diagonals, compared to your version which distributes them on both triangles. The two are equivalent in the case we have where the input is constrained to be symmetric.", "body": "@tvercaut I haven't thought through your math/indexing but I've empirically tested your gradient against @dpfau's suggestion, which uses existing differentiable tf ops but only works for symmetric matrices (so we can test your method for symmetric matrices).\r\n\r\nThe gradients seem to agree with your gist tests (modulo the note below). See my gist, based on yours: \r\nhttps://gist.github.com/kmcnaught/e499cf89b76fa05b9bfb75f5b419404b\r\n\r\nNote that `self_adjoint_eig` uses only the lower triangular part of the input matrix, so the resulting gradients are either double or zero on the off-diagonals, compared to your version which distributes them on both triangles. The two are equivalent in the case we have where the input is constrained to be symmetric. \r\n\r\n"}