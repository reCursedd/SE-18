{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361614510", "html_url": "https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-361614510", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15465", "id": 361614510, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTYxNDUxMA==", "user": {"login": "dpfau", "id": 143368, "node_id": "MDQ6VXNlcjE0MzM2OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/143368?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dpfau", "html_url": "https://github.com/dpfau", "followers_url": "https://api.github.com/users/dpfau/followers", "following_url": "https://api.github.com/users/dpfau/following{/other_user}", "gists_url": "https://api.github.com/users/dpfau/gists{/gist_id}", "starred_url": "https://api.github.com/users/dpfau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dpfau/subscriptions", "organizations_url": "https://api.github.com/users/dpfau/orgs", "repos_url": "https://api.github.com/users/dpfau/repos", "events_url": "https://api.github.com/users/dpfau/events{/privacy}", "received_events_url": "https://api.github.com/users/dpfau/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T14:45:07Z", "updated_at": "2018-01-30T14:45:07Z", "author_association": "NONE", "body_html": "<p>I added the original MatrixExponential op. It should be straightforward to add a gradient op written in Python using the closed-form expression for the backward pass. In the meantime, you can compute the matrix exponential by computing the eigendecomposition, exponentiating the eigenvalues, and multiplying the matrices back together. It will be slower, but those ops all should have gradients implemented.</p>", "body_text": "I added the original MatrixExponential op. It should be straightforward to add a gradient op written in Python using the closed-form expression for the backward pass. In the meantime, you can compute the matrix exponential by computing the eigendecomposition, exponentiating the eigenvalues, and multiplying the matrices back together. It will be slower, but those ops all should have gradients implemented.", "body": "I added the original MatrixExponential op. It should be straightforward to add a gradient op written in Python using the closed-form expression for the backward pass. In the meantime, you can compute the matrix exponential by computing the eigendecomposition, exponentiating the eigenvalues, and multiplying the matrices back together. It will be slower, but those ops all should have gradients implemented."}