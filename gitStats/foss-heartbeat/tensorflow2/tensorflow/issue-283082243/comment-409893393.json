{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/409893393", "html_url": "https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-409893393", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15465", "id": 409893393, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTg5MzM5Mw==", "user": {"login": "tvercaut", "id": 1614505, "node_id": "MDQ6VXNlcjE2MTQ1MDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/1614505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tvercaut", "html_url": "https://github.com/tvercaut", "followers_url": "https://api.github.com/users/tvercaut/followers", "following_url": "https://api.github.com/users/tvercaut/following{/other_user}", "gists_url": "https://api.github.com/users/tvercaut/gists{/gist_id}", "starred_url": "https://api.github.com/users/tvercaut/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tvercaut/subscriptions", "organizations_url": "https://api.github.com/users/tvercaut/orgs", "repos_url": "https://api.github.com/users/tvercaut/repos", "events_url": "https://api.github.com/users/tvercaut/events{/privacy}", "received_events_url": "https://api.github.com/users/tvercaut/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-02T11:20:59Z", "updated_at": "2018-08-02T11:20:59Z", "author_association": "NONE", "body_html": "<p>Thanks! Did you benchmark the accuracy and efficiency of the resulting autodiff gradient with respect to a version based on</p>\n<blockquote>\n<p>Awad H. Al-Mohy and Nicholas J. Higham (2009) Computing the Frechet Derivative of the Matrix Exponential, with an application to Condition Number Estimation. SIAM Journal On Matrix Analysis and Applications., 30 (4). pp. 1639-1657. ISSN 1095-7162</p>\n</blockquote>\n<p>such as the one implemented in <code>scipy.linalg.expm_frechet</code>:<br>\n<a href=\"http://scipy.github.io/devdocs/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet\" rel=\"nofollow\">http://scipy.github.io/devdocs/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet</a><br>\n<a href=\"https://github.com/scipy/scipy/blob/v1.1.0/scipy/linalg/_expm_frechet.py#L225-L278\">https://github.com/scipy/scipy/blob/v1.1.0/scipy/linalg/_expm_frechet.py#L225-L278</a></p>\n<p>The algorithm is also based on Pad\u00e9 and scaling and squaring. As mentioned above it would need to be applied to the transpose of the input matrix to get the backward-mode gradient.</p>", "body_text": "Thanks! Did you benchmark the accuracy and efficiency of the resulting autodiff gradient with respect to a version based on\n\nAwad H. Al-Mohy and Nicholas J. Higham (2009) Computing the Frechet Derivative of the Matrix Exponential, with an application to Condition Number Estimation. SIAM Journal On Matrix Analysis and Applications., 30 (4). pp. 1639-1657. ISSN 1095-7162\n\nsuch as the one implemented in scipy.linalg.expm_frechet:\nhttp://scipy.github.io/devdocs/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet\nhttps://github.com/scipy/scipy/blob/v1.1.0/scipy/linalg/_expm_frechet.py#L225-L278\nThe algorithm is also based on Pad\u00e9 and scaling and squaring. As mentioned above it would need to be applied to the transpose of the input matrix to get the backward-mode gradient.", "body": "Thanks! Did you benchmark the accuracy and efficiency of the resulting autodiff gradient with respect to a version based on \r\n> Awad H. Al-Mohy and Nicholas J. Higham (2009) Computing the Frechet Derivative of the Matrix Exponential, with an application to Condition Number Estimation. SIAM Journal On Matrix Analysis and Applications., 30 (4). pp. 1639-1657. ISSN 1095-7162\r\n\r\nsuch as the one implemented in `scipy.linalg.expm_frechet`:\r\nhttp://scipy.github.io/devdocs/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet\r\nhttps://github.com/scipy/scipy/blob/v1.1.0/scipy/linalg/_expm_frechet.py#L225-L278\r\n\r\nThe algorithm is also based on Pad\u00e9 and scaling and squaring. As mentioned above it would need to be applied to the transpose of the input matrix to get the backward-mode gradient."}