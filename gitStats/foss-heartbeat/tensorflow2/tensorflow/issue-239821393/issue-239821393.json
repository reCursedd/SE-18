{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11181", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11181/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11181/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11181/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11181", "id": 239821393, "node_id": "MDU6SXNzdWUyMzk4MjEzOTM=", "number": 11181, "title": "The 2015 Inception checkpoint gives incorrect results", "user": {"login": "korrawat", "id": 23010630, "node_id": "MDQ6VXNlcjIzMDEwNjMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23010630?v=4", "gravatar_id": "", "url": "https://api.github.com/users/korrawat", "html_url": "https://github.com/korrawat", "followers_url": "https://api.github.com/users/korrawat/followers", "following_url": "https://api.github.com/users/korrawat/following{/other_user}", "gists_url": "https://api.github.com/users/korrawat/gists{/gist_id}", "starred_url": "https://api.github.com/users/korrawat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/korrawat/subscriptions", "organizations_url": "https://api.github.com/users/korrawat/orgs", "repos_url": "https://api.github.com/users/korrawat/repos", "events_url": "https://api.github.com/users/korrawat/events{/privacy}", "received_events_url": "https://api.github.com/users/korrawat/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-06-30T15:57:51Z", "updated_at": "2018-01-24T19:39:55Z", "closed_at": "2018-01-24T19:39:55Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Problems</h3>\n<ul>\n<li>The 2015 Inception checkpoint file called <code>classify_image_graph_def.pb</code>, which can be found in <a href=\"http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\" rel=\"nofollow\"><code>inception-2015-12-05.tgz</code></a>, gives weird results.</li>\n<li>The <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models\">tutorial on quantization</a> (together with <a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">this one</a>) needs corrections.</li>\n</ul>\n<h3>Explanation</h3>\n<p>I originally wanted to try <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models\">this tutorial on quantization</a> to quantize the model and then check the result with <code>label_image</code>.</p>\n<p>Using the same name convention as in the tutorial, the quantize step will quantize the original graph, called <code>classify_image_graph_def.pb</code>, into a quantized one, called <code>quantized_graph.pb</code>. So before checking the latter, I wanted to first <code>label_image</code> using this <code>classify_image_graph_def.pb</code> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/data/grace_hopper.jpg\">this sample image</a>.</p>\n<p>The result should be the same as in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/\">this <code>label_image</code>'s README</a>, as shown below. This example, however, uses a newer pre-trained graph, <code>inception_v3_2016_08_28_frozen.pb</code>, from <a href=\"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" rel=\"nofollow\">this compressed file</a>. In addition to the pb file, the compressed tarball also contains <code>imagenet_slim_labels.txt</code>, containing 1,001 lines of label names, e.g. \"military uniform\".</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> expected result</span>\nmilitary uniform (653): 0.834306\nmortarboard (668): 0.0218692\nacademic gown (401): 0.0103579\npickelhaube (716): 0.00800814\nbulletproof vest (466): 0.00535088</pre></div>\n<p>So, to check <code>classify_image_graph_def.pb</code>, I looked at <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models\">the tutorial</a> and changed the <code>--graph</code> argument from <code>/tmp/quantized_graph.pb</code> to <code>/tmp/classify_image_graph_def.pb</code>. However, the <code>--labels</code> argument also needs to be fixed, because it suggests using <code>imagenet_synset_to_human_label_map.txt</code> (from <a href=\"http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\" rel=\"nofollow\"><code>inception-2015-12-05.tgz</code></a>), which actually has 21,842 lines of (synset id, label name). So, <strong>after downloading <code>imagenet_slim_labels.txt</code> by following <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/\">this <code>label_image</code>'s README</a></strong>, fix the <code>--labels</code> argument by either leaving it out (because it's the default if following those steps) or explicitly calling the labels file.</p>\n<p>The code to run <code>label_image</code> for <code>classify_image_graph_def.pb</code> becomes:</p>\n<div class=\"highlight highlight-source-shell\"><pre>bazel build tensorflow/examples/label_image:label_image\nbazel-bin/tensorflow/examples/label_image/label_image \\\n--image=tensorflow/examples/label_image/data/grace_hopper.jpg \\\n--graph=/tmp/classify_image_class_def.pb \\\n--input_width=299 \\\n--input_height=299 \\\n--input_mean=128 \\\n--input_std=128 \\\n--input_layer=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Mul:0<span class=\"pl-pds\">\"</span></span> \\\n--output_layer=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax:0<span class=\"pl-pds\">\"</span></span></pre></div>\n<p>And here is the result.</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> classify_image_graph_def.pb</span>\ntoyshop (866): 0.684115\nshower cap (794): 0.0394605\nwarplane (896): 0.0219743\ntape player (849): 0.0138034\nzucchini (940): 0.013588</pre></div>\n<h3>Differences between checkpoint files</h3>\n<p><em>There is also another Inception v3 checkpoint file, <code>inception-v3-2016-03-01.tar.gz</code>, according to <a href=\"https://github.com/tensorflow/models/tree/master/inception#how-to-fine-tune-a-pre-trained-model-on-a-new-task\">this tutorial on how to fine-tune Inception</a>!</em></p>\n<p>So I used <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py\"><code>import_pb_to_tensorboard.py</code></a> to check why the two checkpoints are different.</p>\n<p>Besides the difference in implementation (e.g. how each layers are named), the most important difference I noticed is in the last FC layer. While <code>inception_v3_2016_08_28_frozen.pb</code> maps from 2048 to 1001 (the 0th class is \"dummy\"), <strong><code>classify_image_graph_def.pb</code> maps from 2048 to 1008!</strong> I'm not sure what 1008 means, but maybe it was trained with a different set of labels in a different order?</p>\n<p>The naming difference also makes things a little more complicated. When calling <code>label_image</code> with <code>classify_image_graph_def.pb</code>, I need to specify <code>--input_layer=\"Mul:0\" --output_layer=\"softmax:0\"</code> because these are the names of the input and output layers. However, with the new checkpoint that works, we don't have to specify these two arguments because <code>input_layer = \"input\"</code> and <code>output_layer = \"InceptionV3/Predictions/Reshape_1\"</code> are already hard-coded as the default values in the implementations (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc\">C++</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py\">python</a>).</p>\n<h3>Related issues</h3>\n<p>There are question related to the differences between these two checkpoints and why they give different performances on certain tasks. I've found these issues in the <a href=\"https://github.com/tensorflow/models\">tensorflow/models</a> repo: <a href=\"https://github.com/tensorflow/models/issues/1314\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/1314/hovercard\">#1314</a>, <a href=\"https://github.com/tensorflow/models/issues/1316\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/1316/hovercard\">#1316</a>.</p>\n<h3>Quantization</h3>\n<p>Now, back to quantization ... Following the current code in the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md\">tutorial</a> to quantize <code>classify_image_graph_def.pb</code> into <code>quantized_graph.pb</code> will give an error. This is because the sample code is missing an <code>--inputs</code> argument, compared to <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#eight-bit-calculations\">this tutorial on <code>graph_transforms</code></a>. <strong>The <code>transform_graph</code> example needs <code>--inputs=\"Mul\"</code>.</strong> Then it will work, i.e. predicting \"toyshop\" as the top choice.</p>\n<p>Actually, this quantization process works with the new <code>inception_v3_2016_08_28_frozen.pb</code> -- predicting that the picture is \"military uniform\" by both original and quantized graphs.</p>\n<h3>Questions</h3>\n<ul>\n<li>What are the actual differences between the three checkpoints? Which one should we use? Any clarification would be really helpful. When different tutorials refer to different checkpoint files, at first I thought all of them can be used interchangeably. As it turns out, they are actually not the same and this has caused a lot of confusion.</li>\n<li>Why is the last layer in <code>classify_image_graph_def.pb</code> from 2015 have 1008 nodes, not 1001?</li>\n<li>Is there a way to make <code>classify_image_graph_def.pb</code> work, following the tutorial? Did I miss any arguments or other settings?</li>\n<li><em>Somewhat unrelated question</em>: The <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md\">tutorial</a> on quantization mentioned above seems to be the same as <a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">this one on tensorflow.org</a>, except for the last few commits. Therefore, the tutorial on the website is not up-to-date. Are they supposed to be the same?</li>\n<li>How should the tutorials on quantization be updated? Personally, I think it can be fixed to use the new checkpoint, then adjust the example codes accordingly.</li>\n</ul>\n<p>Thank you!</p>", "body_text": "Problems\n\nThe 2015 Inception checkpoint file called classify_image_graph_def.pb, which can be found in inception-2015-12-05.tgz, gives weird results.\nThe tutorial on quantization (together with this one) needs corrections.\n\nExplanation\nI originally wanted to try this tutorial on quantization to quantize the model and then check the result with label_image.\nUsing the same name convention as in the tutorial, the quantize step will quantize the original graph, called classify_image_graph_def.pb, into a quantized one, called quantized_graph.pb. So before checking the latter, I wanted to first label_image using this classify_image_graph_def.pb and this sample image.\nThe result should be the same as in this label_image's README, as shown below. This example, however, uses a newer pre-trained graph, inception_v3_2016_08_28_frozen.pb, from this compressed file. In addition to the pb file, the compressed tarball also contains imagenet_slim_labels.txt, containing 1,001 lines of label names, e.g. \"military uniform\".\n# expected result\nmilitary uniform (653): 0.834306\nmortarboard (668): 0.0218692\nacademic gown (401): 0.0103579\npickelhaube (716): 0.00800814\nbulletproof vest (466): 0.00535088\nSo, to check classify_image_graph_def.pb, I looked at the tutorial and changed the --graph argument from /tmp/quantized_graph.pb to /tmp/classify_image_graph_def.pb. However, the --labels argument also needs to be fixed, because it suggests using imagenet_synset_to_human_label_map.txt (from inception-2015-12-05.tgz), which actually has 21,842 lines of (synset id, label name). So, after downloading imagenet_slim_labels.txt by following this label_image's README, fix the --labels argument by either leaving it out (because it's the default if following those steps) or explicitly calling the labels file.\nThe code to run label_image for classify_image_graph_def.pb becomes:\nbazel build tensorflow/examples/label_image:label_image\nbazel-bin/tensorflow/examples/label_image/label_image \\\n--image=tensorflow/examples/label_image/data/grace_hopper.jpg \\\n--graph=/tmp/classify_image_class_def.pb \\\n--input_width=299 \\\n--input_height=299 \\\n--input_mean=128 \\\n--input_std=128 \\\n--input_layer=\"Mul:0\" \\\n--output_layer=\"softmax:0\"\nAnd here is the result.\n# classify_image_graph_def.pb\ntoyshop (866): 0.684115\nshower cap (794): 0.0394605\nwarplane (896): 0.0219743\ntape player (849): 0.0138034\nzucchini (940): 0.013588\nDifferences between checkpoint files\nThere is also another Inception v3 checkpoint file, inception-v3-2016-03-01.tar.gz, according to this tutorial on how to fine-tune Inception!\nSo I used import_pb_to_tensorboard.py to check why the two checkpoints are different.\nBesides the difference in implementation (e.g. how each layers are named), the most important difference I noticed is in the last FC layer. While inception_v3_2016_08_28_frozen.pb maps from 2048 to 1001 (the 0th class is \"dummy\"), classify_image_graph_def.pb maps from 2048 to 1008! I'm not sure what 1008 means, but maybe it was trained with a different set of labels in a different order?\nThe naming difference also makes things a little more complicated. When calling label_image with classify_image_graph_def.pb, I need to specify --input_layer=\"Mul:0\" --output_layer=\"softmax:0\" because these are the names of the input and output layers. However, with the new checkpoint that works, we don't have to specify these two arguments because input_layer = \"input\" and output_layer = \"InceptionV3/Predictions/Reshape_1\" are already hard-coded as the default values in the implementations (C++ and python).\nRelated issues\nThere are question related to the differences between these two checkpoints and why they give different performances on certain tasks. I've found these issues in the tensorflow/models repo: #1314, #1316.\nQuantization\nNow, back to quantization ... Following the current code in the tutorial to quantize classify_image_graph_def.pb into quantized_graph.pb will give an error. This is because the sample code is missing an --inputs argument, compared to this tutorial on graph_transforms. The transform_graph example needs --inputs=\"Mul\". Then it will work, i.e. predicting \"toyshop\" as the top choice.\nActually, this quantization process works with the new inception_v3_2016_08_28_frozen.pb -- predicting that the picture is \"military uniform\" by both original and quantized graphs.\nQuestions\n\nWhat are the actual differences between the three checkpoints? Which one should we use? Any clarification would be really helpful. When different tutorials refer to different checkpoint files, at first I thought all of them can be used interchangeably. As it turns out, they are actually not the same and this has caused a lot of confusion.\nWhy is the last layer in classify_image_graph_def.pb from 2015 have 1008 nodes, not 1001?\nIs there a way to make classify_image_graph_def.pb work, following the tutorial? Did I miss any arguments or other settings?\nSomewhat unrelated question: The tutorial on quantization mentioned above seems to be the same as this one on tensorflow.org, except for the last few commits. Therefore, the tutorial on the website is not up-to-date. Are they supposed to be the same?\nHow should the tutorials on quantization be updated? Personally, I think it can be fixed to use the new checkpoint, then adjust the example codes accordingly.\n\nThank you!", "body": "### Problems\r\n- The 2015 Inception checkpoint file called `classify_image_graph_def.pb`, which can be found in [`inception-2015-12-05.tgz`](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz), gives weird results.\r\n- The [tutorial on quantization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) (together with [this one](https://www.tensorflow.org/performance/quantization)) needs corrections.\r\n\r\n### Explanation\r\nI originally wanted to try [this tutorial on quantization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) to quantize the model and then check the result with `label_image`.\r\n\r\nUsing the same name convention as in the tutorial, the quantize step will quantize the original graph, called `classify_image_graph_def.pb`, into a quantized one, called `quantized_graph.pb`. So before checking the latter, I wanted to first `label_image` using this `classify_image_graph_def.pb` and [this sample image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/data/grace_hopper.jpg).\r\n\r\nThe result should be the same as in [this `label_image`'s README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/), as shown below. This example, however, uses a newer pre-trained graph, `inception_v3_2016_08_28_frozen.pb`, from [this compressed file](https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz). In addition to the pb file, the compressed tarball also contains `imagenet_slim_labels.txt`, containing 1,001 lines of label names, e.g. \"military uniform\".\r\n\r\n```bash\r\n# expected result\r\nmilitary uniform (653): 0.834306\r\nmortarboard (668): 0.0218692\r\nacademic gown (401): 0.0103579\r\npickelhaube (716): 0.00800814\r\nbulletproof vest (466): 0.00535088\r\n```\r\n\r\nSo, to check `classify_image_graph_def.pb`, I looked at [the tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) and changed the `--graph` argument from `/tmp/quantized_graph.pb` to `/tmp/classify_image_graph_def.pb`. However, the `--labels` argument also needs to be fixed, because it suggests using `imagenet_synset_to_human_label_map.txt` (from [`inception-2015-12-05.tgz`](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz)), which actually has 21,842 lines of (synset id, label name). So, **after downloading `imagenet_slim_labels.txt` by following [this `label_image`'s README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/)**, fix the `--labels` argument by either leaving it out (because it's the default if following those steps) or explicitly calling the labels file.\r\n\r\nThe code to run `label_image` for `classify_image_graph_def.pb` becomes:\r\n\r\n```bash\r\nbazel build tensorflow/examples/label_image:label_image\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--image=tensorflow/examples/label_image/data/grace_hopper.jpg \\\r\n--graph=/tmp/classify_image_class_def.pb \\\r\n--input_width=299 \\\r\n--input_height=299 \\\r\n--input_mean=128 \\\r\n--input_std=128 \\\r\n--input_layer=\"Mul:0\" \\\r\n--output_layer=\"softmax:0\"\r\n```\r\n\r\nAnd here is the result.\r\n\r\n```bash\r\n# classify_image_graph_def.pb\r\ntoyshop (866): 0.684115\r\nshower cap (794): 0.0394605\r\nwarplane (896): 0.0219743\r\ntape player (849): 0.0138034\r\nzucchini (940): 0.013588\r\n```\r\n\r\n### Differences between checkpoint files\r\n\r\n_There is also another Inception v3 checkpoint file, `inception-v3-2016-03-01.tar.gz`, according to [this tutorial on how to fine-tune Inception](https://github.com/tensorflow/models/tree/master/inception#how-to-fine-tune-a-pre-trained-model-on-a-new-task)!_\r\n\r\nSo I used [`import_pb_to_tensorboard.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py) to check why the two checkpoints are different.\r\n\r\nBesides the difference in implementation (e.g. how each layers are named), the most important difference I noticed is in the last FC layer. While `inception_v3_2016_08_28_frozen.pb` maps from 2048 to 1001 (the 0th class is \"dummy\"), **`classify_image_graph_def.pb` maps from 2048 to 1008!** I'm not sure what 1008 means, but maybe it was trained with a different set of labels in a different order?\r\n\r\nThe naming difference also makes things a little more complicated. When calling `label_image` with `classify_image_graph_def.pb`, I need to specify `--input_layer=\"Mul:0\" --output_layer=\"softmax:0\"` because these are the names of the input and output layers. However, with the new checkpoint that works, we don't have to specify these two arguments because `input_layer = \"input\"` and `output_layer = \"InceptionV3/Predictions/Reshape_1\"` are already hard-coded as the default values in the implementations ([C++](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) and [python](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py)).\r\n\r\n### Related issues\r\nThere are question related to the differences between these two checkpoints and why they give different performances on certain tasks. I've found these issues in the [tensorflow/models](https://github.com/tensorflow/models) repo: [#1314](https://github.com/tensorflow/models/issues/1314), [#1316](https://github.com/tensorflow/models/issues/1316).\r\n\r\n### Quantization\r\nNow, back to quantization ... Following the current code in the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md) to quantize `classify_image_graph_def.pb` into `quantized_graph.pb` will give an error. This is because the sample code is missing an `--inputs` argument, compared to [this tutorial on `graph_transforms`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#eight-bit-calculations). **The `transform_graph` example needs `--inputs=\"Mul\"`.** Then it will work, i.e. predicting \"toyshop\" as the top choice.\r\n\r\nActually, this quantization process works with the new `inception_v3_2016_08_28_frozen.pb` -- predicting that the picture is \"military uniform\" by both original and quantized graphs.\r\n\r\n### Questions\r\n- What are the actual differences between the three checkpoints? Which one should we use? Any clarification would be really helpful. When different tutorials refer to different checkpoint files, at first I thought all of them can be used interchangeably. As it turns out, they are actually not the same and this has caused a lot of confusion.\r\n- Why is the last layer in `classify_image_graph_def.pb` from 2015 have 1008 nodes, not 1001?\r\n- Is there a way to make `classify_image_graph_def.pb` work, following the tutorial? Did I miss any arguments or other settings?\r\n- _Somewhat unrelated question_: The [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md) on quantization mentioned above seems to be the same as [this one on tensorflow.org](https://www.tensorflow.org/performance/quantization), except for the last few commits. Therefore, the tutorial on the website is not up-to-date. Are they supposed to be the same?\r\n- How should the tutorials on quantization be updated? Personally, I think it can be fixed to use the new checkpoint, then adjust the example codes accordingly.\r\n\r\nThank you!"}