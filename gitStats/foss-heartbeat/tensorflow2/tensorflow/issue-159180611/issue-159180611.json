{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2732", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2732/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2732/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2732/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2732", "id": 159180611, "node_id": "MDU6SXNzdWUxNTkxODA2MTE=", "number": 2732, "title": "Mention that GPU reductions are nondeterministic in docs", "user": {"login": "shiviser", "id": 1592961, "node_id": "MDQ6VXNlcjE1OTI5NjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1592961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shiviser", "html_url": "https://github.com/shiviser", "followers_url": "https://api.github.com/users/shiviser/followers", "following_url": "https://api.github.com/users/shiviser/following{/other_user}", "gists_url": "https://api.github.com/users/shiviser/gists{/gist_id}", "starred_url": "https://api.github.com/users/shiviser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shiviser/subscriptions", "organizations_url": "https://api.github.com/users/shiviser/orgs", "repos_url": "https://api.github.com/users/shiviser/repos", "events_url": "https://api.github.com/users/shiviser/events{/privacy}", "received_events_url": "https://api.github.com/users/shiviser/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 46, "created_at": "2016-06-08T14:38:47Z", "updated_at": "2018-09-18T17:10:13Z", "closed_at": "2018-09-18T17:10:04Z", "author_association": "NONE", "body_html": "<h1>The problem</h1>\n<p>I am trying out the <a href=\"https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#deep-mnist-for-experts\" rel=\"nofollow\">MNIST for experts tutorial</a> and I have inconsistent results on the GPU.</p>\n<h3>What do I mean by inconsistent?</h3>\n<p>With the exactly same network parameters (and randomness removed: read below in the post) every time I run the complete train-then-test process the accuracy is slightly different.</p>\n<h3>What have I done to visualize this problem?</h3>\n<p>For each iteration, I have calculated the differences between the variables (weights, biases) from two <em>independent but identical</em> runs and computed the L1 norm of those differences - <br></p>\n<ul>\n<li><a href=\"http://i.stack.imgur.com/W5PqZ.png\" rel=\"nofollow\">plot</a> of L1 norm for the first 1000 iterations in steps of 20.</li>\n</ul>\n<p>In a consistent world, these differences should be always zero!</p>\n<h3>How did I remove randomness in the code?</h3>\n<ul>\n<li>Removed dropout entirely</li>\n<li>added a graph level seed (<code>tf.set_random_seed(1234)</code>). With this the variable initialization is deterministic and also any other randomization in the code.</li>\n<li>The <a href=\"https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#deep-mnist-for-experts\" rel=\"nofollow\">MNIST for experts tutorial</a> uses <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py\">this script</a> to download/load the MNIST data. I have added <code>numpy.random.seed(3)</code> in <code>DataSet.__init__(self, images, labels, fake_data=False, one_hot=False, dtype=dtypes.float32)</code> in this script to remove randomness during the shuffling process (line 154 in <code>DataSet.next_batch(self, batch_size, fake_data=False)</code>)</li>\n<li><code>config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)</code> which goes into the creation of session as <code>sess = tf.Session(config=config)</code></li>\n</ul>\n<h3>What system am I using?</h3>\n<ul>\n<li>tensorflow 0.8 gpu version (installed via pip)</li>\n<li>OpenSUSE LEAP 42.1 (x86_64)</li>\n<li>Cuda Toolkit 7.5</li>\n<li>CuDNN 4.0</li>\n<li>Tesla K20c card with Nvidia driver 352.79</li>\n</ul>", "body_text": "The problem\nI am trying out the MNIST for experts tutorial and I have inconsistent results on the GPU.\nWhat do I mean by inconsistent?\nWith the exactly same network parameters (and randomness removed: read below in the post) every time I run the complete train-then-test process the accuracy is slightly different.\nWhat have I done to visualize this problem?\nFor each iteration, I have calculated the differences between the variables (weights, biases) from two independent but identical runs and computed the L1 norm of those differences - \n\nplot of L1 norm for the first 1000 iterations in steps of 20.\n\nIn a consistent world, these differences should be always zero!\nHow did I remove randomness in the code?\n\nRemoved dropout entirely\nadded a graph level seed (tf.set_random_seed(1234)). With this the variable initialization is deterministic and also any other randomization in the code.\nThe MNIST for experts tutorial uses this script to download/load the MNIST data. I have added numpy.random.seed(3) in DataSet.__init__(self, images, labels, fake_data=False, one_hot=False, dtype=dtypes.float32) in this script to remove randomness during the shuffling process (line 154 in DataSet.next_batch(self, batch_size, fake_data=False))\nconfig = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1) which goes into the creation of session as sess = tf.Session(config=config)\n\nWhat system am I using?\n\ntensorflow 0.8 gpu version (installed via pip)\nOpenSUSE LEAP 42.1 (x86_64)\nCuda Toolkit 7.5\nCuDNN 4.0\nTesla K20c card with Nvidia driver 352.79", "body": "# The problem\n\nI am trying out the [MNIST for experts tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#deep-mnist-for-experts) and I have inconsistent results on the GPU.\n### What do I mean by inconsistent?\n\nWith the exactly same network parameters (and randomness removed: read below in the post) every time I run the complete train-then-test process the accuracy is slightly different.\n### What have I done to visualize this problem?\n\nFor each iteration, I have calculated the differences between the variables (weights, biases) from two _independent but identical_ runs and computed the L1 norm of those differences - <br/>\n- [plot](http://i.stack.imgur.com/W5PqZ.png) of L1 norm for the first 1000 iterations in steps of 20.\n\nIn a consistent world, these differences should be always zero! \n### How did I remove randomness in the code?\n- Removed dropout entirely\n- added a graph level seed (`tf.set_random_seed(1234)`). With this the variable initialization is deterministic and also any other randomization in the code. \n- The [MNIST for experts tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#deep-mnist-for-experts) uses [this script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py) to download/load the MNIST data. I have added `numpy.random.seed(3)` in `DataSet.__init__(self, images, labels, fake_data=False, one_hot=False, dtype=dtypes.float32)` in this script to remove randomness during the shuffling process (line 154 in `DataSet.next_batch(self, batch_size, fake_data=False)`)\n- `config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)` which goes into the creation of session as `sess = tf.Session(config=config)`\n### What system am I using?\n- tensorflow 0.8 gpu version (installed via pip)\n- OpenSUSE LEAP 42.1 (x86_64)\n- Cuda Toolkit 7.5\n- CuDNN 4.0\n- Tesla K20c card with Nvidia driver 352.79\n"}