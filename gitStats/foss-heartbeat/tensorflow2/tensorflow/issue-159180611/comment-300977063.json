{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300977063", "html_url": "https://github.com/tensorflow/tensorflow/issues/2732#issuecomment-300977063", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2732", "id": 300977063, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDk3NzA2Mw==", "user": {"login": "jkschin", "id": 6997460, "node_id": "MDQ6VXNlcjY5OTc0NjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/6997460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jkschin", "html_url": "https://github.com/jkschin", "followers_url": "https://api.github.com/users/jkschin/followers", "following_url": "https://api.github.com/users/jkschin/following{/other_user}", "gists_url": "https://api.github.com/users/jkschin/gists{/gist_id}", "starred_url": "https://api.github.com/users/jkschin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jkschin/subscriptions", "organizations_url": "https://api.github.com/users/jkschin/orgs", "repos_url": "https://api.github.com/users/jkschin/repos", "events_url": "https://api.github.com/users/jkschin/events{/privacy}", "received_events_url": "https://api.github.com/users/jkschin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-12T03:51:20Z", "updated_at": "2017-05-12T04:01:20Z", "author_association": "NONE", "body_html": "<p>BUILD<br>\nTensorFlow GPU 1.1.0<br>\nUbuntu 16.04<br>\nCUDA 8.0.61<br>\nCUDNN 5.1<br>\nNVIDIA Drivers 375<br>\n2 GTX 1080Ti, with 1 1080Ti used for 2 monitors. I set CUDA_VISIBLE_DEVICES=0 for all experiments. This uses the GPU with nothing attached.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> why is the forward pass deterministic but the very first backward pass not deterministic?</p>\n<p>On a simple MNIST example, the logits are 100% deterministic over 100 runs (I did more runs than that but plotted 100).</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981433/a9473ee0-3707-11e7-8a60-240963c6c471.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981433/a9473ee0-3707-11e7-8a60-240963c6c471.png\" alt=\"logits\" style=\"max-width:100%;\"></a></p>\n<p>However, when we look at the gradients computed on the last set of variables, we see small errors of scale 1e-8.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981444/c9561e0e-3707-11e7-8ce9-20ced60186e2.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981444/c9561e0e-3707-11e7-8ce9-20ced60186e2.png\" alt=\"3d\" style=\"max-width:100%;\"></a></p>\n<p>I proceeded to do a simpler example of simply trying to train a neural network to add.</p>\n<p>Inputs: 10x1 (numpy generated with same seed)<br>\nWeights: 10x1 (random normal initialized with same seed)<br>\nLabels: Sum of the 10x1 input.</p>\n<p>Effectively, the neural network is trying to tune the weights such that they all become 1.0.</p>\n<p>In fact, it's strange because the randomness has some form of determinism, as seen in the graph below.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981528/4ea32fca-3708-11e7-98b0-a6abcfd58347.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981528/4ea32fca-3708-11e7-98b0-a6abcfd58347.png\" alt=\"run1\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981532/516018a4-3708-11e7-8712-6b3824a664cf.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981532/516018a4-3708-11e7-8712-6b3824a664cf.png\" alt=\"run5\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981535/54733dc8-3708-11e7-9fe8-9911f64942c5.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981535/54733dc8-3708-11e7-9fe8-9911f64942c5.png\" alt=\"run7\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981537/596a5de8-3708-11e7-83fe-b10af531501b.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981537/596a5de8-3708-11e7-83fe-b10af531501b.png\" alt=\"run11\" style=\"max-width:100%;\"></a></p>\n<p>Furthermore, the anomalous gradients are consistently the same throughout the runs!</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981559/7d6c0926-3708-11e7-849d-bc30beb2591f.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981559/7d6c0926-3708-11e7-849d-bc30beb2591f.png\" alt=\"screenshot from 2017-05-12 11-45-15\" style=\"max-width:100%;\"></a></p>\n<p>And they have the exact same error.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981796/e71ee284-3709-11e7-91f2-e0777bd6f48b.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981796/e71ee284-3709-11e7-91f2-e0777bd6f48b.png\" alt=\"screenshot from 2017-05-12 11-54-42\" style=\"max-width:100%;\"></a></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> mentioned <a href=\"https://github.com/tensorflow/tensorflow/issues/5527\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5527/hovercard\">here</a> that mismatches between CUDA and Drivers could be the problem. So I upgraded my driver to 381, and then I got these results. 1 - 7 gradients have errors, but previously, only 5 had errors and when errors happened, these 5 gradients had exactly the same values.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6997460/25981589/b574d00a-3708-11e7-8fc9-22e0575ec94b.png\"><img src=\"https://cloud.githubusercontent.com/assets/6997460/25981589/b574d00a-3708-11e7-8fc9-22e0575ec94b.png\" alt=\"11\" style=\"max-width:100%;\"></a></p>\n<p>I haven't looked deeply into the exact 7 gradients that have errors but a brief glance showed that they are the same as the 5 before and more.</p>\n<p>Could it be the GPU reduce order? Or could there be errors in the computation of gradients themselves?<br>\nIf anyone has any insight into what experiments to try next, I'll be glad to do them and post the results here.</p>", "body_text": "BUILD\nTensorFlow GPU 1.1.0\nUbuntu 16.04\nCUDA 8.0.61\nCUDNN 5.1\nNVIDIA Drivers 375\n2 GTX 1080Ti, with 1 1080Ti used for 2 monitors. I set CUDA_VISIBLE_DEVICES=0 for all experiments. This uses the GPU with nothing attached.\n@zheng-xq why is the forward pass deterministic but the very first backward pass not deterministic?\nOn a simple MNIST example, the logits are 100% deterministic over 100 runs (I did more runs than that but plotted 100).\n\nHowever, when we look at the gradients computed on the last set of variables, we see small errors of scale 1e-8.\n\nI proceeded to do a simpler example of simply trying to train a neural network to add.\nInputs: 10x1 (numpy generated with same seed)\nWeights: 10x1 (random normal initialized with same seed)\nLabels: Sum of the 10x1 input.\nEffectively, the neural network is trying to tune the weights such that they all become 1.0.\nIn fact, it's strange because the randomness has some form of determinism, as seen in the graph below.\n\n\n\n\nFurthermore, the anomalous gradients are consistently the same throughout the runs!\n\nAnd they have the exact same error.\n\n@asimshankar mentioned here that mismatches between CUDA and Drivers could be the problem. So I upgraded my driver to 381, and then I got these results. 1 - 7 gradients have errors, but previously, only 5 had errors and when errors happened, these 5 gradients had exactly the same values.\n\nI haven't looked deeply into the exact 7 gradients that have errors but a brief glance showed that they are the same as the 5 before and more.\nCould it be the GPU reduce order? Or could there be errors in the computation of gradients themselves?\nIf anyone has any insight into what experiments to try next, I'll be glad to do them and post the results here.", "body": "BUILD\r\nTensorFlow GPU 1.1.0\r\nUbuntu 16.04\r\nCUDA 8.0.61\r\nCUDNN 5.1\r\nNVIDIA Drivers 375\r\n2 GTX 1080Ti, with 1 1080Ti used for 2 monitors. I set CUDA_VISIBLE_DEVICES=0 for all experiments. This uses the GPU with nothing attached.\r\n\r\n@zheng-xq why is the forward pass deterministic but the very first backward pass not deterministic?\r\n\r\nOn a simple MNIST example, the logits are 100% deterministic over 100 runs (I did more runs than that but plotted 100).\r\n\r\n![logits](https://cloud.githubusercontent.com/assets/6997460/25981433/a9473ee0-3707-11e7-8a60-240963c6c471.png)\r\n\r\nHowever, when we look at the gradients computed on the last set of variables, we see small errors of scale 1e-8.\r\n\r\n![3d](https://cloud.githubusercontent.com/assets/6997460/25981444/c9561e0e-3707-11e7-8ce9-20ced60186e2.png)\r\n\r\nI proceeded to do a simpler example of simply trying to train a neural network to add.\r\n\r\nInputs: 10x1 (numpy generated with same seed)\r\nWeights: 10x1 (random normal initialized with same seed)\r\nLabels: Sum of the 10x1 input.\r\n\r\nEffectively, the neural network is trying to tune the weights such that they all become 1.0.\r\n\r\nIn fact, it's strange because the randomness has some form of determinism, as seen in the graph below.\r\n\r\n![run1](https://cloud.githubusercontent.com/assets/6997460/25981528/4ea32fca-3708-11e7-98b0-a6abcfd58347.png)\r\n![run5](https://cloud.githubusercontent.com/assets/6997460/25981532/516018a4-3708-11e7-8712-6b3824a664cf.png)\r\n![run7](https://cloud.githubusercontent.com/assets/6997460/25981535/54733dc8-3708-11e7-9fe8-9911f64942c5.png)\r\n![run11](https://cloud.githubusercontent.com/assets/6997460/25981537/596a5de8-3708-11e7-83fe-b10af531501b.png)\r\n\r\nFurthermore, the anomalous gradients are consistently the same throughout the runs!\r\n\r\n![screenshot from 2017-05-12 11-45-15](https://cloud.githubusercontent.com/assets/6997460/25981559/7d6c0926-3708-11e7-849d-bc30beb2591f.png)\r\n\r\nAnd they have the exact same error.\r\n\r\n![screenshot from 2017-05-12 11-54-42](https://cloud.githubusercontent.com/assets/6997460/25981796/e71ee284-3709-11e7-91f2-e0777bd6f48b.png)\r\n\r\n@asimshankar mentioned [here](https://github.com/tensorflow/tensorflow/issues/5527) that mismatches between CUDA and Drivers could be the problem. So I upgraded my driver to 381, and then I got these results. 1 - 7 gradients have errors, but previously, only 5 had errors and when errors happened, these 5 gradients had exactly the same values.\r\n\r\n![11](https://cloud.githubusercontent.com/assets/6997460/25981589/b574d00a-3708-11e7-8fc9-22e0575ec94b.png)\r\n\r\nI haven't looked deeply into the exact 7 gradients that have errors but a brief glance showed that they are the same as the 5 before and more.\r\n\r\nCould it be the GPU reduce order? Or could there be errors in the computation of gradients themselves?\r\n If anyone has any insight into what experiments to try next, I'll be glad to do them and post the results here. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}