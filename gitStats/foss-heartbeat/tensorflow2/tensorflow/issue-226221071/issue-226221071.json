{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9655", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9655/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9655/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9655/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9655", "id": 226221071, "node_id": "MDU6SXNzdWUyMjYyMjEwNzE=", "number": 9655, "title": "why can't i recover cifar-100/cifar-10 image from binary files using tensorflow?", "user": {"login": "qiushi223", "id": 11583292, "node_id": "MDQ6VXNlcjExNTgzMjky", "avatar_url": "https://avatars1.githubusercontent.com/u/11583292?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qiushi223", "html_url": "https://github.com/qiushi223", "followers_url": "https://api.github.com/users/qiushi223/followers", "following_url": "https://api.github.com/users/qiushi223/following{/other_user}", "gists_url": "https://api.github.com/users/qiushi223/gists{/gist_id}", "starred_url": "https://api.github.com/users/qiushi223/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qiushi223/subscriptions", "organizations_url": "https://api.github.com/users/qiushi223/orgs", "repos_url": "https://api.github.com/users/qiushi223/repos", "events_url": "https://api.github.com/users/qiushi223/events{/privacy}", "received_events_url": "https://api.github.com/users/qiushi223/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-04T09:18:50Z", "updated_at": "2017-05-04T15:02:09Z", "closed_at": "2017-05-04T15:02:09Z", "author_association": "NONE", "body_html": "<p><strong>my code likes following:</strong></p>\n<p>////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////<br>\nimport tensorflow as tf<br>\nfrom matplotlib import pyplot as plt</p>\n<p>def read_cifar_100_bin(batch_size):</p>\n<pre><code>coarse_label_bytes = 1\nfinal_lable_bytes = 1\nimage_bytes = 3072\nfilenames = [\"./cifar-100-binary/train.bin\"]\nfilename_queue = tf.train.string_input_producer(filenames, shuffle=False)\nreader = tf.FixedLengthRecordReader(record_bytes=3074)\nkey,value = reader.read(filename_queue)\nrecord_bytes = tf.decode_raw(value, tf.uint8)\ncoarse_label = tf.cast(tf.slice(record_bytes, [0], [coarse_label_bytes]), tf.uint8)   \nfinal_label = tf.cast(tf.slice(record_bytes, [1], [final_lable_bytes]), tf.uint8)  #final_label:0~99\nexample = tf.reshape(tf.slice(record_bytes, [2], [image_bytes]),[3,32,32])\nexample_tr = tf.transpose(example, [1, 2, 0])\nexample_batch, coarse_label_batch,final_label_batch = tf.train.batch([example_tr, coarse_label,final_label],` batch_size=batch_size)\n\n# example_batch, coarse_label_batch,final_label_batch  = tf.train.shuffle_batch(\n#     [example, coarse_label,final_label],\n#     batch_size=batch_size,\n#     num_threads=4,\n#     capacity=50000,\n#     min_after_dequeue=10000)\n\nwith tf.Session() as sess:\n    coord = tf.train.Coordinator()  \n    threads = tf.train.start_queue_runners(coord=coord)\n    #sess.run(tf.global_variables_initializer())\n\n    images_batch = example_batch.eval()\n    coarse_labels_batch = coarse_label_batch.eval()\n    final_labels_batch = final_label_batch.eval()\n    plt.imshow(images_batch[0])\n    plt.show()\n\n    coord.request_stop()\n    coord.join(threads)\n\nreturn images_batch,coarse_labels_batch,final_labels_batch\n</code></pre>\n<p>images, coarse_labels,final_labels = read_cifar_100_bin(5)<br>\nprint(images[0])<br>\n/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////<br>\n<strong>my problem is: when i use plt to recover image from images_batch[0],it show like this:</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/11583292/25697490/ba6a2f2c-30ed-11e7-9af5-636b11607d76.png\"><img src=\"https://cloud.githubusercontent.com/assets/11583292/25697490/ba6a2f2c-30ed-11e7-9af5-636b11607d76.png\" alt=\"figure_1 copy\" style=\"max-width:100%;\"></a></p>\n<p><strong>Anyone knows why? Thanks a lot</strong></p>\n<p><strong>Another question:</strong><br>\nI want to choose random mini-batch data from whole dataset so i use tf.train.shuffle_batch() function you can see above,i wonder if it's a good way to realize it w.r.t speed and efficiency.</p>", "body_text": "my code likes following:\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\ndef read_cifar_100_bin(batch_size):\ncoarse_label_bytes = 1\nfinal_lable_bytes = 1\nimage_bytes = 3072\nfilenames = [\"./cifar-100-binary/train.bin\"]\nfilename_queue = tf.train.string_input_producer(filenames, shuffle=False)\nreader = tf.FixedLengthRecordReader(record_bytes=3074)\nkey,value = reader.read(filename_queue)\nrecord_bytes = tf.decode_raw(value, tf.uint8)\ncoarse_label = tf.cast(tf.slice(record_bytes, [0], [coarse_label_bytes]), tf.uint8)   \nfinal_label = tf.cast(tf.slice(record_bytes, [1], [final_lable_bytes]), tf.uint8)  #final_label:0~99\nexample = tf.reshape(tf.slice(record_bytes, [2], [image_bytes]),[3,32,32])\nexample_tr = tf.transpose(example, [1, 2, 0])\nexample_batch, coarse_label_batch,final_label_batch = tf.train.batch([example_tr, coarse_label,final_label],` batch_size=batch_size)\n\n# example_batch, coarse_label_batch,final_label_batch  = tf.train.shuffle_batch(\n#     [example, coarse_label,final_label],\n#     batch_size=batch_size,\n#     num_threads=4,\n#     capacity=50000,\n#     min_after_dequeue=10000)\n\nwith tf.Session() as sess:\n    coord = tf.train.Coordinator()  \n    threads = tf.train.start_queue_runners(coord=coord)\n    #sess.run(tf.global_variables_initializer())\n\n    images_batch = example_batch.eval()\n    coarse_labels_batch = coarse_label_batch.eval()\n    final_labels_batch = final_label_batch.eval()\n    plt.imshow(images_batch[0])\n    plt.show()\n\n    coord.request_stop()\n    coord.join(threads)\n\nreturn images_batch,coarse_labels_batch,final_labels_batch\n\nimages, coarse_labels,final_labels = read_cifar_100_bin(5)\nprint(images[0])\n/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\nmy problem is: when i use plt to recover image from images_batch[0],it show like this:\n\nAnyone knows why? Thanks a lot\nAnother question:\nI want to choose random mini-batch data from whole dataset so i use tf.train.shuffle_batch() function you can see above,i wonder if it's a good way to realize it w.r.t speed and efficiency.", "body": "**my code likes following:**\r\n\r\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\r\nimport tensorflow as tf\r\nfrom matplotlib import pyplot as plt\r\n\r\ndef read_cifar_100_bin(batch_size):\r\n\r\n    coarse_label_bytes = 1\r\n    final_lable_bytes = 1\r\n    image_bytes = 3072\r\n    filenames = [\"./cifar-100-binary/train.bin\"]\r\n    filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\r\n    reader = tf.FixedLengthRecordReader(record_bytes=3074)\r\n    key,value = reader.read(filename_queue)\r\n    record_bytes = tf.decode_raw(value, tf.uint8)\r\n    coarse_label = tf.cast(tf.slice(record_bytes, [0], [coarse_label_bytes]), tf.uint8)   \r\n    final_label = tf.cast(tf.slice(record_bytes, [1], [final_lable_bytes]), tf.uint8)  #final_label:0~99\r\n    example = tf.reshape(tf.slice(record_bytes, [2], [image_bytes]),[3,32,32])\r\n    example_tr = tf.transpose(example, [1, 2, 0])\r\n    example_batch, coarse_label_batch,final_label_batch = tf.train.batch([example_tr, coarse_label,final_label],` batch_size=batch_size)\r\n\r\n    # example_batch, coarse_label_batch,final_label_batch  = tf.train.shuffle_batch(\r\n    #     [example, coarse_label,final_label],\r\n    #     batch_size=batch_size,\r\n    #     num_threads=4,\r\n    #     capacity=50000,\r\n    #     min_after_dequeue=10000)\r\n\r\n    with tf.Session() as sess:\r\n        coord = tf.train.Coordinator()  \r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        #sess.run(tf.global_variables_initializer())\r\n\r\n        images_batch = example_batch.eval()\r\n        coarse_labels_batch = coarse_label_batch.eval()\r\n        final_labels_batch = final_label_batch.eval()\r\n        plt.imshow(images_batch[0])\r\n        plt.show()\r\n\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n\r\n    return images_batch,coarse_labels_batch,final_labels_batch\r\nimages, coarse_labels,final_labels = read_cifar_100_bin(5)\r\nprint(images[0])\r\n/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\r\n**my problem is: when i use plt to recover image from images_batch[0],it show like this:**\r\n![figure_1 copy](https://cloud.githubusercontent.com/assets/11583292/25697490/ba6a2f2c-30ed-11e7-9af5-636b11607d76.png)\r\n\r\n\r\n**Anyone knows why? Thanks a lot**\r\n\r\n**Another question:**\r\n  I want to choose random mini-batch data from whole dataset so i use tf.train.shuffle_batch() function you can see above,i wonder if it's a good way to realize it w.r.t speed and efficiency."}