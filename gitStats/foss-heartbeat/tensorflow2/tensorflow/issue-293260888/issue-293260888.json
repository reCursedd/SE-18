{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16628", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16628/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16628/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16628/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16628", "id": 293260888, "node_id": "MDU6SXNzdWUyOTMyNjA4ODg=", "number": 16628, "title": "Tensorflow switches to CPU when using Variable.assign", "user": {"login": "Spenhouet", "id": 7819068, "node_id": "MDQ6VXNlcjc4MTkwNjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7819068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Spenhouet", "html_url": "https://github.com/Spenhouet", "followers_url": "https://api.github.com/users/Spenhouet/followers", "following_url": "https://api.github.com/users/Spenhouet/following{/other_user}", "gists_url": "https://api.github.com/users/Spenhouet/gists{/gist_id}", "starred_url": "https://api.github.com/users/Spenhouet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Spenhouet/subscriptions", "organizations_url": "https://api.github.com/users/Spenhouet/orgs", "repos_url": "https://api.github.com/users/Spenhouet/repos", "events_url": "https://api.github.com/users/Spenhouet/events{/privacy}", "received_events_url": "https://api.github.com/users/Spenhouet/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-31T18:17:24Z", "updated_at": "2018-01-31T19:20:50Z", "closed_at": "2018-01-31T19:20:50Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS</strong>:Windows 10</li>\n<li><strong>TensorFlow installed from</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.0</li>\n<li><strong>Python version</strong>: 3.6.4</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0 / 64</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080 8 GB</li>\n<li><strong>Exact command to reproduce</strong>: run the provided code below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm using a <code>tf.Variable</code> for the learning rate of a optimizer. If I change its value with <code>sess.run(var.assign(0.1))</code> the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).</p>\n<h3>Source code / logs</h3>\n<p>I did write a minimal working example (training a network with XOR). <strong>To see the difference just comment the line <code>sess.run(learning_rate.assign(0.1))</code> out</strong> and it will run much much faster using the GPU.</p>\n<pre><code>import tensorflow as tf\n\n\ndef XOR(x_, y_):\n    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=\"Theta1\")\n    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=\"Theta2\")\n\n    Bias1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n    Bias2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n\n    with tf.name_scope(\"layer2\"):\n        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)\n\n    with tf.name_scope(\"layer3\"):\n        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)\n\n    with tf.name_scope(\"cost\"):\n        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +\n                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)\n\n    return cost\n\n\nif __name__ == \"__main__\":\n    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')\n    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')\n    xor_cost = XOR(x_, y_)\n\n    learning_rate = tf.Variable(0.1, dtype=tf.float32)\n\n    with tf.name_scope(\"train\"):\n        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)\n\n    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n    XOR_Y = [[0], [1], [1], [0]]\n\n    sess = tf.Session()\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    for i in range(40001):\n        sess.run(learning_rate.assign(0.1))\n\n        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})\n\n        if i % 100 == 0:\n            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))\n\n\n</code></pre>", "body_text": "System information\n\nOS:Windows 10\nTensorFlow installed from: binary\nTensorFlow version (use command below): 1.4.0\nPython version: 3.6.4\nCUDA/cuDNN version: 8.0 / 64\nGPU model and memory: GeForce GTX 1080 8 GB\nExact command to reproduce: run the provided code below\n\nDescribe the problem\nI'm using a tf.Variable for the learning rate of a optimizer. If I change its value with sess.run(var.assign(0.1)) the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).\nSource code / logs\nI did write a minimal working example (training a network with XOR). To see the difference just comment the line sess.run(learning_rate.assign(0.1)) out and it will run much much faster using the GPU.\nimport tensorflow as tf\n\n\ndef XOR(x_, y_):\n    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=\"Theta1\")\n    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=\"Theta2\")\n\n    Bias1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n    Bias2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n\n    with tf.name_scope(\"layer2\"):\n        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)\n\n    with tf.name_scope(\"layer3\"):\n        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)\n\n    with tf.name_scope(\"cost\"):\n        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +\n                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)\n\n    return cost\n\n\nif __name__ == \"__main__\":\n    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')\n    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')\n    xor_cost = XOR(x_, y_)\n\n    learning_rate = tf.Variable(0.1, dtype=tf.float32)\n\n    with tf.name_scope(\"train\"):\n        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)\n\n    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n    XOR_Y = [[0], [1], [1], [0]]\n\n    sess = tf.Session()\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    for i in range(40001):\n        sess.run(learning_rate.assign(0.1))\n\n        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})\n\n        if i % 100 == 0:\n            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))", "body": "### System information\r\n- **OS**:Windows 10\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6.4\r\n- **CUDA/cuDNN version**: 8.0 / 64\r\n- **GPU model and memory**: GeForce GTX 1080 8 GB\r\n- **Exact command to reproduce**: run the provided code below\r\n\r\n### Describe the problem\r\nI'm using a `tf.Variable` for the learning rate of a optimizer. If I change its value with `sess.run(var.assign(0.1))` the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).\r\n\r\n### Source code / logs\r\n\r\nI did write a minimal working example (training a network with XOR). **To see the difference just comment the line `sess.run(learning_rate.assign(0.1))` out** and it will run much much faster using the GPU.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef XOR(x_, y_):\r\n    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=\"Theta1\")\r\n    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=\"Theta2\")\r\n\r\n    Bias1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\r\n    Bias2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\r\n\r\n    with tf.name_scope(\"layer2\"):\r\n        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)\r\n\r\n    with tf.name_scope(\"layer3\"):\r\n        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)\r\n\r\n    with tf.name_scope(\"cost\"):\r\n        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +\r\n                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)\r\n\r\n    return cost\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')\r\n    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')\r\n    xor_cost = XOR(x_, y_)\r\n\r\n    learning_rate = tf.Variable(0.1, dtype=tf.float32)\r\n\r\n    with tf.name_scope(\"train\"):\r\n        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)\r\n\r\n    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]\r\n    XOR_Y = [[0], [1], [1], [0]]\r\n\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    for i in range(40001):\r\n        sess.run(learning_rate.assign(0.1))\r\n\r\n        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})\r\n\r\n        if i % 100 == 0:\r\n            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))\r\n\r\n\r\n```"}