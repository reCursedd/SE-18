{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6688", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6688/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6688/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6688/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6688", "id": 199153925, "node_id": "MDU6SXNzdWUxOTkxNTM5MjU=", "number": 6688, "title": "Outer product based Conv filters consume disproportionately high memory", "user": {"login": "sahiliitm", "id": 5723372, "node_id": "MDQ6VXNlcjU3MjMzNzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/5723372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sahiliitm", "html_url": "https://github.com/sahiliitm", "followers_url": "https://api.github.com/users/sahiliitm/followers", "following_url": "https://api.github.com/users/sahiliitm/following{/other_user}", "gists_url": "https://api.github.com/users/sahiliitm/gists{/gist_id}", "starred_url": "https://api.github.com/users/sahiliitm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sahiliitm/subscriptions", "organizations_url": "https://api.github.com/users/sahiliitm/orgs", "repos_url": "https://api.github.com/users/sahiliitm/repos", "events_url": "https://api.github.com/users/sahiliitm/events{/privacy}", "received_events_url": "https://api.github.com/users/sahiliitm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-01-06T09:11:29Z", "updated_at": "2017-01-07T02:45:06Z", "closed_at": "2017-01-06T19:40:57Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None</p>\n<h3>Environment info</h3>\n<p>Operating System:</p>\n<p>Ubuntu 14.04<br>\nInstalled version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<p>None, I'm using the CPU version of TF.<br>\nIf installed from binary pip package, provide:</p>\n<ol>\n<li>\n<p>A link to the pip package you installed:<br>\n<a href=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl</a></p>\n</li>\n<li>\n<p>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.<br>\n0.10.0rc0<br>\nIf installed from source, provide</p>\n</li>\n<li>\n<p>The commit hash (<code>git rev-parse HEAD</code>)</p>\n</li>\n<li>\n<p>The output of <code>bazel version</code></p>\n</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>The minimum reproducible example is in this <a href=\"https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232\">github gist</a>. The issue is of a disproportionately large memory usage.</p>\n<p>I modified the cifar10 example which ships with tensorflow to use outer-product of 3 vectors as the weights of the convolutional layers. This change can be seen in this <a href=\"https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232#file-cifar10-py-L91\">part of the code</a>.</p>\n<p>For simplicity, i have removed all parameter training operations and even loss computations. The current model only computes logits (forward pass) again and again.<br>\nThe unmodified code (can be setting the <code>use_outerp</code> flag to <code>False</code>) uses approximately 11.4 GB RAM<br>\nwhereas the modified code (with outer product of vectors used as the convolutional weight tensor) uses a disproportionately high 17 GB RAM.</p>\n<p>Any idea why this is the case?<br>\nMy intuition as to why this might happen is that maybe the outer product operations are being executed <em>every single time that the conv filter is needed</em>  instead of being executed exactly once in every forward pass. Is this really the case? Is there a way to fix this?</p>\n<p>Steps to reproduce:</p>\n<ol>\n<li>\n<p>To run the default version of the code (low memory footprint):<br>\npython train.py --use_outerp='False'</p>\n</li>\n<li>\n<p>To run the modified version of the code (high memory footprint):<br>\npython train.py --use_outerp='True'</p>\n</li>\n</ol>\n<h3>What other attempted solutions have you tried?</h3>\n<p>I'm not sure what the problem is, so haven't tried anything.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>", "body_text": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone\nEnvironment info\nOperating System:\nUbuntu 14.04\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nNone, I'm using the CPU version of TF.\nIf installed from binary pip package, provide:\n\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n0.10.0rc0\nIf installed from source, provide\n\n\nThe commit hash (git rev-parse HEAD)\n\n\nThe output of bazel version\n\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nThe minimum reproducible example is in this github gist. The issue is of a disproportionately large memory usage.\nI modified the cifar10 example which ships with tensorflow to use outer-product of 3 vectors as the weights of the convolutional layers. This change can be seen in this part of the code.\nFor simplicity, i have removed all parameter training operations and even loss computations. The current model only computes logits (forward pass) again and again.\nThe unmodified code (can be setting the use_outerp flag to False) uses approximately 11.4 GB RAM\nwhereas the modified code (with outer product of vectors used as the convolutional weight tensor) uses a disproportionately high 17 GB RAM.\nAny idea why this is the case?\nMy intuition as to why this might happen is that maybe the outer product operations are being executed every single time that the conv filter is needed  instead of being executed exactly once in every forward pass. Is this really the case? Is there a way to fix this?\nSteps to reproduce:\n\n\nTo run the default version of the code (low memory footprint):\npython train.py --use_outerp='False'\n\n\nTo run the modified version of the code (high memory footprint):\npython train.py --use_outerp='True'\n\n\nWhat other attempted solutions have you tried?\nI'm not sure what the problem is, so haven't tried anything.\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n### Environment info\r\nOperating System:\r\n\r\nUbuntu 14.04\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nNone, I'm using the CPU version of TF. \r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.10.0rc0\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThe minimum reproducible example is in this [github gist](https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232). The issue is of a disproportionately large memory usage. \r\n\r\nI modified the cifar10 example which ships with tensorflow to use outer-product of 3 vectors as the weights of the convolutional layers. This change can be seen in this [part of the code](https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232#file-cifar10-py-L91). \r\n\r\nFor simplicity, i have removed all parameter training operations and even loss computations. The current model only computes logits (forward pass) again and again. \r\nThe unmodified code (can be setting the `use_outerp` flag to `False`) uses approximately 11.4 GB RAM\r\nwhereas the modified code (with outer product of vectors used as the convolutional weight tensor) uses a disproportionately high 17 GB RAM. \r\n\r\nAny idea why this is the case? \r\nMy intuition as to why this might happen is that maybe the outer product operations are being executed _every single time that the conv filter is needed_  instead of being executed exactly once in every forward pass. Is this really the case? Is there a way to fix this? \r\n\r\nSteps to reproduce:\r\n1. To run the default version of the code (low memory footprint):\r\npython train.py --use_outerp='False'\r\n\r\n2. To run the modified version of the code (high memory footprint):\r\npython train.py --use_outerp='True'\r\n\r\n### What other attempted solutions have you tried?\r\nI'm not sure what the problem is, so haven't tried anything.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n"}