{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289314086", "html_url": "https://github.com/tensorflow/tensorflow/issues/8725#issuecomment-289314086", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8725", "id": 289314086, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTMxNDA4Ng==", "user": {"login": "mckinziebrandon", "id": 11165945, "node_id": "MDQ6VXNlcjExMTY1OTQ1", "avatar_url": "https://avatars2.githubusercontent.com/u/11165945?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mckinziebrandon", "html_url": "https://github.com/mckinziebrandon", "followers_url": "https://api.github.com/users/mckinziebrandon/followers", "following_url": "https://api.github.com/users/mckinziebrandon/following{/other_user}", "gists_url": "https://api.github.com/users/mckinziebrandon/gists{/gist_id}", "starred_url": "https://api.github.com/users/mckinziebrandon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mckinziebrandon/subscriptions", "organizations_url": "https://api.github.com/users/mckinziebrandon/orgs", "repos_url": "https://api.github.com/users/mckinziebrandon/repos", "events_url": "https://api.github.com/users/mckinziebrandon/events{/privacy}", "received_events_url": "https://api.github.com/users/mckinziebrandon/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-26T20:49:25Z", "updated_at": "2017-03-26T20:49:25Z", "author_association": "NONE", "body_html": "<p>This belongs on StackOverflow (it has actually been asked many times on SO, see links below). <strong>z</strong> is the log probability. That is a standard interpretation for the output of an affine transformation being fed to a softmax layer. For more, see:</p>\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with\" rel=\"nofollow\">SO: Difference between softmax and sofmtax with logits</a></li>\n<li><a href=\"http://stackoverflow.com/questions/40871797/tensorflow-softmax-cross-entropy-with-logits-asks-for-unscaled-log-probabilities\" rel=\"nofollow\">SO: tensorflow and unscaled log probabilities</a></li>\n</ul>", "body_text": "This belongs on StackOverflow (it has actually been asked many times on SO, see links below). z is the log probability. That is a standard interpretation for the output of an affine transformation being fed to a softmax layer. For more, see:\n\nSO: Difference between softmax and sofmtax with logits\nSO: tensorflow and unscaled log probabilities", "body": "This belongs on StackOverflow (it has actually been asked many times on SO, see links below). __z__ is the log probability. That is a standard interpretation for the output of an affine transformation being fed to a softmax layer. For more, see:\r\n\r\n* [SO: Difference between softmax and sofmtax with logits](http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with)\r\n* [SO: tensorflow and unscaled log probabilities](http://stackoverflow.com/questions/40871797/tensorflow-softmax-cross-entropy-with-logits-asks-for-unscaled-log-probabilities)"}