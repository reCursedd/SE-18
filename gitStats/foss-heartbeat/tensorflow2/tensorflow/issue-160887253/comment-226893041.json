{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226893041", "html_url": "https://github.com/tensorflow/tensorflow/issues/2927#issuecomment-226893041", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2927", "id": 226893041, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjg5MzA0MQ==", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-17T22:07:34Z", "updated_at": "2016-06-17T22:07:34Z", "author_association": "MEMBER", "body_html": "<p>Sorry you're hitting problems! We actually have a few different ways to reduce memory usage. One of them is to run tensorflow/contrib/quantization/tools/quantize_graph with --mode=weights to shrink the size of the model on disk by quantizing the weights to eight bits. I'm also working on folding in the batch normalization ops into the weights, which will reduce memory pressure.</p>\n<p>We also have memory-mapped constants, which you can try by running this script on your graph:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/convert_graphdef_memmapped_format.cc\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/convert_graphdef_memmapped_format.cc</a></p>\n<p>I haven't tried this myself yet on iOS, but with previous projects it was a great way to reduce memory problems since mapped files don't seem to count towards your overall usage, and are automatically swapped out when pressure is high.</p>", "body_text": "Sorry you're hitting problems! We actually have a few different ways to reduce memory usage. One of them is to run tensorflow/contrib/quantization/tools/quantize_graph with --mode=weights to shrink the size of the model on disk by quantizing the weights to eight bits. I'm also working on folding in the batch normalization ops into the weights, which will reduce memory pressure.\nWe also have memory-mapped constants, which you can try by running this script on your graph:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/convert_graphdef_memmapped_format.cc\nI haven't tried this myself yet on iOS, but with previous projects it was a great way to reduce memory problems since mapped files don't seem to count towards your overall usage, and are automatically swapped out when pressure is high.", "body": "Sorry you're hitting problems! We actually have a few different ways to reduce memory usage. One of them is to run tensorflow/contrib/quantization/tools/quantize_graph with --mode=weights to shrink the size of the model on disk by quantizing the weights to eight bits. I'm also working on folding in the batch normalization ops into the weights, which will reduce memory pressure.\n\nWe also have memory-mapped constants, which you can try by running this script on your graph:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/convert_graphdef_memmapped_format.cc\n\nI haven't tried this myself yet on iOS, but with previous projects it was a great way to reduce memory problems since mapped files don't seem to count towards your overall usage, and are automatically swapped out when pressure is high.\n"}