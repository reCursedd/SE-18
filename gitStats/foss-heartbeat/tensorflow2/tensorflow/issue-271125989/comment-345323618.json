{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345323618", "html_url": "https://github.com/tensorflow/tensorflow/issues/14232#issuecomment-345323618", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14232", "id": 345323618, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTMyMzYxOA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-17T18:25:27Z", "updated_at": "2017-11-17T18:27:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7183627\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/UWFrank\">@UWFrank</a> In a async SGD cluster there's typically two jobs, <code>job_name='ps'</code> and <code>job_name='worker'</code>. Your API suggests to use following to create coordinator:<br>\n<code>create_distributed_server(num_workers, job_name, task_index, master_host, is_master=False)</code></p>\n<p>Does this mean you would create two distributed servers, one for each job?</p>\n<p>For efficient async SGD, the number of live hosts in worker job should be allowed to change over time without affecting session creation, or session run calls of remaining live hosts. This allows training to proceed even if some hosts are unavailable. Currently this is achieved using sparse job config (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/8177edd7700ccbe0c831e680a1acb5275819d762/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/8177edd7700ccbe0c831e680a1acb5275819d762\"><tt>8177edd</tt></a>). The cluster propagation design seems to preclude this use-case.</p>\n<p>In this use case, here's what some ClusterSpecs look like for a cluster with 4 <code>ps</code> tasks and 2 <code>worker</code> tasks:</p>\n<pre><code>job_name=worker, task_id=1:\n{'worker': {1: '172.31.35.121:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\n\njob_name=worker, task_id=2\n{'worker': {2: '172.31.41.31:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\n\njob_name=ps, task_id=3:\n{'worker': ['172.31.40.171:3000', '172.31.35.121:3000', '172.31.41.31:3000', '172.31.36.117:3000', '172.31.38.4:3000'], 'ps': {3: '172.31.35.253:3000'}})\n</code></pre>\n<p>Each <code>tf.train.Server</code> instance (worker or ps) knows about itself. Worker servers need to know locations of all ps shards, but not of other workers. PS servers need to know about all workers but not other ps shards. As long as ps servers are only used as <a href=\"https://github.com/tensorflow/tensorflow/blob/9089ab59826ce656b6f8026b08471ca6696cacf9/tensorflow/core/protobuf/worker.proto\">workers</a>, and not as <a href=\"https://github.com/tensorflow/tensorflow/blob/2eac53d5ea540b0b09326ba9330b6051d742532d/tensorflow/core/protobuf/master.proto\">masters</a>, the number of live worker servers can change without affecting training. On other hand, training will stop if any ps server goes down.</p>", "body_text": "@UWFrank In a async SGD cluster there's typically two jobs, job_name='ps' and job_name='worker'. Your API suggests to use following to create coordinator:\ncreate_distributed_server(num_workers, job_name, task_index, master_host, is_master=False)\nDoes this mean you would create two distributed servers, one for each job?\nFor efficient async SGD, the number of live hosts in worker job should be allowed to change over time without affecting session creation, or session run calls of remaining live hosts. This allows training to proceed even if some hosts are unavailable. Currently this is achieved using sparse job config (8177edd). The cluster propagation design seems to preclude this use-case.\nIn this use case, here's what some ClusterSpecs look like for a cluster with 4 ps tasks and 2 worker tasks:\njob_name=worker, task_id=1:\n{'worker': {1: '172.31.35.121:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\n\njob_name=worker, task_id=2\n{'worker': {2: '172.31.41.31:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\n\njob_name=ps, task_id=3:\n{'worker': ['172.31.40.171:3000', '172.31.35.121:3000', '172.31.41.31:3000', '172.31.36.117:3000', '172.31.38.4:3000'], 'ps': {3: '172.31.35.253:3000'}})\n\nEach tf.train.Server instance (worker or ps) knows about itself. Worker servers need to know locations of all ps shards, but not of other workers. PS servers need to know about all workers but not other ps shards. As long as ps servers are only used as workers, and not as masters, the number of live worker servers can change without affecting training. On other hand, training will stop if any ps server goes down.", "body": "@UWFrank In a async SGD cluster there's typically two jobs, `job_name='ps'` and `job_name='worker'`. Your API suggests to use following to create coordinator:\r\n`create_distributed_server(num_workers, job_name, task_index, master_host, is_master=False)`\r\n\r\nDoes this mean you would create two distributed servers, one for each job?\r\n\r\nFor efficient async SGD, the number of live hosts in worker job should be allowed to change over time without affecting session creation, or session run calls of remaining live hosts. This allows training to proceed even if some hosts are unavailable. Currently this is achieved using sparse job config (https://github.com/tensorflow/tensorflow/commit/8177edd7700ccbe0c831e680a1acb5275819d762). The cluster propagation design seems to preclude this use-case.\r\n\r\nIn this use case, here's what some ClusterSpecs look like for a cluster with 4 `ps` tasks and 2 `worker` tasks:\r\n\r\n```\r\njob_name=worker, task_id=1:\r\n{'worker': {1: '172.31.35.121:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\r\n\r\njob_name=worker, task_id=2\r\n{'worker': {2: '172.31.41.31:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\r\n\r\njob_name=ps, task_id=3:\r\n{'worker': ['172.31.40.171:3000', '172.31.35.121:3000', '172.31.41.31:3000', '172.31.36.117:3000', '172.31.38.4:3000'], 'ps': {3: '172.31.35.253:3000'}})\r\n```\r\n\r\nEach `tf.train.Server` instance (worker or ps) knows about itself. Worker servers need to know locations of all ps shards, but not of other workers. PS servers need to know about all workers but not other ps shards. As long as ps servers are only used as [workers](https://github.com/tensorflow/tensorflow/blob/9089ab59826ce656b6f8026b08471ca6696cacf9/tensorflow/core/protobuf/worker.proto), and not as [masters](https://github.com/tensorflow/tensorflow/blob/2eac53d5ea540b0b09326ba9330b6051d742532d/tensorflow/core/protobuf/master.proto), the number of live worker servers can change without affecting training. On other hand, training will stop if any ps server goes down."}