{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14232", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14232/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14232/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14232/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14232", "id": 271125989, "node_id": "MDU6SXNzdWUyNzExMjU5ODk=", "number": 14232, "title": "[Feature Request] Automatic ClusterSpec Propagation for multiple hosts", "user": {"login": "UWFrank", "id": 7183627, "node_id": "MDQ6VXNlcjcxODM2Mjc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7183627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/UWFrank", "html_url": "https://github.com/UWFrank", "followers_url": "https://api.github.com/users/UWFrank/followers", "following_url": "https://api.github.com/users/UWFrank/following{/other_user}", "gists_url": "https://api.github.com/users/UWFrank/gists{/gist_id}", "starred_url": "https://api.github.com/users/UWFrank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/UWFrank/subscriptions", "organizations_url": "https://api.github.com/users/UWFrank/orgs", "repos_url": "https://api.github.com/users/UWFrank/repos", "events_url": "https://api.github.com/users/UWFrank/events{/privacy}", "received_events_url": "https://api.github.com/users/UWFrank/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "frankchn", "id": 691628, "node_id": "MDQ6VXNlcjY5MTYyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/691628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frankchn", "html_url": "https://github.com/frankchn", "followers_url": "https://api.github.com/users/frankchn/followers", "following_url": "https://api.github.com/users/frankchn/following{/other_user}", "gists_url": "https://api.github.com/users/frankchn/gists{/gist_id}", "starred_url": "https://api.github.com/users/frankchn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frankchn/subscriptions", "organizations_url": "https://api.github.com/users/frankchn/orgs", "repos_url": "https://api.github.com/users/frankchn/repos", "events_url": "https://api.github.com/users/frankchn/events{/privacy}", "received_events_url": "https://api.github.com/users/frankchn/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "frankchn", "id": 691628, "node_id": "MDQ6VXNlcjY5MTYyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/691628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frankchn", "html_url": "https://github.com/frankchn", "followers_url": "https://api.github.com/users/frankchn/followers", "following_url": "https://api.github.com/users/frankchn/following{/other_user}", "gists_url": "https://api.github.com/users/frankchn/gists{/gist_id}", "starred_url": "https://api.github.com/users/frankchn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frankchn/subscriptions", "organizations_url": "https://api.github.com/users/frankchn/orgs", "repos_url": "https://api.github.com/users/frankchn/repos", "events_url": "https://api.github.com/users/frankchn/events{/privacy}", "received_events_url": "https://api.github.com/users/frankchn/received_events", "type": "User", "site_admin": false}, {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2017-11-03T21:39:32Z", "updated_at": "2018-09-05T18:48:57Z", "closed_at": "2018-09-05T18:48:13Z", "author_association": "NONE", "body_html": "<p>Currently, when launching a distributed TensorFlow job, user need to manually input all the worker hosts' IP and port number. This is not too convenient and does not scale well. It would be really nice to have the native TensorFlow functionality that workers can automatically register themselves on master service without knowing all the host IP and port beforehand. Not sure if there is existing solution to solve this problem. But I used some of the building blocks (ClusterSpec Propagation and ClusterResolver) to enable this feature on my client code. Please let me know if it is a good approach to do this, I'd love to contribute if there's interest in this functionality.</p>\n<p>The solution I have involve the following steps:</p>\n<ol>\n<li>Master service start server with user specified port and wait for all workers to register</li>\n<li>Worker register themselves by sending ClusterSpec to Master service and wait for response</li>\n<li>Master service waits until numbers of workers registered matches requested worker number</li>\n<li>Master service merges ClusterSpecs and propagates to all registered workers</li>\n<li>Master service and workers continues</li>\n</ol>", "body_text": "Currently, when launching a distributed TensorFlow job, user need to manually input all the worker hosts' IP and port number. This is not too convenient and does not scale well. It would be really nice to have the native TensorFlow functionality that workers can automatically register themselves on master service without knowing all the host IP and port beforehand. Not sure if there is existing solution to solve this problem. But I used some of the building blocks (ClusterSpec Propagation and ClusterResolver) to enable this feature on my client code. Please let me know if it is a good approach to do this, I'd love to contribute if there's interest in this functionality.\nThe solution I have involve the following steps:\n\nMaster service start server with user specified port and wait for all workers to register\nWorker register themselves by sending ClusterSpec to Master service and wait for response\nMaster service waits until numbers of workers registered matches requested worker number\nMaster service merges ClusterSpecs and propagates to all registered workers\nMaster service and workers continues", "body": "Currently, when launching a distributed TensorFlow job, user need to manually input all the worker hosts' IP and port number. This is not too convenient and does not scale well. It would be really nice to have the native TensorFlow functionality that workers can automatically register themselves on master service without knowing all the host IP and port beforehand. Not sure if there is existing solution to solve this problem. But I used some of the building blocks (ClusterSpec Propagation and ClusterResolver) to enable this feature on my client code. Please let me know if it is a good approach to do this, I'd love to contribute if there's interest in this functionality. \r\n\r\nThe solution I have involve the following steps:\r\n1) Master service start server with user specified port and wait for all workers to register\r\n2) Worker register themselves by sending ClusterSpec to Master service and wait for response\r\n3) Master service waits until numbers of workers registered matches requested worker number\r\n4) Master service merges ClusterSpecs and propagates to all registered workers\r\n5) Master service and workers continues"}