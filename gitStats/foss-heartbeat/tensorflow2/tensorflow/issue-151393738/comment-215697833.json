{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215697833", "html_url": "https://github.com/tensorflow/tensorflow/issues/2130#issuecomment-215697833", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2130", "id": 215697833, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTY5NzgzMw==", "user": {"login": "ivankreso", "id": 2056432, "node_id": "MDQ6VXNlcjIwNTY0MzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2056432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ivankreso", "html_url": "https://github.com/ivankreso", "followers_url": "https://api.github.com/users/ivankreso/followers", "following_url": "https://api.github.com/users/ivankreso/following{/other_user}", "gists_url": "https://api.github.com/users/ivankreso/gists{/gist_id}", "starred_url": "https://api.github.com/users/ivankreso/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ivankreso/subscriptions", "organizations_url": "https://api.github.com/users/ivankreso/orgs", "repos_url": "https://api.github.com/users/ivankreso/repos", "events_url": "https://api.github.com/users/ivankreso/events{/privacy}", "received_events_url": "https://api.github.com/users/ivankreso/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-29T12:25:43Z", "updated_at": "2016-04-29T12:25:43Z", "author_association": "NONE", "body_html": "<p>Documentation is great. It was a wrong impression on my side regarding code examples. There are examples on how to train and evaluate on single GPU like:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py</a></p>\n<p>My mistake was that I was using an older version of Slim wrapper which didn't have support for variable reuse which lead me to completely forget about reuse mechanism. This was a problem because some layers behave differently in inference mode (like batch normalization) and there was no option to define two versions of the same network using different is_training flag in older Slim. In the end I wrote a code which saves, rebuilds and restores a graph from scratch each epoch which is still a good workaround as the time to do this is negligible compared to all the forward and backward passes. Now I found out that Slim supports variable reuse and I just tested it and it works.</p>\n<p>Thanks for help!</p>", "body_text": "Documentation is great. It was a wrong impression on my side regarding code examples. There are examples on how to train and evaluate on single GPU like:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py\nMy mistake was that I was using an older version of Slim wrapper which didn't have support for variable reuse which lead me to completely forget about reuse mechanism. This was a problem because some layers behave differently in inference mode (like batch normalization) and there was no option to define two versions of the same network using different is_training flag in older Slim. In the end I wrote a code which saves, rebuilds and restores a graph from scratch each epoch which is still a good workaround as the time to do this is negligible compared to all the forward and backward passes. Now I found out that Slim supports variable reuse and I just tested it and it works.\nThanks for help!", "body": "Documentation is great. It was a wrong impression on my side regarding code examples. There are examples on how to train and evaluate on single GPU like:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py\n\nMy mistake was that I was using an older version of Slim wrapper which didn't have support for variable reuse which lead me to completely forget about reuse mechanism. This was a problem because some layers behave differently in inference mode (like batch normalization) and there was no option to define two versions of the same network using different is_training flag in older Slim. In the end I wrote a code which saves, rebuilds and restores a graph from scratch each epoch which is still a good workaround as the time to do this is negligible compared to all the forward and backward passes. Now I found out that Slim supports variable reuse and I just tested it and it works.\n\nThanks for help!\n"}