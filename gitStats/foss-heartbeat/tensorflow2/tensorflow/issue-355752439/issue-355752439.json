{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21980", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21980/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21980/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21980/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21980", "id": 355752439, "node_id": "MDU6SXNzdWUzNTU3NTI0Mzk=", "number": 21980, "title": "Variable in a while loop - Segmentation fault (core dumped) ", "user": {"login": "mpekalski", "id": 2975068, "node_id": "MDQ6VXNlcjI5NzUwNjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2975068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpekalski", "html_url": "https://github.com/mpekalski", "followers_url": "https://api.github.com/users/mpekalski/followers", "following_url": "https://api.github.com/users/mpekalski/following{/other_user}", "gists_url": "https://api.github.com/users/mpekalski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpekalski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpekalski/subscriptions", "organizations_url": "https://api.github.com/users/mpekalski/orgs", "repos_url": "https://api.github.com/users/mpekalski/repos", "events_url": "https://api.github.com/users/mpekalski/events{/privacy}", "received_events_url": "https://api.github.com/users/mpekalski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-08-30T21:01:04Z", "updated_at": "2018-09-01T21:27:25Z", "closed_at": "2018-09-01T21:27:25Z", "author_association": "NONE", "body_html": "<p>I use TF 1.10.0, build from master last week.</p>\n<p>I am trying to make a function to erase patches in images and wanted to apply it with map to a Dataset. I use scatter_nd_update, so I had to define a variable that would be updated outside of the loop and assign tensor to it within the loop, but then I get segmentation fault.</p>\n<p>Log and the code</p>\n<pre><code>Using TensorFlow backend.\n2018-08-30 21:36:51.556778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-08-30 21:36:51.557216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 10.03GiB\n2018-08-30 21:36:51.557236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-08-30 21:36:51.755752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-30 21:36:51.755792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-08-30 21:36:51.755799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-08-30 21:36:51.755996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-30 21:36:51.840570: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nUsing device /gpu:0, and data format channels_last.\nStarting\nTensor(\"learning_rate_decay:0\", shape=(), dtype=float32)\nWARNING:tensorflow:case: An unordered dictionary of predicate/fn pairs was provided, but exclusive=False. The order of conditional tests is deterministic but not guaranteed.\n(300, 28, 28, 1)\n2018-08-30 21:36:52.518523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-08-30 21:36:52.518567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-30 21:36:52.518576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-08-30 21:36:52.518584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-08-30 21:36:52.518686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-30 21:36:52.518876: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nSegmentation fault (core dumped)\n</code></pre>\n<pre><code>def random_erase(img_z, img, label, width=28, height=28):\n        #with g.as_default():\n        a = tf.random_uniform(minval=0,   maxval=width-1, shape=[], dtype=tf.int32)\n        b = tf.random_uniform(minval=a+1, maxval=width,   shape=[], dtype=tf.int32)\n\n        c = tf.random_uniform(minval=0,   maxval=height-1, shape=[], dtype=tf.int32)\n        d = tf.random_uniform(minval=c+1, maxval=height,   shape=[], dtype=tf.int32)\n\n        h = tf.range(start=a, limit=b, dtype=tf.int32)\n        v = tf.range(start=c, limit=d, dtype=tf.int32)\n\n        h, v = h[ None, :, None ], v[ :, None, None ]\n        cartesian_product = tf.concat( [ h + tf.zeros_like( v ),\n                                        tf.zeros_like( h ) + v ], axis = 2, name=\"caretsian_product\")\n        \n        random_patch = tf.random_uniform(minval=0, maxval=255, shape=[d-c,b-a], dtype=tf.float32, name=\"random_patch\")\n        return tf.scatter_nd_update(tf.assign(img_z, img), cartesian_product, random_patch, name=\"image_with_patch\"), label\n\n</code></pre>\n<p>and I use it in</p>\n<pre><code>    img_z = tf.Variable(tf.zeros(shape=[28*28], dtype=tf.float32),\n                            trainable=False, use_resource=True)\n    ds = mnist_train('.')\n    ds = ds.map(lambda img, label:random_erase(img_z, img, label))\n</code></pre>", "body_text": "I use TF 1.10.0, build from master last week.\nI am trying to make a function to erase patches in images and wanted to apply it with map to a Dataset. I use scatter_nd_update, so I had to define a variable that would be updated outside of the loop and assign tensor to it within the loop, but then I get segmentation fault.\nLog and the code\nUsing TensorFlow backend.\n2018-08-30 21:36:51.556778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-08-30 21:36:51.557216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 10.03GiB\n2018-08-30 21:36:51.557236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-08-30 21:36:51.755752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-30 21:36:51.755792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-08-30 21:36:51.755799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-08-30 21:36:51.755996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-30 21:36:51.840570: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nUsing device /gpu:0, and data format channels_last.\nStarting\nTensor(\"learning_rate_decay:0\", shape=(), dtype=float32)\nWARNING:tensorflow:case: An unordered dictionary of predicate/fn pairs was provided, but exclusive=False. The order of conditional tests is deterministic but not guaranteed.\n(300, 28, 28, 1)\n2018-08-30 21:36:52.518523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-08-30 21:36:52.518567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-30 21:36:52.518576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-08-30 21:36:52.518584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-08-30 21:36:52.518686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-30 21:36:52.518876: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nSegmentation fault (core dumped)\n\ndef random_erase(img_z, img, label, width=28, height=28):\n        #with g.as_default():\n        a = tf.random_uniform(minval=0,   maxval=width-1, shape=[], dtype=tf.int32)\n        b = tf.random_uniform(minval=a+1, maxval=width,   shape=[], dtype=tf.int32)\n\n        c = tf.random_uniform(minval=0,   maxval=height-1, shape=[], dtype=tf.int32)\n        d = tf.random_uniform(minval=c+1, maxval=height,   shape=[], dtype=tf.int32)\n\n        h = tf.range(start=a, limit=b, dtype=tf.int32)\n        v = tf.range(start=c, limit=d, dtype=tf.int32)\n\n        h, v = h[ None, :, None ], v[ :, None, None ]\n        cartesian_product = tf.concat( [ h + tf.zeros_like( v ),\n                                        tf.zeros_like( h ) + v ], axis = 2, name=\"caretsian_product\")\n        \n        random_patch = tf.random_uniform(minval=0, maxval=255, shape=[d-c,b-a], dtype=tf.float32, name=\"random_patch\")\n        return tf.scatter_nd_update(tf.assign(img_z, img), cartesian_product, random_patch, name=\"image_with_patch\"), label\n\n\nand I use it in\n    img_z = tf.Variable(tf.zeros(shape=[28*28], dtype=tf.float32),\n                            trainable=False, use_resource=True)\n    ds = mnist_train('.')\n    ds = ds.map(lambda img, label:random_erase(img_z, img, label))", "body": "I use TF 1.10.0, build from master last week.\r\n\r\nI am trying to make a function to erase patches in images and wanted to apply it with map to a Dataset. I use scatter_nd_update, so I had to define a variable that would be updated outside of the loop and assign tensor to it within the loop, but then I get segmentation fault.\r\n\r\n\r\nLog and the code\r\n\r\n```\r\nUsing TensorFlow backend.\r\n2018-08-30 21:36:51.556778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-30 21:36:51.557216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.03GiB\r\n2018-08-30 21:36:51.557236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-30 21:36:51.755752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-30 21:36:51.755792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-08-30 21:36:51.755799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-08-30 21:36:51.755996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-30 21:36:51.840570: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nUsing device /gpu:0, and data format channels_last.\r\nStarting\r\nTensor(\"learning_rate_decay:0\", shape=(), dtype=float32)\r\nWARNING:tensorflow:case: An unordered dictionary of predicate/fn pairs was provided, but exclusive=False. The order of conditional tests is deterministic but not guaranteed.\r\n(300, 28, 28, 1)\r\n2018-08-30 21:36:52.518523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-08-30 21:36:52.518567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-30 21:36:52.518576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-08-30 21:36:52.518584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-08-30 21:36:52.518686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-30 21:36:52.518876: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n```\r\ndef random_erase(img_z, img, label, width=28, height=28):\r\n        #with g.as_default():\r\n        a = tf.random_uniform(minval=0,   maxval=width-1, shape=[], dtype=tf.int32)\r\n        b = tf.random_uniform(minval=a+1, maxval=width,   shape=[], dtype=tf.int32)\r\n\r\n        c = tf.random_uniform(minval=0,   maxval=height-1, shape=[], dtype=tf.int32)\r\n        d = tf.random_uniform(minval=c+1, maxval=height,   shape=[], dtype=tf.int32)\r\n\r\n        h = tf.range(start=a, limit=b, dtype=tf.int32)\r\n        v = tf.range(start=c, limit=d, dtype=tf.int32)\r\n\r\n        h, v = h[ None, :, None ], v[ :, None, None ]\r\n        cartesian_product = tf.concat( [ h + tf.zeros_like( v ),\r\n                                        tf.zeros_like( h ) + v ], axis = 2, name=\"caretsian_product\")\r\n        \r\n        random_patch = tf.random_uniform(minval=0, maxval=255, shape=[d-c,b-a], dtype=tf.float32, name=\"random_patch\")\r\n        return tf.scatter_nd_update(tf.assign(img_z, img), cartesian_product, random_patch, name=\"image_with_patch\"), label\r\n\r\n```\r\n\r\nand I use it in\r\n\r\n```\r\n    img_z = tf.Variable(tf.zeros(shape=[28*28], dtype=tf.float32),\r\n                            trainable=False, use_resource=True)\r\n    ds = mnist_train('.')\r\n    ds = ds.map(lambda img, label:random_erase(img_z, img, label))\r\n```"}