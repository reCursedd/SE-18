{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/432067321", "html_url": "https://github.com/tensorflow/tensorflow/issues/22755#issuecomment-432067321", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22755", "id": 432067321, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjA2NzMyMQ==", "user": {"login": "panfengli", "id": 18660165, "node_id": "MDQ6VXNlcjE4NjYwMTY1", "avatar_url": "https://avatars2.githubusercontent.com/u/18660165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panfengli", "html_url": "https://github.com/panfengli", "followers_url": "https://api.github.com/users/panfengli/followers", "following_url": "https://api.github.com/users/panfengli/following{/other_user}", "gists_url": "https://api.github.com/users/panfengli/gists{/gist_id}", "starred_url": "https://api.github.com/users/panfengli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panfengli/subscriptions", "organizations_url": "https://api.github.com/users/panfengli/orgs", "repos_url": "https://api.github.com/users/panfengli/repos", "events_url": "https://api.github.com/users/panfengli/events{/privacy}", "received_events_url": "https://api.github.com/users/panfengli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-23T02:43:32Z", "updated_at": "2018-10-23T02:43:59Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=18137551\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jianlong-yuan\">@jianlong-yuan</a> Thanks for your work! I am wondering whether it follows the similar way in cifar multi gpu example, especially when combining different towers' gradient results like:</p>\n<pre><code>def average_gradients(tower_grads):\n    \"\"\"Calculate the average gradient for each shared variable across all towers.\n      Note that this function provides a synchronization point across all towers.\n\n    Args:\n        tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n        List of pairs of (gradient, variable) where the gradient has been averaged\n        across all towers.\n    \"\"\"\n    average_grads = []\n    for grads_and_vars in zip(*tower_grads):\n        # Each grad_and_vars looks like the following:\n        # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = [g for g,_ in grads_and_vars]\n        if grads[0] is not None: # Gradient can be None\n            # Average the grads\n            grad = tf.reduce_mean(tf.stack(grads, 0), 0)\n            # The Variables are redundant because they are shared across towers\n            v = grads_and_vars[0][1]\n            grad_and_var = (grad, v)\n        else:\n            grad_and_var = grads_and_vars[0]\n\n        average_grads += [grad_and_var]\n    return average_grads\n</code></pre>", "body_text": "@jianlong-yuan Thanks for your work! I am wondering whether it follows the similar way in cifar multi gpu example, especially when combining different towers' gradient results like:\ndef average_gradients(tower_grads):\n    \"\"\"Calculate the average gradient for each shared variable across all towers.\n      Note that this function provides a synchronization point across all towers.\n\n    Args:\n        tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n        List of pairs of (gradient, variable) where the gradient has been averaged\n        across all towers.\n    \"\"\"\n    average_grads = []\n    for grads_and_vars in zip(*tower_grads):\n        # Each grad_and_vars looks like the following:\n        # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = [g for g,_ in grads_and_vars]\n        if grads[0] is not None: # Gradient can be None\n            # Average the grads\n            grad = tf.reduce_mean(tf.stack(grads, 0), 0)\n            # The Variables are redundant because they are shared across towers\n            v = grads_and_vars[0][1]\n            grad_and_var = (grad, v)\n        else:\n            grad_and_var = grads_and_vars[0]\n\n        average_grads += [grad_and_var]\n    return average_grads", "body": "@jianlong-yuan Thanks for your work! I am wondering whether it follows the similar way in cifar multi gpu example, especially when combining different towers' gradient results like:\r\n\r\n```\r\ndef average_gradients(tower_grads):\r\n    \"\"\"Calculate the average gradient for each shared variable across all towers.\r\n      Note that this function provides a synchronization point across all towers.\r\n\r\n    Args:\r\n        tower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n        is over individual gradients. The inner list is over the gradient\r\n        calculation for each tower.\r\n    Returns:\r\n        List of pairs of (gradient, variable) where the gradient has been averaged\r\n        across all towers.\r\n    \"\"\"\r\n    average_grads = []\r\n    for grads_and_vars in zip(*tower_grads):\r\n        # Each grad_and_vars looks like the following:\r\n        # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n        grads = [g for g,_ in grads_and_vars]\r\n        if grads[0] is not None: # Gradient can be None\r\n            # Average the grads\r\n            grad = tf.reduce_mean(tf.stack(grads, 0), 0)\r\n            # The Variables are redundant because they are shared across towers\r\n            v = grads_and_vars[0][1]\r\n            grad_and_var = (grad, v)\r\n        else:\r\n            grad_and_var = grads_and_vars[0]\r\n\r\n        average_grads += [grad_and_var]\r\n    return average_grads\r\n```"}