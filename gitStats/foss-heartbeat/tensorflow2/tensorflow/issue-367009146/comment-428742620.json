{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428742620", "html_url": "https://github.com/tensorflow/tensorflow/issues/22755#issuecomment-428742620", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22755", "id": 428742620, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODc0MjYyMA==", "user": {"login": "panfengli", "id": 18660165, "node_id": "MDQ6VXNlcjE4NjYwMTY1", "avatar_url": "https://avatars2.githubusercontent.com/u/18660165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panfengli", "html_url": "https://github.com/panfengli", "followers_url": "https://api.github.com/users/panfengli/followers", "following_url": "https://api.github.com/users/panfengli/following{/other_user}", "gists_url": "https://api.github.com/users/panfengli/gists{/gist_id}", "starred_url": "https://api.github.com/users/panfengli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panfengli/subscriptions", "organizations_url": "https://api.github.com/users/panfengli/orgs", "repos_url": "https://api.github.com/users/panfengli/repos", "events_url": "https://api.github.com/users/panfengli/events{/privacy}", "received_events_url": "https://api.github.com/users/panfengli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T21:40:39Z", "updated_at": "2018-10-10T21:53:20Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a> Yeah, I see you have implemented in tensorpack. But I wish sync bn could at least be provided in <code>tf.contrib</code>, since generally the original framework has better maintenance especially when new version comes out.</p>\n<p>Since Tensorflow 2.0 claims that it will support \"Use DistributionStrategy to utilize multiple GPUs and multiple TPU cores\", and <code>tf.contrib</code> will be deprecated, so that I am wondering whether sync bn will be provided in TF 2.0.</p>\n<p>I don't think it is a duplication, at least currently sync bn is not provided in Tensorflow. As you mentioned, there do exist a lot of working solutions, the one in pytorch, in tensorpack, but they are not in official Tensorflow or Pytorch package, once newer and newer version comes out, there might be some error due to update and the largest percentage of users do use the original Tensorflow framework and they need the sync bn feature be a Tensorflow official function.</p>\n<p>This kind of issue has been keeping coming out on Tensorflow feature request. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13829174\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/holyseven\">@holyseven</a> use list of inputs as a solution, <a href=\"https://github.com/tensorflow/tensorflow/issues/7439\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7439/hovercard\">Batch Normalization for Multi-GPU / Data Parallelism</a>. You also implement it in tensorpack as your solution. But they are not solutions to the majority of users. Tensorflow 2.0 seems to be a big change, and if the sync bn could not be provided in TF 2.0, a lot of Tensorflow users might need to wait for another year for this feature to come out.</p>", "body_text": "@ppwwyyxx Yeah, I see you have implemented in tensorpack. But I wish sync bn could at least be provided in tf.contrib, since generally the original framework has better maintenance especially when new version comes out.\nSince Tensorflow 2.0 claims that it will support \"Use DistributionStrategy to utilize multiple GPUs and multiple TPU cores\", and tf.contrib will be deprecated, so that I am wondering whether sync bn will be provided in TF 2.0.\nI don't think it is a duplication, at least currently sync bn is not provided in Tensorflow. As you mentioned, there do exist a lot of working solutions, the one in pytorch, in tensorpack, but they are not in official Tensorflow or Pytorch package, once newer and newer version comes out, there might be some error due to update and the largest percentage of users do use the original Tensorflow framework and they need the sync bn feature be a Tensorflow official function.\nThis kind of issue has been keeping coming out on Tensorflow feature request. @holyseven use list of inputs as a solution, Batch Normalization for Multi-GPU / Data Parallelism. You also implement it in tensorpack as your solution. But they are not solutions to the majority of users. Tensorflow 2.0 seems to be a big change, and if the sync bn could not be provided in TF 2.0, a lot of Tensorflow users might need to wait for another year for this feature to come out.", "body": "@ppwwyyxx Yeah, I see you have implemented in tensorpack. But I wish sync bn could at least be provided in `tf.contrib`, since generally the original framework has better maintenance especially when new version comes out.\r\n\r\nSince Tensorflow 2.0 claims that it will support \"Use DistributionStrategy to utilize multiple GPUs and multiple TPU cores\", and `tf.contrib` will be deprecated, so that I am wondering whether sync bn will be provided in TF 2.0.\r\n\r\nI don't think it is a duplication, at least currently sync bn is not provided in Tensorflow. As you mentioned, there do exist a lot of working solutions, the one in pytorch, in tensorpack, but they are not in official Tensorflow or Pytorch package, once newer and newer version comes out, there might be some error due to update and the largest percentage of users do use the original Tensorflow framework and they need the sync bn feature be a Tensorflow official function.\r\n\r\nThis kind of issue has been keeping coming out on Tensorflow feature request. @holyseven use list of inputs as a solution, [Batch Normalization for Multi-GPU / Data Parallelism](https://github.com/tensorflow/tensorflow/issues/7439). You also implement it in tensorpack as your solution. But they are not solutions to the majority of users. Tensorflow 2.0 seems to be a big change, and if the sync bn could not be provided in TF 2.0, a lot of Tensorflow users might need to wait for another year for this feature to come out. "}