{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23404", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23404/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23404/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23404/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23404", "id": 375975004, "node_id": "MDU6SXNzdWUzNzU5NzUwMDQ=", "number": 23404, "title": "TFLite Android: My lite Model file does not loaded", "user": {"login": "vholod", "id": 32450207, "node_id": "MDQ6VXNlcjMyNDUwMjA3", "avatar_url": "https://avatars1.githubusercontent.com/u/32450207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vholod", "html_url": "https://github.com/vholod", "followers_url": "https://api.github.com/users/vholod/followers", "following_url": "https://api.github.com/users/vholod/following{/other_user}", "gists_url": "https://api.github.com/users/vholod/gists{/gist_id}", "starred_url": "https://api.github.com/users/vholod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vholod/subscriptions", "organizations_url": "https://api.github.com/users/vholod/orgs", "repos_url": "https://api.github.com/users/vholod/repos", "events_url": "https://api.github.com/users/vholod/events{/privacy}", "received_events_url": "https://api.github.com/users/vholod/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "tofulawrence", "id": 24887415, "node_id": "MDQ6VXNlcjI0ODg3NDE1", "avatar_url": "https://avatars2.githubusercontent.com/u/24887415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tofulawrence", "html_url": "https://github.com/tofulawrence", "followers_url": "https://api.github.com/users/tofulawrence/followers", "following_url": "https://api.github.com/users/tofulawrence/following{/other_user}", "gists_url": "https://api.github.com/users/tofulawrence/gists{/gist_id}", "starred_url": "https://api.github.com/users/tofulawrence/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tofulawrence/subscriptions", "organizations_url": "https://api.github.com/users/tofulawrence/orgs", "repos_url": "https://api.github.com/users/tofulawrence/repos", "events_url": "https://api.github.com/users/tofulawrence/events{/privacy}", "received_events_url": "https://api.github.com/users/tofulawrence/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tofulawrence", "id": 24887415, "node_id": "MDQ6VXNlcjI0ODg3NDE1", "avatar_url": "https://avatars2.githubusercontent.com/u/24887415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tofulawrence", "html_url": "https://github.com/tofulawrence", "followers_url": "https://api.github.com/users/tofulawrence/followers", "following_url": "https://api.github.com/users/tofulawrence/following{/other_user}", "gists_url": "https://api.github.com/users/tofulawrence/gists{/gist_id}", "starred_url": "https://api.github.com/users/tofulawrence/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tofulawrence/subscriptions", "organizations_url": "https://api.github.com/users/tofulawrence/orgs", "repos_url": "https://api.github.com/users/tofulawrence/repos", "events_url": "https://api.github.com/users/tofulawrence/events{/privacy}", "received_events_url": "https://api.github.com/users/tofulawrence/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-31T13:56:55Z", "updated_at": "2018-11-15T19:00:32Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm having a problem with loading a lite model to the using the android tensorflow lite Interpreter.<br>\nI am trying to use the workflow of  Tensorflow-for-poets-2 TFLite tutorial, <a href=\"url\">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6</a></p>\n<p>instead of using the function:</p>\n<pre><code>private MappedByteBuffer loadModelFile(Activity activity,String MODEL_FILE) throws IOException {\n    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\n    FileChannel fileChannel = inputStream.getChannel();\n    long startOffset = fileDescriptor.getStartOffset();\n    long declaredLength = fileDescriptor.getDeclaredLength();\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\n</code></pre>\n<p>in order to load my model, I use:</p>\n<pre><code>AssetFileDescriptor fileDescriptor = null;\ntry {\nfileDescriptor = getAssets().openFd(MODEL_PATH);\n} catch (IOException e) {\ne.printStackTrace();\n }\nFileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\nFileChannel fileChannel = inputStream.getChannel();\nlong startOffset = fileDescriptor.getStartOffset();\nlong declaredLength = fileDescriptor.getDeclaredLength();\ntry {\nMByteBuffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\n} catch (IOException e) {\ne.printStackTrace();\n}\n\n</code></pre>\n<p>then, I use the tflite Interpreter by calling:<br>\n<code>tflite = new Interpreter(MByteBuffer);</code></p>\n<p>Using this snippet of an android code, I am able to laod the .lite graph that i got following the Tensorflow-for-poets-2 TFLite tutorial, <a href=\"url\">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6</a></p>\n<p>But when i try to load my .lite model I got the following error:</p>\n<p><code>A/libc: Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 9320 (flitecamerademo), pid 9320 (flitecamerademo)</code></p>\n<p>I want to notify that my model works (preforms a style transfer). I test it using python API, in the following way:</p>\n<pre><code>tflite_graph_filename = 'debug_graph.lite' # this is my model\n# Load TFLite model and allocate tensors.\ninterpreter = tf.contrib.lite.Interpreter(model_path=tflite_graph_filename)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\nprint('input_details: \\n{}\\n'.format(input_details))\noutput_details = interpreter.get_output_details()\nprint('output_details: \\n{}\\n'.format(output_details))\n\n# Test model on random input data.\ninput_shape = input_details[0]['shape']\nprint('input_shape: \\n{}\\n'.format(input_shape))\n\nX = np.zeros(input_shape,np.float32)\nX[0] = content_image\n\ninput_data = X # np.array(np.random.random_sample(input_shape), dtype=np.float32)\ninterpreter.set_tensor(input_details[0]['index'], input_data)\n\ninterpreter.invoke()\noutput_data = interpreter.get_tensor(output_details[0]['index'])\n\n</code></pre>\n<p>The above test is working so I assume my model is successfully converted to .lite format.</p>\n<p>use the following command to toco convert:</p>\n<p><code>toco --graph_def_file=/fullpathto/debug_graph.pb  --output_file=/fullpathto/debug_graph.lite --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --input_shape=1,474,712,3 --input_array=img_placeholder --output_array=transform/up-sample/mul --inference_type=FLOAT   --input_data_type=FLOAT</code></p>\n<p>Can you please help me with ideas on how to solve such problem or how can I debug this?</p>\n<p>Thanks,<br>\nVadim</p>", "body_text": "I'm having a problem with loading a lite model to the using the android tensorflow lite Interpreter.\nI am trying to use the workflow of  Tensorflow-for-poets-2 TFLite tutorial, https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6\ninstead of using the function:\nprivate MappedByteBuffer loadModelFile(Activity activity,String MODEL_FILE) throws IOException {\n    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\n    FileChannel fileChannel = inputStream.getChannel();\n    long startOffset = fileDescriptor.getStartOffset();\n    long declaredLength = fileDescriptor.getDeclaredLength();\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\n\nin order to load my model, I use:\nAssetFileDescriptor fileDescriptor = null;\ntry {\nfileDescriptor = getAssets().openFd(MODEL_PATH);\n} catch (IOException e) {\ne.printStackTrace();\n }\nFileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\nFileChannel fileChannel = inputStream.getChannel();\nlong startOffset = fileDescriptor.getStartOffset();\nlong declaredLength = fileDescriptor.getDeclaredLength();\ntry {\nMByteBuffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\n} catch (IOException e) {\ne.printStackTrace();\n}\n\n\nthen, I use the tflite Interpreter by calling:\ntflite = new Interpreter(MByteBuffer);\nUsing this snippet of an android code, I am able to laod the .lite graph that i got following the Tensorflow-for-poets-2 TFLite tutorial, https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6\nBut when i try to load my .lite model I got the following error:\nA/libc: Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 9320 (flitecamerademo), pid 9320 (flitecamerademo)\nI want to notify that my model works (preforms a style transfer). I test it using python API, in the following way:\ntflite_graph_filename = 'debug_graph.lite' # this is my model\n# Load TFLite model and allocate tensors.\ninterpreter = tf.contrib.lite.Interpreter(model_path=tflite_graph_filename)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\nprint('input_details: \\n{}\\n'.format(input_details))\noutput_details = interpreter.get_output_details()\nprint('output_details: \\n{}\\n'.format(output_details))\n\n# Test model on random input data.\ninput_shape = input_details[0]['shape']\nprint('input_shape: \\n{}\\n'.format(input_shape))\n\nX = np.zeros(input_shape,np.float32)\nX[0] = content_image\n\ninput_data = X # np.array(np.random.random_sample(input_shape), dtype=np.float32)\ninterpreter.set_tensor(input_details[0]['index'], input_data)\n\ninterpreter.invoke()\noutput_data = interpreter.get_tensor(output_details[0]['index'])\n\n\nThe above test is working so I assume my model is successfully converted to .lite format.\nuse the following command to toco convert:\ntoco --graph_def_file=/fullpathto/debug_graph.pb  --output_file=/fullpathto/debug_graph.lite --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --input_shape=1,474,712,3 --input_array=img_placeholder --output_array=transform/up-sample/mul --inference_type=FLOAT   --input_data_type=FLOAT\nCan you please help me with ideas on how to solve such problem or how can I debug this?\nThanks,\nVadim", "body": "I'm having a problem with loading a lite model to the using the android tensorflow lite Interpreter.\r\nI am trying to use the workflow of  Tensorflow-for-poets-2 TFLite tutorial, [https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6](url)\r\n\r\ninstead of using the function:\r\n```\r\nprivate MappedByteBuffer loadModelFile(Activity activity,String MODEL_FILE) throws IOException {\r\n    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n```\r\nin order to load my model, I use: \r\n\r\n```\r\nAssetFileDescriptor fileDescriptor = null;\r\ntry {\r\nfileDescriptor = getAssets().openFd(MODEL_PATH);\r\n} catch (IOException e) {\r\ne.printStackTrace();\r\n }\r\nFileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\nFileChannel fileChannel = inputStream.getChannel();\r\nlong startOffset = fileDescriptor.getStartOffset();\r\nlong declaredLength = fileDescriptor.getDeclaredLength();\r\ntry {\r\nMByteBuffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n} catch (IOException e) {\r\ne.printStackTrace();\r\n}\r\n\r\n```\r\nthen, I use the tflite Interpreter by calling:\r\n`tflite = new Interpreter(MByteBuffer);`\r\n\r\nUsing this snippet of an android code, I am able to laod the .lite graph that i got following the Tensorflow-for-poets-2 TFLite tutorial, [https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6](url)\r\n\r\nBut when i try to load my .lite model I got the following error:\r\n\r\n`A/libc: Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 9320 (flitecamerademo), pid 9320 (flitecamerademo)`\r\n\r\nI want to notify that my model works (preforms a style transfer). I test it using python API, in the following way:\r\n\r\n```\r\ntflite_graph_filename = 'debug_graph.lite' # this is my model\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.contrib.lite.Interpreter(model_path=tflite_graph_filename)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\nprint('input_details: \\n{}\\n'.format(input_details))\r\noutput_details = interpreter.get_output_details()\r\nprint('output_details: \\n{}\\n'.format(output_details))\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\nprint('input_shape: \\n{}\\n'.format(input_shape))\r\n\r\nX = np.zeros(input_shape,np.float32)\r\nX[0] = content_image\r\n\r\ninput_data = X # np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n```\r\nThe above test is working so I assume my model is successfully converted to .lite format.\r\n\r\nuse the following command to toco convert:\r\n\r\n`toco --graph_def_file=/fullpathto/debug_graph.pb  --output_file=/fullpathto/debug_graph.lite --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --input_shape=1,474,712,3 --input_array=img_placeholder --output_array=transform/up-sample/mul --inference_type=FLOAT   --input_data_type=FLOAT`\r\n\r\nCan you please help me with ideas on how to solve such problem or how can I debug this?\r\n\r\nThanks,\r\nVadim\r\n\r\n"}