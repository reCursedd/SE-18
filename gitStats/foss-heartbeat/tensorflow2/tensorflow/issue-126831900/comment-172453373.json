{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/172453373", "html_url": "https://github.com/tensorflow/tensorflow/pull/782#issuecomment-172453373", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/782", "id": 172453373, "node_id": "MDEyOklzc3VlQ29tbWVudDE3MjQ1MzM3Mw==", "user": {"login": "fpmchu", "id": 7635883, "node_id": "MDQ6VXNlcjc2MzU4ODM=", "avatar_url": "https://avatars0.githubusercontent.com/u/7635883?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fpmchu", "html_url": "https://github.com/fpmchu", "followers_url": "https://api.github.com/users/fpmchu/followers", "following_url": "https://api.github.com/users/fpmchu/following{/other_user}", "gists_url": "https://api.github.com/users/fpmchu/gists{/gist_id}", "starred_url": "https://api.github.com/users/fpmchu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fpmchu/subscriptions", "organizations_url": "https://api.github.com/users/fpmchu/orgs", "repos_url": "https://api.github.com/users/fpmchu/repos", "events_url": "https://api.github.com/users/fpmchu/events{/privacy}", "received_events_url": "https://api.github.com/users/fpmchu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-18T07:45:39Z", "updated_at": "2016-01-18T07:45:39Z", "author_association": "NONE", "body_html": "<p>Ok, added atomic ops and range check.</p>\n<p>For atomics, the benchmark code as is doesn't really run on GPU (not that I can tell).  I wrote a custom script to benchmark, and it shows very little difference in speed between the non-atomic and the atomic version, for the common case when there are no repeats.  I think this is to be expected as the atomics are lock free.  It might be slow for when there are repeat indices, but that really seems like a odd use case anyway...</p>\n<p>I had to add atomicAdd/atomicExch to support double, because CUDA does not support them directly.  The code is copied straight from CUDA's own manual.  I put them in <code>cuda_kernel_helper.h</code>.  Not sure if that's a good idea and/or it's the right place.</p>\n<p>For range checks, I wrote a simple CUDA kernel for doing range checks, and also added tests.</p>\n<p>Btw, I'm not checking for CUDA errors, and it seems like I should.  I'm following examples from bias_ops though, and in there there's also no error checking... Any suggestions?</p>", "body_text": "Ok, added atomic ops and range check.\nFor atomics, the benchmark code as is doesn't really run on GPU (not that I can tell).  I wrote a custom script to benchmark, and it shows very little difference in speed between the non-atomic and the atomic version, for the common case when there are no repeats.  I think this is to be expected as the atomics are lock free.  It might be slow for when there are repeat indices, but that really seems like a odd use case anyway...\nI had to add atomicAdd/atomicExch to support double, because CUDA does not support them directly.  The code is copied straight from CUDA's own manual.  I put them in cuda_kernel_helper.h.  Not sure if that's a good idea and/or it's the right place.\nFor range checks, I wrote a simple CUDA kernel for doing range checks, and also added tests.\nBtw, I'm not checking for CUDA errors, and it seems like I should.  I'm following examples from bias_ops though, and in there there's also no error checking... Any suggestions?", "body": "Ok, added atomic ops and range check.\n\nFor atomics, the benchmark code as is doesn't really run on GPU (not that I can tell).  I wrote a custom script to benchmark, and it shows very little difference in speed between the non-atomic and the atomic version, for the common case when there are no repeats.  I think this is to be expected as the atomics are lock free.  It might be slow for when there are repeat indices, but that really seems like a odd use case anyway...\n\nI had to add atomicAdd/atomicExch to support double, because CUDA does not support them directly.  The code is copied straight from CUDA's own manual.  I put them in `cuda_kernel_helper.h`.  Not sure if that's a good idea and/or it's the right place.\n\nFor range checks, I wrote a simple CUDA kernel for doing range checks, and also added tests.\n\nBtw, I'm not checking for CUDA errors, and it seems like I should.  I'm following examples from bias_ops though, and in there there's also no error checking... Any suggestions?\n"}