{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/782", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/782/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/782/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/782/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/782", "id": 126831900, "node_id": "MDExOlB1bGxSZXF1ZXN0NTYxNDQ3MDY=", "number": 782, "title": "Adds GPU CUDA kernel for scatter ops", "user": {"login": "fpmchu", "id": 7635883, "node_id": "MDQ6VXNlcjc2MzU4ODM=", "avatar_url": "https://avatars0.githubusercontent.com/u/7635883?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fpmchu", "html_url": "https://github.com/fpmchu", "followers_url": "https://api.github.com/users/fpmchu/followers", "following_url": "https://api.github.com/users/fpmchu/following{/other_user}", "gists_url": "https://api.github.com/users/fpmchu/gists{/gist_id}", "starred_url": "https://api.github.com/users/fpmchu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fpmchu/subscriptions", "organizations_url": "https://api.github.com/users/fpmchu/orgs", "repos_url": "https://api.github.com/users/fpmchu/repos", "events_url": "https://api.github.com/users/fpmchu/events{/privacy}", "received_events_url": "https://api.github.com/users/fpmchu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 35, "created_at": "2016-01-15T09:00:40Z", "updated_at": "2016-02-09T08:27:18Z", "closed_at": "2016-02-09T08:27:18Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/782", "html_url": "https://github.com/tensorflow/tensorflow/pull/782", "diff_url": "https://github.com/tensorflow/tensorflow/pull/782.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/782.patch"}, "body_html": "<p>This adds GPU CUDA kernels for scatter_add, scatter_sub, and scatter_update.</p>\n<p>Testing on a local machine with a weak graphics card already shows good improvement.  For about 20M floats, split as a 2Mx10 matrix, 1M size for idx, the op on a weak 3-core GPU is about twice as fast as on a 12-core CPU.  On very small data sets (1000 floats) the GPU code seems a bit slower (20%).</p>\n<p>Some notes:</p>\n<ol>\n<li>range checking in CUDA kernel currently not implemented.  It's not possible to include <code>OpKernelContext</code> since it includes protos hence nvcc fails to compile.  I can do it as a method in the .h/.cc files.  However, it seems rather inefficient as for low-dimensional input it is basically as slow as doing the entire scatter op sequentially on CPU.  Would it make sense to run the entire kernel, ignore bad indices, and report after?  This does mean the parameters may be updated when the code does fail because the modifications are happening in place.  Advice?<br></li>\n<li>There's a weird bug in <code>scatter_op_gpu.cu.cc</code> where I cannot use <code>TF_CALL_GPU_NUMBER_TYPES</code>.  I suspect it has to do with the <code>defined(__ANDROID__)</code> magic in <code>register_types.h</code> interacting badly with nvcc, but cannot confirm.</li>\n</ol>", "body_text": "This adds GPU CUDA kernels for scatter_add, scatter_sub, and scatter_update.\nTesting on a local machine with a weak graphics card already shows good improvement.  For about 20M floats, split as a 2Mx10 matrix, 1M size for idx, the op on a weak 3-core GPU is about twice as fast as on a 12-core CPU.  On very small data sets (1000 floats) the GPU code seems a bit slower (20%).\nSome notes:\n\nrange checking in CUDA kernel currently not implemented.  It's not possible to include OpKernelContext since it includes protos hence nvcc fails to compile.  I can do it as a method in the .h/.cc files.  However, it seems rather inefficient as for low-dimensional input it is basically as slow as doing the entire scatter op sequentially on CPU.  Would it make sense to run the entire kernel, ignore bad indices, and report after?  This does mean the parameters may be updated when the code does fail because the modifications are happening in place.  Advice?\nThere's a weird bug in scatter_op_gpu.cu.cc where I cannot use TF_CALL_GPU_NUMBER_TYPES.  I suspect it has to do with the defined(__ANDROID__) magic in register_types.h interacting badly with nvcc, but cannot confirm.", "body": "This adds GPU CUDA kernels for scatter_add, scatter_sub, and scatter_update.\n\nTesting on a local machine with a weak graphics card already shows good improvement.  For about 20M floats, split as a 2Mx10 matrix, 1M size for idx, the op on a weak 3-core GPU is about twice as fast as on a 12-core CPU.  On very small data sets (1000 floats) the GPU code seems a bit slower (20%).\n\nSome notes:\n1. range checking in CUDA kernel currently not implemented.  It's not possible to include `OpKernelContext` since it includes protos hence nvcc fails to compile.  I can do it as a method in the .h/.cc files.  However, it seems rather inefficient as for low-dimensional input it is basically as slow as doing the entire scatter op sequentially on CPU.  Would it make sense to run the entire kernel, ignore bad indices, and report after?  This does mean the parameters may be updated when the code does fail because the modifications are happening in place.  Advice?<br>\n2. There's a weird bug in `scatter_op_gpu.cu.cc` where I cannot use `TF_CALL_GPU_NUMBER_TYPES`.  I suspect it has to do with the `defined(__ANDROID__)` magic in `register_types.h` interacting badly with nvcc, but cannot confirm.\n"}