{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215492329", "html_url": "https://github.com/tensorflow/tensorflow/issues/386#issuecomment-215492329", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/386", "id": 215492329, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTQ5MjMyOQ==", "user": {"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-28T16:51:55Z", "updated_at": "2016-04-28T16:52:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Ryan's right. Though a tiny correction (might not matter due to auto placement),  the code misplaced the with tf.device('/gpu:0') I think. It should be</p>\n<pre><code>  # Construct a computation graph and place a constant e and fft(e) on /gpu:0\nwith g.as_default(), tf.device('/gpu:0'):\n  # Here we start to add nodes into 'g'\n  e = tf.constant(x, tf.complex64)\n  fft = tf.batch_fft2d(e)\n  # Here we finish adding nodes into 'g'. \n  # Typically, a tf proram is easier to read and is more\n  # robust to keep the graph g intact after this point.\n\n  with tf.Session() as sess:\n    for _ in range(100):\n      sess.run(barrier)\n\n</code></pre>\n<p>tf.device('/gpu:0') has no bearing on how sess.run exectuting stuff. It affects how the computation graph is placed.</p>", "body_text": "Ryan's right. Though a tiny correction (might not matter due to auto placement),  the code misplaced the with tf.device('/gpu:0') I think. It should be\n  # Construct a computation graph and place a constant e and fft(e) on /gpu:0\nwith g.as_default(), tf.device('/gpu:0'):\n  # Here we start to add nodes into 'g'\n  e = tf.constant(x, tf.complex64)\n  fft = tf.batch_fft2d(e)\n  # Here we finish adding nodes into 'g'. \n  # Typically, a tf proram is easier to read and is more\n  # robust to keep the graph g intact after this point.\n\n  with tf.Session() as sess:\n    for _ in range(100):\n      sess.run(barrier)\n\n\ntf.device('/gpu:0') has no bearing on how sess.run exectuting stuff. It affects how the computation graph is placed.", "body": "Ryan's right. Though a tiny correction (might not matter due to auto placement),  the code misplaced the with tf.device('/gpu:0') I think. It should be \n\n```\n  # Construct a computation graph and place a constant e and fft(e) on /gpu:0\nwith g.as_default(), tf.device('/gpu:0'):\n  # Here we start to add nodes into 'g'\n  e = tf.constant(x, tf.complex64)\n  fft = tf.batch_fft2d(e)\n  # Here we finish adding nodes into 'g'. \n  # Typically, a tf proram is easier to read and is more\n  # robust to keep the graph g intact after this point.\n\n  with tf.Session() as sess:\n    for _ in range(100):\n      sess.run(barrier)\n\n```\n\ntf.device('/gpu:0') has no bearing on how sess.run exectuting stuff. It affects how the computation graph is placed.\n"}