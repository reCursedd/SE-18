{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/353804222", "html_url": "https://github.com/tensorflow/tensorflow/issues/386#issuecomment-353804222", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/386", "id": 353804222, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzgwNDIyMg==", "user": {"login": "rryan", "id": 26527, "node_id": "MDQ6VXNlcjI2NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/26527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rryan", "html_url": "https://github.com/rryan", "followers_url": "https://api.github.com/users/rryan/followers", "following_url": "https://api.github.com/users/rryan/following{/other_user}", "gists_url": "https://api.github.com/users/rryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rryan/subscriptions", "organizations_url": "https://api.github.com/users/rryan/orgs", "repos_url": "https://api.github.com/users/rryan/repos", "events_url": "https://api.github.com/users/rryan/events{/privacy}", "received_events_url": "https://api.github.com/users/rryan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-24T21:41:34Z", "updated_at": "2017-12-24T21:41:34Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26527\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rryan\">@rryan</a>, is there anything special we would have to do? I get new warnings now, like these: (you can copy paste the whole thing)</p>\n</blockquote>\n<p>I think what you're seeing is independent of whether FFT is differentiable. <a href=\"https://colab.research.google.com/notebook#fileId=1fh1D_okZ8fRLOyjHRiaY7hOS5bBHfOIy\" rel=\"nofollow\">Here's a colab</a> where I copied your example. You need to do as the error message says and provide a <code>grad_ys</code> argument to <code>tf.gradients</code> if <code>ys</code> is complex, because the TF gradient infrastructure currently won't assume that <code>grad_ys</code> is all ones if the type is complex, as it does when the type is real (I'm not totally sure why this is... maybe the original author of that code didn't want to assume that <code>1+1j</code> was the proper <code>grad_ys</code> for a complex number because its magnitude is greater than 1?).</p>\n<p>You can see this logic here:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/70e33e0f77cb4f92ddd70a8dd601873b178124fc/tensorflow/python/ops/gradients_impl.py#L232-L239\">tensorflow/tensorflow/python/ops/gradients_impl.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 232 to 239\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/70e33e0f77cb4f92ddd70a8dd601873b178124fc\">70e33e0</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L232\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"232\"></td>\n          <td id=\"LC232\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> grad_y <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L233\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"233\"></td>\n          <td id=\"LC233\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">if</span> y.dtype.is_complex: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L234\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"234\"></td>\n          <td id=\"LC234\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">TypeError</span>( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L235\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"235\"></td>\n          <td id=\"LC235\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Gradients of complex tensors must set grad_ys (y.dtype = <span class=\"pl-c1\">%r</span>)<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L236\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"236\"></td>\n          <td id=\"LC236\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">         y.dtype) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L237\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"237\"></td>\n          <td id=\"LC237\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   new_grad_ys.append(array_ops.fill( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L238\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"238\"></td>\n          <td id=\"LC238\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       array_ops.shape(y), constant_op.constant( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L239\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"239\"></td>\n          <td id=\"LC239\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>y.dtype, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>grad_ys_<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> i))) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>", "body_text": "@rryan, is there anything special we would have to do? I get new warnings now, like these: (you can copy paste the whole thing)\n\nI think what you're seeing is independent of whether FFT is differentiable. Here's a colab where I copied your example. You need to do as the error message says and provide a grad_ys argument to tf.gradients if ys is complex, because the TF gradient infrastructure currently won't assume that grad_ys is all ones if the type is complex, as it does when the type is real (I'm not totally sure why this is... maybe the original author of that code didn't want to assume that 1+1j was the proper grad_ys for a complex number because its magnitude is greater than 1?).\nYou can see this logic here:\n\n  \n    \n      tensorflow/tensorflow/python/ops/gradients_impl.py\n    \n    \n        Lines 232 to 239\n      in\n      70e33e0\n    \n    \n    \n    \n\n        \n          \n           if grad_y is None: \n        \n\n        \n          \n             if y.dtype.is_complex: \n        \n\n        \n          \n               raise TypeError( \n        \n\n        \n          \n                   \"Gradients of complex tensors must set grad_ys (y.dtype = %r)\" % \n        \n\n        \n          \n                   y.dtype) \n        \n\n        \n          \n             new_grad_ys.append(array_ops.fill( \n        \n\n        \n          \n                 array_ops.shape(y), constant_op.constant( \n        \n\n        \n          \n                     1, dtype=y.dtype, name=\"grad_ys_%d\" % i)))", "body": "> @rryan, is there anything special we would have to do? I get new warnings now, like these: (you can copy paste the whole thing)\r\n\r\nI think what you're seeing is independent of whether FFT is differentiable. [Here's a colab](\r\nhttps://colab.research.google.com/notebook#fileId=1fh1D_okZ8fRLOyjHRiaY7hOS5bBHfOIy) where I copied your example. You need to do as the error message says and provide a `grad_ys` argument to `tf.gradients` if `ys` is complex, because the TF gradient infrastructure currently won't assume that `grad_ys` is all ones if the type is complex, as it does when the type is real (I'm not totally sure why this is... maybe the original author of that code didn't want to assume that `1+1j` was the proper `grad_ys` for a complex number because its magnitude is greater than 1?). \r\n\r\nYou can see this logic here:\r\nhttps://github.com/tensorflow/tensorflow/blob/70e33e0f77cb4f92ddd70a8dd601873b178124fc/tensorflow/python/ops/gradients_impl.py#L232-L239\r\n"}