{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/380282933", "html_url": "https://github.com/tensorflow/tensorflow/issues/18402#issuecomment-380282933", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18402", "id": 380282933, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDI4MjkzMw==", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-10T23:52:16Z", "updated_at": "2018-04-10T23:52:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> I tried your first suggestion, issue is the same.<br>\nI also tried using <code>return port::StatusOr&lt;std::tuple&lt;int,int,int&gt;&gt;(port::UnimplementedError( \"DnnSupport::GetVersion not implemented on this platform.\"))</code> and it didn't change the result.</p>\n<p><a href=\"https://stackoverflow.com/questions/44475317/variadic-template-issue\" rel=\"nofollow\">StackOverflow</a> and <a href=\"https://akrzemi1.wordpress.com/2013/10/10/too-perfect-forwarding/\" rel=\"nofollow\">Andrzej's blog</a> describes the issue. It is not happening for every tuple.</p>\n<p>I added a new class to workaround it which is below. Unfortunately my editor ran clang-format before saving so diff looks bigger than it should, sorry about that. In essence I added a class with 3 int members and using it instead of tuple.</p>\n<p>Please take a look and let me know if I should issue a PR or if you have an alternative solution.</p>\n<pre><code>diff --git a/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc b/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\nindex d6b457a91b..28177e0abe 100644\n--- a/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\n+++ b/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\n@@ -99,9 +99,9 @@ bool ShouldIncludeWinogradNonfusedAlgo(const Shape&amp; input_shape,\n                                        const ConvolutionDimensionNumbers&amp; dnums,\n                                        se::StreamExecutor* stream_exec) {\n   // Skip this check for cudnn7 and newer.\n-  se::port::StatusOr&lt;std::tuple&lt;int, int, int&gt;&gt; version =\n+  auto version =\n       stream_exec-&gt;AsDnn()-&gt;GetVersion();\n-  if (version.ok() &amp;&amp; std::get&lt;0&gt;(version.ValueOrDie()) &gt;= 7) {\n+  if (version.ok() &amp;&amp; version.ValueOrDie().maj() &gt;= 7) {\n     return true;\n   }\n \ndiff --git a/tensorflow/stream_executor/cuda/cuda_dnn.cc b/tensorflow/stream_executor/cuda/cuda_dnn.cc\nindex 1dc7f991b3..c514e84b9c 100644\n--- a/tensorflow/stream_executor/cuda/cuda_dnn.cc\n+++ b/tensorflow/stream_executor/cuda/cuda_dnn.cc\n@@ -18,7 +18,6 @@ limitations under the License.\n #include &lt;functional&gt;\n #include &lt;memory&gt;\n \n-#include \"third_party/eigen3/Eigen/Core\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/util/env_var.h\"\n #include \"tensorflow/stream_executor/cuda/cuda_activation.h\"\n@@ -41,6 +40,7 @@ limitations under the License.\n #include \"tensorflow/stream_executor/scratch_allocator.h\"\n #include \"tensorflow/stream_executor/stream.h\"\n #include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\n+#include \"third_party/eigen3/Eigen/Core\"\n // clang-format off\n #include \"cuda/include/cudnn.h\"\n // clang-format on\n@@ -63,10 +63,10 @@ namespace perftools {\n namespace gputools {\n \n using dnn::BatchDescriptor;\n-using dnn::FilterDescriptor;\n using dnn::ConvolutionDescriptor;\n-using dnn::PoolingDescriptor;\n+using dnn::FilterDescriptor;\n using dnn::NormalizeDescriptor;\n+using dnn::PoolingDescriptor;\n \n namespace cuda {\n \n@@ -215,9 +215,9 @@ CUDNN_DNN_ROUTINE_EACH(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n #if CUDNN_VERSION &gt;= 3000\n #define CUDNN_DNN_ROUTINE_EACH_AFTER_R3(__macro)              \\\n   __macro(cudnnGetConvolutionBackwardFilterWorkspaceSize)     \\\n-  __macro(cudnnGetConvolutionBackwardDataAlgorithm)           \\\n-  __macro(cudnnGetConvolutionBackwardFilterAlgorithm)         \\\n-  __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\n+      __macro(cudnnGetConvolutionBackwardDataAlgorithm)       \\\n+          __macro(cudnnGetConvolutionBackwardFilterAlgorithm) \\\n+              __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\n CUDNN_DNN_ROUTINE_EACH_AFTER_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n #undef CUDNN_DNN_ROUTINE_EACH_AFTER_R3\n #endif\n@@ -477,11 +477,12 @@ port::Status CudnnSupport::Init() {\n                                    ToString(status))};\n }\n \n-port::StatusOr&lt;std::tuple&lt;int, int, int&gt;&gt; CudnnSupport::GetVersion() {\n+port::StatusOr&lt;perftools::gputools::dnn::VersionInfo&gt;\n+CudnnSupport::GetVersion() {\n   CudnnVersion version;\n   TF_RETURN_IF_ERROR(GetLoadedCudnnVersion(&amp;version));\n-  return std::make_tuple(version.major_version, version.minor_version,\n-                         version.patch_level);\n+  return perftools::gputools::dnn::VersionInfo(\n+      version.major_version, version.minor_version, version.patch_level);\n }\n \n // Turns a BatchDescriptor structure into a cudnn tensor handle within a scope.\n@@ -1219,8 +1220,7 @@ class CudnnRnnDescriptor : public CudnnDescriptorCommon&lt;dnn::RnnDescriptor&gt; {\n       cudnnStatus_t status =\n           wrap::cudnnSetRNNMatrixMathType(parent_, rnn_desc_, math_type);\n       if (status != CUDNN_STATUS_SUCCESS) {\n-        LOG(FATAL) &lt;&lt; \"could not set cudnn RNN math type: \"\n-                   &lt;&lt; ToString(status);\n+        LOG(FATAL) &lt;&lt; \"could not set cudnn RNN math type: \" &lt;&lt; ToString(status);\n       }\n     }\n #endif\n@@ -2542,33 +2542,32 @@ bool CudnnSupport::DoConvolveImpl(\n   //   GetCudnnConvolutionForwardAlgorithm().\n   if (algorithm_config.algorithm().is_default()) {\n     // With the default algorithm, use Cudnn's heuristics.\n-    auto get_algorithm =\n-        [&amp;](bool specify_limit) SHARED_LOCKS_REQUIRED(dnn_handle_mutex_) {\n-          cudnnConvolutionFwdPreference_t preference =\n-              specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n-                            : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n-\n-          auto memory_limit_bytes =\n-              scratch_allocator == nullptr\n-                  ? 0\n-                  : scratch_allocator-&gt;GetMemoryLimitInBytes(stream);\n-          if (memory_limit_bytes &lt; 0) {\n-            memory_limit_bytes = 0;\n-          }\n+    auto get_algorithm = [&amp;](bool specify_limit) SHARED_LOCKS_REQUIRED(\n+                             dnn_handle_mutex_) {\n+      cudnnConvolutionFwdPreference_t preference =\n+          specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n+                        : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n \n-          cudnnConvolutionFwdAlgo_t algo_to_use;\n-          status = wrap::cudnnGetConvolutionForwardAlgorithm(\n-              parent_, ToHandle(dnn_handle_), input_nd.handle(),\n-              filter.handle(), conv.handle(), output_nd.handle(),\n-              /*preference=*/preference,\n-              /*memoryLimitInBytes=*/memory_limit_bytes,\n-              /*algo=*/&amp;algo_to_use);\n-          CHECK_EQ(status, CUDNN_STATUS_SUCCESS)\n-              &lt;&lt; \"Unable to find a suitable \"\n-                 \"algorithm for doing forward \"\n-                 \"convolution\";\n-          return algo_to_use;\n-        };\n+      auto memory_limit_bytes =\n+          scratch_allocator == nullptr\n+              ? 0\n+              : scratch_allocator-&gt;GetMemoryLimitInBytes(stream);\n+      if (memory_limit_bytes &lt; 0) {\n+        memory_limit_bytes = 0;\n+      }\n+\n+      cudnnConvolutionFwdAlgo_t algo_to_use;\n+      status = wrap::cudnnGetConvolutionForwardAlgorithm(\n+          parent_, ToHandle(dnn_handle_), input_nd.handle(), filter.handle(),\n+          conv.handle(), output_nd.handle(),\n+          /*preference=*/preference,\n+          /*memoryLimitInBytes=*/memory_limit_bytes,\n+          /*algo=*/&amp;algo_to_use);\n+      CHECK_EQ(status, CUDNN_STATUS_SUCCESS) &lt;&lt; \"Unable to find a suitable \"\n+                                                \"algorithm for doing forward \"\n+                                                \"convolution\";\n+      return algo_to_use;\n+    };\n \n     algo = get_algorithm(/*specify_limit=*/scratch_allocator != nullptr);\n     use_tensor_ops = true;\n@@ -3301,10 +3300,9 @@ bool CudnnSupport::DoFusedConvolve(\n #endif\n }\n \n-template&lt;class T&gt;\n+template &lt;class T&gt;\n DeviceMemory&lt;T&gt; CudnnSupport::MaybeTransformLayout(\n-    Stream* stream,\n-    BatchDescriptor* output_descriptor,\n+    Stream* stream, BatchDescriptor* output_descriptor,\n     DeviceMemory&lt;T&gt; backward_output_data,\n     std::unique_ptr&lt;TemporaryDeviceMemory&lt;T&gt;&gt;* transform_scratch) {\n   if (output_descriptor-&gt;layout() == dnn::DataLayout::kBatchDepthYX) {\n@@ -3373,8 +3371,7 @@ bool CudnnSupport::DoTransformTensor(Stream* stream,\n \n template &lt;class T&gt;\n bool CudnnSupport::DoConvolveBackwardDataImpl(\n-    Stream* stream,\n-    const FilterDescriptor&amp; filter_descriptor,\n+    Stream* stream, const FilterDescriptor&amp; filter_descriptor,\n     const DeviceMemory&lt;T&gt;&amp; filter_data,\n     const BatchDescriptor&amp; output_descriptor_in,\n     DeviceMemory&lt;T&gt; backward_output_data,\n@@ -3422,8 +3419,9 @@ bool CudnnSupport::DoConvolveBackwardDataImpl(\n \n   if (algorithm_config.algorithm().is_default()) {\n     // With the default algorithm, use Cudnn's heuristics.\n-    auto get_algorithm = [&amp;](bool specify_limit) SHARED_LOCKS_REQUIRED(\n-        dnn_handle_mutex_) -&gt; cudnnConvolutionBwdDataAlgo_t {\n+    auto get_algorithm =\n+        [&amp;](bool specify_limit) SHARED_LOCKS_REQUIRED(\n+            dnn_handle_mutex_) -&gt; cudnnConvolutionBwdDataAlgo_t {\n       cudnnConvolutionBwdDataPreference_t preference =\n           specify_limit ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\n                         : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\n@@ -3698,7 +3696,7 @@ bool CudnnSupport::DoConvolveBackwardFilterImpl(\n     // specify_limit will occur when we have a scratch allocator and it succeeds\n     // in allocating; otherwise, we'll fall back to the \"no workspace\" version.\n     auto get_algorithm = [&amp;](bool specify_limit) SHARED_LOCKS_REQUIRED(\n-        dnn_handle_mutex_) {\n+                             dnn_handle_mutex_) {\n       cudnnConvolutionBwdFilterPreference_t preference =\n           specify_limit ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\n                         : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\n@@ -4691,8 +4689,8 @@ void initialize_cudnn() {\n       gpu::PluginRegistry::Instance()\n           -&gt;RegisterFactory&lt;gpu::PluginRegistry::DnnFactory&gt;(\n               gpu::cuda::kCudaPlatformId, gpu::cuda::kCuDnnPlugin, \"cuDNN\",\n-              [](gpu::internal::StreamExecutorInterface*\n-                     parent) -&gt; gpu::dnn::DnnSupport* {\n+              [](gpu::internal::StreamExecutorInterface* parent)\n+                  -&gt; gpu::dnn::DnnSupport* {\n                 gpu::cuda::CUDAExecutor* cuda_executor =\n                     dynamic_cast&lt;gpu::cuda::CUDAExecutor*&gt;(parent);\n                 if (cuda_executor == nullptr) {\ndiff --git a/tensorflow/stream_executor/cuda/cuda_dnn.h b/tensorflow/stream_executor/cuda/cuda_dnn.h\nindex 0e5368aca8..09d248f137 100644\n--- a/tensorflow/stream_executor/cuda/cuda_dnn.h\n+++ b/tensorflow/stream_executor/cuda/cuda_dnn.h\n@@ -46,7 +46,7 @@ class CudnnSupport : public dnn::DnnSupport {\n   ~CudnnSupport() override;\n \n   port::Status Init() override;\n-  port::StatusOr&lt;std::tuple&lt;int, int, int&gt;&gt; GetVersion() override;\n+  port::StatusOr&lt;perftools::gputools::dnn::VersionInfo&gt; GetVersion() override;\n \n   port::StatusOr&lt;std::unique_ptr&lt;dnn::RnnDescriptor&gt;&gt; createRnnDescriptor(\n       int num_layers, int hidden_size, int input_size,\ndiff --git a/tensorflow/stream_executor/dnn.h b/tensorflow/stream_executor/dnn.h\nindex 3c47d2c2e8..024c5787c0 100644\n--- a/tensorflow/stream_executor/dnn.h\n+++ b/tensorflow/stream_executor/dnn.h\n@@ -879,6 +879,19 @@ string ElementwiseOperationString(ElementwiseOperation op);\n // Suite of operations typically used for implementing Deep/Convolutional Neural\n // Nets. Note: A false return value of an operation indicates the\n // implementation is not available.\n+class VersionInfo {\n+ public:\n+  VersionInfo(int major = 0, int minor = 0, int patch = 0)\n+      : major_(major), minor_(minor), patch_(patch) {}\n+  int maj() { return major_; }\n+  int min() { return minor_; }\n+  int patch() { return patch_; }\n+ private:\n+  int major_;\n+  int minor_;\n+  int patch_;\n+};\n+\n class DnnSupport {\n  public:\n   DnnSupport() {}\n@@ -887,7 +900,7 @@ class DnnSupport {\n   virtual port::Status Init() = 0;\n \n   // Gets the version of the backing library, as a {major, minor, patch} tuple.\n-  virtual port::StatusOr&lt;std::tuple&lt;int, int, int&gt;&gt; GetVersion() {\n+  virtual port::StatusOr&lt;VersionInfo&gt; GetVersion() {\n     return port::UnimplementedError(\n         \"DnnSupport::GetVersion not implemented on this platform.\");\n   }\n@@ -1865,10 +1878,10 @@ class DnnSupport {\n   //  bottom_pad: Amount to pad the input at the bottom (high Y).\n   //  output_data: un-owned device memory region in which to place the\n   //    padded result.\n-  virtual bool DoXYPad(Stream* stream, const dnn::BatchDescriptor &amp;dimensions,\n-                       const DeviceMemory&lt;float&gt; &amp;input_data,\n-                       int64 left_pad, int64 right_pad, int64 top_pad,\n-                       int64 bottom_pad, DeviceMemory&lt;float&gt; *output_data) = 0;\n+  virtual bool DoXYPad(Stream* stream, const dnn::BatchDescriptor&amp; dimensions,\n+                       const DeviceMemory&lt;float&gt;&amp; input_data, int64 left_pad,\n+                       int64 right_pad, int64 top_pad, int64 bottom_pad,\n+                       DeviceMemory&lt;float&gt;* output_data) = 0;\n \n   // Extracts a slice of the input in the X and Y dimensions. The feature_map\n   // dimension is unchanged.\n@@ -1885,10 +1898,10 @@ class DnnSupport {\n   //  bottom_trim: Amount to cut off the input at the bottom (high Y).\n   //  output_data: un-owned device memory region in which to place the\n   //    padded result.\n-  virtual bool DoXYSlice(Stream* stream, const dnn::BatchDescriptor &amp;dimensions,\n-                    const DeviceMemory&lt;float&gt; &amp;input_data,\n-                    int64 left_trim, int64 right_trim, int64 top_trim,\n-                    int64 bottom_trim, DeviceMemory&lt;float&gt; *output_data) = 0;\n+  virtual bool DoXYSlice(Stream* stream, const dnn::BatchDescriptor&amp; dimensions,\n+                         const DeviceMemory&lt;float&gt;&amp; input_data, int64 left_trim,\n+                         int64 right_trim, int64 top_trim, int64 bottom_trim,\n+                         DeviceMemory&lt;float&gt;* output_data) = 0;\n \n   // Grows the input tensor by replicating the X and Y dimensions. The batch and\n   // depth/feature_map dimensions are unchanged. Currently, the input tensor is\n</code></pre>", "body_text": "@jlebar I tried your first suggestion, issue is the same.\nI also tried using return port::StatusOr<std::tuple<int,int,int>>(port::UnimplementedError( \"DnnSupport::GetVersion not implemented on this platform.\")) and it didn't change the result.\nStackOverflow and Andrzej's blog describes the issue. It is not happening for every tuple.\nI added a new class to workaround it which is below. Unfortunately my editor ran clang-format before saving so diff looks bigger than it should, sorry about that. In essence I added a class with 3 int members and using it instead of tuple.\nPlease take a look and let me know if I should issue a PR or if you have an alternative solution.\ndiff --git a/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc b/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\nindex d6b457a91b..28177e0abe 100644\n--- a/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\n+++ b/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\n@@ -99,9 +99,9 @@ bool ShouldIncludeWinogradNonfusedAlgo(const Shape& input_shape,\n                                        const ConvolutionDimensionNumbers& dnums,\n                                        se::StreamExecutor* stream_exec) {\n   // Skip this check for cudnn7 and newer.\n-  se::port::StatusOr<std::tuple<int, int, int>> version =\n+  auto version =\n       stream_exec->AsDnn()->GetVersion();\n-  if (version.ok() && std::get<0>(version.ValueOrDie()) >= 7) {\n+  if (version.ok() && version.ValueOrDie().maj() >= 7) {\n     return true;\n   }\n \ndiff --git a/tensorflow/stream_executor/cuda/cuda_dnn.cc b/tensorflow/stream_executor/cuda/cuda_dnn.cc\nindex 1dc7f991b3..c514e84b9c 100644\n--- a/tensorflow/stream_executor/cuda/cuda_dnn.cc\n+++ b/tensorflow/stream_executor/cuda/cuda_dnn.cc\n@@ -18,7 +18,6 @@ limitations under the License.\n #include <functional>\n #include <memory>\n \n-#include \"third_party/eigen3/Eigen/Core\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/util/env_var.h\"\n #include \"tensorflow/stream_executor/cuda/cuda_activation.h\"\n@@ -41,6 +40,7 @@ limitations under the License.\n #include \"tensorflow/stream_executor/scratch_allocator.h\"\n #include \"tensorflow/stream_executor/stream.h\"\n #include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\n+#include \"third_party/eigen3/Eigen/Core\"\n // clang-format off\n #include \"cuda/include/cudnn.h\"\n // clang-format on\n@@ -63,10 +63,10 @@ namespace perftools {\n namespace gputools {\n \n using dnn::BatchDescriptor;\n-using dnn::FilterDescriptor;\n using dnn::ConvolutionDescriptor;\n-using dnn::PoolingDescriptor;\n+using dnn::FilterDescriptor;\n using dnn::NormalizeDescriptor;\n+using dnn::PoolingDescriptor;\n \n namespace cuda {\n \n@@ -215,9 +215,9 @@ CUDNN_DNN_ROUTINE_EACH(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n #if CUDNN_VERSION >= 3000\n #define CUDNN_DNN_ROUTINE_EACH_AFTER_R3(__macro)              \\\n   __macro(cudnnGetConvolutionBackwardFilterWorkspaceSize)     \\\n-  __macro(cudnnGetConvolutionBackwardDataAlgorithm)           \\\n-  __macro(cudnnGetConvolutionBackwardFilterAlgorithm)         \\\n-  __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\n+      __macro(cudnnGetConvolutionBackwardDataAlgorithm)       \\\n+          __macro(cudnnGetConvolutionBackwardFilterAlgorithm) \\\n+              __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\n CUDNN_DNN_ROUTINE_EACH_AFTER_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n #undef CUDNN_DNN_ROUTINE_EACH_AFTER_R3\n #endif\n@@ -477,11 +477,12 @@ port::Status CudnnSupport::Init() {\n                                    ToString(status))};\n }\n \n-port::StatusOr<std::tuple<int, int, int>> CudnnSupport::GetVersion() {\n+port::StatusOr<perftools::gputools::dnn::VersionInfo>\n+CudnnSupport::GetVersion() {\n   CudnnVersion version;\n   TF_RETURN_IF_ERROR(GetLoadedCudnnVersion(&version));\n-  return std::make_tuple(version.major_version, version.minor_version,\n-                         version.patch_level);\n+  return perftools::gputools::dnn::VersionInfo(\n+      version.major_version, version.minor_version, version.patch_level);\n }\n \n // Turns a BatchDescriptor structure into a cudnn tensor handle within a scope.\n@@ -1219,8 +1220,7 @@ class CudnnRnnDescriptor : public CudnnDescriptorCommon<dnn::RnnDescriptor> {\n       cudnnStatus_t status =\n           wrap::cudnnSetRNNMatrixMathType(parent_, rnn_desc_, math_type);\n       if (status != CUDNN_STATUS_SUCCESS) {\n-        LOG(FATAL) << \"could not set cudnn RNN math type: \"\n-                   << ToString(status);\n+        LOG(FATAL) << \"could not set cudnn RNN math type: \" << ToString(status);\n       }\n     }\n #endif\n@@ -2542,33 +2542,32 @@ bool CudnnSupport::DoConvolveImpl(\n   //   GetCudnnConvolutionForwardAlgorithm().\n   if (algorithm_config.algorithm().is_default()) {\n     // With the default algorithm, use Cudnn's heuristics.\n-    auto get_algorithm =\n-        [&](bool specify_limit) SHARED_LOCKS_REQUIRED(dnn_handle_mutex_) {\n-          cudnnConvolutionFwdPreference_t preference =\n-              specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n-                            : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n-\n-          auto memory_limit_bytes =\n-              scratch_allocator == nullptr\n-                  ? 0\n-                  : scratch_allocator->GetMemoryLimitInBytes(stream);\n-          if (memory_limit_bytes < 0) {\n-            memory_limit_bytes = 0;\n-          }\n+    auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n+                             dnn_handle_mutex_) {\n+      cudnnConvolutionFwdPreference_t preference =\n+          specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n+                        : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n \n-          cudnnConvolutionFwdAlgo_t algo_to_use;\n-          status = wrap::cudnnGetConvolutionForwardAlgorithm(\n-              parent_, ToHandle(dnn_handle_), input_nd.handle(),\n-              filter.handle(), conv.handle(), output_nd.handle(),\n-              /*preference=*/preference,\n-              /*memoryLimitInBytes=*/memory_limit_bytes,\n-              /*algo=*/&algo_to_use);\n-          CHECK_EQ(status, CUDNN_STATUS_SUCCESS)\n-              << \"Unable to find a suitable \"\n-                 \"algorithm for doing forward \"\n-                 \"convolution\";\n-          return algo_to_use;\n-        };\n+      auto memory_limit_bytes =\n+          scratch_allocator == nullptr\n+              ? 0\n+              : scratch_allocator->GetMemoryLimitInBytes(stream);\n+      if (memory_limit_bytes < 0) {\n+        memory_limit_bytes = 0;\n+      }\n+\n+      cudnnConvolutionFwdAlgo_t algo_to_use;\n+      status = wrap::cudnnGetConvolutionForwardAlgorithm(\n+          parent_, ToHandle(dnn_handle_), input_nd.handle(), filter.handle(),\n+          conv.handle(), output_nd.handle(),\n+          /*preference=*/preference,\n+          /*memoryLimitInBytes=*/memory_limit_bytes,\n+          /*algo=*/&algo_to_use);\n+      CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\n+                                                \"algorithm for doing forward \"\n+                                                \"convolution\";\n+      return algo_to_use;\n+    };\n \n     algo = get_algorithm(/*specify_limit=*/scratch_allocator != nullptr);\n     use_tensor_ops = true;\n@@ -3301,10 +3300,9 @@ bool CudnnSupport::DoFusedConvolve(\n #endif\n }\n \n-template<class T>\n+template <class T>\n DeviceMemory<T> CudnnSupport::MaybeTransformLayout(\n-    Stream* stream,\n-    BatchDescriptor* output_descriptor,\n+    Stream* stream, BatchDescriptor* output_descriptor,\n     DeviceMemory<T> backward_output_data,\n     std::unique_ptr<TemporaryDeviceMemory<T>>* transform_scratch) {\n   if (output_descriptor->layout() == dnn::DataLayout::kBatchDepthYX) {\n@@ -3373,8 +3371,7 @@ bool CudnnSupport::DoTransformTensor(Stream* stream,\n \n template <class T>\n bool CudnnSupport::DoConvolveBackwardDataImpl(\n-    Stream* stream,\n-    const FilterDescriptor& filter_descriptor,\n+    Stream* stream, const FilterDescriptor& filter_descriptor,\n     const DeviceMemory<T>& filter_data,\n     const BatchDescriptor& output_descriptor_in,\n     DeviceMemory<T> backward_output_data,\n@@ -3422,8 +3419,9 @@ bool CudnnSupport::DoConvolveBackwardDataImpl(\n \n   if (algorithm_config.algorithm().is_default()) {\n     // With the default algorithm, use Cudnn's heuristics.\n-    auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n-        dnn_handle_mutex_) -> cudnnConvolutionBwdDataAlgo_t {\n+    auto get_algorithm =\n+        [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n+            dnn_handle_mutex_) -> cudnnConvolutionBwdDataAlgo_t {\n       cudnnConvolutionBwdDataPreference_t preference =\n           specify_limit ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\n                         : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\n@@ -3698,7 +3696,7 @@ bool CudnnSupport::DoConvolveBackwardFilterImpl(\n     // specify_limit will occur when we have a scratch allocator and it succeeds\n     // in allocating; otherwise, we'll fall back to the \"no workspace\" version.\n     auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n-        dnn_handle_mutex_) {\n+                             dnn_handle_mutex_) {\n       cudnnConvolutionBwdFilterPreference_t preference =\n           specify_limit ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\n                         : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\n@@ -4691,8 +4689,8 @@ void initialize_cudnn() {\n       gpu::PluginRegistry::Instance()\n           ->RegisterFactory<gpu::PluginRegistry::DnnFactory>(\n               gpu::cuda::kCudaPlatformId, gpu::cuda::kCuDnnPlugin, \"cuDNN\",\n-              [](gpu::internal::StreamExecutorInterface*\n-                     parent) -> gpu::dnn::DnnSupport* {\n+              [](gpu::internal::StreamExecutorInterface* parent)\n+                  -> gpu::dnn::DnnSupport* {\n                 gpu::cuda::CUDAExecutor* cuda_executor =\n                     dynamic_cast<gpu::cuda::CUDAExecutor*>(parent);\n                 if (cuda_executor == nullptr) {\ndiff --git a/tensorflow/stream_executor/cuda/cuda_dnn.h b/tensorflow/stream_executor/cuda/cuda_dnn.h\nindex 0e5368aca8..09d248f137 100644\n--- a/tensorflow/stream_executor/cuda/cuda_dnn.h\n+++ b/tensorflow/stream_executor/cuda/cuda_dnn.h\n@@ -46,7 +46,7 @@ class CudnnSupport : public dnn::DnnSupport {\n   ~CudnnSupport() override;\n \n   port::Status Init() override;\n-  port::StatusOr<std::tuple<int, int, int>> GetVersion() override;\n+  port::StatusOr<perftools::gputools::dnn::VersionInfo> GetVersion() override;\n \n   port::StatusOr<std::unique_ptr<dnn::RnnDescriptor>> createRnnDescriptor(\n       int num_layers, int hidden_size, int input_size,\ndiff --git a/tensorflow/stream_executor/dnn.h b/tensorflow/stream_executor/dnn.h\nindex 3c47d2c2e8..024c5787c0 100644\n--- a/tensorflow/stream_executor/dnn.h\n+++ b/tensorflow/stream_executor/dnn.h\n@@ -879,6 +879,19 @@ string ElementwiseOperationString(ElementwiseOperation op);\n // Suite of operations typically used for implementing Deep/Convolutional Neural\n // Nets. Note: A false return value of an operation indicates the\n // implementation is not available.\n+class VersionInfo {\n+ public:\n+  VersionInfo(int major = 0, int minor = 0, int patch = 0)\n+      : major_(major), minor_(minor), patch_(patch) {}\n+  int maj() { return major_; }\n+  int min() { return minor_; }\n+  int patch() { return patch_; }\n+ private:\n+  int major_;\n+  int minor_;\n+  int patch_;\n+};\n+\n class DnnSupport {\n  public:\n   DnnSupport() {}\n@@ -887,7 +900,7 @@ class DnnSupport {\n   virtual port::Status Init() = 0;\n \n   // Gets the version of the backing library, as a {major, minor, patch} tuple.\n-  virtual port::StatusOr<std::tuple<int, int, int>> GetVersion() {\n+  virtual port::StatusOr<VersionInfo> GetVersion() {\n     return port::UnimplementedError(\n         \"DnnSupport::GetVersion not implemented on this platform.\");\n   }\n@@ -1865,10 +1878,10 @@ class DnnSupport {\n   //  bottom_pad: Amount to pad the input at the bottom (high Y).\n   //  output_data: un-owned device memory region in which to place the\n   //    padded result.\n-  virtual bool DoXYPad(Stream* stream, const dnn::BatchDescriptor &dimensions,\n-                       const DeviceMemory<float> &input_data,\n-                       int64 left_pad, int64 right_pad, int64 top_pad,\n-                       int64 bottom_pad, DeviceMemory<float> *output_data) = 0;\n+  virtual bool DoXYPad(Stream* stream, const dnn::BatchDescriptor& dimensions,\n+                       const DeviceMemory<float>& input_data, int64 left_pad,\n+                       int64 right_pad, int64 top_pad, int64 bottom_pad,\n+                       DeviceMemory<float>* output_data) = 0;\n \n   // Extracts a slice of the input in the X and Y dimensions. The feature_map\n   // dimension is unchanged.\n@@ -1885,10 +1898,10 @@ class DnnSupport {\n   //  bottom_trim: Amount to cut off the input at the bottom (high Y).\n   //  output_data: un-owned device memory region in which to place the\n   //    padded result.\n-  virtual bool DoXYSlice(Stream* stream, const dnn::BatchDescriptor &dimensions,\n-                    const DeviceMemory<float> &input_data,\n-                    int64 left_trim, int64 right_trim, int64 top_trim,\n-                    int64 bottom_trim, DeviceMemory<float> *output_data) = 0;\n+  virtual bool DoXYSlice(Stream* stream, const dnn::BatchDescriptor& dimensions,\n+                         const DeviceMemory<float>& input_data, int64 left_trim,\n+                         int64 right_trim, int64 top_trim, int64 bottom_trim,\n+                         DeviceMemory<float>* output_data) = 0;\n \n   // Grows the input tensor by replicating the X and Y dimensions. The batch and\n   // depth/feature_map dimensions are unchanged. Currently, the input tensor is", "body": "@jlebar I tried your first suggestion, issue is the same. \r\nI also tried using `return port::StatusOr<std::tuple<int,int,int>>(port::UnimplementedError(\r\n        \"DnnSupport::GetVersion not implemented on this platform.\"))` and it didn't change the result.\r\n\r\n[StackOverflow](https://stackoverflow.com/questions/44475317/variadic-template-issue) and [Andrzej's blog](https://akrzemi1.wordpress.com/2013/10/10/too-perfect-forwarding/) describes the issue. It is not happening for every tuple.\r\n\r\nI added a new class to workaround it which is below. Unfortunately my editor ran clang-format before saving so diff looks bigger than it should, sorry about that. In essence I added a class with 3 int members and using it instead of tuple.\r\n\r\nPlease take a look and let me know if I should issue a PR or if you have an alternative solution.\r\n\r\n```\r\ndiff --git a/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc b/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\r\nindex d6b457a91b..28177e0abe 100644\r\n--- a/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\r\n+++ b/tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc\r\n@@ -99,9 +99,9 @@ bool ShouldIncludeWinogradNonfusedAlgo(const Shape& input_shape,\r\n                                        const ConvolutionDimensionNumbers& dnums,\r\n                                        se::StreamExecutor* stream_exec) {\r\n   // Skip this check for cudnn7 and newer.\r\n-  se::port::StatusOr<std::tuple<int, int, int>> version =\r\n+  auto version =\r\n       stream_exec->AsDnn()->GetVersion();\r\n-  if (version.ok() && std::get<0>(version.ValueOrDie()) >= 7) {\r\n+  if (version.ok() && version.ValueOrDie().maj() >= 7) {\r\n     return true;\r\n   }\r\n \r\ndiff --git a/tensorflow/stream_executor/cuda/cuda_dnn.cc b/tensorflow/stream_executor/cuda/cuda_dnn.cc\r\nindex 1dc7f991b3..c514e84b9c 100644\r\n--- a/tensorflow/stream_executor/cuda/cuda_dnn.cc\r\n+++ b/tensorflow/stream_executor/cuda/cuda_dnn.cc\r\n@@ -18,7 +18,6 @@ limitations under the License.\r\n #include <functional>\r\n #include <memory>\r\n \r\n-#include \"third_party/eigen3/Eigen/Core\"\r\n #include \"tensorflow/core/lib/core/errors.h\"\r\n #include \"tensorflow/core/util/env_var.h\"\r\n #include \"tensorflow/stream_executor/cuda/cuda_activation.h\"\r\n@@ -41,6 +40,7 @@ limitations under the License.\r\n #include \"tensorflow/stream_executor/scratch_allocator.h\"\r\n #include \"tensorflow/stream_executor/stream.h\"\r\n #include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\r\n+#include \"third_party/eigen3/Eigen/Core\"\r\n // clang-format off\r\n #include \"cuda/include/cudnn.h\"\r\n // clang-format on\r\n@@ -63,10 +63,10 @@ namespace perftools {\r\n namespace gputools {\r\n \r\n using dnn::BatchDescriptor;\r\n-using dnn::FilterDescriptor;\r\n using dnn::ConvolutionDescriptor;\r\n-using dnn::PoolingDescriptor;\r\n+using dnn::FilterDescriptor;\r\n using dnn::NormalizeDescriptor;\r\n+using dnn::PoolingDescriptor;\r\n \r\n namespace cuda {\r\n \r\n@@ -215,9 +215,9 @@ CUDNN_DNN_ROUTINE_EACH(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\r\n #if CUDNN_VERSION >= 3000\r\n #define CUDNN_DNN_ROUTINE_EACH_AFTER_R3(__macro)              \\\r\n   __macro(cudnnGetConvolutionBackwardFilterWorkspaceSize)     \\\r\n-  __macro(cudnnGetConvolutionBackwardDataAlgorithm)           \\\r\n-  __macro(cudnnGetConvolutionBackwardFilterAlgorithm)         \\\r\n-  __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\r\n+      __macro(cudnnGetConvolutionBackwardDataAlgorithm)       \\\r\n+          __macro(cudnnGetConvolutionBackwardFilterAlgorithm) \\\r\n+              __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\r\n CUDNN_DNN_ROUTINE_EACH_AFTER_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\r\n #undef CUDNN_DNN_ROUTINE_EACH_AFTER_R3\r\n #endif\r\n@@ -477,11 +477,12 @@ port::Status CudnnSupport::Init() {\r\n                                    ToString(status))};\r\n }\r\n \r\n-port::StatusOr<std::tuple<int, int, int>> CudnnSupport::GetVersion() {\r\n+port::StatusOr<perftools::gputools::dnn::VersionInfo>\r\n+CudnnSupport::GetVersion() {\r\n   CudnnVersion version;\r\n   TF_RETURN_IF_ERROR(GetLoadedCudnnVersion(&version));\r\n-  return std::make_tuple(version.major_version, version.minor_version,\r\n-                         version.patch_level);\r\n+  return perftools::gputools::dnn::VersionInfo(\r\n+      version.major_version, version.minor_version, version.patch_level);\r\n }\r\n \r\n // Turns a BatchDescriptor structure into a cudnn tensor handle within a scope.\r\n@@ -1219,8 +1220,7 @@ class CudnnRnnDescriptor : public CudnnDescriptorCommon<dnn::RnnDescriptor> {\r\n       cudnnStatus_t status =\r\n           wrap::cudnnSetRNNMatrixMathType(parent_, rnn_desc_, math_type);\r\n       if (status != CUDNN_STATUS_SUCCESS) {\r\n-        LOG(FATAL) << \"could not set cudnn RNN math type: \"\r\n-                   << ToString(status);\r\n+        LOG(FATAL) << \"could not set cudnn RNN math type: \" << ToString(status);\r\n       }\r\n     }\r\n #endif\r\n@@ -2542,33 +2542,32 @@ bool CudnnSupport::DoConvolveImpl(\r\n   //   GetCudnnConvolutionForwardAlgorithm().\r\n   if (algorithm_config.algorithm().is_default()) {\r\n     // With the default algorithm, use Cudnn's heuristics.\r\n-    auto get_algorithm =\r\n-        [&](bool specify_limit) SHARED_LOCKS_REQUIRED(dnn_handle_mutex_) {\r\n-          cudnnConvolutionFwdPreference_t preference =\r\n-              specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\r\n-                            : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\r\n-\r\n-          auto memory_limit_bytes =\r\n-              scratch_allocator == nullptr\r\n-                  ? 0\r\n-                  : scratch_allocator->GetMemoryLimitInBytes(stream);\r\n-          if (memory_limit_bytes < 0) {\r\n-            memory_limit_bytes = 0;\r\n-          }\r\n+    auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\r\n+                             dnn_handle_mutex_) {\r\n+      cudnnConvolutionFwdPreference_t preference =\r\n+          specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\r\n+                        : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\r\n \r\n-          cudnnConvolutionFwdAlgo_t algo_to_use;\r\n-          status = wrap::cudnnGetConvolutionForwardAlgorithm(\r\n-              parent_, ToHandle(dnn_handle_), input_nd.handle(),\r\n-              filter.handle(), conv.handle(), output_nd.handle(),\r\n-              /*preference=*/preference,\r\n-              /*memoryLimitInBytes=*/memory_limit_bytes,\r\n-              /*algo=*/&algo_to_use);\r\n-          CHECK_EQ(status, CUDNN_STATUS_SUCCESS)\r\n-              << \"Unable to find a suitable \"\r\n-                 \"algorithm for doing forward \"\r\n-                 \"convolution\";\r\n-          return algo_to_use;\r\n-        };\r\n+      auto memory_limit_bytes =\r\n+          scratch_allocator == nullptr\r\n+              ? 0\r\n+              : scratch_allocator->GetMemoryLimitInBytes(stream);\r\n+      if (memory_limit_bytes < 0) {\r\n+        memory_limit_bytes = 0;\r\n+      }\r\n+\r\n+      cudnnConvolutionFwdAlgo_t algo_to_use;\r\n+      status = wrap::cudnnGetConvolutionForwardAlgorithm(\r\n+          parent_, ToHandle(dnn_handle_), input_nd.handle(), filter.handle(),\r\n+          conv.handle(), output_nd.handle(),\r\n+          /*preference=*/preference,\r\n+          /*memoryLimitInBytes=*/memory_limit_bytes,\r\n+          /*algo=*/&algo_to_use);\r\n+      CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\r\n+                                                \"algorithm for doing forward \"\r\n+                                                \"convolution\";\r\n+      return algo_to_use;\r\n+    };\r\n \r\n     algo = get_algorithm(/*specify_limit=*/scratch_allocator != nullptr);\r\n     use_tensor_ops = true;\r\n@@ -3301,10 +3300,9 @@ bool CudnnSupport::DoFusedConvolve(\r\n #endif\r\n }\r\n \r\n-template<class T>\r\n+template <class T>\r\n DeviceMemory<T> CudnnSupport::MaybeTransformLayout(\r\n-    Stream* stream,\r\n-    BatchDescriptor* output_descriptor,\r\n+    Stream* stream, BatchDescriptor* output_descriptor,\r\n     DeviceMemory<T> backward_output_data,\r\n     std::unique_ptr<TemporaryDeviceMemory<T>>* transform_scratch) {\r\n   if (output_descriptor->layout() == dnn::DataLayout::kBatchDepthYX) {\r\n@@ -3373,8 +3371,7 @@ bool CudnnSupport::DoTransformTensor(Stream* stream,\r\n \r\n template <class T>\r\n bool CudnnSupport::DoConvolveBackwardDataImpl(\r\n-    Stream* stream,\r\n-    const FilterDescriptor& filter_descriptor,\r\n+    Stream* stream, const FilterDescriptor& filter_descriptor,\r\n     const DeviceMemory<T>& filter_data,\r\n     const BatchDescriptor& output_descriptor_in,\r\n     DeviceMemory<T> backward_output_data,\r\n@@ -3422,8 +3419,9 @@ bool CudnnSupport::DoConvolveBackwardDataImpl(\r\n \r\n   if (algorithm_config.algorithm().is_default()) {\r\n     // With the default algorithm, use Cudnn's heuristics.\r\n-    auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\r\n-        dnn_handle_mutex_) -> cudnnConvolutionBwdDataAlgo_t {\r\n+    auto get_algorithm =\r\n+        [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\r\n+            dnn_handle_mutex_) -> cudnnConvolutionBwdDataAlgo_t {\r\n       cudnnConvolutionBwdDataPreference_t preference =\r\n           specify_limit ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\r\n                         : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\r\n@@ -3698,7 +3696,7 @@ bool CudnnSupport::DoConvolveBackwardFilterImpl(\r\n     // specify_limit will occur when we have a scratch allocator and it succeeds\r\n     // in allocating; otherwise, we'll fall back to the \"no workspace\" version.\r\n     auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\r\n-        dnn_handle_mutex_) {\r\n+                             dnn_handle_mutex_) {\r\n       cudnnConvolutionBwdFilterPreference_t preference =\r\n           specify_limit ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\r\n                         : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\r\n@@ -4691,8 +4689,8 @@ void initialize_cudnn() {\r\n       gpu::PluginRegistry::Instance()\r\n           ->RegisterFactory<gpu::PluginRegistry::DnnFactory>(\r\n               gpu::cuda::kCudaPlatformId, gpu::cuda::kCuDnnPlugin, \"cuDNN\",\r\n-              [](gpu::internal::StreamExecutorInterface*\r\n-                     parent) -> gpu::dnn::DnnSupport* {\r\n+              [](gpu::internal::StreamExecutorInterface* parent)\r\n+                  -> gpu::dnn::DnnSupport* {\r\n                 gpu::cuda::CUDAExecutor* cuda_executor =\r\n                     dynamic_cast<gpu::cuda::CUDAExecutor*>(parent);\r\n                 if (cuda_executor == nullptr) {\r\ndiff --git a/tensorflow/stream_executor/cuda/cuda_dnn.h b/tensorflow/stream_executor/cuda/cuda_dnn.h\r\nindex 0e5368aca8..09d248f137 100644\r\n--- a/tensorflow/stream_executor/cuda/cuda_dnn.h\r\n+++ b/tensorflow/stream_executor/cuda/cuda_dnn.h\r\n@@ -46,7 +46,7 @@ class CudnnSupport : public dnn::DnnSupport {\r\n   ~CudnnSupport() override;\r\n \r\n   port::Status Init() override;\r\n-  port::StatusOr<std::tuple<int, int, int>> GetVersion() override;\r\n+  port::StatusOr<perftools::gputools::dnn::VersionInfo> GetVersion() override;\r\n \r\n   port::StatusOr<std::unique_ptr<dnn::RnnDescriptor>> createRnnDescriptor(\r\n       int num_layers, int hidden_size, int input_size,\r\ndiff --git a/tensorflow/stream_executor/dnn.h b/tensorflow/stream_executor/dnn.h\r\nindex 3c47d2c2e8..024c5787c0 100644\r\n--- a/tensorflow/stream_executor/dnn.h\r\n+++ b/tensorflow/stream_executor/dnn.h\r\n@@ -879,6 +879,19 @@ string ElementwiseOperationString(ElementwiseOperation op);\r\n // Suite of operations typically used for implementing Deep/Convolutional Neural\r\n // Nets. Note: A false return value of an operation indicates the\r\n // implementation is not available.\r\n+class VersionInfo {\r\n+ public:\r\n+  VersionInfo(int major = 0, int minor = 0, int patch = 0)\r\n+      : major_(major), minor_(minor), patch_(patch) {}\r\n+  int maj() { return major_; }\r\n+  int min() { return minor_; }\r\n+  int patch() { return patch_; }\r\n+ private:\r\n+  int major_;\r\n+  int minor_;\r\n+  int patch_;\r\n+};\r\n+\r\n class DnnSupport {\r\n  public:\r\n   DnnSupport() {}\r\n@@ -887,7 +900,7 @@ class DnnSupport {\r\n   virtual port::Status Init() = 0;\r\n \r\n   // Gets the version of the backing library, as a {major, minor, patch} tuple.\r\n-  virtual port::StatusOr<std::tuple<int, int, int>> GetVersion() {\r\n+  virtual port::StatusOr<VersionInfo> GetVersion() {\r\n     return port::UnimplementedError(\r\n         \"DnnSupport::GetVersion not implemented on this platform.\");\r\n   }\r\n@@ -1865,10 +1878,10 @@ class DnnSupport {\r\n   //  bottom_pad: Amount to pad the input at the bottom (high Y).\r\n   //  output_data: un-owned device memory region in which to place the\r\n   //    padded result.\r\n-  virtual bool DoXYPad(Stream* stream, const dnn::BatchDescriptor &dimensions,\r\n-                       const DeviceMemory<float> &input_data,\r\n-                       int64 left_pad, int64 right_pad, int64 top_pad,\r\n-                       int64 bottom_pad, DeviceMemory<float> *output_data) = 0;\r\n+  virtual bool DoXYPad(Stream* stream, const dnn::BatchDescriptor& dimensions,\r\n+                       const DeviceMemory<float>& input_data, int64 left_pad,\r\n+                       int64 right_pad, int64 top_pad, int64 bottom_pad,\r\n+                       DeviceMemory<float>* output_data) = 0;\r\n \r\n   // Extracts a slice of the input in the X and Y dimensions. The feature_map\r\n   // dimension is unchanged.\r\n@@ -1885,10 +1898,10 @@ class DnnSupport {\r\n   //  bottom_trim: Amount to cut off the input at the bottom (high Y).\r\n   //  output_data: un-owned device memory region in which to place the\r\n   //    padded result.\r\n-  virtual bool DoXYSlice(Stream* stream, const dnn::BatchDescriptor &dimensions,\r\n-                    const DeviceMemory<float> &input_data,\r\n-                    int64 left_trim, int64 right_trim, int64 top_trim,\r\n-                    int64 bottom_trim, DeviceMemory<float> *output_data) = 0;\r\n+  virtual bool DoXYSlice(Stream* stream, const dnn::BatchDescriptor& dimensions,\r\n+                         const DeviceMemory<float>& input_data, int64 left_trim,\r\n+                         int64 right_trim, int64 top_trim, int64 bottom_trim,\r\n+                         DeviceMemory<float>* output_data) = 0;\r\n \r\n   // Grows the input tensor by replicating the X and Y dimensions. The batch and\r\n   // depth/feature_map dimensions are unchanged. Currently, the input tensor is\r\n```"}