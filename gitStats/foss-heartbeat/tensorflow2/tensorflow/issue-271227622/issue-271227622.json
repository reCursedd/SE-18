{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14250", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14250/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14250/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14250/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14250", "id": 271227622, "node_id": "MDU6SXNzdWUyNzEyMjc2MjI=", "number": 14250, "title": "[Feature request] tf.data.Dataset sort and skip buckets", "user": {"login": "georgesterpu", "id": 6018251, "node_id": "MDQ6VXNlcjYwMTgyNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6018251?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgesterpu", "html_url": "https://github.com/georgesterpu", "followers_url": "https://api.github.com/users/georgesterpu/followers", "following_url": "https://api.github.com/users/georgesterpu/following{/other_user}", "gists_url": "https://api.github.com/users/georgesterpu/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgesterpu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgesterpu/subscriptions", "organizations_url": "https://api.github.com/users/georgesterpu/orgs", "repos_url": "https://api.github.com/users/georgesterpu/repos", "events_url": "https://api.github.com/users/georgesterpu/events{/privacy}", "received_events_url": "https://api.github.com/users/georgesterpu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-11-04T21:25:34Z", "updated_at": "2018-05-22T05:31:39Z", "closed_at": "2018-01-27T20:27:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi<br>\nIt would be useful to sort the variable length inputs by their lengths in order to accelerate the training process. However I cannot find this functionality yet.</p>\n<p>In <a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308789560\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">[1]</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4805513\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/guillaumekln\">@guillaumekln</a> already suggested something similar through his code snippet, yet the requested feature was batching inputs of similar length together, regardless of the processing order of the batches, and the solution of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> in <a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-326098305\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">[2]</a> using <code>group_by_window()</code> addressed this request just fine. <em>First question</em>: Would it be possible to make the iterator return the batches in the ascending order of their ids (given by <code>key_func</code>), while maintaining the shuffling operation applied before batching?</p>\n<p>Additionally, I would like to skip the longer sentences early in training, with a length threshold that would gradually increase depending on the <code>global_step</code>. <em>Second question</em>: Could you reserve one batch id (e.g. -1) in <code>group_by_window</code> to tag the batches that will be skipped ? At the moment, it seems that all the ids are considered, even the negative values, and it would not be restrictive at all to allow only positive values (as there would still be 63 bits left to group the inputs). Thus, in <code>key_func</code> we could simply compare the input length with the threshold and return a negative value when it is above it.</p>\n<p>Apologies if both functionalities are already available, feel free to stackoverflow me.</p>", "body_text": "Hi\nIt would be useful to sort the variable length inputs by their lengths in order to accelerate the training process. However I cannot find this functionality yet.\nIn [1], @guillaumekln already suggested something similar through his code snippet, yet the requested feature was batching inputs of similar length together, regardless of the processing order of the batches, and the solution of @mrry in [2] using group_by_window() addressed this request just fine. First question: Would it be possible to make the iterator return the batches in the ascending order of their ids (given by key_func), while maintaining the shuffling operation applied before batching?\nAdditionally, I would like to skip the longer sentences early in training, with a length threshold that would gradually increase depending on the global_step. Second question: Could you reserve one batch id (e.g. -1) in group_by_window to tag the batches that will be skipped ? At the moment, it seems that all the ids are considered, even the negative values, and it would not be restrictive at all to allow only positive values (as there would still be 63 bits left to group the inputs). Thus, in key_func we could simply compare the input length with the threshold and return a negative value when it is above it.\nApologies if both functionalities are already available, feel free to stackoverflow me.", "body": "Hi\r\nIt would be useful to sort the variable length inputs by their lengths in order to accelerate the training process. However I cannot find this functionality yet.\r\n\r\nIn [[1]](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308789560), @guillaumekln already suggested something similar through his code snippet, yet the requested feature was batching inputs of similar length together, regardless of the processing order of the batches, and the solution of @mrry in [[2]](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-326098305) using `group_by_window()` addressed this request just fine. _First question_: Would it be possible to make the iterator return the batches in the ascending order of their ids (given by `key_func`), while maintaining the shuffling operation applied before batching?\r\n\r\nAdditionally, I would like to skip the longer sentences early in training, with a length threshold that would gradually increase depending on the `global_step`. _Second question_: Could you reserve one batch id (e.g. -1) in `group_by_window` to tag the batches that will be skipped ? At the moment, it seems that all the ids are considered, even the negative values, and it would not be restrictive at all to allow only positive values (as there would still be 63 bits left to group the inputs). Thus, in `key_func` we could simply compare the input length with the threshold and return a negative value when it is above it.\r\n\r\nApologies if both functionalities are already available, feel free to stackoverflow me."}