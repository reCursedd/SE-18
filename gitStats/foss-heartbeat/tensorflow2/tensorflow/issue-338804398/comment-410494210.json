{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/410494210", "html_url": "https://github.com/tensorflow/tensorflow/issues/20585#issuecomment-410494210", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20585", "id": 410494210, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDQ5NDIxMA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-05T04:05:56Z", "updated_at": "2018-08-05T04:05:56Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">It still looks like you've got an error in either creating or running the\ngraph that you're just not catching. See if you're accidentally piping or\nthrowing out your stderr, or maybe it's in another log file?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sat, Aug 4, 2018, 5:56 PM Jun Shi ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; I just realized there could still\n be a bug in tensorflow.\n Indeed, the code above works perfectly. Note I used the following\n \"lookup-then-transpose\" to handle time_major:\n\n         encoder_emb_inp = tf.nn.embedding_lookup(\n            embedding_encoder, source)\n         if FLAGS.time_major:\n             # swap (batch, time) axes if time_major\n             encoder_emb_inp=tf.transpose(encoder_emb_inp, [1, 0, 2])\n\n Another approach could be \"transpose-then-lookup\":\n\n       source = np.ones((batch_size, src_len), dtype=int)\n       if FLAGS.time_major:\n           source = tf.transpose(source)\n       ...\n       with tf.variable_scope(\"encoder\") as decoder_scope:\n         encoder_emb_inp = tf.nn.embedding_lookup(\n            embedding_encoder, source)\n\n the latter causes non-chief worker to hang. There is no error message, the\n worker just keeps starting master sessions. I found the latter hung because\n it is the approach taken by tensorflow/nmt, which you co-authored (thanks\n for the great contribution, I benefit enormously from the nmt code). When I\n try to extended nmt to distributed setting, I found the issue.\n\n Do you know why \"transpose-then-lookup\" causes the non-chief worker to\n hang? I attached the \"transpose-then-lookup\" code for your reference.\n\n import argparseimport sys\n import tensorflow as tfimport numpy as npimport timeFLAGS = None\n def main(_):\n   ps_hosts = FLAGS.ps_hosts.split(\",\")\n   worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n   # Create a cluster from the parameter server and worker hosts.\n   cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n   # Create and start a server for the local task.\n   server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n   if FLAGS.job_name == \"ps\":\n     server.join()\n   elif FLAGS.job_name == \"worker\":\n\n     # Assigns ops to the local worker by default.\n     with tf.device(tf.train.replica_device_setter(\n         worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n         cluster=cluster)):\n\n       # Build model...\n\n       batch_size = 24\n       src_len = 25\n       source = np.ones((batch_size, src_len), dtype=int)\n\n       if FLAGS.time_major:\n           source = tf.transpose(source)\n\n       embedding_encoder = tf.get_variable(\n           \"embedding_encoder\", (99, 128), tf.float32)\n\n       embedding_decoder = tf.get_variable(\n           \"embedding_decoder\", (99, 128), tf.float32)\n\n       with tf.variable_scope(\"encoder\") as decoder_scope:\n         encoder_emb_inp = tf.nn.embedding_lookup(\n            embedding_encoder, source)\n\n         encoder_cell = tf.contrib.rnn.BasicLSTMCell(\n           num_units=256,\n           forget_bias=1.0)\n         sequence_length=tf.fill([batch_size], src_len)\n\n         encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n             encoder_cell,\n             encoder_emb_inp,\n             dtype=tf.float32,\n             sequence_length=sequence_length,\n             time_major=FLAGS.time_major,\n             swap_memory=True)\n\n       with tf.variable_scope(\"decoder\") as decoder_scope:\n         target_input = np.ones((batch_size, src_len), dtype=int)\n\n         if FLAGS.time_major:\n             target_input = tf.transpose(target_input)\n\n         decoder_emb_inp = tf.nn.embedding_lookup(\n             embedding_decoder, target_input)\n\n         # Helper\n         helper = tf.contrib.seq2seq.TrainingHelper(\n             decoder_emb_inp, sequence_length,\n             time_major=FLAGS.time_major)\n\n         decoder_cell = tf.contrib.rnn.BasicLSTMCell(\n             num_units=256,\n             forget_bias=1.0)\n         decoder_initial_state = encoder_state\n\n         # Decoder\n         my_decoder = tf.contrib.seq2seq.BasicDecoder(\n             decoder_cell,\n             helper,\n             decoder_initial_state)\n\n         # Dynamic decoding\n         decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n             my_decoder,\n             output_time_major=FLAGS.time_major,\n             swap_memory=True,\n             scope=decoder_scope)\n\n       global_step = tf.train.get_or_create_global_step()\n\n       train_op = decoder_outputs\n\n     # The StopAtStepHook handles stopping after running given steps.\n     hooks=[tf.train.StopAtStepHook(last_step=100)]\n\n     # The MonitoredTrainingSession takes care of session initialization,\n     # restoring from a checkpoint, saving to a checkpoint, and closing when done\n     # or an error occurs.\n     with tf.train.MonitoredTrainingSession(master=server.target,\n                                            is_chief=(FLAGS.task_index == 0),\n                                            checkpoint_dir=FLAGS.out_dir,\n                                            hooks=hooks) as mon_sess:\n       # This is an infinite while loop since global_step does not increment at all.\n       # In a real training, global_step is passed to an optimizer, then increments\n       # after each step.\n       while not mon_sess.should_stop():\n         x = mon_sess.run(train_op)\n         global_step_val = global_step.eval(session=mon_sess)\n         print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\n\n if __name__ == \"__main__\":\n   parser = argparse.ArgumentParser()\n   parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n   # Flags for defining the tf.train.ClusterSpec\n   parser.add_argument(\n       \"--ps_hosts\",\n       type=str,\n       default=\"\",\n       help=\"Comma-separated list of hostname:port pairs\"\n   )\n   parser.add_argument(\n       \"--worker_hosts\",\n       type=str,\n       default=\"\",\n       help=\"Comma-separated list of hostname:port pairs\"\n   )\n   parser.add_argument(\n       \"--job_name\",\n       type=str,\n       default=\"\",\n       help=\"One of 'ps', 'worker'\"\n   )\n   # Flags for defining the tf.train.Server\n   parser.add_argument(\n       \"--task_index\",\n       type=int,\n       default=0,\n       help=\"Index of task within the job\"\n   )\n   parser.add_argument(\n       \"--out_dir\",\n       type=str,\n       default=\"\",\n       help=\"output dir\"\n   )\n\n   parser.add_argument(\n       \"--time_major\",\n       action='store_true',\n       help=\"time major\"\n   )\n   FLAGS, unparsed = parser.parse_known_args()\n   tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"338804398\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20585\" href=\"https://github.com/tensorflow/tensorflow/issues/20585#issuecomment-410487365\">#20585 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim4SaQE8gkdYzMFe64GB_AqTqlNqMks5uNkLTgaJpZM4VE4Qr\">https://github.com/notifications/unsubscribe-auth/ABtim4SaQE8gkdYzMFe64GB_AqTqlNqMks5uNkLTgaJpZM4VE4Qr</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "It still looks like you've got an error in either creating or running the\ngraph that you're just not catching. See if you're accidentally piping or\nthrowing out your stderr, or maybe it's in another log file?\n\u2026\nOn Sat, Aug 4, 2018, 5:56 PM Jun Shi ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> I just realized there could still\n be a bug in tensorflow.\n Indeed, the code above works perfectly. Note I used the following\n \"lookup-then-transpose\" to handle time_major:\n\n         encoder_emb_inp = tf.nn.embedding_lookup(\n            embedding_encoder, source)\n         if FLAGS.time_major:\n             # swap (batch, time) axes if time_major\n             encoder_emb_inp=tf.transpose(encoder_emb_inp, [1, 0, 2])\n\n Another approach could be \"transpose-then-lookup\":\n\n       source = np.ones((batch_size, src_len), dtype=int)\n       if FLAGS.time_major:\n           source = tf.transpose(source)\n       ...\n       with tf.variable_scope(\"encoder\") as decoder_scope:\n         encoder_emb_inp = tf.nn.embedding_lookup(\n            embedding_encoder, source)\n\n the latter causes non-chief worker to hang. There is no error message, the\n worker just keeps starting master sessions. I found the latter hung because\n it is the approach taken by tensorflow/nmt, which you co-authored (thanks\n for the great contribution, I benefit enormously from the nmt code). When I\n try to extended nmt to distributed setting, I found the issue.\n\n Do you know why \"transpose-then-lookup\" causes the non-chief worker to\n hang? I attached the \"transpose-then-lookup\" code for your reference.\n\n import argparseimport sys\n import tensorflow as tfimport numpy as npimport timeFLAGS = None\n def main(_):\n   ps_hosts = FLAGS.ps_hosts.split(\",\")\n   worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n   # Create a cluster from the parameter server and worker hosts.\n   cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n   # Create and start a server for the local task.\n   server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n   if FLAGS.job_name == \"ps\":\n     server.join()\n   elif FLAGS.job_name == \"worker\":\n\n     # Assigns ops to the local worker by default.\n     with tf.device(tf.train.replica_device_setter(\n         worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n         cluster=cluster)):\n\n       # Build model...\n\n       batch_size = 24\n       src_len = 25\n       source = np.ones((batch_size, src_len), dtype=int)\n\n       if FLAGS.time_major:\n           source = tf.transpose(source)\n\n       embedding_encoder = tf.get_variable(\n           \"embedding_encoder\", (99, 128), tf.float32)\n\n       embedding_decoder = tf.get_variable(\n           \"embedding_decoder\", (99, 128), tf.float32)\n\n       with tf.variable_scope(\"encoder\") as decoder_scope:\n         encoder_emb_inp = tf.nn.embedding_lookup(\n            embedding_encoder, source)\n\n         encoder_cell = tf.contrib.rnn.BasicLSTMCell(\n           num_units=256,\n           forget_bias=1.0)\n         sequence_length=tf.fill([batch_size], src_len)\n\n         encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n             encoder_cell,\n             encoder_emb_inp,\n             dtype=tf.float32,\n             sequence_length=sequence_length,\n             time_major=FLAGS.time_major,\n             swap_memory=True)\n\n       with tf.variable_scope(\"decoder\") as decoder_scope:\n         target_input = np.ones((batch_size, src_len), dtype=int)\n\n         if FLAGS.time_major:\n             target_input = tf.transpose(target_input)\n\n         decoder_emb_inp = tf.nn.embedding_lookup(\n             embedding_decoder, target_input)\n\n         # Helper\n         helper = tf.contrib.seq2seq.TrainingHelper(\n             decoder_emb_inp, sequence_length,\n             time_major=FLAGS.time_major)\n\n         decoder_cell = tf.contrib.rnn.BasicLSTMCell(\n             num_units=256,\n             forget_bias=1.0)\n         decoder_initial_state = encoder_state\n\n         # Decoder\n         my_decoder = tf.contrib.seq2seq.BasicDecoder(\n             decoder_cell,\n             helper,\n             decoder_initial_state)\n\n         # Dynamic decoding\n         decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n             my_decoder,\n             output_time_major=FLAGS.time_major,\n             swap_memory=True,\n             scope=decoder_scope)\n\n       global_step = tf.train.get_or_create_global_step()\n\n       train_op = decoder_outputs\n\n     # The StopAtStepHook handles stopping after running given steps.\n     hooks=[tf.train.StopAtStepHook(last_step=100)]\n\n     # The MonitoredTrainingSession takes care of session initialization,\n     # restoring from a checkpoint, saving to a checkpoint, and closing when done\n     # or an error occurs.\n     with tf.train.MonitoredTrainingSession(master=server.target,\n                                            is_chief=(FLAGS.task_index == 0),\n                                            checkpoint_dir=FLAGS.out_dir,\n                                            hooks=hooks) as mon_sess:\n       # This is an infinite while loop since global_step does not increment at all.\n       # In a real training, global_step is passed to an optimizer, then increments\n       # after each step.\n       while not mon_sess.should_stop():\n         x = mon_sess.run(train_op)\n         global_step_val = global_step.eval(session=mon_sess)\n         print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\n\n if __name__ == \"__main__\":\n   parser = argparse.ArgumentParser()\n   parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n   # Flags for defining the tf.train.ClusterSpec\n   parser.add_argument(\n       \"--ps_hosts\",\n       type=str,\n       default=\"\",\n       help=\"Comma-separated list of hostname:port pairs\"\n   )\n   parser.add_argument(\n       \"--worker_hosts\",\n       type=str,\n       default=\"\",\n       help=\"Comma-separated list of hostname:port pairs\"\n   )\n   parser.add_argument(\n       \"--job_name\",\n       type=str,\n       default=\"\",\n       help=\"One of 'ps', 'worker'\"\n   )\n   # Flags for defining the tf.train.Server\n   parser.add_argument(\n       \"--task_index\",\n       type=int,\n       default=0,\n       help=\"Index of task within the job\"\n   )\n   parser.add_argument(\n       \"--out_dir\",\n       type=str,\n       default=\"\",\n       help=\"output dir\"\n   )\n\n   parser.add_argument(\n       \"--time_major\",\n       action='store_true',\n       help=\"time major\"\n   )\n   FLAGS, unparsed = parser.parse_known_args()\n   tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#20585 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim4SaQE8gkdYzMFe64GB_AqTqlNqMks5uNkLTgaJpZM4VE4Qr>\n .", "body": "It still looks like you've got an error in either creating or running the\ngraph that you're just not catching. See if you're accidentally piping or\nthrowing out your stderr, or maybe it's in another log file?\n\nOn Sat, Aug 4, 2018, 5:56 PM Jun Shi <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> I just realized there could still\n> be a bug in tensorflow.\n> Indeed, the code above works perfectly. Note I used the following\n> \"lookup-then-transpose\" to handle time_major:\n>\n>         encoder_emb_inp = tf.nn.embedding_lookup(\n>            embedding_encoder, source)\n>         if FLAGS.time_major:\n>             # swap (batch, time) axes if time_major\n>             encoder_emb_inp=tf.transpose(encoder_emb_inp, [1, 0, 2])\n>\n> Another approach could be \"transpose-then-lookup\":\n>\n>       source = np.ones((batch_size, src_len), dtype=int)\n>       if FLAGS.time_major:\n>           source = tf.transpose(source)\n>       ...\n>       with tf.variable_scope(\"encoder\") as decoder_scope:\n>         encoder_emb_inp = tf.nn.embedding_lookup(\n>            embedding_encoder, source)\n>\n> the latter causes non-chief worker to hang. There is no error message, the\n> worker just keeps starting master sessions. I found the latter hung because\n> it is the approach taken by tensorflow/nmt, which you co-authored (thanks\n> for the great contribution, I benefit enormously from the nmt code). When I\n> try to extended nmt to distributed setting, I found the issue.\n>\n> Do you know why \"transpose-then-lookup\" causes the non-chief worker to\n> hang? I attached the \"transpose-then-lookup\" code for your reference.\n>\n> import argparseimport sys\n> import tensorflow as tfimport numpy as npimport timeFLAGS = None\n> def main(_):\n>   ps_hosts = FLAGS.ps_hosts.split(\",\")\n>   worker_hosts = FLAGS.worker_hosts.split(\",\")\n>\n>   # Create a cluster from the parameter server and worker hosts.\n>   cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n>\n>   # Create and start a server for the local task.\n>   server = tf.train.Server(cluster,\n>                            job_name=FLAGS.job_name,\n>                            task_index=FLAGS.task_index)\n>   if FLAGS.job_name == \"ps\":\n>     server.join()\n>   elif FLAGS.job_name == \"worker\":\n>\n>     # Assigns ops to the local worker by default.\n>     with tf.device(tf.train.replica_device_setter(\n>         worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n>         cluster=cluster)):\n>\n>       # Build model...\n>\n>       batch_size = 24\n>       src_len = 25\n>       source = np.ones((batch_size, src_len), dtype=int)\n>\n>       if FLAGS.time_major:\n>           source = tf.transpose(source)\n>\n>       embedding_encoder = tf.get_variable(\n>           \"embedding_encoder\", (99, 128), tf.float32)\n>\n>       embedding_decoder = tf.get_variable(\n>           \"embedding_decoder\", (99, 128), tf.float32)\n>\n>       with tf.variable_scope(\"encoder\") as decoder_scope:\n>         encoder_emb_inp = tf.nn.embedding_lookup(\n>            embedding_encoder, source)\n>\n>         encoder_cell = tf.contrib.rnn.BasicLSTMCell(\n>           num_units=256,\n>           forget_bias=1.0)\n>         sequence_length=tf.fill([batch_size], src_len)\n>\n>         encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n>             encoder_cell,\n>             encoder_emb_inp,\n>             dtype=tf.float32,\n>             sequence_length=sequence_length,\n>             time_major=FLAGS.time_major,\n>             swap_memory=True)\n>\n>       with tf.variable_scope(\"decoder\") as decoder_scope:\n>         target_input = np.ones((batch_size, src_len), dtype=int)\n>\n>         if FLAGS.time_major:\n>             target_input = tf.transpose(target_input)\n>\n>         decoder_emb_inp = tf.nn.embedding_lookup(\n>             embedding_decoder, target_input)\n>\n>         # Helper\n>         helper = tf.contrib.seq2seq.TrainingHelper(\n>             decoder_emb_inp, sequence_length,\n>             time_major=FLAGS.time_major)\n>\n>         decoder_cell = tf.contrib.rnn.BasicLSTMCell(\n>             num_units=256,\n>             forget_bias=1.0)\n>         decoder_initial_state = encoder_state\n>\n>         # Decoder\n>         my_decoder = tf.contrib.seq2seq.BasicDecoder(\n>             decoder_cell,\n>             helper,\n>             decoder_initial_state)\n>\n>         # Dynamic decoding\n>         decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n>             my_decoder,\n>             output_time_major=FLAGS.time_major,\n>             swap_memory=True,\n>             scope=decoder_scope)\n>\n>       global_step = tf.train.get_or_create_global_step()\n>\n>       train_op = decoder_outputs\n>\n>     # The StopAtStepHook handles stopping after running given steps.\n>     hooks=[tf.train.StopAtStepHook(last_step=100)]\n>\n>     # The MonitoredTrainingSession takes care of session initialization,\n>     # restoring from a checkpoint, saving to a checkpoint, and closing when done\n>     # or an error occurs.\n>     with tf.train.MonitoredTrainingSession(master=server.target,\n>                                            is_chief=(FLAGS.task_index == 0),\n>                                            checkpoint_dir=FLAGS.out_dir,\n>                                            hooks=hooks) as mon_sess:\n>       # This is an infinite while loop since global_step does not increment at all.\n>       # In a real training, global_step is passed to an optimizer, then increments\n>       # after each step.\n>       while not mon_sess.should_stop():\n>         x = mon_sess.run(train_op)\n>         global_step_val = global_step.eval(session=mon_sess)\n>         print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\n>\n> if __name__ == \"__main__\":\n>   parser = argparse.ArgumentParser()\n>   parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n>   # Flags for defining the tf.train.ClusterSpec\n>   parser.add_argument(\n>       \"--ps_hosts\",\n>       type=str,\n>       default=\"\",\n>       help=\"Comma-separated list of hostname:port pairs\"\n>   )\n>   parser.add_argument(\n>       \"--worker_hosts\",\n>       type=str,\n>       default=\"\",\n>       help=\"Comma-separated list of hostname:port pairs\"\n>   )\n>   parser.add_argument(\n>       \"--job_name\",\n>       type=str,\n>       default=\"\",\n>       help=\"One of 'ps', 'worker'\"\n>   )\n>   # Flags for defining the tf.train.Server\n>   parser.add_argument(\n>       \"--task_index\",\n>       type=int,\n>       default=0,\n>       help=\"Index of task within the job\"\n>   )\n>   parser.add_argument(\n>       \"--out_dir\",\n>       type=str,\n>       default=\"\",\n>       help=\"output dir\"\n>   )\n>\n>   parser.add_argument(\n>       \"--time_major\",\n>       action='store_true',\n>       help=\"time major\"\n>   )\n>   FLAGS, unparsed = parser.parse_known_args()\n>   tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20585#issuecomment-410487365>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4SaQE8gkdYzMFe64GB_AqTqlNqMks5uNkLTgaJpZM4VE4Qr>\n> .\n>\n"}