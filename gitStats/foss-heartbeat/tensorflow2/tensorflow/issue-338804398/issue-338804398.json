{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20585", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20585/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20585/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20585/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20585", "id": 338804398, "node_id": "MDU6SXNzdWUzMzg4MDQzOTg=", "number": 20585, "title": "non-chief workers hang in training distributed seq2seq model when time-major is true", "user": {"login": "junshi15", "id": 12075848, "node_id": "MDQ6VXNlcjEyMDc1ODQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/12075848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junshi15", "html_url": "https://github.com/junshi15", "followers_url": "https://api.github.com/users/junshi15/followers", "following_url": "https://api.github.com/users/junshi15/following{/other_user}", "gists_url": "https://api.github.com/users/junshi15/gists{/gist_id}", "starred_url": "https://api.github.com/users/junshi15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junshi15/subscriptions", "organizations_url": "https://api.github.com/users/junshi15/orgs", "repos_url": "https://api.github.com/users/junshi15/repos", "events_url": "https://api.github.com/users/junshi15/events{/privacy}", "received_events_url": "https://api.github.com/users/junshi15/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-07-06T04:51:35Z", "updated_at": "2018-11-20T13:28:59Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: MacOs High Sierra version 10.13.3 (also found in Linux RHEL 6.5)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.7.0 (also found in 1.8.0)</li>\n<li><strong>Python version</strong>: 2.7.10</li>\n<li><strong>Bazel version (if compiling from source)</strong>:N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:N/A</li>\n<li><strong>CUDA/cuDNN version</strong>:N/A CPU version</li>\n<li><strong>GPU model and memory</strong>:N/A</li>\n<li><strong>Exact command to reproduce</strong>:<br>\npython nonchief_hang.py --out_dir=/tmp/rnn/work0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=worker --time_major<br>\npython nonchief_hang.py --out_dir=/tmp/rnn/work1 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=1 --job_name=worker --time_major<br>\npython nonchief_hang.py --out_dir=/tmp/rnn/ps0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=ps --time_major</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.<br>\nIn a distributed seq2seq model (see a short example below), when time-major is set to true, the non-chief workers hang, further debugging showed that the non-chief workers were waiting for embedding matrices to be initialized. The chief worker trains without issues.</p>\n<p>If we remove (\"--time_major\") from the command line, then non-chief workers run fine.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.<br>\nThis is a simple seq2seq model following the implementation of <a href=\"https://github.com/tensorflow/nmt\">tensorflow/nmt</a>, with optimizer removed.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> sys\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">_</span>):\n  ps_hosts <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.ps_hosts.split(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>,<span class=\"pl-pds\">\"</span></span>)\n  worker_hosts <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.worker_hosts.split(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>,<span class=\"pl-pds\">\"</span></span>)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a cluster from the parameter server and worker hosts.</span>\n  cluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>: ps_hosts, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: worker_hosts})\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create and start a server for the local task.</span>\n  server <span class=\"pl-k\">=</span> tf.train.Server(cluster,\n                           <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.job_name,\n                           <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.task_index)\n  <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>:\n    server.join()\n  <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">FLAGS</span>.job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>:\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assigns ops to the local worker by default.</span>\n    <span class=\"pl-k\">with</span> tf.device(tf.train.replica_device_setter(\n        <span class=\"pl-v\">worker_device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/job:worker/task:<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">FLAGS</span>.task_index,\n        <span class=\"pl-v\">cluster</span><span class=\"pl-k\">=</span>cluster)):\n\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> Build model...</span>\n      batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">24</span>\n      src_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">25</span>\n      source <span class=\"pl-k\">=</span> np.ones((batch_size, src_len), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)\n\n      embedding_encoder <span class=\"pl-k\">=</span> tf.get_variable(\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding_encoder<span class=\"pl-pds\">\"</span></span>, (<span class=\"pl-c1\">99</span>, <span class=\"pl-c1\">128</span>), tf.float32)\n\n      embedding_decoder <span class=\"pl-k\">=</span> tf.get_variable(\n          <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding_decoder<span class=\"pl-pds\">\"</span></span>, (<span class=\"pl-c1\">99</span>, <span class=\"pl-c1\">128</span>), tf.float32)\n\n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>encoder<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> decoder_scope:\n        encoder_emb_inp <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(\n           embedding_encoder, source)\n\n        encoder_cell <span class=\"pl-k\">=</span> tf.contrib.rnn.BasicLSTMCell(\n          <span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>,\n          <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.time_major:\n            sequence_length<span class=\"pl-k\">=</span>np.ones(src_len, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)<span class=\"pl-k\">*</span>batch_size\n        <span class=\"pl-k\">else</span>:\n            sequence_length<span class=\"pl-k\">=</span>np.ones(batch_size, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)<span class=\"pl-k\">*</span>src_len\n \n        encoder_outputs, encoder_state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n            encoder_cell,\n            encoder_emb_inp,\n            <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n            <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_length,\n            <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.time_major,\n            <span class=\"pl-v\">swap_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>decoder<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> decoder_scope:\n        target_input <span class=\"pl-k\">=</span> np.ones((batch_size, src_len), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)\n        decoder_emb_inp <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(\n            embedding_decoder, target_input) \n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Helper</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.time_major:\n           dummy <span class=\"pl-k\">=</span> tf.fill([src_len], batch_size)\n        <span class=\"pl-k\">else</span>:\n           dummy <span class=\"pl-k\">=</span> tf.fill([batch_size], src_len)\n\n        helper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(\n            decoder_emb_inp, dummy,\n            <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.time_major)\n\n        decoder_cell <span class=\"pl-k\">=</span> tf.contrib.rnn.BasicLSTMCell(\n            <span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>,\n            <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n        decoder_initial_state <span class=\"pl-k\">=</span> encoder_state\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Decoder</span>\n        my_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(\n            decoder_cell,\n            helper,\n            decoder_initial_state)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Dynamic decoding</span>\n        decoder_outputs, final_context_state, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n            my_decoder,\n            <span class=\"pl-v\">output_time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.time_major,\n            <span class=\"pl-v\">swap_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>decoder_scope)\n\n      global_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\n      \n      train_op <span class=\"pl-k\">=</span> decoder_outputs\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The StopAtStepHook handles stopping after running given steps.</span>\n    hooks<span class=\"pl-k\">=</span>[tf.train.StopAtStepHook(<span class=\"pl-v\">last_step</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>)]\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The MonitoredTrainingSession takes care of session initialization,</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> restoring from a checkpoint, saving to a checkpoint, and closing when done</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> or an error occurs.</span>\n    <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(<span class=\"pl-v\">master</span><span class=\"pl-k\">=</span>server.target,\n                                           <span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">FLAGS</span>.task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>),\n                                           <span class=\"pl-v\">checkpoint_dir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.out_dir,\n                                           <span class=\"pl-v\">hooks</span><span class=\"pl-k\">=</span>hooks) <span class=\"pl-k\">as</span> mon_sess:\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> This is an infinite while loop since global_step does not increment at all.</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> In a real training, global_step is passed to an optimizer, then increments</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> after each step.</span>\n      <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> mon_sess.should_stop():\n        x <span class=\"pl-k\">=</span> mon_sess.run(train_op)\n        global_step_val <span class=\"pl-k\">=</span> global_step.eval(<span class=\"pl-v\">session</span><span class=\"pl-k\">=</span>mon_sess)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_step = <span class=\"pl-c1\">{0}</span>, time_major is <span class=\"pl-c1\">{1}</span><span class=\"pl-pds\">'</span></span>.format(global_step_val, <span class=\"pl-c1\">FLAGS</span>.time_major))\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n  parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n  parser.register(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bool<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">v</span>: v.lower() <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Flags for defining the tf.train.ClusterSpec</span>\n  parser.add_argument(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--ps_hosts<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n      <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Comma-separated list of hostname:port pairs<span class=\"pl-pds\">\"</span></span>\n  )\n  parser.add_argument(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--worker_hosts<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n      <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Comma-separated list of hostname:port pairs<span class=\"pl-pds\">\"</span></span>\n  )\n  parser.add_argument(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--job_name<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n      <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>One of 'ps', 'worker'<span class=\"pl-pds\">\"</span></span>\n  )\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Flags for defining the tf.train.Server</span>\n  parser.add_argument(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--task_index<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>,\n      <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>,\n      <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Index of task within the job<span class=\"pl-pds\">\"</span></span>\n  )\n  parser.add_argument(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--out_dir<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n      <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output dir<span class=\"pl-pds\">\"</span></span>\n  )\n\n  parser.add_argument(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--time_major<span class=\"pl-pds\">\"</span></span>,\n      <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>,\n      <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time major<span class=\"pl-pds\">\"</span></span>\n  )\n  <span class=\"pl-c1\">FLAGS</span>, unparsed <span class=\"pl-k\">=</span> parser.parse_known_args()\n  tf.app.run(<span class=\"pl-v\">main</span><span class=\"pl-k\">=</span>main, <span class=\"pl-v\">argv</span><span class=\"pl-k\">=</span>[sys.argv[<span class=\"pl-c1\">0</span>]] <span class=\"pl-k\">+</span> unparsed)</pre></div>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs High Sierra version 10.13.3 (also found in Linux RHEL 6.5)\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.7.0 (also found in 1.8.0)\nPython version: 2.7.10\nBazel version (if compiling from source):N/A\nGCC/Compiler version (if compiling from source):N/A\nCUDA/cuDNN version:N/A CPU version\nGPU model and memory:N/A\nExact command to reproduce:\npython nonchief_hang.py --out_dir=/tmp/rnn/work0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=worker --time_major\npython nonchief_hang.py --out_dir=/tmp/rnn/work1 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=1 --job_name=worker --time_major\npython nonchief_hang.py --out_dir=/tmp/rnn/ps0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=ps --time_major\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nIn a distributed seq2seq model (see a short example below), when time-major is set to true, the non-chief workers hang, further debugging showed that the non-chief workers were waiting for embedding matrices to be initialized. The chief worker trains without issues.\nIf we remove (\"--time_major\") from the command line, then non-chief workers run fine.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nThis is a simple seq2seq model following the implementation of tensorflow/nmt, with optimizer removed.\nimport argparse\nimport sys\n\nimport tensorflow as tf\nimport numpy as np\nimport time\nFLAGS = None\n\n\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n      # Build model...\n      batch_size = 24\n      src_len = 25\n      source = np.ones((batch_size, src_len), dtype=int)\n\n      embedding_encoder = tf.get_variable(\n          \"embedding_encoder\", (99, 128), tf.float32)\n\n      embedding_decoder = tf.get_variable(\n          \"embedding_decoder\", (99, 128), tf.float32)\n\n      with tf.variable_scope(\"encoder\") as decoder_scope:\n        encoder_emb_inp = tf.nn.embedding_lookup(\n           embedding_encoder, source)\n\n        encoder_cell = tf.contrib.rnn.BasicLSTMCell(\n          num_units=256,\n          forget_bias=1.0)\n\n        if FLAGS.time_major:\n            sequence_length=np.ones(src_len, dtype=int)*batch_size\n        else:\n            sequence_length=np.ones(batch_size, dtype=int)*src_len\n \n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n            encoder_cell,\n            encoder_emb_inp,\n            dtype=tf.float32,\n            sequence_length=sequence_length,\n            time_major=FLAGS.time_major,\n            swap_memory=True)\n\n      with tf.variable_scope(\"decoder\") as decoder_scope:\n        target_input = np.ones((batch_size, src_len), dtype=int)\n        decoder_emb_inp = tf.nn.embedding_lookup(\n            embedding_decoder, target_input) \n\n        # Helper\n        if FLAGS.time_major:\n           dummy = tf.fill([src_len], batch_size)\n        else:\n           dummy = tf.fill([batch_size], src_len)\n\n        helper = tf.contrib.seq2seq.TrainingHelper(\n            decoder_emb_inp, dummy,\n            time_major=FLAGS.time_major)\n\n        decoder_cell = tf.contrib.rnn.BasicLSTMCell(\n            num_units=256,\n            forget_bias=1.0)\n        decoder_initial_state = encoder_state\n\n        # Decoder\n        my_decoder = tf.contrib.seq2seq.BasicDecoder(\n            decoder_cell,\n            helper,\n            decoder_initial_state)\n\n        # Dynamic decoding\n        decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n            my_decoder,\n            output_time_major=FLAGS.time_major,\n            swap_memory=True,\n            scope=decoder_scope)\n\n      global_step = tf.train.get_or_create_global_step()\n      \n      train_op = decoder_outputs\n\n    # The StopAtStepHook handles stopping after running given steps.\n    hooks=[tf.train.StopAtStepHook(last_step=100)]\n\n    # The MonitoredTrainingSession takes care of session initialization,\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\n    # or an error occurs.\n    with tf.train.MonitoredTrainingSession(master=server.target,\n                                           is_chief=(FLAGS.task_index == 0),\n                                           checkpoint_dir=FLAGS.out_dir,\n                                           hooks=hooks) as mon_sess:\n      # This is an infinite while loop since global_step does not increment at all.\n      # In a real training, global_step is passed to an optimizer, then increments\n      # after each step.\n      while not mon_sess.should_stop():\n        x = mon_sess.run(train_op)\n        global_step_val = global_step.eval(session=mon_sess)\n        print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser()\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n  # Flags for defining the tf.train.ClusterSpec\n  parser.add_argument(\n      \"--ps_hosts\",\n      type=str,\n      default=\"\",\n      help=\"Comma-separated list of hostname:port pairs\"\n  )\n  parser.add_argument(\n      \"--worker_hosts\",\n      type=str,\n      default=\"\",\n      help=\"Comma-separated list of hostname:port pairs\"\n  )\n  parser.add_argument(\n      \"--job_name\",\n      type=str,\n      default=\"\",\n      help=\"One of 'ps', 'worker'\"\n  )\n  # Flags for defining the tf.train.Server\n  parser.add_argument(\n      \"--task_index\",\n      type=int,\n      default=0,\n      help=\"Index of task within the job\"\n  )\n  parser.add_argument(\n      \"--out_dir\",\n      type=str,\n      default=\"\",\n      help=\"output dir\"\n  )\n\n  parser.add_argument(\n      \"--time_major\",\n      action='store_true',\n      help=\"time major\"\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs High Sierra version 10.13.3 (also found in Linux RHEL 6.5)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0 (also found in 1.8.0)\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A CPU version\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:\r\npython nonchief_hang.py --out_dir=/tmp/rnn/work0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=worker --time_major\r\npython nonchief_hang.py --out_dir=/tmp/rnn/work1 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=1 --job_name=worker --time_major\r\npython nonchief_hang.py --out_dir=/tmp/rnn/ps0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=ps --time_major\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nIn a distributed seq2seq model (see a short example below), when time-major is set to true, the non-chief workers hang, further debugging showed that the non-chief workers were waiting for embedding matrices to be initialized. The chief worker trains without issues.\r\n\r\nIf we remove (\"--time_major\") from the command line, then non-chief workers run fine.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nThis is a simple seq2seq model following the implementation of [tensorflow/nmt](https://github.com/tensorflow/nmt), with optimizer removed.\r\n\r\n```python\r\nimport argparse\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nFLAGS = None\r\n\r\n\r\ndef main(_):\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n\r\n    # Assigns ops to the local worker by default.\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n\r\n      # Build model...\r\n      batch_size = 24\r\n      src_len = 25\r\n      source = np.ones((batch_size, src_len), dtype=int)\r\n\r\n      embedding_encoder = tf.get_variable(\r\n          \"embedding_encoder\", (99, 128), tf.float32)\r\n\r\n      embedding_decoder = tf.get_variable(\r\n          \"embedding_decoder\", (99, 128), tf.float32)\r\n\r\n      with tf.variable_scope(\"encoder\") as decoder_scope:\r\n        encoder_emb_inp = tf.nn.embedding_lookup(\r\n           embedding_encoder, source)\r\n\r\n        encoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n          num_units=256,\r\n          forget_bias=1.0)\r\n\r\n        if FLAGS.time_major:\r\n            sequence_length=np.ones(src_len, dtype=int)*batch_size\r\n        else:\r\n            sequence_length=np.ones(batch_size, dtype=int)*src_len\r\n \r\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n            encoder_cell,\r\n            encoder_emb_inp,\r\n            dtype=tf.float32,\r\n            sequence_length=sequence_length,\r\n            time_major=FLAGS.time_major,\r\n            swap_memory=True)\r\n\r\n      with tf.variable_scope(\"decoder\") as decoder_scope:\r\n        target_input = np.ones((batch_size, src_len), dtype=int)\r\n        decoder_emb_inp = tf.nn.embedding_lookup(\r\n            embedding_decoder, target_input) \r\n\r\n        # Helper\r\n        if FLAGS.time_major:\r\n           dummy = tf.fill([src_len], batch_size)\r\n        else:\r\n           dummy = tf.fill([batch_size], src_len)\r\n\r\n        helper = tf.contrib.seq2seq.TrainingHelper(\r\n            decoder_emb_inp, dummy,\r\n            time_major=FLAGS.time_major)\r\n\r\n        decoder_cell = tf.contrib.rnn.BasicLSTMCell(\r\n            num_units=256,\r\n            forget_bias=1.0)\r\n        decoder_initial_state = encoder_state\r\n\r\n        # Decoder\r\n        my_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            decoder_cell,\r\n            helper,\r\n            decoder_initial_state)\r\n\r\n        # Dynamic decoding\r\n        decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n            my_decoder,\r\n            output_time_major=FLAGS.time_major,\r\n            swap_memory=True,\r\n            scope=decoder_scope)\r\n\r\n      global_step = tf.train.get_or_create_global_step()\r\n      \r\n      train_op = decoder_outputs\r\n\r\n    # The StopAtStepHook handles stopping after running given steps.\r\n    hooks=[tf.train.StopAtStepHook(last_step=100)]\r\n\r\n    # The MonitoredTrainingSession takes care of session initialization,\r\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\r\n    # or an error occurs.\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(FLAGS.task_index == 0),\r\n                                           checkpoint_dir=FLAGS.out_dir,\r\n                                           hooks=hooks) as mon_sess:\r\n      # This is an infinite while loop since global_step does not increment at all.\r\n      # In a real training, global_step is passed to an optimizer, then increments\r\n      # after each step.\r\n      while not mon_sess.should_stop():\r\n        x = mon_sess.run(train_op)\r\n        global_step_val = global_step.eval(session=mon_sess)\r\n        print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))\r\n\r\nif __name__ == \"__main__\":\r\n  parser = argparse.ArgumentParser()\r\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n  # Flags for defining the tf.train.ClusterSpec\r\n  parser.add_argument(\r\n      \"--ps_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--worker_hosts\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"Comma-separated list of hostname:port pairs\"\r\n  )\r\n  parser.add_argument(\r\n      \"--job_name\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"One of 'ps', 'worker'\"\r\n  )\r\n  # Flags for defining the tf.train.Server\r\n  parser.add_argument(\r\n      \"--task_index\",\r\n      type=int,\r\n      default=0,\r\n      help=\"Index of task within the job\"\r\n  )\r\n  parser.add_argument(\r\n      \"--out_dir\",\r\n      type=str,\r\n      default=\"\",\r\n      help=\"output dir\"\r\n  )\r\n\r\n  parser.add_argument(\r\n      \"--time_major\",\r\n      action='store_true',\r\n      help=\"time major\"\r\n  )\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n"}