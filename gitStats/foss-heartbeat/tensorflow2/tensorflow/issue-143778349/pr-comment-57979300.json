{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57979300", "pull_request_review_id": null, "id": 57979300, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3OTc5MzAw", "diff_hunk": "@@ -0,0 +1,353 @@\n+# Copyright 2015 Google Inc. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"Module for constructing GridRNN cells\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from collections import namedtuple\n+\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import nn\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import rnn_cell\n+\n+\n+class GridRNNCell(rnn_cell.RNNCell):\n+  \"\"\"Grid recurrent cell.\n+\n+  This implementation is based on:\n+\n+    http://arxiv.org/pdf/1507.01526v3.pdf\n+\n+    This is the generic implementation of GridRNN. Users can specify arbitrary number of dimensions,\n+    set some of them to be priority (section 3.2), non-recurrent (section 3.3)\n+    and input/output dimensions (section 3.4).\n+    Weight sharing can also be specified using the `tied` parameter.\n+    Type of recurrent units can be specified via `cell_fn`.\n+  \"\"\"\n+\n+  def __init__(self, num_units, input_size=None, num_dims=1, input_dims=None, output_dims=None, priority_dims=None,\n+               non_recurrent_dims=None, tied=False, cell_fn=None, non_recurrent_fn=None):\n+    \"\"\"Initialize the parameters of a Grid RNN cell\n+\n+    Args:\n+      num_units: int, The number of units in the cells in all dimensions of this GridRNN cell\n+      input_size: int, The dimensionality of the inputs into the cells. If None, input_size is equal to num_units.\n+      num_dims: int, Number of dimensions of this grid.\n+      input_dims: int or list, List of dimensions which will receive input data.\n+      output_dims: int or list, List of dimensions from which the output will be recorded.\n+      priority_dims: int or list, List of dimensions to be considered as priority dimensions.\n+              If None, no dimension is prioritized.\n+      non_recurrent_dims: int or list, List of dimensions that are not recurrent.\n+              The transfer function for non-recurrent dimensions is specified via `non_recurrent_fn`,\n+              which is default to be `tensorflow.nn.relu`.\n+      tied: bool, Whether to share the weights among the dimensions.\n+              If there are non-recurrent dimensions in the grid, weights are shared between each\n+              group of recurrent and non-recurrent dimensions.\n+      cell_fn: function, a function which returns the recurrent cell object. Has to be in the following signature:\n+              def cell_func(num_units, input_size):\n+                # ...\n+\n+              and returns an object of type `RNNCell`. If None, LSTMCell with default parameters will be used.\n+      non_recurrent_fn: a tensorflow Op that will be the transfer function of the non-recurrent dimensions\n+    \"\"\"\n+    if num_dims < 1:\n+      raise ValueError('dims must be >= 1: {}'.format(num_dims))\n+\n+    self._config = _parse_rnn_config(num_dims, input_dims, output_dims, priority_dims,\n+                                     non_recurrent_dims, non_recurrent_fn or nn.relu, tied, num_units)\n+    self._input_size = input_size or num_units\n+\n+    cell_input_size = (self._config.num_dims - 1) * num_units\n+    if cell_fn is None:\n+      self._cell = rnn_cell.LSTMCell(num_units=num_units, input_size=cell_input_size)\n+    else:\n+      self._cell = cell_fn(num_units, cell_input_size)\n+      if not isinstance(self._cell, rnn_cell.RNNCell):\n+        raise ValueError('cell_fn must return an object of type RNNCell')\n+\n+  @property\n+  def input_size(self):\n+    return self._input_size * len(self._config.inputs)\n+\n+  @property\n+  def output_size(self):\n+    return self._cell.output_size * len(self._config.outputs)\n+\n+  @property\n+  def state_size(self):\n+    return self._cell.state_size * len(self._config.recurrents)\n+\n+  def __call__(self, inputs, state, scope=None):\n+    \"\"\"Run one step of GridRNN.\n+\n+    Args:\n+      inputs: input Tensor, 2D, batch x input_size. Or None\n+      state: state Tensor, 2D, batch x state_size. Note that state_size = cell_state_size * recurrent_dims\n+      scope: VariableScope for the created subgraph; defaults to \"GridRNNCell\".\n+\n+    Returns:\n+      A tuple containing:\n+      - A 2D, batch x output_size, Tensor representing the output of the cell\n+        after reading \"inputs\" when previous state was \"state\".\n+      - A 2D, batch x state_size, Tensor representing the new state of the cell\n+        after reading \"inputs\" when previous state was \"state\".\n+    \"\"\"\n+    sz = state.get_shape().as_list()[1]\n+    if self.state_size != sz:\n+      raise ValueError('Actual state size not same as specified: {} vs {}.'.format(sz, self.state_size))\n+\n+    # c_prev is `m`, and m_prev is `h` in the paper. Keep c and m here for consistency with the codebase\n+    c_prev = [None] * self._config.num_dims\n+    m_prev = [None] * self._config.num_dims\n+    cell_units = self._cell.state_size - self._cell.output_size\n+\n+    # for LSTM cell: state_size = num_units + output_size\n+    # for GRU/RNN: state_size = output_size\n+    for recurrent_dim, start_idx in zip(self._config.recurrents, range(0, self.state_size, self._cell.state_size)):\n+      if cell_units > 0:\n+        c_prev[recurrent_dim] = array_ops.slice(state, [0, start_idx], [-1, cell_units])\n+      m_prev[recurrent_dim] = array_ops.slice(state, [0, start_idx + cell_units], [-1, self._cell.output_size])\n+\n+    conf = self._config\n+\n+    # input dimensions\n+    dtype = inputs.dtype if inputs is not None else state.dtype\n+\n+    new_output = [None] * conf.num_dims\n+    new_state = [None] * conf.num_dims\n+\n+    with vs.variable_scope(scope or type(self).__name__) as grid_scope:  # GridRNNCell\n+\n+      # project input\n+      if inputs is not None:\n+        input_splits = array_ops.split(1, len(conf.inputs), inputs)\n+        for i, j in enumerate(conf.inputs):\n+          input_project_m = vs.get_variable('project_m_{}'.format(j), [self._input_size, conf.num_units], dtype=dtype)\n+          m_prev[j] = math_ops.matmul(input_splits[i], input_project_m)\n+\n+          if cell_units > 0:\n+            input_project_c = vs.get_variable('project_c_{}'.format(j), [self._input_size, conf.num_units], dtype=dtype)\n+            c_prev[j] = math_ops.matmul(input_splits[i], input_project_c)\n+\n+\n+      _propagate(conf.non_priority, conf, self._cell, c_prev, m_prev, new_output, new_state, True)\n+      _propagate(conf.priority, conf, self._cell, c_prev, m_prev, new_output, new_state, False)\n+\n+      output_tensors = [new_output[i] for i in self._config.outputs]\n+      output = array_ops.zeros([0, 0], inputs.dtype) if len(output_tensors) == 0 else array_ops.concat(1,\n+                                                                                                       output_tensors)\n+\n+      state_tensors = [new_state[i] for i in self._config.recurrents]\n+      states = array_ops.zeros([0, 0], inputs.dtype) if len(state_tensors) == 0 else array_ops.concat(1, state_tensors)\n+\n+      grid_scope.reuse_variables()\n+\n+    return output, states\n+\n+\n+\"\"\"\n+Specialized cells, for convenience\n+\"\"\"\n+\n+class Grid1BasicRNNCell(GridRNNCell):\n+  \"\"\"1D BasicRNN cell\"\"\"\n+  def __init__(self, num_units, input_size=None):\n+    super(Grid1BasicRNNCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=1,\n+                                            input_dims=0, output_dims=0, priority_dims=0, tied=False,\n+                                            cell_fn=lambda n, i: rnn_cell.BasicRNNCell(num_units=n, input_size=i))\n+\n+\n+class Grid2BasicRNNCell(GridRNNCell):\n+  \"\"\"2D BasicRNN cell\n+  This creates a 2D cell which receives input and gives output in the first dimension.\n+  The first dimension can optionally be non-recurrent if `non_recurrent_fn` is specified.\n+  \"\"\"\n+  def __init__(self, num_units, input_size=None, tied=False, non_recurrent_fn=None):\n+    super(Grid2BasicRNNCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=2,\n+                                            input_dims=0, output_dims=0, priority_dims=0, tied=tied,\n+                                            non_recurrent_dims=None if non_recurrent_fn is None else 0,\n+                                            cell_fn=lambda n, i: rnn_cell.BasicRNNCell(num_units=n, input_size=i),\n+                                            non_recurrent_fn=non_recurrent_fn)\n+\n+\n+class Grid1BasicLSTMCell(GridRNNCell):\n+  \"\"\"1D BasicLSTM cell\"\"\"\n+  def __init__(self, num_units, input_size=None, forget_bias=1):\n+    super(Grid1BasicLSTMCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=1,\n+                                             input_dims=0, output_dims=0, priority_dims=0, tied=False,\n+                                             cell_fn=lambda n, i: rnn_cell.BasicLSTMCell(num_units=n,\n+                                                                                forget_bias=forget_bias, input_size=i))\n+\n+\n+class Grid2BasicLSTMCell(GridRNNCell):\n+  \"\"\"2D BasicLSTM cell\n+    This creates a 2D cell which receives input and gives output in the first dimension.\n+    The first dimension can optionally be non-recurrent if `non_recurrent_fn` is specified.\n+  \"\"\"\n+  def __init__(self, num_units, input_size=None, tied=False, non_recurrent_fn=None, forget_bias=1):\n+    super(Grid2BasicLSTMCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=2,\n+                                             input_dims=0, output_dims=0, priority_dims=0, tied=tied,\n+                                             non_recurrent_dims=None if non_recurrent_fn is None else 0,\n+                                             cell_fn=lambda n, i: rnn_cell.BasicLSTMCell(\n+                                               num_units=n, forget_bias=forget_bias, input_size=i),\n+                                             non_recurrent_fn=non_recurrent_fn)\n+\n+\n+class Grid1LSTMCell(GridRNNCell):\n+  \"\"\"1D LSTM cell\n+    This is different from Grid1BasicLSTMCell because it gives options to specify the forget bias and enabling peepholes\n+  \"\"\"\n+  def __init__(self, num_units, input_size=None, use_peepholes=False, forget_bias=1.0):\n+    super(Grid1LSTMCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=1,\n+                                        input_dims=0, output_dims=0, priority_dims=0,\n+                                        cell_fn=lambda n, i: rnn_cell.LSTMCell(\n+                                          num_units=n, input_size=i, use_peepholes=use_peepholes,\n+                                          forget_bias=forget_bias))\n+\n+\n+class Grid2LSTMCell(GridRNNCell):\n+  \"\"\"2D LSTM cell\n+    This creates a 2D cell which receives input and gives output in the first dimension.\n+    The first dimension can optionally be non-recurrent if `non_recurrent_fn` is specified.\n+  \"\"\"\n+  def __init__(self, num_units, input_size=None, tied=False, non_recurrent_fn=None,\n+               use_peepholes=False, forget_bias=1.0):\n+    super(Grid2LSTMCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=2,\n+                                        input_dims=0, output_dims=0, priority_dims=0, tied=tied,\n+                                        non_recurrent_dims=None if non_recurrent_fn is None else 0,\n+                                        cell_fn=lambda n, i: rnn_cell.LSTMCell(\n+                                          num_units=n, input_size=i, forget_bias=forget_bias,\n+                                          use_peepholes=use_peepholes),\n+                                        non_recurrent_fn=non_recurrent_fn)\n+\n+\n+class Grid3LSTMCell(GridRNNCell):\n+  \"\"\"3D BasicLSTM cell\n+    This creates a 2D cell which receives input and gives output in the first dimension.\n+    The first dimension can optionally be non-recurrent if `non_recurrent_fn` is specified.\n+    The second and third dimensions are LSTM.\n+  \"\"\"\n+  def __init__(self, num_units, input_size=None, tied=False, non_recurrent_fn=None,\n+               use_peepholes=False, forget_bias=1.0):\n+    super(Grid3LSTMCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=3,\n+                                        input_dims=0, output_dims=0, priority_dims=0, tied=tied,\n+                                        non_recurrent_dims=None if non_recurrent_fn is None else 0,\n+                                        cell_fn=lambda n, i: rnn_cell.LSTMCell(\n+                                          num_units=n, input_size=i, forget_bias=forget_bias,\n+                                          use_peepholes=use_peepholes),\n+                                        non_recurrent_fn=non_recurrent_fn)\n+\n+class Grid2GRUCell(GridRNNCell):\n+  \"\"\"2D LSTM cell\n+    This creates a 2D cell which receives input and gives output in the first dimension.\n+    The first dimension can optionally be non-recurrent if `non_recurrent_fn` is specified.\n+  \"\"\"\n+\n+  def __init__(self, num_units, input_size=None, tied=False, non_recurrent_fn=None):\n+    super(Grid2GRUCell, self).__init__(num_units=num_units, input_size=input_size, num_dims=2,\n+                                        input_dims=0, output_dims=0, priority_dims=0, tied=tied,\n+                                        non_recurrent_dims=None if non_recurrent_fn is None else 0,\n+                                        cell_fn=lambda n, i: rnn_cell.GRUCell(num_units=n, input_size=i),\n+                                        non_recurrent_fn=non_recurrent_fn)\n+\n+\"\"\"\n+Helpers\n+\"\"\"\n+\n+_GridRNNDimension = namedtuple('_GridRNNDimension', ['idx', 'is_input', 'is_output', 'is_priority', 'non_recurrent_fn'])\n+\n+_GridRNNConfig = namedtuple('_GridRNNConfig', ['num_dims', 'dims',\n+                                               'inputs', 'outputs', 'recurrents',\n+                                               'priority', 'non_priority', 'tied', 'num_units'])\n+\n+\n+def _parse_rnn_config(num_dims, ls_input_dims, ls_output_dims, ls_priority_dims, ls_non_recurrent_dims,\n+                      non_recurrent_fn, tied, num_units):\n+  def check_dim_list(ls):\n+    if ls is None:\n+      ls = []\n+    if not isinstance(ls, (list, tuple)):\n+      ls = [ls]\n+    ls = sorted(set(ls))\n+    if any(_ < 0 or _ >= num_dims for _ in ls):\n+      raise ValueError('Invalid dims: {}. Must be in [0, {})'.format(ls, num_dims))\n+    return ls\n+\n+  input_dims = check_dim_list(ls_input_dims)\n+  output_dims = check_dim_list(ls_output_dims)\n+  priority_dims = check_dim_list(ls_priority_dims)\n+  non_recurrent_dims = check_dim_list(ls_non_recurrent_dims)\n+\n+  rnn_dims = []\n+  for i in range(num_dims):\n+    rnn_dims.append(_GridRNNDimension(idx=i, is_input=(i in input_dims), is_output=(i in output_dims),\n+                                      is_priority=(i in priority_dims),\n+                                      non_recurrent_fn=non_recurrent_fn if i in non_recurrent_dims else None))\n+  return _GridRNNConfig(num_dims=num_dims, dims=rnn_dims, inputs=input_dims, outputs=output_dims,\n+                        recurrents=[x for x in range(num_dims) if x not in non_recurrent_dims],\n+                        priority=priority_dims,\n+                        non_priority=[x for x in range(num_dims) if x not in priority_dims],\n+                        tied=tied, num_units=num_units)\n+\n+\n+def _get_first(*args):\n+  return next((i for i in args if i is not None), None)\n+\n+\n+def _propagate(dim_indices, conf, cell, c_prev, m_prev, new_output, new_state, first_call):\n+  \"\"\"\n+  \"\"\"\n+  if len(dim_indices) == 0:\n+    return\n+\n+  if conf.num_dims > 1:\n+    ls_cell_inputs = [None] * (conf.num_dims - 1)\n+    for d in conf.dims[:-1]:\n+      ls_cell_inputs[d.idx] = _get_first(new_output[d.idx], m_prev[d.idx])\n+    cell_inputs = array_ops.concat(1, ls_cell_inputs)\n+  else:\n+    cell_inputs = array_ops.zeros([m_prev[0].get_shape().as_list()[0], 0], m_prev[0].dtype)\n+\n+  last_dim_output = _get_first(new_output[-1], m_prev[-1])\n+\n+  for i in dim_indices:\n+    d = conf.dims[i]\n+    if d.non_recurrent_fn:\n+      linear_args = [cell_inputs, last_dim_output] if conf.num_dims > 1 else last_dim_output\n+      if conf.tied:\n+        with vs.variable_scope('non_recurrent', reuse=not(first_call and i == dim_indices[0])):\n+          new_output[d.idx] = d.non_recurrent_fn(rnn_cell.linear(args=linear_args,\n+                                                                 output_size=conf.num_units, bias=True))\n+      else:\n+          new_output[d.idx] = d.non_recurrent_fn(rnn_cell.linear(args=linear_args,", "path": "tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py", "position": null, "original_position": 338, "commit_id": "c80a374fca39e117ce87932f2cf7b8cf20574b1f", "original_commit_id": "5bf8826dbe57e942d7e4966c28c01d2d4429b97b", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "can you use the contrib.layers.fully_connected here instead?  we're trying to get rid of rnn_cell.linear with fire.\n", "created_at": "2016-03-30T23:11:55Z", "updated_at": "2016-04-06T18:32:22Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1665#discussion_r57979300", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1665", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57979300"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1665#discussion_r57979300"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1665"}}, "body_html": "<p>can you use the contrib.layers.fully_connected here instead?  we're trying to get rid of rnn_cell.linear with fire.</p>", "body_text": "can you use the contrib.layers.fully_connected here instead?  we're trying to get rid of rnn_cell.linear with fire."}