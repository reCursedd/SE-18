{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233604414", "html_url": "https://github.com/tensorflow/tensorflow/pull/1665#issuecomment-233604414", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1665", "id": 233604414, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzYwNDQxNA==", "user": {"login": "phvu", "id": 1087992, "node_id": "MDQ6VXNlcjEwODc5OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1087992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phvu", "html_url": "https://github.com/phvu", "followers_url": "https://api.github.com/users/phvu/followers", "following_url": "https://api.github.com/users/phvu/following{/other_user}", "gists_url": "https://api.github.com/users/phvu/gists{/gist_id}", "starred_url": "https://api.github.com/users/phvu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phvu/subscriptions", "organizations_url": "https://api.github.com/users/phvu/orgs", "repos_url": "https://api.github.com/users/phvu/repos", "events_url": "https://api.github.com/users/phvu/events{/privacy}", "received_events_url": "https://api.github.com/users/phvu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-19T11:26:25Z", "updated_at": "2016-07-19T11:26:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1364252\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jstaker7\">@jstaker7</a>,<br>\nThere are some nuances here.<br>\nFirst, the <code>__call__</code> method expects the state tensor of <code>batch_size * ( (c + m) * num_dims)</code>. When c=m=2 then c*m = c+m. Sorry for the bad test case, I should have used other values.</p>\n<p>Second, in the MNIST case, (as far as I understand) I suppose you could use Grid3LSTMCell (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py#L242\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py#L242</a>). This cells receives input and gives output in the first dimension (index 0), and uses LSTM in the other 2 dimensions.<br>\nIn order to replicate Figure 11 in the paper, you will need to construct 4 3-LSTM cells, each cell will handle 1 scan direction of the input image. However the first hidden LSTM layer receives original pixels as input, while other hidden LSTM layers receives the output of the LSTM just below it.<br>\nHow to do that is up to you. One simple way is to have a loop to scan the image in a given scan direction (say left-to-right top-to-bottom), which will give a sequence of vectors, and then feed that sequence into the corresponding 3-LSTM cell for that scan direction. For bigger images, you might want to use <code>tf.scan()</code></p>\n<p>So your assumption that input_dims should be <code>[1,2]</code> is not true. The cells should receive input at dimension 0. For me this is the only reasonable interpretation of the paper, which also take 1-LSTM and 2-LSTM cells into account. (unless the authors release their implementation so that we can do a comparison).</p>", "body_text": "Hi @jstaker7,\nThere are some nuances here.\nFirst, the __call__ method expects the state tensor of batch_size * ( (c + m) * num_dims). When c=m=2 then c*m = c+m. Sorry for the bad test case, I should have used other values.\nSecond, in the MNIST case, (as far as I understand) I suppose you could use Grid3LSTMCell (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py#L242). This cells receives input and gives output in the first dimension (index 0), and uses LSTM in the other 2 dimensions.\nIn order to replicate Figure 11 in the paper, you will need to construct 4 3-LSTM cells, each cell will handle 1 scan direction of the input image. However the first hidden LSTM layer receives original pixels as input, while other hidden LSTM layers receives the output of the LSTM just below it.\nHow to do that is up to you. One simple way is to have a loop to scan the image in a given scan direction (say left-to-right top-to-bottom), which will give a sequence of vectors, and then feed that sequence into the corresponding 3-LSTM cell for that scan direction. For bigger images, you might want to use tf.scan()\nSo your assumption that input_dims should be [1,2] is not true. The cells should receive input at dimension 0. For me this is the only reasonable interpretation of the paper, which also take 1-LSTM and 2-LSTM cells into account. (unless the authors release their implementation so that we can do a comparison).", "body": "Hi @jstaker7,\nThere are some nuances here.\nFirst, the `__call__` method expects the state tensor of `batch_size * ( (c + m) * num_dims)`. When c=m=2 then c*m = c+m. Sorry for the bad test case, I should have used other values.\n\nSecond, in the MNIST case, (as far as I understand) I suppose you could use Grid3LSTMCell (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py#L242). This cells receives input and gives output in the first dimension (index 0), and uses LSTM in the other 2 dimensions.\nIn order to replicate Figure 11 in the paper, you will need to construct 4 3-LSTM cells, each cell will handle 1 scan direction of the input image. However the first hidden LSTM layer receives original pixels as input, while other hidden LSTM layers receives the output of the LSTM just below it.\nHow to do that is up to you. One simple way is to have a loop to scan the image in a given scan direction (say left-to-right top-to-bottom), which will give a sequence of vectors, and then feed that sequence into the corresponding 3-LSTM cell for that scan direction. For bigger images, you might want to use `tf.scan()`\n\nSo your assumption that input_dims should be `[1,2]` is not true. The cells should receive input at dimension 0. For me this is the only reasonable interpretation of the paper, which also take 1-LSTM and 2-LSTM cells into account. (unless the authors release their implementation so that we can do a comparison).\n"}