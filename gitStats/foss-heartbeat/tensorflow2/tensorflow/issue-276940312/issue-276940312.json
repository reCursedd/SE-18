{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14906", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14906/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14906/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14906/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14906", "id": 276940312, "node_id": "MDU6SXNzdWUyNzY5NDAzMTI=", "number": 14906, "title": "Functionality Available?: Dataset Input perform slicing along time axis", "user": {"login": "Gumilton", "id": 9038746, "node_id": "MDQ6VXNlcjkwMzg3NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/9038746?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gumilton", "html_url": "https://github.com/Gumilton", "followers_url": "https://api.github.com/users/Gumilton/followers", "following_url": "https://api.github.com/users/Gumilton/following{/other_user}", "gists_url": "https://api.github.com/users/Gumilton/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gumilton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gumilton/subscriptions", "organizations_url": "https://api.github.com/users/Gumilton/orgs", "repos_url": "https://api.github.com/users/Gumilton/repos", "events_url": "https://api.github.com/users/Gumilton/events{/privacy}", "received_events_url": "https://api.github.com/users/Gumilton/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-11-27T08:49:04Z", "updated_at": "2017-11-29T11:25:46Z", "closed_at": "2017-11-28T02:48:36Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 7</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: from pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0, 6.1</li>\n<li><strong>GPU model and memory</strong>: k2200</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Currently, as many suggested, when we want to train LSTM to predict the values of next time step, we would better slice all the samples along time axis to tuples of (lookback numbers of values, target predicted values) as (x, y), then store all these in a variable in RAM or as data file in disk. Then we build dataset to point to either form. However, this method makes LSTM stateless. Currently, our scheme to stateful LSTM is as follows:<br>\nfor an input signal [1, 2, 3, 4, 5, 6, ..., 9, 10, 11]<br>\nwe initialize LSTM and set state to 0.<br>\nfeed ([1, 2, 3], 4) to train (<code>tf.nn.dynamic_rnn</code>), then feed ([2, 3, 4], 5) ....... ([8, 9, 10], 11), until end of this sequence.<br>\nIn this batch, we have a number (Batch size, like 32) of signals like this and they will be processed in parallel in GPU by TF.<br>\nIn the next batch of new signal samples, we reset LSTM with initial state being 0. And then repeat this process.</p>\n<p>In this way, we think that given [8, 9, 10] to the model to predict next value (as 11), it is helpful for the model to choose whether to utilize the information of its current state (whether it is at beginning of a series of signal, zero initial; or it is at middle of a signal sequence).</p>\n<p>Currently, before version 1.4, we built two generators, one (batch generator) is to generate a batch of signals. The other (time slicer) is to generate (x, y) [shape of (batch size, max time step, number of features) ] along time axis by using the data yield by the batch generator.</p>\n<p>In version 1.4, we find Dataset, Estimator and Experiment pipeline powerful. Is there a way to implement the same idea using such pipeline?<br>\nThanks!</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\nTensorFlow installed from (source or binary): from pip\nTensorFlow version (use command below): 1.3\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 8.0, 6.1\nGPU model and memory: k2200\nExact command to reproduce:\n\nDescribe the problem\nCurrently, as many suggested, when we want to train LSTM to predict the values of next time step, we would better slice all the samples along time axis to tuples of (lookback numbers of values, target predicted values) as (x, y), then store all these in a variable in RAM or as data file in disk. Then we build dataset to point to either form. However, this method makes LSTM stateless. Currently, our scheme to stateful LSTM is as follows:\nfor an input signal [1, 2, 3, 4, 5, 6, ..., 9, 10, 11]\nwe initialize LSTM and set state to 0.\nfeed ([1, 2, 3], 4) to train (tf.nn.dynamic_rnn), then feed ([2, 3, 4], 5) ....... ([8, 9, 10], 11), until end of this sequence.\nIn this batch, we have a number (Batch size, like 32) of signals like this and they will be processed in parallel in GPU by TF.\nIn the next batch of new signal samples, we reset LSTM with initial state being 0. And then repeat this process.\nIn this way, we think that given [8, 9, 10] to the model to predict next value (as 11), it is helpful for the model to choose whether to utilize the information of its current state (whether it is at beginning of a series of signal, zero initial; or it is at middle of a signal sequence).\nCurrently, before version 1.4, we built two generators, one (batch generator) is to generate a batch of signals. The other (time slicer) is to generate (x, y) [shape of (batch size, max time step, number of features) ] along time axis by using the data yield by the batch generator.\nIn version 1.4, we find Dataset, Estimator and Experiment pipeline powerful. Is there a way to implement the same idea using such pipeline?\nThanks!", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: from pip\r\n- **TensorFlow version (use command below)**: 1.3 \r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0, 6.1\r\n- **GPU model and memory**: k2200\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nCurrently, as many suggested, when we want to train LSTM to predict the values of next time step, we would better slice all the samples along time axis to tuples of (lookback numbers of values, target predicted values) as (x, y), then store all these in a variable in RAM or as data file in disk. Then we build dataset to point to either form. However, this method makes LSTM stateless. Currently, our scheme to stateful LSTM is as follows:\r\nfor an input signal [1, 2, 3, 4, 5, 6, ..., 9, 10, 11]\r\nwe initialize LSTM and set state to 0.\r\nfeed ([1, 2, 3], 4) to train (`tf.nn.dynamic_rnn`), then feed ([2, 3, 4], 5) ....... ([8, 9, 10], 11), until end of this sequence.\r\nIn this batch, we have a number (Batch size, like 32) of signals like this and they will be processed in parallel in GPU by TF.\r\nIn the next batch of new signal samples, we reset LSTM with initial state being 0. And then repeat this process.\r\n\r\nIn this way, we think that given [8, 9, 10] to the model to predict next value (as 11), it is helpful for the model to choose whether to utilize the information of its current state (whether it is at beginning of a series of signal, zero initial; or it is at middle of a signal sequence).\r\n\r\nCurrently, before version 1.4, we built two generators, one (batch generator) is to generate a batch of signals. The other (time slicer) is to generate (x, y) [shape of (batch size, max time step, number of features) ] along time axis by using the data yield by the batch generator.\r\n\r\nIn version 1.4, we find Dataset, Estimator and Experiment pipeline powerful. Is there a way to implement the same idea using such pipeline?\r\nThanks!\r\n"}