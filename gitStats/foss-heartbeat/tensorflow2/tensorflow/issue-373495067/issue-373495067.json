{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23213", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23213/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23213/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23213/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23213", "id": 373495067, "node_id": "MDU6SXNzdWUzNzM0OTUwNjc=", "number": 23213, "title": "When using TFLite for network inference on mobile phone, the shallow layer of same model takes more time to infer than the deep layer.", "user": {"login": "TXQGIT", "id": 40796349, "node_id": "MDQ6VXNlcjQwNzk2MzQ5", "avatar_url": "https://avatars0.githubusercontent.com/u/40796349?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TXQGIT", "html_url": "https://github.com/TXQGIT", "followers_url": "https://api.github.com/users/TXQGIT/followers", "following_url": "https://api.github.com/users/TXQGIT/following{/other_user}", "gists_url": "https://api.github.com/users/TXQGIT/gists{/gist_id}", "starred_url": "https://api.github.com/users/TXQGIT/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TXQGIT/subscriptions", "organizations_url": "https://api.github.com/users/TXQGIT/orgs", "repos_url": "https://api.github.com/users/TXQGIT/repos", "events_url": "https://api.github.com/users/TXQGIT/events{/privacy}", "received_events_url": "https://api.github.com/users/TXQGIT/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-10-24T13:53:15Z", "updated_at": "2018-11-22T06:42:10Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code:<br>\nN/A</li>\n<li>OS Platform and Distribution:<br>\nUbuntu14.04, Win 10, Android Studio 3.0.1, sdk version 26</li>\n<li>Mobile device:<br>\nGome K1 (CPU: MTK MT6757,  2.3GHZ)</li>\n<li>TensorFlow installed from (source or binary):<br>\nbinary</li>\n<li>TensorFlow version (use command below):<br>\n1.11.0</li>\n<li>Python version:<br>\n2.7</li>\n<li>Bazel version (if compiling from source):<br>\nN/A</li>\n<li>GCC/Compiler version (if compiling from source):<br>\nN/A</li>\n<li>CUDA/cuDNN version:<br>\nN/A</li>\n<li>GPU model and memory:<br>\nN/A</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nI am using the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo\">offical demo(Java interface)</a> of TFLite on Android, which classify the frame captured by the camera.  The model is offical released <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md\">Mobilenet_v1_1.0_224 </a>. The inference time is about 240ms in my mobile phone. Now, I need to get the feature maps of the last convolution layer, so I get the new tflite interpreter based on <a href=\"https://www.tensorflow.org/lite/convert/cmdline_examples\" rel=\"nofollow\">TF Lite converter </a> and the frozen graphs in the offical released  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md\">Mobilenet_v1_1.0_224 </a>, the comandline that I used is:<br>\n<code>tflite_convert --graph_def_file=mobilenet_v1_1.0_224_frozen.pb --output_file=mobilenet_v1_1.0_224.tflite --input_shapes=1,224,224,3 --input_array=input --output_arrays=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/BatchNorm/FusedBatchNorm</code><br>\nWhen I use the new tflite interpreter for inference to get the feature maps of the last conv layer, <strong>the inference time is 320ms, which is more than the original 240ms when get the prob.</strong> This makes me very confused, since the global avg pooling and FC is no longer needed when using the new interpreter to get the feature map of last conv layer and the compution costs is decreased, so the inference time should not increase. By the way, the code for time measurement remains unchanged, which is:</p>\n<pre><code>// Here's where the magic happens!!!\nlong startTime = SystemClock.uptimeMillis();\nrunInference();\nlong endTime = SystemClock.uptimeMillis();\nLog.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\n</code></pre>\n<p>I also test the inference time when I get the feature maps of the middle conv layers:<br>\n--------layer--------------------shape of feature map------------inference time(ms)<br>\n-------prob------------------------1x1001-------------------------------~240<br>\nconv2d_13_pointwise-----------7x7x1024------------------------------~320<br>\nconv2d_10_pointwise-----------14x14x512----------------------------~340<br>\nconv2d_6_pointwise------------14x14x512----------------------------~260<br>\nconv2d_5_pointwise------------28x28x256----------------------------~380<br>\nconv2d_3_pointwise------------56x56x128----------------------------~600</p>\n<p><strong>It seems that the larger the feature map is, the bigger the inference time is,</strong> and the effect of depth on inferenc time is much smaller.<br>\nSo, What is the reason? Any help would be grateful. Thanks!</p>\n<p><strong>Exact command to reproduce:</strong><br>\nN/A</p>", "body_text": "System information\n\nHave I written custom code:\nN/A\nOS Platform and Distribution:\nUbuntu14.04, Win 10, Android Studio 3.0.1, sdk version 26\nMobile device:\nGome K1 (CPU: MTK MT6757,  2.3GHZ)\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\n1.11.0\nPython version:\n2.7\nBazel version (if compiling from source):\nN/A\nGCC/Compiler version (if compiling from source):\nN/A\nCUDA/cuDNN version:\nN/A\nGPU model and memory:\nN/A\n\nDescribe the current behavior\nI am using the offical demo(Java interface) of TFLite on Android, which classify the frame captured by the camera.  The model is offical released Mobilenet_v1_1.0_224 . The inference time is about 240ms in my mobile phone. Now, I need to get the feature maps of the last convolution layer, so I get the new tflite interpreter based on TF Lite converter  and the frozen graphs in the offical released  Mobilenet_v1_1.0_224 , the comandline that I used is:\ntflite_convert --graph_def_file=mobilenet_v1_1.0_224_frozen.pb --output_file=mobilenet_v1_1.0_224.tflite --input_shapes=1,224,224,3 --input_array=input --output_arrays=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/BatchNorm/FusedBatchNorm\nWhen I use the new tflite interpreter for inference to get the feature maps of the last conv layer, the inference time is 320ms, which is more than the original 240ms when get the prob. This makes me very confused, since the global avg pooling and FC is no longer needed when using the new interpreter to get the feature map of last conv layer and the compution costs is decreased, so the inference time should not increase. By the way, the code for time measurement remains unchanged, which is:\n// Here's where the magic happens!!!\nlong startTime = SystemClock.uptimeMillis();\nrunInference();\nlong endTime = SystemClock.uptimeMillis();\nLog.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\n\nI also test the inference time when I get the feature maps of the middle conv layers:\n--------layer--------------------shape of feature map------------inference time(ms)\n-------prob------------------------1x1001-------------------------------~240\nconv2d_13_pointwise-----------7x7x1024------------------------------~320\nconv2d_10_pointwise-----------14x14x512----------------------------~340\nconv2d_6_pointwise------------14x14x512----------------------------~260\nconv2d_5_pointwise------------28x28x256----------------------------~380\nconv2d_3_pointwise------------56x56x128----------------------------~600\nIt seems that the larger the feature map is, the bigger the inference time is, and the effect of depth on inferenc time is much smaller.\nSo, What is the reason? Any help would be grateful. Thanks!\nExact command to reproduce:\nN/A", "body": "**System information**\r\n- Have I written custom code:\r\nN/A\r\n- OS Platform and Distribution:\r\nUbuntu14.04, Win 10, Android Studio 3.0.1, sdk version 26\r\n- Mobile device:\r\nGome K1 (CPU: MTK MT6757,  2.3GHZ)\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.11.0\r\n- Python version:\r\n2.7\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI am using the [offical demo(Java interface)](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo) of TFLite on Android, which classify the frame captured by the camera.  The model is offical released [Mobilenet_v1_1.0_224 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md). The inference time is about 240ms in my mobile phone. Now, I need to get the feature maps of the last convolution layer, so I get the new tflite interpreter based on [TF Lite converter ](https://www.tensorflow.org/lite/convert/cmdline_examples) and the frozen graphs in the offical released  [Mobilenet_v1_1.0_224 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md), the comandline that I used is:\r\n`tflite_convert --graph_def_file=mobilenet_v1_1.0_224_frozen.pb --output_file=mobilenet_v1_1.0_224.tflite --input_shapes=1,224,224,3 --input_array=input --output_arrays=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/BatchNorm/FusedBatchNorm`\r\nWhen I use the new tflite interpreter for inference to get the feature maps of the last conv layer, **the inference time is 320ms, which is more than the original 240ms when get the prob.** This makes me very confused, since the global avg pooling and FC is no longer needed when using the new interpreter to get the feature map of last conv layer and the compution costs is decreased, so the inference time should not increase. By the way, the code for time measurement remains unchanged, which is:\r\n```\r\n// Here's where the magic happens!!!\r\nlong startTime = SystemClock.uptimeMillis();\r\nrunInference();\r\nlong endTime = SystemClock.uptimeMillis();\r\nLog.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\r\n```\r\nI also test the inference time when I get the feature maps of the middle conv layers: \r\n--------layer--------------------shape of feature map------------inference time(ms)\r\n-------prob------------------------1x1001-------------------------------~240\r\nconv2d_13_pointwise-----------7x7x1024------------------------------~320\r\nconv2d_10_pointwise-----------14x14x512----------------------------~340\r\nconv2d_6_pointwise------------14x14x512----------------------------~260\r\nconv2d_5_pointwise------------28x28x256----------------------------~380\r\nconv2d_3_pointwise------------56x56x128----------------------------~600\r\n\r\n**It seems that the larger the feature map is, the bigger the inference time is,** and the effect of depth on inferenc time is much smaller. \r\nSo, What is the reason? Any help would be grateful. Thanks!\r\n\r\n**Exact command to reproduce:**\r\nN/A\r\n"}