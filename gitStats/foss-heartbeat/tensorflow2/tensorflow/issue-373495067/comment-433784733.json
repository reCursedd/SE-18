{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433784733", "html_url": "https://github.com/tensorflow/tensorflow/issues/23213#issuecomment-433784733", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23213", "id": 433784733, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzc4NDczMw==", "user": {"login": "TXQGIT", "id": 40796349, "node_id": "MDQ6VXNlcjQwNzk2MzQ5", "avatar_url": "https://avatars0.githubusercontent.com/u/40796349?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TXQGIT", "html_url": "https://github.com/TXQGIT", "followers_url": "https://api.github.com/users/TXQGIT/followers", "following_url": "https://api.github.com/users/TXQGIT/following{/other_user}", "gists_url": "https://api.github.com/users/TXQGIT/gists{/gist_id}", "starred_url": "https://api.github.com/users/TXQGIT/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TXQGIT/subscriptions", "organizations_url": "https://api.github.com/users/TXQGIT/orgs", "repos_url": "https://api.github.com/users/TXQGIT/repos", "events_url": "https://api.github.com/users/TXQGIT/events{/privacy}", "received_events_url": "https://api.github.com/users/TXQGIT/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-29T04:25:07Z", "updated_at": "2018-10-29T04:27:26Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785357\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ymodak\">@ymodak</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25754898\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrehentz\">@andrehentz</a><br>\nWhen I dig into the source code of TFLite, I found that the above issue was caused by the time-consuming <code>Shapeof()</code> functions in Tensor class. To be more specific, the <code>runInference()</code> in ImageClassifier.java</p>\n<pre><code>    // Here's where the magic happens!!!\n    long startTime = SystemClock.uptimeMillis();\n    runInference();\n    long endTime = SystemClock.uptimeMillis();\n    Log.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\n</code></pre>\n<p>will call the tflite.run() in ImageClassifierFloatMobileNet.java which is almost same as ImageClassifierFloatInception.java,</p>\n<pre><code>  @Override\n  protected void runInference() {\n    //tflite.run(imgData, labelProbArray);  //original official code\n    tflite.run(imgData, featMap);\n  }\n</code></pre>\n<p>where the featMap is defined as:</p>\n<pre><code>    labelProbArray = new float[1][getNumLabels()];  //float[1][1001]  //original official code\n    featMap = new float[1][getFeatSizeH()][getFeatSizeW()][getFeatSizeC()];  //float[1][7][7][1024]\n</code></pre>\n<hr>\n<p>The<code> tflite.run()</code> function will call the <code>run()</code> method of NativeInterpreterWrapper :</p>\n<pre><code>void run(Object[] inputs, Map&lt;Integer, Object&gt; outputs) {\n        this.inferenceDurationNanoseconds = -1L;\n        if(inputs != null &amp;&amp; inputs.length != 0) {\n            if(outputs != null &amp;&amp; !outputs.isEmpty()) {\n                int i;\n                for(i = 0; i &lt; inputs.length; ++i) {\n                    Tensor tensor = this.getInputTensor(i);\n                    int[] newShape = tensor.getInputShapeIfDifferent(inputs[i]);\n                    if(newShape != null) {\n                        this.resizeInput(i, newShape);\n                    }\n                }\n\n                if(!this.isMemoryAllocated) {\n                    allocateTensors(this.interpreterHandle, this.errorHandle);\n                    this.isMemoryAllocated = true;\n                    Arrays.fill(this.outputTensors, (Object)null);\n                }\n\n                for(i = 0; i &lt; inputs.length; ++i) {\n                    this.getInputTensor(i).setTo(inputs[i]);\n                }\n\n                long inferenceStartNanos = System.nanoTime();\n                run(this.interpreterHandle, this.errorHandle);\n                long inferenceDurationNanoseconds = System.nanoTime() - inferenceStartNanos;\n                Iterator var7 = outputs.entrySet().iterator();\n\n                while(var7.hasNext()) {\n                    Entry&lt;Integer, Object&gt; output = (Entry)var7.next();\n                    this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue());\n                }\n\n                this.inferenceDurationNanoseconds = inferenceDurationNanoseconds;\n            } else {\n                throw new IllegalArgumentException(\"Input error: Outputs should not be null or empty.\");\n            }\n        } else {\n            throw new IllegalArgumentException(\"Input error: Inputs should not be null or empty.\");\n        }\n    }\n</code></pre>\n<p>From the code, we can find that network inference is done by <code>run(this.interpreterHandle, this.errorHandle);</code> and <code>this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue());</code> copy the feature maps to the output array (featMap =new float[1][7][7][1024]).</p>\n<p>The time of network inference is decreased with shallower layer in Mobilenet (from 220ms for conv2d_13_pointwise to 77ms forconv2d_3_pointwise). However, the <code>copyTo()</code> function is the opposite, the shallowr layer with larger feature maps costs more time to finish the copy operation. This lead to my initial issue described above.</p>\n<p>As for <code>copyTo()</code> function which belong to Tensor class, <code>this.throwExceptionIfTypeIsIncompatible(dst);</code> is where the issue occured. When judging whether the shape of the pre-defined output array (featMap) is consistent with the shape of feature map of conv layer, <code>shapeOf()</code> function in Tensor class is used to get the shape of pre-defined output array (featMap).</p>\n<pre><code>    static int[] shapeOf(Object o) {\n        int size = numDimensions(o);\n        int[] dimensions = new int[size];\n        fillShape(o, 0, dimensions);\n        return dimensions;\n    }\n    static void fillShape(Object o, int dim, int[] shape) {\n        if(shape != null &amp;&amp; dim != shape.length) {\n            int len = Array.getLength(o);\n            if(shape[dim] == 0) {\n                shape[dim] = len;\n            } else if(shape[dim] != len) {\n                throw new IllegalArgumentException(String.format(\"Mismatched lengths (%d and %d) in dimension %d\", new Object[]{Integer.valueOf(shape[dim]), Integer.valueOf(len), Integer.valueOf(dim)}));\n            }\n\n            for(int i = 0; i &lt; len; ++i) {\n                fillShape(Array.get(o, i), dim + 1, shape);\n            }\n\n        }\n    }\n</code></pre>\n<p><strong>However, the time complexity of <code>fillShape()</code> is O(NxHxWxC), where N is the batchsize, H,W,C is the size of feature map. So, the larger the feature map is, the more time it will costs.</strong> Since the output array (feataMap) is defined as new float[1][7][7][1024], I think <code>shapeOf()</code> can be simpler and the fillShape() is not necessary.</p>\n<p>Now, I have figured out what the issue is. However, since I use TFLite by adding <code>compile 'org.tensorflow:tensorflow-lite:+'</code> in the build.gradle in Android Studio project, the source code of TFLite is read-only and I can't modify it for my project. Is there any other way for me to use the modified source code? Thanks.</p>", "body_text": "@ymodak @andrehentz\nWhen I dig into the source code of TFLite, I found that the above issue was caused by the time-consuming Shapeof() functions in Tensor class. To be more specific, the runInference() in ImageClassifier.java\n    // Here's where the magic happens!!!\n    long startTime = SystemClock.uptimeMillis();\n    runInference();\n    long endTime = SystemClock.uptimeMillis();\n    Log.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\n\nwill call the tflite.run() in ImageClassifierFloatMobileNet.java which is almost same as ImageClassifierFloatInception.java,\n  @Override\n  protected void runInference() {\n    //tflite.run(imgData, labelProbArray);  //original official code\n    tflite.run(imgData, featMap);\n  }\n\nwhere the featMap is defined as:\n    labelProbArray = new float[1][getNumLabels()];  //float[1][1001]  //original official code\n    featMap = new float[1][getFeatSizeH()][getFeatSizeW()][getFeatSizeC()];  //float[1][7][7][1024]\n\n\nThe tflite.run() function will call the run() method of NativeInterpreterWrapper :\nvoid run(Object[] inputs, Map<Integer, Object> outputs) {\n        this.inferenceDurationNanoseconds = -1L;\n        if(inputs != null && inputs.length != 0) {\n            if(outputs != null && !outputs.isEmpty()) {\n                int i;\n                for(i = 0; i < inputs.length; ++i) {\n                    Tensor tensor = this.getInputTensor(i);\n                    int[] newShape = tensor.getInputShapeIfDifferent(inputs[i]);\n                    if(newShape != null) {\n                        this.resizeInput(i, newShape);\n                    }\n                }\n\n                if(!this.isMemoryAllocated) {\n                    allocateTensors(this.interpreterHandle, this.errorHandle);\n                    this.isMemoryAllocated = true;\n                    Arrays.fill(this.outputTensors, (Object)null);\n                }\n\n                for(i = 0; i < inputs.length; ++i) {\n                    this.getInputTensor(i).setTo(inputs[i]);\n                }\n\n                long inferenceStartNanos = System.nanoTime();\n                run(this.interpreterHandle, this.errorHandle);\n                long inferenceDurationNanoseconds = System.nanoTime() - inferenceStartNanos;\n                Iterator var7 = outputs.entrySet().iterator();\n\n                while(var7.hasNext()) {\n                    Entry<Integer, Object> output = (Entry)var7.next();\n                    this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue());\n                }\n\n                this.inferenceDurationNanoseconds = inferenceDurationNanoseconds;\n            } else {\n                throw new IllegalArgumentException(\"Input error: Outputs should not be null or empty.\");\n            }\n        } else {\n            throw new IllegalArgumentException(\"Input error: Inputs should not be null or empty.\");\n        }\n    }\n\nFrom the code, we can find that network inference is done by run(this.interpreterHandle, this.errorHandle); and this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue()); copy the feature maps to the output array (featMap =new float[1][7][7][1024]).\nThe time of network inference is decreased with shallower layer in Mobilenet (from 220ms for conv2d_13_pointwise to 77ms forconv2d_3_pointwise). However, the copyTo() function is the opposite, the shallowr layer with larger feature maps costs more time to finish the copy operation. This lead to my initial issue described above.\nAs for copyTo() function which belong to Tensor class, this.throwExceptionIfTypeIsIncompatible(dst); is where the issue occured. When judging whether the shape of the pre-defined output array (featMap) is consistent with the shape of feature map of conv layer, shapeOf() function in Tensor class is used to get the shape of pre-defined output array (featMap).\n    static int[] shapeOf(Object o) {\n        int size = numDimensions(o);\n        int[] dimensions = new int[size];\n        fillShape(o, 0, dimensions);\n        return dimensions;\n    }\n    static void fillShape(Object o, int dim, int[] shape) {\n        if(shape != null && dim != shape.length) {\n            int len = Array.getLength(o);\n            if(shape[dim] == 0) {\n                shape[dim] = len;\n            } else if(shape[dim] != len) {\n                throw new IllegalArgumentException(String.format(\"Mismatched lengths (%d and %d) in dimension %d\", new Object[]{Integer.valueOf(shape[dim]), Integer.valueOf(len), Integer.valueOf(dim)}));\n            }\n\n            for(int i = 0; i < len; ++i) {\n                fillShape(Array.get(o, i), dim + 1, shape);\n            }\n\n        }\n    }\n\nHowever, the time complexity of fillShape() is O(NxHxWxC), where N is the batchsize, H,W,C is the size of feature map. So, the larger the feature map is, the more time it will costs. Since the output array (feataMap) is defined as new float[1][7][7][1024], I think shapeOf() can be simpler and the fillShape() is not necessary.\nNow, I have figured out what the issue is. However, since I use TFLite by adding compile 'org.tensorflow:tensorflow-lite:+' in the build.gradle in Android Studio project, the source code of TFLite is read-only and I can't modify it for my project. Is there any other way for me to use the modified source code? Thanks.", "body": "@ymodak @andrehentz \r\nWhen I dig into the source code of TFLite, I found that the above issue was caused by the time-consuming `Shapeof()` functions in Tensor class. To be more specific, the `runInference()` in ImageClassifier.java \r\n```\r\n    // Here's where the magic happens!!!\r\n    long startTime = SystemClock.uptimeMillis();\r\n    runInference();\r\n    long endTime = SystemClock.uptimeMillis();\r\n    Log.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\r\n```\r\nwill call the tflite.run() in ImageClassifierFloatMobileNet.java which is almost same as ImageClassifierFloatInception.java,\r\n```\r\n  @Override\r\n  protected void runInference() {\r\n    //tflite.run(imgData, labelProbArray);  //original official code\r\n    tflite.run(imgData, featMap);\r\n  }\r\n```\r\nwhere the featMap is defined as:\r\n```\r\n    labelProbArray = new float[1][getNumLabels()];  //float[1][1001]  //original official code\r\n    featMap = new float[1][getFeatSizeH()][getFeatSizeW()][getFeatSizeC()];  //float[1][7][7][1024]\r\n```\r\n\r\n************\r\nThe` tflite.run()` function will call the `run()` method of NativeInterpreterWrapper :\r\n```\r\nvoid run(Object[] inputs, Map<Integer, Object> outputs) {\r\n        this.inferenceDurationNanoseconds = -1L;\r\n        if(inputs != null && inputs.length != 0) {\r\n            if(outputs != null && !outputs.isEmpty()) {\r\n                int i;\r\n                for(i = 0; i < inputs.length; ++i) {\r\n                    Tensor tensor = this.getInputTensor(i);\r\n                    int[] newShape = tensor.getInputShapeIfDifferent(inputs[i]);\r\n                    if(newShape != null) {\r\n                        this.resizeInput(i, newShape);\r\n                    }\r\n                }\r\n\r\n                if(!this.isMemoryAllocated) {\r\n                    allocateTensors(this.interpreterHandle, this.errorHandle);\r\n                    this.isMemoryAllocated = true;\r\n                    Arrays.fill(this.outputTensors, (Object)null);\r\n                }\r\n\r\n                for(i = 0; i < inputs.length; ++i) {\r\n                    this.getInputTensor(i).setTo(inputs[i]);\r\n                }\r\n\r\n                long inferenceStartNanos = System.nanoTime();\r\n                run(this.interpreterHandle, this.errorHandle);\r\n                long inferenceDurationNanoseconds = System.nanoTime() - inferenceStartNanos;\r\n                Iterator var7 = outputs.entrySet().iterator();\r\n\r\n                while(var7.hasNext()) {\r\n                    Entry<Integer, Object> output = (Entry)var7.next();\r\n                    this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue());\r\n                }\r\n\r\n                this.inferenceDurationNanoseconds = inferenceDurationNanoseconds;\r\n            } else {\r\n                throw new IllegalArgumentException(\"Input error: Outputs should not be null or empty.\");\r\n            }\r\n        } else {\r\n            throw new IllegalArgumentException(\"Input error: Inputs should not be null or empty.\");\r\n        }\r\n    }\r\n```\r\nFrom the code, we can find that network inference is done by `run(this.interpreterHandle, this.errorHandle);` and `this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue());` copy the feature maps to the output array (featMap =new float[1][7][7][1024]).\r\n\r\nThe time of network inference is decreased with shallower layer in Mobilenet (from 220ms for conv2d_13_pointwise to 77ms forconv2d_3_pointwise). However, the `copyTo()` function is the opposite, the shallowr layer with larger feature maps costs more time to finish the copy operation. This lead to my initial issue described above.\r\n\r\nAs for `copyTo()` function which belong to Tensor class, `this.throwExceptionIfTypeIsIncompatible(dst);` is where the issue occured. When judging whether the shape of the pre-defined output array (featMap) is consistent with the shape of feature map of conv layer, `shapeOf()` function in Tensor class is used to get the shape of pre-defined output array (featMap).\r\n```\r\n    static int[] shapeOf(Object o) {\r\n        int size = numDimensions(o);\r\n        int[] dimensions = new int[size];\r\n        fillShape(o, 0, dimensions);\r\n        return dimensions;\r\n    }\r\n    static void fillShape(Object o, int dim, int[] shape) {\r\n        if(shape != null && dim != shape.length) {\r\n            int len = Array.getLength(o);\r\n            if(shape[dim] == 0) {\r\n                shape[dim] = len;\r\n            } else if(shape[dim] != len) {\r\n                throw new IllegalArgumentException(String.format(\"Mismatched lengths (%d and %d) in dimension %d\", new Object[]{Integer.valueOf(shape[dim]), Integer.valueOf(len), Integer.valueOf(dim)}));\r\n            }\r\n\r\n            for(int i = 0; i < len; ++i) {\r\n                fillShape(Array.get(o, i), dim + 1, shape);\r\n            }\r\n\r\n        }\r\n    }\r\n```\r\n**However, the time complexity of `fillShape()` is O(NxHxWxC), where N is the batchsize, H,W,C is the size of feature map. So, the larger the feature map is, the more time it will costs.** Since the output array (feataMap) is defined as new float[1][7][7][1024], I think `shapeOf()` can be simpler and the fillShape() is not necessary.\r\n\r\nNow, I have figured out what the issue is. However, since I use TFLite by adding `compile 'org.tensorflow:tensorflow-lite:+'` in the build.gradle in Android Studio project, the source code of TFLite is read-only and I can't modify it for my project. Is there any other way for me to use the modified source code? Thanks."}