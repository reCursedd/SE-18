{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16895", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16895/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16895/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16895/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16895", "id": 295949612, "node_id": "MDU6SXNzdWUyOTU5NDk2MTI=", "number": 16895, "title": "Variables disappearing when freezing graph with (fused) BatchNorm", "user": {"login": "GPhilo", "id": 4441724, "node_id": "MDQ6VXNlcjQ0NDE3MjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/4441724?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GPhilo", "html_url": "https://github.com/GPhilo", "followers_url": "https://api.github.com/users/GPhilo/followers", "following_url": "https://api.github.com/users/GPhilo/following{/other_user}", "gists_url": "https://api.github.com/users/GPhilo/gists{/gist_id}", "starred_url": "https://api.github.com/users/GPhilo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GPhilo/subscriptions", "organizations_url": "https://api.github.com/users/GPhilo/orgs", "repos_url": "https://api.github.com/users/GPhilo/repos", "events_url": "https://api.github.com/users/GPhilo/events{/privacy}", "received_events_url": "https://api.github.com/users/GPhilo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-09T17:31:52Z", "updated_at": "2018-02-23T09:31:22Z", "closed_at": "2018-02-23T08:48:41Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: I'm using an inception_resnet_v2 from the slim model zoo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6.0rc0</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: -</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: -</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7.0.5</li>\n<li><strong>GPU model and memory</strong>: 1080, 8GB</li>\n<li><strong>Exact command to reproduce</strong>: -</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>There seem to be a bug in handling fused BatchNorm's variables when freezing a graph.<br>\nBoth <code>moving_mean</code> and <code>moving_average</code> disappear from the graph after <code>inference_graph = extract_sub_graph(input_graph_def, output_node_names)</code> in <code>graph_util.convert_variables_to_constants</code> and this breaks afterwards the <code>optimize_for_inference</code> script, that searches for those two variables-turned-const.</p>\n<p>From inspecting the input and output GraphDef, what I think is the culprit is the definition of the FusedBatchNorm node:</p>\n<details>\n  <summary>Node GraphDef</summary>\n<pre><code>node {\n  name: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/FusedBatchNorm\"\n  op: \"FusedBatchNorm\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/Conv2D\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta/read\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_1\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_2\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"_output_shapes\"\n    value {\n      list {\n        shape {\n          dim {\n            size: -1\n          }\n          dim {\n            size: 149\n          }\n          dim {\n            size: 149\n          }\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n      }\n    }\n  }\n  attr {\n    key: \"data_format\"\n    value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    key: \"epsilon\"\n    value {\n      f: 0.001\n    }\n  }\n  attr {\n    key: \"is_training\"\n    value {\n      b: true\n    }\n  }\n}\n</code></pre>\n</details>\n<p>The <code>input</code>s don't mention neither the mean or average, my suspicion is that <code>extract_sub_graph</code> does not detect them as part of the graph to be kept and strips the nodes.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): I'm using an inception_resnet_v2 from the slim model zoo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.6.0rc0\nPython version: 3.5\nBazel version (if compiling from source): -\nGCC/Compiler version (if compiling from source): -\nCUDA/cuDNN version: 9.0/7.0.5\nGPU model and memory: 1080, 8GB\nExact command to reproduce: -\n\nDescribe the problem\nThere seem to be a bug in handling fused BatchNorm's variables when freezing a graph.\nBoth moving_mean and moving_average disappear from the graph after inference_graph = extract_sub_graph(input_graph_def, output_node_names) in graph_util.convert_variables_to_constants and this breaks afterwards the optimize_for_inference script, that searches for those two variables-turned-const.\nFrom inspecting the input and output GraphDef, what I think is the culprit is the definition of the FusedBatchNorm node:\n\n  Node GraphDef\nnode {\n  name: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/FusedBatchNorm\"\n  op: \"FusedBatchNorm\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/Conv2D\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta/read\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_1\"\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_2\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"_output_shapes\"\n    value {\n      list {\n        shape {\n          dim {\n            size: -1\n          }\n          dim {\n            size: 149\n          }\n          dim {\n            size: 149\n          }\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n        shape {\n          dim {\n            size: 32\n          }\n        }\n      }\n    }\n  }\n  attr {\n    key: \"data_format\"\n    value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    key: \"epsilon\"\n    value {\n      f: 0.001\n    }\n  }\n  attr {\n    key: \"is_training\"\n    value {\n      b: true\n    }\n  }\n}\n\n\nThe inputs don't mention neither the mean or average, my suspicion is that extract_sub_graph does not detect them as part of the graph to be kept and strips the nodes.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I'm using an inception_resnet_v2 from the slim model zoo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0rc0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: 1080, 8GB\r\n- **Exact command to reproduce**: - \r\n\r\n### Describe the problem\r\nThere seem to be a bug in handling fused BatchNorm's variables when freezing a graph.\r\nBoth `moving_mean` and `moving_average` disappear from the graph after `inference_graph = extract_sub_graph(input_graph_def, output_node_names)` in `graph_util.convert_variables_to_constants` and this breaks afterwards the `optimize_for_inference` script, that searches for those two variables-turned-const.\r\n\r\nFrom inspecting the input and output GraphDef, what I think is the culprit is the definition of the FusedBatchNorm node:\r\n\r\n<details>\r\n  <summary>Node GraphDef</summary>\r\n\r\n```\r\nnode {\r\n  name: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/FusedBatchNorm\"\r\n  op: \"FusedBatchNorm\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/Conv2D\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta/read\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_1\"\r\n  input: \"InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/Const_2\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n          dim {\r\n            size: 149\r\n          }\r\n          dim {\r\n            size: 149\r\n          }\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n        shape {\r\n          dim {\r\n            size: 32\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"data_format\"\r\n    value {\r\n      s: \"NHWC\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"epsilon\"\r\n    value {\r\n      f: 0.001\r\n    }\r\n  }\r\n  attr {\r\n    key: \"is_training\"\r\n    value {\r\n      b: true\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n</details>\r\n\r\nThe `input`s don't mention neither the mean or average, my suspicion is that `extract_sub_graph` does not detect them as part of the graph to be kept and strips the nodes.\r\n\r\n"}