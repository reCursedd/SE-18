{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9870", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9870/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9870/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9870/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9870", "id": 228432149, "node_id": "MDU6SXNzdWUyMjg0MzIxNDk=", "number": 9870, "title": "Incorrect inference in convnet with batch size >65535", "user": {"login": "joe-antognini", "id": 7061933, "node_id": "MDQ6VXNlcjcwNjE5MzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7061933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joe-antognini", "html_url": "https://github.com/joe-antognini", "followers_url": "https://api.github.com/users/joe-antognini/followers", "following_url": "https://api.github.com/users/joe-antognini/following{/other_user}", "gists_url": "https://api.github.com/users/joe-antognini/gists{/gist_id}", "starred_url": "https://api.github.com/users/joe-antognini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joe-antognini/subscriptions", "organizations_url": "https://api.github.com/users/joe-antognini/orgs", "repos_url": "https://api.github.com/users/joe-antognini/repos", "events_url": "https://api.github.com/users/joe-antognini/events{/privacy}", "received_events_url": "https://api.github.com/users/joe-antognini/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2017-05-12T23:44:01Z", "updated_at": "2018-01-18T01:22:35Z", "closed_at": "2018-01-18T01:22:35Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.1.0-rc0-61-g1ec6ed5 1.1.0</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0.44</li>\n<li><strong>GPU model and memory</strong>: GTX 1080, 16GB</li>\n<li><strong>Exact command to reproduce</strong>: See source code below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When inference is performed on a convolutional NN with batch sizes larger than 65,535, the outputs on cases after 65,535 are incorrect.  The convolution operation appears to be important because I have not been able to reproduce this problem with a fully connected NN.</p>\n<p>In the source code below, I am training a convnet with one convolutional layer to distinguish between sine waves of two periods.  If I perform inference on a batch size of 100,000, I find that the output for case number 65,535 is correct, but the output for case number 65,536 is zero.</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nn_features = 80\ndef gen_data(size):\n  '''Generate sine waves of one of two periods.'''\n  labels = np.random.randint(0, 2, size)\n  periods = np.zeros(size)\n  periods[labels == 0] = 1\n  periods[labels == 1] = 10\n\n  x = np.arange(0, n_features)\n  data = np.zeros((size, n_features))\n  for i in range(size):\n    data[i] = np.sin(x / periods[i])\n\n    data = np.expand_dims(data, axis=2)\n    return (data, labels)\n\nX = tf.placeholder(tf.float32, [None, n_features, 1])\nY = tf.placeholder(tf.int32, [None])\n\nW_conv = tf.get_variable('W_conv', [8, 1, 16])\npreact = tf.nn.conv1d(X, W_conv, stride=1, padding='SAME')\npooled = tf.nn.pool(preact, window_shape=[2], pooling_type='MAX', padding='SAME', strides=[2])\nconv_output = tf.contrib.layers.flatten(tf.nn.relu(pooled))\n\nW_out = tf.get_variable('W_out', [int(n_features/2) * 16, 1])\nlogits = tf.squeeze(tf.matmul(conv_output, W_out))\n\nxentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n    labels=tf.cast(Y, tf.float32), logits=logits))\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(xentropy)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# Train a little while with small batches\nfor i in range(1000):\n  data, labels = gen_data(size=128)\n  sess.run(train_step, feed_dict={X: data, Y: labels})\n\n# Now try performing inference on a much larger batch\nlarge_batch = np.zeros((100000, n_features, 1))\nlarge_batch[2**16 - 1] = data[0]\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\n\n# This should be some non-zero number\nprint('Logit for input number 65,535:', logits_[2**16-1])\n\n# Now do the same thing, but putting the input in spot 65,536\nlarge_batch = np.zeros((100000, n_features, 1))\nlarge_batch[2**16] = data[0]\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\n\n# This is now zero on my system\nprint('Logit for input number 65,536:', logits_[2**16])\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): v1.1.0-rc0-61-g1ec6ed5 1.1.0\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: 8.0.44\nGPU model and memory: GTX 1080, 16GB\nExact command to reproduce: See source code below\n\nDescribe the problem\nWhen inference is performed on a convolutional NN with batch sizes larger than 65,535, the outputs on cases after 65,535 are incorrect.  The convolution operation appears to be important because I have not been able to reproduce this problem with a fully connected NN.\nIn the source code below, I am training a convnet with one convolutional layer to distinguish between sine waves of two periods.  If I perform inference on a batch size of 100,000, I find that the output for case number 65,535 is correct, but the output for case number 65,536 is zero.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\n\nn_features = 80\ndef gen_data(size):\n  '''Generate sine waves of one of two periods.'''\n  labels = np.random.randint(0, 2, size)\n  periods = np.zeros(size)\n  periods[labels == 0] = 1\n  periods[labels == 1] = 10\n\n  x = np.arange(0, n_features)\n  data = np.zeros((size, n_features))\n  for i in range(size):\n    data[i] = np.sin(x / periods[i])\n\n    data = np.expand_dims(data, axis=2)\n    return (data, labels)\n\nX = tf.placeholder(tf.float32, [None, n_features, 1])\nY = tf.placeholder(tf.int32, [None])\n\nW_conv = tf.get_variable('W_conv', [8, 1, 16])\npreact = tf.nn.conv1d(X, W_conv, stride=1, padding='SAME')\npooled = tf.nn.pool(preact, window_shape=[2], pooling_type='MAX', padding='SAME', strides=[2])\nconv_output = tf.contrib.layers.flatten(tf.nn.relu(pooled))\n\nW_out = tf.get_variable('W_out', [int(n_features/2) * 16, 1])\nlogits = tf.squeeze(tf.matmul(conv_output, W_out))\n\nxentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n    labels=tf.cast(Y, tf.float32), logits=logits))\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(xentropy)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# Train a little while with small batches\nfor i in range(1000):\n  data, labels = gen_data(size=128)\n  sess.run(train_step, feed_dict={X: data, Y: labels})\n\n# Now try performing inference on a much larger batch\nlarge_batch = np.zeros((100000, n_features, 1))\nlarge_batch[2**16 - 1] = data[0]\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\n\n# This should be some non-zero number\nprint('Logit for input number 65,535:', logits_[2**16-1])\n\n# Now do the same thing, but putting the input in spot 65,536\nlarge_batch = np.zeros((100000, n_features, 1))\nlarge_batch[2**16] = data[0]\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\n\n# This is now zero on my system\nprint('Logit for input number 65,536:', logits_[2**16])", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0.44\r\n- **GPU model and memory**: GTX 1080, 16GB\r\n- **Exact command to reproduce**: See source code below\r\n\r\n\r\n### Describe the problem\r\n\r\nWhen inference is performed on a convolutional NN with batch sizes larger than 65,535, the outputs on cases after 65,535 are incorrect.  The convolution operation appears to be important because I have not been able to reproduce this problem with a fully connected NN.\r\n\r\nIn the source code below, I am training a convnet with one convolutional layer to distinguish between sine waves of two periods.  If I perform inference on a batch size of 100,000, I find that the output for case number 65,535 is correct, but the output for case number 65,536 is zero.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nn_features = 80\r\ndef gen_data(size):\r\n  '''Generate sine waves of one of two periods.'''\r\n  labels = np.random.randint(0, 2, size)\r\n  periods = np.zeros(size)\r\n  periods[labels == 0] = 1\r\n  periods[labels == 1] = 10\r\n\r\n  x = np.arange(0, n_features)\r\n  data = np.zeros((size, n_features))\r\n  for i in range(size):\r\n    data[i] = np.sin(x / periods[i])\r\n\r\n    data = np.expand_dims(data, axis=2)\r\n    return (data, labels)\r\n\r\nX = tf.placeholder(tf.float32, [None, n_features, 1])\r\nY = tf.placeholder(tf.int32, [None])\r\n\r\nW_conv = tf.get_variable('W_conv', [8, 1, 16])\r\npreact = tf.nn.conv1d(X, W_conv, stride=1, padding='SAME')\r\npooled = tf.nn.pool(preact, window_shape=[2], pooling_type='MAX', padding='SAME', strides=[2])\r\nconv_output = tf.contrib.layers.flatten(tf.nn.relu(pooled))\r\n\r\nW_out = tf.get_variable('W_out', [int(n_features/2) * 16, 1])\r\nlogits = tf.squeeze(tf.matmul(conv_output, W_out))\r\n\r\nxentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\r\n    labels=tf.cast(Y, tf.float32), logits=logits))\r\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(xentropy)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n# Train a little while with small batches\r\nfor i in range(1000):\r\n  data, labels = gen_data(size=128)\r\n  sess.run(train_step, feed_dict={X: data, Y: labels})\r\n\r\n# Now try performing inference on a much larger batch\r\nlarge_batch = np.zeros((100000, n_features, 1))\r\nlarge_batch[2**16 - 1] = data[0]\r\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\r\n\r\n# This should be some non-zero number\r\nprint('Logit for input number 65,535:', logits_[2**16-1])\r\n\r\n# Now do the same thing, but putting the input in spot 65,536\r\nlarge_batch = np.zeros((100000, n_features, 1))\r\nlarge_batch[2**16] = data[0]\r\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\r\n\r\n# This is now zero on my system\r\nprint('Logit for input number 65,536:', logits_[2**16])\r\n```"}