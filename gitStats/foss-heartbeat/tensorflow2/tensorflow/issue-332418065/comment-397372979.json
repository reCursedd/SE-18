{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/397372979", "html_url": "https://github.com/tensorflow/tensorflow/issues/20021#issuecomment-397372979", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20021", "id": 397372979, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzM3Mjk3OQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T17:20:46Z", "updated_at": "2018-06-14T17:20:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Akshay: I've run a few versions of this, and think it could be a bug in <code>tfe.py_func()</code>, so could you please take a look? Here are some notes:</p>\n<ul>\n<li>The crash only happens (1) when a GPU is available, and (2) when <code>Dataset.prefetch()</code> is used.</li>\n<li>The crash doesn't seem to depend on using <code>tf.cast()</code>. Replacing <code>fn</code> with <code>lambda _: tf.constant([1., 2., 3.]) + tf.constant([4., 5., 6.])</code> gives the same failure.</li>\n<li>The crashing stack suggests that we are treating an <code>EagerTensor</code> that is actually in GPU memory as if it were in host memory:</li>\n</ul>\n<pre><code>    @     ...\n    @     0x55e3661fb60c        400  tensorflow::TF_TensorToPyArray()\n    @     0x55e3661fd2e3         96  tensorflow::TensorToNdarray()\n    @     0x55e3661c30ea         96  EagerTensor_numpy()\n    @     0x55e36295a4f3        320  PyEval_EvalFrameEx\n    @     ...\n</code></pre>\n<ul>\n<li>When <code>Dataset.prefetch()</code> is used, the <code>tf.cast()</code> kernel runs on GPU. When the <code>Dataset.prefetch()</code> is removed, the <code>tf.cast()</code> kernel runs on CPU. The main difference between these two  cases is that when <code>Dataset.prefetch()</code> is used, the body of the <code>py_func</code> will run on a background thread, so maybe some thread-local state isn't configured?</li>\n<li>Even though the <code>tf.cast()</code> kernel runs on GPU (when <code>Dataset.prefetch()</code> is used), the <code>TensorHandle</code> for its result has <code>nullptr</code> for its device, which appears to be why <code>EagerTensor_numpy()</code> thinks it is safe to copy.</li>\n</ul>\n<p>/CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a>, since the <code>TensorHandle::device()</code> method has comments with TODOs against his name, and so he might know where the skeletons are :).</p>", "body_text": "Akshay: I've run a few versions of this, and think it could be a bug in tfe.py_func(), so could you please take a look? Here are some notes:\n\nThe crash only happens (1) when a GPU is available, and (2) when Dataset.prefetch() is used.\nThe crash doesn't seem to depend on using tf.cast(). Replacing fn with lambda _: tf.constant([1., 2., 3.]) + tf.constant([4., 5., 6.]) gives the same failure.\nThe crashing stack suggests that we are treating an EagerTensor that is actually in GPU memory as if it were in host memory:\n\n    @     ...\n    @     0x55e3661fb60c        400  tensorflow::TF_TensorToPyArray()\n    @     0x55e3661fd2e3         96  tensorflow::TensorToNdarray()\n    @     0x55e3661c30ea         96  EagerTensor_numpy()\n    @     0x55e36295a4f3        320  PyEval_EvalFrameEx\n    @     ...\n\n\nWhen Dataset.prefetch() is used, the tf.cast() kernel runs on GPU. When the Dataset.prefetch() is removed, the tf.cast() kernel runs on CPU. The main difference between these two  cases is that when Dataset.prefetch() is used, the body of the py_func will run on a background thread, so maybe some thread-local state isn't configured?\nEven though the tf.cast() kernel runs on GPU (when Dataset.prefetch() is used), the TensorHandle for its result has nullptr for its device, which appears to be why EagerTensor_numpy() thinks it is safe to copy.\n\n/CC @asimshankar, since the TensorHandle::device() method has comments with TODOs against his name, and so he might know where the skeletons are :).", "body": "Akshay: I've run a few versions of this, and think it could be a bug in `tfe.py_func()`, so could you please take a look? Here are some notes:\r\n\r\n* The crash only happens (1) when a GPU is available, and (2) when `Dataset.prefetch()` is used.\r\n* The crash doesn't seem to depend on using `tf.cast()`. Replacing `fn` with `lambda _: tf.constant([1., 2., 3.]) + tf.constant([4., 5., 6.])` gives the same failure.\r\n* The crashing stack suggests that we are treating an `EagerTensor` that is actually in GPU memory as if it were in host memory:\r\n\r\n```\r\n    @     ...\r\n    @     0x55e3661fb60c        400  tensorflow::TF_TensorToPyArray()\r\n    @     0x55e3661fd2e3         96  tensorflow::TensorToNdarray()\r\n    @     0x55e3661c30ea         96  EagerTensor_numpy()\r\n    @     0x55e36295a4f3        320  PyEval_EvalFrameEx\r\n    @     ...\r\n```\r\n\r\n* When `Dataset.prefetch()` is used, the `tf.cast()` kernel runs on GPU. When the `Dataset.prefetch()` is removed, the `tf.cast()` kernel runs on CPU. The main difference between these two  cases is that when `Dataset.prefetch()` is used, the body of the `py_func` will run on a background thread, so maybe some thread-local state isn't configured?\r\n* Even though the `tf.cast()` kernel runs on GPU (when `Dataset.prefetch()` is used), the `TensorHandle` for its result has `nullptr` for its device, which appears to be why `EagerTensor_numpy()` thinks it is safe to copy.\r\n\r\n/CC @asimshankar, since the `TensorHandle::device()` method has comments with TODOs against his name, and so he might know where the skeletons are :)."}