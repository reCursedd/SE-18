{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23516", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23516/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23516/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23516/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23516", "id": 377313015, "node_id": "MDU6SXNzdWUzNzczMTMwMTU=", "number": 23516, "title": "tensorflow conv2d NCHW with mkl slower than NHWC without mkl  on cpu platform", "user": {"login": "lxn179208", "id": 811605, "node_id": "MDQ6VXNlcjgxMTYwNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/811605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lxn179208", "html_url": "https://github.com/lxn179208", "followers_url": "https://api.github.com/users/lxn179208/followers", "following_url": "https://api.github.com/users/lxn179208/following{/other_user}", "gists_url": "https://api.github.com/users/lxn179208/gists{/gist_id}", "starred_url": "https://api.github.com/users/lxn179208/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lxn179208/subscriptions", "organizations_url": "https://api.github.com/users/lxn179208/orgs", "repos_url": "https://api.github.com/users/lxn179208/repos", "events_url": "https://api.github.com/users/lxn179208/events{/privacy}", "received_events_url": "https://api.github.com/users/lxn179208/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1104829434, "node_id": "MDU6TGFiZWwxMTA0ODI5NDM0", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:mkl", "name": "comp:mkl", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "TensorFlow-MKL", "id": 44416303, "node_id": "MDQ6VXNlcjQ0NDE2MzAz", "avatar_url": "https://avatars2.githubusercontent.com/u/44416303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TensorFlow-MKL", "html_url": "https://github.com/TensorFlow-MKL", "followers_url": "https://api.github.com/users/TensorFlow-MKL/followers", "following_url": "https://api.github.com/users/TensorFlow-MKL/following{/other_user}", "gists_url": "https://api.github.com/users/TensorFlow-MKL/gists{/gist_id}", "starred_url": "https://api.github.com/users/TensorFlow-MKL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TensorFlow-MKL/subscriptions", "organizations_url": "https://api.github.com/users/TensorFlow-MKL/orgs", "repos_url": "https://api.github.com/users/TensorFlow-MKL/repos", "events_url": "https://api.github.com/users/TensorFlow-MKL/events{/privacy}", "received_events_url": "https://api.github.com/users/TensorFlow-MKL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "TensorFlow-MKL", "id": 44416303, "node_id": "MDQ6VXNlcjQ0NDE2MzAz", "avatar_url": "https://avatars2.githubusercontent.com/u/44416303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TensorFlow-MKL", "html_url": "https://github.com/TensorFlow-MKL", "followers_url": "https://api.github.com/users/TensorFlow-MKL/followers", "following_url": "https://api.github.com/users/TensorFlow-MKL/following{/other_user}", "gists_url": "https://api.github.com/users/TensorFlow-MKL/gists{/gist_id}", "starred_url": "https://api.github.com/users/TensorFlow-MKL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TensorFlow-MKL/subscriptions", "organizations_url": "https://api.github.com/users/TensorFlow-MKL/orgs", "repos_url": "https://api.github.com/users/TensorFlow-MKL/repos", "events_url": "https://api.github.com/users/TensorFlow-MKL/events{/privacy}", "received_events_url": "https://api.github.com/users/TensorFlow-MKL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-05T09:24:24Z", "updated_at": "2018-11-20T13:25:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I tested tensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.</p>\n<p><strong>System information</strong></p>\n<ul>\n<li>\n<p>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y</p>\n</li>\n<li>\n<p>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux</p>\n</li>\n<li>\n<p>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N</p>\n</li>\n<li>\n<p>TensorFlow installed from (source or binary):binary<br>\ntf-mkl:<br>\npip install <a href=\"https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl</a><br>\ntf-no-mkl:<br>\nwget <a href=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.11.0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.11.0-cp27-none-linux_x86_64.whl</a><br>\npip install tensorflow-1.11.0-cp27-none-linux_x86_64.whl</p>\n</li>\n<li>\n<p>TensorFlow version (use command below):1.111.0</p>\n</li>\n<li>\n<p>Python version:2.7.5</p>\n</li>\n<li>\n<p>Bazel version (if compiling from source):N</p>\n</li>\n<li>\n<p>GCC/Compiler version (if compiling from source):N</p>\n</li>\n<li>\n<p>CUDA/cuDNN version:N</p>\n</li>\n<li>\n<p>GPU model and memory:N</p>\n</li>\n<li>\n<p>CPU: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz<br>\nThread(s) per core:    2<br>\nCore(s) per socket:    16<br>\nSocket(s):             2</p>\n</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\ntensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p><strong>Code to reproduce the issue</strong><br>\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<pre><code>import os\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport time\n\n\"\"\"\ndefault NWC\ninput=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]\n=&gt;tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]\n\nmkl NCW\ninput=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]\n=&gt;tf.nn.conv2d []\n\"\"\"\n\nbatch = 32\nin_width = 102400\nin_height = 1\nin_channels = 128\nfilter_width = 3\nfilter_height = 1\nout_channels = 128\ndata_format_2d = sys.argv[2]\nn = 100\nstart = 99\ninput_2d = {}\n\nif len(sys.argv) &gt; 1 and sys.argv[1] == \"ow\":\n    print(\"ow=1\")\n    in_width,in_height = in_height, in_width\n    filter_width,filter_height = filter_height, filter_width\nelse:\n    print(\"oh=1\")\nif data_format_2d == \"NCHW\":\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_channels, in_height,in_width]), dtype=np.float32)\nelse:\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_height,in_width,in_channels]), dtype=np.float32)\nfor i in range(start,n):\n    input_2d[i] = input_ + i*n*i*1.0\nkernel_2d = np.array(np.arange(1, 1 + filter_width*filter_height*in_channels*out_channels), dtype=np.float32).reshape([filter_height, filter_width, in_channels, out_channels])\na = tf.placeholder(dtype=tf.float32)\nconv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)\n\nconfig = tf.ConfigProto()\nconfig.intra_op_parallelism_threads = 32\nconfig.inter_op_parallelism_threads = 2\nsess = tf.Session(config=config)\n\nprint(\"start\")\nsess.run(tf.global_variables_initializer())\nstart_ts = time.time()\nfor i in range(start,n):\n    sess.run(conv2d, feed_dict={a:input_2d[i]})\nprint(\"data_format_2d=%s %d epoch cost %f\" % (data_format_2d,n-start, (time.time()) - start_ts))\nprint(\"shape:\")\nprint(np.shape(input_2d[start]))\n</code></pre>\n<p>cmd to exec</p>\n<pre><code>MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\n</code></pre>\n<p>or</p>\n<pre><code>MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NCHW\n</code></pre>\n<p><strong>Other info / logs</strong><br>\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.<br>\ntf-1.11.0 with mkl</p>\n<pre><code>MKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  ow NCHW\n</code></pre>\n<p>output:</p>\n<pre><code>ow=1\n2018-11-05 17:17:06.151312: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nstart\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x102400x1,83.739\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x3x1,0.157959\nmkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih102400oh102398kh3sh1dh0ph0_iw1ow1kw1sw1dw0pw0,1010.23\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x102398x1,181.471\ndata_format_2d=NCHW 1 epoch cost 2.135184\nshape:\n(32, 128, 102400, 1)\n</code></pre>\n<p>tf-1.11.0 without mkl</p>\n<pre><code>MKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\n</code></pre>\n<p>output</p>\n<pre><code>oh=1\n2018-11-05 17:16:30.370120: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nstart\ndata_format_2d=NHWC 1 epoch cost 1.402521\nshape:\n(32, 1, 102400, 128)\n</code></pre>", "body_text": "I tested tensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.\nSystem information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N\n\n\nTensorFlow installed from (source or binary):binary\ntf-mkl:\npip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl\ntf-no-mkl:\nwget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.11.0-cp27-none-linux_x86_64.whl\npip install tensorflow-1.11.0-cp27-none-linux_x86_64.whl\n\n\nTensorFlow version (use command below):1.111.0\n\n\nPython version:2.7.5\n\n\nBazel version (if compiling from source):N\n\n\nGCC/Compiler version (if compiling from source):N\n\n\nCUDA/cuDNN version:N\n\n\nGPU model and memory:N\n\n\nCPU: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz\nThread(s) per core:    2\nCore(s) per socket:    16\nSocket(s):             2\n\n\nDescribe the current behavior\ntensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.\nDescribe the expected behavior\nCode to reproduce the issue\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\nimport os\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport time\n\n\"\"\"\ndefault NWC\ninput=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]\n=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]\n\nmkl NCW\ninput=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]\n=>tf.nn.conv2d []\n\"\"\"\n\nbatch = 32\nin_width = 102400\nin_height = 1\nin_channels = 128\nfilter_width = 3\nfilter_height = 1\nout_channels = 128\ndata_format_2d = sys.argv[2]\nn = 100\nstart = 99\ninput_2d = {}\n\nif len(sys.argv) > 1 and sys.argv[1] == \"ow\":\n    print(\"ow=1\")\n    in_width,in_height = in_height, in_width\n    filter_width,filter_height = filter_height, filter_width\nelse:\n    print(\"oh=1\")\nif data_format_2d == \"NCHW\":\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_channels, in_height,in_width]), dtype=np.float32)\nelse:\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_height,in_width,in_channels]), dtype=np.float32)\nfor i in range(start,n):\n    input_2d[i] = input_ + i*n*i*1.0\nkernel_2d = np.array(np.arange(1, 1 + filter_width*filter_height*in_channels*out_channels), dtype=np.float32).reshape([filter_height, filter_width, in_channels, out_channels])\na = tf.placeholder(dtype=tf.float32)\nconv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)\n\nconfig = tf.ConfigProto()\nconfig.intra_op_parallelism_threads = 32\nconfig.inter_op_parallelism_threads = 2\nsess = tf.Session(config=config)\n\nprint(\"start\")\nsess.run(tf.global_variables_initializer())\nstart_ts = time.time()\nfor i in range(start,n):\n    sess.run(conv2d, feed_dict={a:input_2d[i]})\nprint(\"data_format_2d=%s %d epoch cost %f\" % (data_format_2d,n-start, (time.time()) - start_ts))\nprint(\"shape:\")\nprint(np.shape(input_2d[start]))\n\ncmd to exec\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\n\nor\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NCHW\n\nOther info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\ntf-1.11.0 with mkl\nMKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  ow NCHW\n\noutput:\now=1\n2018-11-05 17:17:06.151312: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nstart\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x102400x1,83.739\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x3x1,0.157959\nmkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih102400oh102398kh3sh1dh0ph0_iw1ow1kw1sw1dw0pw0,1010.23\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x102398x1,181.471\ndata_format_2d=NCHW 1 epoch cost 2.135184\nshape:\n(32, 128, 102400, 1)\n\ntf-1.11.0 without mkl\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\n\noutput\noh=1\n2018-11-05 17:16:30.370120: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nstart\ndata_format_2d=NHWC 1 epoch cost 1.402521\nshape:\n(32, 1, 102400, 128)", "body": "I tested tensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N\r\n- TensorFlow installed from (source or binary):binary\r\ntf-mkl:\r\npip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl \r\ntf-no-mkl:\r\nwget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.11.0-cp27-none-linux_x86_64.whl\r\npip install tensorflow-1.11.0-cp27-none-linux_x86_64.whl\r\n\r\n- TensorFlow version (use command below):1.111.0\r\n- Python version:2.7.5\r\n- Bazel version (if compiling from source):N\r\n- GCC/Compiler version (if compiling from source):N\r\n- CUDA/cuDNN version:N\r\n- GPU model and memory:N\r\n- CPU: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz\r\nThread(s) per core:    2\r\nCore(s) per socket:    16\r\nSocket(s):             2\r\n\r\n**Describe the current behavior**\r\ntensorflow conv2d with NCHW with mkl slower than NHWC wihout mkl on cpu platform.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n\"\"\"\r\ndefault NWC\r\ninput=[batch, in_width, in_channels]  filter=[ filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d  [batch,1,in_width, in_channels]  [1, filter_width, in_channels, out_channels]\r\n\r\nmkl NCW\r\ninput=[batch, in_channels, in_width] filter=[filter_width, in_channels, out_channels]\r\n=>tf.nn.conv2d []\r\n\"\"\"\r\n\r\nbatch = 32\r\nin_width = 102400\r\nin_height = 1\r\nin_channels = 128\r\nfilter_width = 3\r\nfilter_height = 1\r\nout_channels = 128\r\ndata_format_2d = sys.argv[2]\r\nn = 100\r\nstart = 99\r\ninput_2d = {}\r\n\r\nif len(sys.argv) > 1 and sys.argv[1] == \"ow\":\r\n    print(\"ow=1\")\r\n    in_width,in_height = in_height, in_width\r\n    filter_width,filter_height = filter_height, filter_width\r\nelse:\r\n    print(\"oh=1\")\r\nif data_format_2d == \"NCHW\":\r\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_channels, in_height,in_width]), dtype=np.float32)\r\nelse:\r\n    input_ = np.array(np.arange(1, 1 + batch*in_width*in_height*in_channels).reshape([batch, in_height,in_width,in_channels]), dtype=np.float32)\r\nfor i in range(start,n):\r\n    input_2d[i] = input_ + i*n*i*1.0\r\nkernel_2d = np.array(np.arange(1, 1 + filter_width*filter_height*in_channels*out_channels), dtype=np.float32).reshape([filter_height, filter_width, in_channels, out_channels])\r\na = tf.placeholder(dtype=tf.float32)\r\nconv2d = tf.nn.conv2d(a, kernel_2d, [1,1,1,1], 'VALID', data_format=data_format_2d)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.intra_op_parallelism_threads = 32\r\nconfig.inter_op_parallelism_threads = 2\r\nsess = tf.Session(config=config)\r\n\r\nprint(\"start\")\r\nsess.run(tf.global_variables_initializer())\r\nstart_ts = time.time()\r\nfor i in range(start,n):\r\n    sess.run(conv2d, feed_dict={a:input_2d[i]})\r\nprint(\"data_format_2d=%s %d epoch cost %f\" % (data_format_2d,n-start, (time.time()) - start_ts))\r\nprint(\"shape:\")\r\nprint(np.shape(input_2d[start]))\r\n```\r\ncmd to exec\r\n```\r\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\r\n```\r\nor\r\n```\r\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NCHW\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\ntf-1.11.0 with mkl\r\n```\r\nMKLDNN_VERBOSE=1 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  ow NCHW\r\n```\r\noutput:\r\n```\r\now=1\r\n2018-11-05 17:17:06.151312: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nstart\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nchw out:f32_nChw8c,num:1,32x128x102400x1,83.739\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_hwio out:f32_OIhw8i8o,num:1,128x128x3x1,0.157959\r\nmkldnn_verbose,exec,convolution,jit:avx2,forward_training,fsrc:nChw8c fwei:OIhw8i8o fbia:undef fdst:nChw8c,alg:convolution_direct,mb32_g1ic128oc128_ih102400oh102398kh3sh1dh0ph0_iw1ow1kw1sw1dw0pw0,1010.23\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nChw8c out:f32_nchw,num:1,32x128x102398x1,181.471\r\ndata_format_2d=NCHW 1 epoch cost 2.135184\r\nshape:\r\n(32, 128, 102400, 1)\r\n```\r\ntf-1.11.0 without mkl\r\n```\r\nMKLDNN_VERBOSE=0 KMP_BLOCKTIME=0 CUDA_VISIBLE_DEVICES=-1  OMP_NUM_THREADS=32 python conv2d.py  oh NHWC\r\n```\r\noutput\r\n```\r\noh=1\r\n2018-11-05 17:16:30.370120: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nstart\r\ndata_format_2d=NHWC 1 epoch cost 1.402521\r\nshape:\r\n(32, 1, 102400, 128)\r\n```\r\n"}