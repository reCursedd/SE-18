{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6635", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6635/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6635/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6635/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6635", "id": 198703969, "node_id": "MDU6SXNzdWUxOTg3MDM5Njk=", "number": 6635, "title": "Incorrect gradient when using tf.dynamic_stitch and tf.gather?", "user": {"login": "adamsyu", "id": 7608404, "node_id": "MDQ6VXNlcjc2MDg0MDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/7608404?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamsyu", "html_url": "https://github.com/adamsyu", "followers_url": "https://api.github.com/users/adamsyu/followers", "following_url": "https://api.github.com/users/adamsyu/following{/other_user}", "gists_url": "https://api.github.com/users/adamsyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamsyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamsyu/subscriptions", "organizations_url": "https://api.github.com/users/adamsyu/orgs", "repos_url": "https://api.github.com/users/adamsyu/repos", "events_url": "https://api.github.com/users/adamsyu/events{/privacy}", "received_events_url": "https://api.github.com/users/adamsyu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-01-04T13:11:04Z", "updated_at": "2018-02-07T23:52:03Z", "closed_at": "2018-02-07T23:52:03Z", "author_association": "NONE", "body_html": "<p>In Tensorflow 0.12, I find the discrepancy of gradients in two mathematically equivalent training procedures of LSTM, probably due to the use of tf.gather and tf.dynamic_stitch.  One is the normal procedure using the whole batch of training examples to unroll the LSTM in each step. The other first uses tf.gather to select ALL the examples of the whole batch in each step, then unroll the LSTM with those examples and finally use tf.dynamic_stitch to update the corresponding states and outputs.</p>\n<p>These two procedures should be equivalent as they both essentially use the whole batch. However, the gradients of the same variables are significantly different.</p>\n<p>The code is as follows (the core parts are essentially <code># 1.</code> and  <code># 2.</code>):</p>\n<div class=\"highlight highlight-source-python\"><pre>batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\nnum_timesteps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nvocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nnum_embedding_nodes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nhidden_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\nn_class <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\nlearning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.001</span>\ninputs <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, [batch_size, num_timesteps])\ntargets <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, [batch_size])\nembedding <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding<span class=\"pl-pds\">\"</span></span>, [vocab_size, num_embedding_nodes])\nx <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(embedding, inputs)\nw_predict <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>w_predict<span class=\"pl-pds\">\"</span></span>, [hidden_size, n_class])\nb_predict <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>b_predict<span class=\"pl-pds\">\"</span></span>, [n_class])\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>lstm<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> lstm_scope:\n  cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(hidden_size, <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n  state <span class=\"pl-k\">=</span> cell.zero_state(batch_size, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  state1 <span class=\"pl-k\">=</span> cell.zero_state(batch_size, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  <span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_timesteps):\n    <span class=\"pl-k\">if</span> t <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n      output, state <span class=\"pl-k\">=</span> cell(x[:, t, :], state)\n      lstm_scope.reuse_variables()\n      output1, state1 <span class=\"pl-k\">=</span> cell(x[:, t, :], state1)\n    <span class=\"pl-k\">else</span>:\n      lstm_scope.reuse_variables()\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1. normal lstm </span>\n      output, state <span class=\"pl-k\">=</span> cell(x[:, t, :], state)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> 2. lstm using tf.gather and tf.dynamic_stitch to select all samples from batch</span>\n      idx_select <span class=\"pl-k\">=</span> tf.range(batch_size)\n      tmp_output, tmp_state <span class=\"pl-k\">=</span> cell(tf.gather(x[:, t, :], idx_select), tf.gather(state1, idx_select))\n      output1 <span class=\"pl-k\">=</span> tf.dynamic_stitch([tf.range(batch_size), idx_select], [output1, tmp_output])\n      state1 <span class=\"pl-k\">=</span> tf.dynamic_stitch([tf.range(batch_size), idx_select], [state1, tmp_state])\nlogits <span class=\"pl-k\">=</span> tf.nn.xw_plus_b(output, w_predict, b_predict)\ncross_entropy <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\nlogits1 <span class=\"pl-k\">=</span> tf.nn.xw_plus_b(output1, w_predict, b_predict)\ncross_entropy1 <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, targets, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\ntrainable <span class=\"pl-k\">=</span> tf.trainable_variables()\ngrad <span class=\"pl-k\">=</span> tf.gradients(cross_entropy, trainable)\ngrad1 <span class=\"pl-k\">=</span> tf.gradients(cross_entropy1, trainable)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>####### gradient log</span>\ncg <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> g <span class=\"pl-k\">in</span> grad:\n  <span class=\"pl-k\">if</span> g <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n    cg <span class=\"pl-k\">+=</span> [g]\ncg1 <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> g <span class=\"pl-k\">in</span> grad1:\n  <span class=\"pl-k\">if</span> g <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n    cg1 <span class=\"pl-k\">+=</span> [g]\n\noptimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(learning_rate)\ntrain_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(grad, trainable))\ninit_op <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n\n\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Training <span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># arbitrary synthetic data</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> we use two training examples, each with length 10</span>\nmy_input <span class=\"pl-k\">=</span> np.array([[<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">6</span>,<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">9</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">4</span>], [<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">6</span>,<span class=\"pl-c1\">1</span>]])\nmy_target <span class=\"pl-k\">=</span> np.array([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>])\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(init_op)\n\nepoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">while</span> epoch <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>:\n  epoch <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n  fetches <span class=\"pl-k\">=</span> [train_op, cg, cg1]\n  outputs <span class=\"pl-k\">=</span> sess.run(fetches, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{inputs: my_input, targets: my_target})\n  gradients <span class=\"pl-k\">=</span> outputs[<span class=\"pl-c1\">1</span>] \n  gradients1 <span class=\"pl-k\">=</span> outputs[<span class=\"pl-c1\">2</span>] \n  <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch <span class=\"pl-c1\">%d</span>:<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> epoch \n    <span class=\"pl-k\">for</span> i, g <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(gradients):\n      <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>norm of gradient of var <span class=\"pl-c1\">%d</span>: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (i, <span class=\"pl-c1\">LA</span>.norm(gradients[i])))\n\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>norm of gradient1 of var <span class=\"pl-c1\">%d</span>: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (i, <span class=\"pl-c1\">LA</span>.norm(gradients1[i])))</pre></div>\n<p>The sample output is:</p>\n<pre><code>epoch 1:\nnorm of gradient of var 1: 0.644620\nnorm of gradient1 of var 1: 0.644620\nnorm of gradient of var 2: 0.393020\nnorm of gradient1 of var 2: 0.393020\nnorm of gradient of var 3: 0.838759\nnorm of gradient1 of var 3: 102.815338\nnorm of gradient of var 4: 0.435841\nnorm of gradient1 of var 4: 44.867126\nepoch 2:\nnorm of gradient of var 1: 0.613848\nnorm of gradient1 of var 1: 0.611423\nnorm of gradient of var 2: 0.355387\nnorm of gradient1 of var 2: 0.351987\nnorm of gradient of var 3: 0.797761\nnorm of gradient1 of var 3: 96.391121\nnorm of gradient of var 4: 0.397020\nnorm of gradient1 of var 4: 39.937107\nepoch 3:\nnorm of gradient of var 1: 0.603118\nnorm of gradient1 of var 1: 0.603118\nnorm of gradient of var 2: 0.318260\nnorm of gradient1 of var 2: 0.318260\nnorm of gradient of var 3: 0.773661\nnorm of gradient1 of var 3: 93.290131\nnorm of gradient of var 4: 0.366684\nnorm of gradient1 of var 4: 36.636879\nepoch 4:\nnorm of gradient of var 1: 0.607643\nnorm of gradient1 of var 1: 0.607643\nnorm of gradient of var 2: 0.280101\nnorm of gradient1 of var 2: 0.280101\nnorm of gradient of var 3: 0.763007\nnorm of gradient1 of var 3: 92.295441\nnorm of gradient of var 4: 0.340769\nnorm of gradient1 of var 4: 33.630474\nepoch 5:\nnorm of gradient of var 1: 0.622874\nnorm of gradient1 of var 1: 0.619443\nnorm of gradient of var 2: 0.239731\nnorm of gradient1 of var 2: 0.235509\nnorm of gradient of var 3: 0.757205\nnorm of gradient1 of var 3: 93.335388\nnorm of gradient of var 4: 0.312203\nnorm of gradient1 of var 4: 30.536301\n</code></pre>\n<p>We can see that the gradient and gradient1 of var3 have significantly different norms in every epoch, which should be the same. So is var4. Those two are the trainable variables of LSTM. In fact, if the sequence length is 50 instead of 10, the discrepancy is even much larger.</p>\n<p>Could anybody tell me why it is the case?</p>\n<h3>Environment info</h3>\n<p>Operating System: ubuntu 14.04</p>", "body_text": "In Tensorflow 0.12, I find the discrepancy of gradients in two mathematically equivalent training procedures of LSTM, probably due to the use of tf.gather and tf.dynamic_stitch.  One is the normal procedure using the whole batch of training examples to unroll the LSTM in each step. The other first uses tf.gather to select ALL the examples of the whole batch in each step, then unroll the LSTM with those examples and finally use tf.dynamic_stitch to update the corresponding states and outputs.\nThese two procedures should be equivalent as they both essentially use the whole batch. However, the gradients of the same variables are significantly different.\nThe code is as follows (the core parts are essentially # 1. and  # 2.):\nbatch_size = 2\nnum_timesteps = 10\nvocab_size = 10\nnum_embedding_nodes = 32\nhidden_size = 128\nn_class = 2\nlearning_rate = 0.001\ninputs = tf.placeholder(tf.int64, [batch_size, num_timesteps])\ntargets = tf.placeholder(tf.int64, [batch_size])\nembedding = tf.get_variable(\"embedding\", [vocab_size, num_embedding_nodes])\nx = tf.nn.embedding_lookup(embedding, inputs)\nw_predict = tf.get_variable(\"w_predict\", [hidden_size, n_class])\nb_predict = tf.get_variable(\"b_predict\", [n_class])\n\nwith tf.variable_scope('lstm') as lstm_scope:\n  cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=False)\n  state = cell.zero_state(batch_size, dtype=tf.float32)\n  state1 = cell.zero_state(batch_size, dtype=tf.float32)\n  for t in range(num_timesteps):\n    if t == 0:\n      output, state = cell(x[:, t, :], state)\n      lstm_scope.reuse_variables()\n      output1, state1 = cell(x[:, t, :], state1)\n    else:\n      lstm_scope.reuse_variables()\n      # 1. normal lstm \n      output, state = cell(x[:, t, :], state)\n      # 2. lstm using tf.gather and tf.dynamic_stitch to select all samples from batch\n      idx_select = tf.range(batch_size)\n      tmp_output, tmp_state = cell(tf.gather(x[:, t, :], idx_select), tf.gather(state1, idx_select))\n      output1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [output1, tmp_output])\n      state1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [state1, tmp_state])\nlogits = tf.nn.xw_plus_b(output, w_predict, b_predict)\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets, name=None)\nlogits1 = tf.nn.xw_plus_b(output1, w_predict, b_predict)\ncross_entropy1 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, targets, name=None)\ntrainable = tf.trainable_variables()\ngrad = tf.gradients(cross_entropy, trainable)\ngrad1 = tf.gradients(cross_entropy1, trainable)\n\n######## gradient log\ncg = []\nfor g in grad:\n  if g is not None:\n    cg += [g]\ncg1 = []\nfor g in grad1:\n  if g is not None:\n    cg1 += [g]\n\noptimizer = tf.train.AdamOptimizer(learning_rate)\ntrain_op = optimizer.apply_gradients(zip(grad, trainable))\ninit_op = tf.initialize_all_variables()\n\n\n\"\"\" Training \"\"\"\n## arbitrary synthetic data\n# we use two training examples, each with length 10\nmy_input = np.array([[3,4,6,8,3,5,8,9,2,4], [4,2,3,8,5,2,2,3,6,1]])\nmy_target = np.array([0,1])\nsess = tf.Session()\nsess.run(init_op)\n\nepoch = 0\nwhile epoch < 5:\n  epoch += 1\n  fetches = [train_op, cg, cg1]\n  outputs = sess.run(fetches, feed_dict={inputs: my_input, targets: my_target})\n  gradients = outputs[1] \n  gradients1 = outputs[2] \n  print 'epoch %d:' % epoch \n    for i, g in enumerate(gradients):\n      if i > 0:\n\tprint('norm of gradient of var %d: %f' % (i, LA.norm(gradients[i])))\n\tprint('norm of gradient1 of var %d: %f' % (i, LA.norm(gradients1[i])))\nThe sample output is:\nepoch 1:\nnorm of gradient of var 1: 0.644620\nnorm of gradient1 of var 1: 0.644620\nnorm of gradient of var 2: 0.393020\nnorm of gradient1 of var 2: 0.393020\nnorm of gradient of var 3: 0.838759\nnorm of gradient1 of var 3: 102.815338\nnorm of gradient of var 4: 0.435841\nnorm of gradient1 of var 4: 44.867126\nepoch 2:\nnorm of gradient of var 1: 0.613848\nnorm of gradient1 of var 1: 0.611423\nnorm of gradient of var 2: 0.355387\nnorm of gradient1 of var 2: 0.351987\nnorm of gradient of var 3: 0.797761\nnorm of gradient1 of var 3: 96.391121\nnorm of gradient of var 4: 0.397020\nnorm of gradient1 of var 4: 39.937107\nepoch 3:\nnorm of gradient of var 1: 0.603118\nnorm of gradient1 of var 1: 0.603118\nnorm of gradient of var 2: 0.318260\nnorm of gradient1 of var 2: 0.318260\nnorm of gradient of var 3: 0.773661\nnorm of gradient1 of var 3: 93.290131\nnorm of gradient of var 4: 0.366684\nnorm of gradient1 of var 4: 36.636879\nepoch 4:\nnorm of gradient of var 1: 0.607643\nnorm of gradient1 of var 1: 0.607643\nnorm of gradient of var 2: 0.280101\nnorm of gradient1 of var 2: 0.280101\nnorm of gradient of var 3: 0.763007\nnorm of gradient1 of var 3: 92.295441\nnorm of gradient of var 4: 0.340769\nnorm of gradient1 of var 4: 33.630474\nepoch 5:\nnorm of gradient of var 1: 0.622874\nnorm of gradient1 of var 1: 0.619443\nnorm of gradient of var 2: 0.239731\nnorm of gradient1 of var 2: 0.235509\nnorm of gradient of var 3: 0.757205\nnorm of gradient1 of var 3: 93.335388\nnorm of gradient of var 4: 0.312203\nnorm of gradient1 of var 4: 30.536301\n\nWe can see that the gradient and gradient1 of var3 have significantly different norms in every epoch, which should be the same. So is var4. Those two are the trainable variables of LSTM. In fact, if the sequence length is 50 instead of 10, the discrepancy is even much larger.\nCould anybody tell me why it is the case?\nEnvironment info\nOperating System: ubuntu 14.04", "body": "In Tensorflow 0.12, I find the discrepancy of gradients in two mathematically equivalent training procedures of LSTM, probably due to the use of tf.gather and tf.dynamic_stitch.  One is the normal procedure using the whole batch of training examples to unroll the LSTM in each step. The other first uses tf.gather to select ALL the examples of the whole batch in each step, then unroll the LSTM with those examples and finally use tf.dynamic_stitch to update the corresponding states and outputs.\r\n\r\nThese two procedures should be equivalent as they both essentially use the whole batch. However, the gradients of the same variables are significantly different.\r\n\r\nThe code is as follows (the core parts are essentially `# 1.` and  `# 2.`):\r\n\r\n```python\r\nbatch_size = 2\r\nnum_timesteps = 10\r\nvocab_size = 10\r\nnum_embedding_nodes = 32\r\nhidden_size = 128\r\nn_class = 2\r\nlearning_rate = 0.001\r\ninputs = tf.placeholder(tf.int64, [batch_size, num_timesteps])\r\ntargets = tf.placeholder(tf.int64, [batch_size])\r\nembedding = tf.get_variable(\"embedding\", [vocab_size, num_embedding_nodes])\r\nx = tf.nn.embedding_lookup(embedding, inputs)\r\nw_predict = tf.get_variable(\"w_predict\", [hidden_size, n_class])\r\nb_predict = tf.get_variable(\"b_predict\", [n_class])\r\n\r\nwith tf.variable_scope('lstm') as lstm_scope:\r\n  cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=False)\r\n  state = cell.zero_state(batch_size, dtype=tf.float32)\r\n  state1 = cell.zero_state(batch_size, dtype=tf.float32)\r\n  for t in range(num_timesteps):\r\n    if t == 0:\r\n      output, state = cell(x[:, t, :], state)\r\n      lstm_scope.reuse_variables()\r\n      output1, state1 = cell(x[:, t, :], state1)\r\n    else:\r\n      lstm_scope.reuse_variables()\r\n      # 1. normal lstm \r\n      output, state = cell(x[:, t, :], state)\r\n      # 2. lstm using tf.gather and tf.dynamic_stitch to select all samples from batch\r\n      idx_select = tf.range(batch_size)\r\n      tmp_output, tmp_state = cell(tf.gather(x[:, t, :], idx_select), tf.gather(state1, idx_select))\r\n      output1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [output1, tmp_output])\r\n      state1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [state1, tmp_state])\r\nlogits = tf.nn.xw_plus_b(output, w_predict, b_predict)\r\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets, name=None)\r\nlogits1 = tf.nn.xw_plus_b(output1, w_predict, b_predict)\r\ncross_entropy1 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, targets, name=None)\r\ntrainable = tf.trainable_variables()\r\ngrad = tf.gradients(cross_entropy, trainable)\r\ngrad1 = tf.gradients(cross_entropy1, trainable)\r\n\r\n######## gradient log\r\ncg = []\r\nfor g in grad:\r\n  if g is not None:\r\n    cg += [g]\r\ncg1 = []\r\nfor g in grad1:\r\n  if g is not None:\r\n    cg1 += [g]\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate)\r\ntrain_op = optimizer.apply_gradients(zip(grad, trainable))\r\ninit_op = tf.initialize_all_variables()\r\n\r\n\r\n\"\"\" Training \"\"\"\r\n## arbitrary synthetic data\r\n# we use two training examples, each with length 10\r\nmy_input = np.array([[3,4,6,8,3,5,8,9,2,4], [4,2,3,8,5,2,2,3,6,1]])\r\nmy_target = np.array([0,1])\r\nsess = tf.Session()\r\nsess.run(init_op)\r\n\r\nepoch = 0\r\nwhile epoch < 5:\r\n  epoch += 1\r\n  fetches = [train_op, cg, cg1]\r\n  outputs = sess.run(fetches, feed_dict={inputs: my_input, targets: my_target})\r\n  gradients = outputs[1] \r\n  gradients1 = outputs[2] \r\n  print 'epoch %d:' % epoch \r\n    for i, g in enumerate(gradients):\r\n      if i > 0:\r\n\tprint('norm of gradient of var %d: %f' % (i, LA.norm(gradients[i])))\r\n\tprint('norm of gradient1 of var %d: %f' % (i, LA.norm(gradients1[i])))\r\n```\r\n\r\nThe sample output is:\r\n```\r\nepoch 1:\r\nnorm of gradient of var 1: 0.644620\r\nnorm of gradient1 of var 1: 0.644620\r\nnorm of gradient of var 2: 0.393020\r\nnorm of gradient1 of var 2: 0.393020\r\nnorm of gradient of var 3: 0.838759\r\nnorm of gradient1 of var 3: 102.815338\r\nnorm of gradient of var 4: 0.435841\r\nnorm of gradient1 of var 4: 44.867126\r\nepoch 2:\r\nnorm of gradient of var 1: 0.613848\r\nnorm of gradient1 of var 1: 0.611423\r\nnorm of gradient of var 2: 0.355387\r\nnorm of gradient1 of var 2: 0.351987\r\nnorm of gradient of var 3: 0.797761\r\nnorm of gradient1 of var 3: 96.391121\r\nnorm of gradient of var 4: 0.397020\r\nnorm of gradient1 of var 4: 39.937107\r\nepoch 3:\r\nnorm of gradient of var 1: 0.603118\r\nnorm of gradient1 of var 1: 0.603118\r\nnorm of gradient of var 2: 0.318260\r\nnorm of gradient1 of var 2: 0.318260\r\nnorm of gradient of var 3: 0.773661\r\nnorm of gradient1 of var 3: 93.290131\r\nnorm of gradient of var 4: 0.366684\r\nnorm of gradient1 of var 4: 36.636879\r\nepoch 4:\r\nnorm of gradient of var 1: 0.607643\r\nnorm of gradient1 of var 1: 0.607643\r\nnorm of gradient of var 2: 0.280101\r\nnorm of gradient1 of var 2: 0.280101\r\nnorm of gradient of var 3: 0.763007\r\nnorm of gradient1 of var 3: 92.295441\r\nnorm of gradient of var 4: 0.340769\r\nnorm of gradient1 of var 4: 33.630474\r\nepoch 5:\r\nnorm of gradient of var 1: 0.622874\r\nnorm of gradient1 of var 1: 0.619443\r\nnorm of gradient of var 2: 0.239731\r\nnorm of gradient1 of var 2: 0.235509\r\nnorm of gradient of var 3: 0.757205\r\nnorm of gradient1 of var 3: 93.335388\r\nnorm of gradient of var 4: 0.312203\r\nnorm of gradient1 of var 4: 30.536301\r\n```\r\n\r\nWe can see that the gradient and gradient1 of var3 have significantly different norms in every epoch, which should be the same. So is var4. Those two are the trainable variables of LSTM. In fact, if the sequence length is 50 instead of 10, the discrepancy is even much larger.\r\n\r\nCould anybody tell me why it is the case?\r\n\r\n### Environment info\r\nOperating System: ubuntu 14.04\r\n"}