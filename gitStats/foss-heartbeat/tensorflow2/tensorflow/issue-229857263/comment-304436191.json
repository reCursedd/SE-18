{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/304436191", "html_url": "https://github.com/tensorflow/tensorflow/issues/10021#issuecomment-304436191", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10021", "id": 304436191, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDQzNjE5MQ==", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-27T07:56:17Z", "updated_at": "2017-05-27T07:56:17Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=560016\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/foxik\">@foxik</a> [batch_size, class_size] is the size of both the logits and the onehot_labels we already passing to the softmax. I believe you should be able to pass weights with the same size. and if they happen to be just [bastch_size] then they should be broadcasted to each class, or if they are only [class_size] they should be broadcasted to each example in the batch, and finally if you pass a scalar it should be broadcasted in both batch_size and class_size. Hope this is more clear now.</p>\n<p>Adding an additional class weights parameter would be ok, but if you multiply the class weights and the batch weight parameter you get the [bach_size, class_size] weight matrix I'm talking about. Either way would be fine IMHO.</p>\n<p>I believe that an optional parameter, if useful, does no big harm and doesn't make the API less simple, it may actually make the model's code slightly simpler. Anyway, I understand that the effort to implement this may not be worth both the rather small advantage that it would give, but I'm proposing a feature I'd like to have, it's not up to me to decide how important or urgent it is :) I respect your point of view too anyway.</p>", "body_text": "@foxik [batch_size, class_size] is the size of both the logits and the onehot_labels we already passing to the softmax. I believe you should be able to pass weights with the same size. and if they happen to be just [bastch_size] then they should be broadcasted to each class, or if they are only [class_size] they should be broadcasted to each example in the batch, and finally if you pass a scalar it should be broadcasted in both batch_size and class_size. Hope this is more clear now.\nAdding an additional class weights parameter would be ok, but if you multiply the class weights and the batch weight parameter you get the [bach_size, class_size] weight matrix I'm talking about. Either way would be fine IMHO.\nI believe that an optional parameter, if useful, does no big harm and doesn't make the API less simple, it may actually make the model's code slightly simpler. Anyway, I understand that the effort to implement this may not be worth both the rather small advantage that it would give, but I'm proposing a feature I'd like to have, it's not up to me to decide how important or urgent it is :) I respect your point of view too anyway.", "body": "@foxik [batch_size, class_size] is the size of both the logits and the onehot_labels we already passing to the softmax. I believe you should be able to pass weights with the same size. and if they happen to be just [bastch_size] then they should be broadcasted to each class, or if they are only [class_size] they should be broadcasted to each example in the batch, and finally if you pass a scalar it should be broadcasted in both batch_size and class_size. Hope this is more clear now.\r\n\r\nAdding an additional class weights parameter would be ok, but if you multiply the class weights and the batch weight parameter you get the [bach_size, class_size] weight matrix I'm talking about. Either way would be fine IMHO.\r\n\r\nI believe that an optional parameter, if useful, does no big harm and doesn't make the API less simple, it may actually make the model's code slightly simpler. Anyway, I understand that the effort to implement this may not be worth both the rather small advantage that it would give, but I'm proposing a feature I'd like to have, it's not up to me to decide how important or urgent it is :) I respect your point of view too anyway."}