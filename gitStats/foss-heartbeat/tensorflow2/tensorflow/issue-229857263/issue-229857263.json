{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10021", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10021/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10021/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10021/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10021", "id": 229857263, "node_id": "MDU6SXNzdWUyMjk4NTcyNjM=", "number": 10021, "title": "Class weighting in tf.losses.softmax_cross_entropy", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-05-19T02:55:16Z", "updated_at": "2018-11-12T02:48:05Z", "closed_at": "2018-11-09T23:10:25Z", "author_association": "NONE", "body_html": "<p>Feature request</p>\n<p>In <code>tf.losses.softmax_cross_entropy</code> there's an optional field <code>weights</code>. I assumed this field was used for assigning a different weight to each class, but it actually is used to assign a weight to each sample in the batch. In my use case i have a <code>batch_size</code> of <code>128</code> and <code>30</code> classes, so I was passing a <code>[1, 30]</code> tensor and got this error:</p>\n<pre><code>InvalidArgumentError (see above for traceback): Incompatible shapes: [128] vs. [30]\n\t [[Node: optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape, optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape_1)]]\n</code></pre>\n<p>I looked at the implementation, confirmed that the function expects <code>batch_size</code> as the dimension of thensor and realized that my expected behavior cannot be achieved easily as <code>tf.nn.softmax_cross_entropy_with_logits</code> doesn't have a weight parameter.</p>\n<p>My current workaround solution is to re-implement this function calculating the loss for each class and then multiplying for the weight vector that I pass, but that's inefficient compared to the optimized implementation of <code>tf.nn.softmax_cross_entropy_with_logits</code>.</p>\n<p>So my request is:</p>\n<ul>\n<li>provide an optimized <code>tf.nn.softmax_cross_entropy_with_logits</code> that also accepts <code>weights</code> for each class as a parameter</li>\n<li>use it inside <code>tf.losses.softmax_cross_entropy</code> so that one can pass weights as a scalar, a <code>[batch_size, 1]</code> tensor, a <code>[1, num_classes]</code> tensor or a <code>[batch_size, num_classes]</code> tensor (the same dimension of  <code>onehot_labels</code>)</li>\n</ul>", "body_text": "Feature request\nIn tf.losses.softmax_cross_entropy there's an optional field weights. I assumed this field was used for assigning a different weight to each class, but it actually is used to assign a weight to each sample in the batch. In my use case i have a batch_size of 128 and 30 classes, so I was passing a [1, 30] tensor and got this error:\nInvalidArgumentError (see above for traceback): Incompatible shapes: [128] vs. [30]\n\t [[Node: optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape, optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape_1)]]\n\nI looked at the implementation, confirmed that the function expects batch_size as the dimension of thensor and realized that my expected behavior cannot be achieved easily as tf.nn.softmax_cross_entropy_with_logits doesn't have a weight parameter.\nMy current workaround solution is to re-implement this function calculating the loss for each class and then multiplying for the weight vector that I pass, but that's inefficient compared to the optimized implementation of tf.nn.softmax_cross_entropy_with_logits.\nSo my request is:\n\nprovide an optimized tf.nn.softmax_cross_entropy_with_logits that also accepts weights for each class as a parameter\nuse it inside tf.losses.softmax_cross_entropy so that one can pass weights as a scalar, a [batch_size, 1] tensor, a [1, num_classes] tensor or a [batch_size, num_classes] tensor (the same dimension of  onehot_labels)", "body": "Feature request\r\n\r\nIn `tf.losses.softmax_cross_entropy` there's an optional field `weights`. I assumed this field was used for assigning a different weight to each class, but it actually is used to assign a weight to each sample in the batch. In my use case i have a `batch_size` of `128` and `30` classes, so I was passing a `[1, 30]` tensor and got this error:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [128] vs. [30]\r\n\t [[Node: optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape, optimizer/gradients/cross_entropy/softmax_cross_entropy_loss/Mul_grad/Shape_1)]]\r\n```\r\n\r\nI looked at the implementation, confirmed that the function expects `batch_size` as the dimension of thensor and realized that my expected behavior cannot be achieved easily as `tf.nn.softmax_cross_entropy_with_logits` doesn't have a weight parameter.\r\n\r\nMy current workaround solution is to re-implement this function calculating the loss for each class and then multiplying for the weight vector that I pass, but that's inefficient compared to the optimized implementation of `tf.nn.softmax_cross_entropy_with_logits`.\r\n\r\nSo my request is:\r\n- provide an optimized `tf.nn.softmax_cross_entropy_with_logits` that also accepts `weights` for each class as a parameter\r\n- use it inside `tf.losses.softmax_cross_entropy` so that one can pass weights as a scalar, a `[batch_size, 1]` tensor, a `[1, num_classes]` tensor or a `[batch_size, num_classes]` tensor (the same dimension of  `onehot_labels`)"}