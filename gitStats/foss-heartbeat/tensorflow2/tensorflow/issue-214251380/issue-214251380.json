{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8414", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8414/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8414/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8414/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8414", "id": 214251380, "node_id": "MDU6SXNzdWUyMTQyNTEzODA=", "number": 8414, "title": "softmax_cross_entropy_with_logits() behaves differently with it's comment", "user": {"login": "metorm", "id": 12082680, "node_id": "MDQ6VXNlcjEyMDgyNjgw", "avatar_url": "https://avatars0.githubusercontent.com/u/12082680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/metorm", "html_url": "https://github.com/metorm", "followers_url": "https://api.github.com/users/metorm/followers", "following_url": "https://api.github.com/users/metorm/following{/other_user}", "gists_url": "https://api.github.com/users/metorm/gists{/gist_id}", "starred_url": "https://api.github.com/users/metorm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/metorm/subscriptions", "organizations_url": "https://api.github.com/users/metorm/orgs", "repos_url": "https://api.github.com/users/metorm/repos", "events_url": "https://api.github.com/users/metorm/events{/privacy}", "received_events_url": "https://api.github.com/users/metorm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-15T00:56:08Z", "updated_at": "2017-03-22T22:10:25Z", "closed_at": "2017-03-22T22:10:25Z", "author_association": "NONE", "body_html": "<p>version: 1.0.1 CPU compiled by cmake from master branch.</p>\n<p>According to <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\" rel=\"nofollow\">the document</a>, <code>softmax_cross_entropy_with_logits()</code> accept inputs with any shape. However, the comments in the source code says:</p>\n<p><code>logits</code> and <code>labels</code> must have the same shape <code>[batch_size, num_classes]</code>  and the same dtype (either <code>float16</code>, <code>float32</code>, or <code>float64</code>).</p>\n<p>meaning that only rank-2 tensors are allowed.</p>\n<p>I tried the following code to construct a net which performs binary-classify on each pixel of an image :</p>\n<pre><code>    final_two_channel = tf.layers.conv2d(\n        inputs=res_out_iter,\n        filters=2,\n        kernel_size=[1, 1],\n        padding='same',\n        activation=tf.nn.sigmoid\n    )\n    # logits = tf.reshape(tensor=final_two_channel, shape=[-1, 2], name='flatten_net_out')\n    label_one_hot = tf.one_hot(\n        indices=output_field,\n        depth=2,\n        on_value=1, off_value=0,\n        name='label_one_hot'\n    )\n    crs_etp_loss = tf.nn.softmax_cross_entropy_with_logits(labels=label_one_hot, logits=final_two_channel)\n    print(crs_etp_loss.shape)\n    crs_etp_loss = tf.reshape(tensor=crs_etp_loss, shape=[-1, field_width * field_width])\n    crs_etp_loss = tf.reduce_mean(input_tensor=crs_etp_loss, axis=1)\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss=crs_etp_loss)\n</code></pre>\n<p>The program runs without error and <code>print(crs_etp_loss.shape)</code> gives <code>(?, 512, 128)</code>, but I am not sure if it's safe/correct to do like this.</p>\n<p>I posted a question on <a href=\"http://stackoverflow.com/questions/42799167/is-it-safe-to-use-softmax-cross-entropy-with-logits-when-each-instance-has-mul\" rel=\"nofollow\">stackoverflow</a> about this.</p>", "body_text": "version: 1.0.1 CPU compiled by cmake from master branch.\nAccording to the document, softmax_cross_entropy_with_logits() accept inputs with any shape. However, the comments in the source code says:\nlogits and labels must have the same shape [batch_size, num_classes]  and the same dtype (either float16, float32, or float64).\nmeaning that only rank-2 tensors are allowed.\nI tried the following code to construct a net which performs binary-classify on each pixel of an image :\n    final_two_channel = tf.layers.conv2d(\n        inputs=res_out_iter,\n        filters=2,\n        kernel_size=[1, 1],\n        padding='same',\n        activation=tf.nn.sigmoid\n    )\n    # logits = tf.reshape(tensor=final_two_channel, shape=[-1, 2], name='flatten_net_out')\n    label_one_hot = tf.one_hot(\n        indices=output_field,\n        depth=2,\n        on_value=1, off_value=0,\n        name='label_one_hot'\n    )\n    crs_etp_loss = tf.nn.softmax_cross_entropy_with_logits(labels=label_one_hot, logits=final_two_channel)\n    print(crs_etp_loss.shape)\n    crs_etp_loss = tf.reshape(tensor=crs_etp_loss, shape=[-1, field_width * field_width])\n    crs_etp_loss = tf.reduce_mean(input_tensor=crs_etp_loss, axis=1)\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss=crs_etp_loss)\n\nThe program runs without error and print(crs_etp_loss.shape) gives (?, 512, 128), but I am not sure if it's safe/correct to do like this.\nI posted a question on stackoverflow about this.", "body": "version: 1.0.1 CPU compiled by cmake from master branch.\r\n\r\nAccording to [the document](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits), `softmax_cross_entropy_with_logits()` accept inputs with any shape. However, the comments in the source code says:\r\n\r\n`logits` and `labels` must have the same shape `[batch_size, num_classes]`  and the same dtype (either `float16`, `float32`, or `float64`).\r\n\r\nmeaning that only rank-2 tensors are allowed.\r\n\r\nI tried the following code to construct a net which performs binary-classify on each pixel of an image :\r\n\r\n```\r\n    final_two_channel = tf.layers.conv2d(\r\n        inputs=res_out_iter,\r\n        filters=2,\r\n        kernel_size=[1, 1],\r\n        padding='same',\r\n        activation=tf.nn.sigmoid\r\n    )\r\n    # logits = tf.reshape(tensor=final_two_channel, shape=[-1, 2], name='flatten_net_out')\r\n    label_one_hot = tf.one_hot(\r\n        indices=output_field,\r\n        depth=2,\r\n        on_value=1, off_value=0,\r\n        name='label_one_hot'\r\n    )\r\n    crs_etp_loss = tf.nn.softmax_cross_entropy_with_logits(labels=label_one_hot, logits=final_two_channel)\r\n    print(crs_etp_loss.shape)\r\n    crs_etp_loss = tf.reshape(tensor=crs_etp_loss, shape=[-1, field_width * field_width])\r\n    crs_etp_loss = tf.reduce_mean(input_tensor=crs_etp_loss, axis=1)\r\n\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss=crs_etp_loss)\r\n```\r\n\r\nThe program runs without error and `print(crs_etp_loss.shape)` gives `(?, 512, 128)`, but I am not sure if it's safe/correct to do like this.\r\n\r\nI posted a question on [stackoverflow](http://stackoverflow.com/questions/42799167/is-it-safe-to-use-softmax-cross-entropy-with-logits-when-each-instance-has-mul) about this."}