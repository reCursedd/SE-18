{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/196629019", "pull_request_review_id": 130217817, "id": 196629019, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjYyOTAxOQ==", "diff_hunk": "@@ -0,0 +1,179 @@\n+# TFLMS: Graph Editing Library for Large Model Support (LMS) in TensorFlow\n+\n+This library provides an approach to training large models that cannot be fit into GPU memory.\n+It takes a computational graph defined by users, and automatically adds swap-in and swap-out nodes for transferring tensors from GPUs to the host and vice versa.\n+The computational graph is statically modified. Hence, it needs to be done before a session actually starts.\n+\n+## How to use\n+TFLMS needs to know some information about user-defined models.\n+There is one requirement for a user-defined model: it must have scopes for the optimizers/solvers.\n+\n+Enabling LMS for a model depends on how users write their training. The\n+following guidelines cover three ways to train:\n+- [Session](https://www.tensorflow.org/programmers_guide/graphs)-based training\n+- [Estimator](https://www.tensorflow.org/programmers_guide/estimators)-based training\n+- [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)-based training\n+\n+### [Session](https://www.tensorflow.org/programmers_guide/graphs)-based training\n+#### Step 1: define optimizer/solver scopes\n+```python\n+with tf.name_scope('adam_optimizer'):\n+\ttrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n+```\n+#### Step 2: define an LMS object and run it\n+```python\n+from tensorflow.contrib.lms import LMS\n+lms_obj = LMS({'adam_optimizer'})\n+lms_obj.run(graph=tf.get_default_graph())\n+```\n+The above lines must be put before starting a training session, for example:\n+- Before inserting LMS code\n+```python\n+with tf.Session() as sess:\n+    sess.run(tf.global_variables_initializer())\n+\tbatch = mnist.train.next_batch(50)\n+\ttrain_step.run(feed_dict={x: batch[0], y_: batch[1]})\n+```\n+- After inserting LMS code\n+```python\n+from tensorflow.contrib.lms import LMS\n+lms_obj = LMS({'adam_optimizer'})\n+lms_obj.run(graph=tf.get_default_graph())\n+\n+with tf.Session() as sess:\n+    sess.run(tf.global_variables_initializer())\n+\tbatch = mnist.train.next_batch(50)\n+\ttrain_step.run(feed_dict={x: batch[0], y_: batch[1]})\n+```\n+For a working example of LMS integration with Session based training see:\n+`examples/mnist_deep_lms.py`\n+which is an LMS enabled version of `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py`.\n+\n+### [Estimator](https://www.tensorflow.org/programmers_guide/estimators)-based training\n+#### Step 1: define optimizer/solver scopes\n+```python\n+with tf.name_scope('adam_optimizer'):\n+      optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n+      train_op = optimizer.minimize(\n+        loss=loss,\n+\tglobal_step=tf.train.get_global_step())\n+```\n+#### Step 2: define an LMSHook (LMSHook and LMS share the same set of parameters)\n+```python\n+# Hook for Large Model Support\n+from tensorflow.contrib.lms import LMSHook\n+lms_hook = LMSHook({'adam_optimizer'})\n+```\n+#### Step 3: add the LMSHook into Estimator's hook list\n+```python\n+mnist_classifier.train(\n+      input_fn=train_input_fn,\n+      steps=20000\n+      hooks=[logging_hook, lms_hook])\n+```\n+\n+For a working example of LMS integration with Estimator based training see:\n+`examples/cnn_mnist_lms.py`\n+which is an LMS enabled version of `https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py`.\n+\n+### [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)-based training\n+#### Step 1: define an LMSKerasCallback.\n+```python\n+from tensorflow.contrib.lms import LMSKerasCallback\n+# LMSKerasCallback and LMS share a set of keyword arguments. Here we just\n+# use the default options.\n+lms_callback = LMSKerasCallback()\n+```\n+#### Step 2: pass the callback to the Keras `fit` or `fit_generator` function.\n+```python\n+model.fit_generator(generator=training_gen, callbacks=[lms_callback])\n+```\n+\n+### Parameters for LMS/LMSHook/LMSKerasCallback\n+#### Required parameters\n+_graph_ :: the graph we will modify for LMS. This should be the graph of user-defined neural network. (not required in LMSHook and LMSKerasCallback)\n+\n+_optimizer_scopes_ :: scopes for the optimizers/solvers.\n+\n+#### Optional parameters\n+_starting_scope_ :: Tensors that are reachable from the operations in this scope will be swapped for LMS. Set this to the scope of the first layer if we would like to modify the whole graph. Default `None`.\n+\n+_starting_op_names_ :: Tensors that are reachable from the operations with these names will be swapped for LMS. Default `None`.\n+\n+_excl_scopes_ :: a set of scopes for operations whose tensors will not be swapped out to the host. Default `empty`.\n+\n+_incl_scopes_ :: a set of scopes for operations whose tensors will be swapped out to the host. Default `empty`.\n+\n+_excl_types_ :: a set of types for operations whose tensors will not be swapped out to the host. Default `empty`.\n+\n+_incl_types_ :: a set of types for operations whose tensors will be swapped out to the host. Default `empty`.\n+\n+_n_tensors_ :: The number of tensors for LMS, counting from the `starting_scope`. To turn off LMS, set `n_tensors` to `0`. Default `-1` (all reachable tensors will be swapped for LMS).\n+\n+_lb_ :: Lowerbound value for LMS. A tensor will be swapped in during the backward phase at least `lb` nodes before it in the graph. Default `1`.\n+\n+_ub_ :: Upperbound value for LMS. Default `10000`.\n+\n+_fuse_swapins_ :: Fuse \"close\" swap-in operations into one operation. This may improve the performance. Default `False`.", "path": "tensorflow/contrib/lms/README.md", "position": 117, "original_position": 117, "commit_id": "1ce0b96857606948b2ab41e3a9a5333323a9c958", "original_commit_id": "3b39b5ecd69bebb1e9222c2292a57c5532931ade", "user": {"login": "tungld", "id": 1230113, "node_id": "MDQ6VXNlcjEyMzAxMTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1230113?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tungld", "html_url": "https://github.com/tungld", "followers_url": "https://api.github.com/users/tungld/followers", "following_url": "https://api.github.com/users/tungld/following{/other_user}", "gists_url": "https://api.github.com/users/tungld/gists{/gist_id}", "starred_url": "https://api.github.com/users/tungld/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tungld/subscriptions", "organizations_url": "https://api.github.com/users/tungld/orgs", "repos_url": "https://api.github.com/users/tungld/repos", "events_url": "https://api.github.com/users/tungld/events{/privacy}", "received_events_url": "https://api.github.com/users/tungld/received_events", "type": "User", "site_admin": false}, "body": "Performance may be improved but the maximum batch size we are able to train is decreased because tensors are kept in GPU for a longer time to be reused by operations.", "created_at": "2018-06-20T01:57:59Z", "updated_at": "2018-07-02T07:41:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19845#discussion_r196629019", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19845", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/196629019"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19845#discussion_r196629019"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19845"}}, "body_html": "<p>Performance may be improved but the maximum batch size we are able to train is decreased because tensors are kept in GPU for a longer time to be reused by operations.</p>", "body_text": "Performance may be improved but the maximum batch size we are able to train is decreased because tensors are kept in GPU for a longer time to be reused by operations.", "in_reply_to_id": 196296429}