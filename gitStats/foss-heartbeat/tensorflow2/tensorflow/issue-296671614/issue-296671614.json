{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16979", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16979/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16979/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16979/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16979", "id": 296671614, "node_id": "MDU6SXNzdWUyOTY2NzE2MTQ=", "number": 16979, "title": "[Critical some questions] Tensorflow-lite, Neural Networks API", "user": {"login": "nanamare", "id": 17498974, "node_id": "MDQ6VXNlcjE3NDk4OTc0", "avatar_url": "https://avatars3.githubusercontent.com/u/17498974?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nanamare", "html_url": "https://github.com/nanamare", "followers_url": "https://api.github.com/users/nanamare/followers", "following_url": "https://api.github.com/users/nanamare/following{/other_user}", "gists_url": "https://api.github.com/users/nanamare/gists{/gist_id}", "starred_url": "https://api.github.com/users/nanamare/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nanamare/subscriptions", "organizations_url": "https://api.github.com/users/nanamare/orgs", "repos_url": "https://api.github.com/users/nanamare/repos", "events_url": "https://api.github.com/users/nanamare/events{/privacy}", "received_events_url": "https://api.github.com/users/nanamare/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-02-13T09:43:05Z", "updated_at": "2018-04-25T17:33:28Z", "closed_at": "2018-04-25T17:33:28Z", "author_association": "NONE", "body_html": "<p>Hi.</p>\n<p><strong>Some question.</strong></p>\n<p><strong>1.</strong></p>\n<blockquote>\n<p>Current i use  tensorflow library <code>  compile 'org.tensorflow:tensorflow-lite:+'</code><br>\nand i want to use Neural Networks Api, to reduce inference time.</p>\n<p>but wrapper function is not existing in <code>Interpreter.java</code> class<br>\n<code>  private static native void useNNAPI(long var0, boolean var2);</code><br>\nso i can't use it..</p>\n<p>If I create a wrapper function, can I use Neural Networks API? (my device level &gt;  8.1)</p>\n</blockquote>\n<p>What is <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/testhelper/java/org/tensorflow/lite/TestHelper.java\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/testhelper/java/org/tensorflow/lite/TestHelper.java</a><br>\n?</p>\n<p><strong>2.</strong></p>\n<blockquote>\n<p>i'm converting pb to tflite. mobilenet_v1_224.pb (17 mb) mobilenet_v1_224_uint8.tflite (4 mb)<br>\ninference speed about 290 ms -&gt; 73ms decreased</p>\n</blockquote>\n<blockquote>\n<p>Command line(mobileNet)</p>\n</blockquote>\n<pre><code>\nbazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_graph.pb \\\n  --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint_graph.tflite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=QUANTIZED_UINT8 \\\n  --input_shape=1,224,224,3 \\\n  --input_array=input \\\n  --output_array=MobilenetV1/Predictions/Reshape_1 \\\n  --default_ranges_min=0 \\\n  --default_ranges_max=6 \\\n  --mean_value=127.5 \\\n  --std_value=127.5\n</code></pre>\n<blockquote>\n<p>In a similar way,<br>\nBut frozen_cpm.pb(120 mb) convert to frozen_cpm_uint8.tflite(30 mb)<br>\ninference speed about 11000 ms -&gt; 11000ms.<br>\n<strong>If the model size is reduced, does not the inference time generally decrease?</strong></p>\n</blockquote>", "body_text": "Hi.\nSome question.\n1.\n\nCurrent i use  tensorflow library   compile 'org.tensorflow:tensorflow-lite:+'\nand i want to use Neural Networks Api, to reduce inference time.\nbut wrapper function is not existing in Interpreter.java class\n  private static native void useNNAPI(long var0, boolean var2);\nso i can't use it..\nIf I create a wrapper function, can I use Neural Networks API? (my device level >  8.1)\n\nWhat is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/testhelper/java/org/tensorflow/lite/TestHelper.java\n?\n2.\n\ni'm converting pb to tflite. mobilenet_v1_224.pb (17 mb) mobilenet_v1_224_uint8.tflite (4 mb)\ninference speed about 290 ms -> 73ms decreased\n\n\nCommand line(mobileNet)\n\n\nbazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_graph.pb \\\n  --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint_graph.tflite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=QUANTIZED_UINT8 \\\n  --input_shape=1,224,224,3 \\\n  --input_array=input \\\n  --output_array=MobilenetV1/Predictions/Reshape_1 \\\n  --default_ranges_min=0 \\\n  --default_ranges_max=6 \\\n  --mean_value=127.5 \\\n  --std_value=127.5\n\n\nIn a similar way,\nBut frozen_cpm.pb(120 mb) convert to frozen_cpm_uint8.tflite(30 mb)\ninference speed about 11000 ms -> 11000ms.\nIf the model size is reduced, does not the inference time generally decrease?", "body": "Hi.\r\n\r\n**Some question.**\r\n\r\n **1.**\r\n\r\n> Current i use  tensorflow library `  compile 'org.tensorflow:tensorflow-lite:+'`\r\n> and i want to use Neural Networks Api, to reduce inference time.\r\n> \r\n> but wrapper function is not existing in `Interpreter.java` class \r\n> `  private static native void useNNAPI(long var0, boolean var2);`\r\n> so i can't use it..\r\n> \r\n> If I create a wrapper function, can I use Neural Networks API? (my device level >  8.1)\r\n\r\nWhat is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/testhelper/java/org/tensorflow/lite/TestHelper.java\r\n?\r\n\r\n**2.**\r\n\r\n> i'm converting pb to tflite. mobilenet_v1_224.pb (17 mb) mobilenet_v1_224_uint8.tflite (4 mb) \r\n> inference speed about 290 ms -> 73ms decreased\r\n\r\n> Command line(mobileNet)\r\n\r\n<pre><code>\r\nbazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_graph.pb \\\r\n  --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint_graph.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=1,224,224,3 \\\r\n  --input_array=input \\\r\n  --output_array=MobilenetV1/Predictions/Reshape_1 \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6 \\\r\n  --mean_value=127.5 \\\r\n  --std_value=127.5\r\n</pre></code>\r\n\r\n> In a similar way,\r\n> But frozen_cpm.pb(120 mb) convert to frozen_cpm_uint8.tflite(30 mb)\r\n> inference speed about 11000 ms -> 11000ms. \r\n>  **If the model size is reduced, does not the inference time generally decrease?**\r\n\r\n"}