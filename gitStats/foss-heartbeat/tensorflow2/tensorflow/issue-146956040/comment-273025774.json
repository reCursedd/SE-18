{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273025774", "html_url": "https://github.com/tensorflow/tensorflow/issues/1823#issuecomment-273025774", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1823", "id": 273025774, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzAyNTc3NA==", "user": {"login": "ugtony", "id": 6657593, "node_id": "MDQ6VXNlcjY2NTc1OTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6657593?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ugtony", "html_url": "https://github.com/ugtony", "followers_url": "https://api.github.com/users/ugtony/followers", "following_url": "https://api.github.com/users/ugtony/following{/other_user}", "gists_url": "https://api.github.com/users/ugtony/gists{/gist_id}", "starred_url": "https://api.github.com/users/ugtony/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ugtony/subscriptions", "organizations_url": "https://api.github.com/users/ugtony/orgs", "repos_url": "https://api.github.com/users/ugtony/repos", "events_url": "https://api.github.com/users/ugtony/events{/privacy}", "received_events_url": "https://api.github.com/users/ugtony/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-17T05:25:54Z", "updated_at": "2017-01-18T07:18:06Z", "author_association": "NONE", "body_html": "<p>Thanks for <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7245472\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/danielgordon10\">@danielgordon10</a> 's Caffe-style restore implementation. It is very useful when loading a pretrained model with network change.</p>\n<p>I tried the function in my task, but it doesn't work correctly when running this code line</p>\n<p><code>curr_var = tf.get_variable(saved_var_name)</code></p>\n<p>I guess it's because the variable_scope of my networks are nested, and makes the variable names prepended with scope names (the variable names are therefore something like scope1_scope2_scope3_op:0).<br>\nIn this case using an empty variable_scope might be inappropriate. (Not sure whether this is the reason)</p>\n<p><code>with tf.variable_scope('', reuse=True):</code></p>\n<p>Anyway, I modified the function to get the variables by their full names without the empty variable_scope:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">optimistic_restore</span>(<span class=\"pl-smi\">session</span>, <span class=\"pl-smi\">save_file</span>, <span class=\"pl-smi\">graph</span><span class=\"pl-k\">=</span>tf.get_default_graph()):\n    reader <span class=\"pl-k\">=</span> tf.train.NewCheckpointReader(save_file)\n    saved_shapes <span class=\"pl-k\">=</span> reader.get_variable_to_shape_map()\n    var_names <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sorted</span>([(var.name, var.name.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>:<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>]) <span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> tf.global_variables()\n            <span class=\"pl-k\">if</span> var.name.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>:<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">in</span> saved_shapes])    \n    restore_vars <span class=\"pl-k\">=</span> []    \n    <span class=\"pl-k\">for</span> var_name, saved_var_name <span class=\"pl-k\">in</span> var_names:            \n        curr_var <span class=\"pl-k\">=</span> graph.get_tensor_by_name(var_name)\n        var_shape <span class=\"pl-k\">=</span> curr_var.get_shape().as_list()\n        <span class=\"pl-k\">if</span> var_shape <span class=\"pl-k\">==</span> saved_shapes[saved_var_name]:\n            restore_vars.append(curr_var)\n    opt_saver <span class=\"pl-k\">=</span> tf.train.Saver(restore_vars)\n    opt_saver.restore(session, save_file)</pre></div>\n<p>The new function works well in my case. But I'm not quite confident of the implementation.</p>", "body_text": "Thanks for @danielgordon10 's Caffe-style restore implementation. It is very useful when loading a pretrained model with network change.\nI tried the function in my task, but it doesn't work correctly when running this code line\ncurr_var = tf.get_variable(saved_var_name)\nI guess it's because the variable_scope of my networks are nested, and makes the variable names prepended with scope names (the variable names are therefore something like scope1_scope2_scope3_op:0).\nIn this case using an empty variable_scope might be inappropriate. (Not sure whether this is the reason)\nwith tf.variable_scope('', reuse=True):\nAnyway, I modified the function to get the variables by their full names without the empty variable_scope:\ndef optimistic_restore(session, save_file, graph=tf.get_default_graph()):\n    reader = tf.train.NewCheckpointReader(save_file)\n    saved_shapes = reader.get_variable_to_shape_map()\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\n            if var.name.split(':')[0] in saved_shapes])    \n    restore_vars = []    \n    for var_name, saved_var_name in var_names:            \n        curr_var = graph.get_tensor_by_name(var_name)\n        var_shape = curr_var.get_shape().as_list()\n        if var_shape == saved_shapes[saved_var_name]:\n            restore_vars.append(curr_var)\n    opt_saver = tf.train.Saver(restore_vars)\n    opt_saver.restore(session, save_file)\nThe new function works well in my case. But I'm not quite confident of the implementation.", "body": "Thanks for @danielgordon10 's Caffe-style restore implementation. It is very useful when loading a pretrained model with network change. \r\n\r\nI tried the function in my task, but it doesn't work correctly when running this code line\r\n\r\n   ```curr_var = tf.get_variable(saved_var_name)```\r\n\r\nI guess it's because the variable_scope of my networks are nested, and makes the variable names prepended with scope names (the variable names are therefore something like scope1_scope2_scope3_op:0). \r\nIn this case using an empty variable_scope might be inappropriate. (Not sure whether this is the reason)\r\n\r\n```with tf.variable_scope('', reuse=True):```\r\n\r\nAnyway, I modified the function to get the variables by their full names without the empty variable_scope:\r\n```python\r\ndef optimistic_restore(session, save_file, graph=tf.get_default_graph()):\r\n    reader = tf.train.NewCheckpointReader(save_file)\r\n    saved_shapes = reader.get_variable_to_shape_map()\r\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n            if var.name.split(':')[0] in saved_shapes])    \r\n    restore_vars = []    \r\n    for var_name, saved_var_name in var_names:            \r\n        curr_var = graph.get_tensor_by_name(var_name)\r\n        var_shape = curr_var.get_shape().as_list()\r\n        if var_shape == saved_shapes[saved_var_name]:\r\n            restore_vars.append(curr_var)\r\n    opt_saver = tf.train.Saver(restore_vars)\r\n    opt_saver.restore(session, save_file)\r\n```\r\nThe new function works well in my case. But I'm not quite confident of the implementation. "}