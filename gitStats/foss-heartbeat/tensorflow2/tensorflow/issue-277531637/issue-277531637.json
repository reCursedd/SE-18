{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14954", "id": 277531637, "node_id": "MDU6SXNzdWUyNzc1MzE2Mzc=", "number": 14954, "title": "enable get the 'mathematical' gradient of output w.r.t neural network parameters", "user": {"login": "Erichliu00", "id": 23612416, "node_id": "MDQ6VXNlcjIzNjEyNDE2", "avatar_url": "https://avatars1.githubusercontent.com/u/23612416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erichliu00", "html_url": "https://github.com/Erichliu00", "followers_url": "https://api.github.com/users/Erichliu00/followers", "following_url": "https://api.github.com/users/Erichliu00/following{/other_user}", "gists_url": "https://api.github.com/users/Erichliu00/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erichliu00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erichliu00/subscriptions", "organizations_url": "https://api.github.com/users/Erichliu00/orgs", "repos_url": "https://api.github.com/users/Erichliu00/repos", "events_url": "https://api.github.com/users/Erichliu00/events{/privacy}", "received_events_url": "https://api.github.com/users/Erichliu00/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-11-28T20:44:24Z", "updated_at": "2017-11-29T20:40:12Z", "closed_at": "2017-11-29T17:16:54Z", "author_association": "NONE", "body_html": "<p>In some situation, it is necessary to get the 'mathematical' gradient of output w.r.t neural network parameters.</p>\n<p>For example, suppose I have a neural network <code>out = f(s)</code>, where <code>s</code> is a batch of input with shape<code>[None, dim_s]</code>, while out is a scaler, <code>f</code> is simply a MLP. With <code>tf.gradient(out, tf.trainable_variables())</code> I can get gradient of out w.r.t neural network parameters of f, which is a list of gradient. Now, I have two different batch of <code>s</code>: <code>s1</code> and <code>s2</code>, then we can get two different the above gradients <code>G1</code> and <code>G2</code>. It seems that it is impossible to compute cosine between <code>G1</code> and <code>G2</code> using current tensorflow? Do I need to flatten both gradients first? Do <code>G1</code> and <code>G2</code> are the usual gradient in math?</p>", "body_text": "In some situation, it is necessary to get the 'mathematical' gradient of output w.r.t neural network parameters.\nFor example, suppose I have a neural network out = f(s), where s is a batch of input with shape[None, dim_s], while out is a scaler, f is simply a MLP. With tf.gradient(out, tf.trainable_variables()) I can get gradient of out w.r.t neural network parameters of f, which is a list of gradient. Now, I have two different batch of s: s1 and s2, then we can get two different the above gradients G1 and G2. It seems that it is impossible to compute cosine between G1 and G2 using current tensorflow? Do I need to flatten both gradients first? Do G1 and G2 are the usual gradient in math?", "body": "In some situation, it is necessary to get the 'mathematical' gradient of output w.r.t neural network parameters. \r\n\r\nFor example, suppose I have a neural network `out = f(s)`, where `s` is a batch of input with shape`[None, dim_s]`, while out is a scaler, `f` is simply a MLP. With `tf.gradient(out, tf.trainable_variables())` I can get gradient of out w.r.t neural network parameters of f, which is a list of gradient. Now, I have two different batch of `s`: `s1` and `s2`, then we can get two different the above gradients `G1` and `G2`. It seems that it is impossible to compute cosine between `G1` and `G2` using current tensorflow? Do I need to flatten both gradients first? Do `G1` and `G2` are the usual gradient in math? "}