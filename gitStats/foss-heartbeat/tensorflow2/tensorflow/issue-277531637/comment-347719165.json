{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/347719165", "html_url": "https://github.com/tensorflow/tensorflow/issues/14954#issuecomment-347719165", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954", "id": 347719165, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzcxOTE2NQ==", "user": {"login": "Erichliu00", "id": 23612416, "node_id": "MDQ6VXNlcjIzNjEyNDE2", "avatar_url": "https://avatars1.githubusercontent.com/u/23612416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erichliu00", "html_url": "https://github.com/Erichliu00", "followers_url": "https://api.github.com/users/Erichliu00/followers", "following_url": "https://api.github.com/users/Erichliu00/following{/other_user}", "gists_url": "https://api.github.com/users/Erichliu00/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erichliu00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erichliu00/subscriptions", "organizations_url": "https://api.github.com/users/Erichliu00/orgs", "repos_url": "https://api.github.com/users/Erichliu00/repos", "events_url": "https://api.github.com/users/Erichliu00/events{/privacy}", "received_events_url": "https://api.github.com/users/Erichliu00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T01:12:58Z", "updated_at": "2017-11-29T01:24:03Z", "author_association": "NONE", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> Thanks. I have changed problem to scaler output to simplifier statement.</p>\n<p>An example:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nbatch_1=np.random.normal(0,1, [2, 3])\nbatch_2=np.random.normal(0,1, [2, 3])\nx = tf.placeholder(tf.float32, shape=(None, 3))\n\npredictions = tf.layers.dense(x, 1, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / 100)))\n\nopt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\n\nsess=tf.Session()\nsess.run(tf.global_variables_initializer())\ngradients_1 = sess.run(gradient_step, feed_dict={x: batch_1})\ngradients_2 = sess.run(gradient_step, feed_dict={x: batch_2})\n</code></pre>\n<p>Then both <code>gradients_1</code> and <code>gradients_2</code> will look something like:</p>\n<blockquote>\n<p>[(array([[-0.80507326],<br>\n[-1.30997419],<br>\n[-1.71417713]], dtype=float32), array([[-0.07164487],<br>\n[-0.05090996],<br>\n[ 0.01569027]], dtype=float32)), (array([ 1.99042296], dtype=float32), array([ 0.], dtype=float32))]</p>\n</blockquote>\n<p>with shape as following:</p>\n<blockquote>\n<p>gradients_1[0][0].shape<br>\n(3, 1)<br>\ngradients_1[0][1].shape<br>\n(3, 1)<br>\ngradients_1[1][1].shape<br>\n(1,)<br>\ngradients_1[1][0].shape<br>\n(1,)</p>\n</blockquote>\n<p>Given the gradient is of such kind of shape, I think there is a gap between tensorflow gradient and the usual mathematical gradient(which usually is a vector or matrix), so it makes some applications, such as compute the cosine between <code>gradients_1</code> and <code>gradients_2</code> seems intractable.</p>", "body_text": "Hi, @aselle Thanks. I have changed problem to scaler output to simplifier statement.\nAn example:\nimport tensorflow as tf\nimport numpy as np\n\nbatch_1=np.random.normal(0,1, [2, 3])\nbatch_2=np.random.normal(0,1, [2, 3])\nx = tf.placeholder(tf.float32, shape=(None, 3))\n\npredictions = tf.layers.dense(x, 1, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / 100)))\n\nopt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\n\nsess=tf.Session()\nsess.run(tf.global_variables_initializer())\ngradients_1 = sess.run(gradient_step, feed_dict={x: batch_1})\ngradients_2 = sess.run(gradient_step, feed_dict={x: batch_2})\n\nThen both gradients_1 and gradients_2 will look something like:\n\n[(array([[-0.80507326],\n[-1.30997419],\n[-1.71417713]], dtype=float32), array([[-0.07164487],\n[-0.05090996],\n[ 0.01569027]], dtype=float32)), (array([ 1.99042296], dtype=float32), array([ 0.], dtype=float32))]\n\nwith shape as following:\n\ngradients_1[0][0].shape\n(3, 1)\ngradients_1[0][1].shape\n(3, 1)\ngradients_1[1][1].shape\n(1,)\ngradients_1[1][0].shape\n(1,)\n\nGiven the gradient is of such kind of shape, I think there is a gap between tensorflow gradient and the usual mathematical gradient(which usually is a vector or matrix), so it makes some applications, such as compute the cosine between gradients_1 and gradients_2 seems intractable.", "body": "Hi, @aselle Thanks. I have changed problem to scaler output to simplifier statement.\r\n\r\nAn example: \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_1=np.random.normal(0,1, [2, 3])\r\nbatch_2=np.random.normal(0,1, [2, 3])\r\nx = tf.placeholder(tf.float32, shape=(None, 3))\r\n\r\npredictions = tf.layers.dense(x, 1, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\n\r\nopt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\r\n\r\nsess=tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ngradients_1 = sess.run(gradient_step, feed_dict={x: batch_1})\r\ngradients_2 = sess.run(gradient_step, feed_dict={x: batch_2})\r\n```\r\n\r\n\r\nThen both `gradients_1` and `gradients_2` will look something like:\r\n\r\n> [(array([[-0.80507326],\r\n>        [-1.30997419],\r\n>        [-1.71417713]], dtype=float32), array([[-0.07164487],\r\n>        [-0.05090996],\r\n>        [ 0.01569027]], dtype=float32)), (array([ 1.99042296], dtype=float32), array([ 0.], dtype=float32))]\r\n\r\nwith shape as following:\r\n> gradients_1[0][0].shape\r\n(3, 1)\r\n> gradients_1[0][1].shape\r\n(3, 1)\r\n> gradients_1[1][1].shape\r\n(1,)\r\n> gradients_1[1][0].shape\r\n(1,)\r\n\r\nGiven the gradient is of such kind of shape, I think there is a gap between tensorflow gradient and the usual mathematical gradient(which usually is a vector or matrix), so it makes some applications, such as compute the cosine between `gradients_1` and `gradients_2` seems intractable.\r\n\r\n\r\n\r\n"}