{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/347983099", "html_url": "https://github.com/tensorflow/tensorflow/issues/14954#issuecomment-347983099", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954", "id": 347983099, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Nzk4MzA5OQ==", "user": {"login": "Erichliu00", "id": 23612416, "node_id": "MDQ6VXNlcjIzNjEyNDE2", "avatar_url": "https://avatars1.githubusercontent.com/u/23612416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erichliu00", "html_url": "https://github.com/Erichliu00", "followers_url": "https://api.github.com/users/Erichliu00/followers", "following_url": "https://api.github.com/users/Erichliu00/following{/other_user}", "gists_url": "https://api.github.com/users/Erichliu00/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erichliu00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erichliu00/subscriptions", "organizations_url": "https://api.github.com/users/Erichliu00/orgs", "repos_url": "https://api.github.com/users/Erichliu00/repos", "events_url": "https://api.github.com/users/Erichliu00/events{/privacy}", "received_events_url": "https://api.github.com/users/Erichliu00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T20:16:48Z", "updated_at": "2017-11-29T20:40:12Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> , thanks, you are right, I should consider gradient of each variable to compute cosine and also should not expect a flatten gradient since variable has shape that is not flattened.  However, I still believe that if gradient is not the usual flattened gradient, then some computation may not applicable.<br>\nFor example, suppose we have a neural network parameter <code>x</code> with shape <code>[2,4]</code>, then the gradient of output scaler w.r.t it is of the same shape <code>[2,4]</code>, then given four different batches of input, we can have four values of <code>x</code>, say <code>xi= np.random.normal(0,1, [2,4])</code> for i = 0,1,2,3,  how can we compute the variance of <code>x</code> given four observations?</p>", "body_text": "Hi @aselle , thanks, you are right, I should consider gradient of each variable to compute cosine and also should not expect a flatten gradient since variable has shape that is not flattened.  However, I still believe that if gradient is not the usual flattened gradient, then some computation may not applicable.\nFor example, suppose we have a neural network parameter x with shape [2,4], then the gradient of output scaler w.r.t it is of the same shape [2,4], then given four different batches of input, we can have four values of x, say xi= np.random.normal(0,1, [2,4]) for i = 0,1,2,3,  how can we compute the variance of x given four observations?", "body": "Hi @aselle , thanks, you are right, I should consider gradient of each variable to compute cosine and also should not expect a flatten gradient since variable has shape that is not flattened.  However, I still believe that if gradient is not the usual flattened gradient, then some computation may not applicable.\r\nFor example, suppose we have a neural network parameter `x` with shape `[2,4]`, then the gradient of output scaler w.r.t it is of the same shape `[2,4]`, then given four different batches of input, we can have four values of `x`, say `xi= np.random.normal(0,1, [2,4])` for i = 0,1,2,3,  how can we compute the variance of `x` given four observations?"}