{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/347915977", "html_url": "https://github.com/tensorflow/tensorflow/issues/14954#issuecomment-347915977", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954", "id": 347915977, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzkxNTk3Nw==", "user": {"login": "Erichliu00", "id": 23612416, "node_id": "MDQ6VXNlcjIzNjEyNDE2", "avatar_url": "https://avatars1.githubusercontent.com/u/23612416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erichliu00", "html_url": "https://github.com/Erichliu00", "followers_url": "https://api.github.com/users/Erichliu00/followers", "following_url": "https://api.github.com/users/Erichliu00/following{/other_user}", "gists_url": "https://api.github.com/users/Erichliu00/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erichliu00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erichliu00/subscriptions", "organizations_url": "https://api.github.com/users/Erichliu00/orgs", "repos_url": "https://api.github.com/users/Erichliu00/repos", "events_url": "https://api.github.com/users/Erichliu00/events{/privacy}", "received_events_url": "https://api.github.com/users/Erichliu00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T16:29:09Z", "updated_at": "2017-11-29T17:13:28Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1112263\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/facaiy\">@facaiy</a> , thanks.  Indeed I want to get the gradient of output <code>predictions </code> w.r.t neural network parameters <code>tf.trainable_variables </code>, however, my concern is that <code>tf.trainable_variables </code> will contains some variables with shape like <code>3x2</code> then the gradient w.r.t such variable denoted as <code>g1</code> and <code>g2</code> will be of the same shape <code>3x2</code>, then my concerns are:</p>\n<ol>\n<li>\n<p>are the gradient <code>g1</code> and <code>g2</code> averaged over batch input or summed over batch, mathematically, seems we should get a averaged gradient.</p>\n</li>\n<li>\n<p>how can we compute cosine between <code>g1</code> and <code>g2</code> given their shapes are not a vector.</p>\n</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nbatch_1<span class=\"pl-k\">=</span>np.random.normal(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])\nbatch_2<span class=\"pl-k\">=</span>np.random.normal(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">3</span>))\n\nout <span class=\"pl-k\">=</span> tf.layers.dense(x, <span class=\"pl-c1\">2</span>, tf.tanh,\n                              <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(\n                                  <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span>np.sqrt(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">100</span>)))\npredictions <span class=\"pl-k\">=</span> tf.layers.dense(out, <span class=\"pl-c1\">1</span>, tf.tanh,\n                              <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(\n                                  <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span>np.sqrt(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">100</span>)))\n\nopt <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\ngradient_step <span class=\"pl-k\">=</span> opt.compute_gradients(predictions, tf.trainable_variables())\n\nsess<span class=\"pl-k\">=</span>tf.Session()\nsess.run(tf.global_variables_initializer())\ngradients_1 <span class=\"pl-k\">=</span> sess.run(gradient_step, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch_1})\ngradients_2 <span class=\"pl-k\">=</span> sess.run(gradient_step, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch_2})</pre></div>", "body_text": "Hi @facaiy , thanks.  Indeed I want to get the gradient of output predictions  w.r.t neural network parameters tf.trainable_variables , however, my concern is that tf.trainable_variables  will contains some variables with shape like 3x2 then the gradient w.r.t such variable denoted as g1 and g2 will be of the same shape 3x2, then my concerns are:\n\n\nare the gradient g1 and g2 averaged over batch input or summed over batch, mathematically, seems we should get a averaged gradient.\n\n\nhow can we compute cosine between g1 and g2 given their shapes are not a vector.\n\n\nimport tensorflow as tf\nimport numpy as np\n\nbatch_1=np.random.normal(0,1, [2, 3])\nbatch_2=np.random.normal(0,1, [2, 3])\nx = tf.placeholder(tf.float32, shape=(None, 3))\n\nout = tf.layers.dense(x, 2, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / 100)))\npredictions = tf.layers.dense(out, 1, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / 100)))\n\nopt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\n\nsess=tf.Session()\nsess.run(tf.global_variables_initializer())\ngradients_1 = sess.run(gradient_step, feed_dict={x: batch_1})\ngradients_2 = sess.run(gradient_step, feed_dict={x: batch_2})", "body": "Hi @facaiy , thanks.  Indeed I want to get the gradient of output `predictions ` w.r.t neural network parameters `tf.trainable_variables `, however, my concern is that `tf.trainable_variables ` will contains some variables with shape like `3x2` then the gradient w.r.t such variable denoted as `g1` and `g2` will be of the same shape `3x2`, then my concerns are: \r\n   1. are the gradient `g1` and `g2` averaged over batch input or summed over batch, mathematically, seems we should get a averaged gradient. \r\n   \r\n   2.  how can we compute cosine between `g1` and `g2` given their shapes are not a vector.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_1=np.random.normal(0,1, [2, 3])\r\nbatch_2=np.random.normal(0,1, [2, 3])\r\nx = tf.placeholder(tf.float32, shape=(None, 3))\r\n\r\nout = tf.layers.dense(x, 2, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\npredictions = tf.layers.dense(out, 1, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\n\r\nopt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\r\n\r\nsess=tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ngradients_1 = sess.run(gradient_step, feed_dict={x: batch_1})\r\ngradients_2 = sess.run(gradient_step, feed_dict={x: batch_2})\r\n```"}