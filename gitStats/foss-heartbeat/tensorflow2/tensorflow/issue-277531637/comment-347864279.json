{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/347864279", "html_url": "https://github.com/tensorflow/tensorflow/issues/14954#issuecomment-347864279", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14954", "id": 347864279, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Nzg2NDI3OQ==", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T13:45:16Z", "updated_at": "2017-11-29T13:47:18Z", "author_association": "MEMBER", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23612416\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Erichliu00\">@Erichliu00</a> .</p>\n<p>You build the function: <code>prediction = y = tanh(w x + b)</code> and calculate gradients as</p>\n<div class=\"highlight highlight-source-python\"><pre>gradient_step <span class=\"pl-k\">=</span> opt.compute_gradients(predictions, tf.trainable_variables())</pre></div>\n<p>here <code>tf.trainable_variables()</code> is <code>[w, b]</code>, so we get: <code>dy / dw</code> and <code>dy / db</code>, the corresponding shapes are (3, 1) and (1,). I think everything works well.</p>\n<p>Since you mentioned usual mathematical gradient, I guess that you might expect <code>dy / dx</code>, like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nbatch_1<span class=\"pl-k\">=</span>np.random.normal(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])\nbatch_2<span class=\"pl-k\">=</span>np.random.normal(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">3</span>))\n\npredictions <span class=\"pl-k\">=</span> tf.layers.dense(x, <span class=\"pl-c1\">1</span>, tf.tanh,\n                              <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(\n                                  <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span>np.sqrt(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">100</span>)))\n\ngrad_by_x <span class=\"pl-k\">=</span> tf.gradients(predictions, x)[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    gradients_1 <span class=\"pl-k\">=</span> sess.run(grad_by_x, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch_1})\n    gradients_2 <span class=\"pl-k\">=</span> sess.run(grad_by_x, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch_2})\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gradients_1: shape: <span class=\"pl-c1\">{}</span>, value:<span class=\"pl-cce\">\\n</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(gradients_1.shape, gradients_1))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gradients_2: shape: <span class=\"pl-c1\">{}</span>, value:<span class=\"pl-cce\">\\n</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(gradients_2.shape, gradients_2))</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">~</span>/Downloads \u276f\u276f\u276f python test.py\ngradients_1: shape: (2, 3), value:\n[[-0.00683434  0.2317003  -0.04384441]\n [-0.0068151   0.23104796 -0.04372097]]\ngradients_2: shape: (2, 3), value:\n[[-0.00593388  0.20117244 -0.03806766]\n [-0.00684012  0.23189643 -0.04388152]]</pre></div>", "body_text": "Hi, @Erichliu00 .\nYou build the function: prediction = y = tanh(w x + b) and calculate gradients as\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\nhere tf.trainable_variables() is [w, b], so we get: dy / dw and dy / db, the corresponding shapes are (3, 1) and (1,). I think everything works well.\nSince you mentioned usual mathematical gradient, I guess that you might expect dy / dx, like:\nimport tensorflow as tf\nimport numpy as np\n\nbatch_1=np.random.normal(0,1, [2, 3])\nbatch_2=np.random.normal(0,1, [2, 3])\nx = tf.placeholder(tf.float32, shape=(None, 3))\n\npredictions = tf.layers.dense(x, 1, tf.tanh,\n                              kernel_initializer=tf.random_normal_initializer(\n                                  stddev=np.sqrt(1 / 100)))\n\ngrad_by_x = tf.gradients(predictions, x)[0]\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    gradients_1 = sess.run(grad_by_x, feed_dict={x: batch_1})\n    gradients_2 = sess.run(grad_by_x, feed_dict={x: batch_2})\n    print(\"gradients_1: shape: {}, value:\\n{}\".format(gradients_1.shape, gradients_1))\n    print(\"gradients_2: shape: {}, value:\\n{}\".format(gradients_2.shape, gradients_2))\n~/Downloads \u276f\u276f\u276f python test.py\ngradients_1: shape: (2, 3), value:\n[[-0.00683434  0.2317003  -0.04384441]\n [-0.0068151   0.23104796 -0.04372097]]\ngradients_2: shape: (2, 3), value:\n[[-0.00593388  0.20117244 -0.03806766]\n [-0.00684012  0.23189643 -0.04388152]]", "body": "Hi, @Erichliu00 . \r\n\r\nYou build the function: `prediction = y = tanh(w x + b)` and calculate gradients as\r\n\r\n```python\r\ngradient_step = opt.compute_gradients(predictions, tf.trainable_variables())\r\n```\r\n\r\nhere `tf.trainable_variables()` is `[w, b]`, so we get: `dy / dw` and `dy / db`, the corresponding shapes are (3, 1) and (1,). I think everything works well.\r\n\r\nSince you mentioned usual mathematical gradient, I guess that you might expect `dy / dx`, like:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_1=np.random.normal(0,1, [2, 3])\r\nbatch_2=np.random.normal(0,1, [2, 3])\r\nx = tf.placeholder(tf.float32, shape=(None, 3))\r\n\r\npredictions = tf.layers.dense(x, 1, tf.tanh,\r\n                              kernel_initializer=tf.random_normal_initializer(\r\n                                  stddev=np.sqrt(1 / 100)))\r\n\r\ngrad_by_x = tf.gradients(predictions, x)[0]\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    gradients_1 = sess.run(grad_by_x, feed_dict={x: batch_1})\r\n    gradients_2 = sess.run(grad_by_x, feed_dict={x: batch_2})\r\n    print(\"gradients_1: shape: {}, value:\\n{}\".format(gradients_1.shape, gradients_1))\r\n    print(\"gradients_2: shape: {}, value:\\n{}\".format(gradients_2.shape, gradients_2))\r\n```\r\n\r\n```bash\r\n~/Downloads \u276f\u276f\u276f python test.py\r\ngradients_1: shape: (2, 3), value:\r\n[[-0.00683434  0.2317003  -0.04384441]\r\n [-0.0068151   0.23104796 -0.04372097]]\r\ngradients_2: shape: (2, 3), value:\r\n[[-0.00593388  0.20117244 -0.03806766]\r\n [-0.00684012  0.23189643 -0.04388152]]\r\n```\r\n"}