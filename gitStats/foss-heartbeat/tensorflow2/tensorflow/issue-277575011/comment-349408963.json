{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/349408963", "html_url": "https://github.com/tensorflow/tensorflow/issues/14957#issuecomment-349408963", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14957", "id": 349408963, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTQwODk2Mw==", "user": {"login": "drasmuss", "id": 1952220, "node_id": "MDQ6VXNlcjE5NTIyMjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1952220?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drasmuss", "html_url": "https://github.com/drasmuss", "followers_url": "https://api.github.com/users/drasmuss/followers", "following_url": "https://api.github.com/users/drasmuss/following{/other_user}", "gists_url": "https://api.github.com/users/drasmuss/gists{/gist_id}", "starred_url": "https://api.github.com/users/drasmuss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drasmuss/subscriptions", "organizations_url": "https://api.github.com/users/drasmuss/orgs", "repos_url": "https://api.github.com/users/drasmuss/repos", "events_url": "https://api.github.com/users/drasmuss/events{/privacy}", "received_events_url": "https://api.github.com/users/drasmuss/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-05T19:10:38Z", "updated_at": "2017-12-05T19:11:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>One more case; I'm not sure if the profiler takes into account allocations that happen within a loop.  E.g., this code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nsteps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\ntranspose <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\ngraph <span class=\"pl-k\">=</span> tf.Graph()\n<span class=\"pl-k\">with</span> graph.as_default(), tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/cpu:0<span class=\"pl-pds\">\"</span></span>):\n    p <span class=\"pl-k\">=</span> tf.TensorArray(tf.float64, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>steps)\n    a <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float64)\n    n <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">loop_body</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">z</span>):\n        <span class=\"pl-k\">if</span> transpose:\n            y <span class=\"pl-k\">=</span> tf.transpose(y)\n\n        z <span class=\"pl-k\">=</span> z.write(x, y[<span class=\"pl-c1\">0</span>])\n\n        <span class=\"pl-k\">return</span> x <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, y, z\n\n    _, _, c <span class=\"pl-k\">=</span> tf.while_loop(\n        <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">z</span>: x <span class=\"pl-k\">&lt;</span> n, loop_body,\n        [tf.constant(<span class=\"pl-c1\">0</span>), a, p])\n\n    c <span class=\"pl-k\">=</span> c.stack()\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph) <span class=\"pl-k\">as</span> sess:\n    run_options <span class=\"pl-k\">=</span> tf.RunOptions(<span class=\"pl-v\">trace_level</span><span class=\"pl-k\">=</span>tf.RunOptions.<span class=\"pl-c1\">FULL_TRACE</span>)\n    run_metadata <span class=\"pl-k\">=</span> tf.RunMetadata()\n    feed <span class=\"pl-k\">=</span> {n: steps, a: np.zeros((<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>))}\n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">sess_run</span>():\n        sess.run(c, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed,\n                 <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>run_options, <span class=\"pl-v\">run_metadata</span><span class=\"pl-k\">=</span>run_metadata\n                 )\n\n\n    sess_run()\n\n    options <span class=\"pl-k\">=</span> tf.profiler.ProfileOptionBuilder.time_and_memory()\n    options[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>min_bytes<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    options[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>min_micros<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    options[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>select<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bytes<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>peak_bytes<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_bytes<span class=\"pl-pds\">\"</span></span>,\n                         <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>residual_bytes<span class=\"pl-pds\">\"</span></span>)\n    tf.profiler.profile(graph, <span class=\"pl-v\">run_meta</span><span class=\"pl-k\">=</span>run_metadata, <span class=\"pl-v\">cmd</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scope<span class=\"pl-pds\">\"</span></span>,\n                        <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>options)</pre></div>\n<p>shows memory usage that increases linearly with the number of loop <code>steps</code></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1952220/33625352-46a52048-d9c5-11e7-9f32-e046527f3398.png\"><img src=\"https://user-images.githubusercontent.com/1952220/33625352-46a52048-d9c5-11e7-9f32-e046527f3398.png\" alt=\"mem_usage\" style=\"max-width:100%;\"></a></p>\n<p>But if we look at the profiler output, the reported memory usage of the <code>while</code> loop is constant regardless of the value of <code>steps</code> (and much less than the true memory usage):</p>\n<pre><code>==================Model Analysis Report======================\nnode name | requested bytes | peak bytes | residual bytes | output bytes\n_TFProfRoot (--/24.00MB, --/24.00MB, --/24.00MB, --/72.00MB)\n  while (0B/16.00MB, 0B/16.00MB, 0B/16.00MB, 0B/56.00MB)\n    while/transpose_1 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\n      while/transpose_1/sub (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n        while/transpose_1/sub/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n      while/transpose_1/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n      while/transpose_1/Range (8B/8B, 8B/8B, 8B/8B, 8B/8B)\n        while/transpose_1/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n        while/transpose_1/Range/start (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n      while/transpose_1/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\n    while/transpose (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\n      while/transpose/sub (4B/4B, 4B/4B, 4B/4B, 4B/8B)\n        while/transpose/sub/y (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      while/transpose/Range (8B/8B, 8B/8B, 8B/8B, 8B/12B)\n        while/transpose/Range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n        while/transpose/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n      while/transpose/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\n      while/transpose/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n    while/strided_slice (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n      while/strided_slice/stack (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      while/strided_slice/stack_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      while/strided_slice/stack_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/TensorArrayWrite (0B/0B, 0B/0B, 0B/0B, 0B/140B)\n      while/TensorArrayWrite/TensorArrayWriteV3 (0B/0B, 0B/0B, 0B/0B, 4B/140B)\n        while/TensorArrayWrite/TensorArrayWriteV3/Enter (0B/0B, 0B/0B, 0B/0B, 136B/136B)\n    while/Less (1B/1B, 1B/1B, 1B/1B, 1B/5B)\n      while/Less/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/add (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n      while/add/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Merge_1 (4B/4B, 4B/4B, 4B/4B, 8.00MB/8.00MB)\n    while/NextIteration_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n    while/NextIteration (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Merge_2 (4B/4B, 4B/4B, 4B/4B, 8B/8B)\n    while/Merge (4B/4B, 4B/4B, 4B/4B, 8B/8B)\n    while/LoopCond (0B/0B, 0B/0B, 0B/0B, 1B/1B)\n    while/Switch_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Switch (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/NextIteration_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Identity (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Switch_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n    while/Enter_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n    while/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Enter_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Exit_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Exit (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Exit_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Identity_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Identity_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  TensorArrayStack (0B/8.00MB, 0B/8.00MB, 0B/8.00MB, 0B/8.00MB)\n    TensorArrayStack/TensorArrayGatherV3 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\n    TensorArrayStack/range (4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.01KB)\n      TensorArrayStack/range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      TensorArrayStack/range/delta (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    TensorArrayStack/TensorArraySizeV3 (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n  TensorArray (204B/204B, 204B/204B, 204B/204B, 140B/144B)\n    TensorArray/size (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n  _arg_Placeholder_0_0 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n  _arg_Placeholder_1_0_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n  _retval_TensorArrayStack (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    _retval_TensorArrayStack/TensorArrayGatherV3_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  Const (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n  Placeholder (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  Placeholder_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  init (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n</code></pre>", "body_text": "One more case; I'm not sure if the profiler takes into account allocations that happen within a loop.  E.g., this code\nimport tensorflow as tf\nimport numpy as np\n\nsteps = 1000\ntranspose = True\n\ngraph = tf.Graph()\nwith graph.as_default(), tf.device(\"/cpu:0\"):\n    p = tf.TensorArray(tf.float64, size=steps)\n    a = tf.placeholder(shape=(1000, 1000), dtype=tf.float64)\n    n = tf.placeholder(shape=(), dtype=tf.int32)\n\n    def loop_body(x, y, z):\n        if transpose:\n            y = tf.transpose(y)\n\n        z = z.write(x, y[0])\n\n        return x + 1, y, z\n\n    _, _, c = tf.while_loop(\n        lambda x, y, z: x < n, loop_body,\n        [tf.constant(0), a, p])\n\n    c = c.stack()\n\nwith tf.Session(graph=graph) as sess:\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    feed = {n: steps, a: np.zeros((1000, 1000))}\n\n\n    def sess_run():\n        sess.run(c, feed_dict=feed,\n                 options=run_options, run_metadata=run_metadata\n                 )\n\n\n    sess_run()\n\n    options = tf.profiler.ProfileOptionBuilder.time_and_memory()\n    options[\"min_bytes\"] = 0\n    options[\"min_micros\"] = 0\n    options[\"select\"] = (\"bytes\", \"peak_bytes\", \"output_bytes\",\n                         \"residual_bytes\")\n    tf.profiler.profile(graph, run_meta=run_metadata, cmd=\"scope\",\n                        options=options)\nshows memory usage that increases linearly with the number of loop steps\n\nBut if we look at the profiler output, the reported memory usage of the while loop is constant regardless of the value of steps (and much less than the true memory usage):\n==================Model Analysis Report======================\nnode name | requested bytes | peak bytes | residual bytes | output bytes\n_TFProfRoot (--/24.00MB, --/24.00MB, --/24.00MB, --/72.00MB)\n  while (0B/16.00MB, 0B/16.00MB, 0B/16.00MB, 0B/56.00MB)\n    while/transpose_1 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\n      while/transpose_1/sub (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n        while/transpose_1/sub/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n      while/transpose_1/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n      while/transpose_1/Range (8B/8B, 8B/8B, 8B/8B, 8B/8B)\n        while/transpose_1/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n        while/transpose_1/Range/start (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n      while/transpose_1/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\n    while/transpose (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\n      while/transpose/sub (4B/4B, 4B/4B, 4B/4B, 4B/8B)\n        while/transpose/sub/y (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      while/transpose/Range (8B/8B, 8B/8B, 8B/8B, 8B/12B)\n        while/transpose/Range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n        while/transpose/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n      while/transpose/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\n      while/transpose/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n    while/strided_slice (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n      while/strided_slice/stack (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      while/strided_slice/stack_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      while/strided_slice/stack_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/TensorArrayWrite (0B/0B, 0B/0B, 0B/0B, 0B/140B)\n      while/TensorArrayWrite/TensorArrayWriteV3 (0B/0B, 0B/0B, 0B/0B, 4B/140B)\n        while/TensorArrayWrite/TensorArrayWriteV3/Enter (0B/0B, 0B/0B, 0B/0B, 136B/136B)\n    while/Less (1B/1B, 1B/1B, 1B/1B, 1B/5B)\n      while/Less/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/add (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n      while/add/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Merge_1 (4B/4B, 4B/4B, 4B/4B, 8.00MB/8.00MB)\n    while/NextIteration_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n    while/NextIteration (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Merge_2 (4B/4B, 4B/4B, 4B/4B, 8B/8B)\n    while/Merge (4B/4B, 4B/4B, 4B/4B, 8B/8B)\n    while/LoopCond (0B/0B, 0B/0B, 0B/0B, 1B/1B)\n    while/Switch_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Switch (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/NextIteration_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Identity (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Switch_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n    while/Enter_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n    while/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Enter_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Exit_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    while/Exit (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Exit_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Identity_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    while/Identity_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  TensorArrayStack (0B/8.00MB, 0B/8.00MB, 0B/8.00MB, 0B/8.00MB)\n    TensorArrayStack/TensorArrayGatherV3 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\n    TensorArrayStack/range (4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.01KB)\n      TensorArrayStack/range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n      TensorArrayStack/range/delta (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n    TensorArrayStack/TensorArraySizeV3 (4B/4B, 4B/4B, 4B/4B, 4B/4B)\n  TensorArray (204B/204B, 204B/204B, 204B/204B, 140B/144B)\n    TensorArray/size (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n  _arg_Placeholder_0_0 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n  _arg_Placeholder_1_0_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n  _retval_TensorArrayStack (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n    _retval_TensorArrayStack/TensorArrayGatherV3_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  Const (0B/0B, 0B/0B, 0B/0B, 4B/4B)\n  Placeholder (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  Placeholder_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n  init (0B/0B, 0B/0B, 0B/0B, 0B/0B)", "body": "One more case; I'm not sure if the profiler takes into account allocations that happen within a loop.  E.g., this code\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsteps = 1000\r\ntranspose = True\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default(), tf.device(\"/cpu:0\"):\r\n    p = tf.TensorArray(tf.float64, size=steps)\r\n    a = tf.placeholder(shape=(1000, 1000), dtype=tf.float64)\r\n    n = tf.placeholder(shape=(), dtype=tf.int32)\r\n\r\n    def loop_body(x, y, z):\r\n        if transpose:\r\n            y = tf.transpose(y)\r\n\r\n        z = z.write(x, y[0])\r\n\r\n        return x + 1, y, z\r\n\r\n    _, _, c = tf.while_loop(\r\n        lambda x, y, z: x < n, loop_body,\r\n        [tf.constant(0), a, p])\r\n\r\n    c = c.stack()\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    feed = {n: steps, a: np.zeros((1000, 1000))}\r\n\r\n\r\n    def sess_run():\r\n        sess.run(c, feed_dict=feed,\r\n                 options=run_options, run_metadata=run_metadata\r\n                 )\r\n\r\n\r\n    sess_run()\r\n\r\n    options = tf.profiler.ProfileOptionBuilder.time_and_memory()\r\n    options[\"min_bytes\"] = 0\r\n    options[\"min_micros\"] = 0\r\n    options[\"select\"] = (\"bytes\", \"peak_bytes\", \"output_bytes\",\r\n                         \"residual_bytes\")\r\n    tf.profiler.profile(graph, run_meta=run_metadata, cmd=\"scope\",\r\n                        options=options)\r\n```\r\nshows memory usage that increases linearly with the number of loop `steps`\r\n\r\n![mem_usage](https://user-images.githubusercontent.com/1952220/33625352-46a52048-d9c5-11e7-9f32-e046527f3398.png)\r\n\r\nBut if we look at the profiler output, the reported memory usage of the `while` loop is constant regardless of the value of `steps` (and much less than the true memory usage):\r\n```\r\n==================Model Analysis Report======================\r\nnode name | requested bytes | peak bytes | residual bytes | output bytes\r\n_TFProfRoot (--/24.00MB, --/24.00MB, --/24.00MB, --/72.00MB)\r\n  while (0B/16.00MB, 0B/16.00MB, 0B/16.00MB, 0B/56.00MB)\r\n    while/transpose_1 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\r\n      while/transpose_1/sub (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n        while/transpose_1/sub/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n      while/transpose_1/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n      while/transpose_1/Range (8B/8B, 8B/8B, 8B/8B, 8B/8B)\r\n        while/transpose_1/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n        while/transpose_1/Range/start (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n      while/transpose_1/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\r\n    while/transpose (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\r\n      while/transpose/sub (4B/4B, 4B/4B, 4B/4B, 4B/8B)\r\n        while/transpose/sub/y (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      while/transpose/Range (8B/8B, 8B/8B, 8B/8B, 8B/12B)\r\n        while/transpose/Range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n        while/transpose/Range/delta (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n      while/transpose/sub_1 (0B/0B, 0B/0B, 0B/0B, 8B/8B)\r\n      while/transpose/Rank (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n    while/strided_slice (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n      while/strided_slice/stack (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      while/strided_slice/stack_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      while/strided_slice/stack_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/TensorArrayWrite (0B/0B, 0B/0B, 0B/0B, 0B/140B)\r\n      while/TensorArrayWrite/TensorArrayWriteV3 (0B/0B, 0B/0B, 0B/0B, 4B/140B)\r\n        while/TensorArrayWrite/TensorArrayWriteV3/Enter (0B/0B, 0B/0B, 0B/0B, 136B/136B)\r\n    while/Less (1B/1B, 1B/1B, 1B/1B, 1B/5B)\r\n      while/Less/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/add (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n      while/add/y (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Merge_1 (4B/4B, 4B/4B, 4B/4B, 8.00MB/8.00MB)\r\n    while/NextIteration_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n    while/NextIteration (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Merge_2 (4B/4B, 4B/4B, 4B/4B, 8B/8B)\r\n    while/Merge (4B/4B, 4B/4B, 4B/4B, 8B/8B)\r\n    while/LoopCond (0B/0B, 0B/0B, 0B/0B, 1B/1B)\r\n    while/Switch_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Switch (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/NextIteration_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Identity (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Switch_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n    while/Enter_1 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n    while/Enter (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Enter_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Exit_2 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    while/Exit (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Exit_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Identity_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    while/Identity_2 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  TensorArrayStack (0B/8.00MB, 0B/8.00MB, 0B/8.00MB, 0B/8.00MB)\r\n    TensorArrayStack/TensorArrayGatherV3 (8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB, 8.00MB/8.00MB)\r\n    TensorArrayStack/range (4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.00KB, 4.00KB/4.01KB)\r\n      TensorArrayStack/range/start (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n      TensorArrayStack/range/delta (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n    TensorArrayStack/TensorArraySizeV3 (4B/4B, 4B/4B, 4B/4B, 4B/4B)\r\n  TensorArray (204B/204B, 204B/204B, 204B/204B, 140B/144B)\r\n    TensorArray/size (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n  _arg_Placeholder_0_0 (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n  _arg_Placeholder_1_0_1 (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n  _retval_TensorArrayStack (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n    _retval_TensorArrayStack/TensorArrayGatherV3_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  Const (0B/0B, 0B/0B, 0B/0B, 4B/4B)\r\n  Placeholder (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  Placeholder_1 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n  init (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n```\r\n\r\n"}