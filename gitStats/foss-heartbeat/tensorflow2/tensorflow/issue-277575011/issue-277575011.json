{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14957", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14957/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14957/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14957/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14957", "id": 277575011, "node_id": "MDU6SXNzdWUyNzc1NzUwMTE=", "number": 14957, "title": "tf.profiler reports 0B memory usage", "user": {"login": "drasmuss", "id": 1952220, "node_id": "MDQ6VXNlcjE5NTIyMjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1952220?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drasmuss", "html_url": "https://github.com/drasmuss", "followers_url": "https://api.github.com/users/drasmuss/followers", "following_url": "https://api.github.com/users/drasmuss/following{/other_user}", "gists_url": "https://api.github.com/users/drasmuss/gists{/gist_id}", "starred_url": "https://api.github.com/users/drasmuss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drasmuss/subscriptions", "organizations_url": "https://api.github.com/users/drasmuss/orgs", "repos_url": "https://api.github.com/users/drasmuss/repos", "events_url": "https://api.github.com/users/drasmuss/events{/privacy}", "received_events_url": "https://api.github.com/users/drasmuss/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "panyx0718", "id": 2887803, "node_id": "MDQ6VXNlcjI4ODc4MDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2887803?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panyx0718", "html_url": "https://github.com/panyx0718", "followers_url": "https://api.github.com/users/panyx0718/followers", "following_url": "https://api.github.com/users/panyx0718/following{/other_user}", "gists_url": "https://api.github.com/users/panyx0718/gists{/gist_id}", "starred_url": "https://api.github.com/users/panyx0718/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panyx0718/subscriptions", "organizations_url": "https://api.github.com/users/panyx0718/orgs", "repos_url": "https://api.github.com/users/panyx0718/repos", "events_url": "https://api.github.com/users/panyx0718/events{/privacy}", "received_events_url": "https://api.github.com/users/panyx0718/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "panyx0718", "id": 2887803, "node_id": "MDQ6VXNlcjI4ODc4MDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2887803?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panyx0718", "html_url": "https://github.com/panyx0718", "followers_url": "https://api.github.com/users/panyx0718/followers", "following_url": "https://api.github.com/users/panyx0718/following{/other_user}", "gists_url": "https://api.github.com/users/panyx0718/gists{/gist_id}", "starred_url": "https://api.github.com/users/panyx0718/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panyx0718/subscriptions", "organizations_url": "https://api.github.com/users/panyx0718/orgs", "repos_url": "https://api.github.com/users/panyx0718/repos", "events_url": "https://api.github.com/users/panyx0718/events{/privacy}", "received_events_url": "https://api.github.com/users/panyx0718/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-11-28T23:25:49Z", "updated_at": "2017-12-22T19:05:26Z", "closed_at": "2017-12-22T19:04:56Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5.0-dev20171103</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\ngraph <span class=\"pl-k\">=</span> tf.Graph()\n<span class=\"pl-k\">with</span> graph.as_default(), tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/cpu:0<span class=\"pl-pds\">\"</span></span>):\n    a <span class=\"pl-k\">=</span> tf.constant(np.ones((<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>)))\n    b <span class=\"pl-k\">=</span> tf.constant(np.ones((<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>)))\n    c <span class=\"pl-k\">=</span> a <span class=\"pl-k\">*</span> b\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph) <span class=\"pl-k\">as</span> sess:\n    run_options <span class=\"pl-k\">=</span> tf.RunOptions(<span class=\"pl-v\">trace_level</span><span class=\"pl-k\">=</span>tf.RunOptions.<span class=\"pl-c1\">FULL_TRACE</span>)\n    run_metadata <span class=\"pl-k\">=</span> tf.RunMetadata()\n    sess.run(c, <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>run_options, <span class=\"pl-v\">run_metadata</span><span class=\"pl-k\">=</span>run_metadata)\n\n    options <span class=\"pl-k\">=</span> tf.profiler.ProfileOptionBuilder.time_and_memory()\n    options[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>min_bytes<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    options[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>select<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bytes<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>peak_bytes<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_bytes<span class=\"pl-pds\">\"</span></span>,\n                         <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>residual_bytes<span class=\"pl-pds\">\"</span></span>)\n    tf.profiler.profile(graph, <span class=\"pl-v\">run_meta</span><span class=\"pl-k\">=</span>run_metadata, <span class=\"pl-v\">cmd</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scope<span class=\"pl-pds\">\"</span></span>,\n                        <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>options)</pre></div>\n<h3>Describe the problem</h3>\n<p>The above script gives output</p>\n<pre><code>==================Model Analysis Report======================\nnode name | requested bytes | peak bytes | residual bytes | output bytes\n_TFProfRoot (--/0B, --/0B, --/0B, --/8.00MB)\n  mul (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n  _retval_mul_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n</code></pre>\n<p><code>requested bytes</code> and <code>peak bytes</code> are both reported as 0, whereas I would have thought they would be 8MB (based on the description of these measures <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/options.md\">here</a>).</p>\n<p>I think the memory is not being recorded in the <code>RunMetadata</code> the way the profiler expects.  For example, if we look at</p>\n<div class=\"highlight highlight-source-python\"><pre>    mul_stats <span class=\"pl-k\">=</span> run_metadata.step_stats.dev_stats[<span class=\"pl-c1\">0</span>].node_stats[<span class=\"pl-c1\">1</span>]\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>total_bytes<span class=\"pl-pds\">\"</span></span>, [x.total_bytes <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> mul_stats.memory])   <span class=\"pl-c\"><span class=\"pl-c\">#</span> --&gt; [0]</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>persistent_mem<span class=\"pl-pds\">\"</span></span>, mul_stats.memory_stats.host_persistent_memory_size)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> --&gt; 8000000</span></pre></div>\n<p><code>\"total_bytes\"</code> is what the profiler reports as \"requested bytes\" above.  It seems like that data isn't being updated in the <code>run_metadata</code>.  The correct value is in <code>memory_stats.host_persistent_memory_size</code>, but that value isn't available in the profiler output.  And according to the <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/profiler/tfprof_log.proto\">profiler proto</a> that value is supposed to represent the bytes allocated to persistent objects (like Variables), even though none of the values in the example are persistent.  So I'm not sure if this is an issue with <code>tf.profiler</code>, or with how memory information is stored in <code>RunMetadata</code>.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.5.0-dev20171103\nPython version: 3.5\nExact command to reproduce:\n\nimport tensorflow as tf\nimport numpy as np\n\ngraph = tf.Graph()\nwith graph.as_default(), tf.device(\"/cpu:0\"):\n    a = tf.constant(np.ones((1000, 1000)))\n    b = tf.constant(np.ones((1000, 1000)))\n    c = a * b\n\nwith tf.Session(graph=graph) as sess:\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    sess.run(c, options=run_options, run_metadata=run_metadata)\n\n    options = tf.profiler.ProfileOptionBuilder.time_and_memory()\n    options[\"min_bytes\"] = 0\n    options[\"select\"] = (\"bytes\", \"peak_bytes\", \"output_bytes\",\n                         \"residual_bytes\")\n    tf.profiler.profile(graph, run_meta=run_metadata, cmd=\"scope\",\n                        options=options)\nDescribe the problem\nThe above script gives output\n==================Model Analysis Report======================\nnode name | requested bytes | peak bytes | residual bytes | output bytes\n_TFProfRoot (--/0B, --/0B, --/0B, --/8.00MB)\n  mul (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\n  _retval_mul_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\n\nrequested bytes and peak bytes are both reported as 0, whereas I would have thought they would be 8MB (based on the description of these measures here).\nI think the memory is not being recorded in the RunMetadata the way the profiler expects.  For example, if we look at\n    mul_stats = run_metadata.step_stats.dev_stats[0].node_stats[1]\n\n    print(\"total_bytes\", [x.total_bytes for x in mul_stats.memory])   # --> [0]\n    print(\"persistent_mem\", mul_stats.memory_stats.host_persistent_memory_size)  # --> 8000000\n\"total_bytes\" is what the profiler reports as \"requested bytes\" above.  It seems like that data isn't being updated in the run_metadata.  The correct value is in memory_stats.host_persistent_memory_size, but that value isn't available in the profiler output.  And according to the profiler proto that value is supposed to represent the bytes allocated to persistent objects (like Variables), even though none of the values in the example are persistent.  So I'm not sure if this is an issue with tf.profiler, or with how memory information is stored in RunMetadata.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171103\r\n- **Python version**: 3.5\r\n- **Exact command to reproduce**:\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default(), tf.device(\"/cpu:0\"):\r\n    a = tf.constant(np.ones((1000, 1000)))\r\n    b = tf.constant(np.ones((1000, 1000)))\r\n    c = a * b\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(c, options=run_options, run_metadata=run_metadata)\r\n\r\n    options = tf.profiler.ProfileOptionBuilder.time_and_memory()\r\n    options[\"min_bytes\"] = 0\r\n    options[\"select\"] = (\"bytes\", \"peak_bytes\", \"output_bytes\",\r\n                         \"residual_bytes\")\r\n    tf.profiler.profile(graph, run_meta=run_metadata, cmd=\"scope\",\r\n                        options=options)\r\n```\r\n\r\n### Describe the problem\r\nThe above script gives output\r\n```\r\n==================Model Analysis Report======================\r\nnode name | requested bytes | peak bytes | residual bytes | output bytes\r\n_TFProfRoot (--/0B, --/0B, --/0B, --/8.00MB)\r\n  mul (0B/0B, 0B/0B, 0B/0B, 8.00MB/8.00MB)\r\n  _retval_mul_0_0 (0B/0B, 0B/0B, 0B/0B, 0B/0B)\r\n```\r\n`requested bytes` and `peak bytes` are both reported as 0, whereas I would have thought they would be 8MB (based on the description of these measures [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/options.md)).  \r\n\r\nI think the memory is not being recorded in the `RunMetadata` the way the profiler expects.  For example, if we look at \r\n``` python\r\n    mul_stats = run_metadata.step_stats.dev_stats[0].node_stats[1]\r\n\r\n    print(\"total_bytes\", [x.total_bytes for x in mul_stats.memory])   # --> [0]\r\n    print(\"persistent_mem\", mul_stats.memory_stats.host_persistent_memory_size)  # --> 8000000\r\n```\r\n`\"total_bytes\"` is what the profiler reports as \"requested bytes\" above.  It seems like that data isn't being updated in the `run_metadata`.  The correct value is in `memory_stats.host_persistent_memory_size`, but that value isn't available in the profiler output.  And according to the [profiler proto](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/profiler/tfprof_log.proto) that value is supposed to represent the bytes allocated to persistent objects (like Variables), even though none of the values in the example are persistent.  So I'm not sure if this is an issue with `tf.profiler`, or with how memory information is stored in `RunMetadata`."}