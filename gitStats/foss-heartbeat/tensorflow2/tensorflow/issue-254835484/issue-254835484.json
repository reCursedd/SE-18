{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12772", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12772/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12772/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12772/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12772", "id": 254835484, "node_id": "MDU6SXNzdWUyNTQ4MzU0ODQ=", "number": 12772, "title": "Using Sparse tensors to apply gradients in BackPropogation. ", "user": {"login": "raginisharma14", "id": 21701931, "node_id": "MDQ6VXNlcjIxNzAxOTMx", "avatar_url": "https://avatars1.githubusercontent.com/u/21701931?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raginisharma14", "html_url": "https://github.com/raginisharma14", "followers_url": "https://api.github.com/users/raginisharma14/followers", "following_url": "https://api.github.com/users/raginisharma14/following{/other_user}", "gists_url": "https://api.github.com/users/raginisharma14/gists{/gist_id}", "starred_url": "https://api.github.com/users/raginisharma14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raginisharma14/subscriptions", "organizations_url": "https://api.github.com/users/raginisharma14/orgs", "repos_url": "https://api.github.com/users/raginisharma14/repos", "events_url": "https://api.github.com/users/raginisharma14/events{/privacy}", "received_events_url": "https://api.github.com/users/raginisharma14/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-09-02T20:55:37Z", "updated_at": "2017-12-05T17:18:07Z", "closed_at": "2017-12-05T17:18:07Z", "author_association": "NONE", "body_html": "<p>Hi All,</p>\n<p>I am trying to use sparse tensors while applying gradients and i see below error.<br>\n<strong>Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"Adam_1/update_conv1_1/weights/sub_2:0\", shape=(), dtype=float32)'</strong></p>\n<p>I am assuming this to be a bug as the dtype int64 must have been hardcoded for sparse tensors.<br>\nIs there any workaround for this issue?</p>\n<p>Here is what I am trying to do:<br>\n#################### this function multiplies gradients with prune weights so that gradient updation happens on pruned gradients.</p>\n<pre><code>def apply_prune_on_grads(grads_and_vars, sess, dict_n):\n     print(\"im inside pply_prune_on_grads\")\n     i=0\n     for k, v in dict_n.items():\n         count=0\n         for grad, var in grads_and_vars:\n             if var.name == k:\n                 op = (var.name).split(\"/\")\n                 if op[1] != 'biases:0':\n                     v = tf.cast(tf.constant(v), tf.float32)\n                     mask_tensor = tf.multiply(v,grad)\n                     idx = tf.where(tf.not_equal(mask_tensor, 0))\n                     sparse = tf.IndexedSlices(idx,tf.gather_nd(mask_tensor, idx))\n                     grads_and_vars[count] = (sparse, var)\n             count = count+1\n         i=i+1\n     return grads_and_vars\n\ndef prune_weights(weights_prune, sess,threshold = 0.01):\n         print(\"im inside prune weights\")\n         sparse_weights = {}\n         for v in weights_prune:\n             value = sess.run(v) \n             under_threshold = abs(value) &lt; threshold\n             value[under_threshold] = 0\n             sess.run(v.assign(value))\n             sparse_weights[v.name] = np.logical_not(under_threshold)\n         return sparse_weights\n</code></pre>\n<p>############################### The order in which i am calling the above functions in sess.run</p>\n<pre><code> sparse_weights = prune_weights(variables_to_restore, sess)\n grads_and_vars = train_op.compute_gradients(loss, )\n grads_and_vars =  apply_prune_on_grads(grads_and_vars,sess,  sparse_weights)\n train_step = train_op.apply_gradients(grads_and_vars) \n\n</code></pre>", "body_text": "Hi All,\nI am trying to use sparse tensors while applying gradients and i see below error.\nTensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"Adam_1/update_conv1_1/weights/sub_2:0\", shape=(), dtype=float32)'\nI am assuming this to be a bug as the dtype int64 must have been hardcoded for sparse tensors.\nIs there any workaround for this issue?\nHere is what I am trying to do:\n#################### this function multiplies gradients with prune weights so that gradient updation happens on pruned gradients.\ndef apply_prune_on_grads(grads_and_vars, sess, dict_n):\n     print(\"im inside pply_prune_on_grads\")\n     i=0\n     for k, v in dict_n.items():\n         count=0\n         for grad, var in grads_and_vars:\n             if var.name == k:\n                 op = (var.name).split(\"/\")\n                 if op[1] != 'biases:0':\n                     v = tf.cast(tf.constant(v), tf.float32)\n                     mask_tensor = tf.multiply(v,grad)\n                     idx = tf.where(tf.not_equal(mask_tensor, 0))\n                     sparse = tf.IndexedSlices(idx,tf.gather_nd(mask_tensor, idx))\n                     grads_and_vars[count] = (sparse, var)\n             count = count+1\n         i=i+1\n     return grads_and_vars\n\ndef prune_weights(weights_prune, sess,threshold = 0.01):\n         print(\"im inside prune weights\")\n         sparse_weights = {}\n         for v in weights_prune:\n             value = sess.run(v) \n             under_threshold = abs(value) < threshold\n             value[under_threshold] = 0\n             sess.run(v.assign(value))\n             sparse_weights[v.name] = np.logical_not(under_threshold)\n         return sparse_weights\n\n############################### The order in which i am calling the above functions in sess.run\n sparse_weights = prune_weights(variables_to_restore, sess)\n grads_and_vars = train_op.compute_gradients(loss, )\n grads_and_vars =  apply_prune_on_grads(grads_and_vars,sess,  sparse_weights)\n train_step = train_op.apply_gradients(grads_and_vars)", "body": "Hi All,\r\n\r\nI am trying to use sparse tensors while applying gradients and i see below error.\r\n **Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"Adam_1/update_conv1_1/weights/sub_2:0\", shape=(), dtype=float32)'**\r\n\r\nI am assuming this to be a bug as the dtype int64 must have been hardcoded for sparse tensors. \r\nIs there any workaround for this issue? \r\n\r\nHere is what I am trying to do:\r\n#################### this function multiplies gradients with prune weights so that gradient updation happens on pruned gradients.\r\n```\r\ndef apply_prune_on_grads(grads_and_vars, sess, dict_n):\r\n     print(\"im inside pply_prune_on_grads\")\r\n     i=0\r\n     for k, v in dict_n.items():\r\n         count=0\r\n         for grad, var in grads_and_vars:\r\n             if var.name == k:\r\n                 op = (var.name).split(\"/\")\r\n                 if op[1] != 'biases:0':\r\n                     v = tf.cast(tf.constant(v), tf.float32)\r\n                     mask_tensor = tf.multiply(v,grad)\r\n                     idx = tf.where(tf.not_equal(mask_tensor, 0))\r\n                     sparse = tf.IndexedSlices(idx,tf.gather_nd(mask_tensor, idx))\r\n                     grads_and_vars[count] = (sparse, var)\r\n             count = count+1\r\n         i=i+1\r\n     return grads_and_vars\r\n\r\ndef prune_weights(weights_prune, sess,threshold = 0.01):\r\n         print(\"im inside prune weights\")\r\n         sparse_weights = {}\r\n         for v in weights_prune:\r\n             value = sess.run(v) \r\n             under_threshold = abs(value) < threshold\r\n             value[under_threshold] = 0\r\n             sess.run(v.assign(value))\r\n             sparse_weights[v.name] = np.logical_not(under_threshold)\r\n         return sparse_weights\r\n```\r\n\r\n############################### The order in which i am calling the above functions in sess.run\r\n```\r\n sparse_weights = prune_weights(variables_to_restore, sess)\r\n grads_and_vars = train_op.compute_gradients(loss, )\r\n grads_and_vars =  apply_prune_on_grads(grads_and_vars,sess,  sparse_weights)\r\n train_step = train_op.apply_gradients(grads_and_vars) \r\n\r\n```\r\n\r\n\r\n\r\n"}