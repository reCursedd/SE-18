{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6800", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6800/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6800/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6800/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6800", "id": 200260604, "node_id": "MDU6SXNzdWUyMDAyNjA2MDQ=", "number": 6800, "title": "Getting AbortionError when running modified tensorflow serving client", "user": {"login": "Caisho", "id": 20179795, "node_id": "MDQ6VXNlcjIwMTc5Nzk1", "avatar_url": "https://avatars1.githubusercontent.com/u/20179795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Caisho", "html_url": "https://github.com/Caisho", "followers_url": "https://api.github.com/users/Caisho/followers", "following_url": "https://api.github.com/users/Caisho/following{/other_user}", "gists_url": "https://api.github.com/users/Caisho/gists{/gist_id}", "starred_url": "https://api.github.com/users/Caisho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Caisho/subscriptions", "organizations_url": "https://api.github.com/users/Caisho/orgs", "repos_url": "https://api.github.com/users/Caisho/repos", "events_url": "https://api.github.com/users/Caisho/events{/privacy}", "received_events_url": "https://api.github.com/users/Caisho/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-01-12T02:09:25Z", "updated_at": "2017-01-12T23:09:54Z", "closed_at": "2017-01-12T23:09:43Z", "author_association": "NONE", "body_html": "<p>(I initially posted this on stackoverflow but have gotten no response.) I modified the mnist_export.py and mnist_client.py to run an LSTM on some excel data. No issue with the training and exporting, but I run into this error below when running the client code.</p>\n<pre><code>grpc.framework.interfaces.face.face.AbortionError: \n    AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type \n    double does not match declared output type float for node _recv_x_0 = \n    _Recv[client_terminated=true, \n    recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", \n    send_device=\"/job:localhost/replica:0/task:0/cpu:0\", \n    send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", \n    tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"()\")\n</code></pre>\n<p>My input data is in the shape of [None, 1, 20] where 1 is the time_step and 20 is the features.</p>\n<p>Below are the relevant parts of my training and export code:</p>\n<pre><code>def RNN(x, weights, biases):\n     # Prepare data shape to match `rnn` function requirements\n     # Current data input shape: (batch_size, n_steps, n_input)\n     # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n     # Permuting batch_size and n_steps\n     x = tf.transpose(x, [1, 0, 2])\n     # Reshaping to (n_steps*batch_size, n_input)\n     x = tf.reshape(x, [-1, n_input])\n     # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n     x = tf.split(0, n_steps, x)\n     # Define a lstm cell with tensorflow\n     lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n     # Add dropout\n     #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=keep_prob)\n     #lstm_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * n_layers, state_is_tuple=True)\n     # Get lstm cell output\n     outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)\n     # Linear activation, using rnn inner loop last output\n     return tf.matmul(outputs[-1], weights['out']) + biases['out']\n \n # get data\n train_data_set, test_data_set = read_data_sets('AUDJPY Data.csv')\n \n # tf Graph input\n #x = tf.placeholder(\"float\", [None, n_steps, n_input])\n y_ = tf.placeholder(\"float\", [None, n_classes])\n keep_prob = tf.placeholder(tf.float32)\n \n # Exporter signatures\n serialized_tf_example = tf.placeholder(tf.string, name='tf_example')\n feature_configs = {\n     'x': tf.FixedLenFeature(shape=[n_steps,n_input], dtype=tf.float32),\n }\n tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name\n \n # Define weights\n weights = {\n     'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n }\n biases = {\n     'out': tf.Variable(tf.random_normal([n_classes]))\n }\n \n pred = RNN(x, weights, biases)\n \n # Define loss and optimizer\n y = tf.nn.softmax(pred, name='y')\n values, indices = tf.nn.top_k(y, k=4)\n classes = tf.contrib.lookup.index_to_string(tf.to_int64(indices), mapping=tf.constant([str(i) for i in range(n_classes)]))\n cost = -tf.reduce_sum(y_ * tf.log(y))\n optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n \n # Evaluate model\n correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n \n # Initializing the variables\n init = tf.global_variables_initializer()\n \n # Launch the graph\n with tf.Session() as sess:\n     sess.run(init)\n     step = 1\n     # Keep training until reach max iterations\n     while step * batch_size &lt; training_iters:\n         batch_x, batch_y = train_data_set.next_batch(batch_size)\n         # Reshape data to get 28 seq of 28 elements\n         batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n         # Run optimization op (backprop)\n         sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y})\n         if step % display_step == 0:\n             # Calculate batch accuracy\n             acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y})\n             # Calculate batch loss\n             loss = sess.run(cost, feed_dict={x: batch_x, y_: batch_y})\n             print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n                   \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                   \"{:.5f}\".format(acc))\n         step += 1\n     print(\"Optimization Finished!\")\n     print(\"Testing Accuracy:\", \\\n             sess.run(accuracy, feed_dict={x: test_data_set.features, y_: test_data_set.labels}))\n     # Export inference model.\n     export_path = '/tmp/'\n     print('Exporting trained model to %s' % export_path)\n     init_op = tf.group(tf.initialize_all_tables(), name='init_op')\n     saver = tf.train.Saver(sharded=True)\n     classification_signature = exporter.classification_signature(\n         input_tensor=serialized_tf_example,\n         classes_tensor=classes,\n         scores_tensor=values)\n     named_graph_signature = {\n         'inputs': exporter.generic_signature({'images': x}),\n         'outputs': exporter.generic_signature({'scores': y})}\n     model_exporter = exporter.Exporter(saver)\n     model_exporter.init(\n         init_op=init_op,\n         default_graph_signature=classification_signature,\n         named_graph_signatures=named_graph_signature)\n     model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\n     print('Done exporting!')\n</code></pre>\n<p>And below are the relevant part of the client code:</p>\n<pre><code>def do_inference(hostport, work_dir, concurrency, num_tests):\n      \"\"\"Tests PredictionService with concurrent requests.\n      Args:\n        hostport: Host:port address of the PredictionService.\n        work_dir: The full path of working directory for test data set.\n        concurrency: Maximum number of concurrent requests.\n        num_tests: Number of test images to use.\n      Returns:\n        The classification error rate.\n      Raises:\n        IOError: An error occurred processing test data set.\n      \"\"\"\n      train_data_set, test_data_set = read_data_sets(work_dir)\n      print('read test data')\n      host, port = hostport.split(':')\n      channel = implementations.insecure_channel(host, int(port))\n      stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n      request = predict_pb2.PredictRequest()\n      request.model_spec.name = 'mnist'\n      image, label = test_data_set.next_batch(1)\n      print(image.shape)\n      request.inputs['images'].CopyFrom(\n          tf.contrib.util.make_tensor_proto(image[0], shape=[1,1,20]))\n      result = stub.Predict(request, 10.0)\n</code></pre>\n<p>Also below is the traceback if that helps:</p>\n<pre><code>Traceback (most recent call last):\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 225, in &lt;module&gt;\n        tf.app.run()\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 220, in main\n        FLAGS.concurrency, FLAGS.num_tests)\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 208, in do_inference\n        result = stub.Predict(request, 10.0)\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\n        self._request_serializer, self._response_deserializer)\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\n        raise _abortion_error(rpc_error_call)\n    grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type float for node _recv_x_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")\n</code></pre>\n<p>Appreciate any help on how to fix this!</p>", "body_text": "(I initially posted this on stackoverflow but have gotten no response.) I modified the mnist_export.py and mnist_client.py to run an LSTM on some excel data. No issue with the training and exporting, but I run into this error below when running the client code.\ngrpc.framework.interfaces.face.face.AbortionError: \n    AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type \n    double does not match declared output type float for node _recv_x_0 = \n    _Recv[client_terminated=true, \n    recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", \n    send_device=\"/job:localhost/replica:0/task:0/cpu:0\", \n    send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", \n    tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"()\")\n\nMy input data is in the shape of [None, 1, 20] where 1 is the time_step and 20 is the features.\nBelow are the relevant parts of my training and export code:\ndef RNN(x, weights, biases):\n     # Prepare data shape to match `rnn` function requirements\n     # Current data input shape: (batch_size, n_steps, n_input)\n     # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n     # Permuting batch_size and n_steps\n     x = tf.transpose(x, [1, 0, 2])\n     # Reshaping to (n_steps*batch_size, n_input)\n     x = tf.reshape(x, [-1, n_input])\n     # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n     x = tf.split(0, n_steps, x)\n     # Define a lstm cell with tensorflow\n     lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n     # Add dropout\n     #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=keep_prob)\n     #lstm_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * n_layers, state_is_tuple=True)\n     # Get lstm cell output\n     outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)\n     # Linear activation, using rnn inner loop last output\n     return tf.matmul(outputs[-1], weights['out']) + biases['out']\n \n # get data\n train_data_set, test_data_set = read_data_sets('AUDJPY Data.csv')\n \n # tf Graph input\n #x = tf.placeholder(\"float\", [None, n_steps, n_input])\n y_ = tf.placeholder(\"float\", [None, n_classes])\n keep_prob = tf.placeholder(tf.float32)\n \n # Exporter signatures\n serialized_tf_example = tf.placeholder(tf.string, name='tf_example')\n feature_configs = {\n     'x': tf.FixedLenFeature(shape=[n_steps,n_input], dtype=tf.float32),\n }\n tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name\n \n # Define weights\n weights = {\n     'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n }\n biases = {\n     'out': tf.Variable(tf.random_normal([n_classes]))\n }\n \n pred = RNN(x, weights, biases)\n \n # Define loss and optimizer\n y = tf.nn.softmax(pred, name='y')\n values, indices = tf.nn.top_k(y, k=4)\n classes = tf.contrib.lookup.index_to_string(tf.to_int64(indices), mapping=tf.constant([str(i) for i in range(n_classes)]))\n cost = -tf.reduce_sum(y_ * tf.log(y))\n optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n \n # Evaluate model\n correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n \n # Initializing the variables\n init = tf.global_variables_initializer()\n \n # Launch the graph\n with tf.Session() as sess:\n     sess.run(init)\n     step = 1\n     # Keep training until reach max iterations\n     while step * batch_size < training_iters:\n         batch_x, batch_y = train_data_set.next_batch(batch_size)\n         # Reshape data to get 28 seq of 28 elements\n         batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n         # Run optimization op (backprop)\n         sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y})\n         if step % display_step == 0:\n             # Calculate batch accuracy\n             acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y})\n             # Calculate batch loss\n             loss = sess.run(cost, feed_dict={x: batch_x, y_: batch_y})\n             print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n                   \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                   \"{:.5f}\".format(acc))\n         step += 1\n     print(\"Optimization Finished!\")\n     print(\"Testing Accuracy:\", \\\n             sess.run(accuracy, feed_dict={x: test_data_set.features, y_: test_data_set.labels}))\n     # Export inference model.\n     export_path = '/tmp/'\n     print('Exporting trained model to %s' % export_path)\n     init_op = tf.group(tf.initialize_all_tables(), name='init_op')\n     saver = tf.train.Saver(sharded=True)\n     classification_signature = exporter.classification_signature(\n         input_tensor=serialized_tf_example,\n         classes_tensor=classes,\n         scores_tensor=values)\n     named_graph_signature = {\n         'inputs': exporter.generic_signature({'images': x}),\n         'outputs': exporter.generic_signature({'scores': y})}\n     model_exporter = exporter.Exporter(saver)\n     model_exporter.init(\n         init_op=init_op,\n         default_graph_signature=classification_signature,\n         named_graph_signatures=named_graph_signature)\n     model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\n     print('Done exporting!')\n\nAnd below are the relevant part of the client code:\ndef do_inference(hostport, work_dir, concurrency, num_tests):\n      \"\"\"Tests PredictionService with concurrent requests.\n      Args:\n        hostport: Host:port address of the PredictionService.\n        work_dir: The full path of working directory for test data set.\n        concurrency: Maximum number of concurrent requests.\n        num_tests: Number of test images to use.\n      Returns:\n        The classification error rate.\n      Raises:\n        IOError: An error occurred processing test data set.\n      \"\"\"\n      train_data_set, test_data_set = read_data_sets(work_dir)\n      print('read test data')\n      host, port = hostport.split(':')\n      channel = implementations.insecure_channel(host, int(port))\n      stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n      request = predict_pb2.PredictRequest()\n      request.model_spec.name = 'mnist'\n      image, label = test_data_set.next_batch(1)\n      print(image.shape)\n      request.inputs['images'].CopyFrom(\n          tf.contrib.util.make_tensor_proto(image[0], shape=[1,1,20]))\n      result = stub.Predict(request, 10.0)\n\nAlso below is the traceback if that helps:\nTraceback (most recent call last):\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 225, in <module>\n        tf.app.run()\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 220, in main\n        FLAGS.concurrency, FLAGS.num_tests)\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 208, in do_inference\n        result = stub.Predict(request, 10.0)\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\n        self._request_serializer, self._response_deserializer)\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\n        raise _abortion_error(rpc_error_call)\n    grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type float for node _recv_x_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")\n\nAppreciate any help on how to fix this!", "body": "(I initially posted this on stackoverflow but have gotten no response.) I modified the mnist_export.py and mnist_client.py to run an LSTM on some excel data. No issue with the training and exporting, but I run into this error below when running the client code. \r\n\r\n```\r\ngrpc.framework.interfaces.face.face.AbortionError: \r\n    AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type \r\n    double does not match declared output type float for node _recv_x_0 = \r\n    _Recv[client_terminated=true, \r\n    recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", \r\n    send_device=\"/job:localhost/replica:0/task:0/cpu:0\", \r\n    send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", \r\n    tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"()\")\r\n```\r\nMy input data is in the shape of [None, 1, 20] where 1 is the time_step and 20 is the features. \r\n\r\nBelow are the relevant parts of my training and export code: \r\n\r\n   ```\r\n def RNN(x, weights, biases):\r\n        # Prepare data shape to match `rnn` function requirements\r\n        # Current data input shape: (batch_size, n_steps, n_input)\r\n        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\r\n        # Permuting batch_size and n_steps\r\n        x = tf.transpose(x, [1, 0, 2])\r\n        # Reshaping to (n_steps*batch_size, n_input)\r\n        x = tf.reshape(x, [-1, n_input])\r\n        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\r\n        x = tf.split(0, n_steps, x)\r\n        # Define a lstm cell with tensorflow\r\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\r\n        # Add dropout\r\n        #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=keep_prob)\r\n        #lstm_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * n_layers, state_is_tuple=True)\r\n        # Get lstm cell output\r\n        outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)\r\n        # Linear activation, using rnn inner loop last output\r\n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\r\n    \r\n    # get data\r\n    train_data_set, test_data_set = read_data_sets('AUDJPY Data.csv')\r\n    \r\n    # tf Graph input\r\n    #x = tf.placeholder(\"float\", [None, n_steps, n_input])\r\n    y_ = tf.placeholder(\"float\", [None, n_classes])\r\n    keep_prob = tf.placeholder(tf.float32)\r\n    \r\n    # Exporter signatures\r\n    serialized_tf_example = tf.placeholder(tf.string, name='tf_example')\r\n    feature_configs = {\r\n        'x': tf.FixedLenFeature(shape=[n_steps,n_input], dtype=tf.float32),\r\n    }\r\n    tf_example = tf.parse_example(serialized_tf_example, feature_configs)\r\n    x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name\r\n    \r\n    # Define weights\r\n    weights = {\r\n        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\r\n    }\r\n    biases = {\r\n        'out': tf.Variable(tf.random_normal([n_classes]))\r\n    }\r\n    \r\n    pred = RNN(x, weights, biases)\r\n    \r\n    # Define loss and optimizer\r\n    y = tf.nn.softmax(pred, name='y')\r\n    values, indices = tf.nn.top_k(y, k=4)\r\n    classes = tf.contrib.lookup.index_to_string(tf.to_int64(indices), mapping=tf.constant([str(i) for i in range(n_classes)]))\r\n    cost = -tf.reduce_sum(y_ * tf.log(y))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n    \r\n    # Evaluate model\r\n    correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n    \r\n    # Initializing the variables\r\n    init = tf.global_variables_initializer()\r\n    \r\n    # Launch the graph\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        step = 1\r\n        # Keep training until reach max iterations\r\n        while step * batch_size < training_iters:\r\n            batch_x, batch_y = train_data_set.next_batch(batch_size)\r\n            # Reshape data to get 28 seq of 28 elements\r\n            batch_x = batch_x.reshape((batch_size, n_steps, n_input))\r\n            # Run optimization op (backprop)\r\n            sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y})\r\n            if step % display_step == 0:\r\n                # Calculate batch accuracy\r\n                acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y})\r\n                # Calculate batch loss\r\n                loss = sess.run(cost, feed_dict={x: batch_x, y_: batch_y})\r\n                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\r\n                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\r\n                      \"{:.5f}\".format(acc))\r\n            step += 1\r\n        print(\"Optimization Finished!\")\r\n        print(\"Testing Accuracy:\", \\\r\n                sess.run(accuracy, feed_dict={x: test_data_set.features, y_: test_data_set.labels}))\r\n        # Export inference model.\r\n        export_path = '/tmp/'\r\n        print('Exporting trained model to %s' % export_path)\r\n        init_op = tf.group(tf.initialize_all_tables(), name='init_op')\r\n        saver = tf.train.Saver(sharded=True)\r\n        classification_signature = exporter.classification_signature(\r\n            input_tensor=serialized_tf_example,\r\n            classes_tensor=classes,\r\n            scores_tensor=values)\r\n        named_graph_signature = {\r\n            'inputs': exporter.generic_signature({'images': x}),\r\n            'outputs': exporter.generic_signature({'scores': y})}\r\n        model_exporter = exporter.Exporter(saver)\r\n        model_exporter.init(\r\n            init_op=init_op,\r\n            default_graph_signature=classification_signature,\r\n            named_graph_signatures=named_graph_signature)\r\n        model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\r\n        print('Done exporting!')\r\n```\r\n\r\nAnd below are the relevant part of the client code:\r\n\r\n```\r\ndef do_inference(hostport, work_dir, concurrency, num_tests):\r\n      \"\"\"Tests PredictionService with concurrent requests.\r\n      Args:\r\n        hostport: Host:port address of the PredictionService.\r\n        work_dir: The full path of working directory for test data set.\r\n        concurrency: Maximum number of concurrent requests.\r\n        num_tests: Number of test images to use.\r\n      Returns:\r\n        The classification error rate.\r\n      Raises:\r\n        IOError: An error occurred processing test data set.\r\n      \"\"\"\r\n      train_data_set, test_data_set = read_data_sets(work_dir)\r\n      print('read test data')\r\n      host, port = hostport.split(':')\r\n      channel = implementations.insecure_channel(host, int(port))\r\n      stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n      request = predict_pb2.PredictRequest()\r\n      request.model_spec.name = 'mnist'\r\n      image, label = test_data_set.next_batch(1)\r\n      print(image.shape)\r\n      request.inputs['images'].CopyFrom(\r\n          tf.contrib.util.make_tensor_proto(image[0], shape=[1,1,20]))\r\n      result = stub.Predict(request, 10.0)\r\n```\r\n\r\nAlso below is the traceback if that helps:\r\n\r\n```\r\nTraceback (most recent call last):\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 225, in <module>\r\n        tf.app.run()\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 220, in main\r\n        FLAGS.concurrency, FLAGS.num_tests)\r\n      File \"/home/joel/Projects/serving/bazel-bin/tensorflow_serving/example/venatus_client.runfiles/tf_serving/tensorflow_serving/example/venatus_client.py\", line 208, in do_inference\r\n        result = stub.Predict(request, 10.0)\r\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\r\n        self._request_serializer, self._response_deserializer)\r\n      File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\r\n        raise _abortion_error(rpc_error_call)\r\n    grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type float for node _recv_x_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-9032417372349471954, tensor_name=\"x:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")\r\n```\r\n\r\nAppreciate any help on how to fix this!\r\n\r\n"}