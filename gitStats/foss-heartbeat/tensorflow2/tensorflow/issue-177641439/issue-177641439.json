{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4436", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4436/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4436/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4436/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4436", "id": 177641439, "node_id": "MDU6SXNzdWUxNzc2NDE0Mzk=", "number": 4436, "title": "How to restore the model when use distributed  tensorflow ??", "user": {"login": "thewintersun", "id": 8595690, "node_id": "MDQ6VXNlcjg1OTU2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8595690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thewintersun", "html_url": "https://github.com/thewintersun", "followers_url": "https://api.github.com/users/thewintersun/followers", "following_url": "https://api.github.com/users/thewintersun/following{/other_user}", "gists_url": "https://api.github.com/users/thewintersun/gists{/gist_id}", "starred_url": "https://api.github.com/users/thewintersun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thewintersun/subscriptions", "organizations_url": "https://api.github.com/users/thewintersun/orgs", "repos_url": "https://api.github.com/users/thewintersun/repos", "events_url": "https://api.github.com/users/thewintersun/events{/privacy}", "received_events_url": "https://api.github.com/users/thewintersun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2016-09-18T09:27:18Z", "updated_at": "2016-10-11T01:48:19Z", "closed_at": "2016-10-11T01:48:15Z", "author_association": "NONE", "body_html": "<p>I write a simple distributed tensorflow example, the code is here:<br>\n<a href=\"https://github.com/thewintersun/distributeTensorflowExample\">https://github.com/thewintersun/distributeTensorflowExample</a></p>\n<p>I use 2 server as the ps server, 2 server as the worker server.<br>\n<strong>The command is as follow:</strong><br>\n<strong>At ps server:</strong></p>\n<p>CUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=0</p>\n<p>CUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=1</p>\n<p><strong>At worker server:</strong></p>\n<p>CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=0</p>\n<p>CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=1</p>\n<p>**The train process is ok.</p>\n<h2>But when I stop the chief worker process and restart it ,  I expect the program will restore the model from the checkpoint file , but it print the following error and exit :**</h2>\n<p>I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)<br>\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {192.168.100.42:2222, 192.168.100.22:2223}<br>\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {localhost:2224, 192.168.100.253:2225}<br>\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224<br>\nE tensorflow/core/client/tensor_c_api.cc:485] Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0<br>\n[[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]<br>\nTraceback (most recent call last):<br>\nFile \"distribute.py\", line 77, in <br>\ntf.app.run()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv))<br>\nFile \"distribute.py\", line 60, in main<br>\nwith sv.managed_session(server.target) as sess:<br>\nFile \"/usr/lib/python2.7/contextlib.py\", line 17, in <strong>enter</strong><br>\nreturn self.gen.next()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 942, in managed_session<br>\nself.stop(close_summary_writer=close_summary_writer)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 768, in stop<br>\nstop_grace_period_secs=self._stop_grace_secs)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 357, in join<br>\nsix.reraise(*self._exc_info_to_raise)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 931, in managed_session<br>\nstart_standard_services=start_standard_services)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 680, in prepare_or_wait_for_session<br>\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 164, in prepare_session<br>\nmax_wait_secs=max_wait_secs, config=config)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 224, in recover_session<br>\nsaver.restore(sess, ckpt.model_checkpoint_path)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1129, in restore<br>\n{self.saver_def.filename_tensor_name: save_path})<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run<br>\nfeed_dict_string, options, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run<br>\ntarget_list, options, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0<br>\n[[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]<br>\nCaused by op u'save/restore_slice_2', defined at:<br>\nFile \"distribute.py\", line 77, in <br>\ntf.app.run()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv))<br>\nFile \"distribute.py\", line 49, in main<br>\nsaver = tf.train.Saver()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 861, in <strong>init</strong><br>\nrestore_sequentially=restore_sequentially)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 519, in build<br>\nfilename_tensor, vars_to_save, restore_sequentially, reshape)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps<br>\nvalues = self.restore_op(filename_tensor, vs, preferred_shard)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 187, in restore_op<br>\npreferred_shard=preferred_shard)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice<br>\npreferred_shard, name=name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice<br>\npreferred_shard=preferred_shard, name=name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op<br>\nop_def=op_def)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in <strong>init</strong></p>\n<h2>self._traceback = _extract_stack()</h2>\n<p>Is anyone can tell me how to restore the model when use distribute tensorflow??</p>", "body_text": "I write a simple distributed tensorflow example, the code is here:\nhttps://github.com/thewintersun/distributeTensorflowExample\nI use 2 server as the ps server, 2 server as the worker server.\nThe command is as follow:\nAt ps server:\nCUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=0\nCUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=1\nAt worker server:\nCUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=0\nCUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=1\n**The train process is ok.\nBut when I stop the chief worker process and restart it ,  I expect the program will restore the model from the checkpoint file , but it print the following error and exit :**\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {192.168.100.42:2222, 192.168.100.22:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2224, 192.168.100.253:2225}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224\nE tensorflow/core/client/tensor_c_api.cc:485] Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0\n[[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\nTraceback (most recent call last):\nFile \"distribute.py\", line 77, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv))\nFile \"distribute.py\", line 60, in main\nwith sv.managed_session(server.target) as sess:\nFile \"/usr/lib/python2.7/contextlib.py\", line 17, in enter\nreturn self.gen.next()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 942, in managed_session\nself.stop(close_summary_writer=close_summary_writer)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 768, in stop\nstop_grace_period_secs=self._stop_grace_secs)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 357, in join\nsix.reraise(*self._exc_info_to_raise)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 931, in managed_session\nstart_standard_services=start_standard_services)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 680, in prepare_or_wait_for_session\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 164, in prepare_session\nmax_wait_secs=max_wait_secs, config=config)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 224, in recover_session\nsaver.restore(sess, ckpt.model_checkpoint_path)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1129, in restore\n{self.saver_def.filename_tensor_name: save_path})\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\nrun_metadata_ptr)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\nfeed_dict_string, options, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\ntarget_list, options, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0\n[[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\nCaused by op u'save/restore_slice_2', defined at:\nFile \"distribute.py\", line 77, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv))\nFile \"distribute.py\", line 49, in main\nsaver = tf.train.Saver()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 861, in init\nrestore_sequentially=restore_sequentially)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 519, in build\nfilename_tensor, vars_to_save, restore_sequentially, reshape)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\nvalues = self.restore_op(filename_tensor, vs, preferred_shard)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\npreferred_shard=preferred_shard)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\npreferred_shard, name=name)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\npreferred_shard=preferred_shard, name=name)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\nop_def=op_def)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in init\nself._traceback = _extract_stack()\nIs anyone can tell me how to restore the model when use distribute tensorflow??", "body": "I write a simple distributed tensorflow example, the code is here:\nhttps://github.com/thewintersun/distributeTensorflowExample\n\nI use 2 server as the ps server, 2 server as the worker server. \n**The command is as follow:**\n**At ps server:**\n\nCUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=0\n\nCUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=ps --task_index=1\n\n**At worker server:**\n\nCUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=0\n\nCUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222,192.168.100.22:2223 --worker_hosts=192.168.100.30:2224,192.168.100.253:2225 --job_name=worker --task_index=1\n\n**The train process is ok.\n## But when I stop the chief worker process and restart it ,  I expect the program will restore the model from the checkpoint file , but it print the following error and exit :**\n\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {192.168.100.42:2222, 192.168.100.22:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2224, 192.168.100.253:2225}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224\nE tensorflow/core/client/tensor_c_api.cc:485] Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0\n         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\nTraceback (most recent call last):\n  File \"distribute.py\", line 77, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"distribute.py\", line 60, in main\n    with sv.managed_session(server.target) as sess:\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in **enter**\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 942, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 768, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 357, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 931, in managed_session\n    start_standard_services=start_standard_services)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 680, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 164, in prepare_session\n    max_wait_secs=max_wait_secs, config=config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 224, in recover_session\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1129, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoint/model.ckpt-0\n         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S1, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\nCaused by op u'save/restore_slice_2', defined at:\n  File \"distribute.py\", line 77, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"distribute.py\", line 49, in main\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 861, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 519, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n##     self._traceback = _extract_stack()\n\nIs anyone can tell me how to restore the model when use distribute tensorflow??\n"}