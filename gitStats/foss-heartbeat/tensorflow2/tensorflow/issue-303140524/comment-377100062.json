{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377100062", "html_url": "https://github.com/tensorflow/tensorflow/issues/17510#issuecomment-377100062", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17510", "id": 377100062, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzEwMDA2Mg==", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-29T02:26:13Z", "updated_at": "2018-03-29T07:06:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Oh I see it. Thanks for your patience.</p>\n<p>In the OrderMatters paper, <a href=\"https://www.reddit.com/r/MachineLearning/comments/6efl5g/d_order_matters_attention_mechanisms/di9wws9/\" rel=\"nofollow\">they say</a> their LSTM has no input, the input state size is <code>2h</code>, output state size is <code>h</code>, and the recurrent matrices are of shape <code>2h</code>-by-<code>h</code>.</p>\n<p>I think it is just a misnomer. Actually, you don't need the concatenation stuff: just rewrite the equation (3) as <code>q_t=LSTM(q_{t-1}, r_{t-1})</code>, where <code>q</code> is the LSTM state of size <code>h</code> and <code>r</code> is the input vector of size <code>h</code>(i.e.: vector <code>x</code> in Lipton, Berkowitz, and Elkan's notation), then this LSTM can fit into standard LSTM APIs.</p>\n<p>What's more, even if you use the notation in the paper, you'll need an attention mechanism over the memory <code>m</code>s, so you'll need to combine it with an <code>AttentionWrapper</code>, which is beyond the scope of an RNNCell.</p>\n<p>Of course you can post your code here (maybe a github repo link). If your API design is good enough and compatible with the TF library very well, it is worthwhile to include the new RNN cell into TF. And as far as I know, there is no open source implementation of OrderMatters. So it'll be very kind of you to do this.</p>", "body_text": "Oh I see it. Thanks for your patience.\nIn the OrderMatters paper, they say their LSTM has no input, the input state size is 2h, output state size is h, and the recurrent matrices are of shape 2h-by-h.\nI think it is just a misnomer. Actually, you don't need the concatenation stuff: just rewrite the equation (3) as q_t=LSTM(q_{t-1}, r_{t-1}), where q is the LSTM state of size h and r is the input vector of size h(i.e.: vector x in Lipton, Berkowitz, and Elkan's notation), then this LSTM can fit into standard LSTM APIs.\nWhat's more, even if you use the notation in the paper, you'll need an attention mechanism over the memory ms, so you'll need to combine it with an AttentionWrapper, which is beyond the scope of an RNNCell.\nOf course you can post your code here (maybe a github repo link). If your API design is good enough and compatible with the TF library very well, it is worthwhile to include the new RNN cell into TF. And as far as I know, there is no open source implementation of OrderMatters. So it'll be very kind of you to do this.", "body": "Oh I see it. Thanks for your patience.\r\n\r\nIn the OrderMatters paper, [they say](https://www.reddit.com/r/MachineLearning/comments/6efl5g/d_order_matters_attention_mechanisms/di9wws9/) their LSTM has no input, the input state size is `2h`, output state size is `h`, and the recurrent matrices are of shape `2h`-by-`h`.\r\n\r\nI think it is just a misnomer. Actually, you don't need the concatenation stuff: just rewrite the equation (3) as `q_t=LSTM(q_{t-1}, r_{t-1})`, where `q` is the LSTM state of size `h` and `r` is the input vector of size `h`(i.e.: vector `x` in Lipton, Berkowitz, and Elkan's notation), then this LSTM can fit into standard LSTM APIs.\r\n\r\nWhat's more, even if you use the notation in the paper, you'll need an attention mechanism over the memory `m`s, so you'll need to combine it with an `AttentionWrapper`, which is beyond the scope of an RNNCell.\r\n\r\nOf course you can post your code here (maybe a github repo link). If your API design is good enough and compatible with the TF library very well, it is worthwhile to include the new RNN cell into TF. And as far as I know, there is no open source implementation of OrderMatters. So it'll be very kind of you to do this.\r\n"}