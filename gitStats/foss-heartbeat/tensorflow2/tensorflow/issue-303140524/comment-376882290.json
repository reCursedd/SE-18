{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/376882290", "html_url": "https://github.com/tensorflow/tensorflow/issues/17510#issuecomment-376882290", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17510", "id": 376882290, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Njg4MjI5MA==", "user": {"login": "dkoguciuk", "id": 9368849, "node_id": "MDQ6VXNlcjkzNjg4NDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/9368849?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dkoguciuk", "html_url": "https://github.com/dkoguciuk", "followers_url": "https://api.github.com/users/dkoguciuk/followers", "following_url": "https://api.github.com/users/dkoguciuk/following{/other_user}", "gists_url": "https://api.github.com/users/dkoguciuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/dkoguciuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dkoguciuk/subscriptions", "organizations_url": "https://api.github.com/users/dkoguciuk/orgs", "repos_url": "https://api.github.com/users/dkoguciuk/repos", "events_url": "https://api.github.com/users/dkoguciuk/events{/privacy}", "received_events_url": "https://api.github.com/users/dkoguciuk/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-28T13:15:29Z", "updated_at": "2018-03-28T13:15:29Z", "author_association": "NONE", "body_html": "<p>Yes, but that's exactly what is proposed in <a href=\"https://arxiv.org/pdf/1511.06391.pdf\" rel=\"nofollow\">OrderMatters</a> paper!  After each timestep, the previous output state is concatenated with attention mechanism over the inputs (equation 7).</p>\n<p>I've already written all the code and it's working like a charm <g-emoji class=\"g-emoji\" alias=\"smiley\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f603.png\">\ud83d\ude03</g-emoji></p>", "body_text": "Yes, but that's exactly what is proposed in OrderMatters paper!  After each timestep, the previous output state is concatenated with attention mechanism over the inputs (equation 7).\nI've already written all the code and it's working like a charm \ud83d\ude03", "body": "Yes, but that's exactly what is proposed in [OrderMatters](https://arxiv.org/pdf/1511.06391.pdf) paper!  After each timestep, the previous output state is concatenated with attention mechanism over the inputs (equation 7).\r\n\r\nI've already written all the code and it's working like a charm :smiley: "}