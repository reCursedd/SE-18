{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23824", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23824/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23824/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23824/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23824", "id": 381905815, "node_id": "MDU6SXNzdWUzODE5MDU4MTU=", "number": 23824, "title": "Load SavedModel for Estimator API", "user": {"login": "SumNeuron", "id": 22868585, "node_id": "MDQ6VXNlcjIyODY4NTg1", "avatar_url": "https://avatars3.githubusercontent.com/u/22868585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SumNeuron", "html_url": "https://github.com/SumNeuron", "followers_url": "https://api.github.com/users/SumNeuron/followers", "following_url": "https://api.github.com/users/SumNeuron/following{/other_user}", "gists_url": "https://api.github.com/users/SumNeuron/gists{/gist_id}", "starred_url": "https://api.github.com/users/SumNeuron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SumNeuron/subscriptions", "organizations_url": "https://api.github.com/users/SumNeuron/orgs", "repos_url": "https://api.github.com/users/SumNeuron/repos", "events_url": "https://api.github.com/users/SumNeuron/events{/privacy}", "received_events_url": "https://api.github.com/users/SumNeuron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-17T22:09:04Z", "updated_at": "2018-11-17T22:09:04Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a feature request. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>TensorFlow version (you are using): 1.10+</li>\n<li>Are you willing to contribute it (Yes/No): yes</li>\n</ul>\n<p><strong>Describe the feature and the current behavior/state.</strong></p>\n<p>TensorFlow current touts several high level APIs including:</p>\n<blockquote>\n<ul>\n<li>Estimators, a high-level API that provides fully-packaged models ready for large-scale training and production.</li>\n</ul>\n</blockquote>\n<p>Focusing on what Estimator's encapsulate:</p>\n<blockquote>\n<p>Estimators encapsulate the following actions:</p>\n<ul>\n<li>training</li>\n<li>evaluation</li>\n<li>prediction</li>\n<li>export for serving</li>\n</ul>\n</blockquote>\n<p>there seems to be a glaring omission that straddles prediction and serving.</p>\n<p>Where is the import?</p>\n<p>Preparing a custom estimator for serving is an arduous journey, so once one has a <code>saved_model</code>, it would be nice to be able to load the saved model via the estimator class, rather than the <code>contrib</code> module:</p>\n<pre><code>from tensorflow.contrib import predictor\npredict_fn = predictor.from_saved_model('&lt;model-dir&gt;')\n</code></pre>\n<p>e.g.</p>\n<pre><code>est = tf.estimator.Estimator.from_saved_model(model_dir)\n\n# retrain new dataset or whatever, using estimator api rather than SavedModelPredictor\n</code></pre>\n<p>perhaps this was left out as it was the goal to only used exported models in the serving context.</p>\n<p>If that is the case, however, then the serving documentation needs critical updates. In addition it (the interface to serving) would benefit from a higher level api centered around estimators rather than any saved model.</p>\n<p>If I have a saved model and it is this easy to predict with it:</p>\n<pre><code>from tensorflow.contrib import predictor\npredict_fn = predictor.from_saved_model('&lt;model-dir&gt;')\npredict_fn(&lt;pred_features&gt;)\n</code></pre>\n<p>then it should be this easy to get a restful api to my model</p>\n<pre><code>&gt; tf-serve &lt;model-dir&gt; --host=localhost --port=8000\n</code></pre>\n<p>so that I can \"GET\" my prediction</p>\n<p>Now, I could be misinformed. It is very likely I do not understand the tensorflow/serving.git to the fullest and it might very well be that easy. If that is the case, then the documentation needs to be updated to clarify how one can just serve their model.</p>\n<p><strong>Will this change the current api? How?</strong></p>\n<p>Add a method to the <code>tf.estimator.Estimator</code> class <code>from_saved_model</code> or <code>import_saved_model</code> which allows one to recover the estimator.</p>\n<p>Also if the Estimator API is the high level API that the TF documentation leads me to believe it to be, perhaps make it directly compatible with TF.JS</p>\n<p><strong>Who will benefit with this feature?</strong></p>\n<p>People using the estimator api.</p>\n<p>People confused by the obtuse protocol to serve a custom estimator</p>\n<p><strong>Any Other info.</strong></p>", "body_text": "Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template\nSystem information\n\nTensorFlow version (you are using): 1.10+\nAre you willing to contribute it (Yes/No): yes\n\nDescribe the feature and the current behavior/state.\nTensorFlow current touts several high level APIs including:\n\n\nEstimators, a high-level API that provides fully-packaged models ready for large-scale training and production.\n\n\nFocusing on what Estimator's encapsulate:\n\nEstimators encapsulate the following actions:\n\ntraining\nevaluation\nprediction\nexport for serving\n\n\nthere seems to be a glaring omission that straddles prediction and serving.\nWhere is the import?\nPreparing a custom estimator for serving is an arduous journey, so once one has a saved_model, it would be nice to be able to load the saved model via the estimator class, rather than the contrib module:\nfrom tensorflow.contrib import predictor\npredict_fn = predictor.from_saved_model('<model-dir>')\n\ne.g.\nest = tf.estimator.Estimator.from_saved_model(model_dir)\n\n# retrain new dataset or whatever, using estimator api rather than SavedModelPredictor\n\nperhaps this was left out as it was the goal to only used exported models in the serving context.\nIf that is the case, however, then the serving documentation needs critical updates. In addition it (the interface to serving) would benefit from a higher level api centered around estimators rather than any saved model.\nIf I have a saved model and it is this easy to predict with it:\nfrom tensorflow.contrib import predictor\npredict_fn = predictor.from_saved_model('<model-dir>')\npredict_fn(<pred_features>)\n\nthen it should be this easy to get a restful api to my model\n> tf-serve <model-dir> --host=localhost --port=8000\n\nso that I can \"GET\" my prediction\nNow, I could be misinformed. It is very likely I do not understand the tensorflow/serving.git to the fullest and it might very well be that easy. If that is the case, then the documentation needs to be updated to clarify how one can just serve their model.\nWill this change the current api? How?\nAdd a method to the tf.estimator.Estimator class from_saved_model or import_saved_model which allows one to recover the estimator.\nAlso if the Estimator API is the high level API that the TF documentation leads me to believe it to be, perhaps make it directly compatible with TF.JS\nWho will benefit with this feature?\nPeople using the estimator api.\nPeople confused by the obtuse protocol to serve a custom estimator\nAny Other info.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.10+\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorFlow current touts several high level APIs including:\r\n\r\n> - Estimators, a high-level API that provides fully-packaged models ready for large-scale training and production.\r\n\r\nFocusing on what Estimator's encapsulate:\r\n\r\n> Estimators encapsulate the following actions:\r\n> - training\r\n> - evaluation\r\n> - prediction\r\n> - export for serving\r\n\r\n\r\nthere seems to be a glaring omission that straddles prediction and serving.\r\n\r\nWhere is the import?\r\n\r\nPreparing a custom estimator for serving is an arduous journey, so once one has a `saved_model`, it would be nice to be able to load the saved model via the estimator class, rather than the `contrib` module:\r\n\r\n```\r\nfrom tensorflow.contrib import predictor\r\npredict_fn = predictor.from_saved_model('<model-dir>')\r\n```\r\n\r\ne.g.\r\n\r\n```\r\nest = tf.estimator.Estimator.from_saved_model(model_dir)\r\n\r\n# retrain new dataset or whatever, using estimator api rather than SavedModelPredictor\r\n```\r\n\r\nperhaps this was left out as it was the goal to only used exported models in the serving context. \r\n\r\nIf that is the case, however, then the serving documentation needs critical updates. In addition it (the interface to serving) would benefit from a higher level api centered around estimators rather than any saved model.   \r\n\r\nIf I have a saved model and it is this easy to predict with it:\r\n\r\n```\r\nfrom tensorflow.contrib import predictor\r\npredict_fn = predictor.from_saved_model('<model-dir>')\r\npredict_fn(<pred_features>)\r\n```\r\n\r\nthen it should be this easy to get a restful api to my model\r\n\r\n```\r\n> tf-serve <model-dir> --host=localhost --port=8000\r\n```\r\n\r\nso that I can \"GET\" my prediction\r\n\r\n\r\nNow, I could be misinformed. It is very likely I do not understand the tensorflow/serving.git to the fullest and it might very well be that easy. If that is the case, then the documentation needs to be updated to clarify how one can just serve their model. \r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nAdd a method to the `tf.estimator.Estimator` class `from_saved_model` or `import_saved_model` which allows one to recover the estimator. \r\n\r\n\r\nAlso if the Estimator API is the high level API that the TF documentation leads me to believe it to be, perhaps make it directly compatible with TF.JS\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople using the estimator api.\r\n\r\nPeople confused by the obtuse protocol to serve a custom estimator\r\n\r\n**Any Other info.**\r\n"}