{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/672", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/672/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/672/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/672/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/672", "id": 124656693, "node_id": "MDU6SXNzdWUxMjQ2NTY2OTM=", "number": 672, "title": "Running different parts of a graph at a time without recomputation", "user": {"login": "Jabberwockyll", "id": 8118567, "node_id": "MDQ6VXNlcjgxMTg1Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8118567?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jabberwockyll", "html_url": "https://github.com/Jabberwockyll", "followers_url": "https://api.github.com/users/Jabberwockyll/followers", "following_url": "https://api.github.com/users/Jabberwockyll/following{/other_user}", "gists_url": "https://api.github.com/users/Jabberwockyll/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jabberwockyll/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jabberwockyll/subscriptions", "organizations_url": "https://api.github.com/users/Jabberwockyll/orgs", "repos_url": "https://api.github.com/users/Jabberwockyll/repos", "events_url": "https://api.github.com/users/Jabberwockyll/events{/privacy}", "received_events_url": "https://api.github.com/users/Jabberwockyll/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2016-01-03T16:15:56Z", "updated_at": "2018-08-05T16:01:05Z", "closed_at": "2016-03-11T23:56:15Z", "author_association": "NONE", "body_html": "<p>Motivated by <a href=\"http://stackoverflow.com/questions/34536340/how-to-use-tensorflow-optimizer-without-recomputing-activations-in-reinforcement\" rel=\"nofollow\">this StackOverflow question</a></p>\n<p>According to George Dahl's answer, there is currently no way to run part of a graph, then later run the entire graph without recomputing that part of the graph.  Couldn't you always just run the whole thing in the first place, you ask?  Consider this scenario:</p>\n<h3>Context/Example</h3>\n<p>You're using tensorflow to implement a reinforcement learning agent with value function approximation trained using stochastic gradient descent.  You're agent contains a method that is called once every iteration/timestep in the experiment.  The method takes an obervation and reward as input and outputs an action.  Consider this sequence of events (Say you're doing q-learning):</p>\n<ol>\n<li>At the end of an iteration, you compute your state-action values to choose an action and then return control to the calling program to simulate a step in the environment.</li>\n<li>At the beginning of the next iteration, it's time to update the parameters.  You want to use tensorflow's optimizer class to automatically calculate the gradients.  Here, you have three options:</li>\n</ol>\n<ul>\n<li>Just recompute the state-action values, so you can use the optimizer.  This option leaves you inefficiently calculating part of the graph twice.  This also might not even be viable if you were using something like recurrent neural networks, where calculating the activations changes the state of the nodes.</li>\n<li>Hardcode the gradients.  You could just write a function to calculate the gradients yourself and not even use optimizer.  However, if you were experimenting with different network architectures and activation functions in a big convolutional network, this could get pretty cumbersome.</li>\n<li>Call everything else from within your agent.  This sacrifices a lot of modularity and neatness in any framework bigger than a small experiment.</li>\n</ul>\n<p>So, in this case, the problem is that your graph (learning with SGD) depends on a value (observation/reward), calculated externally, that depends on the result of part of the graph (state-action values).  And we can't save the state-action values as variables to feed later as placeholders, because then they wouldn't be associated with the operations that produced them, which are required for automatic differentiation.</p>\n<h3>Solution</h3>\n<p>I'm not yet very familiar with the inner workings of tensorflow, so I'm not sure what the best solution would be.  It would need to be able to give Optimizer the tensors already calculated and the graph with the operations that produced them.  Maybe some kind of Session.partial_run() that saves the computed tensors associated with their nodes until a normal run is done?  Maybe a new method under Optimizer that returns an operation to calculate gradients, depending on a placeholder?  What do you think?</p>", "body_text": "Motivated by this StackOverflow question\nAccording to George Dahl's answer, there is currently no way to run part of a graph, then later run the entire graph without recomputing that part of the graph.  Couldn't you always just run the whole thing in the first place, you ask?  Consider this scenario:\nContext/Example\nYou're using tensorflow to implement a reinforcement learning agent with value function approximation trained using stochastic gradient descent.  You're agent contains a method that is called once every iteration/timestep in the experiment.  The method takes an obervation and reward as input and outputs an action.  Consider this sequence of events (Say you're doing q-learning):\n\nAt the end of an iteration, you compute your state-action values to choose an action and then return control to the calling program to simulate a step in the environment.\nAt the beginning of the next iteration, it's time to update the parameters.  You want to use tensorflow's optimizer class to automatically calculate the gradients.  Here, you have three options:\n\n\nJust recompute the state-action values, so you can use the optimizer.  This option leaves you inefficiently calculating part of the graph twice.  This also might not even be viable if you were using something like recurrent neural networks, where calculating the activations changes the state of the nodes.\nHardcode the gradients.  You could just write a function to calculate the gradients yourself and not even use optimizer.  However, if you were experimenting with different network architectures and activation functions in a big convolutional network, this could get pretty cumbersome.\nCall everything else from within your agent.  This sacrifices a lot of modularity and neatness in any framework bigger than a small experiment.\n\nSo, in this case, the problem is that your graph (learning with SGD) depends on a value (observation/reward), calculated externally, that depends on the result of part of the graph (state-action values).  And we can't save the state-action values as variables to feed later as placeholders, because then they wouldn't be associated with the operations that produced them, which are required for automatic differentiation.\nSolution\nI'm not yet very familiar with the inner workings of tensorflow, so I'm not sure what the best solution would be.  It would need to be able to give Optimizer the tensors already calculated and the graph with the operations that produced them.  Maybe some kind of Session.partial_run() that saves the computed tensors associated with their nodes until a normal run is done?  Maybe a new method under Optimizer that returns an operation to calculate gradients, depending on a placeholder?  What do you think?", "body": "Motivated by [this StackOverflow question](http://stackoverflow.com/questions/34536340/how-to-use-tensorflow-optimizer-without-recomputing-activations-in-reinforcement)\n\nAccording to George Dahl's answer, there is currently no way to run part of a graph, then later run the entire graph without recomputing that part of the graph.  Couldn't you always just run the whole thing in the first place, you ask?  Consider this scenario:\n### Context/Example\n\nYou're using tensorflow to implement a reinforcement learning agent with value function approximation trained using stochastic gradient descent.  You're agent contains a method that is called once every iteration/timestep in the experiment.  The method takes an obervation and reward as input and outputs an action.  Consider this sequence of events (Say you're doing q-learning):\n1.  At the end of an iteration, you compute your state-action values to choose an action and then return control to the calling program to simulate a step in the environment.\n2.  At the beginning of the next iteration, it's time to update the parameters.  You want to use tensorflow's optimizer class to automatically calculate the gradients.  Here, you have three options:\n-  Just recompute the state-action values, so you can use the optimizer.  This option leaves you inefficiently calculating part of the graph twice.  This also might not even be viable if you were using something like recurrent neural networks, where calculating the activations changes the state of the nodes.\n-  Hardcode the gradients.  You could just write a function to calculate the gradients yourself and not even use optimizer.  However, if you were experimenting with different network architectures and activation functions in a big convolutional network, this could get pretty cumbersome.\n-  Call everything else from within your agent.  This sacrifices a lot of modularity and neatness in any framework bigger than a small experiment.\n\nSo, in this case, the problem is that your graph (learning with SGD) depends on a value (observation/reward), calculated externally, that depends on the result of part of the graph (state-action values).  And we can't save the state-action values as variables to feed later as placeholders, because then they wouldn't be associated with the operations that produced them, which are required for automatic differentiation.\n### Solution\n\nI'm not yet very familiar with the inner workings of tensorflow, so I'm not sure what the best solution would be.  It would need to be able to give Optimizer the tensors already calculated and the graph with the operations that produced them.  Maybe some kind of Session.partial_run() that saves the computed tensors associated with their nodes until a normal run is done?  Maybe a new method under Optimizer that returns an operation to calculate gradients, depending on a placeholder?  What do you think?\n"}