{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/198210284", "html_url": "https://github.com/tensorflow/tensorflow/issues/672#issuecomment-198210284", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/672", "id": 198210284, "node_id": "MDEyOklzc3VlQ29tbWVudDE5ODIxMDI4NA==", "user": {"login": "markusdr", "id": 1832155, "node_id": "MDQ6VXNlcjE4MzIxNTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1832155?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markusdr", "html_url": "https://github.com/markusdr", "followers_url": "https://api.github.com/users/markusdr/followers", "following_url": "https://api.github.com/users/markusdr/following{/other_user}", "gists_url": "https://api.github.com/users/markusdr/gists{/gist_id}", "starred_url": "https://api.github.com/users/markusdr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markusdr/subscriptions", "organizations_url": "https://api.github.com/users/markusdr/orgs", "repos_url": "https://api.github.com/users/markusdr/repos", "events_url": "https://api.github.com/users/markusdr/events{/privacy}", "received_events_url": "https://api.github.com/users/markusdr/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-18T05:08:53Z", "updated_at": "2016-03-18T05:08:53Z", "author_association": "NONE", "body_html": "<p><code>partial_run</code> is a very useful feature!</p>\n<p>Just to make sure I understand correctly:</p>\n<p>In the following toy code, when I first derive <code>tmp</code> using <code>partial_run</code> and then compute the <code>cost</code> and <code>gradient</code>, is <code>tmp</code> only computed once and then reused multiple times?</p>\n<pre><code>import tensorflow as tf\n\nw = tf.Variable(1.0)\ninit = tf.initialize_all_variables()\n\nx = tf.placeholder(\"float\")\nslow = tf.exp(w * x * tf.constant(2.0))\ncost = x * slow + x * slow\ngrad = tf.gradients(cost, w)[0]\nfeed = {x: 3.0}\n\n# Efficient\nwith tf.Session() as sess:\n    sess.run(init)\n    h = sess.partial_run_setup([slow, cost, grad], [x])\n    # First derive slow\n    sess.partial_run(h, slow, feed_dict=feed)\n    # Then re-use slow multiple times (w/o recomputing it)\n    print(sess.partial_run(h, cost))\n    print(sess.partial_run(h, grad))\n\n# Naive / Inefficient \nwith tf.Session() as sess:\n    sess.run(init)\n    # Cost and grad each recompute 'slow' twice\n    print(sess.run(cost, feed_dict=feed))\n    print(sess.run(grad, feed_dict=feed))\n</code></pre>\n<p>And, is there a way to see/check the computation steps it's doing?</p>", "body_text": "partial_run is a very useful feature!\nJust to make sure I understand correctly:\nIn the following toy code, when I first derive tmp using partial_run and then compute the cost and gradient, is tmp only computed once and then reused multiple times?\nimport tensorflow as tf\n\nw = tf.Variable(1.0)\ninit = tf.initialize_all_variables()\n\nx = tf.placeholder(\"float\")\nslow = tf.exp(w * x * tf.constant(2.0))\ncost = x * slow + x * slow\ngrad = tf.gradients(cost, w)[0]\nfeed = {x: 3.0}\n\n# Efficient\nwith tf.Session() as sess:\n    sess.run(init)\n    h = sess.partial_run_setup([slow, cost, grad], [x])\n    # First derive slow\n    sess.partial_run(h, slow, feed_dict=feed)\n    # Then re-use slow multiple times (w/o recomputing it)\n    print(sess.partial_run(h, cost))\n    print(sess.partial_run(h, grad))\n\n# Naive / Inefficient \nwith tf.Session() as sess:\n    sess.run(init)\n    # Cost and grad each recompute 'slow' twice\n    print(sess.run(cost, feed_dict=feed))\n    print(sess.run(grad, feed_dict=feed))\n\nAnd, is there a way to see/check the computation steps it's doing?", "body": "`partial_run` is a very useful feature! \n\nJust to make sure I understand correctly: \n\nIn the following toy code, when I first derive `tmp` using `partial_run` and then compute the `cost` and `gradient`, is `tmp` only computed once and then reused multiple times?\n\n```\nimport tensorflow as tf\n\nw = tf.Variable(1.0)\ninit = tf.initialize_all_variables()\n\nx = tf.placeholder(\"float\")\nslow = tf.exp(w * x * tf.constant(2.0))\ncost = x * slow + x * slow\ngrad = tf.gradients(cost, w)[0]\nfeed = {x: 3.0}\n\n# Efficient\nwith tf.Session() as sess:\n    sess.run(init)\n    h = sess.partial_run_setup([slow, cost, grad], [x])\n    # First derive slow\n    sess.partial_run(h, slow, feed_dict=feed)\n    # Then re-use slow multiple times (w/o recomputing it)\n    print(sess.partial_run(h, cost))\n    print(sess.partial_run(h, grad))\n\n# Naive / Inefficient \nwith tf.Session() as sess:\n    sess.run(init)\n    # Cost and grad each recompute 'slow' twice\n    print(sess.run(cost, feed_dict=feed))\n    print(sess.run(grad, feed_dict=feed))\n```\n\nAnd, is there a way to see/check the computation steps it's doing?\n"}