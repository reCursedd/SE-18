{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439115946", "html_url": "https://github.com/tensorflow/tensorflow/issues/23767#issuecomment-439115946", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23767", "id": 439115946, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTExNTk0Ng==", "user": {"login": "brge17", "id": 33430930, "node_id": "MDQ6VXNlcjMzNDMwOTMw", "avatar_url": "https://avatars3.githubusercontent.com/u/33430930?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brge17", "html_url": "https://github.com/brge17", "followers_url": "https://api.github.com/users/brge17/followers", "following_url": "https://api.github.com/users/brge17/following{/other_user}", "gists_url": "https://api.github.com/users/brge17/gists{/gist_id}", "starred_url": "https://api.github.com/users/brge17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brge17/subscriptions", "organizations_url": "https://api.github.com/users/brge17/orgs", "repos_url": "https://api.github.com/users/brge17/repos", "events_url": "https://api.github.com/users/brge17/events{/privacy}", "received_events_url": "https://api.github.com/users/brge17/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-15T17:09:12Z", "updated_at": "2018-11-15T17:18:18Z", "author_association": "NONE", "body_html": "<p>I can also confirm this issue, looks like it was introduced in <code>Tensorflow==1.11.0</code></p>\n<p>The same issue effects class weights too:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.keras.layers <span class=\"pl-k\">import</span> Input, Dense\n<span class=\"pl-k\">from</span> tensorflow.python.keras.models <span class=\"pl-k\">import</span> Model\n\n<span class=\"pl-c1\">print</span> (tf.<span class=\"pl-c1\">__version__</span>)\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nnum_input <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\nnum_output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\npositive_weight <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_to_one_hot</span>(<span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">num_classes</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Compute one-hot encoding of an array. <span class=\"pl-pds\">\"\"\"</span></span>\n    y_one_hot <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">len</span>(y), num_classes)).astype(np.float32)\n    y_one_hot[np.arange(<span class=\"pl-c1\">len</span>(y)), y.squeeze()] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-k\">return</span> y_one_hot\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DummyGenerator</span>(<span class=\"pl-c1\">object</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">class_weights</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            X <span class=\"pl-k\">=</span> np.random.rand(batch_size, num_input)\n            y <span class=\"pl-k\">=</span> np.random.randint(num_output, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>batch_size)\n            y <span class=\"pl-k\">=</span> _to_one_hot(y, <span class=\"pl-c1\">2</span>)\n            <span class=\"pl-k\">yield</span> X, y\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_weights</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            X <span class=\"pl-k\">=</span> np.random.rand(batch_size, num_input)\n            y <span class=\"pl-k\">=</span> np.random.randint(num_output, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>batch_size)\n            w <span class=\"pl-k\">=</span> y <span class=\"pl-k\">*</span> (positive_weight <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n            y <span class=\"pl-k\">=</span> _to_one_hot(y, <span class=\"pl-c1\">2</span>)\n            <span class=\"pl-k\">yield</span> X, y, w\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">dummy_model</span>():\n    inputs <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(num_input,))\n    outputs <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax<span class=\"pl-pds\">\"</span></span>)(inputs)\n    model <span class=\"pl-k\">=</span> Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>inputs, <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>outputs)\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>categorical_crossentropy<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>adam<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">return</span> model\n\n\nmodel <span class=\"pl-k\">=</span> dummy_model()\ngen <span class=\"pl-k\">=</span> DummyGenerator()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Training with sample_weights<span class=\"pl-pds\">\"</span></span>)\nmodel.fit_generator(gen.sample_weights(), <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Training with class_weights<span class=\"pl-pds\">\"</span></span>)\nmodel.fit_generator(gen.class_weights(), <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">class_weight</span><span class=\"pl-k\">=</span>{<span class=\"pl-c1\">0</span>:<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>:positive_weight})</pre></div>", "body_text": "I can also confirm this issue, looks like it was introduced in Tensorflow==1.11.0\nThe same issue effects class weights too:\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras.layers import Input, Dense\nfrom tensorflow.python.keras.models import Model\n\nprint (tf.__version__)\nbatch_size = 10\nnum_input = 5\nnum_output = 2\npositive_weight = 10000\n\n\ndef _to_one_hot(y, num_classes):\n    \"\"\" Compute one-hot encoding of an array. \"\"\"\n    y_one_hot = np.zeros((len(y), num_classes)).astype(np.float32)\n    y_one_hot[np.arange(len(y)), y.squeeze()] = 1\n\n    return y_one_hot\n\n\nclass DummyGenerator(object):\n\n    def class_weights(self):\n        while True:\n            X = np.random.rand(batch_size, num_input)\n            y = np.random.randint(num_output, size=batch_size)\n            y = _to_one_hot(y, 2)\n            yield X, y\n\n    def sample_weights(self):\n        while True:\n            X = np.random.rand(batch_size, num_input)\n            y = np.random.randint(num_output, size=batch_size)\n            w = y * (positive_weight - 1) + 1\n            y = _to_one_hot(y, 2)\n            yield X, y, w\n\n\ndef dummy_model():\n    inputs = Input(shape=(num_input,))\n    outputs = Dense(2, activation=\"softmax\")(inputs)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n    return model\n\n\nmodel = dummy_model()\ngen = DummyGenerator()\n\nprint(\"Training with sample_weights\")\nmodel.fit_generator(gen.sample_weights(), steps_per_epoch=1, epochs=1)\n\nprint(\"Training with class_weights\")\nmodel.fit_generator(gen.class_weights(), steps_per_epoch=1, epochs=1, class_weight={0:1, 1:positive_weight})", "body": "I can also confirm this issue, looks like it was introduced in ``Tensorflow==1.11.0``\r\n\r\nThe same issue effects class weights too:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Model\r\n\r\nprint (tf.__version__)\r\nbatch_size = 10\r\nnum_input = 5\r\nnum_output = 2\r\npositive_weight = 10000\r\n\r\n\r\ndef _to_one_hot(y, num_classes):\r\n    \"\"\" Compute one-hot encoding of an array. \"\"\"\r\n    y_one_hot = np.zeros((len(y), num_classes)).astype(np.float32)\r\n    y_one_hot[np.arange(len(y)), y.squeeze()] = 1\r\n\r\n    return y_one_hot\r\n\r\n\r\nclass DummyGenerator(object):\r\n\r\n    def class_weights(self):\r\n        while True:\r\n            X = np.random.rand(batch_size, num_input)\r\n            y = np.random.randint(num_output, size=batch_size)\r\n            y = _to_one_hot(y, 2)\r\n            yield X, y\r\n\r\n    def sample_weights(self):\r\n        while True:\r\n            X = np.random.rand(batch_size, num_input)\r\n            y = np.random.randint(num_output, size=batch_size)\r\n            w = y * (positive_weight - 1) + 1\r\n            y = _to_one_hot(y, 2)\r\n            yield X, y, w\r\n\r\n\r\ndef dummy_model():\r\n    inputs = Input(shape=(num_input,))\r\n    outputs = Dense(2, activation=\"softmax\")(inputs)\r\n    model = Model(inputs=inputs, outputs=outputs)\r\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\r\n    return model\r\n\r\n\r\nmodel = dummy_model()\r\ngen = DummyGenerator()\r\n\r\nprint(\"Training with sample_weights\")\r\nmodel.fit_generator(gen.sample_weights(), steps_per_epoch=1, epochs=1)\r\n\r\nprint(\"Training with class_weights\")\r\nmodel.fit_generator(gen.class_weights(), steps_per_epoch=1, epochs=1, class_weight={0:1, 1:positive_weight})\r\n```"}