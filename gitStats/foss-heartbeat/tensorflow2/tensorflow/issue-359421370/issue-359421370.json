{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22234", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22234/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22234/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22234/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22234", "id": 359421370, "node_id": "MDU6SXNzdWUzNTk0MjEzNzA=", "number": 22234, "title": "[Bug] Keras TimeDistributed Concatenation", "user": {"login": "OneMoreSecond", "id": 16792948, "node_id": "MDQ6VXNlcjE2NzkyOTQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/16792948?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OneMoreSecond", "html_url": "https://github.com/OneMoreSecond", "followers_url": "https://api.github.com/users/OneMoreSecond/followers", "following_url": "https://api.github.com/users/OneMoreSecond/following{/other_user}", "gists_url": "https://api.github.com/users/OneMoreSecond/gists{/gist_id}", "starred_url": "https://api.github.com/users/OneMoreSecond/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OneMoreSecond/subscriptions", "organizations_url": "https://api.github.com/users/OneMoreSecond/orgs", "repos_url": "https://api.github.com/users/OneMoreSecond/repos", "events_url": "https://api.github.com/users/OneMoreSecond/events{/privacy}", "received_events_url": "https://api.github.com/users/OneMoreSecond/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-12T10:42:11Z", "updated_at": "2018-11-20T07:55:57Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Code</strong>:</li>\n</ul>\n<pre><code>import tensorflow.keras as k\nimport numpy as np\n\ninput1 = k.layers.Input(shape=(None, 10))\ninput2 = k.layers.Input(shape=(10,))\n\nconcat_layer = k.layers.Lambda(lambda x: k.layers.concatenate([x, input2]))\nresults = k.layers.TimeDistributed(concat_layer)(input1)\n\nmodel = k.models.Model([input1, input2], results)\n\ndata1 = np.zeros((3,7,10))\ndata2 = np.zeros((3,10))\n\nmodel.predict([data1,data2])\n</code></pre>\n<ul>\n<li><strong>OS Platform and Distribution</strong>: Windows server 2016</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 'v1.10.0-rc1-19-g656e7a2b34' 1.10.0</li>\n<li><strong>Python version</strong>: 3.6.6</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0, cuDNN 7.0</li>\n<li><strong>Have I written custom code</strong>: all the same as above code</li>\n<li><strong>Bazel version</strong> : N/A</li>\n<li><strong>GPU model and memory</strong>: Quadro P600, 2GiB</li>\n<li><strong>Exact command to reproduce</strong>: python test.py</li>\n<li><strong>Mobile device</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I try to run the code mentioned above.<br>\nMy goal is to concatenate input2 to each timestep of input1.<br>\nBut the log shows TF reshaped first two dimensions of input1 when concatenating.</p>\n<p>Line 235 in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/wrappers.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/wrappers.py</a> is wrong.<br>\nEven the batch size is None, the time-distributed layer's behavior may relate to actual batch size.</p>\n<pre><code>if input_shape[0]:\n      # batch size matters, use rnn-based implementation\n      ...................\nelse:\n      # No batch size specified, therefore the layer will be able\n      # to process batches of any size. (TRUE)\n      # We can go with reshape-based implementation for performance.\n      (FALSE! BEHAVIOR MAY RELATE TO ACTUAL BATCH SIZE)\n</code></pre>\n<h3>Source code / logs</h3>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1493, in predict<br>\nself, x, batch_size=batch_size, verbose=verbose, steps=steps)<br>\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 374, in predict_loop<br>\nbatch_outs = f(ins_batch)<br>\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 2914, in <strong>call</strong><br>\nfetched = self._callable_fn(*array_vals)<br>\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1382, in <strong>call</strong><br>\nrun_metadata_ptr)<br>\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in <strong>exit</strong><br>\nc_api.TF_GetCode(self.status.status))<br>\ntensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [21,10] vs. shape[1] = [3,10]<br>\n[[Node: time_distributed_2/concatenate_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_2/Reshape, _arg_input_3_0_1/_3, time_distributed_2/concatenate_1<br>\n/concat/axis)]]<br>\n[[Node: time_distributed_2/Reshape_1/_5 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_n<br>\name=\"edge_27_time_distributed_2/Reshape_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]</p>\n</blockquote>", "body_text": "System information\n\nCode:\n\nimport tensorflow.keras as k\nimport numpy as np\n\ninput1 = k.layers.Input(shape=(None, 10))\ninput2 = k.layers.Input(shape=(10,))\n\nconcat_layer = k.layers.Lambda(lambda x: k.layers.concatenate([x, input2]))\nresults = k.layers.TimeDistributed(concat_layer)(input1)\n\nmodel = k.models.Model([input1, input2], results)\n\ndata1 = np.zeros((3,7,10))\ndata2 = np.zeros((3,10))\n\nmodel.predict([data1,data2])\n\n\nOS Platform and Distribution: Windows server 2016\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\nPython version: 3.6.6\nCUDA/cuDNN version: CUDA 9.0, cuDNN 7.0\nHave I written custom code: all the same as above code\nBazel version : N/A\nGPU model and memory: Quadro P600, 2GiB\nExact command to reproduce: python test.py\nMobile device: N/A\n\nDescribe the problem\nI try to run the code mentioned above.\nMy goal is to concatenate input2 to each timestep of input1.\nBut the log shows TF reshaped first two dimensions of input1 when concatenating.\nLine 235 in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/wrappers.py is wrong.\nEven the batch size is None, the time-distributed layer's behavior may relate to actual batch size.\nif input_shape[0]:\n      # batch size matters, use rnn-based implementation\n      ...................\nelse:\n      # No batch size specified, therefore the layer will be able\n      # to process batches of any size. (TRUE)\n      # We can go with reshape-based implementation for performance.\n      (FALSE! BEHAVIOR MAY RELATE TO ACTUAL BATCH SIZE)\n\nSource code / logs\n\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1493, in predict\nself, x, batch_size=batch_size, verbose=verbose, steps=steps)\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 374, in predict_loop\nbatch_outs = f(ins_batch)\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 2914, in call\nfetched = self._callable_fn(*array_vals)\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1382, in call\nrun_metadata_ptr)\nFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [21,10] vs. shape[1] = [3,10]\n[[Node: time_distributed_2/concatenate_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_2/Reshape, _arg_input_3_0_1/_3, time_distributed_2/concatenate_1\n/concat/axis)]]\n[[Node: time_distributed_2/Reshape_1/_5 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_n\name=\"edge_27_time_distributed_2/Reshape_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]", "body": "### System information\r\n- **Code**:\r\n```\r\nimport tensorflow.keras as k\r\nimport numpy as np\r\n\r\ninput1 = k.layers.Input(shape=(None, 10))\r\ninput2 = k.layers.Input(shape=(10,))\r\n\r\nconcat_layer = k.layers.Lambda(lambda x: k.layers.concatenate([x, input2]))\r\nresults = k.layers.TimeDistributed(concat_layer)(input1)\r\n\r\nmodel = k.models.Model([input1, input2], results)\r\n\r\ndata1 = np.zeros((3,7,10))\r\ndata2 = np.zeros((3,10))\r\n\r\nmodel.predict([data1,data2])\r\n```\r\n- **OS Platform and Distribution**: Windows server 2016\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version**: 3.6.6\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0\r\n- **Have I written custom code**: all the same as above code\r\n- **Bazel version** : N/A\r\n- **GPU model and memory**: Quadro P600, 2GiB\r\n- **Exact command to reproduce**: python test.py\r\n- **Mobile device**: N/A\r\n\r\n### Describe the problem\r\nI try to run the code mentioned above.\r\nMy goal is to concatenate input2 to each timestep of input1.\r\nBut the log shows TF reshaped first two dimensions of input1 when concatenating.\r\n\r\nLine 235 in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/wrappers.py is wrong.\r\nEven the batch size is None, the time-distributed layer's behavior may relate to actual batch size.\r\n\r\n```\r\nif input_shape[0]:\r\n      # batch size matters, use rnn-based implementation\r\n      ...................\r\nelse:\r\n      # No batch size specified, therefore the layer will be able\r\n      # to process batches of any size. (TRUE)\r\n      # We can go with reshape-based implementation for performance.\r\n      (FALSE! BEHAVIOR MAY RELATE TO ACTUAL BATCH SIZE)\r\n```\r\n\r\n### Source code / logs\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1493, in predict\r\n>     self, x, batch_size=batch_size, verbose=verbose, steps=steps)\r\n>   File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 374, in predict_loop\r\n>     batch_outs = f(ins_batch)\r\n>   File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 2914, in __call__\r\n>     fetched = self._callable_fn(*array_vals)\r\n>   File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1382, in __call__\r\n>     run_metadata_ptr)\r\n>   File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n>     c_api.TF_GetCode(self.status.status))\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [21,10] vs. shape[1] = [3,10]\r\n>          [[Node: time_distributed_2/concatenate_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_2/Reshape, _arg_input_3_0_1/_3, time_distributed_2/concatenate_1\r\n> /concat/axis)]]\r\n>          [[Node: time_distributed_2/Reshape_1/_5 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_n\r\n> ame=\"edge_27_time_distributed_2/Reshape_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"}