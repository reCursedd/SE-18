{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/426903311", "html_url": "https://github.com/tensorflow/tensorflow/issues/22692#issuecomment-426903311", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22692", "id": 426903311, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjkwMzMxMQ==", "user": {"login": "kazemSafari", "id": 32110820, "node_id": "MDQ6VXNlcjMyMTEwODIw", "avatar_url": "https://avatars0.githubusercontent.com/u/32110820?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kazemSafari", "html_url": "https://github.com/kazemSafari", "followers_url": "https://api.github.com/users/kazemSafari/followers", "following_url": "https://api.github.com/users/kazemSafari/following{/other_user}", "gists_url": "https://api.github.com/users/kazemSafari/gists{/gist_id}", "starred_url": "https://api.github.com/users/kazemSafari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kazemSafari/subscriptions", "organizations_url": "https://api.github.com/users/kazemSafari/orgs", "repos_url": "https://api.github.com/users/kazemSafari/repos", "events_url": "https://api.github.com/users/kazemSafari/events{/privacy}", "received_events_url": "https://api.github.com/users/kazemSafari/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-04T06:43:47Z", "updated_at": "2018-10-04T06:46:49Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a> Thank you so much for your answer. I certainly can write my code with Keras. But based on what i have read on Tensorflows latest documentation in order to be able to use all_reduce I have to go back to Estimators, which is what i would prefer to avoid. I would greatly appreciate if you can help me find a way to tune the code in <strong><a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\">https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py</a></strong> so that I could use the all_reduce functionality as well even if it might add some complicated code.</p>\n<p>The Chinese link i shared above has the following code snippets:</p>\n<pre><code>def allreduce_grads(all_grads, average=True):\n    from tensorflow.contrib import nccl\n    nr_tower = len(all_grads)\n    if nr_tower == 1:\n        return all_grads\n    new_all_grads = []  # N x K\n    for grads in zip(*all_grads):\n        summed = nccl.all_sum(grads)\n\n        grads_for_devices = []  # K\n        for g in summed:\n            with tf.device(g.device):\n                # tensorflow/benchmarks didn't average gradients\n               if average:\n                   g = tf.multiply(g, 1.0 / nr_tower, name='allreduce_avg')\n            grads_for_devices.append(g)\n        new_all_grads.append(grads_for_devices)\n\n    # transpose to K x N\n    ret = list(zip(*new_all_grads))\n    return ret](url)\n</code></pre>", "body_text": "@tfboyd Thank you so much for your answer. I certainly can write my code with Keras. But based on what i have read on Tensorflows latest documentation in order to be able to use all_reduce I have to go back to Estimators, which is what i would prefer to avoid. I would greatly appreciate if you can help me find a way to tune the code in https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py so that I could use the all_reduce functionality as well even if it might add some complicated code.\nThe Chinese link i shared above has the following code snippets:\ndef allreduce_grads(all_grads, average=True):\n    from tensorflow.contrib import nccl\n    nr_tower = len(all_grads)\n    if nr_tower == 1:\n        return all_grads\n    new_all_grads = []  # N x K\n    for grads in zip(*all_grads):\n        summed = nccl.all_sum(grads)\n\n        grads_for_devices = []  # K\n        for g in summed:\n            with tf.device(g.device):\n                # tensorflow/benchmarks didn't average gradients\n               if average:\n                   g = tf.multiply(g, 1.0 / nr_tower, name='allreduce_avg')\n            grads_for_devices.append(g)\n        new_all_grads.append(grads_for_devices)\n\n    # transpose to K x N\n    ret = list(zip(*new_all_grads))\n    return ret](url)", "body": "@tfboyd Thank you so much for your answer. I certainly can write my code with Keras. But based on what i have read on Tensorflows latest documentation in order to be able to use all_reduce I have to go back to Estimators, which is what i would prefer to avoid. I would greatly appreciate if you can help me find a way to tune the code in **https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py** so that I could use the all_reduce functionality as well even if it might add some complicated code.\r\n\r\nThe Chinese link i shared above has the following code snippets:\r\n\r\n    def allreduce_grads(all_grads, average=True):\r\n        from tensorflow.contrib import nccl\r\n        nr_tower = len(all_grads)\r\n        if nr_tower == 1:\r\n            return all_grads\r\n        new_all_grads = []  # N x K\r\n        for grads in zip(*all_grads):\r\n            summed = nccl.all_sum(grads)\r\n\r\n            grads_for_devices = []  # K\r\n            for g in summed:\r\n                with tf.device(g.device):\r\n                    # tensorflow/benchmarks didn't average gradients\r\n                   if average:\r\n                       g = tf.multiply(g, 1.0 / nr_tower, name='allreduce_avg')\r\n                grads_for_devices.append(g)\r\n            new_all_grads.append(grads_for_devices)\r\n\r\n        # transpose to K x N\r\n        ret = list(zip(*new_all_grads))\r\n        return ret](url)"}