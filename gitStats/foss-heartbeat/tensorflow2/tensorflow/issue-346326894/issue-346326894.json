{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21287", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21287/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21287/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21287/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21287", "id": 346326894, "node_id": "MDU6SXNzdWUzNDYzMjY4OTQ=", "number": 21287, "title": "Possible bug in dynamic_rnn when training on TPU for iterations_per_loop > 1", "user": {"login": "mrezak", "id": 4903456, "node_id": "MDQ6VXNlcjQ5MDM0NTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/4903456?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrezak", "html_url": "https://github.com/mrezak", "followers_url": "https://api.github.com/users/mrezak/followers", "following_url": "https://api.github.com/users/mrezak/following{/other_user}", "gists_url": "https://api.github.com/users/mrezak/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrezak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrezak/subscriptions", "organizations_url": "https://api.github.com/users/mrezak/orgs", "repos_url": "https://api.github.com/users/mrezak/repos", "events_url": "https://api.github.com/users/mrezak/events{/privacy}", "received_events_url": "https://api.github.com/users/mrezak/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-07-31T19:29:42Z", "updated_at": "2018-08-18T19:26:54Z", "closed_at": "2018-08-18T18:48:31Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Google Cloud Platform (Linux Debian)</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: NA</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA (TPU training)</li>\n<li><strong>GPU model and memory</strong>: NA (TPU training)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Training an RNN (constructed with <code>dynamic_rnn</code>) on TPU gives largely different loss values for <code>iterations_per_loop=1</code> and <code>iterations_per_loop=100</code>. The loss when training on TPU with <code>iterations_per_loop=1</code> is very close to the loss when training on CPU, but the loss for <code>iterations_per_loop=100</code> case is orders of magnitude different.</p>\n<p>See below for the code to reproduce this issue. I also tested it with BasicRNNCell (instead of GRU) and observed the same issue. For easier debugging, I have made the runs deterministic (all the random ops are seeded, repeated runs produce the exact same values).</p>\n<p>Note that if I replace my model_fn with a simple linear model containing only matrix multiplication (instead of <code>dynamic_rnn</code>) the loss for any value of <code>iterations_per_loop</code> will be the same which is as expected. So I suspect there is a bug in using <code>dynamic_rnn</code> with TPU.</p>\n<h3>Source code / logs</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator\nimport subprocess\nimport os\n\nSEED = 10010\n\nUSE_TPU = True\n\ndef make_data(params):\n    # make training and validation data: sinusoids with random phases\n    np.random.seed(SEED)\n    num_samp_tr = 1000\n    num_samp_val = 1000\n    ramps_tr = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_tr, params['dims'], 100)), (0, 2, 1))\n    rand_phase = np.transpose(np.tile(np.random.randn(num_samp_tr, params['dims']), (100,1,1)), (1, 0, 2))\n    ramps_val = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_val, params['dims'], 100)), (0, 2, 1))\n    rand_phase_val = np.transpose(np.tile(np.random.randn(num_samp_val, params['dims']), (100,1,1)), (1, 0, 2))\n    data = {'train_data': np.sin(ramps_tr + rand_phase),\n            'valid_data': np.sin(ramps_val + rand_phase_val)}\n    return data\n\ndef input_fn(data_dict, mode):\n    def data_fn(params):\n        batch_size = params['batch_size']\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32)).cache().repeat().shuffle(buffer_size=10000, seed=SEED)\n        else:\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32)).cache().repeat()\n        dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n        return dataset\n    return data_fn\n\ndef model_fn(features, mode, params):\n    #tf.set_random_seed(SEED) # Use for BasicRNNCell, it does not get an initializer\n    batch_size=params['batch_size']\n    ts = features.get_shape().as_list()[1]\n    seq_len = ts * np.ones([batch_size,])\n    with tf.variable_scope('encoder'):\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\n        _, output_latent = tf.nn.dynamic_rnn(cell=cell, inputs=features, sequence_length=seq_len, dtype=tf.float32)\n    with tf.variable_scope('decoder'):\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\n        z_inps = tf.zeros([batch_size, ts, 1])\n        output_recon, _ = tf.nn.dynamic_rnn(cell=cell, inputs=z_inps, initial_state=output_latent, sequence_length=seq_len, dtype=tf.float32)    \n\n    winit = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\n    output_recon = tf.contrib.layers.fully_connected(inputs=output_recon, num_outputs=params['dims'], activation_fn=None, weights_initializer=winit)\n    loss = tf.losses.mean_squared_error(features, output_recon)\n\n    global_step = tf.train.get_global_step()\n    opt = tf.train.AdamOptimizer(0.01)\n    if USE_TPU:\n        opt = tf.contrib.tpu.CrossShardOptimizer(opt)\n    train_op = opt.minimize(loss, global_step)\n    def metric_fn(labels, rec):\n        return {\n        'MSE': tf.metrics.mean_squared_error(labels, rec),\n        }\n    tpu_eval_metrics = (metric_fn, [features, output_recon])\n\n    return tpu_estimator.TPUEstimatorSpec(mode=mode,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metrics=tpu_eval_metrics,\n                                      )\n\ndef train_model(num_steps, iterations_per_loop, num_shards=1):\n    if USE_TPU:\n        my_project = subprocess.check_output([\n            'gcloud','config','get-value','project'])\n        my_zone = subprocess.check_output([\n            'gcloud','config','get-value','compute/zone'])\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n                tpu=[os.environ['TPU_NAME']],\n                )\n        tpu_cluster_resolver = tpu_cluster_resolver\n    else:\n        tpu_cluster_resolver = None\n\n    #tf.logging.set_verbosity(tf.logging.INFO)\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            log_device_placement=True)\n\n    run_config = tf.contrib.tpu.RunConfig(\n        save_checkpoints_steps=400,\n        cluster=tpu_cluster_resolver,\n        keep_checkpoint_max=1,\n        model_dir = 'gs://test-bucket/runs',\n        session_config=config,\n        tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=iterations_per_loop, num_shards=num_shards))\n    \n    params = {'dims': 5}\n    data = make_data(params)\n\n    train_input = input_fn(data, tf.estimator.ModeKeys.TRAIN)\n    eval_input = input_fn(data, tf.estimator.ModeKeys.EVAL)\n\n    model = tf.contrib.tpu.TPUEstimator(model_fn=model_fn, params=params, config=run_config,\n        use_tpu=USE_TPU, train_batch_size=100, eval_batch_size=100)\n    model.train(train_input, steps=num_steps)\n    \n    valid_costs = model.evaluate(eval_input, name='valid_data', steps=2)\n    print('==== Evaluation:')\n    print(valid_costs)\n    return valid_costs\n\n\n\nprint(\"==================== Training with iterations_per_loop = 1\")\nrun1 = train_model(num_steps=100, iterations_per_loop=1, num_shards=1)\n\n# remove checkpoints\nsubprocess.call(\"gsutil -m rm -r gs://test-bucket/runs/*\", shell=True)\n\nprint(\"==================== Training with iterations_per_loop = 100\")\nrun2 = train_model(num_steps=100, iterations_per_loop=100, num_shards=1)\n\nprint('Summary:')\nprint('====== iterations_per_loop = 1 :')\nprint(run1)\n\nprint('====== iterations_per_loop = 100 :')\nprint(run2)\n</code></pre>\n<h3>Output (multiple runs):</h3>\n<p><strong>CPU Run:</strong><br>\n<code>{'loss': 0.2408253, 'MSE': 0.24082531, 'global_step': 100}</code></p>\n<p><strong>TPU Runs:</strong><br>\niterations_per_loop=1<br>\nRun1:<br>\n<code>{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}</code><br>\nRun2:<br>\n<code>{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}</code></p>\n<p>iterations_per_loop=100<br>\nRun1:<br>\n<code>{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}</code><br>\nRun2:<br>\n<code>{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}</code></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform (Linux Debian)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.9\nPython version: 2.7\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA (TPU training)\nGPU model and memory: NA (TPU training)\nExact command to reproduce:\n\nDescribe the problem\nTraining an RNN (constructed with dynamic_rnn) on TPU gives largely different loss values for iterations_per_loop=1 and iterations_per_loop=100. The loss when training on TPU with iterations_per_loop=1 is very close to the loss when training on CPU, but the loss for iterations_per_loop=100 case is orders of magnitude different.\nSee below for the code to reproduce this issue. I also tested it with BasicRNNCell (instead of GRU) and observed the same issue. For easier debugging, I have made the runs deterministic (all the random ops are seeded, repeated runs produce the exact same values).\nNote that if I replace my model_fn with a simple linear model containing only matrix multiplication (instead of dynamic_rnn) the loss for any value of iterations_per_loop will be the same which is as expected. So I suspect there is a bug in using dynamic_rnn with TPU.\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator\nimport subprocess\nimport os\n\nSEED = 10010\n\nUSE_TPU = True\n\ndef make_data(params):\n    # make training and validation data: sinusoids with random phases\n    np.random.seed(SEED)\n    num_samp_tr = 1000\n    num_samp_val = 1000\n    ramps_tr = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_tr, params['dims'], 100)), (0, 2, 1))\n    rand_phase = np.transpose(np.tile(np.random.randn(num_samp_tr, params['dims']), (100,1,1)), (1, 0, 2))\n    ramps_val = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_val, params['dims'], 100)), (0, 2, 1))\n    rand_phase_val = np.transpose(np.tile(np.random.randn(num_samp_val, params['dims']), (100,1,1)), (1, 0, 2))\n    data = {'train_data': np.sin(ramps_tr + rand_phase),\n            'valid_data': np.sin(ramps_val + rand_phase_val)}\n    return data\n\ndef input_fn(data_dict, mode):\n    def data_fn(params):\n        batch_size = params['batch_size']\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32)).cache().repeat().shuffle(buffer_size=10000, seed=SEED)\n        else:\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32)).cache().repeat()\n        dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n        return dataset\n    return data_fn\n\ndef model_fn(features, mode, params):\n    #tf.set_random_seed(SEED) # Use for BasicRNNCell, it does not get an initializer\n    batch_size=params['batch_size']\n    ts = features.get_shape().as_list()[1]\n    seq_len = ts * np.ones([batch_size,])\n    with tf.variable_scope('encoder'):\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\n        _, output_latent = tf.nn.dynamic_rnn(cell=cell, inputs=features, sequence_length=seq_len, dtype=tf.float32)\n    with tf.variable_scope('decoder'):\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\n        z_inps = tf.zeros([batch_size, ts, 1])\n        output_recon, _ = tf.nn.dynamic_rnn(cell=cell, inputs=z_inps, initial_state=output_latent, sequence_length=seq_len, dtype=tf.float32)    \n\n    winit = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\n    output_recon = tf.contrib.layers.fully_connected(inputs=output_recon, num_outputs=params['dims'], activation_fn=None, weights_initializer=winit)\n    loss = tf.losses.mean_squared_error(features, output_recon)\n\n    global_step = tf.train.get_global_step()\n    opt = tf.train.AdamOptimizer(0.01)\n    if USE_TPU:\n        opt = tf.contrib.tpu.CrossShardOptimizer(opt)\n    train_op = opt.minimize(loss, global_step)\n    def metric_fn(labels, rec):\n        return {\n        'MSE': tf.metrics.mean_squared_error(labels, rec),\n        }\n    tpu_eval_metrics = (metric_fn, [features, output_recon])\n\n    return tpu_estimator.TPUEstimatorSpec(mode=mode,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metrics=tpu_eval_metrics,\n                                      )\n\ndef train_model(num_steps, iterations_per_loop, num_shards=1):\n    if USE_TPU:\n        my_project = subprocess.check_output([\n            'gcloud','config','get-value','project'])\n        my_zone = subprocess.check_output([\n            'gcloud','config','get-value','compute/zone'])\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n                tpu=[os.environ['TPU_NAME']],\n                )\n        tpu_cluster_resolver = tpu_cluster_resolver\n    else:\n        tpu_cluster_resolver = None\n\n    #tf.logging.set_verbosity(tf.logging.INFO)\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            log_device_placement=True)\n\n    run_config = tf.contrib.tpu.RunConfig(\n        save_checkpoints_steps=400,\n        cluster=tpu_cluster_resolver,\n        keep_checkpoint_max=1,\n        model_dir = 'gs://test-bucket/runs',\n        session_config=config,\n        tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=iterations_per_loop, num_shards=num_shards))\n    \n    params = {'dims': 5}\n    data = make_data(params)\n\n    train_input = input_fn(data, tf.estimator.ModeKeys.TRAIN)\n    eval_input = input_fn(data, tf.estimator.ModeKeys.EVAL)\n\n    model = tf.contrib.tpu.TPUEstimator(model_fn=model_fn, params=params, config=run_config,\n        use_tpu=USE_TPU, train_batch_size=100, eval_batch_size=100)\n    model.train(train_input, steps=num_steps)\n    \n    valid_costs = model.evaluate(eval_input, name='valid_data', steps=2)\n    print('==== Evaluation:')\n    print(valid_costs)\n    return valid_costs\n\n\n\nprint(\"==================== Training with iterations_per_loop = 1\")\nrun1 = train_model(num_steps=100, iterations_per_loop=1, num_shards=1)\n\n# remove checkpoints\nsubprocess.call(\"gsutil -m rm -r gs://test-bucket/runs/*\", shell=True)\n\nprint(\"==================== Training with iterations_per_loop = 100\")\nrun2 = train_model(num_steps=100, iterations_per_loop=100, num_shards=1)\n\nprint('Summary:')\nprint('====== iterations_per_loop = 1 :')\nprint(run1)\n\nprint('====== iterations_per_loop = 100 :')\nprint(run2)\n\nOutput (multiple runs):\nCPU Run:\n{'loss': 0.2408253, 'MSE': 0.24082531, 'global_step': 100}\nTPU Runs:\niterations_per_loop=1\nRun1:\n{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}\nRun2:\n{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}\niterations_per_loop=100\nRun1:\n{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}\nRun2:\n{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Cloud Platform (Linux Debian)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA (TPU training)\r\n- **GPU model and memory**: NA (TPU training)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nTraining an RNN (constructed with `dynamic_rnn`) on TPU gives largely different loss values for `iterations_per_loop=1` and `iterations_per_loop=100`. The loss when training on TPU with `iterations_per_loop=1` is very close to the loss when training on CPU, but the loss for `iterations_per_loop=100` case is orders of magnitude different.\r\n\r\nSee below for the code to reproduce this issue. I also tested it with BasicRNNCell (instead of GRU) and observed the same issue. For easier debugging, I have made the runs deterministic (all the random ops are seeded, repeated runs produce the exact same values).\r\n\r\nNote that if I replace my model_fn with a simple linear model containing only matrix multiplication (instead of `dynamic_rnn`) the loss for any value of `iterations_per_loop` will be the same which is as expected. So I suspect there is a bug in using `dynamic_rnn` with TPU. \r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator\r\nimport subprocess\r\nimport os\r\n\r\nSEED = 10010\r\n\r\nUSE_TPU = True\r\n\r\ndef make_data(params):\r\n    # make training and validation data: sinusoids with random phases\r\n    np.random.seed(SEED)\r\n    num_samp_tr = 1000\r\n    num_samp_val = 1000\r\n    ramps_tr = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_tr, params['dims'], 100)), (0, 2, 1))\r\n    rand_phase = np.transpose(np.tile(np.random.randn(num_samp_tr, params['dims']), (100,1,1)), (1, 0, 2))\r\n    ramps_val = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_val, params['dims'], 100)), (0, 2, 1))\r\n    rand_phase_val = np.transpose(np.tile(np.random.randn(num_samp_val, params['dims']), (100,1,1)), (1, 0, 2))\r\n    data = {'train_data': np.sin(ramps_tr + rand_phase),\r\n            'valid_data': np.sin(ramps_val + rand_phase_val)}\r\n    return data\r\n\r\ndef input_fn(data_dict, mode):\r\n    def data_fn(params):\r\n        batch_size = params['batch_size']\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32)).cache().repeat().shuffle(buffer_size=10000, seed=SEED)\r\n        else:\r\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32)).cache().repeat()\r\n        dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n        return dataset\r\n    return data_fn\r\n\r\ndef model_fn(features, mode, params):\r\n    #tf.set_random_seed(SEED) # Use for BasicRNNCell, it does not get an initializer\r\n    batch_size=params['batch_size']\r\n    ts = features.get_shape().as_list()[1]\r\n    seq_len = ts * np.ones([batch_size,])\r\n    with tf.variable_scope('encoder'):\r\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\r\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\r\n        _, output_latent = tf.nn.dynamic_rnn(cell=cell, inputs=features, sequence_length=seq_len, dtype=tf.float32)\r\n    with tf.variable_scope('decoder'):\r\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\r\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\r\n        z_inps = tf.zeros([batch_size, ts, 1])\r\n        output_recon, _ = tf.nn.dynamic_rnn(cell=cell, inputs=z_inps, initial_state=output_latent, sequence_length=seq_len, dtype=tf.float32)    \r\n\r\n    winit = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n    output_recon = tf.contrib.layers.fully_connected(inputs=output_recon, num_outputs=params['dims'], activation_fn=None, weights_initializer=winit)\r\n    loss = tf.losses.mean_squared_error(features, output_recon)\r\n\r\n    global_step = tf.train.get_global_step()\r\n    opt = tf.train.AdamOptimizer(0.01)\r\n    if USE_TPU:\r\n        opt = tf.contrib.tpu.CrossShardOptimizer(opt)\r\n    train_op = opt.minimize(loss, global_step)\r\n    def metric_fn(labels, rec):\r\n        return {\r\n        'MSE': tf.metrics.mean_squared_error(labels, rec),\r\n        }\r\n    tpu_eval_metrics = (metric_fn, [features, output_recon])\r\n\r\n    return tpu_estimator.TPUEstimatorSpec(mode=mode,\r\n                                      loss=loss,\r\n                                      train_op=train_op,\r\n                                      eval_metrics=tpu_eval_metrics,\r\n                                      )\r\n\r\ndef train_model(num_steps, iterations_per_loop, num_shards=1):\r\n    if USE_TPU:\r\n        my_project = subprocess.check_output([\r\n            'gcloud','config','get-value','project'])\r\n        my_zone = subprocess.check_output([\r\n            'gcloud','config','get-value','compute/zone'])\r\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\r\n                tpu=[os.environ['TPU_NAME']],\r\n                )\r\n        tpu_cluster_resolver = tpu_cluster_resolver\r\n    else:\r\n        tpu_cluster_resolver = None\r\n\r\n    #tf.logging.set_verbosity(tf.logging.INFO)\r\n    config = tf.ConfigProto(allow_soft_placement=True,\r\n                            log_device_placement=True)\r\n\r\n    run_config = tf.contrib.tpu.RunConfig(\r\n        save_checkpoints_steps=400,\r\n        cluster=tpu_cluster_resolver,\r\n        keep_checkpoint_max=1,\r\n        model_dir = 'gs://test-bucket/runs',\r\n        session_config=config,\r\n        tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=iterations_per_loop, num_shards=num_shards))\r\n    \r\n    params = {'dims': 5}\r\n    data = make_data(params)\r\n\r\n    train_input = input_fn(data, tf.estimator.ModeKeys.TRAIN)\r\n    eval_input = input_fn(data, tf.estimator.ModeKeys.EVAL)\r\n\r\n    model = tf.contrib.tpu.TPUEstimator(model_fn=model_fn, params=params, config=run_config,\r\n        use_tpu=USE_TPU, train_batch_size=100, eval_batch_size=100)\r\n    model.train(train_input, steps=num_steps)\r\n    \r\n    valid_costs = model.evaluate(eval_input, name='valid_data', steps=2)\r\n    print('==== Evaluation:')\r\n    print(valid_costs)\r\n    return valid_costs\r\n\r\n\r\n\r\nprint(\"==================== Training with iterations_per_loop = 1\")\r\nrun1 = train_model(num_steps=100, iterations_per_loop=1, num_shards=1)\r\n\r\n# remove checkpoints\r\nsubprocess.call(\"gsutil -m rm -r gs://test-bucket/runs/*\", shell=True)\r\n\r\nprint(\"==================== Training with iterations_per_loop = 100\")\r\nrun2 = train_model(num_steps=100, iterations_per_loop=100, num_shards=1)\r\n\r\nprint('Summary:')\r\nprint('====== iterations_per_loop = 1 :')\r\nprint(run1)\r\n\r\nprint('====== iterations_per_loop = 100 :')\r\nprint(run2)\r\n```\r\n\r\n### Output (multiple runs):\r\n**CPU Run:**\r\n```{'loss': 0.2408253, 'MSE': 0.24082531, 'global_step': 100}```\r\n\r\n**TPU Runs:**\r\niterations_per_loop=1\r\nRun1:\r\n```{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}```\r\nRun2:\r\n```{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}```\r\n\r\niterations_per_loop=100\r\nRun1:\r\n```{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}```\r\nRun2:\r\n```{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}```"}