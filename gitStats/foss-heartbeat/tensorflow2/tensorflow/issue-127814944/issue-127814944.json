{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/827", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/827/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/827/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/827/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/827", "id": 127814944, "node_id": "MDU6SXNzdWUxMjc4MTQ5NDQ=", "number": 827, "title": "Making a composite assignment + optimization op", "user": {"login": "joshburkart", "id": 3888181, "node_id": "MDQ6VXNlcjM4ODgxODE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3888181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joshburkart", "html_url": "https://github.com/joshburkart", "followers_url": "https://api.github.com/users/joshburkart/followers", "following_url": "https://api.github.com/users/joshburkart/following{/other_user}", "gists_url": "https://api.github.com/users/joshburkart/gists{/gist_id}", "starred_url": "https://api.github.com/users/joshburkart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joshburkart/subscriptions", "organizations_url": "https://api.github.com/users/joshburkart/orgs", "repos_url": "https://api.github.com/users/joshburkart/repos", "events_url": "https://api.github.com/users/joshburkart/events{/privacy}", "received_events_url": "https://api.github.com/users/joshburkart/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "dave-andersen", "id": 827870, "node_id": "MDQ6VXNlcjgyNzg3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/827870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dave-andersen", "html_url": "https://github.com/dave-andersen", "followers_url": "https://api.github.com/users/dave-andersen/followers", "following_url": "https://api.github.com/users/dave-andersen/following{/other_user}", "gists_url": "https://api.github.com/users/dave-andersen/gists{/gist_id}", "starred_url": "https://api.github.com/users/dave-andersen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dave-andersen/subscriptions", "organizations_url": "https://api.github.com/users/dave-andersen/orgs", "repos_url": "https://api.github.com/users/dave-andersen/repos", "events_url": "https://api.github.com/users/dave-andersen/events{/privacy}", "received_events_url": "https://api.github.com/users/dave-andersen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "dave-andersen", "id": 827870, "node_id": "MDQ6VXNlcjgyNzg3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/827870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dave-andersen", "html_url": "https://github.com/dave-andersen", "followers_url": "https://api.github.com/users/dave-andersen/followers", "following_url": "https://api.github.com/users/dave-andersen/following{/other_user}", "gists_url": "https://api.github.com/users/dave-andersen/gists{/gist_id}", "starred_url": "https://api.github.com/users/dave-andersen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dave-andersen/subscriptions", "organizations_url": "https://api.github.com/users/dave-andersen/orgs", "repos_url": "https://api.github.com/users/dave-andersen/repos", "events_url": "https://api.github.com/users/dave-andersen/events{/privacy}", "received_events_url": "https://api.github.com/users/dave-andersen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-01-21T00:06:23Z", "updated_at": "2016-01-22T22:03:03Z", "closed_at": "2016-01-22T22:03:03Z", "author_association": "NONE", "body_html": "<p>I want to make an op that, when run, changes a <code>Variable</code>'s value and then updates it by optimizing some dependent scalar <code>Tensor</code>.</p>\n<p>I came up with this, which runs successfully:</p>\n<div class=\"highlight highlight-source-python\"><pre>v <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">3</span>]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>v<span class=\"pl-pds\">'</span></span>)\nv_input <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>v_input<span class=\"pl-pds\">'</span></span>)\nloss <span class=\"pl-k\">=</span> tf.reduce_sum(tf.square(v))\n\n<span class=\"pl-k\">with</span> tf.control_dependencies([v.assign(v_input)]):\n  op <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>).minimize(loss)\n\nsession <span class=\"pl-k\">=</span> tf.Session()\nsession.run(tf.initialize_all_variables())\nsession.run(op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{v_input: [<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">3</span>.]})</pre></div>\n<p>However, if I change <code>GradientDescentOptimizer</code> to <code>AdagradOptimizer</code> (or <code>RMSProp</code>, <code>Adam</code>, ...), it fails in the variable-initialization (second-to-last) line, complaining that <code>v_input</code> must be fed:</p>\n<pre><code>W tensorflow/core/common_runtime/executor.cc:1076] 0x15cb160 Compute status: Invalid argument: You must feed a value for placeholder tensor 'v_input' with dtype float and shape dim { size: 3 }\n</code></pre>\n<p>Obviously to actually execute the training op I need to feed in a value for <code>v_input</code> due to the control dependency. But just to initialize variables? I don't understand why I would need to feed anything in...</p>\n<p>I guess since <code>AdagradOptimizer</code> creates its own <code>Variable</code>s (accumulated gradient) when you call <code>minimize</code>? I really only want the optimizer's update step to be dependent on the assignment op I create, but I'm not sure how to do that. Any help would be appreciated!</p>", "body_text": "I want to make an op that, when run, changes a Variable's value and then updates it by optimizing some dependent scalar Tensor.\nI came up with this, which runs successfully:\nv = tf.Variable(tf.zeros([3]), name='v')\nv_input = tf.placeholder(tf.float32, [3], name='v_input')\nloss = tf.reduce_sum(tf.square(v))\n\nwith tf.control_dependencies([v.assign(v_input)]):\n  op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n\nsession = tf.Session()\nsession.run(tf.initialize_all_variables())\nsession.run(op, feed_dict={v_input: [1., 2., 3.]})\nHowever, if I change GradientDescentOptimizer to AdagradOptimizer (or RMSProp, Adam, ...), it fails in the variable-initialization (second-to-last) line, complaining that v_input must be fed:\nW tensorflow/core/common_runtime/executor.cc:1076] 0x15cb160 Compute status: Invalid argument: You must feed a value for placeholder tensor 'v_input' with dtype float and shape dim { size: 3 }\n\nObviously to actually execute the training op I need to feed in a value for v_input due to the control dependency. But just to initialize variables? I don't understand why I would need to feed anything in...\nI guess since AdagradOptimizer creates its own Variables (accumulated gradient) when you call minimize? I really only want the optimizer's update step to be dependent on the assignment op I create, but I'm not sure how to do that. Any help would be appreciated!", "body": "I want to make an op that, when run, changes a `Variable`'s value and then updates it by optimizing some dependent scalar `Tensor`.\n\nI came up with this, which runs successfully:\n\n``` python\nv = tf.Variable(tf.zeros([3]), name='v')\nv_input = tf.placeholder(tf.float32, [3], name='v_input')\nloss = tf.reduce_sum(tf.square(v))\n\nwith tf.control_dependencies([v.assign(v_input)]):\n  op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n\nsession = tf.Session()\nsession.run(tf.initialize_all_variables())\nsession.run(op, feed_dict={v_input: [1., 2., 3.]})\n```\n\nHowever, if I change `GradientDescentOptimizer` to `AdagradOptimizer` (or `RMSProp`, `Adam`, ...), it fails in the variable-initialization (second-to-last) line, complaining that `v_input` must be fed:\n\n```\nW tensorflow/core/common_runtime/executor.cc:1076] 0x15cb160 Compute status: Invalid argument: You must feed a value for placeholder tensor 'v_input' with dtype float and shape dim { size: 3 }\n```\n\nObviously to actually execute the training op I need to feed in a value for `v_input` due to the control dependency. But just to initialize variables? I don't understand why I would need to feed anything in...\n\nI guess since `AdagradOptimizer` creates its own `Variable`s (accumulated gradient) when you call `minimize`? I really only want the optimizer's update step to be dependent on the assignment op I create, but I'm not sure how to do that. Any help would be appreciated!\n"}