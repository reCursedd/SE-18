{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336001126", "html_url": "https://github.com/tensorflow/tensorflow/issues/13500#issuecomment-336001126", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13500", "id": 336001126, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjAwMTEyNg==", "user": {"login": "MartinZZZ", "id": 12166108, "node_id": "MDQ6VXNlcjEyMTY2MTA4", "avatar_url": "https://avatars0.githubusercontent.com/u/12166108?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MartinZZZ", "html_url": "https://github.com/MartinZZZ", "followers_url": "https://api.github.com/users/MartinZZZ/followers", "following_url": "https://api.github.com/users/MartinZZZ/following{/other_user}", "gists_url": "https://api.github.com/users/MartinZZZ/gists{/gist_id}", "starred_url": "https://api.github.com/users/MartinZZZ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MartinZZZ/subscriptions", "organizations_url": "https://api.github.com/users/MartinZZZ/orgs", "repos_url": "https://api.github.com/users/MartinZZZ/repos", "events_url": "https://api.github.com/users/MartinZZZ/events{/privacy}", "received_events_url": "https://api.github.com/users/MartinZZZ/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-12T02:20:31Z", "updated_at": "2017-10-12T02:20:31Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5453737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tatatodd\">@tatatodd</a> Thanks for the information. I specified those flags in the <code>tf_library</code>, but the log still exists:</p>\n<p>The <code>tfcompile_flags</code> is specified as following (in //tensorflow/tensorflow/compiler/aot/tfcompile.bzl):</p>\n<pre><code># -*- Python -*-\n\nload(\"//tensorflow:tensorflow.bzl\", \"if_android\", \"tf_copts\")\n\ndef tf_library(name, graph, config,\n               freeze_checkpoint=None, freeze_saver=None,\n               cpp_class=None, gen_test=True, gen_benchmark=True,\n               visibility=None, testonly=None,\n               tfcompile_flags=str('--target_cpu=\"x86-64\" --target_features=\"+sse4.1\" --target_features=\"+sse4.2\" --target_features=\"+avx\" --target_features=\"+avx2\" --target_features=\"+fma\"'),\n               #tfcompile_flags=None,\n               tfcompile_tool=\"//tensorflow/compiler/aot:tfcompile\",\n               include_standard_runtime_deps=True, deps=None, tags=None):\n \n  if not cpp_class:\n    fail(\"cpp_class must be specified\")\n\n  tfcompile_graph = graph\n  if freeze_checkpoint or freeze_saver:\n    if not freeze_checkpoint:\n      fail(\"freeze_checkpoint must be specified when freeze_saver is specified\")\n\n    freeze_name = \"freeze_\" + name\n    freeze_file = freeze_name + \".pb\"\n\n    # First run tfcompile to generate the list of out_nodes.\n    out_nodes_file = \"out_nodes_\" + freeze_name\n    native.genrule(\n        name=(\"gen_\" + out_nodes_file),\n        srcs=[config],\n        outs=[out_nodes_file],\n        cmd=(\"$(location \" + tfcompile_tool + \")\" +\n             \" --config=$(location \" + config + \")\" +\n             \" --dump_fetch_nodes &gt; $@\"),\n        tools=[tfcompile_tool],\n        # Run tfcompile on the build host, rather than forge, since it's\n        # typically way faster on the local machine.\n        local=1,\n        tags=tags,\n    )\n\n    # Now run freeze_graph to convert variables into constants.\n    freeze_args = (\" --input_graph=$(location \" + graph + \")\" +\n                   \" --input_binary=\" + str(not graph.endswith(\".pbtxt\")) +\n                   \" --input_checkpoint=$(location \" + freeze_checkpoint + \")\" +\n                   \" --output_graph=$(location \" + freeze_file + \")\" +\n                   \" --output_node_names=$$(&lt;$(location \" + out_nodes_file +\n                   \"))\")\n    freeze_saver_srcs = []\n    if freeze_saver:\n      freeze_args += \" --input_saver=$(location \" + freeze_saver + \")\"\n      freeze_saver_srcs += [freeze_saver]\n    native.genrule(\n        name=freeze_name,\n        srcs=[\n            graph,\n            freeze_checkpoint,\n            out_nodes_file,\n        ] + freeze_saver_srcs,\n        outs=[freeze_file],\n        cmd=(\"$(location //tensorflow/python/tools:freeze_graph)\" +\n             freeze_args),\n        tools=[\"//tensorflow/python/tools:freeze_graph\"],\n        tags=tags,\n    )\n    tfcompile_graph = freeze_file\n\n  # Rule that runs tfcompile to produce the header and object file.\n  header_file = name + \".h\"\n  object_file = name + \".o\"\n  ep = (\"__\" + PACKAGE_NAME + \"__\" + name).replace(\"/\", \"_\")\n  native.genrule(\n      name=(\"gen_\" + name),\n      srcs=[\n          tfcompile_graph,\n          config,\n      ],\n      outs=[\n          header_file,\n          object_file,\n      ],\n      cmd=(\"$(location \" + tfcompile_tool + \")\" +\n           \" --graph=$(location \" + tfcompile_graph + \")\" +\n           \" --config=$(location \" + config + \")\" +\n           \" --entry_point=\" + ep +\n           \" --cpp_class=\" + cpp_class +\n           \" --target_triple=\" + target_llvm_triple() +\n           \" --out_header=$(@D)/\" + header_file +\n           \" --out_object=$(@D)/\" + object_file +\n           \" \" + (tfcompile_flags or \"\")),\n      tools=[tfcompile_tool],\n      visibility=visibility,\n      testonly=testonly,\n      # Run tfcompile on the build host since it's typically faster on the local\n      # machine.\n      #\n      # Note that setting the local=1 attribute on a *test target* causes the\n      # test infrastructure to skip that test.  However this is a genrule, not a\n      # test target, and runs with --genrule_strategy=forced_forge, meaning the\n      # local=1 attribute is ignored, and the genrule is still run.\n      #\n      # https://www.bazel.io/versions/master/docs/be/general.html#genrule\n      local=1,\n      tags=tags,\n  )\n\n  # The cc_library rule packaging up the header and object file, and needed\n  # kernel implementations.\n  need_xla_data_proto = (tfcompile_flags and\n                         tfcompile_flags.find(\"--gen_program_shape\") != -1)\n  native.cc_library(\n      name=name,\n      srcs=[object_file],\n      hdrs=[header_file],\n      visibility=visibility,\n      testonly=testonly,\n      deps = [\n          # These deps are required by all tf_library targets even if\n          # include_standard_runtime_deps is False.  Without them, the\n          # generated code will fail to compile.\n          \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\n          \"//tensorflow/core:framework_lite\",\n      ] + (need_xla_data_proto and [\n          # If we're generating the program shape, we must depend on the proto.\n          \"//tensorflow/compiler/xla:xla_data_proto\",\n      ] or []) + (include_standard_runtime_deps and [\n          # TODO(cwhipkey): only depend on kernel code that the model actually needed.\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\",\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\",\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\",\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\",\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_avx\",\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_neon\",\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_sse4_1\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\n          \"//third_party/eigen3\",\n      ] or []) + (deps or []),\n      tags=tags,\n  )\n\n  # Variables used for gen_test and gen_benchmark.\n  no_ns_name = \"\"\n  cpp_class_split = cpp_class.rsplit(\"::\", maxsplit=2)\n  if len(cpp_class_split) == 1:\n    no_ns_name = cpp_class_split[0]\n  else:\n    no_ns_name = cpp_class_split[1]\n  sed_replace = (\n      \"-e \\\"s|{{TFCOMPILE_HEADER}}|$(location \" + header_file + \")|g\\\" \" +\n      \"-e \\\"s|{{TFCOMPILE_CPP_CLASS}}|\" + cpp_class + \"|g\\\" \" +\n      \"-e \\\"s|{{TFCOMPILE_NAME}}|\" + no_ns_name + \"|g\\\" \")\n\n  if gen_test:\n    test_name = name + \"_test\"\n    test_file = test_name + \".cc\"\n    # Rule to rewrite test.cc to produce the test_file.\n    native.genrule(\n        name=(\"gen_\" + test_name),\n        testonly=1,\n        srcs=[\n            \"//tensorflow/compiler/aot:test.cc\",\n            header_file,\n        ],\n        outs=[test_file],\n        cmd=(\"sed \" + sed_replace +\n             \" $(location //tensorflow/compiler/aot:test.cc) \" +\n             \"&gt; $(OUTS)\"),\n        tags=tags,\n    )\n\n    # The cc_test rule for the generated code.\n    native.cc_test(\n        name=test_name,\n        srcs=[test_file],\n        deps=[\n            \":\" + name,\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\n            \"//tensorflow/compiler/aot:runtime\",\n            \"//tensorflow/compiler/aot:tf_library_test_main\",\n            \"//tensorflow/compiler/xla:executable_run_options\",\n            \"//third_party/eigen3\",\n            \"//tensorflow/core:lib\",\n            \"//tensorflow/core:test\",\n            ],\n        tags=tags,\n    )\n\n  if gen_benchmark:\n    benchmark_name = name + \"_benchmark\"\n    benchmark_file = benchmark_name + \".cc\"\n    benchmark_main = (\"//tensorflow/compiler/aot:\" +\n                      \"benchmark_main.template\")\n\n    # Rule to rewrite benchmark.cc to produce the benchmark_file.\n    native.genrule(\n        name=(\"gen_\" + benchmark_name),\n        srcs=[\n            benchmark_main,\n            header_file,\n        ],\n        testonly = testonly,\n        outs=[benchmark_file],\n        cmd=(\"sed \" + sed_replace +\n             \" $(location \" + benchmark_main + \") \" +\n             \"&gt; $(OUTS)\"),\n        tags=tags,\n    )\n\n    # The cc_benchmark rule for the generated code.\n    #\n    # Note: to get smaller size on android for comparison, compile with:\n    #    --copt=-fvisibility=hidden\n    #    --copt=-D_LIBCPP_TYPE_VIS=_LIBCPP_HIDDEN\n    #    --copt=-D_LIBCPP_EXCEPTION_ABI=_LIBCPP_HIDDEN\n    native.cc_binary(\n        name=benchmark_name,\n        srcs=[benchmark_file],\n        testonly = testonly,\n        copts = tf_copts(),\n        linkopts = if_android([\"-pie\", \"-s\"]),\n        deps=[\n            \":\" + name,\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\n            \"//tensorflow/compiler/aot:benchmark\",\n            \"//tensorflow/compiler/aot:runtime\",\n            \"//tensorflow/compiler/xla:executable_run_options\",\n            \"//third_party/eigen3\",\n        ] + if_android([\n            \"//tensorflow/compiler/aot:benchmark_extra_android\",\n        ]),\n        tags=tags,\n    )\n\n\ndef target_llvm_triple():\n  \"\"\"Returns the target LLVM triple to be used for compiling the target.\"\"\"\n  # TODO(toddw): Add target_triple for other targets.  For details see:\n  # http://llvm.org/docs/doxygen/html/Triple_8h_source.html\n  return select({\n      \"//tensorflow:android_armeabi\": \"armv5-none-android\",\n      \"//tensorflow:android_arm\": \"armv7-none-android\",\n      \"//tensorflow:android_arm64\": \"aarch64-none-android\",\n      \"//tensorflow:android_x86\": \"i686-none-android\",\n      \"//tensorflow:linux_ppc64le\": \"ppc64le-ibm-linux-gnu\",\n      \"//tensorflow:darwin\": \"x86_64-none-darwin\",\n      \"//conditions:default\": \"x86_64-pc-linux\",\n  })\n</code></pre>\n<p>Then instead of running <code>tfcompile</code> manually, I built the <code>cc_library</code> using the <code>tf_library</code> macro (i.e., skipping Step 2.2), and the final binary can be successfully created now with command:</p>\n<pre><code>bazel build //tensorflow/compiler/aot/tests:my_binary\n</code></pre>\n<p>but the logging info still exists:</p>\n<pre><code>INFO: Analysed target //tensorflow/compiler/aot/tests:my_binary (2 packages loaded).\nINFO: Found 1 target...\nINFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:\n2017-10-12 12:56:13.846822: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\nTarget //tensorflow/compiler/aot/tests:my_binary up-to-date:\n  bazel-bin/tensorflow/compiler/aot/tests/my_binary\nINFO: Elapsed time: 0.331s, Critical Path: 0.05s\nINFO: Build completed successfully, 2 total actions\n</code></pre>\n<p>May I have your advice?</p>", "body_text": "@tatatodd Thanks for the information. I specified those flags in the tf_library, but the log still exists:\nThe tfcompile_flags is specified as following (in //tensorflow/tensorflow/compiler/aot/tfcompile.bzl):\n# -*- Python -*-\n\nload(\"//tensorflow:tensorflow.bzl\", \"if_android\", \"tf_copts\")\n\ndef tf_library(name, graph, config,\n               freeze_checkpoint=None, freeze_saver=None,\n               cpp_class=None, gen_test=True, gen_benchmark=True,\n               visibility=None, testonly=None,\n               tfcompile_flags=str('--target_cpu=\"x86-64\" --target_features=\"+sse4.1\" --target_features=\"+sse4.2\" --target_features=\"+avx\" --target_features=\"+avx2\" --target_features=\"+fma\"'),\n               #tfcompile_flags=None,\n               tfcompile_tool=\"//tensorflow/compiler/aot:tfcompile\",\n               include_standard_runtime_deps=True, deps=None, tags=None):\n \n  if not cpp_class:\n    fail(\"cpp_class must be specified\")\n\n  tfcompile_graph = graph\n  if freeze_checkpoint or freeze_saver:\n    if not freeze_checkpoint:\n      fail(\"freeze_checkpoint must be specified when freeze_saver is specified\")\n\n    freeze_name = \"freeze_\" + name\n    freeze_file = freeze_name + \".pb\"\n\n    # First run tfcompile to generate the list of out_nodes.\n    out_nodes_file = \"out_nodes_\" + freeze_name\n    native.genrule(\n        name=(\"gen_\" + out_nodes_file),\n        srcs=[config],\n        outs=[out_nodes_file],\n        cmd=(\"$(location \" + tfcompile_tool + \")\" +\n             \" --config=$(location \" + config + \")\" +\n             \" --dump_fetch_nodes > $@\"),\n        tools=[tfcompile_tool],\n        # Run tfcompile on the build host, rather than forge, since it's\n        # typically way faster on the local machine.\n        local=1,\n        tags=tags,\n    )\n\n    # Now run freeze_graph to convert variables into constants.\n    freeze_args = (\" --input_graph=$(location \" + graph + \")\" +\n                   \" --input_binary=\" + str(not graph.endswith(\".pbtxt\")) +\n                   \" --input_checkpoint=$(location \" + freeze_checkpoint + \")\" +\n                   \" --output_graph=$(location \" + freeze_file + \")\" +\n                   \" --output_node_names=$$(<$(location \" + out_nodes_file +\n                   \"))\")\n    freeze_saver_srcs = []\n    if freeze_saver:\n      freeze_args += \" --input_saver=$(location \" + freeze_saver + \")\"\n      freeze_saver_srcs += [freeze_saver]\n    native.genrule(\n        name=freeze_name,\n        srcs=[\n            graph,\n            freeze_checkpoint,\n            out_nodes_file,\n        ] + freeze_saver_srcs,\n        outs=[freeze_file],\n        cmd=(\"$(location //tensorflow/python/tools:freeze_graph)\" +\n             freeze_args),\n        tools=[\"//tensorflow/python/tools:freeze_graph\"],\n        tags=tags,\n    )\n    tfcompile_graph = freeze_file\n\n  # Rule that runs tfcompile to produce the header and object file.\n  header_file = name + \".h\"\n  object_file = name + \".o\"\n  ep = (\"__\" + PACKAGE_NAME + \"__\" + name).replace(\"/\", \"_\")\n  native.genrule(\n      name=(\"gen_\" + name),\n      srcs=[\n          tfcompile_graph,\n          config,\n      ],\n      outs=[\n          header_file,\n          object_file,\n      ],\n      cmd=(\"$(location \" + tfcompile_tool + \")\" +\n           \" --graph=$(location \" + tfcompile_graph + \")\" +\n           \" --config=$(location \" + config + \")\" +\n           \" --entry_point=\" + ep +\n           \" --cpp_class=\" + cpp_class +\n           \" --target_triple=\" + target_llvm_triple() +\n           \" --out_header=$(@D)/\" + header_file +\n           \" --out_object=$(@D)/\" + object_file +\n           \" \" + (tfcompile_flags or \"\")),\n      tools=[tfcompile_tool],\n      visibility=visibility,\n      testonly=testonly,\n      # Run tfcompile on the build host since it's typically faster on the local\n      # machine.\n      #\n      # Note that setting the local=1 attribute on a *test target* causes the\n      # test infrastructure to skip that test.  However this is a genrule, not a\n      # test target, and runs with --genrule_strategy=forced_forge, meaning the\n      # local=1 attribute is ignored, and the genrule is still run.\n      #\n      # https://www.bazel.io/versions/master/docs/be/general.html#genrule\n      local=1,\n      tags=tags,\n  )\n\n  # The cc_library rule packaging up the header and object file, and needed\n  # kernel implementations.\n  need_xla_data_proto = (tfcompile_flags and\n                         tfcompile_flags.find(\"--gen_program_shape\") != -1)\n  native.cc_library(\n      name=name,\n      srcs=[object_file],\n      hdrs=[header_file],\n      visibility=visibility,\n      testonly=testonly,\n      deps = [\n          # These deps are required by all tf_library targets even if\n          # include_standard_runtime_deps is False.  Without them, the\n          # generated code will fail to compile.\n          \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\n          \"//tensorflow/core:framework_lite\",\n      ] + (need_xla_data_proto and [\n          # If we're generating the program shape, we must depend on the proto.\n          \"//tensorflow/compiler/xla:xla_data_proto\",\n      ] or []) + (include_standard_runtime_deps and [\n          # TODO(cwhipkey): only depend on kernel code that the model actually needed.\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\",\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\",\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\",\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\",\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_avx\",\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_neon\",\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_sse4_1\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\n          \"//third_party/eigen3\",\n      ] or []) + (deps or []),\n      tags=tags,\n  )\n\n  # Variables used for gen_test and gen_benchmark.\n  no_ns_name = \"\"\n  cpp_class_split = cpp_class.rsplit(\"::\", maxsplit=2)\n  if len(cpp_class_split) == 1:\n    no_ns_name = cpp_class_split[0]\n  else:\n    no_ns_name = cpp_class_split[1]\n  sed_replace = (\n      \"-e \\\"s|{{TFCOMPILE_HEADER}}|$(location \" + header_file + \")|g\\\" \" +\n      \"-e \\\"s|{{TFCOMPILE_CPP_CLASS}}|\" + cpp_class + \"|g\\\" \" +\n      \"-e \\\"s|{{TFCOMPILE_NAME}}|\" + no_ns_name + \"|g\\\" \")\n\n  if gen_test:\n    test_name = name + \"_test\"\n    test_file = test_name + \".cc\"\n    # Rule to rewrite test.cc to produce the test_file.\n    native.genrule(\n        name=(\"gen_\" + test_name),\n        testonly=1,\n        srcs=[\n            \"//tensorflow/compiler/aot:test.cc\",\n            header_file,\n        ],\n        outs=[test_file],\n        cmd=(\"sed \" + sed_replace +\n             \" $(location //tensorflow/compiler/aot:test.cc) \" +\n             \"> $(OUTS)\"),\n        tags=tags,\n    )\n\n    # The cc_test rule for the generated code.\n    native.cc_test(\n        name=test_name,\n        srcs=[test_file],\n        deps=[\n            \":\" + name,\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\n            \"//tensorflow/compiler/aot:runtime\",\n            \"//tensorflow/compiler/aot:tf_library_test_main\",\n            \"//tensorflow/compiler/xla:executable_run_options\",\n            \"//third_party/eigen3\",\n            \"//tensorflow/core:lib\",\n            \"//tensorflow/core:test\",\n            ],\n        tags=tags,\n    )\n\n  if gen_benchmark:\n    benchmark_name = name + \"_benchmark\"\n    benchmark_file = benchmark_name + \".cc\"\n    benchmark_main = (\"//tensorflow/compiler/aot:\" +\n                      \"benchmark_main.template\")\n\n    # Rule to rewrite benchmark.cc to produce the benchmark_file.\n    native.genrule(\n        name=(\"gen_\" + benchmark_name),\n        srcs=[\n            benchmark_main,\n            header_file,\n        ],\n        testonly = testonly,\n        outs=[benchmark_file],\n        cmd=(\"sed \" + sed_replace +\n             \" $(location \" + benchmark_main + \") \" +\n             \"> $(OUTS)\"),\n        tags=tags,\n    )\n\n    # The cc_benchmark rule for the generated code.\n    #\n    # Note: to get smaller size on android for comparison, compile with:\n    #    --copt=-fvisibility=hidden\n    #    --copt=-D_LIBCPP_TYPE_VIS=_LIBCPP_HIDDEN\n    #    --copt=-D_LIBCPP_EXCEPTION_ABI=_LIBCPP_HIDDEN\n    native.cc_binary(\n        name=benchmark_name,\n        srcs=[benchmark_file],\n        testonly = testonly,\n        copts = tf_copts(),\n        linkopts = if_android([\"-pie\", \"-s\"]),\n        deps=[\n            \":\" + name,\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\n            \"//tensorflow/compiler/aot:benchmark\",\n            \"//tensorflow/compiler/aot:runtime\",\n            \"//tensorflow/compiler/xla:executable_run_options\",\n            \"//third_party/eigen3\",\n        ] + if_android([\n            \"//tensorflow/compiler/aot:benchmark_extra_android\",\n        ]),\n        tags=tags,\n    )\n\n\ndef target_llvm_triple():\n  \"\"\"Returns the target LLVM triple to be used for compiling the target.\"\"\"\n  # TODO(toddw): Add target_triple for other targets.  For details see:\n  # http://llvm.org/docs/doxygen/html/Triple_8h_source.html\n  return select({\n      \"//tensorflow:android_armeabi\": \"armv5-none-android\",\n      \"//tensorflow:android_arm\": \"armv7-none-android\",\n      \"//tensorflow:android_arm64\": \"aarch64-none-android\",\n      \"//tensorflow:android_x86\": \"i686-none-android\",\n      \"//tensorflow:linux_ppc64le\": \"ppc64le-ibm-linux-gnu\",\n      \"//tensorflow:darwin\": \"x86_64-none-darwin\",\n      \"//conditions:default\": \"x86_64-pc-linux\",\n  })\n\nThen instead of running tfcompile manually, I built the cc_library using the tf_library macro (i.e., skipping Step 2.2), and the final binary can be successfully created now with command:\nbazel build //tensorflow/compiler/aot/tests:my_binary\n\nbut the logging info still exists:\nINFO: Analysed target //tensorflow/compiler/aot/tests:my_binary (2 packages loaded).\nINFO: Found 1 target...\nINFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:\n2017-10-12 12:56:13.846822: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\nTarget //tensorflow/compiler/aot/tests:my_binary up-to-date:\n  bazel-bin/tensorflow/compiler/aot/tests/my_binary\nINFO: Elapsed time: 0.331s, Critical Path: 0.05s\nINFO: Build completed successfully, 2 total actions\n\nMay I have your advice?", "body": "@tatatodd Thanks for the information. I specified those flags in the `tf_library`, but the log still exists:\r\n\r\nThe `tfcompile_flags` is specified as following (in //tensorflow/tensorflow/compiler/aot/tfcompile.bzl):\r\n```\r\n# -*- Python -*-\r\n\r\nload(\"//tensorflow:tensorflow.bzl\", \"if_android\", \"tf_copts\")\r\n\r\ndef tf_library(name, graph, config,\r\n               freeze_checkpoint=None, freeze_saver=None,\r\n               cpp_class=None, gen_test=True, gen_benchmark=True,\r\n               visibility=None, testonly=None,\r\n               tfcompile_flags=str('--target_cpu=\"x86-64\" --target_features=\"+sse4.1\" --target_features=\"+sse4.2\" --target_features=\"+avx\" --target_features=\"+avx2\" --target_features=\"+fma\"'),\r\n               #tfcompile_flags=None,\r\n               tfcompile_tool=\"//tensorflow/compiler/aot:tfcompile\",\r\n               include_standard_runtime_deps=True, deps=None, tags=None):\r\n \r\n  if not cpp_class:\r\n    fail(\"cpp_class must be specified\")\r\n\r\n  tfcompile_graph = graph\r\n  if freeze_checkpoint or freeze_saver:\r\n    if not freeze_checkpoint:\r\n      fail(\"freeze_checkpoint must be specified when freeze_saver is specified\")\r\n\r\n    freeze_name = \"freeze_\" + name\r\n    freeze_file = freeze_name + \".pb\"\r\n\r\n    # First run tfcompile to generate the list of out_nodes.\r\n    out_nodes_file = \"out_nodes_\" + freeze_name\r\n    native.genrule(\r\n        name=(\"gen_\" + out_nodes_file),\r\n        srcs=[config],\r\n        outs=[out_nodes_file],\r\n        cmd=(\"$(location \" + tfcompile_tool + \")\" +\r\n             \" --config=$(location \" + config + \")\" +\r\n             \" --dump_fetch_nodes > $@\"),\r\n        tools=[tfcompile_tool],\r\n        # Run tfcompile on the build host, rather than forge, since it's\r\n        # typically way faster on the local machine.\r\n        local=1,\r\n        tags=tags,\r\n    )\r\n\r\n    # Now run freeze_graph to convert variables into constants.\r\n    freeze_args = (\" --input_graph=$(location \" + graph + \")\" +\r\n                   \" --input_binary=\" + str(not graph.endswith(\".pbtxt\")) +\r\n                   \" --input_checkpoint=$(location \" + freeze_checkpoint + \")\" +\r\n                   \" --output_graph=$(location \" + freeze_file + \")\" +\r\n                   \" --output_node_names=$$(<$(location \" + out_nodes_file +\r\n                   \"))\")\r\n    freeze_saver_srcs = []\r\n    if freeze_saver:\r\n      freeze_args += \" --input_saver=$(location \" + freeze_saver + \")\"\r\n      freeze_saver_srcs += [freeze_saver]\r\n    native.genrule(\r\n        name=freeze_name,\r\n        srcs=[\r\n            graph,\r\n            freeze_checkpoint,\r\n            out_nodes_file,\r\n        ] + freeze_saver_srcs,\r\n        outs=[freeze_file],\r\n        cmd=(\"$(location //tensorflow/python/tools:freeze_graph)\" +\r\n             freeze_args),\r\n        tools=[\"//tensorflow/python/tools:freeze_graph\"],\r\n        tags=tags,\r\n    )\r\n    tfcompile_graph = freeze_file\r\n\r\n  # Rule that runs tfcompile to produce the header and object file.\r\n  header_file = name + \".h\"\r\n  object_file = name + \".o\"\r\n  ep = (\"__\" + PACKAGE_NAME + \"__\" + name).replace(\"/\", \"_\")\r\n  native.genrule(\r\n      name=(\"gen_\" + name),\r\n      srcs=[\r\n          tfcompile_graph,\r\n          config,\r\n      ],\r\n      outs=[\r\n          header_file,\r\n          object_file,\r\n      ],\r\n      cmd=(\"$(location \" + tfcompile_tool + \")\" +\r\n           \" --graph=$(location \" + tfcompile_graph + \")\" +\r\n           \" --config=$(location \" + config + \")\" +\r\n           \" --entry_point=\" + ep +\r\n           \" --cpp_class=\" + cpp_class +\r\n           \" --target_triple=\" + target_llvm_triple() +\r\n           \" --out_header=$(@D)/\" + header_file +\r\n           \" --out_object=$(@D)/\" + object_file +\r\n           \" \" + (tfcompile_flags or \"\")),\r\n      tools=[tfcompile_tool],\r\n      visibility=visibility,\r\n      testonly=testonly,\r\n      # Run tfcompile on the build host since it's typically faster on the local\r\n      # machine.\r\n      #\r\n      # Note that setting the local=1 attribute on a *test target* causes the\r\n      # test infrastructure to skip that test.  However this is a genrule, not a\r\n      # test target, and runs with --genrule_strategy=forced_forge, meaning the\r\n      # local=1 attribute is ignored, and the genrule is still run.\r\n      #\r\n      # https://www.bazel.io/versions/master/docs/be/general.html#genrule\r\n      local=1,\r\n      tags=tags,\r\n  )\r\n\r\n  # The cc_library rule packaging up the header and object file, and needed\r\n  # kernel implementations.\r\n  need_xla_data_proto = (tfcompile_flags and\r\n                         tfcompile_flags.find(\"--gen_program_shape\") != -1)\r\n  native.cc_library(\r\n      name=name,\r\n      srcs=[object_file],\r\n      hdrs=[header_file],\r\n      visibility=visibility,\r\n      testonly=testonly,\r\n      deps = [\r\n          # These deps are required by all tf_library targets even if\r\n          # include_standard_runtime_deps is False.  Without them, the\r\n          # generated code will fail to compile.\r\n          \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\r\n          \"//tensorflow/core:framework_lite\",\r\n      ] + (need_xla_data_proto and [\r\n          # If we're generating the program shape, we must depend on the proto.\r\n          \"//tensorflow/compiler/xla:xla_data_proto\",\r\n      ] or []) + (include_standard_runtime_deps and [\r\n          # TODO(cwhipkey): only depend on kernel code that the model actually needed.\r\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_avx\",\r\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_neon\",\r\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_sse4_1\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\r\n          \"//third_party/eigen3\",\r\n      ] or []) + (deps or []),\r\n      tags=tags,\r\n  )\r\n\r\n  # Variables used for gen_test and gen_benchmark.\r\n  no_ns_name = \"\"\r\n  cpp_class_split = cpp_class.rsplit(\"::\", maxsplit=2)\r\n  if len(cpp_class_split) == 1:\r\n    no_ns_name = cpp_class_split[0]\r\n  else:\r\n    no_ns_name = cpp_class_split[1]\r\n  sed_replace = (\r\n      \"-e \\\"s|{{TFCOMPILE_HEADER}}|$(location \" + header_file + \")|g\\\" \" +\r\n      \"-e \\\"s|{{TFCOMPILE_CPP_CLASS}}|\" + cpp_class + \"|g\\\" \" +\r\n      \"-e \\\"s|{{TFCOMPILE_NAME}}|\" + no_ns_name + \"|g\\\" \")\r\n\r\n  if gen_test:\r\n    test_name = name + \"_test\"\r\n    test_file = test_name + \".cc\"\r\n    # Rule to rewrite test.cc to produce the test_file.\r\n    native.genrule(\r\n        name=(\"gen_\" + test_name),\r\n        testonly=1,\r\n        srcs=[\r\n            \"//tensorflow/compiler/aot:test.cc\",\r\n            header_file,\r\n        ],\r\n        outs=[test_file],\r\n        cmd=(\"sed \" + sed_replace +\r\n             \" $(location //tensorflow/compiler/aot:test.cc) \" +\r\n             \"> $(OUTS)\"),\r\n        tags=tags,\r\n    )\r\n\r\n    # The cc_test rule for the generated code.\r\n    native.cc_test(\r\n        name=test_name,\r\n        srcs=[test_file],\r\n        deps=[\r\n            \":\" + name,\r\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n            \"//tensorflow/compiler/aot:runtime\",\r\n            \"//tensorflow/compiler/aot:tf_library_test_main\",\r\n            \"//tensorflow/compiler/xla:executable_run_options\",\r\n            \"//third_party/eigen3\",\r\n            \"//tensorflow/core:lib\",\r\n            \"//tensorflow/core:test\",\r\n            ],\r\n        tags=tags,\r\n    )\r\n\r\n  if gen_benchmark:\r\n    benchmark_name = name + \"_benchmark\"\r\n    benchmark_file = benchmark_name + \".cc\"\r\n    benchmark_main = (\"//tensorflow/compiler/aot:\" +\r\n                      \"benchmark_main.template\")\r\n\r\n    # Rule to rewrite benchmark.cc to produce the benchmark_file.\r\n    native.genrule(\r\n        name=(\"gen_\" + benchmark_name),\r\n        srcs=[\r\n            benchmark_main,\r\n            header_file,\r\n        ],\r\n        testonly = testonly,\r\n        outs=[benchmark_file],\r\n        cmd=(\"sed \" + sed_replace +\r\n             \" $(location \" + benchmark_main + \") \" +\r\n             \"> $(OUTS)\"),\r\n        tags=tags,\r\n    )\r\n\r\n    # The cc_benchmark rule for the generated code.\r\n    #\r\n    # Note: to get smaller size on android for comparison, compile with:\r\n    #    --copt=-fvisibility=hidden\r\n    #    --copt=-D_LIBCPP_TYPE_VIS=_LIBCPP_HIDDEN\r\n    #    --copt=-D_LIBCPP_EXCEPTION_ABI=_LIBCPP_HIDDEN\r\n    native.cc_binary(\r\n        name=benchmark_name,\r\n        srcs=[benchmark_file],\r\n        testonly = testonly,\r\n        copts = tf_copts(),\r\n        linkopts = if_android([\"-pie\", \"-s\"]),\r\n        deps=[\r\n            \":\" + name,\r\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n            \"//tensorflow/compiler/aot:benchmark\",\r\n            \"//tensorflow/compiler/aot:runtime\",\r\n            \"//tensorflow/compiler/xla:executable_run_options\",\r\n            \"//third_party/eigen3\",\r\n        ] + if_android([\r\n            \"//tensorflow/compiler/aot:benchmark_extra_android\",\r\n        ]),\r\n        tags=tags,\r\n    )\r\n\r\n\r\ndef target_llvm_triple():\r\n  \"\"\"Returns the target LLVM triple to be used for compiling the target.\"\"\"\r\n  # TODO(toddw): Add target_triple for other targets.  For details see:\r\n  # http://llvm.org/docs/doxygen/html/Triple_8h_source.html\r\n  return select({\r\n      \"//tensorflow:android_armeabi\": \"armv5-none-android\",\r\n      \"//tensorflow:android_arm\": \"armv7-none-android\",\r\n      \"//tensorflow:android_arm64\": \"aarch64-none-android\",\r\n      \"//tensorflow:android_x86\": \"i686-none-android\",\r\n      \"//tensorflow:linux_ppc64le\": \"ppc64le-ibm-linux-gnu\",\r\n      \"//tensorflow:darwin\": \"x86_64-none-darwin\",\r\n      \"//conditions:default\": \"x86_64-pc-linux\",\r\n  })\r\n```\r\n\r\nThen instead of running `tfcompile` manually, I built the `cc_library` using the `tf_library` macro (i.e., skipping Step 2.2), and the final binary can be successfully created now with command:\r\n```\r\nbazel build //tensorflow/compiler/aot/tests:my_binary\r\n```\r\n\r\n but the logging info still exists:\r\n```\r\nINFO: Analysed target //tensorflow/compiler/aot/tests:my_binary (2 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:\r\n2017-10-12 12:56:13.846822: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTarget //tensorflow/compiler/aot/tests:my_binary up-to-date:\r\n  bazel-bin/tensorflow/compiler/aot/tests/my_binary\r\nINFO: Elapsed time: 0.331s, Critical Path: 0.05s\r\nINFO: Build completed successfully, 2 total actions\r\n``` \r\n\r\nMay I have your advice?"}