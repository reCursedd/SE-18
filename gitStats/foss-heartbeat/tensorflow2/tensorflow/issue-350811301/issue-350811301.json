{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21631", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21631/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21631/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21631/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21631", "id": 350811301, "node_id": "MDU6SXNzdWUzNTA4MTEzMDE=", "number": 21631, "title": "Keras model converted to an Estimator does not handle 1D labels correctly", "user": {"login": "ageron", "id": 76661, "node_id": "MDQ6VXNlcjc2NjYx", "avatar_url": "https://avatars3.githubusercontent.com/u/76661?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ageron", "html_url": "https://github.com/ageron", "followers_url": "https://api.github.com/users/ageron/followers", "following_url": "https://api.github.com/users/ageron/following{/other_user}", "gists_url": "https://api.github.com/users/ageron/gists{/gist_id}", "starred_url": "https://api.github.com/users/ageron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ageron/subscriptions", "organizations_url": "https://api.github.com/users/ageron/orgs", "repos_url": "https://api.github.com/users/ageron/repos", "events_url": "https://api.github.com/users/ageron/events{/privacy}", "received_events_url": "https://api.github.com/users/ageron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-08-15T13:35:15Z", "updated_at": "2018-11-19T23:26:20Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: MacOSX 10.13.6</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: No</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.10.0-rc1-19-g656e7a2b34 1.10.0</li>\n<li><strong>Python version</strong>: 3.6.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: see script below (keras model converges, not estimator)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Keras model converges on MNIST (&gt; 97% accuracy after 5 epochs with batches of 32 instances), using the class IDs as labels (i.e., a 1D array of int32). However, after I convert the model to an Estimator, it does not converge (around 10% accuracy, again after 5 epochs with batches of 32 instances). Once I reshape the labels to a column vector (<code>y_train.reshape(-1, 1)</code>), it converges well. I double-checked with a <code>DNNClassifier</code>, and it accepts the labels both as a 1D array or as a column vector (2D). I believe this is a bug, and people might spend hours searching for the cause (as I did).</p>\n<h3>Source code / logs</h3>\n<p>If you run the code below, it will train three models on MNIST, first using 1D labels, then using 2D labels (i.e., column vectors):</p>\n<ol>\n<li>a Keras model</li>\n<li>the same Keras model converted to an Estimator</li>\n<li>an equivalent <code>DNNClassifier</code></li>\n</ol>\n<p>The final outputs below show that they all converge except for the Keras model converted to an Estimator when the labels are 1D (I removed the \"You CPU supports...\" and \"Using temporary folder...\" warnings from the outputs). The behavior is identical in eager mode and in graph mode.</p>\n<pre><code>==== Training with 1D labels... ====\nKeras model:\n    Eval: [0.07029167589747813, 0.9792]\nKeras model converted to an estimator:\n    Eval: ({'accuracy': 0.10245253, 'loss': 0.07640489, 'global_step': 18751}, [])\nDNNClassifier:\n    Eval: ({'accuracy': 0.1135, 'average_loss': 2.3010197, 'loss': 291.2683, 'global_step': 9375}, [])\n\n==== Training with 2D labels... ====\nKeras model:\n    Eval: [0.06944960513310507, 0.9779]\nKeras model converted to an estimator:\n    Eval: ({'accuracy': 0.97171676, 'loss': 0.09046984, 'global_step': 18751}, [])\nDNNClassifier:\n    Eval: ({'accuracy': 0.976, 'average_loss': 0.08481478, 'loss': 10.736049, 'global_step': 9375}, [])\n</code></pre>\n<p>Here is the source code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division, print_function, unicode_literals\n\n<span class=\"pl-k\">import</span> logging\nlogging.basicConfig(<span class=\"pl-v\">level</span><span class=\"pl-k\">=</span>logging.<span class=\"pl-c1\">WARNING</span>)\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>tf.enable_eager_execution() # same problem in eager mode or graph mode</span>\n<span class=\"pl-k\">from</span> tensorflow <span class=\"pl-k\">import</span> keras\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nn_epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_keras_model</span>():\n    keras.backend.clear_session()\n    model <span class=\"pl-k\">=</span> keras.models.Sequential([\n        keras.layers.Dense(<span class=\"pl-c1\">300</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">28</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">28</span>]),\n        keras.layers.Dense(<span class=\"pl-c1\">100</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>),\n        keras.layers.Dense(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax<span class=\"pl-pds\">\"</span></span>)\n    ])\n    optimizer <span class=\"pl-k\">=</span> tf.train.MomentumOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sparse_categorical_crossentropy<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>optimizer,\n                  <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>])\n    <span class=\"pl-k\">return</span> model\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_keras_estimator</span>():\n    model <span class=\"pl-k\">=</span> create_keras_model()\n    input_name <span class=\"pl-k\">=</span> model.input_names[<span class=\"pl-c1\">0</span>]\n    estimator <span class=\"pl-k\">=</span> tf.keras.estimator.model_to_estimator(model)\n    <span class=\"pl-k\">return</span> estimator, input_name\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_dnn_estimator</span>():\n    input_name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pixels<span class=\"pl-pds\">\"</span></span>\n    pixels <span class=\"pl-k\">=</span> tf.feature_column.numeric_column(input_name, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">28</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">28</span>])\n    estimator <span class=\"pl-k\">=</span> tf.estimator.DNNClassifier(<span class=\"pl-v\">hidden_units</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>],\n                                           <span class=\"pl-v\">n_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>,\n                                           <span class=\"pl-v\">feature_columns</span><span class=\"pl-k\">=</span>[pixels])\n    <span class=\"pl-k\">return</span> estimator, input_name\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train_and_evaluate_estimator</span>(<span class=\"pl-smi\">estimator</span>, <span class=\"pl-smi\">input_name</span>,\n                                 <span class=\"pl-smi\">X_train</span>, <span class=\"pl-smi\">y_train</span>, <span class=\"pl-smi\">X_test</span>, <span class=\"pl-smi\">y_test</span>):\n    train_input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n        <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{input_name: X_train}, <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>y_train, <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>,\n        <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    test_input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n        <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{input_name: X_test}, <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>y_test, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    train_spec <span class=\"pl-k\">=</span> tf.estimator.TrainSpec(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>train_input_fn)\n    eval_spec <span class=\"pl-k\">=</span> tf.estimator.EvalSpec(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>test_input_fn)\n    <span class=\"pl-k\">return</span> tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">load_and_scale_MNIST</span>():\n    (X_train, y_train), (X_test, y_test) <span class=\"pl-k\">=</span> keras.datasets.mnist.load_data()\n    X_train <span class=\"pl-k\">=</span> X_train.astype(np.float32).reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">28</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">255.0</span>\n    X_test <span class=\"pl-k\">=</span> X_test.astype(np.float32).reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">28</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">255.0</span>\n    y_train <span class=\"pl-k\">=</span> y_train.astype(np.int32)\n    y_test <span class=\"pl-k\">=</span> y_test.astype(np.int32)\n    <span class=\"pl-k\">return</span> X_train, y_train, X_test, y_test\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    X_train, y_train, X_test, y_test <span class=\"pl-k\">=</span> load_and_scale_MNIST()\n    \n    <span class=\"pl-k\">for</span> run <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n        <span class=\"pl-k\">if</span> run <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>==== Training with 1D labels... ====<span class=\"pl-pds\">\"</span></span>)\n            <span class=\"pl-k\">assert</span> y_train.ndim <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">and</span> y_test.ndim <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>\n        <span class=\"pl-k\">else</span>:\n            y_train <span class=\"pl-k\">=</span> y_train.reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n            y_test <span class=\"pl-k\">=</span> y_test.reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n            <span class=\"pl-c1\">print</span>()\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>==== Training with 2D labels... ====<span class=\"pl-pds\">\"</span></span>)\n            <span class=\"pl-k\">assert</span> y_train.ndim <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">and</span> y_test.ndim <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span>\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Keras model:<span class=\"pl-pds\">\"</span></span>)\n        keras_model <span class=\"pl-k\">=</span> create_keras_model()\n        keras_model.fit(X_train, y_train, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span>n_epochs,\n                        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    Eval:<span class=\"pl-pds\">\"</span></span>, keras_model.evaluate(X_test, y_test, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>))\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Keras model converted to an estimator:<span class=\"pl-pds\">\"</span></span>)\n        keras_estimator, input_name <span class=\"pl-k\">=</span> create_keras_estimator()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    Eval:<span class=\"pl-pds\">\"</span></span>, train_and_evaluate_estimator(\n            keras_estimator, input_name, X_train, y_train, X_test, y_test))\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>DNNClassifier:<span class=\"pl-pds\">\"</span></span>)\n        dnn_estimator, input_name <span class=\"pl-k\">=</span> create_dnn_estimator()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    Eval:<span class=\"pl-pds\">\"</span></span>, train_and_evaluate_estimator(\n            dnn_estimator, input_name, X_train, y_train, X_test, y_test))</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.13.6\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): No\nTensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34 1.10.0\nPython version: 3.6.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: see script below (keras model converges, not estimator)\n\nDescribe the problem\nKeras model converges on MNIST (> 97% accuracy after 5 epochs with batches of 32 instances), using the class IDs as labels (i.e., a 1D array of int32). However, after I convert the model to an Estimator, it does not converge (around 10% accuracy, again after 5 epochs with batches of 32 instances). Once I reshape the labels to a column vector (y_train.reshape(-1, 1)), it converges well. I double-checked with a DNNClassifier, and it accepts the labels both as a 1D array or as a column vector (2D). I believe this is a bug, and people might spend hours searching for the cause (as I did).\nSource code / logs\nIf you run the code below, it will train three models on MNIST, first using 1D labels, then using 2D labels (i.e., column vectors):\n\na Keras model\nthe same Keras model converted to an Estimator\nan equivalent DNNClassifier\n\nThe final outputs below show that they all converge except for the Keras model converted to an Estimator when the labels are 1D (I removed the \"You CPU supports...\" and \"Using temporary folder...\" warnings from the outputs). The behavior is identical in eager mode and in graph mode.\n==== Training with 1D labels... ====\nKeras model:\n    Eval: [0.07029167589747813, 0.9792]\nKeras model converted to an estimator:\n    Eval: ({'accuracy': 0.10245253, 'loss': 0.07640489, 'global_step': 18751}, [])\nDNNClassifier:\n    Eval: ({'accuracy': 0.1135, 'average_loss': 2.3010197, 'loss': 291.2683, 'global_step': 9375}, [])\n\n==== Training with 2D labels... ====\nKeras model:\n    Eval: [0.06944960513310507, 0.9779]\nKeras model converted to an estimator:\n    Eval: ({'accuracy': 0.97171676, 'loss': 0.09046984, 'global_step': 18751}, [])\nDNNClassifier:\n    Eval: ({'accuracy': 0.976, 'average_loss': 0.08481478, 'loss': 10.736049, 'global_step': 9375}, [])\n\nHere is the source code:\nfrom __future__ import division, print_function, unicode_literals\n\nimport logging\nlogging.basicConfig(level=logging.WARNING)\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n#tf.enable_eager_execution() # same problem in eager mode or graph mode\nfrom tensorflow import keras\nimport numpy as np\n\nn_epochs = 5\nbatch_size = 32\n\ndef create_keras_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential([\n        keras.layers.Dense(300, activation=\"relu\", input_shape=[28*28]),\n        keras.layers.Dense(100, activation=\"relu\"),\n        keras.layers.Dense(10, activation=\"softmax\")\n    ])\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n                  metrics=[\"accuracy\"])\n    return model\n\ndef create_keras_estimator():\n    model = create_keras_model()\n    input_name = model.input_names[0]\n    estimator = tf.keras.estimator.model_to_estimator(model)\n    return estimator, input_name\n\ndef create_dnn_estimator():\n    input_name = \"pixels\"\n    pixels = tf.feature_column.numeric_column(input_name, shape=[28 * 28])\n    estimator = tf.estimator.DNNClassifier(hidden_units=[300, 100, 10],\n                                           n_classes=10,\n                                           feature_columns=[pixels])\n    return estimator, input_name\n\ndef train_and_evaluate_estimator(estimator, input_name,\n                                 X_train, y_train, X_test, y_test):\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name: X_train}, y=y_train, num_epochs=5, batch_size=32,\n        shuffle=True)\n    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name: X_test}, y=y_test, shuffle=False)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n    eval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn)\n    return tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\ndef load_and_scale_MNIST():\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n    X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n    X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n    y_train = y_train.astype(np.int32)\n    y_test = y_test.astype(np.int32)\n    return X_train, y_train, X_test, y_test\n\nif __name__ == '__main__':\n    X_train, y_train, X_test, y_test = load_and_scale_MNIST()\n    \n    for run in range(2):\n        if run == 0:\n            print(\"==== Training with 1D labels... ====\")\n            assert y_train.ndim == 1 and y_test.ndim == 1\n        else:\n            y_train = y_train.reshape(-1, 1)\n            y_test = y_test.reshape(-1, 1)\n            print()\n            print(\"==== Training with 2D labels... ====\")\n            assert y_train.ndim == 2 and y_test.ndim == 2\n\n        print(\"Keras model:\")\n        keras_model = create_keras_model()\n        keras_model.fit(X_train, y_train, epochs=n_epochs,\n                        batch_size=batch_size, verbose=0)\n        print(\"    Eval:\", keras_model.evaluate(X_test, y_test, verbose=0))\n\n        print(\"Keras model converted to an estimator:\")\n        keras_estimator, input_name = create_keras_estimator()\n        print(\"    Eval:\", train_and_evaluate_estimator(\n            keras_estimator, input_name, X_train, y_train, X_test, y_test))\n\n        print(\"DNNClassifier:\")\n        dnn_estimator, input_name = create_dnn_estimator()\n        print(\"    Eval:\", train_and_evaluate_estimator(\n            dnn_estimator, input_name, X_train, y_train, X_test, y_test))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: No\r\n- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see script below (keras model converges, not estimator)\r\n\r\n### Describe the problem\r\nKeras model converges on MNIST (> 97% accuracy after 5 epochs with batches of 32 instances), using the class IDs as labels (i.e., a 1D array of int32). However, after I convert the model to an Estimator, it does not converge (around 10% accuracy, again after 5 epochs with batches of 32 instances). Once I reshape the labels to a column vector (`y_train.reshape(-1, 1)`), it converges well. I double-checked with a `DNNClassifier`, and it accepts the labels both as a 1D array or as a column vector (2D). I believe this is a bug, and people might spend hours searching for the cause (as I did).\r\n\r\n### Source code / logs\r\nIf you run the code below, it will train three models on MNIST, first using 1D labels, then using 2D labels (i.e., column vectors):\r\n\r\n1. a Keras model\r\n2. the same Keras model converted to an Estimator\r\n3. an equivalent `DNNClassifier`\r\n\r\nThe final outputs below show that they all converge except for the Keras model converted to an Estimator when the labels are 1D (I removed the \"You CPU supports...\" and \"Using temporary folder...\" warnings from the outputs). The behavior is identical in eager mode and in graph mode.\r\n\r\n```\r\n==== Training with 1D labels... ====\r\nKeras model:\r\n    Eval: [0.07029167589747813, 0.9792]\r\nKeras model converted to an estimator:\r\n    Eval: ({'accuracy': 0.10245253, 'loss': 0.07640489, 'global_step': 18751}, [])\r\nDNNClassifier:\r\n    Eval: ({'accuracy': 0.1135, 'average_loss': 2.3010197, 'loss': 291.2683, 'global_step': 9375}, [])\r\n\r\n==== Training with 2D labels... ====\r\nKeras model:\r\n    Eval: [0.06944960513310507, 0.9779]\r\nKeras model converted to an estimator:\r\n    Eval: ({'accuracy': 0.97171676, 'loss': 0.09046984, 'global_step': 18751}, [])\r\nDNNClassifier:\r\n    Eval: ({'accuracy': 0.976, 'average_loss': 0.08481478, 'loss': 10.736049, 'global_step': 9375}, [])\r\n```\r\n\r\nHere is the source code:\r\n\r\n```python\r\nfrom __future__ import division, print_function, unicode_literals\r\n\r\nimport logging\r\nlogging.basicConfig(level=logging.WARNING)\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n#tf.enable_eager_execution() # same problem in eager mode or graph mode\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nn_epochs = 5\r\nbatch_size = 32\r\n\r\ndef create_keras_model():\r\n    keras.backend.clear_session()\r\n    model = keras.models.Sequential([\r\n        keras.layers.Dense(300, activation=\"relu\", input_shape=[28*28]),\r\n        keras.layers.Dense(100, activation=\"relu\"),\r\n        keras.layers.Dense(10, activation=\"softmax\")\r\n    ])\r\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\r\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\r\n                  metrics=[\"accuracy\"])\r\n    return model\r\n\r\ndef create_keras_estimator():\r\n    model = create_keras_model()\r\n    input_name = model.input_names[0]\r\n    estimator = tf.keras.estimator.model_to_estimator(model)\r\n    return estimator, input_name\r\n\r\ndef create_dnn_estimator():\r\n    input_name = \"pixels\"\r\n    pixels = tf.feature_column.numeric_column(input_name, shape=[28 * 28])\r\n    estimator = tf.estimator.DNNClassifier(hidden_units=[300, 100, 10],\r\n                                           n_classes=10,\r\n                                           feature_columns=[pixels])\r\n    return estimator, input_name\r\n\r\ndef train_and_evaluate_estimator(estimator, input_name,\r\n                                 X_train, y_train, X_test, y_test):\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={input_name: X_train}, y=y_train, num_epochs=5, batch_size=32,\r\n        shuffle=True)\r\n    test_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={input_name: X_test}, y=y_test, shuffle=False)\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn)\r\n    return tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\ndef load_and_scale_MNIST():\r\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\r\n    X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\r\n    X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\r\n    y_train = y_train.astype(np.int32)\r\n    y_test = y_test.astype(np.int32)\r\n    return X_train, y_train, X_test, y_test\r\n\r\nif __name__ == '__main__':\r\n    X_train, y_train, X_test, y_test = load_and_scale_MNIST()\r\n    \r\n    for run in range(2):\r\n        if run == 0:\r\n            print(\"==== Training with 1D labels... ====\")\r\n            assert y_train.ndim == 1 and y_test.ndim == 1\r\n        else:\r\n            y_train = y_train.reshape(-1, 1)\r\n            y_test = y_test.reshape(-1, 1)\r\n            print()\r\n            print(\"==== Training with 2D labels... ====\")\r\n            assert y_train.ndim == 2 and y_test.ndim == 2\r\n\r\n        print(\"Keras model:\")\r\n        keras_model = create_keras_model()\r\n        keras_model.fit(X_train, y_train, epochs=n_epochs,\r\n                        batch_size=batch_size, verbose=0)\r\n        print(\"    Eval:\", keras_model.evaluate(X_test, y_test, verbose=0))\r\n\r\n        print(\"Keras model converted to an estimator:\")\r\n        keras_estimator, input_name = create_keras_estimator()\r\n        print(\"    Eval:\", train_and_evaluate_estimator(\r\n            keras_estimator, input_name, X_train, y_train, X_test, y_test))\r\n\r\n        print(\"DNNClassifier:\")\r\n        dnn_estimator, input_name = create_dnn_estimator()\r\n        print(\"    Eval:\", train_and_evaluate_estimator(\r\n            dnn_estimator, input_name, X_train, y_train, X_test, y_test))\r\n```"}