{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387553416", "html_url": "https://github.com/tensorflow/tensorflow/issues/19141#issuecomment-387553416", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19141", "id": 387553416, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzU1MzQxNg==", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-08T21:49:13Z", "updated_at": "2018-05-08T21:49:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13745902\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oscarriddle\">@oscarriddle</a> ,<br>\nYou can't have INT8 without calibration. It should fail. Are you sure that you are running an int8 graph?</p>\n<p>Also TFTRT int8 is a bit more complicated than TRT only int8. In native TRT(C++ version you mention) you are calibrating a graph end-to-end and int8 calibration TRT tries the minimize the information loss between input and output but still introduces some small errors as you describe. In TFTRT you are likely converting a subgraph rather than full graph, meaning there are TF native nodes in the graph after TRT graph. Since the calibration can only minimize the information loss in the engine, the errors introduced at the output of the engine due to calibration, may, and most likely will be, amplified by the downstream nodes. If you want to minimize this, you will need to do calibration aware training. We are thinking on how can we simplify that and have some ideas to test.</p>\n<p>As for the answers,</p>\n<ol>\n<li>No, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?</li>\n<li>In TFTRT, calibration data is not serialized. It is consumed immediately. When you create a calibration graph, a calibrator is created and attached to the graph. When you run the calibration graph data is passed to the calibrator to collect calibration data and when you convert calibration graph to inference graph, nodes in calibration graph is taken out and replaced with TRT engine. If you save inference graph, it will always use the calibration you made and it doesn't require further calibration.</li>\n</ol>\n<p>Cheers,<br>\nSami</p>", "body_text": "@oscarriddle ,\nYou can't have INT8 without calibration. It should fail. Are you sure that you are running an int8 graph?\nAlso TFTRT int8 is a bit more complicated than TRT only int8. In native TRT(C++ version you mention) you are calibrating a graph end-to-end and int8 calibration TRT tries the minimize the information loss between input and output but still introduces some small errors as you describe. In TFTRT you are likely converting a subgraph rather than full graph, meaning there are TF native nodes in the graph after TRT graph. Since the calibration can only minimize the information loss in the engine, the errors introduced at the output of the engine due to calibration, may, and most likely will be, amplified by the downstream nodes. If you want to minimize this, you will need to do calibration aware training. We are thinking on how can we simplify that and have some ideas to test.\nAs for the answers,\n\nNo, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?\nIn TFTRT, calibration data is not serialized. It is consumed immediately. When you create a calibration graph, a calibrator is created and attached to the graph. When you run the calibration graph data is passed to the calibrator to collect calibration data and when you convert calibration graph to inference graph, nodes in calibration graph is taken out and replaced with TRT engine. If you save inference graph, it will always use the calibration you made and it doesn't require further calibration.\n\nCheers,\nSami", "body": "@oscarriddle , \r\nYou can't have INT8 without calibration. It should fail. Are you sure that you are running an int8 graph? \r\n\r\nAlso TFTRT int8 is a bit more complicated than TRT only int8. In native TRT(C++ version you mention) you are calibrating a graph end-to-end and int8 calibration TRT tries the minimize the information loss between input and output but still introduces some small errors as you describe. In TFTRT you are likely converting a subgraph rather than full graph, meaning there are TF native nodes in the graph after TRT graph. Since the calibration can only minimize the information loss in the engine, the errors introduced at the output of the engine due to calibration, may, and most likely will be, amplified by the downstream nodes. If you want to minimize this, you will need to do calibration aware training. We are thinking on how can we simplify that and have some ideas to test. \r\n\r\nAs for the answers,\r\n1) No, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?\r\n2) In TFTRT, calibration data is not serialized. It is consumed immediately. When you create a calibration graph, a calibrator is created and attached to the graph. When you run the calibration graph data is passed to the calibrator to collect calibration data and when you convert calibration graph to inference graph, nodes in calibration graph is taken out and replaced with TRT engine. If you save inference graph, it will always use the calibration you made and it doesn't require further calibration.\r\n\r\nCheers,\r\nSami"}