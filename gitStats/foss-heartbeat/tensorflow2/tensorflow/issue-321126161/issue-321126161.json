{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19141", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19141/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19141/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19141/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19141", "id": 321126161, "node_id": "MDU6SXNzdWUzMjExMjYxNjE=", "number": 19141, "title": "[r1.7][TensorRT] Questions about the calibration in INT8 mode of TensorRT optimization", "user": {"login": "oscarriddle", "id": 13745902, "node_id": "MDQ6VXNlcjEzNzQ1OTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/13745902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oscarriddle", "html_url": "https://github.com/oscarriddle", "followers_url": "https://api.github.com/users/oscarriddle/followers", "following_url": "https://api.github.com/users/oscarriddle/following{/other_user}", "gists_url": "https://api.github.com/users/oscarriddle/gists{/gist_id}", "starred_url": "https://api.github.com/users/oscarriddle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oscarriddle/subscriptions", "organizations_url": "https://api.github.com/users/oscarriddle/orgs", "repos_url": "https://api.github.com/users/oscarriddle/repos", "events_url": "https://api.github.com/users/oscarriddle/events{/privacy}", "received_events_url": "https://api.github.com/users/oscarriddle/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-05-08T10:17:40Z", "updated_at": "2018-05-14T09:18:27Z", "closed_at": "2018-05-14T01:29:35Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:  NO</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip (python 2.7)</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow-gpu==1.7.0</li>\n<li><strong>Python version</strong>:  python 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc 5.3</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA9.0, cuDNN7.0.5</li>\n<li><strong>GPU model and memory</strong>: Tesla P4, 8GB</li>\n<li><strong>Exact command to reproduce</strong>: NA</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I tried to evaluate the accuracy of TF integrated TRT INT8 optimization in Python. However, I followed the procedure as the example test: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py</a><br>\nbut the inference accuracy is not good (FP32: 70%, INT8-without-calib: 16%, INT8-with-calib: 35%).</p>\n<p>I tried to trace the code but on the API level, but I'm a little confused about how does the calibration work. looks like the function run_calibration() just created a session and accepted the calib dataset to run the session, and then the return value was abandoned.</p>\n<p>I checked this introduction:<a href=\"https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/\" rel=\"nofollow\">https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/</a>, seems that the TRT python INT8 calibration will generate a calibration cache in file and then be used in the later inference. But in this TFTRT, the calibration didn't generate a calibration cache.</p>\n<p>Another observation is that, I previously tried the TensorRT optimization in C++ independently, which is not integrated in the Tensorflow. At that time, the accuracy lose on INT8 mode optimization is about 10% (FP32: 70%, INT8-with-calib: 60%). Note that my C++ and Python experiments are using the same model and same calibration dataset and the same test dataset, so from my point of view, perhaps TRT INT8 Python API (at least the TF1.7 integrated version) seems somehow different from the TRT original C++ INT8 API.</p>\n<p>So let me conclude my questions:</p>\n<ol>\n<li>Is the TF integrated TRT INT8 optimization in Python has a different back-end process compared from the independent TRT INT8 optimization in C++?</li>\n<li>Could you share a hint of how does the python version INT8 calibration is realized? Shall the calibration data be generated so that can be loaded in later inference?</li>\n</ol>\n<h3>Source code / logs</h3>\n<p>Please refer to the test:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py</a></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10539540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samikama\">@samikama</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64\nTensorFlow installed from (source or binary): pip (python 2.7)\nTensorFlow version (use command below): tensorflow-gpu==1.7.0\nPython version:  python 2.7\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): gcc 5.3\nCUDA/cuDNN version: CUDA9.0, cuDNN7.0.5\nGPU model and memory: Tesla P4, 8GB\nExact command to reproduce: NA\n\nDescribe the problem\nI tried to evaluate the accuracy of TF integrated TRT INT8 optimization in Python. However, I followed the procedure as the example test: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\nbut the inference accuracy is not good (FP32: 70%, INT8-without-calib: 16%, INT8-with-calib: 35%).\nI tried to trace the code but on the API level, but I'm a little confused about how does the calibration work. looks like the function run_calibration() just created a session and accepted the calib dataset to run the session, and then the return value was abandoned.\nI checked this introduction:https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/, seems that the TRT python INT8 calibration will generate a calibration cache in file and then be used in the later inference. But in this TFTRT, the calibration didn't generate a calibration cache.\nAnother observation is that, I previously tried the TensorRT optimization in C++ independently, which is not integrated in the Tensorflow. At that time, the accuracy lose on INT8 mode optimization is about 10% (FP32: 70%, INT8-with-calib: 60%). Note that my C++ and Python experiments are using the same model and same calibration dataset and the same test dataset, so from my point of view, perhaps TRT INT8 Python API (at least the TF1.7 integrated version) seems somehow different from the TRT original C++ INT8 API.\nSo let me conclude my questions:\n\nIs the TF integrated TRT INT8 optimization in Python has a different back-end process compared from the independent TRT INT8 optimization in C++?\nCould you share a hint of how does the python version INT8 calibration is realized? Shall the calibration data be generated so that can be loaded in later inference?\n\nSource code / logs\nPlease refer to the test:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\n@samikama", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64\r\n- **TensorFlow installed from (source or binary)**: pip (python 2.7)\r\n- **TensorFlow version (use command below)**: tensorflow-gpu==1.7.0\r\n- **Python version**:  python 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.3\r\n- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5\r\n- **GPU model and memory**: Tesla P4, 8GB\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nI tried to evaluate the accuracy of TF integrated TRT INT8 optimization in Python. However, I followed the procedure as the example test: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\r\nbut the inference accuracy is not good (FP32: 70%, INT8-without-calib: 16%, INT8-with-calib: 35%).\r\n\r\nI tried to trace the code but on the API level, but I'm a little confused about how does the calibration work. looks like the function run_calibration() just created a session and accepted the calib dataset to run the session, and then the return value was abandoned. \r\n\r\nI checked this introduction:https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/, seems that the TRT python INT8 calibration will generate a calibration cache in file and then be used in the later inference. But in this TFTRT, the calibration didn't generate a calibration cache.\r\n\r\nAnother observation is that, I previously tried the TensorRT optimization in C++ independently, which is not integrated in the Tensorflow. At that time, the accuracy lose on INT8 mode optimization is about 10% (FP32: 70%, INT8-with-calib: 60%). Note that my C++ and Python experiments are using the same model and same calibration dataset and the same test dataset, so from my point of view, perhaps TRT INT8 Python API (at least the TF1.7 integrated version) seems somehow different from the TRT original C++ INT8 API. \r\n\r\nSo let me conclude my questions:\r\n1) Is the TF integrated TRT INT8 optimization in Python has a different back-end process compared from the independent TRT INT8 optimization in C++?\r\n2) Could you share a hint of how does the python version INT8 calibration is realized? Shall the calibration data be generated so that can be loaded in later inference?\r\n\r\n### Source code / logs\r\nPlease refer to the test:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\r\n\r\n@samikama "}