{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/388199640", "html_url": "https://github.com/tensorflow/tensorflow/issues/19141#issuecomment-388199640", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19141", "id": 388199640, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODE5OTY0MA==", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-10T22:02:13Z", "updated_at": "2018-05-10T22:02:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13745902\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oscarriddle\">@oscarriddle</a>,</p>\n<p>As I mentioned above, int8 engines in TF-TRT falls into partial quantization domain and requires you to do quantization aware training of your graph. It is a complicated process today and we are trying to find ways to simplify this for the users. However for advanced users or for the cases where entire graph replaced with a single TRTEngineOP, current quantization should not have too much accuracy loss, provided you can provide a diverse enough calibration set.</p>", "body_text": "@oscarriddle,\nAs I mentioned above, int8 engines in TF-TRT falls into partial quantization domain and requires you to do quantization aware training of your graph. It is a complicated process today and we are trying to find ways to simplify this for the users. However for advanced users or for the cases where entire graph replaced with a single TRTEngineOP, current quantization should not have too much accuracy loss, provided you can provide a diverse enough calibration set.", "body": "@oscarriddle,\r\n\r\nAs I mentioned above, int8 engines in TF-TRT falls into partial quantization domain and requires you to do quantization aware training of your graph. It is a complicated process today and we are trying to find ways to simplify this for the users. However for advanced users or for the cases where entire graph replaced with a single TRTEngineOP, current quantization should not have too much accuracy loss, provided you can provide a diverse enough calibration set."}