{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387700454", "html_url": "https://github.com/tensorflow/tensorflow/issues/19141#issuecomment-387700454", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19141", "id": 387700454, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzcwMDQ1NA==", "user": {"login": "oscarriddle", "id": 13745902, "node_id": "MDQ6VXNlcjEzNzQ1OTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/13745902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oscarriddle", "html_url": "https://github.com/oscarriddle", "followers_url": "https://api.github.com/users/oscarriddle/followers", "following_url": "https://api.github.com/users/oscarriddle/following{/other_user}", "gists_url": "https://api.github.com/users/oscarriddle/gists{/gist_id}", "starred_url": "https://api.github.com/users/oscarriddle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oscarriddle/subscriptions", "organizations_url": "https://api.github.com/users/oscarriddle/orgs", "repos_url": "https://api.github.com/users/oscarriddle/repos", "events_url": "https://api.github.com/users/oscarriddle/events{/privacy}", "received_events_url": "https://api.github.com/users/oscarriddle/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T10:50:24Z", "updated_at": "2018-05-10T01:56:30Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10539540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samikama\">@samikama</a> ,<br>\nThanks for your detailed reply, it indeed help clarified the answer to my questions.</p>\n<blockquote>\n<p>No, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?</p>\n</blockquote>\n<p>My model is InceptionV3 that contains about 800 nodes, and after the TFTRT optimization, nodes number shrunk to about 7~10 (varies if I set an internal nodes to be the output node).</p>\n<p>Just for now, I think the TFTRT INT8  seems still have some unaddressed issues that would cause a big sacrifice on the accuracy (Not sure whether because a higher efficient models have a higher information entropy per bit).</p>\n<p>Another plan is to just use the FP32 mode TFTRT optimization, which has no accuracy sacrifice but with 50~70% throughput boost. However, I'm wondering whether TFTRT could optimize a graph partially. For example, if I set the parameter \"outputs\" in API \"create_inference_graph()\" to be an internal node, could the downstream nodes just be reserved?</p>\n<p>I think this would be practically useful. For instance, a InceptionV3 graph with a final part like \"GlobalPool(image feature in high dim)-&gt;dropout-&gt;Conv2d_1x1(FC Layer)-&gt;SpatialSqeeze(Classification result)-&gt;Softmax\". Not only the classification result, but also the GlobalPool layer result is useful too, since it represents the feature of a image in higher dimensions.</p>\n<p>I've just give a try to get both the GlobalPool and SpatialSqeeze results in one turn. My procedure is to extract the weights of Conv2d layer from original frozen model, then optimize the original graph from start to the GlobalPool, then use numpy methods to do the dropout, Conv2d, SpatialSqeeze calculation.</p>\n<p>It's just a raw idea that I would like to share with you.</p>\n<p>Thanks,</p>", "body_text": "Hi @samikama ,\nThanks for your detailed reply, it indeed help clarified the answer to my questions.\n\nNo, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?\n\nMy model is InceptionV3 that contains about 800 nodes, and after the TFTRT optimization, nodes number shrunk to about 7~10 (varies if I set an internal nodes to be the output node).\nJust for now, I think the TFTRT INT8  seems still have some unaddressed issues that would cause a big sacrifice on the accuracy (Not sure whether because a higher efficient models have a higher information entropy per bit).\nAnother plan is to just use the FP32 mode TFTRT optimization, which has no accuracy sacrifice but with 50~70% throughput boost. However, I'm wondering whether TFTRT could optimize a graph partially. For example, if I set the parameter \"outputs\" in API \"create_inference_graph()\" to be an internal node, could the downstream nodes just be reserved?\nI think this would be practically useful. For instance, a InceptionV3 graph with a final part like \"GlobalPool(image feature in high dim)->dropout->Conv2d_1x1(FC Layer)->SpatialSqeeze(Classification result)->Softmax\". Not only the classification result, but also the GlobalPool layer result is useful too, since it represents the feature of a image in higher dimensions.\nI've just give a try to get both the GlobalPool and SpatialSqeeze results in one turn. My procedure is to extract the weights of Conv2d layer from original frozen model, then optimize the original graph from start to the GlobalPool, then use numpy methods to do the dropout, Conv2d, SpatialSqeeze calculation.\nIt's just a raw idea that I would like to share with you.\nThanks,", "body": "Hi @samikama ,\r\nThanks for your detailed reply, it indeed help clarified the answer to my questions.\r\n\r\n> No, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?\r\n\r\nMy model is InceptionV3 that contains about 800 nodes, and after the TFTRT optimization, nodes number shrunk to about 7~10 (varies if I set an internal nodes to be the output node).\r\n\r\nJust for now, I think the TFTRT INT8  seems still have some unaddressed issues that would cause a big sacrifice on the accuracy (Not sure whether because a higher efficient models have a higher information entropy per bit). \r\n\r\nAnother plan is to just use the FP32 mode TFTRT optimization, which has no accuracy sacrifice but with 50~70% throughput boost. However, I'm wondering whether TFTRT could optimize a graph partially. For example, if I set the parameter \"outputs\" in API \"create_inference_graph()\" to be an internal node, could the downstream nodes just be reserved? \r\n\r\nI think this would be practically useful. For instance, a InceptionV3 graph with a final part like \"GlobalPool(image feature in high dim)->dropout->Conv2d_1x1(FC Layer)->SpatialSqeeze(Classification result)->Softmax\". Not only the classification result, but also the GlobalPool layer result is useful too, since it represents the feature of a image in higher dimensions. \r\n\r\nI've just give a try to get both the GlobalPool and SpatialSqeeze results in one turn. My procedure is to extract the weights of Conv2d layer from original frozen model, then optimize the original graph from start to the GlobalPool, then use numpy methods to do the dropout, Conv2d, SpatialSqeeze calculation. \r\n\r\nIt's just a raw idea that I would like to share with you. \r\n\r\nThanks,\r\n\r\n\r\n"}