{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/163791554", "html_url": "https://github.com/tensorflow/tensorflow/issues/152#issuecomment-163791554", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/152", "id": 163791554, "node_id": "MDEyOklzc3VlQ29tbWVudDE2Mzc5MTU1NA==", "user": {"login": "noisychannel", "id": 807183, "node_id": "MDQ6VXNlcjgwNzE4Mw==", "avatar_url": "https://avatars3.githubusercontent.com/u/807183?v=4", "gravatar_id": "", "url": "https://api.github.com/users/noisychannel", "html_url": "https://github.com/noisychannel", "followers_url": "https://api.github.com/users/noisychannel/followers", "following_url": "https://api.github.com/users/noisychannel/following{/other_user}", "gists_url": "https://api.github.com/users/noisychannel/gists{/gist_id}", "starred_url": "https://api.github.com/users/noisychannel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/noisychannel/subscriptions", "organizations_url": "https://api.github.com/users/noisychannel/orgs", "repos_url": "https://api.github.com/users/noisychannel/repos", "events_url": "https://api.github.com/users/noisychannel/events{/privacy}", "received_events_url": "https://api.github.com/users/noisychannel/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-11T00:05:04Z", "updated_at": "2015-12-11T00:05:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a> This is still a problem when there are multiple GPUs (in exclusive mode) on the machine and some of them have processes running on them. Setting CUDA_VISIBLE_DEVICES is not ideal if you do not actually know which GPU is available. nvidia-smi will only show the current state of the GPUs and there's no way of ensuring that while you spawn your process, no other process occupied the GPU you set for use. Is there a way of polling GPUs and assigning the process to the first available, compatible GPU on the machine instead of having to manually specify the GPU id?</p>\n<p>PS: I cannot reopen this issue.</p>", "body_text": "@vrv This is still a problem when there are multiple GPUs (in exclusive mode) on the machine and some of them have processes running on them. Setting CUDA_VISIBLE_DEVICES is not ideal if you do not actually know which GPU is available. nvidia-smi will only show the current state of the GPUs and there's no way of ensuring that while you spawn your process, no other process occupied the GPU you set for use. Is there a way of polling GPUs and assigning the process to the first available, compatible GPU on the machine instead of having to manually specify the GPU id?\nPS: I cannot reopen this issue.", "body": "@vrv This is still a problem when there are multiple GPUs (in exclusive mode) on the machine and some of them have processes running on them. Setting CUDA_VISIBLE_DEVICES is not ideal if you do not actually know which GPU is available. nvidia-smi will only show the current state of the GPUs and there's no way of ensuring that while you spawn your process, no other process occupied the GPU you set for use. Is there a way of polling GPUs and assigning the process to the first available, compatible GPU on the machine instead of having to manually specify the GPU id?\n\nPS: I cannot reopen this issue.\n"}