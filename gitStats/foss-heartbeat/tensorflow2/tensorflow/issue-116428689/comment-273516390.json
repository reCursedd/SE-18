{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273516390", "html_url": "https://github.com/tensorflow/tensorflow/issues/152#issuecomment-273516390", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/152", "id": 273516390, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzUxNjM5MA==", "user": {"login": "cbquillen", "id": 22543213, "node_id": "MDQ6VXNlcjIyNTQzMjEz", "avatar_url": "https://avatars1.githubusercontent.com/u/22543213?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbquillen", "html_url": "https://github.com/cbquillen", "followers_url": "https://api.github.com/users/cbquillen/followers", "following_url": "https://api.github.com/users/cbquillen/following{/other_user}", "gists_url": "https://api.github.com/users/cbquillen/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbquillen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbquillen/subscriptions", "organizations_url": "https://api.github.com/users/cbquillen/orgs", "repos_url": "https://api.github.com/users/cbquillen/repos", "events_url": "https://api.github.com/users/cbquillen/events{/privacy}", "received_events_url": "https://api.github.com/users/cbquillen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-18T16:01:09Z", "updated_at": "2017-01-18T16:08:47Z", "author_association": "NONE", "body_html": "<p>One problem that I ran into with using both CUDA_VISIBLE_DEVICES and gpu_options.visible_device_list is that GPU numbering can be different between Nvidia NVML (and nvidia-smi for example) and what you get from cuda if you do cuDeviceGet().  On some systems, NVML and nvidia-smi apparently order the card numbers by sorting the PCI id.  <b>Cuda doesn't necessarily use the same order.</b>  This can happen on a machine where all the GPU cards are identical.</p>\n<p>The result is a crash when CUDA_VISIBILE_DEVICES is used to select a GPU.  With gpu_options.visible_device_list you get a failure opening the session for a mismatched GPU that isn't actually available.</p>\n<p>Here's the mapping I get on a particular system at work:<br>\nNVML id 0 maps to Cuda index 4<br>\nNVML id 1 maps to Cuda index 5<br>\nNVML id 2 maps to Cuda index 6<br>\nNVML id 3 maps to Cuda index 7<br>\nNVML id 4 maps to Cuda index 0<br>\nNVML id 5 maps to Cuda index 1<br>\nNVML id 6 maps to Cuda index 2<br>\nNVML id 7 maps to Cuda index 3<br>\nNVML id 8 maps to Cuda index 8<br>\nNVML id 9 maps to Cuda index 9<br>\nNVML id 10 maps to Cuda index 10<br>\nNVML id 11 maps to Cuda index 11<br>\nNVML id 12 maps to Cuda index 12<br>\nNVML id 13 maps to Cuda index 13<br>\nNVML id 14 maps to Cuda index 14<br>\nNVML id 15 maps to Cuda index 15</p>\n<p>Here is the python that generated this:</p>\n<pre><code>#!/usr/bin/env python\n\nimport pynvml\nimport os\nimport ctypes\n\ncuda = ctypes.CDLL('libcuda.so')\ncuda_device = ctypes.c_int32(0)\ncuda_pci_bus = ctypes.c_int32(0)\ncuda_pci_slot = ctypes.c_int32(0)\n\nif cuda.cuInit(ctypes.c_int32(0)) != 0:\n    raise Exception(\"Cuda failure\")\ntotal_gpus = ctypes.c_int32(0)\nif cuda.cuDeviceGetCount(ctypes.byref(total_gpus)) != 0:\n    raise Exception(\"Cuda failure\")\n\npci_map = {}\nfor gpu in range(total_gpus.value):\n    if cuda.cuDeviceGet(ctypes.byref(cuda_device), ctypes.c_int32(gpu)) != 0:\n        raise Exception(\"Cuda failure\")\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_bus), ctypes.c_int32(33), cuda_device) != 0:\n        raise Exception(\"Cuda failure\")\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_slot), ctypes.c_int32(34), cuda_device) !=0:\n        raise Exception(\"Cuda failure\")\n    # The below is correct on my system but...\n    pci_string = \"0000:%02X:%02X.0\" % (cuda_pci_bus.value, cuda_pci_slot.value)\n    pci_map[pci_string] = gpu\n\npynvml.nvmlInit()\nassert pynvml.nvmlDeviceGetCount() == total_gpus.value\n\nfor gpu in xrange(total_gpus.value):\n    h = pynvml.nvmlDeviceGetHandleByIndex(gpu)\n    bus_id = pynvml.nvmlDeviceGetPciInfo(h).busId\n    print \"NVML id {} maps to Cuda index {}\".format(gpu, pci_map[bus_id])\n\n</code></pre>\n<p>On systems like this, if nvidia-smi says a particular GPU id <i>N</i> is free, you should reserve it e.g. using gpu_options.visible_device_list using the corresponding Cuda index <i>f(N)</i> which may be different.</p>\n<p>Whether this is a bug in the way Tensorflow implements gpu_options.visible_device_list or not is debatable.</p>", "body_text": "One problem that I ran into with using both CUDA_VISIBLE_DEVICES and gpu_options.visible_device_list is that GPU numbering can be different between Nvidia NVML (and nvidia-smi for example) and what you get from cuda if you do cuDeviceGet().  On some systems, NVML and nvidia-smi apparently order the card numbers by sorting the PCI id.  Cuda doesn't necessarily use the same order.  This can happen on a machine where all the GPU cards are identical.\nThe result is a crash when CUDA_VISIBILE_DEVICES is used to select a GPU.  With gpu_options.visible_device_list you get a failure opening the session for a mismatched GPU that isn't actually available.\nHere's the mapping I get on a particular system at work:\nNVML id 0 maps to Cuda index 4\nNVML id 1 maps to Cuda index 5\nNVML id 2 maps to Cuda index 6\nNVML id 3 maps to Cuda index 7\nNVML id 4 maps to Cuda index 0\nNVML id 5 maps to Cuda index 1\nNVML id 6 maps to Cuda index 2\nNVML id 7 maps to Cuda index 3\nNVML id 8 maps to Cuda index 8\nNVML id 9 maps to Cuda index 9\nNVML id 10 maps to Cuda index 10\nNVML id 11 maps to Cuda index 11\nNVML id 12 maps to Cuda index 12\nNVML id 13 maps to Cuda index 13\nNVML id 14 maps to Cuda index 14\nNVML id 15 maps to Cuda index 15\nHere is the python that generated this:\n#!/usr/bin/env python\n\nimport pynvml\nimport os\nimport ctypes\n\ncuda = ctypes.CDLL('libcuda.so')\ncuda_device = ctypes.c_int32(0)\ncuda_pci_bus = ctypes.c_int32(0)\ncuda_pci_slot = ctypes.c_int32(0)\n\nif cuda.cuInit(ctypes.c_int32(0)) != 0:\n    raise Exception(\"Cuda failure\")\ntotal_gpus = ctypes.c_int32(0)\nif cuda.cuDeviceGetCount(ctypes.byref(total_gpus)) != 0:\n    raise Exception(\"Cuda failure\")\n\npci_map = {}\nfor gpu in range(total_gpus.value):\n    if cuda.cuDeviceGet(ctypes.byref(cuda_device), ctypes.c_int32(gpu)) != 0:\n        raise Exception(\"Cuda failure\")\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_bus), ctypes.c_int32(33), cuda_device) != 0:\n        raise Exception(\"Cuda failure\")\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_slot), ctypes.c_int32(34), cuda_device) !=0:\n        raise Exception(\"Cuda failure\")\n    # The below is correct on my system but...\n    pci_string = \"0000:%02X:%02X.0\" % (cuda_pci_bus.value, cuda_pci_slot.value)\n    pci_map[pci_string] = gpu\n\npynvml.nvmlInit()\nassert pynvml.nvmlDeviceGetCount() == total_gpus.value\n\nfor gpu in xrange(total_gpus.value):\n    h = pynvml.nvmlDeviceGetHandleByIndex(gpu)\n    bus_id = pynvml.nvmlDeviceGetPciInfo(h).busId\n    print \"NVML id {} maps to Cuda index {}\".format(gpu, pci_map[bus_id])\n\n\nOn systems like this, if nvidia-smi says a particular GPU id N is free, you should reserve it e.g. using gpu_options.visible_device_list using the corresponding Cuda index f(N) which may be different.\nWhether this is a bug in the way Tensorflow implements gpu_options.visible_device_list or not is debatable.", "body": "One problem that I ran into with using both CUDA_VISIBLE_DEVICES and gpu_options.visible_device_list is that GPU numbering can be different between Nvidia NVML (and nvidia-smi for example) and what you get from cuda if you do cuDeviceGet().  On some systems, NVML and nvidia-smi apparently order the card numbers by sorting the PCI id.  <b>Cuda doesn't necessarily use the same order.</b>  This can happen on a machine where all the GPU cards are identical.\r\n\r\nThe result is a crash when CUDA_VISIBILE_DEVICES is used to select a GPU.  With gpu_options.visible_device_list you get a failure opening the session for a mismatched GPU that isn't actually available.\r\n\r\nHere's the mapping I get on a particular system at work:\r\nNVML id 0 maps to Cuda index 4\r\nNVML id 1 maps to Cuda index 5\r\nNVML id 2 maps to Cuda index 6\r\nNVML id 3 maps to Cuda index 7\r\nNVML id 4 maps to Cuda index 0\r\nNVML id 5 maps to Cuda index 1\r\nNVML id 6 maps to Cuda index 2\r\nNVML id 7 maps to Cuda index 3\r\nNVML id 8 maps to Cuda index 8\r\nNVML id 9 maps to Cuda index 9\r\nNVML id 10 maps to Cuda index 10\r\nNVML id 11 maps to Cuda index 11\r\nNVML id 12 maps to Cuda index 12\r\nNVML id 13 maps to Cuda index 13\r\nNVML id 14 maps to Cuda index 14\r\nNVML id 15 maps to Cuda index 15\r\n\r\nHere is the python that generated this:\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nimport pynvml\r\nimport os\r\nimport ctypes\r\n\r\ncuda = ctypes.CDLL('libcuda.so')\r\ncuda_device = ctypes.c_int32(0)\r\ncuda_pci_bus = ctypes.c_int32(0)\r\ncuda_pci_slot = ctypes.c_int32(0)\r\n\r\nif cuda.cuInit(ctypes.c_int32(0)) != 0:\r\n    raise Exception(\"Cuda failure\")\r\ntotal_gpus = ctypes.c_int32(0)\r\nif cuda.cuDeviceGetCount(ctypes.byref(total_gpus)) != 0:\r\n    raise Exception(\"Cuda failure\")\r\n\r\npci_map = {}\r\nfor gpu in range(total_gpus.value):\r\n    if cuda.cuDeviceGet(ctypes.byref(cuda_device), ctypes.c_int32(gpu)) != 0:\r\n        raise Exception(\"Cuda failure\")\r\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_bus), ctypes.c_int32(33), cuda_device) != 0:\r\n        raise Exception(\"Cuda failure\")\r\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_slot), ctypes.c_int32(34), cuda_device) !=0:\r\n        raise Exception(\"Cuda failure\")\r\n    # The below is correct on my system but...\r\n    pci_string = \"0000:%02X:%02X.0\" % (cuda_pci_bus.value, cuda_pci_slot.value)\r\n    pci_map[pci_string] = gpu\r\n\r\npynvml.nvmlInit()\r\nassert pynvml.nvmlDeviceGetCount() == total_gpus.value\r\n\r\nfor gpu in xrange(total_gpus.value):\r\n    h = pynvml.nvmlDeviceGetHandleByIndex(gpu)\r\n    bus_id = pynvml.nvmlDeviceGetPciInfo(h).busId\r\n    print \"NVML id {} maps to Cuda index {}\".format(gpu, pci_map[bus_id])\r\n\r\n```\r\n\r\nOn systems like this, if nvidia-smi says a particular GPU id <i>N</i> is free, you should reserve it e.g. using gpu_options.visible_device_list using the corresponding Cuda index <i>f(N)</i> which may be different.\r\n\r\nWhether this is a bug in the way Tensorflow implements gpu_options.visible_device_list or not is debatable.\r\n"}