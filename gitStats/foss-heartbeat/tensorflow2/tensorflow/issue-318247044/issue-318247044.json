{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18911", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18911/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18911/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18911/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18911", "id": 318247044, "node_id": "MDU6SXNzdWUzMTgyNDcwNDQ=", "number": 18911, "title": "Check failed: dtype() == expected_dtype (9 vs. 3)", "user": {"login": "achalshah20", "id": 9772589, "node_id": "MDQ6VXNlcjk3NzI1ODk=", "avatar_url": "https://avatars0.githubusercontent.com/u/9772589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achalshah20", "html_url": "https://github.com/achalshah20", "followers_url": "https://api.github.com/users/achalshah20/followers", "following_url": "https://api.github.com/users/achalshah20/following{/other_user}", "gists_url": "https://api.github.com/users/achalshah20/gists{/gist_id}", "starred_url": "https://api.github.com/users/achalshah20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achalshah20/subscriptions", "organizations_url": "https://api.github.com/users/achalshah20/orgs", "repos_url": "https://api.github.com/users/achalshah20/repos", "events_url": "https://api.github.com/users/achalshah20/events{/privacy}", "received_events_url": "https://api.github.com/users/achalshah20/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-04-27T01:19:52Z", "updated_at": "2018-04-27T14:21:38Z", "closed_at": "2018-04-27T14:21:38Z", "author_association": "CONTRIBUTOR", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source (Today's version)</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.8.0-rc1-1107-g8a428cd 1.8.0-rc1</li>\n<li><strong>Python version</strong>: Python3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.12</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.1/7.1.3</li>\n<li><strong>GPU model and memory</strong>: Geforce gtx Titan X</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code> std::vector&lt;tensorflow::Tensor&gt; finalOutput;\n\n  std::string InputName  = \"inp\";\n  std::string OutputName = \"out\";\n  tensorflow::Status run_status =\n      session-&gt;Run({{InputName, input_tensor}}, {OutputName}, {}, &amp;finalOutput);\n\n  for (int y = 0; y &lt; height; y++)\n  {\n    for (int x = 0; x &lt; width; x++)\n    {\n     std::cout &lt;&lt; finalOutput[0].tensor&lt;int, 4&gt;()(0, y, x, 0); // Error: Check failed: dtype() == expected_dtype (9 vs. 3)\n    }\n  }\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I have trained network in python. I froze the model from python and writing inference model in c++. I am using above code to run the inference on frozen graph. It works as I got the tensor output but I am unable to read this tensor file by using above method.</p>\n<p><strong>Based on types.proto, I am comparing DT_INT64(frozen model) with DT_INT32(c++ inference model).</strong></p>\n<p><strong>Things I have tried:</strong></p>\n<ol>\n<li>Specify tf.int32 in argmax layer. (Last layer in frozen model) <strong>-&gt; It works</strong> (But I don't want to modify the network architecture)</li>\n<li>Instead of  finalOutput[0].tensor&lt;int, 4&gt;, I have tried  finalOutput[0].tensor&lt;long int, 4&gt;,  finalOutput[0].tensor&lt;int64_t, 4&gt; but compilation issue as eigen might not be supporting this. (Just a guess)</li>\n</ol>\n<h3>Source code / logs</h3>\n<p>Error: Check failed: dtype() == expected_dtype (9 vs. 3)</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): Source (Today's version)\nTensorFlow version (use command below): v1.8.0-rc1-1107-g8a428cd 1.8.0-rc1\nPython version: Python3\nBazel version (if compiling from source): 0.12\nGCC/Compiler version (if compiling from source): 5.4\nCUDA/cuDNN version: 9.1/7.1.3\nGPU model and memory: Geforce gtx Titan X\nExact command to reproduce:\n\n std::vector<tensorflow::Tensor> finalOutput;\n\n  std::string InputName  = \"inp\";\n  std::string OutputName = \"out\";\n  tensorflow::Status run_status =\n      session->Run({{InputName, input_tensor}}, {OutputName}, {}, &finalOutput);\n\n  for (int y = 0; y < height; y++)\n  {\n    for (int x = 0; x < width; x++)\n    {\n     std::cout << finalOutput[0].tensor<int, 4>()(0, y, x, 0); // Error: Check failed: dtype() == expected_dtype (9 vs. 3)\n    }\n  }\n\nDescribe the problem\nI have trained network in python. I froze the model from python and writing inference model in c++. I am using above code to run the inference on frozen graph. It works as I got the tensor output but I am unable to read this tensor file by using above method.\nBased on types.proto, I am comparing DT_INT64(frozen model) with DT_INT32(c++ inference model).\nThings I have tried:\n\nSpecify tf.int32 in argmax layer. (Last layer in frozen model) -> It works (But I don't want to modify the network architecture)\nInstead of  finalOutput[0].tensor<int, 4>, I have tried  finalOutput[0].tensor<long int, 4>,  finalOutput[0].tensor<int64_t, 4> but compilation issue as eigen might not be supporting this. (Just a guess)\n\nSource code / logs\nError: Check failed: dtype() == expected_dtype (9 vs. 3)", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source (Today's version)\r\n- **TensorFlow version (use command below)**: v1.8.0-rc1-1107-g8a428cd 1.8.0-rc1\r\n- **Python version**: Python3\r\n- **Bazel version (if compiling from source)**: 0.12\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.1/7.1.3\r\n- **GPU model and memory**: Geforce gtx Titan X\r\n- **Exact command to reproduce**: \r\n\r\n```\r\n std::vector<tensorflow::Tensor> finalOutput;\r\n\r\n  std::string InputName  = \"inp\";\r\n  std::string OutputName = \"out\";\r\n  tensorflow::Status run_status =\r\n      session->Run({{InputName, input_tensor}}, {OutputName}, {}, &finalOutput);\r\n\r\n  for (int y = 0; y < height; y++)\r\n  {\r\n    for (int x = 0; x < width; x++)\r\n    {\r\n     std::cout << finalOutput[0].tensor<int, 4>()(0, y, x, 0); // Error: Check failed: dtype() == expected_dtype (9 vs. 3)\r\n    }\r\n  }\r\n```\r\n\r\n### Describe the problem\r\nI have trained network in python. I froze the model from python and writing inference model in c++. I am using above code to run the inference on frozen graph. It works as I got the tensor output but I am unable to read this tensor file by using above method.\r\n\r\n**Based on types.proto, I am comparing DT_INT64(frozen model) with DT_INT32(c++ inference model).**\r\n\r\n**Things I have tried:**\r\n1. Specify tf.int32 in argmax layer. (Last layer in frozen model) **-> It works** (But I don't want to modify the network architecture)\r\n2. Instead of  finalOutput[0].tensor<int, 4>, I have tried  finalOutput[0].tensor<long int, 4>,  finalOutput[0].tensor<int64_t, 4> but compilation issue as eigen might not be supporting this. (Just a guess)\r\n\r\n### Source code / logs\r\n\r\nError: Check failed: dtype() == expected_dtype (9 vs. 3)\r\n"}