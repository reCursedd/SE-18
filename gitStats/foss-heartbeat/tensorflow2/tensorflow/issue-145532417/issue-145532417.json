{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1756", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1756/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1756/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1756/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1756", "id": 145532417, "node_id": "MDU6SXNzdWUxNDU1MzI0MTc=", "number": 1756, "title": "Ran out of memory trying to allocate.....", "user": {"login": "johnrowlay", "id": 17486673, "node_id": "MDQ6VXNlcjE3NDg2Njcz", "avatar_url": "https://avatars3.githubusercontent.com/u/17486673?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johnrowlay", "html_url": "https://github.com/johnrowlay", "followers_url": "https://api.github.com/users/johnrowlay/followers", "following_url": "https://api.github.com/users/johnrowlay/following{/other_user}", "gists_url": "https://api.github.com/users/johnrowlay/gists{/gist_id}", "starred_url": "https://api.github.com/users/johnrowlay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johnrowlay/subscriptions", "organizations_url": "https://api.github.com/users/johnrowlay/orgs", "repos_url": "https://api.github.com/users/johnrowlay/repos", "events_url": "https://api.github.com/users/johnrowlay/events{/privacy}", "received_events_url": "https://api.github.com/users/johnrowlay/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-04-03T19:26:16Z", "updated_at": "2016-04-03T20:32:52Z", "closed_at": "2016-04-03T20:32:52Z", "author_association": "NONE", "body_html": "<p>Operating System: Ubuntu 14.04<br>\nTensor Flow: 0.7.1 (install with pip, 64-bit GPU version)</p>\n<p>My issue is that Tensor Flow is telling me that it has run out of memory when constructing my Convolutional Neural Network. However, according to my calculations, my network should be able to fit well within the memory of my GPU.</p>\n<p>Here is my network structure:</p>\n<blockquote>\n<pre><code>Input image: 256x256, one channel\nConvolutional layer 1: 48 filters of size 11x11, max pool with 4x4 kernel and 4x4 stride\nConvolutional layer 2: 128 filters of size 5x5, max pool with 2x2 kernel and 2x2 stride\nConvolutional layer 3: 128 filters of size 3x3, no max pool\nFully-connected layer 4: 128 * 32 * 32 input nodes and 1024 output nodes\nFully-connected layer 5: 1024 input nodes and 10 output nodes\n</code></pre>\n</blockquote>\n<p>So, according to my calculations, the number of parameters are as follows:</p>\n<blockquote>\n<pre><code>Convolutional layer 1: 48 * 11 * 11 + 48 = 5,856\nConvolutional layer 2: 128 * 48 * 5 * 5 + 128 = 153,728\nConvolutional layer 3: 128 * 128 * 3 * 3 + 128 = 147,584\nFully-connected layer 4: 128 * 64 * 64 * 1024 + 1024 = 536,871,936\nFully-connected layer 5: 10 * 1024 + 10 = 10,250\n\n\nTotal:  537,189,354 (~ 2150 MB with float32 data)\n</code></pre>\n</blockquote>\n<p>And the memory required (number of nodes) to store one image on the network is:</p>\n<blockquote>\n<pre><code>Input: 256 * 256 = 65,536\nConvolutional layer 1: 48 * 256 * 256 = 3,145,728\nConvolutional layer 2: 128 * 64 * 64 = 524,288\nConvolutional layer 3: 128 * 32 * 32 = 131,072\nFully-connected layer 4: 1024\nFully-connected layer 5: 10\n\nTotal:  3,802,122 (~ 1.52 MB with float32 data)\n</code></pre>\n</blockquote>\n<p>So the shared parameters require 2150 MB of memory, and each image requires 1.52 MB of memory.</p>\n<p>Now, my graphics card is the NVIDA GeForce GTX 780 Ti, which has 3072 MB of memory. However, when I run the above network, even with a training minibatch size of 1, I get the following error form Tensor Flow:</p>\n<blockquote>\n<pre><code>Ran out of memory trying to allocate 600.0KiB.\nCompute status: Resource exhausted: OOM when allocating tensor with shape[5,5,48,128]\n</code></pre>\n</blockquote>\n<p>Why am I getting this error? According to my calculations, the weights for the network should be able to fit on my GPU, even without any training images.</p>\n<hr>\n<p>The full output from Tensor Flow can be found here: <a href=\"https://jpst.it/GKtY\" rel=\"nofollow\">https://jpst.it/GKtY</a></p>", "body_text": "Operating System: Ubuntu 14.04\nTensor Flow: 0.7.1 (install with pip, 64-bit GPU version)\nMy issue is that Tensor Flow is telling me that it has run out of memory when constructing my Convolutional Neural Network. However, according to my calculations, my network should be able to fit well within the memory of my GPU.\nHere is my network structure:\n\nInput image: 256x256, one channel\nConvolutional layer 1: 48 filters of size 11x11, max pool with 4x4 kernel and 4x4 stride\nConvolutional layer 2: 128 filters of size 5x5, max pool with 2x2 kernel and 2x2 stride\nConvolutional layer 3: 128 filters of size 3x3, no max pool\nFully-connected layer 4: 128 * 32 * 32 input nodes and 1024 output nodes\nFully-connected layer 5: 1024 input nodes and 10 output nodes\n\n\nSo, according to my calculations, the number of parameters are as follows:\n\nConvolutional layer 1: 48 * 11 * 11 + 48 = 5,856\nConvolutional layer 2: 128 * 48 * 5 * 5 + 128 = 153,728\nConvolutional layer 3: 128 * 128 * 3 * 3 + 128 = 147,584\nFully-connected layer 4: 128 * 64 * 64 * 1024 + 1024 = 536,871,936\nFully-connected layer 5: 10 * 1024 + 10 = 10,250\n\n\nTotal:  537,189,354 (~ 2150 MB with float32 data)\n\n\nAnd the memory required (number of nodes) to store one image on the network is:\n\nInput: 256 * 256 = 65,536\nConvolutional layer 1: 48 * 256 * 256 = 3,145,728\nConvolutional layer 2: 128 * 64 * 64 = 524,288\nConvolutional layer 3: 128 * 32 * 32 = 131,072\nFully-connected layer 4: 1024\nFully-connected layer 5: 10\n\nTotal:  3,802,122 (~ 1.52 MB with float32 data)\n\n\nSo the shared parameters require 2150 MB of memory, and each image requires 1.52 MB of memory.\nNow, my graphics card is the NVIDA GeForce GTX 780 Ti, which has 3072 MB of memory. However, when I run the above network, even with a training minibatch size of 1, I get the following error form Tensor Flow:\n\nRan out of memory trying to allocate 600.0KiB.\nCompute status: Resource exhausted: OOM when allocating tensor with shape[5,5,48,128]\n\n\nWhy am I getting this error? According to my calculations, the weights for the network should be able to fit on my GPU, even without any training images.\n\nThe full output from Tensor Flow can be found here: https://jpst.it/GKtY", "body": "Operating System: Ubuntu 14.04\nTensor Flow: 0.7.1 (install with pip, 64-bit GPU version)\n\nMy issue is that Tensor Flow is telling me that it has run out of memory when constructing my Convolutional Neural Network. However, according to my calculations, my network should be able to fit well within the memory of my GPU.\n\nHere is my network structure:\n\n> ```\n> Input image: 256x256, one channel\n> Convolutional layer 1: 48 filters of size 11x11, max pool with 4x4 kernel and 4x4 stride\n> Convolutional layer 2: 128 filters of size 5x5, max pool with 2x2 kernel and 2x2 stride\n> Convolutional layer 3: 128 filters of size 3x3, no max pool\n> Fully-connected layer 4: 128 * 32 * 32 input nodes and 1024 output nodes\n> Fully-connected layer 5: 1024 input nodes and 10 output nodes\n> ```\n\nSo, according to my calculations, the number of parameters are as follows:\n\n> ```\n> Convolutional layer 1: 48 * 11 * 11 + 48 = 5,856\n> Convolutional layer 2: 128 * 48 * 5 * 5 + 128 = 153,728\n> Convolutional layer 3: 128 * 128 * 3 * 3 + 128 = 147,584\n> Fully-connected layer 4: 128 * 64 * 64 * 1024 + 1024 = 536,871,936\n> Fully-connected layer 5: 10 * 1024 + 10 = 10,250\n> \n> \n> Total:  537,189,354 (~ 2150 MB with float32 data)\n> ```\n\nAnd the memory required (number of nodes) to store one image on the network is:\n\n> ```\n> Input: 256 * 256 = 65,536\n> Convolutional layer 1: 48 * 256 * 256 = 3,145,728\n> Convolutional layer 2: 128 * 64 * 64 = 524,288\n> Convolutional layer 3: 128 * 32 * 32 = 131,072\n> Fully-connected layer 4: 1024\n> Fully-connected layer 5: 10\n> \n> Total:  3,802,122 (~ 1.52 MB with float32 data)\n> ```\n\nSo the shared parameters require 2150 MB of memory, and each image requires 1.52 MB of memory.\n\nNow, my graphics card is the NVIDA GeForce GTX 780 Ti, which has 3072 MB of memory. However, when I run the above network, even with a training minibatch size of 1, I get the following error form Tensor Flow:\n\n> ```\n> Ran out of memory trying to allocate 600.0KiB.\n> Compute status: Resource exhausted: OOM when allocating tensor with shape[5,5,48,128]\n> ```\n\nWhy am I getting this error? According to my calculations, the weights for the network should be able to fit on my GPU, even without any training images.\n\n---\n\nThe full output from Tensor Flow can be found here: https://jpst.it/GKtY\n"}