{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12724", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12724/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12724/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12724/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12724", "id": 254269440, "node_id": "MDU6SXNzdWUyNTQyNjk0NDA=", "number": 12724, "title": "Flatten all gradients in an MLP to a tensor of rank 1 (i.e. 1D array)", "user": {"login": "namp", "id": 869726, "node_id": "MDQ6VXNlcjg2OTcyNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/869726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/namp", "html_url": "https://github.com/namp", "followers_url": "https://api.github.com/users/namp/followers", "following_url": "https://api.github.com/users/namp/following{/other_user}", "gists_url": "https://api.github.com/users/namp/gists{/gist_id}", "starred_url": "https://api.github.com/users/namp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/namp/subscriptions", "organizations_url": "https://api.github.com/users/namp/orgs", "repos_url": "https://api.github.com/users/namp/repos", "events_url": "https://api.github.com/users/namp/events{/privacy}", "received_events_url": "https://api.github.com/users/namp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-08-31T09:20:27Z", "updated_at": "2017-09-01T18:36:05Z", "closed_at": "2017-09-01T18:11:40Z", "author_association": "NONE", "body_html": "<p>Suppose the following Keras model:</p>\n<pre><code>model = Sequential()\nmodel.add(Dense(512, activation='sigmoid', input_shape=(784,)))\nmodel.add(Dense(10, activation='softmax'))\n</code></pre>\n<p>Obviously we can calculate the gradients by:</p>\n<p><code>grads = K.gradients(loss, params) </code></p>\n<p>which just calls:</p>\n<p><code>tf.gradients(loss, variables, colocate_gradients_with_ops=True) </code></p>\n<p>This returns a list of tensors containing:</p>\n<ol>\n<li>a tensor with 512x784 elements (input to hidden connections)</li>\n<li>a tensor with the biases of the 512 units in the hidden layer</li>\n<li>a tensor with 10x512 elements (hidden to output connections)</li>\n<li>a tensor with the biases of the 10 output units</li>\n</ol>\n<p>I would like to ask if there's a simple way to \"flatten\" <code>grads</code> to a single tensor of rank 1 (i.e. 1D array) with <code>(512x784)+512+(10x512)+10 </code>elements, without looping over the layers and corresponding biases.</p>\n<p>Thanks</p>", "body_text": "Suppose the following Keras model:\nmodel = Sequential()\nmodel.add(Dense(512, activation='sigmoid', input_shape=(784,)))\nmodel.add(Dense(10, activation='softmax'))\n\nObviously we can calculate the gradients by:\ngrads = K.gradients(loss, params) \nwhich just calls:\ntf.gradients(loss, variables, colocate_gradients_with_ops=True) \nThis returns a list of tensors containing:\n\na tensor with 512x784 elements (input to hidden connections)\na tensor with the biases of the 512 units in the hidden layer\na tensor with 10x512 elements (hidden to output connections)\na tensor with the biases of the 10 output units\n\nI would like to ask if there's a simple way to \"flatten\" grads to a single tensor of rank 1 (i.e. 1D array) with (512x784)+512+(10x512)+10 elements, without looping over the layers and corresponding biases.\nThanks", "body": "Suppose the following Keras model:\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(Dense(512, activation='sigmoid', input_shape=(784,)))\r\nmodel.add(Dense(10, activation='softmax'))\r\n```\r\nObviously we can calculate the gradients by:\r\n\r\n`grads = K.gradients(loss, params)\r\n` \r\n\r\nwhich just calls:\r\n\r\n`tf.gradients(loss, variables, colocate_gradients_with_ops=True)\r\n`\r\n\r\nThis returns a list of tensors containing:\r\n\r\n1) a tensor with 512x784 elements (input to hidden connections)\r\n2) a tensor with the biases of the 512 units in the hidden layer\r\n3) a tensor with 10x512 elements (hidden to output connections)\r\n4) a tensor with the biases of the 10 output units\r\n\r\nI would like to ask if there's a simple way to \"flatten\" `grads` to a single tensor of rank 1 (i.e. 1D array) with `(512x784)+512+(10x512)+10 `elements, without looping over the layers and corresponding biases.\r\n\r\nThanks"}