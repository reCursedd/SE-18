{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318852616", "html_url": "https://github.com/tensorflow/tensorflow/issues/11067#issuecomment-318852616", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11067", "id": 318852616, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODg1MjYxNg==", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-29T19:15:28Z", "updated_at": "2017-07-29T19:16:31Z", "author_association": "NONE", "body_html": "<p>I thought I'd add more information and code, in case that can help someone help me.</p>\n<p>As background, both my inputs and labeled outputs at each time step are vectors of shape <code>(4, )</code>. I run my encoder for 500 steps i.e. inputs have shape <code>(minibatch size, 500, 4)</code>, and my decoder runs for approximately 40-41 steps i.e. final output has shape <code>(minibatch size, 41, 4)</code>. Each output label depends roughly on 12 sequential inputs, so for example, the first output depends on inputs 1-12, the second output depends on inputs 13-24, etc. I don't use embeddings.</p>\n<p>I reduced my model to a single layer encoder, single layer decoder to eliminate any mistake I might be making with multi-layered architectures. The encoder is a bidirectional RNN.</p>\n<p>At the start of training, my <code>alignment_history</code> has roughly random uniform weights. Its shape is (41, minibatch size, 500) (although I could transpose it from time-major to batch-major). <code>alignment_history</code> will have values between 0.001739 and 0.002241, which makes sense - randomly initialized attention should be around 1/500 = 0.002. Additionally, my model performs at chance (25% classification accuracy).</p>\n<p>During training, my model converges to 100% classification accuracy on both training and validation data, as shown below.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8942987/28747525-c4975280-7455-11e7-8cfa-57680188c16f.png\"><img width=\"894\" alt=\"screen shot 2017-07-29 at 12 01 29 pm\" src=\"https://user-images.githubusercontent.com/8942987/28747525-c4975280-7455-11e7-8cfa-57680188c16f.png\" style=\"max-width:100%;\"></a></p>\n<p>The model never sees the same training data twice, so I'm 99% confident that the model isn't memorizing the training data. However, after training, the values of alignment_history effectively haven't changed; the values now look randomly chosen from between 0.00185 and 0.00219.</p>\n<p>My code is relatively straightforward. I have a class encapsulating my model. One method instantiates a RNN cell:</p>\n<pre><code>@staticmethod\ndef _create_lstm_cell(cell_size):\n    \"\"\"\n    Creates a RNN cell. If lstm_or_gru is True (default), create a Layer\n    Normalized LSTM cell (if layer_norm is True (default); otherwise,\n    create a vanilla LSTM cell. If lstm_or_gru is False, create a Gated\n    Recurrent Unit cell.\n    \"\"\"\n\n    if tf.flags.FLAGS.lstm_or_gru:\n        if tf.flags.FLAGS.layer_norm:\n            return LayerNormBasicLSTMCell(cell_size)\n        else:\n            return BasicLSTMCell(cell_size)\n    else:\n        return GRUCell(cell_size)\n</code></pre>\n<p>I have one method for building the encoder:</p>\n<pre><code>def _define_encoder(self):\n    \"\"\"\n    Construct an encoder RNN using a bidirectional layer.\n    \"\"\"\n\n    with tf.variable_scope('define_encoder'):\n\n        encoder_outputs, encoder_final_states = bidirectional_dynamic_rnn(\n            cell_fw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\n            cell_bw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\n            inputs=self.x,\n            dtype=tf.float32,\n            sequence_length=self.x_lengths,\n            time_major=False  # default\n        )\n\n        # concatenate forward and backwards encoder outputs\n        encoder_outputs = tf.concat(encoder_outputs, axis=-1)\n\n        # concatenate forward and backwards cell states\n        new_c = tf.concat([encoder_final_states[0].c, encoder_final_states[1].c], axis=1)\n        new_h = tf.concat([encoder_final_states[0].h, encoder_final_states[1].h], axis=1)\n        encoder_final_states = (LSTMStateTuple(c=new_c, h=new_h),)\n\n    return encoder_outputs, encoder_final_states\n</code></pre>\n<p>I similarly have another method for building the decoder:</p>\n<pre><code>def _define_decoder(self, encoder_outputs, encoder_final_states):\n    \"\"\"\n    Construct a decoder complete with an attention mechanism. The encoder's\n    final states will be used as the decoder's initial states.\n    \"\"\"\n\n\n\n    with tf.variable_scope('define_decoder'):\n        # instantiate attention mechanism\n        attention_mechanism = BahdanauAttention(num_units=DECODER_SIZE,\n                                                memory=encoder_outputs,\n                                                normalize=True)\n\n        # wrap LSTM cell with attention mechanism\n        attention_cell = AttentionWrapper(cell=self._create_lstm_cell(cell_size=DECODER_SIZE),\n                                          attention_mechanism=attention_mechanism,\n                                          # output_attention=False,  # doesn't seem to affect alignments\n                                          alignment_history=True,\n                                          attention_layer_size=DECODER_SIZE)  # arbitrarily chosen\n\n        # create initial attention state of zeros everywhere\n        decoder_initial_state = attention_cell.zero_state(batch_size=tf.flags.FLAGS.batch_size, dtype=tf.float32).clone(cell_state=encoder_final_states[0])\n\n\n        # TODO: switch this out at inference time\n        training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth\n                                         sequence_length=self.y_lengths)  # feed in sequence lengths\n\n        decoder = BasicDecoder(cell=attention_cell,\n                               helper=training_helper,\n                               initial_state=decoder_initial_state\n                               )\n\n        # run decoder over input sequence\n        decoder_outputs, decoder_final_states, decoder_final_sequence_lengths = dynamic_decode(\n            decoder=decoder,\n            maximum_iterations=41,\n            impute_finished=True)\n\n        decoder_outputs = decoder_outputs[0]\n        decoder_final_states = (decoder_final_states,)\n\n    return decoder_outputs, decoder_final_states\n</code></pre>\n<p>I use both of these methods, and then project the output of the decoder to the same dimensionality as my labels.</p>\n<pre><code>def _add_inference(self):\n    \"\"\"\n    Create a Sequence-to-Sequence model using a bidirectional encoder and an\n    attention mechanism-wrapped decoder.\n    \n    The outputs of the decoder need to be projected to a lower dimensional\n    space i.e. from DECODER_SIZE to 4.\n    \"\"\"\n\n    with tf.variable_scope('add_inference'):\n        encoder_outputs, encoder_final_states = self._define_encoder()\n        decoder_outputs, decoder_final_states = self._define_decoder(encoder_outputs, encoder_final_states)\n\n        weights = tf.Variable(tf.truncated_normal(shape=[DECODER_SIZE, 4]))\n        bias = tf.Variable(tf.truncated_normal(shape=[4]))\n        logits = tf.tensordot(decoder_outputs, weights, axes=[[2], [0]]) + bias  # 2nd dimension of decoder outputs, 0th dimension of weights\n\n    return encoder_final_states, decoder_final_states, logits\n</code></pre>", "body_text": "I thought I'd add more information and code, in case that can help someone help me.\nAs background, both my inputs and labeled outputs at each time step are vectors of shape (4, ). I run my encoder for 500 steps i.e. inputs have shape (minibatch size, 500, 4), and my decoder runs for approximately 40-41 steps i.e. final output has shape (minibatch size, 41, 4). Each output label depends roughly on 12 sequential inputs, so for example, the first output depends on inputs 1-12, the second output depends on inputs 13-24, etc. I don't use embeddings.\nI reduced my model to a single layer encoder, single layer decoder to eliminate any mistake I might be making with multi-layered architectures. The encoder is a bidirectional RNN.\nAt the start of training, my alignment_history has roughly random uniform weights. Its shape is (41, minibatch size, 500) (although I could transpose it from time-major to batch-major). alignment_history will have values between 0.001739 and 0.002241, which makes sense - randomly initialized attention should be around 1/500 = 0.002. Additionally, my model performs at chance (25% classification accuracy).\nDuring training, my model converges to 100% classification accuracy on both training and validation data, as shown below.\n\nThe model never sees the same training data twice, so I'm 99% confident that the model isn't memorizing the training data. However, after training, the values of alignment_history effectively haven't changed; the values now look randomly chosen from between 0.00185 and 0.00219.\nMy code is relatively straightforward. I have a class encapsulating my model. One method instantiates a RNN cell:\n@staticmethod\ndef _create_lstm_cell(cell_size):\n    \"\"\"\n    Creates a RNN cell. If lstm_or_gru is True (default), create a Layer\n    Normalized LSTM cell (if layer_norm is True (default); otherwise,\n    create a vanilla LSTM cell. If lstm_or_gru is False, create a Gated\n    Recurrent Unit cell.\n    \"\"\"\n\n    if tf.flags.FLAGS.lstm_or_gru:\n        if tf.flags.FLAGS.layer_norm:\n            return LayerNormBasicLSTMCell(cell_size)\n        else:\n            return BasicLSTMCell(cell_size)\n    else:\n        return GRUCell(cell_size)\n\nI have one method for building the encoder:\ndef _define_encoder(self):\n    \"\"\"\n    Construct an encoder RNN using a bidirectional layer.\n    \"\"\"\n\n    with tf.variable_scope('define_encoder'):\n\n        encoder_outputs, encoder_final_states = bidirectional_dynamic_rnn(\n            cell_fw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\n            cell_bw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\n            inputs=self.x,\n            dtype=tf.float32,\n            sequence_length=self.x_lengths,\n            time_major=False  # default\n        )\n\n        # concatenate forward and backwards encoder outputs\n        encoder_outputs = tf.concat(encoder_outputs, axis=-1)\n\n        # concatenate forward and backwards cell states\n        new_c = tf.concat([encoder_final_states[0].c, encoder_final_states[1].c], axis=1)\n        new_h = tf.concat([encoder_final_states[0].h, encoder_final_states[1].h], axis=1)\n        encoder_final_states = (LSTMStateTuple(c=new_c, h=new_h),)\n\n    return encoder_outputs, encoder_final_states\n\nI similarly have another method for building the decoder:\ndef _define_decoder(self, encoder_outputs, encoder_final_states):\n    \"\"\"\n    Construct a decoder complete with an attention mechanism. The encoder's\n    final states will be used as the decoder's initial states.\n    \"\"\"\n\n\n\n    with tf.variable_scope('define_decoder'):\n        # instantiate attention mechanism\n        attention_mechanism = BahdanauAttention(num_units=DECODER_SIZE,\n                                                memory=encoder_outputs,\n                                                normalize=True)\n\n        # wrap LSTM cell with attention mechanism\n        attention_cell = AttentionWrapper(cell=self._create_lstm_cell(cell_size=DECODER_SIZE),\n                                          attention_mechanism=attention_mechanism,\n                                          # output_attention=False,  # doesn't seem to affect alignments\n                                          alignment_history=True,\n                                          attention_layer_size=DECODER_SIZE)  # arbitrarily chosen\n\n        # create initial attention state of zeros everywhere\n        decoder_initial_state = attention_cell.zero_state(batch_size=tf.flags.FLAGS.batch_size, dtype=tf.float32).clone(cell_state=encoder_final_states[0])\n\n\n        # TODO: switch this out at inference time\n        training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth\n                                         sequence_length=self.y_lengths)  # feed in sequence lengths\n\n        decoder = BasicDecoder(cell=attention_cell,\n                               helper=training_helper,\n                               initial_state=decoder_initial_state\n                               )\n\n        # run decoder over input sequence\n        decoder_outputs, decoder_final_states, decoder_final_sequence_lengths = dynamic_decode(\n            decoder=decoder,\n            maximum_iterations=41,\n            impute_finished=True)\n\n        decoder_outputs = decoder_outputs[0]\n        decoder_final_states = (decoder_final_states,)\n\n    return decoder_outputs, decoder_final_states\n\nI use both of these methods, and then project the output of the decoder to the same dimensionality as my labels.\ndef _add_inference(self):\n    \"\"\"\n    Create a Sequence-to-Sequence model using a bidirectional encoder and an\n    attention mechanism-wrapped decoder.\n    \n    The outputs of the decoder need to be projected to a lower dimensional\n    space i.e. from DECODER_SIZE to 4.\n    \"\"\"\n\n    with tf.variable_scope('add_inference'):\n        encoder_outputs, encoder_final_states = self._define_encoder()\n        decoder_outputs, decoder_final_states = self._define_decoder(encoder_outputs, encoder_final_states)\n\n        weights = tf.Variable(tf.truncated_normal(shape=[DECODER_SIZE, 4]))\n        bias = tf.Variable(tf.truncated_normal(shape=[4]))\n        logits = tf.tensordot(decoder_outputs, weights, axes=[[2], [0]]) + bias  # 2nd dimension of decoder outputs, 0th dimension of weights\n\n    return encoder_final_states, decoder_final_states, logits", "body": "I thought I'd add more information and code, in case that can help someone help me.\r\n\r\nAs background, both my inputs and labeled outputs at each time step are vectors of shape `(4, )`. I run my encoder for 500 steps i.e. inputs have shape `(minibatch size, 500, 4)`, and my decoder runs for approximately 40-41 steps i.e. final output has shape `(minibatch size, 41, 4)`. Each output label depends roughly on 12 sequential inputs, so for example, the first output depends on inputs 1-12, the second output depends on inputs 13-24, etc. I don't use embeddings.\r\n\r\nI reduced my model to a single layer encoder, single layer decoder to eliminate any mistake I might be making with multi-layered architectures. The encoder is a bidirectional RNN.\r\n\r\nAt the start of training, my `alignment_history` has roughly random uniform weights. Its shape is (41, minibatch size, 500) (although I could transpose it from time-major to batch-major). `alignment_history` will have values between 0.001739 and 0.002241, which makes sense - randomly initialized attention should be around 1/500 = 0.002. Additionally, my model performs at chance (25% classification accuracy).\r\n\r\nDuring training, my model converges to 100% classification accuracy on both training and validation data, as shown below.\r\n\r\n<img width=\"894\" alt=\"screen shot 2017-07-29 at 12 01 29 pm\" src=\"https://user-images.githubusercontent.com/8942987/28747525-c4975280-7455-11e7-8cfa-57680188c16f.png\">\r\n\r\nThe model never sees the same training data twice, so I'm 99% confident that the model isn't memorizing the training data. However, after training, the values of alignment_history effectively haven't changed; the values now look randomly chosen from between 0.00185 and 0.00219.\r\n\r\nMy code is relatively straightforward. I have a class encapsulating my model. One method instantiates a RNN cell:\r\n\r\n    @staticmethod\r\n    def _create_lstm_cell(cell_size):\r\n        \"\"\"\r\n        Creates a RNN cell. If lstm_or_gru is True (default), create a Layer\r\n        Normalized LSTM cell (if layer_norm is True (default); otherwise,\r\n        create a vanilla LSTM cell. If lstm_or_gru is False, create a Gated\r\n        Recurrent Unit cell.\r\n        \"\"\"\r\n\r\n        if tf.flags.FLAGS.lstm_or_gru:\r\n            if tf.flags.FLAGS.layer_norm:\r\n                return LayerNormBasicLSTMCell(cell_size)\r\n            else:\r\n                return BasicLSTMCell(cell_size)\r\n        else:\r\n            return GRUCell(cell_size)\r\n\r\nI have one method for building the encoder:\r\n\r\n    def _define_encoder(self):\r\n        \"\"\"\r\n        Construct an encoder RNN using a bidirectional layer.\r\n        \"\"\"\r\n\r\n        with tf.variable_scope('define_encoder'):\r\n\r\n            encoder_outputs, encoder_final_states = bidirectional_dynamic_rnn(\r\n                cell_fw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\r\n                cell_bw=self._create_lstm_cell(ENCODER_SINGLE_DIRECTION_SIZE),\r\n                inputs=self.x,\r\n                dtype=tf.float32,\r\n                sequence_length=self.x_lengths,\r\n                time_major=False  # default\r\n            )\r\n\r\n            # concatenate forward and backwards encoder outputs\r\n            encoder_outputs = tf.concat(encoder_outputs, axis=-1)\r\n\r\n            # concatenate forward and backwards cell states\r\n            new_c = tf.concat([encoder_final_states[0].c, encoder_final_states[1].c], axis=1)\r\n            new_h = tf.concat([encoder_final_states[0].h, encoder_final_states[1].h], axis=1)\r\n            encoder_final_states = (LSTMStateTuple(c=new_c, h=new_h),)\r\n\r\n        return encoder_outputs, encoder_final_states\r\n\r\nI similarly have another method for building the decoder:\r\n\r\n    def _define_decoder(self, encoder_outputs, encoder_final_states):\r\n        \"\"\"\r\n        Construct a decoder complete with an attention mechanism. The encoder's\r\n        final states will be used as the decoder's initial states.\r\n        \"\"\"\r\n\r\n \r\n\r\n        with tf.variable_scope('define_decoder'):\r\n            # instantiate attention mechanism\r\n            attention_mechanism = BahdanauAttention(num_units=DECODER_SIZE,\r\n                                                    memory=encoder_outputs,\r\n                                                    normalize=True)\r\n\r\n            # wrap LSTM cell with attention mechanism\r\n            attention_cell = AttentionWrapper(cell=self._create_lstm_cell(cell_size=DECODER_SIZE),\r\n                                              attention_mechanism=attention_mechanism,\r\n                                              # output_attention=False,  # doesn't seem to affect alignments\r\n                                              alignment_history=True,\r\n                                              attention_layer_size=DECODER_SIZE)  # arbitrarily chosen\r\n\r\n            # create initial attention state of zeros everywhere\r\n            decoder_initial_state = attention_cell.zero_state(batch_size=tf.flags.FLAGS.batch_size, dtype=tf.float32).clone(cell_state=encoder_final_states[0])\r\n\r\n\r\n            # TODO: switch this out at inference time\r\n            training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth\r\n                                             sequence_length=self.y_lengths)  # feed in sequence lengths\r\n\r\n            decoder = BasicDecoder(cell=attention_cell,\r\n                                   helper=training_helper,\r\n                                   initial_state=decoder_initial_state\r\n                                   )\r\n\r\n            # run decoder over input sequence\r\n            decoder_outputs, decoder_final_states, decoder_final_sequence_lengths = dynamic_decode(\r\n                decoder=decoder,\r\n                maximum_iterations=41,\r\n                impute_finished=True)\r\n\r\n            decoder_outputs = decoder_outputs[0]\r\n            decoder_final_states = (decoder_final_states,)\r\n\r\n        return decoder_outputs, decoder_final_states\r\n\r\nI use both of these methods, and then project the output of the decoder to the same dimensionality as my labels.\r\n\r\n    def _add_inference(self):\r\n        \"\"\"\r\n        Create a Sequence-to-Sequence model using a bidirectional encoder and an\r\n        attention mechanism-wrapped decoder.\r\n        \r\n        The outputs of the decoder need to be projected to a lower dimensional\r\n        space i.e. from DECODER_SIZE to 4.\r\n        \"\"\"\r\n\r\n        with tf.variable_scope('add_inference'):\r\n            encoder_outputs, encoder_final_states = self._define_encoder()\r\n            decoder_outputs, decoder_final_states = self._define_decoder(encoder_outputs, encoder_final_states)\r\n\r\n            weights = tf.Variable(tf.truncated_normal(shape=[DECODER_SIZE, 4]))\r\n            bias = tf.Variable(tf.truncated_normal(shape=[4]))\r\n            logits = tf.tensordot(decoder_outputs, weights, axes=[[2], [0]]) + bias  # 2nd dimension of decoder outputs, 0th dimension of weights\r\n\r\n        return encoder_final_states, decoder_final_states, logits\r\n\r\n"}