{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309180412", "html_url": "https://github.com/tensorflow/tensorflow/issues/6929#issuecomment-309180412", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6929", "id": 309180412, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTE4MDQxMg==", "user": {"login": "MarkDaoust", "id": 1414837, "node_id": "MDQ6VXNlcjE0MTQ4Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1414837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkDaoust", "html_url": "https://github.com/MarkDaoust", "followers_url": "https://api.github.com/users/MarkDaoust/followers", "following_url": "https://api.github.com/users/MarkDaoust/following{/other_user}", "gists_url": "https://api.github.com/users/MarkDaoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkDaoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkDaoust/subscriptions", "organizations_url": "https://api.github.com/users/MarkDaoust/orgs", "repos_url": "https://api.github.com/users/MarkDaoust/repos", "events_url": "https://api.github.com/users/MarkDaoust/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkDaoust/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-17T01:12:43Z", "updated_at": "2017-06-17T01:12:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't see a problem here.</p>\n<p>Feel free to correct me if I'm wrong, but \"logits\" seems to be used throughout TensorFlow to refer to  un-normalized log probabilities like these. see [softmax_cross_entropy_with_logits] (<a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits</a>) and friends for example.</p>\n<p>In the test, <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py#L191\">line 191</a>, we see that this exactly what they're passing in. The rows start out summing to 1, then they take the log and add an arbitrary offset.</p>\n<p>I'm not familiar with the implementation details, but the first thing they do in the implementation is subtract the max (for numerical stability).</p>", "body_text": "I don't see a problem here.\nFeel free to correct me if I'm wrong, but \"logits\" seems to be used throughout TensorFlow to refer to  un-normalized log probabilities like these. see [softmax_cross_entropy_with_logits] (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) and friends for example.\nIn the test, line 191, we see that this exactly what they're passing in. The rows start out summing to 1, then they take the log and add an arbitrary offset.\nI'm not familiar with the implementation details, but the first thing they do in the implementation is subtract the max (for numerical stability).", "body": "I don't see a problem here.\r\n\r\nFeel free to correct me if I'm wrong, but \"logits\" seems to be used throughout TensorFlow to refer to  un-normalized log probabilities like these. see [softmax_cross_entropy_with_logits] (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) and friends for example.\r\n\r\nIn the test, [line 191](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py#L191), we see that this exactly what they're passing in. The rows start out summing to 1, then they take the log and add an arbitrary offset.\r\n\r\nI'm not familiar with the implementation details, but the first thing they do in the implementation is subtract the max (for numerical stability)."}