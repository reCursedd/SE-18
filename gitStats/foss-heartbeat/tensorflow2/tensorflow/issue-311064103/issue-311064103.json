{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18225", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18225/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18225/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18225/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18225", "id": 311064103, "node_id": "MDU6SXNzdWUzMTEwNjQxMDM=", "number": 18225, "title": "Distributed TensorFlow: All works use the same GPU device on host with 4 GPUs", "user": {"login": "wenruij", "id": 5120224, "node_id": "MDQ6VXNlcjUxMjAyMjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/5120224?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenruij", "html_url": "https://github.com/wenruij", "followers_url": "https://api.github.com/users/wenruij/followers", "following_url": "https://api.github.com/users/wenruij/following{/other_user}", "gists_url": "https://api.github.com/users/wenruij/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenruij/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenruij/subscriptions", "organizations_url": "https://api.github.com/users/wenruij/orgs", "repos_url": "https://api.github.com/users/wenruij/repos", "events_url": "https://api.github.com/users/wenruij/events{/privacy}", "received_events_url": "https://api.github.com/users/wenruij/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-04-04T02:32:46Z", "updated_at": "2018-04-11T20:27:25Z", "closed_at": "2018-04-11T20:27:25Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: NO</li>\n<li>**OS Platform and Distribution **: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip2.7 install</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\ntf.VERSION = 1.4.0<br>\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514<br>\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514</li>\n<li><strong>Python version</strong>: python2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: None</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: None</li>\n<li><strong>CUDA/cuDNN version</strong>:  cuda_8.0.61_375.26_linux.run/cudnn-8.0-linux-x64-v6.0.tgz</li>\n<li><strong>GPU model and memory</strong>: TITAN Xp, 12189MiB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n<li><strong>Cuda libs</strong>:<br>\n/usr/local/cuda-8.0/lib64/libcudart_static.a<br>\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>All the 4 works use the same GPU device(GPU:0) when running a distributed training on a host with 4 GPUs.</p>\n<h3>Source code / logs</h3>\n<p><strong>The distributed runner:</strong></p>\n<pre><code>START_PORT = 2222\n\ndef evaluate_on_data(estimator, test_input_fn, steps=1):\n    \"\"\"Evaluates and prints results, set data to test_data or train_data.\"\"\"\n    evaluation = estimator.evaluate(\n        input_fn=test_input_fn,\n        steps=steps)\n    print(\"Evaluation on test data:\")\n    for key in evaluation:\n        print(\"| {}: {:.4f} |\".format(key, evaluation[key]))\n\n\ndef ps_start(num_gpus):\n    with tf.device('/job:worker/task:0/device:CPU:0'):\n        worker_hosts = [\"localhost:\" + str(START_PORT + i + 1) for i in range(num_gpus)]\n        cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:\" + str(START_PORT)], \"worker\": worker_hosts})\n\n        # Create and start a server for the local task.\n        ps_server = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\n        ps_server.join()\n\n\ndef distributed_train(num_gpus,\n                      task_index,\n                      train_input_fn,\n                      test_input_fn,\n                      model_fn,\n                      **model_params):\n    worker_hosts = [\"localhost:\" + str(START_PORT + i + 1) for i in range(num_gpus)]\n    cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:\" + str(START_PORT)], \"worker\": worker_hosts})\n    server = tf.train.Server(cluster, job_name=\"worker\", task_index=task_index)\n\n    if task_index == 0:\n        tf.logging.info('hyper-params:')\n        for k, v in model_params.items():\n            tf.logging.info(\"  - \" + k + \":\\t\" + str(v))\n\n    num_steps_per_epoch = model_params[\"num_steps_per_epoch\"]\n    total_steps = num_steps_per_epoch * model_params[\"num_epochs\"]\n    evaluate_when_train = model_params.get(\"evaluate_when_train\", 0)\n    eval_steps = model_params[\"eval_steps\"]\n    model_dir = model_params[\"model_dir\"]\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d/device:GPU:%d\" % (task_index, task_index),\n            cluster=cluster)):\n\n        os.environ['TF_CONFIG'] = json.dumps(\n            {'is_chief': task_index == 0})\n\n        sess_config = tf.ConfigProto(\n            allow_soft_placement=True,\n            intra_op_parallelism_threads=0)\n        model_config = tf.contrib.learn.RunConfig(\n            master=server.target,\n            log_device_placement=False,\n            save_checkpoints_steps=2000,\n            session_config=sess_config\n        )\n\n        model_params.update({\"device_info\": \"gpu-\" + str(task_index)})\n        model = tf.estimator.Estimator(\n            model_fn=model_fn,\n            model_dir=model_dir,\n            config=model_config,\n            params=model_params\n        )\n\n        if evaluate_when_train &gt; 0:\n            steps_trained = 0\n            while steps_trained &lt; total_steps:\n                train_steps = min(num_steps_per_epoch, total_steps - steps_trained)\n                model.train(input_fn=train_input_fn, steps=train_steps)\n                steps_trained += train_steps\n                tf.logging.info(\"Trained for {} steps, total {} so far.\".format(train_steps, steps_trained))\n                # evaluation on the last device\n                if task_index == num_gpus - 1:\n                    evaluate_on_data(model, test_input_fn, steps=eval_steps)\n        else:\n            model.train(input_fn=train_input_fn, steps=total_steps)\n            # evaluation on the last device\n            if task_index == num_gpus - 1:\n                evaluate_on_data(model, test_input_fn, steps=eval_steps)\n</code></pre>\n<p><strong>Main python script to use the runner:</strong></p>\n<pre><code>def main(_):\n    if FLAGS.job_name == \"ps\":\n        ps_start(FLAGS.num_gpus)\n    else:\n        \"\"\"Train and test input_fn.\"\"\"\n        train_input_fn = functools.partial(\n            generate_sparse_batch_from_proto,\n            data_source=FLAGS.train_data,\n            batch_size=FLAGS.batch_size,\n            max_field_index=FLAGS.max_field_index,\n            mode='train'\n        )\n\n        test_input_fn = functools.partial(\n            generate_sparse_batch_from_proto,\n            data_source=FLAGS.test_data,\n            batch_size=FLAGS.eval_batch_size,\n            max_field_index=FLAGS.max_field_index,\n            mode='eval'\n            )\n\n        num_steps_per_epoch = FLAGS.num_train // FLAGS.batch_size\n        field_size = FLAGS.max_field_index + 1\n        model_params = {\n            \"dropout_keep\": 0.6,\n            \"learning_rate\": FLAGS.learning_rate,\n            \"lr_decay_rate\": FLAGS.lr_decay_rate,\n            \"l2_reg\": FLAGS.l2_reg,\n            \"field_size\": field_size,\n            \"num_steps_per_epoch\": num_steps_per_epoch,\n            \"num_epochs\": FLAGS.train_epochs,\n            \"model_dir\": FLAGS.model_dir,\n            \"eval_steps\": FLAGS.eval_steps,\n            \"evaluate_when_train\": FLAGS.evaluate_when_train\n        }\n\n        distributed_train(\n            task_index=FLAGS.task_index,\n            num_gpus=FLAGS.num_gpus,\n            train_input_fn=train_input_fn,\n            test_input_fn=test_input_fn,\n            model_fn=model_fn,\n            **model_params)\n</code></pre>\n<p><strong>Shell script to trigger the main script:</strong></p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#!</span>/usr/bin/env bash</span>\n\ngpu_index=(0 1 2 3)\n\nnum_gpus=<span class=\"pl-smi\">${<span class=\"pl-k\">#</span>gpu_index[@]}</span>\ndate=<span class=\"pl-s\"><span class=\"pl-pds\">`</span>date  +<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>%Y%m%d<span class=\"pl-pds\">\"</span></span><span class=\"pl-pds\">`</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> start ps</span>\n<span class=\"pl-k\">export</span> CUDA_VISIBLE_DEVICES=-1\nnohup python -m examples/<span class=\"pl-k\">*</span>_async --job_name=ps --num_gpus=<span class=\"pl-smi\">${num_gpus}</span> <span class=\"pl-k\">1&gt;&gt;</span>ps.<span class=\"pl-smi\">${date}</span>.log <span class=\"pl-k\">2&gt;&gt;</span>ps.<span class=\"pl-smi\">${date}</span>.log <span class=\"pl-k\">&amp;</span>\nsleep 5\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> start worker</span>\n<span class=\"pl-k\">for</span> <span class=\"pl-smi\">index</span> <span class=\"pl-k\">in</span> <span class=\"pl-smi\">${gpu_index[@]}</span>\n<span class=\"pl-k\">do</span>\n    <span class=\"pl-k\">export</span> CUDA_VISIBLE_DEVICES=<span class=\"pl-smi\">${index}</span>\n    nohup python -m examples/<span class=\"pl-k\">*</span>_async --task_index=<span class=\"pl-smi\">${index}</span> --num_gpus=<span class=\"pl-smi\">${num_gpus}</span> <span class=\"pl-k\">1&gt;&gt;</span>train.<span class=\"pl-smi\">${date}</span>.log <span class=\"pl-k\">2&gt;&gt;</span>train.<span class=\"pl-smi\">${date}</span>.log <span class=\"pl-k\">&amp;</span>\n<span class=\"pl-k\">done</span></pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\n**OS Platform and Distribution **: Ubuntu 16.04\nTensorFlow installed from (source or binary): pip2.7 install\nTensorFlow version (use command below):\ntf.VERSION = 1.4.0\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\nPython version: python2.7\nBazel version (if compiling from source): None\nGCC/Compiler version (if compiling from source): None\nCUDA/cuDNN version:  cuda_8.0.61_375.26_linux.run/cudnn-8.0-linux-x64-v6.0.tgz\nGPU model and memory: TITAN Xp, 12189MiB\nExact command to reproduce:\nCuda libs:\n/usr/local/cuda-8.0/lib64/libcudart_static.a\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n\nDescribe the problem\nAll the 4 works use the same GPU device(GPU:0) when running a distributed training on a host with 4 GPUs.\nSource code / logs\nThe distributed runner:\nSTART_PORT = 2222\n\ndef evaluate_on_data(estimator, test_input_fn, steps=1):\n    \"\"\"Evaluates and prints results, set data to test_data or train_data.\"\"\"\n    evaluation = estimator.evaluate(\n        input_fn=test_input_fn,\n        steps=steps)\n    print(\"Evaluation on test data:\")\n    for key in evaluation:\n        print(\"| {}: {:.4f} |\".format(key, evaluation[key]))\n\n\ndef ps_start(num_gpus):\n    with tf.device('/job:worker/task:0/device:CPU:0'):\n        worker_hosts = [\"localhost:\" + str(START_PORT + i + 1) for i in range(num_gpus)]\n        cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:\" + str(START_PORT)], \"worker\": worker_hosts})\n\n        # Create and start a server for the local task.\n        ps_server = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\n        ps_server.join()\n\n\ndef distributed_train(num_gpus,\n                      task_index,\n                      train_input_fn,\n                      test_input_fn,\n                      model_fn,\n                      **model_params):\n    worker_hosts = [\"localhost:\" + str(START_PORT + i + 1) for i in range(num_gpus)]\n    cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:\" + str(START_PORT)], \"worker\": worker_hosts})\n    server = tf.train.Server(cluster, job_name=\"worker\", task_index=task_index)\n\n    if task_index == 0:\n        tf.logging.info('hyper-params:')\n        for k, v in model_params.items():\n            tf.logging.info(\"  - \" + k + \":\\t\" + str(v))\n\n    num_steps_per_epoch = model_params[\"num_steps_per_epoch\"]\n    total_steps = num_steps_per_epoch * model_params[\"num_epochs\"]\n    evaluate_when_train = model_params.get(\"evaluate_when_train\", 0)\n    eval_steps = model_params[\"eval_steps\"]\n    model_dir = model_params[\"model_dir\"]\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d/device:GPU:%d\" % (task_index, task_index),\n            cluster=cluster)):\n\n        os.environ['TF_CONFIG'] = json.dumps(\n            {'is_chief': task_index == 0})\n\n        sess_config = tf.ConfigProto(\n            allow_soft_placement=True,\n            intra_op_parallelism_threads=0)\n        model_config = tf.contrib.learn.RunConfig(\n            master=server.target,\n            log_device_placement=False,\n            save_checkpoints_steps=2000,\n            session_config=sess_config\n        )\n\n        model_params.update({\"device_info\": \"gpu-\" + str(task_index)})\n        model = tf.estimator.Estimator(\n            model_fn=model_fn,\n            model_dir=model_dir,\n            config=model_config,\n            params=model_params\n        )\n\n        if evaluate_when_train > 0:\n            steps_trained = 0\n            while steps_trained < total_steps:\n                train_steps = min(num_steps_per_epoch, total_steps - steps_trained)\n                model.train(input_fn=train_input_fn, steps=train_steps)\n                steps_trained += train_steps\n                tf.logging.info(\"Trained for {} steps, total {} so far.\".format(train_steps, steps_trained))\n                # evaluation on the last device\n                if task_index == num_gpus - 1:\n                    evaluate_on_data(model, test_input_fn, steps=eval_steps)\n        else:\n            model.train(input_fn=train_input_fn, steps=total_steps)\n            # evaluation on the last device\n            if task_index == num_gpus - 1:\n                evaluate_on_data(model, test_input_fn, steps=eval_steps)\n\nMain python script to use the runner:\ndef main(_):\n    if FLAGS.job_name == \"ps\":\n        ps_start(FLAGS.num_gpus)\n    else:\n        \"\"\"Train and test input_fn.\"\"\"\n        train_input_fn = functools.partial(\n            generate_sparse_batch_from_proto,\n            data_source=FLAGS.train_data,\n            batch_size=FLAGS.batch_size,\n            max_field_index=FLAGS.max_field_index,\n            mode='train'\n        )\n\n        test_input_fn = functools.partial(\n            generate_sparse_batch_from_proto,\n            data_source=FLAGS.test_data,\n            batch_size=FLAGS.eval_batch_size,\n            max_field_index=FLAGS.max_field_index,\n            mode='eval'\n            )\n\n        num_steps_per_epoch = FLAGS.num_train // FLAGS.batch_size\n        field_size = FLAGS.max_field_index + 1\n        model_params = {\n            \"dropout_keep\": 0.6,\n            \"learning_rate\": FLAGS.learning_rate,\n            \"lr_decay_rate\": FLAGS.lr_decay_rate,\n            \"l2_reg\": FLAGS.l2_reg,\n            \"field_size\": field_size,\n            \"num_steps_per_epoch\": num_steps_per_epoch,\n            \"num_epochs\": FLAGS.train_epochs,\n            \"model_dir\": FLAGS.model_dir,\n            \"eval_steps\": FLAGS.eval_steps,\n            \"evaluate_when_train\": FLAGS.evaluate_when_train\n        }\n\n        distributed_train(\n            task_index=FLAGS.task_index,\n            num_gpus=FLAGS.num_gpus,\n            train_input_fn=train_input_fn,\n            test_input_fn=test_input_fn,\n            model_fn=model_fn,\n            **model_params)\n\nShell script to trigger the main script:\n#!/usr/bin/env bash\n\ngpu_index=(0 1 2 3)\n\nnum_gpus=${#gpu_index[@]}\ndate=`date  +\"%Y%m%d\"`\n# start ps\nexport CUDA_VISIBLE_DEVICES=-1\nnohup python -m examples/*_async --job_name=ps --num_gpus=${num_gpus} 1>>ps.${date}.log 2>>ps.${date}.log &\nsleep 5\n\n# start worker\nfor index in ${gpu_index[@]}\ndo\n    export CUDA_VISIBLE_DEVICES=${index}\n    nohup python -m examples/*_async --task_index=${index} --num_gpus=${num_gpus} 1>>train.${date}.log 2>>train.${date}.log &\ndone", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution **: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip2.7 install\r\n- **TensorFlow version (use command below)**: \r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\r\n- **Python version**: python2.7\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**:  cuda_8.0.61_375.26_linux.run/cudnn-8.0-linux-x64-v6.0.tgz\r\n- **GPU model and memory**: TITAN Xp, 12189MiB\r\n- **Exact command to reproduce**:\r\n- **Cuda libs**:\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n\r\n### Describe the problem\r\nAll the 4 works use the same GPU device(GPU:0) when running a distributed training on a host with 4 GPUs.\r\n\r\n### Source code / logs\r\n**The distributed runner:**\r\n```\r\nSTART_PORT = 2222\r\n\r\ndef evaluate_on_data(estimator, test_input_fn, steps=1):\r\n    \"\"\"Evaluates and prints results, set data to test_data or train_data.\"\"\"\r\n    evaluation = estimator.evaluate(\r\n        input_fn=test_input_fn,\r\n        steps=steps)\r\n    print(\"Evaluation on test data:\")\r\n    for key in evaluation:\r\n        print(\"| {}: {:.4f} |\".format(key, evaluation[key]))\r\n\r\n\r\ndef ps_start(num_gpus):\r\n    with tf.device('/job:worker/task:0/device:CPU:0'):\r\n        worker_hosts = [\"localhost:\" + str(START_PORT + i + 1) for i in range(num_gpus)]\r\n        cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:\" + str(START_PORT)], \"worker\": worker_hosts})\r\n\r\n        # Create and start a server for the local task.\r\n        ps_server = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\n        ps_server.join()\r\n\r\n\r\ndef distributed_train(num_gpus,\r\n                      task_index,\r\n                      train_input_fn,\r\n                      test_input_fn,\r\n                      model_fn,\r\n                      **model_params):\r\n    worker_hosts = [\"localhost:\" + str(START_PORT + i + 1) for i in range(num_gpus)]\r\n    cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:\" + str(START_PORT)], \"worker\": worker_hosts})\r\n    server = tf.train.Server(cluster, job_name=\"worker\", task_index=task_index)\r\n\r\n    if task_index == 0:\r\n        tf.logging.info('hyper-params:')\r\n        for k, v in model_params.items():\r\n            tf.logging.info(\"  - \" + k + \":\\t\" + str(v))\r\n\r\n    num_steps_per_epoch = model_params[\"num_steps_per_epoch\"]\r\n    total_steps = num_steps_per_epoch * model_params[\"num_epochs\"]\r\n    evaluate_when_train = model_params.get(\"evaluate_when_train\", 0)\r\n    eval_steps = model_params[\"eval_steps\"]\r\n    model_dir = model_params[\"model_dir\"]\r\n\r\n    # Assigns ops to the local worker by default.\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/task:%d/device:GPU:%d\" % (task_index, task_index),\r\n            cluster=cluster)):\r\n\r\n        os.environ['TF_CONFIG'] = json.dumps(\r\n            {'is_chief': task_index == 0})\r\n\r\n        sess_config = tf.ConfigProto(\r\n            allow_soft_placement=True,\r\n            intra_op_parallelism_threads=0)\r\n        model_config = tf.contrib.learn.RunConfig(\r\n            master=server.target,\r\n            log_device_placement=False,\r\n            save_checkpoints_steps=2000,\r\n            session_config=sess_config\r\n        )\r\n\r\n        model_params.update({\"device_info\": \"gpu-\" + str(task_index)})\r\n        model = tf.estimator.Estimator(\r\n            model_fn=model_fn,\r\n            model_dir=model_dir,\r\n            config=model_config,\r\n            params=model_params\r\n        )\r\n\r\n        if evaluate_when_train > 0:\r\n            steps_trained = 0\r\n            while steps_trained < total_steps:\r\n                train_steps = min(num_steps_per_epoch, total_steps - steps_trained)\r\n                model.train(input_fn=train_input_fn, steps=train_steps)\r\n                steps_trained += train_steps\r\n                tf.logging.info(\"Trained for {} steps, total {} so far.\".format(train_steps, steps_trained))\r\n                # evaluation on the last device\r\n                if task_index == num_gpus - 1:\r\n                    evaluate_on_data(model, test_input_fn, steps=eval_steps)\r\n        else:\r\n            model.train(input_fn=train_input_fn, steps=total_steps)\r\n            # evaluation on the last device\r\n            if task_index == num_gpus - 1:\r\n                evaluate_on_data(model, test_input_fn, steps=eval_steps)\r\n```\r\n\r\n**Main python script to use the runner:**\r\n```\r\ndef main(_):\r\n    if FLAGS.job_name == \"ps\":\r\n        ps_start(FLAGS.num_gpus)\r\n    else:\r\n        \"\"\"Train and test input_fn.\"\"\"\r\n        train_input_fn = functools.partial(\r\n            generate_sparse_batch_from_proto,\r\n            data_source=FLAGS.train_data,\r\n            batch_size=FLAGS.batch_size,\r\n            max_field_index=FLAGS.max_field_index,\r\n            mode='train'\r\n        )\r\n\r\n        test_input_fn = functools.partial(\r\n            generate_sparse_batch_from_proto,\r\n            data_source=FLAGS.test_data,\r\n            batch_size=FLAGS.eval_batch_size,\r\n            max_field_index=FLAGS.max_field_index,\r\n            mode='eval'\r\n            )\r\n\r\n        num_steps_per_epoch = FLAGS.num_train // FLAGS.batch_size\r\n        field_size = FLAGS.max_field_index + 1\r\n        model_params = {\r\n            \"dropout_keep\": 0.6,\r\n            \"learning_rate\": FLAGS.learning_rate,\r\n            \"lr_decay_rate\": FLAGS.lr_decay_rate,\r\n            \"l2_reg\": FLAGS.l2_reg,\r\n            \"field_size\": field_size,\r\n            \"num_steps_per_epoch\": num_steps_per_epoch,\r\n            \"num_epochs\": FLAGS.train_epochs,\r\n            \"model_dir\": FLAGS.model_dir,\r\n            \"eval_steps\": FLAGS.eval_steps,\r\n            \"evaluate_when_train\": FLAGS.evaluate_when_train\r\n        }\r\n\r\n        distributed_train(\r\n            task_index=FLAGS.task_index,\r\n            num_gpus=FLAGS.num_gpus,\r\n            train_input_fn=train_input_fn,\r\n            test_input_fn=test_input_fn,\r\n            model_fn=model_fn,\r\n            **model_params)\r\n```\r\n\r\n**Shell script to trigger the main script:**\r\n```bash\r\n#!/usr/bin/env bash\r\n\r\ngpu_index=(0 1 2 3)\r\n\r\nnum_gpus=${#gpu_index[@]}\r\ndate=`date  +\"%Y%m%d\"`\r\n# start ps\r\nexport CUDA_VISIBLE_DEVICES=-1\r\nnohup python -m examples/*_async --job_name=ps --num_gpus=${num_gpus} 1>>ps.${date}.log 2>>ps.${date}.log &\r\nsleep 5\r\n\r\n# start worker\r\nfor index in ${gpu_index[@]}\r\ndo\r\n    export CUDA_VISIBLE_DEVICES=${index}\r\n    nohup python -m examples/*_async --task_index=${index} --num_gpus=${num_gpus} 1>>train.${date}.log 2>>train.${date}.log &\r\ndone\r\n```"}