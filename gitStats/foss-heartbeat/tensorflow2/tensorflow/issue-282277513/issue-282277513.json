{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15375", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15375/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15375/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15375/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15375", "id": 282277513, "node_id": "MDU6SXNzdWUyODIyNzc1MTM=", "number": 15375, "title": "Performance  problem TF VS Keras", "user": {"login": "tahaBak", "id": 13881171, "node_id": "MDQ6VXNlcjEzODgxMTcx", "avatar_url": "https://avatars0.githubusercontent.com/u/13881171?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tahaBak", "html_url": "https://github.com/tahaBak", "followers_url": "https://api.github.com/users/tahaBak/followers", "following_url": "https://api.github.com/users/tahaBak/following{/other_user}", "gists_url": "https://api.github.com/users/tahaBak/gists{/gist_id}", "starred_url": "https://api.github.com/users/tahaBak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tahaBak/subscriptions", "organizations_url": "https://api.github.com/users/tahaBak/orgs", "repos_url": "https://api.github.com/users/tahaBak/repos", "events_url": "https://api.github.com/users/tahaBak/events{/privacy}", "received_events_url": "https://api.github.com/users/tahaBak/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-14T23:54:03Z", "updated_at": "2017-12-15T01:55:29Z", "closed_at": "2017-12-15T01:55:29Z", "author_association": "NONE", "body_html": "<p>Hello ,<br>\nI just got huge difference in results using Keras (Back-end TensorFlow) and TensorFlow. I want to know if the difference in performances is normal .</p>\n<p>The keras model produces a loss of 0.2<br>\n<code>model = k.models.Sequential() model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2))) model.add(k.layers.Dropout(0.2)) model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(k.layers.Dropout(0.2)) model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(k.layers.Dropout(0.3)) model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(k.layers.Dropout(0.3)) model.add(k.layers.Flatten()) model.add(k.layers.Dense(512)) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.Dropout(0.2)) model.add(k.layers.Dense(256)) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.Dropout(0.2)) model.add(k.layers.Dense(1)) model.add(Activation('sigmoid')) mypotim=Adam(lr=0.01, decay=0.0) model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])</code></p>\n<p>The TensorFlow normal model produces 0.7 :</p>\n<p><code>x = tf.placeholder(tf.float32, [None, 75,75,3], name=\"DNN_Input\")  learningRateIn= tf.placeholder(tf.float32) keep_prob = tf.placeholder(tf.float32) isTrainPlace=tf.placeholder(tf.bool) with tf.name_scope('conv_1'):   conv_1=tf.layers.conv2d(x,64,[3,3],activation=tf.nn.relu) batch_n1 = tf.contrib.layers.batch_norm(conv_1,center=True, scale=True, is_training=isTrainPlace, scope='bn1') mpool_1=tf.layers.max_pooling2d(batch_n1,pool_size=(2,2),strides=(2,2)) dropout_1=tf.layers.dropout(mpool_1,rate=0.8,training=isTrainPlace) with tf.name_scope('conv_2'):       conv_2=tf.layers.conv2d(dropout_1,128,[3,3],activation=tf.nn.relu) batch_n2 = tf.contrib.layers.batch_norm(conv_2, center=True, scale=True,is_training=isTrainPlace,scope='bn2') mpool_2=tf.layers.max_pooling2d(batch_n2,pool_size=(2,2),strides=(2,2)) dropout_2=tf.layers.dropout(mpool_2,rate=0.8,training=isTrainPlace) with tf.name_scope('conv_3'):       conv_3=tf.layers.conv2d(dropout_2,128,[3,3],activation=tf.nn.relu) batch_n3 = tf.contrib.layers.batch_norm(conv_3, center=True, scale=True,is_training=isTrainPlace,scope='bn3') mpool_3=tf.layers.max_pooling2d(batch_n3,pool_size=(2,2),strides=(2,2)) dropout_3=tf.layers.dropout(mpool_3,rate=0.7,training=isTrainPlace) with tf.name_scope('conv_4'):      conv_4=tf.layers.conv2d(dropout_3,64,[3,3],activation=tf.nn.relu) batch_n4 = tf.contrib.layers.batch_norm(conv_4,center=True, scale=True,is_training=isTrainPlace,scope='bn4') mpool_4=tf.layers.max_pooling2d(batch_n4,pool_size=(2,2),strides=(2,2)) dropout_4=tf.layers.dropout(mpool_4,rate=0.7,training=isTrainPlace) h4=tf.contrib.layers.flatten(dropout_4) with tf.name_scope('dense_1'):       y_dense_1=tf.layers.dense(h4,512,activation=tf.nn.relu) batch_dense_1 = tf.contrib.layers.batch_norm(y_dense_1,center=True, scale=True,is_training=isTrainPlace, scope='bn5') dropout_dense_1=tf.layers.dropout(batch_dense_1,rate=0.8,training=isTrainPlace) with tf.name_scope('dense_2'):       y_dense_2=tf.layers.dense(dropout_dense_1,256,activation=tf.nn.relu) batch_dense_2 = tf.contrib.layers.batch_norm(y_dense_2, center=True, scale=True, is_training=isTrainPlace,scope='bn6') dropout_dense_2=tf.layers.dropout(batch_dense_2 ,rate=0.8, training=isTrainPlace) y_estimated=tf.layers.dense(dropout_dense_2,2)  </code></p>\n<p>PS : the code above , is inspired from : <a href=\"url\"> https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d</a><br>\nCan anybody help me, please, to undersand , if it's normal or not ? does Keras, uses different tensorflow parameters than the default parameters of tensorflow ?<br>\nThanks in advance.<br>\nToetoe.</p>", "body_text": "Hello ,\nI just got huge difference in results using Keras (Back-end TensorFlow) and TensorFlow. I want to know if the difference in performances is normal .\nThe keras model produces a loss of 0.2\nmodel = k.models.Sequential() model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2))) model.add(k.layers.Dropout(0.2)) model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(k.layers.Dropout(0.2)) model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(k.layers.Dropout(0.3)) model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3))) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(k.layers.Dropout(0.3)) model.add(k.layers.Flatten()) model.add(k.layers.Dense(512)) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.Dropout(0.2)) model.add(k.layers.Dense(256)) model.add(Activation('relu')) model.add(BatchNormalization()) model.add(k.layers.Dropout(0.2)) model.add(k.layers.Dense(1)) model.add(Activation('sigmoid')) mypotim=Adam(lr=0.01, decay=0.0) model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])\nThe TensorFlow normal model produces 0.7 :\nx = tf.placeholder(tf.float32, [None, 75,75,3], name=\"DNN_Input\")  learningRateIn= tf.placeholder(tf.float32) keep_prob = tf.placeholder(tf.float32) isTrainPlace=tf.placeholder(tf.bool) with tf.name_scope('conv_1'):   conv_1=tf.layers.conv2d(x,64,[3,3],activation=tf.nn.relu) batch_n1 = tf.contrib.layers.batch_norm(conv_1,center=True, scale=True, is_training=isTrainPlace, scope='bn1') mpool_1=tf.layers.max_pooling2d(batch_n1,pool_size=(2,2),strides=(2,2)) dropout_1=tf.layers.dropout(mpool_1,rate=0.8,training=isTrainPlace) with tf.name_scope('conv_2'):       conv_2=tf.layers.conv2d(dropout_1,128,[3,3],activation=tf.nn.relu) batch_n2 = tf.contrib.layers.batch_norm(conv_2, center=True, scale=True,is_training=isTrainPlace,scope='bn2') mpool_2=tf.layers.max_pooling2d(batch_n2,pool_size=(2,2),strides=(2,2)) dropout_2=tf.layers.dropout(mpool_2,rate=0.8,training=isTrainPlace) with tf.name_scope('conv_3'):       conv_3=tf.layers.conv2d(dropout_2,128,[3,3],activation=tf.nn.relu) batch_n3 = tf.contrib.layers.batch_norm(conv_3, center=True, scale=True,is_training=isTrainPlace,scope='bn3') mpool_3=tf.layers.max_pooling2d(batch_n3,pool_size=(2,2),strides=(2,2)) dropout_3=tf.layers.dropout(mpool_3,rate=0.7,training=isTrainPlace) with tf.name_scope('conv_4'):      conv_4=tf.layers.conv2d(dropout_3,64,[3,3],activation=tf.nn.relu) batch_n4 = tf.contrib.layers.batch_norm(conv_4,center=True, scale=True,is_training=isTrainPlace,scope='bn4') mpool_4=tf.layers.max_pooling2d(batch_n4,pool_size=(2,2),strides=(2,2)) dropout_4=tf.layers.dropout(mpool_4,rate=0.7,training=isTrainPlace) h4=tf.contrib.layers.flatten(dropout_4) with tf.name_scope('dense_1'):       y_dense_1=tf.layers.dense(h4,512,activation=tf.nn.relu) batch_dense_1 = tf.contrib.layers.batch_norm(y_dense_1,center=True, scale=True,is_training=isTrainPlace, scope='bn5') dropout_dense_1=tf.layers.dropout(batch_dense_1,rate=0.8,training=isTrainPlace) with tf.name_scope('dense_2'):       y_dense_2=tf.layers.dense(dropout_dense_1,256,activation=tf.nn.relu) batch_dense_2 = tf.contrib.layers.batch_norm(y_dense_2, center=True, scale=True, is_training=isTrainPlace,scope='bn6') dropout_dense_2=tf.layers.dropout(batch_dense_2 ,rate=0.8, training=isTrainPlace) y_estimated=tf.layers.dense(dropout_dense_2,2)  \nPS : the code above , is inspired from :  https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d\nCan anybody help me, please, to undersand , if it's normal or not ? does Keras, uses different tensorflow parameters than the default parameters of tensorflow ?\nThanks in advance.\nToetoe.", "body": "Hello , \r\nI just got huge difference in results using Keras (Back-end TensorFlow) and TensorFlow. I want to know if the difference in performances is normal .\r\n\r\nThe keras model produces a loss of 0.2\r\n`model = k.models.Sequential()\r\nmodel.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2)))\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\nmodel.add(k.layers.Dropout(0.3))\r\nmodel.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\nmodel.add(k.layers.Dropout(0.3))\r\nmodel.add(k.layers.Flatten())\r\nmodel.add(k.layers.Dense(512))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.Dense(256))\r\nmodel.add(Activation('relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(k.layers.Dropout(0.2))\r\nmodel.add(k.layers.Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\nmypotim=Adam(lr=0.01, decay=0.0)\r\nmodel.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])`\r\n\r\nThe TensorFlow normal model produces 0.7 :\r\n\r\n`x = tf.placeholder(tf.float32, [None, 75,75,3], name=\"DNN_Input\") \r\nlearningRateIn= tf.placeholder(tf.float32)\r\nkeep_prob = tf.placeholder(tf.float32)\r\nisTrainPlace=tf.placeholder(tf.bool)\r\nwith tf.name_scope('conv_1'):  \r\n    conv_1=tf.layers.conv2d(x,64,[3,3],activation=tf.nn.relu)\r\n    batch_n1 = tf.contrib.layers.batch_norm(conv_1,center=True, scale=True, is_training=isTrainPlace, scope='bn1')\r\n    mpool_1=tf.layers.max_pooling2d(batch_n1,pool_size=(2,2),strides=(2,2))\r\n    dropout_1=tf.layers.dropout(mpool_1,rate=0.8,training=isTrainPlace)\r\nwith tf.name_scope('conv_2'):      \r\n    conv_2=tf.layers.conv2d(dropout_1,128,[3,3],activation=tf.nn.relu)\r\n    batch_n2 = tf.contrib.layers.batch_norm(conv_2, center=True, scale=True,is_training=isTrainPlace,scope='bn2')\r\n    mpool_2=tf.layers.max_pooling2d(batch_n2,pool_size=(2,2),strides=(2,2))\r\n    dropout_2=tf.layers.dropout(mpool_2,rate=0.8,training=isTrainPlace)\r\nwith tf.name_scope('conv_3'):      \r\n    conv_3=tf.layers.conv2d(dropout_2,128,[3,3],activation=tf.nn.relu)\r\n    batch_n3 = tf.contrib.layers.batch_norm(conv_3, center=True, scale=True,is_training=isTrainPlace,scope='bn3')\r\n    mpool_3=tf.layers.max_pooling2d(batch_n3,pool_size=(2,2),strides=(2,2))\r\n    dropout_3=tf.layers.dropout(mpool_3,rate=0.7,training=isTrainPlace)\r\nwith tf.name_scope('conv_4'):     \r\n    conv_4=tf.layers.conv2d(dropout_3,64,[3,3],activation=tf.nn.relu)\r\n    batch_n4 = tf.contrib.layers.batch_norm(conv_4,center=True, scale=True,is_training=isTrainPlace,scope='bn4')\r\n    mpool_4=tf.layers.max_pooling2d(batch_n4,pool_size=(2,2),strides=(2,2))\r\n    dropout_4=tf.layers.dropout(mpool_4,rate=0.7,training=isTrainPlace)\r\n    h4=tf.contrib.layers.flatten(dropout_4)\r\nwith tf.name_scope('dense_1'):      \r\n    y_dense_1=tf.layers.dense(h4,512,activation=tf.nn.relu)\r\n    batch_dense_1 = tf.contrib.layers.batch_norm(y_dense_1,center=True, scale=True,is_training=isTrainPlace, scope='bn5')\r\n    dropout_dense_1=tf.layers.dropout(batch_dense_1,rate=0.8,training=isTrainPlace)\r\nwith tf.name_scope('dense_2'):      \r\n    y_dense_2=tf.layers.dense(dropout_dense_1,256,activation=tf.nn.relu)\r\n    batch_dense_2 = tf.contrib.layers.batch_norm(y_dense_2, center=True, scale=True, is_training=isTrainPlace,scope='bn6')\r\n    dropout_dense_2=tf.layers.dropout(batch_dense_2 ,rate=0.8, training=isTrainPlace)\r\n    y_estimated=tf.layers.dense(dropout_dense_2,2)  `\r\n\r\nPS : the code above , is inspired from : [ https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d](url)\r\nCan anybody help me, please, to undersand , if it's normal or not ? does Keras, uses different tensorflow parameters than the default parameters of tensorflow ?\r\nThanks in advance.\r\nToetoe."}