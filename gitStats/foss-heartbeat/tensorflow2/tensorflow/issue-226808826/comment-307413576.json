{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307413576", "html_url": "https://github.com/tensorflow/tensorflow/issues/9731#issuecomment-307413576", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9731", "id": 307413576, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzQxMzU3Ng==", "user": {"login": "ameliajimenez", "id": 3976285, "node_id": "MDQ6VXNlcjM5NzYyODU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3976285?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ameliajimenez", "html_url": "https://github.com/ameliajimenez", "followers_url": "https://api.github.com/users/ameliajimenez/followers", "following_url": "https://api.github.com/users/ameliajimenez/following{/other_user}", "gists_url": "https://api.github.com/users/ameliajimenez/gists{/gist_id}", "starred_url": "https://api.github.com/users/ameliajimenez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ameliajimenez/subscriptions", "organizations_url": "https://api.github.com/users/ameliajimenez/orgs", "repos_url": "https://api.github.com/users/ameliajimenez/repos", "events_url": "https://api.github.com/users/ameliajimenez/events{/privacy}", "received_events_url": "https://api.github.com/users/ameliajimenez/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-09T15:03:30Z", "updated_at": "2017-06-09T15:03:30Z", "author_association": "NONE", "body_html": "<p>Hello,<br>\nI have a question about \"AdagradOptimizer\", I created a node for the learning rates to be updated but I don't see any change from the initial learning rate.</p>\n<p>This is the code for the optimizer within the Trainer object that I'm using</p>\n<p>self.learning_rate_node = tf.Variable(learning_rate)<br>\noptimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(self.net.cost, global_step=global_step)</p>\n<p>Do I have to specify something else in \"AdagradOptimizer\" for the learning rate to be updated? Do I have to define the decay that I want to use when I create \"self.learning_rate_node\"? Thanks in advance.</p>\n<p>Best regards, Amelia.</p>", "body_text": "Hello,\nI have a question about \"AdagradOptimizer\", I created a node for the learning rates to be updated but I don't see any change from the initial learning rate.\nThis is the code for the optimizer within the Trainer object that I'm using\nself.learning_rate_node = tf.Variable(learning_rate)\noptimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(self.net.cost, global_step=global_step)\nDo I have to specify something else in \"AdagradOptimizer\" for the learning rate to be updated? Do I have to define the decay that I want to use when I create \"self.learning_rate_node\"? Thanks in advance.\nBest regards, Amelia.", "body": "Hello, \r\nI have a question about \"AdagradOptimizer\", I created a node for the learning rates to be updated but I don't see any change from the initial learning rate. \r\n\r\nThis is the code for the optimizer within the Trainer object that I'm using\r\n\r\nself.learning_rate_node = tf.Variable(learning_rate)\r\noptimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(self.net.cost, global_step=global_step)\r\n\r\nDo I have to specify something else in \"AdagradOptimizer\" for the learning rate to be updated? Do I have to define the decay that I want to use when I create \"self.learning_rate_node\"? Thanks in advance.\r\n\r\nBest regards, Amelia."}