{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/410720204", "html_url": "https://github.com/tensorflow/tensorflow/issues/21410#issuecomment-410720204", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21410", "id": 410720204, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDcyMDIwNA==", "user": {"login": "olesalscheider", "id": 1006058, "node_id": "MDQ6VXNlcjEwMDYwNTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1006058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olesalscheider", "html_url": "https://github.com/olesalscheider", "followers_url": "https://api.github.com/users/olesalscheider/followers", "following_url": "https://api.github.com/users/olesalscheider/following{/other_user}", "gists_url": "https://api.github.com/users/olesalscheider/gists{/gist_id}", "starred_url": "https://api.github.com/users/olesalscheider/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olesalscheider/subscriptions", "organizations_url": "https://api.github.com/users/olesalscheider/orgs", "repos_url": "https://api.github.com/users/olesalscheider/repos", "events_url": "https://api.github.com/users/olesalscheider/events{/privacy}", "received_events_url": "https://api.github.com/users/olesalscheider/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-06T14:07:31Z", "updated_at": "2018-08-06T14:07:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>leak.py:</p>\n<pre><code>#!/usr/bin/env python3\n\nimport numpy as np\n\nimport tensorflow as tf\n\nwith tf.Graph().as_default():\n    config = tf.ConfigProto()\n    config.allow_soft_placement = True\n    session = tf.Session(config=config)\n\n    width = 2048\n    height = 1024\n    batch = 1\n    cls = 200\n    elems = batch * height * width\n\n    with tf.device('/gpu:0'):\n        x = tf.random_uniform([batch, height, width, 3], minval=0.0, maxval=200.0)\n        conv = tf.keras.layers.Conv2D(cls, [3, 3], padding='same')\n        x = conv(x)\n        x = tf.reshape(x, [-1, cls])\n\n        y = tf.random_uniform([], minval=0, maxval=cls, dtype=tf.int32)\n        def fn(y):\n            res = np.zeros([elems, cls], dtype=np.int32)\n            res[y] = 1\n            return res\n        y = tf.py_func(fn, [y], tf.int32)\n\n        z = tf.random_uniform([elems], minval=0, maxval=2, dtype=tf.int32)\n\n        mask = tf.not_equal(z, tf.constant(1))\n        x = tf.boolean_mask(x, mask)\n        y = tf.boolean_mask(y, mask)\n        y = tf.stop_gradient(y)\n\n        logits = tf.stack([-x, x], axis=-1)\n        labels = tf.stack([1 - y, y], axis=-1)\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n        loss = tf.reduce_sum(loss)\n\n        train_op = tf.train.AdamOptimizer().minimize(loss)\n\n    # Initialize global variables\n    session.run(tf.global_variables_initializer())\n\n    i = 0\n    while True:\n        i += 1\n        print(\"Iteration %d\" % i)\n        session.run(train_op)\n</code></pre>", "body_text": "leak.py:\n#!/usr/bin/env python3\n\nimport numpy as np\n\nimport tensorflow as tf\n\nwith tf.Graph().as_default():\n    config = tf.ConfigProto()\n    config.allow_soft_placement = True\n    session = tf.Session(config=config)\n\n    width = 2048\n    height = 1024\n    batch = 1\n    cls = 200\n    elems = batch * height * width\n\n    with tf.device('/gpu:0'):\n        x = tf.random_uniform([batch, height, width, 3], minval=0.0, maxval=200.0)\n        conv = tf.keras.layers.Conv2D(cls, [3, 3], padding='same')\n        x = conv(x)\n        x = tf.reshape(x, [-1, cls])\n\n        y = tf.random_uniform([], minval=0, maxval=cls, dtype=tf.int32)\n        def fn(y):\n            res = np.zeros([elems, cls], dtype=np.int32)\n            res[y] = 1\n            return res\n        y = tf.py_func(fn, [y], tf.int32)\n\n        z = tf.random_uniform([elems], minval=0, maxval=2, dtype=tf.int32)\n\n        mask = tf.not_equal(z, tf.constant(1))\n        x = tf.boolean_mask(x, mask)\n        y = tf.boolean_mask(y, mask)\n        y = tf.stop_gradient(y)\n\n        logits = tf.stack([-x, x], axis=-1)\n        labels = tf.stack([1 - y, y], axis=-1)\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n        loss = tf.reduce_sum(loss)\n\n        train_op = tf.train.AdamOptimizer().minimize(loss)\n\n    # Initialize global variables\n    session.run(tf.global_variables_initializer())\n\n    i = 0\n    while True:\n        i += 1\n        print(\"Iteration %d\" % i)\n        session.run(train_op)", "body": "leak.py:\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n    config = tf.ConfigProto()\r\n    config.allow_soft_placement = True\r\n    session = tf.Session(config=config)\r\n\r\n    width = 2048\r\n    height = 1024\r\n    batch = 1\r\n    cls = 200\r\n    elems = batch * height * width\r\n\r\n    with tf.device('/gpu:0'):\r\n        x = tf.random_uniform([batch, height, width, 3], minval=0.0, maxval=200.0)\r\n        conv = tf.keras.layers.Conv2D(cls, [3, 3], padding='same')\r\n        x = conv(x)\r\n        x = tf.reshape(x, [-1, cls])\r\n\r\n        y = tf.random_uniform([], minval=0, maxval=cls, dtype=tf.int32)\r\n        def fn(y):\r\n            res = np.zeros([elems, cls], dtype=np.int32)\r\n            res[y] = 1\r\n            return res\r\n        y = tf.py_func(fn, [y], tf.int32)\r\n\r\n        z = tf.random_uniform([elems], minval=0, maxval=2, dtype=tf.int32)\r\n\r\n        mask = tf.not_equal(z, tf.constant(1))\r\n        x = tf.boolean_mask(x, mask)\r\n        y = tf.boolean_mask(y, mask)\r\n        y = tf.stop_gradient(y)\r\n\r\n        logits = tf.stack([-x, x], axis=-1)\r\n        labels = tf.stack([1 - y, y], axis=-1)\r\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\r\n        loss = tf.reduce_sum(loss)\r\n\r\n        train_op = tf.train.AdamOptimizer().minimize(loss)\r\n\r\n    # Initialize global variables\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    i = 0\r\n    while True:\r\n        i += 1\r\n        print(\"Iteration %d\" % i)\r\n        session.run(train_op)\r\n```"}