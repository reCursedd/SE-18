{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14144", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14144/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14144/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14144/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14144", "id": 270163718, "node_id": "MDU6SXNzdWUyNzAxNjM3MTg=", "number": 14144, "title": "Eager: Device Placement of Constant Eager Tensors", "user": {"login": "iganichev", "id": 9123400, "node_id": "MDQ6VXNlcjkxMjM0MDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9123400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iganichev", "html_url": "https://github.com/iganichev", "followers_url": "https://api.github.com/users/iganichev/followers", "following_url": "https://api.github.com/users/iganichev/following{/other_user}", "gists_url": "https://api.github.com/users/iganichev/gists{/gist_id}", "starred_url": "https://api.github.com/users/iganichev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iganichev/subscriptions", "organizations_url": "https://api.github.com/users/iganichev/orgs", "repos_url": "https://api.github.com/users/iganichev/repos", "events_url": "https://api.github.com/users/iganichev/events{/privacy}", "received_events_url": "https://api.github.com/users/iganichev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 736653459, "node_id": "MDU6TGFiZWw3MzY2NTM0NTk=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:eager", "name": "comp:eager", "color": "0052cc", "default": false}, {"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-11-01T01:05:55Z", "updated_at": "2018-05-30T01:33:29Z", "closed_at": "2018-05-30T01:33:29Z", "author_association": "MEMBER", "body_html": "<p>When eager execution is enabled on 64bit machine, the following code snippet causes an error:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  tf.split(np.array([<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>]), np.array([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]))<span class=\"pl-bu\">`</span></pre></div>\n<pre><code>Tensors on conflicting devices: cannot compute SplitV as input #1 was expected to be on \n/job:localhost/replica:0/task:0/device:CPU:0 but is actually on \n/job:localhost/replica:0/task:0/device:GPU:0 (operation running on \n/job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(),\nor transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT).\nCopying tensors between devices may slow down your model [Op:SplitV] name: split\n</code></pre>\n<p>There are a couple of ways to fix this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  tf.split(np.array([<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>]), np.array([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32))<span class=\"pl-bu\">`</span></pre></div>\n<p>or</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  tf.split(np.array([<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>]), [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>])<span class=\"pl-bu\">`</span></pre></div>\n<p>The error is thrown for the following reason. Kernel of operation <code>split</code> expects the <code>num_or_size_splits</code> argument to be in host memory (CPU RAM). For a tensor to be placed in host memory, it needs to be either created in the CPU context (e.g. <code>with tf.device(\"/cpu:0\")</code>) or have dtype of <code>int32</code>.  <code>numpy</code> defaults to creating <code>int64</code> tensors on 64bit machines. Hence the fixes - either explicitly request <code>int32</code> dtype from numpy or use python list (tensorflow defaults to <code>int32</code> for python sequences of small integers)</p>\n<p>Another, more global, workaround is to enable automatic copying of tensors between devices as described in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"270037765\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/14133\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/14133/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/14133\">#14133</a>..</p>", "body_text": "When eager execution is enabled on 64bit machine, the following code snippet causes an error:\nwith tf.device(\"/gpu:0\"):\n  tf.split(np.array([1.0, 2.0]), np.array([1, 1]))`\nTensors on conflicting devices: cannot compute SplitV as input #1 was expected to be on \n/job:localhost/replica:0/task:0/device:CPU:0 but is actually on \n/job:localhost/replica:0/task:0/device:GPU:0 (operation running on \n/job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(),\nor transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT).\nCopying tensors between devices may slow down your model [Op:SplitV] name: split\n\nThere are a couple of ways to fix this:\nwith tf.device(\"/gpu:0\"):\n  tf.split(np.array([1.0, 2.0]), np.array([1, 1], dtype=np.int32))`\nor\nwith tf.device(\"/gpu:0\"):\n  tf.split(np.array([1.0, 2.0]), [1, 1])`\nThe error is thrown for the following reason. Kernel of operation split expects the num_or_size_splits argument to be in host memory (CPU RAM). For a tensor to be placed in host memory, it needs to be either created in the CPU context (e.g. with tf.device(\"/cpu:0\")) or have dtype of int32.  numpy defaults to creating int64 tensors on 64bit machines. Hence the fixes - either explicitly request int32 dtype from numpy or use python list (tensorflow defaults to int32 for python sequences of small integers)\nAnother, more global, workaround is to enable automatic copying of tensors between devices as described in #14133..", "body": "When eager execution is enabled on 64bit machine, the following code snippet causes an error:\r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  tf.split(np.array([1.0, 2.0]), np.array([1, 1]))`\r\n```\r\n```\r\nTensors on conflicting devices: cannot compute SplitV as input #1 was expected to be on \r\n/job:localhost/replica:0/task:0/device:CPU:0 but is actually on \r\n/job:localhost/replica:0/task:0/device:GPU:0 (operation running on \r\n/job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(),\r\nor transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT).\r\nCopying tensors between devices may slow down your model [Op:SplitV] name: split\r\n```\r\nThere are a couple of ways to fix this:\r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  tf.split(np.array([1.0, 2.0]), np.array([1, 1], dtype=np.int32))`\r\n```\r\nor \r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  tf.split(np.array([1.0, 2.0]), [1, 1])`\r\n```\r\n\r\nThe error is thrown for the following reason. Kernel of operation `split` expects the `num_or_size_splits` argument to be in host memory (CPU RAM). For a tensor to be placed in host memory, it needs to be either created in the CPU context (e.g. `with tf.device(\"/cpu:0\")`) or have dtype of `int32`.  `numpy` defaults to creating `int64` tensors on 64bit machines. Hence the fixes - either explicitly request `int32` dtype from numpy or use python list (tensorflow defaults to `int32` for python sequences of small integers)\r\n\r\nAnother, more global, workaround is to enable automatic copying of tensors between devices as described in https://github.com/tensorflow/tensorflow/issues/14133.."}