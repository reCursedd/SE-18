{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417522832", "html_url": "https://github.com/tensorflow/tensorflow/issues/20067#issuecomment-417522832", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20067", "id": 417522832, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzUyMjgzMg==", "user": {"login": "yoonforh", "id": 1460967, "node_id": "MDQ6VXNlcjE0NjA5Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1460967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yoonforh", "html_url": "https://github.com/yoonforh", "followers_url": "https://api.github.com/users/yoonforh/followers", "following_url": "https://api.github.com/users/yoonforh/following{/other_user}", "gists_url": "https://api.github.com/users/yoonforh/gists{/gist_id}", "starred_url": "https://api.github.com/users/yoonforh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yoonforh/subscriptions", "organizations_url": "https://api.github.com/users/yoonforh/orgs", "repos_url": "https://api.github.com/users/yoonforh/repos", "events_url": "https://api.github.com/users/yoonforh/events{/privacy}", "received_events_url": "https://api.github.com/users/yoonforh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-31T01:42:49Z", "updated_at": "2018-08-31T01:44:32Z", "author_association": "NONE", "body_html": "<p>Hi, there. I also met the same problem with CuDNN GRU.</p>\n<p>I know that working example of cudnn lstm exists (below link), and modified it to use gru instead of lstm.<br>\n<a href=\"https://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d\">https://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d</a></p>\n<pre><code>// tested gru version\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport tensorflow as tf\n\n\nshape = [2, 2, 2]\nn_cell_dim = 2\n\nclass TestCuDNN(object) :\n    def init_vars(self, sess):\n      sess.run(tf.global_variables_initializer())\n\n\n    def train_graph(self):\n      with tf.Graph().as_default(), tf.device('/gpu:0'):\n        with tf.Session() as sess :\n          is_training = True\n\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\n\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\n              gru = tf.contrib.cudnn_rnn.CudnnGRU(\n                  num_layers=1,\n                  num_units=n_cell_dim,\n                  direction='bidirectional',\n                  dtype=tf.float32)\n              gru.build(inputs.get_shape())\n              outputs, output_states = gru(inputs, training=is_training)\n\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\n              gru2 = tf.contrib.cudnn_rnn.CudnnGRU(\n                  num_layers=1,\n                  num_units=n_cell_dim,\n                  direction='bidirectional',\n                  dtype=tf.float32)\n              gru2.build(inputs.get_shape())\n              outputs2, output_states2 = gru2(inputs, training=is_training)\n\n          with tf.device('/cpu:0'):\n            saver = tf.train.Saver()\n\n          self.dump_graph('before save')\n        \n          self.init_vars(sess)\n          saver.save(sess, '/tmp/cudnn_gru_test')\n\n\n    def inf_graph(self):\n      with tf.Graph().as_default(), tf.device('/cpu:0'):\n        with tf.Session() as sess:\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\n                outputs = self.do_inference(inputs)\n\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\n                outputs2 = self.do_inference(inputs)\n                \n          # debug graphs\n          g = tf.get_default_graph()\n          print('default tf graph :', g)\n          keys = g.get_all_collection_keys()\n          print('current name scope :', g.get_name_scope())\n          for key in keys :\n                print('all graph (', key, ')  :', g.get_collection(key))\n          print('current name scope :', g.get_name_scope())\n            \n          reader = tf.train.NewCheckpointReader('/tmp/cudnn_test')\n          for var_name in reader.get_variable_to_shape_map() :\n                print(var_name)\n   \n                \n          saver = tf.train.Saver()\n\n          saver.restore(sess, '/tmp/cudnn_gru_test')\n          print(sess.run([outputs, outputs2]))\n\n    def do_inference(self, inputs) :\n      single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(\n          n_cell_dim, reuse=tf.get_variable_scope().reuse)\n\n      gru_fw_cell = [single_cell() for _ in range(1)]\n      print('gru fw cell graph :', gru_fw_cell[0].graph)\n      gru_bw_cell = [single_cell() for _ in range(1)]\n      (outputs, output_state_fw,\n       output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n           gru_fw_cell,\n           gru_bw_cell,\n           inputs,\n           dtype=tf.float32,\n           time_major=True)\n      return outputs\n\n    def dump_graph(self, where) :\n        print('')\n\n        print('--- dumping tensorflow graph [', where, '] ---')\n        g = tf.get_default_graph()\n        print('default tf graph :', g)\n\n        # debug graphs\n        keys = g.get_all_collection_keys()\n        print('current name scope :', g.get_name_scope())\n        for key in keys :\n            print('all graph (', key, ')  :', g.get_collection(key))\n        print('') \n        print('')\n    \n    def main(self):\n        with tf.Graph().as_default() :\n          print('default graph :', tf.get_default_graph())\n          self.train_graph()\n        with tf.Graph().as_default() :\n          print('default graph :', tf.get_default_graph())\n          self.inf_graph()\n        print('finishes')  \n\n\n</code></pre>\n<p>The result is as follows :</p>\n<p>default graph : &lt;tensorflow.python.framework.ops.Graph object at 0x000001D9A04742E8&gt;</p>\n<p>--- dumping tensorflow graph [ before save ] ---<br>\ndefault tf graph : &lt;tensorflow.python.framework.ops.Graph object at 0x000001D9A0474518&gt;<br>\ncurrent name scope :<br>\nall graph ( trainable_variables )  : [&lt;tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref&gt;, &lt;tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref&gt;]<br>\nall graph ( variables )  : [&lt;tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref&gt;, &lt;tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref&gt;]<br>\nall graph ( saveable_objects )  : [&lt;tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D9E5472390&gt;, &lt;tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D99E51F550&gt;]<br>\nall graph ( update_ops )  : []</p>\n<hr>\n<p>UnknownError                              Traceback (most recent call last)<br>\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)<br>\n1277     try:<br>\n-&gt; 1278       return fn(*args)<br>\n1279     except errors.OpError as e:</p>\n<p>~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)<br>\n1262       return self._call_tf_sessionrun(<br>\n-&gt; 1263           options, feed_dict, fetch_list, target_list, run_metadata)<br>\n1264</p>\n<p>~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)<br>\n1349         self._session, options, feed_dict, fetch_list, target_list,<br>\n-&gt; 1350         run_metadata)<br>\n1351</p>\n<p>UnknownError: Fail to find the dnn implementation.<br>\n[[Node: A/cudnn_gru_1/CudnnRNNCanonicalToParams = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=12, rnn_mode=\"gru\", seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_layers, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/random_uniform, A/cudnn_gru_1/random_uniform_1, A/cudnn_gru_1/random_uniform_2, A/cudnn_gru_1/random_uniform_3, A/cudnn_gru_1/random_uniform_4, A/cudnn_gru_1/random_uniform_5, A/cudnn_gru_1/random_uniform_6, A/cudnn_gru_1/random_uniform_7, A/cudnn_gru_1/random_uniform_8, A/cudnn_gru_1/random_uniform_9, A/cudnn_gru_1/random_uniform_10, A/cudnn_gru_1/random_uniform_11, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const)]]</p>\n<p>During handling of the above exception, another exception occurred:<br>\n...</p>", "body_text": "Hi, there. I also met the same problem with CuDNN GRU.\nI know that working example of cudnn lstm exists (below link), and modified it to use gru instead of lstm.\nhttps://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d\n// tested gru version\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport tensorflow as tf\n\n\nshape = [2, 2, 2]\nn_cell_dim = 2\n\nclass TestCuDNN(object) :\n    def init_vars(self, sess):\n      sess.run(tf.global_variables_initializer())\n\n\n    def train_graph(self):\n      with tf.Graph().as_default(), tf.device('/gpu:0'):\n        with tf.Session() as sess :\n          is_training = True\n\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\n\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\n              gru = tf.contrib.cudnn_rnn.CudnnGRU(\n                  num_layers=1,\n                  num_units=n_cell_dim,\n                  direction='bidirectional',\n                  dtype=tf.float32)\n              gru.build(inputs.get_shape())\n              outputs, output_states = gru(inputs, training=is_training)\n\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\n              gru2 = tf.contrib.cudnn_rnn.CudnnGRU(\n                  num_layers=1,\n                  num_units=n_cell_dim,\n                  direction='bidirectional',\n                  dtype=tf.float32)\n              gru2.build(inputs.get_shape())\n              outputs2, output_states2 = gru2(inputs, training=is_training)\n\n          with tf.device('/cpu:0'):\n            saver = tf.train.Saver()\n\n          self.dump_graph('before save')\n        \n          self.init_vars(sess)\n          saver.save(sess, '/tmp/cudnn_gru_test')\n\n\n    def inf_graph(self):\n      with tf.Graph().as_default(), tf.device('/cpu:0'):\n        with tf.Session() as sess:\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\n                outputs = self.do_inference(inputs)\n\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\n                outputs2 = self.do_inference(inputs)\n                \n          # debug graphs\n          g = tf.get_default_graph()\n          print('default tf graph :', g)\n          keys = g.get_all_collection_keys()\n          print('current name scope :', g.get_name_scope())\n          for key in keys :\n                print('all graph (', key, ')  :', g.get_collection(key))\n          print('current name scope :', g.get_name_scope())\n            \n          reader = tf.train.NewCheckpointReader('/tmp/cudnn_test')\n          for var_name in reader.get_variable_to_shape_map() :\n                print(var_name)\n   \n                \n          saver = tf.train.Saver()\n\n          saver.restore(sess, '/tmp/cudnn_gru_test')\n          print(sess.run([outputs, outputs2]))\n\n    def do_inference(self, inputs) :\n      single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(\n          n_cell_dim, reuse=tf.get_variable_scope().reuse)\n\n      gru_fw_cell = [single_cell() for _ in range(1)]\n      print('gru fw cell graph :', gru_fw_cell[0].graph)\n      gru_bw_cell = [single_cell() for _ in range(1)]\n      (outputs, output_state_fw,\n       output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n           gru_fw_cell,\n           gru_bw_cell,\n           inputs,\n           dtype=tf.float32,\n           time_major=True)\n      return outputs\n\n    def dump_graph(self, where) :\n        print('')\n\n        print('--- dumping tensorflow graph [', where, '] ---')\n        g = tf.get_default_graph()\n        print('default tf graph :', g)\n\n        # debug graphs\n        keys = g.get_all_collection_keys()\n        print('current name scope :', g.get_name_scope())\n        for key in keys :\n            print('all graph (', key, ')  :', g.get_collection(key))\n        print('') \n        print('')\n    \n    def main(self):\n        with tf.Graph().as_default() :\n          print('default graph :', tf.get_default_graph())\n          self.train_graph()\n        with tf.Graph().as_default() :\n          print('default graph :', tf.get_default_graph())\n          self.inf_graph()\n        print('finishes')  \n\n\n\nThe result is as follows :\ndefault graph : <tensorflow.python.framework.ops.Graph object at 0x000001D9A04742E8>\n--- dumping tensorflow graph [ before save ] ---\ndefault tf graph : <tensorflow.python.framework.ops.Graph object at 0x000001D9A0474518>\ncurrent name scope :\nall graph ( trainable_variables )  : [<tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref>, <tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref>]\nall graph ( variables )  : [<tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref>, <tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape= dtype=float32_ref>]\nall graph ( saveable_objects )  : [<tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D9E5472390>, <tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D99E51F550>]\nall graph ( update_ops )  : []\n\nUnknownError                              Traceback (most recent call last)\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\n1277     try:\n-> 1278       return fn(*args)\n1279     except errors.OpError as e:\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\n1262       return self._call_tf_sessionrun(\n-> 1263           options, feed_dict, fetch_list, target_list, run_metadata)\n1264\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\n1349         self._session, options, feed_dict, fetch_list, target_list,\n-> 1350         run_metadata)\n1351\nUnknownError: Fail to find the dnn implementation.\n[[Node: A/cudnn_gru_1/CudnnRNNCanonicalToParams = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=12, rnn_mode=\"gru\", seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_layers, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/random_uniform, A/cudnn_gru_1/random_uniform_1, A/cudnn_gru_1/random_uniform_2, A/cudnn_gru_1/random_uniform_3, A/cudnn_gru_1/random_uniform_4, A/cudnn_gru_1/random_uniform_5, A/cudnn_gru_1/random_uniform_6, A/cudnn_gru_1/random_uniform_7, A/cudnn_gru_1/random_uniform_8, A/cudnn_gru_1/random_uniform_9, A/cudnn_gru_1/random_uniform_10, A/cudnn_gru_1/random_uniform_11, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const)]]\nDuring handling of the above exception, another exception occurred:\n...", "body": "Hi, there. I also met the same problem with CuDNN GRU.\r\n\r\nI know that working example of cudnn lstm exists (below link), and modified it to use gru instead of lstm.\r\nhttps://gist.github.com/protoget/9b45881f23c96e201a90581c8f4b692d\r\n\r\n```\r\n// tested gru version\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nshape = [2, 2, 2]\r\nn_cell_dim = 2\r\n\r\nclass TestCuDNN(object) :\r\n    def init_vars(self, sess):\r\n      sess.run(tf.global_variables_initializer())\r\n\r\n\r\n    def train_graph(self):\r\n      with tf.Graph().as_default(), tf.device('/gpu:0'):\r\n        with tf.Session() as sess :\r\n          is_training = True\r\n\r\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\r\n\r\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\r\n              gru = tf.contrib.cudnn_rnn.CudnnGRU(\r\n                  num_layers=1,\r\n                  num_units=n_cell_dim,\r\n                  direction='bidirectional',\r\n                  dtype=tf.float32)\r\n              gru.build(inputs.get_shape())\r\n              outputs, output_states = gru(inputs, training=is_training)\r\n\r\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\r\n              gru2 = tf.contrib.cudnn_rnn.CudnnGRU(\r\n                  num_layers=1,\r\n                  num_units=n_cell_dim,\r\n                  direction='bidirectional',\r\n                  dtype=tf.float32)\r\n              gru2.build(inputs.get_shape())\r\n              outputs2, output_states2 = gru2(inputs, training=is_training)\r\n\r\n          with tf.device('/cpu:0'):\r\n            saver = tf.train.Saver()\r\n\r\n          self.dump_graph('before save')\r\n        \r\n          self.init_vars(sess)\r\n          saver.save(sess, '/tmp/cudnn_gru_test')\r\n\r\n\r\n    def inf_graph(self):\r\n      with tf.Graph().as_default(), tf.device('/cpu:0'):\r\n        with tf.Session() as sess:\r\n          inputs = tf.random_uniform(shape, dtype=tf.float32)\r\n          with tf.variable_scope('A', reuse=tf.AUTO_REUSE) as scope:\r\n                outputs = self.do_inference(inputs)\r\n\r\n          with tf.variable_scope('B', reuse=tf.AUTO_REUSE) as scope:\r\n                outputs2 = self.do_inference(inputs)\r\n                \r\n          # debug graphs\r\n          g = tf.get_default_graph()\r\n          print('default tf graph :', g)\r\n          keys = g.get_all_collection_keys()\r\n          print('current name scope :', g.get_name_scope())\r\n          for key in keys :\r\n                print('all graph (', key, ')  :', g.get_collection(key))\r\n          print('current name scope :', g.get_name_scope())\r\n            \r\n          reader = tf.train.NewCheckpointReader('/tmp/cudnn_test')\r\n          for var_name in reader.get_variable_to_shape_map() :\r\n                print(var_name)\r\n   \r\n                \r\n          saver = tf.train.Saver()\r\n\r\n          saver.restore(sess, '/tmp/cudnn_gru_test')\r\n          print(sess.run([outputs, outputs2]))\r\n\r\n    def do_inference(self, inputs) :\r\n      single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(\r\n          n_cell_dim, reuse=tf.get_variable_scope().reuse)\r\n\r\n      gru_fw_cell = [single_cell() for _ in range(1)]\r\n      print('gru fw cell graph :', gru_fw_cell[0].graph)\r\n      gru_bw_cell = [single_cell() for _ in range(1)]\r\n      (outputs, output_state_fw,\r\n       output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\r\n           gru_fw_cell,\r\n           gru_bw_cell,\r\n           inputs,\r\n           dtype=tf.float32,\r\n           time_major=True)\r\n      return outputs\r\n\r\n    def dump_graph(self, where) :\r\n        print('')\r\n\r\n        print('--- dumping tensorflow graph [', where, '] ---')\r\n        g = tf.get_default_graph()\r\n        print('default tf graph :', g)\r\n\r\n        # debug graphs\r\n        keys = g.get_all_collection_keys()\r\n        print('current name scope :', g.get_name_scope())\r\n        for key in keys :\r\n            print('all graph (', key, ')  :', g.get_collection(key))\r\n        print('') \r\n        print('')\r\n    \r\n    def main(self):\r\n        with tf.Graph().as_default() :\r\n          print('default graph :', tf.get_default_graph())\r\n          self.train_graph()\r\n        with tf.Graph().as_default() :\r\n          print('default graph :', tf.get_default_graph())\r\n          self.inf_graph()\r\n        print('finishes')  \r\n\r\n\r\n```\r\n\r\nThe result is as follows : \r\n\r\ndefault graph : <tensorflow.python.framework.ops.Graph object at 0x000001D9A04742E8>\r\n\r\n--- dumping tensorflow graph [ before save ] ---\r\ndefault tf graph : <tensorflow.python.framework.ops.Graph object at 0x000001D9A0474518>\r\ncurrent name scope : \r\nall graph ( trainable_variables )  : [<tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>]\r\nall graph ( variables )  : [<tf.Variable 'A/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'B/cudnn_gru/opaque_kernel:0' shape=<unknown> dtype=float32_ref>]\r\nall graph ( saveable_objects )  : [<tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D9E5472390>, <tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRUSaveable object at 0x000001D99E51F550>]\r\nall graph ( update_ops )  : []\r\n\r\n\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1277     try:\r\n-> 1278       return fn(*args)\r\n   1279     except errors.OpError as e:\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1262       return self._call_tf_sessionrun(\r\n-> 1263           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1264 \r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1349         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1350         run_metadata)\r\n   1351 \r\n\r\nUnknownError: Fail to find the dnn implementation.\r\n\t [[Node: A/cudnn_gru_1/CudnnRNNCanonicalToParams = CudnnRNNCanonicalToParams[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=12, rnn_mode=\"gru\", seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_layers, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/CudnnRNNCanonicalToParams/num_units, A/cudnn_gru_1/random_uniform, A/cudnn_gru_1/random_uniform_1, A/cudnn_gru_1/random_uniform_2, A/cudnn_gru_1/random_uniform_3, A/cudnn_gru_1/random_uniform_4, A/cudnn_gru_1/random_uniform_5, A/cudnn_gru_1/random_uniform_6, A/cudnn_gru_1/random_uniform_7, A/cudnn_gru_1/random_uniform_8, A/cudnn_gru_1/random_uniform_9, A/cudnn_gru_1/random_uniform_10, A/cudnn_gru_1/random_uniform_11, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const, A/cudnn_gru_1/Const)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\n"}