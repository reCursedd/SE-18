{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/260758192", "html_url": "https://github.com/tensorflow/tensorflow/pull/5142#issuecomment-260758192", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5142", "id": 260758192, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MDc1ODE5Mg==", "user": {"login": "ankush-me", "id": 716901, "node_id": "MDQ6VXNlcjcxNjkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/716901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankush-me", "html_url": "https://github.com/ankush-me", "followers_url": "https://api.github.com/users/ankush-me/followers", "following_url": "https://api.github.com/users/ankush-me/following{/other_user}", "gists_url": "https://api.github.com/users/ankush-me/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankush-me/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankush-me/subscriptions", "organizations_url": "https://api.github.com/users/ankush-me/orgs", "repos_url": "https://api.github.com/users/ankush-me/repos", "events_url": "https://api.github.com/users/ankush-me/events{/privacy}", "received_events_url": "https://api.github.com/users/ankush-me/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-15T20:29:44Z", "updated_at": "2016-11-15T20:40:37Z", "author_association": "NONE", "body_html": "<p>Correct regarding <a href=\"https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L830\">this</a>; should not have been commented out.</p>\n<p>But everything else, due to <a href=\"https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L923\">line 923</a>:</p>\n<div class=\"highlight highlight-source-python\"><pre>const_time_steps, const_batch_size <span class=\"pl-k\">=</span> inputs_got_shape[<span class=\"pl-c1\">0</span>].as_list()[:<span class=\"pl-c1\">2</span>]</pre></div>\n<p>where static shape is required, needs to be taken out (?), i.e.:</p>\n<ol>\n<li>\n<p>Lines <a href=\"https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L930\">930</a> - <a href=\"https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L938\">938</a>:</p>\n<div class=\"highlight highlight-source-python\"><pre>got_time_steps <span class=\"pl-k\">=</span> shape[<span class=\"pl-c1\">0</span>].value\ngot_batch_size <span class=\"pl-k\">=</span> shape[<span class=\"pl-c1\">1</span>].value\n<span class=\"pl-k\">if</span> const_time_steps <span class=\"pl-k\">!=</span> got_time_steps:\n  <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Time steps is not the same for all the elements in the input in a <span class=\"pl-pds\">\"</span></span>\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch.<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">if</span> const_batch_size <span class=\"pl-k\">!=</span> got_batch_size:\n  <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Batch_size is not the same for all the elements in the input.<span class=\"pl-pds\">\"</span></span>)</pre></div>\n</li>\n<li>\n<p>And, lines <a href=\"https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L1026\">1026</a> - <a href=\"https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L1030\">1030</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Restore some shape information</span>\n<span class=\"pl-k\">for</span> output, output_size <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(final_outputs, flat_output_size):\n    shape <span class=\"pl-k\">=</span> _state_size_with_prefix(output_size, <span class=\"pl-v\">prefix</span><span class=\"pl-k\">=</span>[const_time_steps, const_batch_size])\n    output.set_shape(shape)</pre></div>\n</li>\n</ol>", "body_text": "Correct regarding this; should not have been commented out.\nBut everything else, due to line 923:\nconst_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]\nwhere static shape is required, needs to be taken out (?), i.e.:\n\n\nLines 930 - 938:\ngot_time_steps = shape[0].value\ngot_batch_size = shape[1].value\nif const_time_steps != got_time_steps:\n  raise ValueError(\n      \"Time steps is not the same for all the elements in the input in a \"\n      \"batch.\")\nif const_batch_size != got_batch_size:\n  raise ValueError(\n      \"Batch_size is not the same for all the elements in the input.\")\n\n\nAnd, lines 1026 - 1030\n# Restore some shape information\nfor output, output_size in zip(final_outputs, flat_output_size):\n    shape = _state_size_with_prefix(output_size, prefix=[const_time_steps, const_batch_size])\n    output.set_shape(shape)", "body": "Correct regarding [this](https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L830); should not have been commented out.\n\nBut everything else, due to [line 923](https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L923):\n\n``` python\nconst_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]\n```\n\nwhere static shape is required, needs to be taken out (?), i.e.: \n1. Lines [930](https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L930) - [938](https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L938):\n   \n   ``` python\n   got_time_steps = shape[0].value\n   got_batch_size = shape[1].value\n   if const_time_steps != got_time_steps:\n     raise ValueError(\n         \"Time steps is not the same for all the elements in the input in a \"\n         \"batch.\")\n   if const_batch_size != got_batch_size:\n     raise ValueError(\n         \"Batch_size is not the same for all the elements in the input.\")\n   ```\n2. And, lines [1026](https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L1026) - [1030](https://github.com/tensorflow/tensorflow/blob/a2ff968643512b50ea074edcd0561337dad427e2/tensorflow/python/ops/rnn.py#L1030)\n   \n   ``` python\n   # Restore some shape information\n   for output, output_size in zip(final_outputs, flat_output_size):\n       shape = _state_size_with_prefix(output_size, prefix=[const_time_steps, const_batch_size])\n       output.set_shape(shape)\n   ```\n"}