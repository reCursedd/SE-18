{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14750", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14750/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14750/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14750/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14750", "id": 275662697, "node_id": "MDU6SXNzdWUyNzU2NjI2OTc=", "number": 14750, "title": "Test case with multiple threads do not work in TF-1.4", "user": {"login": "sleepfin", "id": 7370869, "node_id": "MDQ6VXNlcjczNzA4Njk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7370869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sleepfin", "html_url": "https://github.com/sleepfin", "followers_url": "https://api.github.com/users/sleepfin/followers", "following_url": "https://api.github.com/users/sleepfin/following{/other_user}", "gists_url": "https://api.github.com/users/sleepfin/gists{/gist_id}", "starred_url": "https://api.github.com/users/sleepfin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sleepfin/subscriptions", "organizations_url": "https://api.github.com/users/sleepfin/orgs", "repos_url": "https://api.github.com/users/sleepfin/repos", "events_url": "https://api.github.com/users/sleepfin/events{/privacy}", "received_events_url": "https://api.github.com/users/sleepfin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-21T10:39:41Z", "updated_at": "2017-11-21T11:45:04Z", "closed_at": "2017-11-21T11:45:04Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nRedHat7.2</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\ntensorflow_gpu binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nr1.4</li>\n<li><strong>Python version</strong>:<br>\npython 2.7</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nCUDA-8.0 CUDNN-6.0</li>\n<li><strong>GPU model and memory</strong>:<br>\nNVIDIA-K80</li>\n<li><strong>Exact command to reproduce</strong>:<br>\npython tensorflow/python/training/sync_replicas_optimizer_test.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When I start a test case for sync optimizer, it always shows that ps:0,ps:1,worker:1 are not ready.<br>\nIt seems this only happens in TF-1.4</p>\n<h3>Source code / logs</h3>\n<p>Source Code:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer_test.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer_test.py</a></p>\n<p>Logs:<br>\n2017-11-21 18:37:58.627374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:                                                                                                                [6/1673]<br>\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235<br>\npciBusID: 0000:06:00.0<br>\ntotalMemory: 11.17GiB freeMemory: 11.11GiB<br>\n2017-11-21 18:37:58.874015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties:<br>\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235<br>\npciBusID: 0000:07:00.0<br>\ntotalMemory: 11.17GiB freeMemory: 11.11GiB<br>\n2017-11-21 18:37:58.874455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix<br>\n2017-11-21 18:37:58.874488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1<br>\n2017-11-21 18:37:58.874499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y<br>\n2017-11-21 18:37:58.874505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y<br>\n2017-11-21 18:37:58.874523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)<br>\n2017-11-21 18:37:58.874532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)<br>\nE1121 18:37:59.075062187   32596 ev_epoll1_linux.c:1051]     grpc epoll fd: 31<br>\n2017-11-21 18:37:59.081712: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:17358, 1 -&gt; localhost:24102}<br>\n2017-11-21 18:37:59.081747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:15405, 1 -&gt; localhost:17501}<br>\n2017-11-21 18:37:59.083912: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:15405<br>\n2017-11-21 18:37:59.084202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)<br>\n2017-11-21 18:37:59.084222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)<br>\n2017-11-21 18:37:59.090916: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:17358, 1 -&gt; localhost:24102}<br>\n2017-11-21 18:37:59.090943: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:15405, 1 -&gt; localhost:17501}<br>\n2017-11-21 18:37:59.091077: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17501<br>\n2017-11-21 18:37:59.091268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)<br>\n2017-11-21 18:37:59.091286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)<br>\n2017-11-21 18:37:59.097254: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:17358, 1 -&gt; localhost:24102}<br>\n2017-11-21 18:37:59.097277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:15405, 1 -&gt; localhost:17501}<br>\n2017-11-21 18:37:59.097399: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17358<br>\n2017-11-21 18:37:59.097548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)<br>\n2017-11-21 18:37:59.097566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)<br>\n2017-11-21 18:37:59.104080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:17358, 1 -&gt; localhost:24102}<br>\n2017-11-21 18:37:59.104127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:15405, 1 -&gt; localhost:17501}<br>\n2017-11-21 18:37:59.104281: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:24102<br>\n2017-11-21 18:38:09.222879: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2017-11-21 18:38:09.222931: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1<br>\n2017-11-21 18:38:09.222941: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2017-11-21 18:38:19.223062: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2017-11-21 18:38:19.223102: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1<br>\n2017-11-21 18:38:19.223110: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2017-11-21 18:38:29.223253: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2017-11-21 18:38:29.223284: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1<br>\n2017-11-21 18:38:29.223291: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2017-11-21 18:38:39.223379: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2017-11-21 18:38:39.224059: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1<br>\n2017-11-21 18:38:39.224068: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nRedHat7.2\nTensorFlow installed from (source or binary):\ntensorflow_gpu binary\nTensorFlow version (use command below):\nr1.4\nPython version:\npython 2.7\nCUDA/cuDNN version:\nCUDA-8.0 CUDNN-6.0\nGPU model and memory:\nNVIDIA-K80\nExact command to reproduce:\npython tensorflow/python/training/sync_replicas_optimizer_test.py\n\nDescribe the problem\nWhen I start a test case for sync optimizer, it always shows that ps:0,ps:1,worker:1 are not ready.\nIt seems this only happens in TF-1.4\nSource code / logs\nSource Code:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer_test.py\nLogs:\n2017-11-21 18:37:58.627374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:                                                                                                                [6/1673]\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:06:00.0\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\n2017-11-21 18:37:58.874015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties:\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:07:00.0\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\n2017-11-21 18:37:58.874455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix\n2017-11-21 18:37:58.874488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1\n2017-11-21 18:37:58.874499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y\n2017-11-21 18:37:58.874505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y\n2017-11-21 18:37:58.874523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\n2017-11-21 18:37:58.874532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\nE1121 18:37:59.075062187   32596 ev_epoll1_linux.c:1051]     grpc epoll fd: 31\n2017-11-21 18:37:59.081712: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\n2017-11-21 18:37:59.081747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\n2017-11-21 18:37:59.083912: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:15405\n2017-11-21 18:37:59.084202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\n2017-11-21 18:37:59.084222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\n2017-11-21 18:37:59.090916: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\n2017-11-21 18:37:59.090943: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\n2017-11-21 18:37:59.091077: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17501\n2017-11-21 18:37:59.091268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\n2017-11-21 18:37:59.091286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\n2017-11-21 18:37:59.097254: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\n2017-11-21 18:37:59.097277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\n2017-11-21 18:37:59.097399: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17358\n2017-11-21 18:37:59.097548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\n2017-11-21 18:37:59.097566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\n2017-11-21 18:37:59.104080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\n2017-11-21 18:37:59.104127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\n2017-11-21 18:37:59.104281: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:24102\n2017-11-21 18:38:09.222879: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2017-11-21 18:38:09.222931: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\n2017-11-21 18:38:09.222941: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2017-11-21 18:38:19.223062: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2017-11-21 18:38:19.223102: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\n2017-11-21 18:38:19.223110: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2017-11-21 18:38:29.223253: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2017-11-21 18:38:29.223284: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\n2017-11-21 18:38:29.223291: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2017-11-21 18:38:39.223379: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2017-11-21 18:38:39.224059: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\n2017-11-21 18:38:39.224068: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRedHat7.2\r\n- **TensorFlow installed from (source or binary)**:\r\ntensorflow_gpu binary\r\n- **TensorFlow version (use command below)**:\r\nr1.4\r\n- **Python version**: \r\npython 2.7\r\n- **CUDA/cuDNN version**:\r\nCUDA-8.0 CUDNN-6.0\r\n- **GPU model and memory**:\r\nNVIDIA-K80\r\n- **Exact command to reproduce**:\r\npython tensorflow/python/training/sync_replicas_optimizer_test.py\r\n\r\n### Describe the problem\r\n\r\nWhen I start a test case for sync optimizer, it always shows that ps:0,ps:1,worker:1 are not ready.\r\nIt seems this only happens in TF-1.4\r\n\r\n### Source code / logs\r\n\r\nSource Code:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer_test.py\r\n\r\nLogs:\r\n2017-11-21 18:37:58.627374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:                                                                                                                [6/1673]\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-11-21 18:37:58.874015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-11-21 18:37:58.874455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix\r\n2017-11-21 18:37:58.874488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1 \r\n2017-11-21 18:37:58.874499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y \r\n2017-11-21 18:37:58.874505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y \r\n2017-11-21 18:37:58.874523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:58.874532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\nE1121 18:37:59.075062187   32596 ev_epoll1_linux.c:1051]     grpc epoll fd: 31\r\n2017-11-21 18:37:59.081712: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.081747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.083912: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:15405\r\n2017-11-21 18:37:59.084202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.084222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.090916: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.090943: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.091077: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17501\r\n2017-11-21 18:37:59.091268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.091286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.097254: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.097277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.097399: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17358\r\n2017-11-21 18:37:59.097548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.097566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.104080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.104127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.104281: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:24102\r\n2017-11-21 18:38:09.222879: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:09.222931: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:09.222941: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2017-11-21 18:38:19.223062: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:19.223102: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:19.223110: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2017-11-21 18:38:29.223253: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:29.223284: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:29.223291: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2017-11-21 18:38:39.223379: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:39.224059: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:39.224068: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n"}