{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20739", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20739/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20739/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20739/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20739", "id": 340643127, "node_id": "MDU6SXNzdWUzNDA2NDMxMjc=", "number": 20739, "title": "Tensorflow lite, invalid quantization ranges", "user": {"login": "Canxes", "id": 18511496, "node_id": "MDQ6VXNlcjE4NTExNDk2", "avatar_url": "https://avatars0.githubusercontent.com/u/18511496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Canxes", "html_url": "https://github.com/Canxes", "followers_url": "https://api.github.com/users/Canxes/followers", "following_url": "https://api.github.com/users/Canxes/following{/other_user}", "gists_url": "https://api.github.com/users/Canxes/gists{/gist_id}", "starred_url": "https://api.github.com/users/Canxes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Canxes/subscriptions", "organizations_url": "https://api.github.com/users/Canxes/orgs", "repos_url": "https://api.github.com/users/Canxes/repos", "events_url": "https://api.github.com/users/Canxes/events{/privacy}", "received_events_url": "https://api.github.com/users/Canxes/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-12T13:41:20Z", "updated_at": "2018-10-15T17:10:36Z", "closed_at": "2018-10-15T17:10:35Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: Y</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:  source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.9.0-rc1-48-ge3f2b5903c 1.9.0-rc2</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version</strong>: release 0.15.0</li>\n<li><strong>GCC/Compiler version</strong>: 5.5</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hello! I am not really sure is it a bug or just my misunderstanding. According to the quantized training <a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">tutorial</a> command <code>create_eval_graph</code> is supposed to generate toco-ready graph with fake quantization nodes inside. For weights it's done via the following command:</p>\n<div class=\"highlight highlight-source-python\"><pre>    _InsertQuantOp(\n        context,\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights_quant<span class=\"pl-pds\">'</span></span>,\n        layer_match.weight_tensor.op, [layer_match.layer_op],\n        is_training,\n        <span class=\"pl-v\">moving_avg</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        <span class=\"pl-v\">ema_decay</span><span class=\"pl-k\">=</span>ema_decay,\n        <span class=\"pl-v\">quant_delay</span><span class=\"pl-k\">=</span>quant_delay,\n        <span class=\"pl-v\">narrow_range</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span> [!!!!!!]</span>\n        <span class=\"pl-v\">vars_collection</span><span class=\"pl-k\">=</span>vars_collection,\n        <span class=\"pl-v\">bits</span><span class=\"pl-k\">=</span>weight_bits,\n        <span class=\"pl-v\">consumer_scope</span><span class=\"pl-k\">=</span>scope)</pre></div>\n<p>Here narrow_range parameter forces mapping of float values to the [1, 255] range. However, as far as I was able to understand, all tensors quantization(weights included) is performed using function <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/quantization_util.h\">ChooseQuantizationParams</a> which assumes, that available range is [0, 255](for uint8). Hence, float weights are mapped to values that might be different then ones that was used during training. In my situation it leads to the significant difference(deep model) between output of the Tensorflow fake-quantized graph and Tensorflow lite model.  Difference is about 1e-1 for a single layer.</p>\n<p>Described is true for the --inference_type=QUANTIZED_UINT8 toco parameter, not sure about --inference_type=FLOAT and --quantize_weights=true(probably the same).</p>\n<h3>Source code / logs</h3>\n<p>I will prepare code snippets to reproduce in case I am right and described behavior can be considered as unexpected.</p>", "body_text": "System information\n\nHave I written custom code: Y\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from (source or binary):  source\nTensorFlow version (use command below): v1.9.0-rc1-48-ge3f2b5903c 1.9.0-rc2\nPython version: 3.5.2\nBazel version: release 0.15.0\nGCC/Compiler version: 5.5\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nHello! I am not really sure is it a bug or just my misunderstanding. According to the quantized training tutorial command create_eval_graph is supposed to generate toco-ready graph with fake quantization nodes inside. For weights it's done via the following command:\n    _InsertQuantOp(\n        context,\n        'weights_quant',\n        layer_match.weight_tensor.op, [layer_match.layer_op],\n        is_training,\n        moving_avg=False,\n        ema_decay=ema_decay,\n        quant_delay=quant_delay,\n        narrow_range=True, # [!!!!!!]\n        vars_collection=vars_collection,\n        bits=weight_bits,\n        consumer_scope=scope)\nHere narrow_range parameter forces mapping of float values to the [1, 255] range. However, as far as I was able to understand, all tensors quantization(weights included) is performed using function ChooseQuantizationParams which assumes, that available range is [0, 255](for uint8). Hence, float weights are mapped to values that might be different then ones that was used during training. In my situation it leads to the significant difference(deep model) between output of the Tensorflow fake-quantized graph and Tensorflow lite model.  Difference is about 1e-1 for a single layer.\nDescribed is true for the --inference_type=QUANTIZED_UINT8 toco parameter, not sure about --inference_type=FLOAT and --quantize_weights=true(probably the same).\nSource code / logs\nI will prepare code snippets to reproduce in case I am right and described behavior can be considered as unexpected.", "body": "### System information\r\n- **Have I written custom code**: Y\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:  source\r\n- **TensorFlow version (use command below)**: v1.9.0-rc1-48-ge3f2b5903c 1.9.0-rc2\r\n- **Python version**: 3.5.2\r\n- **Bazel version**: release 0.15.0\r\n- **GCC/Compiler version**: 5.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A  \r\n\r\n### Describe the problem  \r\nHello! I am not really sure is it a bug or just my misunderstanding. According to the quantized training [tutorial](https://www.tensorflow.org/performance/quantization) command `create_eval_graph` is supposed to generate toco-ready graph with fake quantization nodes inside. For weights it's done via the following command:  \r\n```python \r\n    _InsertQuantOp(\r\n        context,\r\n        'weights_quant',\r\n        layer_match.weight_tensor.op, [layer_match.layer_op],\r\n        is_training,\r\n        moving_avg=False,\r\n        ema_decay=ema_decay,\r\n        quant_delay=quant_delay,\r\n        narrow_range=True, # [!!!!!!]\r\n        vars_collection=vars_collection,\r\n        bits=weight_bits,\r\n        consumer_scope=scope)\r\n```  \r\nHere narrow_range parameter forces mapping of float values to the [1, 255] range. However, as far as I was able to understand, all tensors quantization(weights included) is performed using function [ChooseQuantizationParams](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/quantization_util.h) which assumes, that available range is [0, 255](for uint8). Hence, float weights are mapped to values that might be different then ones that was used during training. In my situation it leads to the significant difference(deep model) between output of the Tensorflow fake-quantized graph and Tensorflow lite model.  Difference is about 1e-1 for a single layer.\r\n\r\nDescribed is true for the --inference_type=QUANTIZED_UINT8 toco parameter, not sure about --inference_type=FLOAT and --quantize_weights=true(probably the same).\r\n\r\n### Source code / logs  \r\nI will prepare code snippets to reproduce in case I am right and described behavior can be considered as unexpected.   "}