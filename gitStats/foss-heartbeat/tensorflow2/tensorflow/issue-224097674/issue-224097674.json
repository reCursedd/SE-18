{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9434", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9434/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9434/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9434/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9434", "id": 224097674, "node_id": "MDU6SXNzdWUyMjQwOTc2NzQ=", "number": 9434, "title": "Using replace to evaluate multiple gradients during training in Keras", "user": {"login": "slavakung", "id": 4301275, "node_id": "MDQ6VXNlcjQzMDEyNzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/4301275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slavakung", "html_url": "https://github.com/slavakung", "followers_url": "https://api.github.com/users/slavakung/followers", "following_url": "https://api.github.com/users/slavakung/following{/other_user}", "gists_url": "https://api.github.com/users/slavakung/gists{/gist_id}", "starred_url": "https://api.github.com/users/slavakung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slavakung/subscriptions", "organizations_url": "https://api.github.com/users/slavakung/orgs", "repos_url": "https://api.github.com/users/slavakung/repos", "events_url": "https://api.github.com/users/slavakung/events{/privacy}", "received_events_url": "https://api.github.com/users/slavakung/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-04-25T10:53:13Z", "updated_at": "2018-01-04T13:34:29Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I am a researcher in optimization and I am interested in testing algorithms for training DNNs using keras, and am now using tensorflow backend.</p>\n<p>In practice, I would like to do something a bit different from the other optimizers, I would like to compute the gradient at a slightly different value of the tensor of parameters than the current one, and the update I will make to the parameters will depend on both the current gradient and this other gradient.</p>\n<p>In practice this has proven more difficult than anticipated.<br>\nSee <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"219856546\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/keras-team/keras/issues/6175\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/6175/hovercard\" href=\"https://github.com/keras-team/keras/issues/6175\">keras-team/keras#6175</a><br>\nit was suggested I come to here for further suggestions.</p>\n<p>My code is a standard keras python code, the body does</p>\n<p>model = Sequential()<br>\nmodel.add(Dense(512, input_shape=(784,)))<br>\n...<br>\nmodel.compile(loss='categorical_crossentropy',<br>\noptimizer = myopt,<br>\nmetrics=['accuracy'])<br>\nhistory = model.fit(X_train, Y_train,<br>\nbatch_size=batch_size, nb_epoch=nb_epoch,<br>\nverbose=1, validation_data=(X_test, Y_test))</p>\n<p>In the get_updates call function of my custom optimizer, it begins as usual</p>\n<pre><code>def get_updates(self, params, constraints, loss):\n    grads = self.get_gradients(loss, params)\n</code></pre>\n<p>Now, I want to now get the gradients at a different value of grads. First I tried just defining another tensor of the same structure but different values and take the get_gradients, but of course the loss is a graph depending on params already. Then I tried changing params itself (then copying the old values of the tensor to another one, to replace params after the evaluation) but apparently as the forward pass was not made this was ineffective. As per the advice in the above github conversation in keras, I tried,</p>\n<pre><code>    tempparams = [a+1. for a in params]\n    replace = {p:npm for p, npm in zip(params, tempparams)}\n    gradsn = [tf.contrib.graph_editor.graph_replace(g.op, replace) for g in grads]\n</code></pre>\n<p>but this is still not OK, as I get the error</p>\n<p>TypeError: Expected a type in (&lt;class 'tensorflow.python.framework.ops.Operation'&gt;), got: &lt;class 'tensorflow.python.ops.variables.Variable'</p>\n<p>Thank you</p>", "body_text": "I am a researcher in optimization and I am interested in testing algorithms for training DNNs using keras, and am now using tensorflow backend.\nIn practice, I would like to do something a bit different from the other optimizers, I would like to compute the gradient at a slightly different value of the tensor of parameters than the current one, and the update I will make to the parameters will depend on both the current gradient and this other gradient.\nIn practice this has proven more difficult than anticipated.\nSee keras-team/keras#6175\nit was suggested I come to here for further suggestions.\nMy code is a standard keras python code, the body does\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(784,)))\n...\nmodel.compile(loss='categorical_crossentropy',\noptimizer = myopt,\nmetrics=['accuracy'])\nhistory = model.fit(X_train, Y_train,\nbatch_size=batch_size, nb_epoch=nb_epoch,\nverbose=1, validation_data=(X_test, Y_test))\nIn the get_updates call function of my custom optimizer, it begins as usual\ndef get_updates(self, params, constraints, loss):\n    grads = self.get_gradients(loss, params)\n\nNow, I want to now get the gradients at a different value of grads. First I tried just defining another tensor of the same structure but different values and take the get_gradients, but of course the loss is a graph depending on params already. Then I tried changing params itself (then copying the old values of the tensor to another one, to replace params after the evaluation) but apparently as the forward pass was not made this was ineffective. As per the advice in the above github conversation in keras, I tried,\n    tempparams = [a+1. for a in params]\n    replace = {p:npm for p, npm in zip(params, tempparams)}\n    gradsn = [tf.contrib.graph_editor.graph_replace(g.op, replace) for g in grads]\n\nbut this is still not OK, as I get the error\nTypeError: Expected a type in (<class 'tensorflow.python.framework.ops.Operation'>), got: <class 'tensorflow.python.ops.variables.Variable'\nThank you", "body": "I am a researcher in optimization and I am interested in testing algorithms for training DNNs using keras, and am now using tensorflow backend.\r\n\r\nIn practice, I would like to do something a bit different from the other optimizers, I would like to compute the gradient at a slightly different value of the tensor of parameters than the current one, and the update I will make to the parameters will depend on both the current gradient and this other gradient.  \r\n\r\nIn practice this has proven more difficult than anticipated.\r\nSee https://github.com/fchollet/keras/issues/6175\r\nit was suggested I come to here for further suggestions.\r\n\r\nMy code is a standard keras python code, the body does\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(512, input_shape=(784,)))\r\n...\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer = myopt,\r\n              metrics=['accuracy'])\r\nhistory = model.fit(X_train, Y_train,\r\n                    batch_size=batch_size, nb_epoch=nb_epoch, \r\n                    verbose=1, validation_data=(X_test, Y_test))\r\n\r\n\r\nIn the get_updates call function of my custom optimizer, it begins as usual\r\n\r\n    def get_updates(self, params, constraints, loss):\r\n        grads = self.get_gradients(loss, params)\r\n\r\nNow, I want to now get the gradients at a different value of grads. First I tried just defining another tensor of the same structure but different values and take the get_gradients, but of course the loss is a graph depending on params already. Then I tried changing params itself (then copying the old values of the tensor to another one, to replace params after the evaluation) but apparently as the forward pass was not made this was ineffective. As per the advice in the above github conversation in keras, I tried,\r\n\r\n        tempparams = [a+1. for a in params]\r\n        replace = {p:npm for p, npm in zip(params, tempparams)}\r\n        gradsn = [tf.contrib.graph_editor.graph_replace(g.op, replace) for g in grads]\r\n\r\n\r\nbut this is still not OK, as I get the error\r\n\r\nTypeError: Expected a type in (<class 'tensorflow.python.framework.ops.Operation'>), got: <class 'tensorflow.python.ops.variables.Variable'\r\n\r\n\r\nThank you\r\n\r\n\r\n\r\n"}