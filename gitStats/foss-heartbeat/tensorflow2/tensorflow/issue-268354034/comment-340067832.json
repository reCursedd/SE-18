{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/340067832", "html_url": "https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-340067832", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13969", "id": 340067832, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDA2NzgzMg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-27T19:45:11Z", "updated_at": "2017-10-27T19:45:11Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">I see.  Certainly LSTMs have traditionally had no problem dealing with\npadding even when masking is not available.\n\nI like the idea of a padding_alignment argument, though the output_offset\ncan't be an attribute (in the sense that attributes are like\nhyperparameters set at graph construction time); it has to be a tensor.  So\nconsider instead adding a AlignedPaddedBatchDataset in tf.contrib.data\nwhich takes an alignment= (string) argument and emits offset and length\ntensors.\n\nI guess one of my main questions is regarding \"peeking\", or having the\nmodel learn things from the batch sizes that you didn't mean to teach it,\nand which depends on the batch size and how you batch.\n\nFor example, if you use group_by_window after padding to ensure your\nbatches all contain entries that are either all few time steps or many time\nsteps, then the LSTM will learn a bias from just seeing how much padding\nthere is at the beginning and/or end of the sequence.  Then at inference\ntime, or if you change your bucketing, this bias goes away and your model\nwill not perform as well as it did before.  Even if you don't\ngroup_by_window, if you use a large window size then you'll always have\nsome long sequences and the shorter ones will see a lot of padding at the\nbeginning.  Again in this case the LSTM will learn a bias when it sees a\nlot of padding at the beginning, and at inference time in batch_size=1\nmode, there'll be no padding and it will not perform as well.\n\nWe've seen this happen with experiments in both rnns and in tensor2tensor\n(Transformer) models.\n\nSo keep this in mind.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Oct 27, 2017 at 12:36 PM, Joan Puigcerver ***@***.***&gt; wrote:\n Hi <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt;,\n\n To give some perspective, I've been using this trick for handwritten text\n recognition for a while (you can also think of speech recognition, as an\n application potentially affected by this). The main reason for this is\n that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not\n support examples of multiple lengths. So, after padding one must assume\n that all examples have the same size.\n\n This, by itself, is not a big issue. The problems arise when one uses *bidirectional\n LSTMs* and the length of the sequences varies a lot within each batch.\n\n Notice that the *left-to-right LSTM has to deal with lots of zeros in the\n input at the end of the sequence*, while the *right-to-left LSTM presents\n this problem at the start of the sequence*. Centering the examples within\n the batch mitigates this problem.\n\n In terms of the loss (CTC), I just ignore the fact that I padded the\n sequences. In practice, this means that the model has to learn to be\n invariant to the amount of \"zeros\" at the beginning *and* the end of the\n examples.\n\n Ideally, I would like that all recurrent operations support examples of\n multiples sizes within the batch, but that requires a significant effort\n from many other developers. In any case, I found that this simple trick of\n centering the features within the batch works quite well in practice, so\n it's a good thing to have this option available if some recurrent op does\n not support multiple layers.\n\n Regarding the PaddedBatchDataset operation in TF, we could add an\n output_offset attribute, similar to output_types and output_shapes that\n contains the offset of each element within the batch. The default\n (left-aligned padding) would be a zeros tensor. This, and the\n output_shapes would be enough if the user wants to take into account the\n padding for any masking in later ops.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"268354034\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13969\" href=\"https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-340065773\">#13969 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim2qf2wb50nRW0ebpvHN0xtPeYKA0ks5swjC8gaJpZM4QFz_p\">https://github.com/notifications/unsubscribe-auth/ABtim2qf2wb50nRW0ebpvHN0xtPeYKA0ks5swjC8gaJpZM4QFz_p</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "I see.  Certainly LSTMs have traditionally had no problem dealing with\npadding even when masking is not available.\n\nI like the idea of a padding_alignment argument, though the output_offset\ncan't be an attribute (in the sense that attributes are like\nhyperparameters set at graph construction time); it has to be a tensor.  So\nconsider instead adding a AlignedPaddedBatchDataset in tf.contrib.data\nwhich takes an alignment= (string) argument and emits offset and length\ntensors.\n\nI guess one of my main questions is regarding \"peeking\", or having the\nmodel learn things from the batch sizes that you didn't mean to teach it,\nand which depends on the batch size and how you batch.\n\nFor example, if you use group_by_window after padding to ensure your\nbatches all contain entries that are either all few time steps or many time\nsteps, then the LSTM will learn a bias from just seeing how much padding\nthere is at the beginning and/or end of the sequence.  Then at inference\ntime, or if you change your bucketing, this bias goes away and your model\nwill not perform as well as it did before.  Even if you don't\ngroup_by_window, if you use a large window size then you'll always have\nsome long sequences and the shorter ones will see a lot of padding at the\nbeginning.  Again in this case the LSTM will learn a bias when it sees a\nlot of padding at the beginning, and at inference time in batch_size=1\nmode, there'll be no padding and it will not perform as well.\n\nWe've seen this happen with experiments in both rnns and in tensor2tensor\n(Transformer) models.\n\nSo keep this in mind.\n\u2026\nOn Fri, Oct 27, 2017 at 12:36 PM, Joan Puigcerver ***@***.***> wrote:\n Hi @ebrevdo <https://github.com/ebrevdo>,\n\n To give some perspective, I've been using this trick for handwritten text\n recognition for a while (you can also think of speech recognition, as an\n application potentially affected by this). The main reason for this is\n that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not\n support examples of multiple lengths. So, after padding one must assume\n that all examples have the same size.\n\n This, by itself, is not a big issue. The problems arise when one uses *bidirectional\n LSTMs* and the length of the sequences varies a lot within each batch.\n\n Notice that the *left-to-right LSTM has to deal with lots of zeros in the\n input at the end of the sequence*, while the *right-to-left LSTM presents\n this problem at the start of the sequence*. Centering the examples within\n the batch mitigates this problem.\n\n In terms of the loss (CTC), I just ignore the fact that I padded the\n sequences. In practice, this means that the model has to learn to be\n invariant to the amount of \"zeros\" at the beginning *and* the end of the\n examples.\n\n Ideally, I would like that all recurrent operations support examples of\n multiples sizes within the batch, but that requires a significant effort\n from many other developers. In any case, I found that this simple trick of\n centering the features within the batch works quite well in practice, so\n it's a good thing to have this option available if some recurrent op does\n not support multiple layers.\n\n Regarding the PaddedBatchDataset operation in TF, we could add an\n output_offset attribute, similar to output_types and output_shapes that\n contains the offset of each element within the batch. The default\n (left-aligned padding) would be a zeros tensor. This, and the\n output_shapes would be enough if the user wants to take into account the\n padding for any masking in later ops.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#13969 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim2qf2wb50nRW0ebpvHN0xtPeYKA0ks5swjC8gaJpZM4QFz_p>\n .", "body": "I see.  Certainly LSTMs have traditionally had no problem dealing with\npadding even when masking is not available.\n\nI like the idea of a padding_alignment argument, though the output_offset\ncan't be an attribute (in the sense that attributes are like\nhyperparameters set at graph construction time); it has to be a tensor.  So\nconsider instead adding a AlignedPaddedBatchDataset in tf.contrib.data\nwhich takes an alignment= (string) argument and emits offset and length\ntensors.\n\nI guess one of my main questions is regarding \"peeking\", or having the\nmodel learn things from the batch sizes that you didn't mean to teach it,\nand which depends on the batch size and how you batch.\n\nFor example, if you use group_by_window after padding to ensure your\nbatches all contain entries that are either all few time steps or many time\nsteps, then the LSTM will learn a bias from just seeing how much padding\nthere is at the beginning and/or end of the sequence.  Then at inference\ntime, or if you change your bucketing, this bias goes away and your model\nwill not perform as well as it did before.  Even if you don't\ngroup_by_window, if you use a large window size then you'll always have\nsome long sequences and the shorter ones will see a lot of padding at the\nbeginning.  Again in this case the LSTM will learn a bias when it sees a\nlot of padding at the beginning, and at inference time in batch_size=1\nmode, there'll be no padding and it will not perform as well.\n\nWe've seen this happen with experiments in both rnns and in tensor2tensor\n(Transformer) models.\n\nSo keep this in mind.\n\nOn Fri, Oct 27, 2017 at 12:36 PM, Joan Puigcerver <notifications@github.com>\nwrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo>,\n>\n> To give some perspective, I've been using this trick for handwritten text\n> recognition for a while (you can also think of speech recognition, as an\n> application potentially affected by this). The main reason for this is\n> that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not\n> support examples of multiple lengths. So, after padding one must assume\n> that all examples have the same size.\n>\n> This, by itself, is not a big issue. The problems arise when one uses *bidirectional\n> LSTMs* and the length of the sequences varies a lot within each batch.\n>\n> Notice that the *left-to-right LSTM has to deal with lots of zeros in the\n> input at the end of the sequence*, while the *right-to-left LSTM presents\n> this problem at the start of the sequence*. Centering the examples within\n> the batch mitigates this problem.\n>\n> In terms of the loss (CTC), I just ignore the fact that I padded the\n> sequences. In practice, this means that the model has to learn to be\n> invariant to the amount of \"zeros\" at the beginning *and* the end of the\n> examples.\n>\n> Ideally, I would like that all recurrent operations support examples of\n> multiples sizes within the batch, but that requires a significant effort\n> from many other developers. In any case, I found that this simple trick of\n> centering the features within the batch works quite well in practice, so\n> it's a good thing to have this option available if some recurrent op does\n> not support multiple layers.\n>\n> Regarding the PaddedBatchDataset operation in TF, we could add an\n> output_offset attribute, similar to output_types and output_shapes that\n> contains the offset of each element within the batch. The default\n> (left-aligned padding) would be a zeros tensor. This, and the\n> output_shapes would be enough if the user wants to take into account the\n> padding for any masking in later ops.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-340065773>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2qf2wb50nRW0ebpvHN0xtPeYKA0ks5swjC8gaJpZM4QFz_p>\n> .\n>\n"}