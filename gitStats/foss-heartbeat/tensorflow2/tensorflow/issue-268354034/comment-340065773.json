{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/340065773", "html_url": "https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-340065773", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13969", "id": 340065773, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDA2NTc3Mw==", "user": {"login": "jpuigcerver", "id": 656080, "node_id": "MDQ6VXNlcjY1NjA4MA==", "avatar_url": "https://avatars1.githubusercontent.com/u/656080?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpuigcerver", "html_url": "https://github.com/jpuigcerver", "followers_url": "https://api.github.com/users/jpuigcerver/followers", "following_url": "https://api.github.com/users/jpuigcerver/following{/other_user}", "gists_url": "https://api.github.com/users/jpuigcerver/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpuigcerver/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpuigcerver/subscriptions", "organizations_url": "https://api.github.com/users/jpuigcerver/orgs", "repos_url": "https://api.github.com/users/jpuigcerver/repos", "events_url": "https://api.github.com/users/jpuigcerver/events{/privacy}", "received_events_url": "https://api.github.com/users/jpuigcerver/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-27T19:35:56Z", "updated_at": "2017-10-27T19:35:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>,</p>\n<p>To give some perspective, I've been using this trick for handwritten text recognition for a while (you can also think of speech recognition, as an application potentially affected by this). The main reason for this is that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not support examples of multiple lengths. So, after padding one must assume that all examples have the same size.</p>\n<p>This, by itself, is not a big issue. The problems arise when one uses <strong>bidirectional LSTMs</strong> and the length of the sequences varies a lot within each batch.</p>\n<p>Notice that the <strong>left-to-right LSTM has to deal with lots of zeros in the input at the end of the sequence</strong>, while the <strong>right-to-left LSTM presents this problem at the start of the sequence</strong>. Centering the examples within the batch mitigates this problem.</p>\n<p>In terms of the loss (CTC), I just ignore the fact that I padded the sequences. In practice, this means that the model has to learn to be invariant to the amount of \"zeros\" at the beginning <em>and</em> the end of the examples.</p>\n<p>Ideally, I would like that all recurrent operations support examples of multiples sizes within the batch, but that requires a significant effort from many other developers. In any case, I found that this simple trick of centering the features within the batch works quite well in practice, so it's a good thing to have this option available if some recurrent op does not support multiple layers.</p>\n<p>Regarding the PaddedBatchDataset operation in TF, we could add an <code>output_offset</code> attribute, similar to <code>output_types</code> and <code>output_shapes</code> that contains the offset of each element within the batch. The default (left-aligned padding) would be a zeros tensor. This, and the <code>output_shapes</code> would be enough if the user wants to take into account the padding for any masking in later ops.</p>", "body_text": "Hi @ebrevdo,\nTo give some perspective, I've been using this trick for handwritten text recognition for a while (you can also think of speech recognition, as an application potentially affected by this). The main reason for this is that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not support examples of multiple lengths. So, after padding one must assume that all examples have the same size.\nThis, by itself, is not a big issue. The problems arise when one uses bidirectional LSTMs and the length of the sequences varies a lot within each batch.\nNotice that the left-to-right LSTM has to deal with lots of zeros in the input at the end of the sequence, while the right-to-left LSTM presents this problem at the start of the sequence. Centering the examples within the batch mitigates this problem.\nIn terms of the loss (CTC), I just ignore the fact that I padded the sequences. In practice, this means that the model has to learn to be invariant to the amount of \"zeros\" at the beginning and the end of the examples.\nIdeally, I would like that all recurrent operations support examples of multiples sizes within the batch, but that requires a significant effort from many other developers. In any case, I found that this simple trick of centering the features within the batch works quite well in practice, so it's a good thing to have this option available if some recurrent op does not support multiple layers.\nRegarding the PaddedBatchDataset operation in TF, we could add an output_offset attribute, similar to output_types and output_shapes that contains the offset of each element within the batch. The default (left-aligned padding) would be a zeros tensor. This, and the output_shapes would be enough if the user wants to take into account the padding for any masking in later ops.", "body": "Hi @ebrevdo,\r\n\r\nTo give some perspective, I've been using this trick for handwritten text recognition for a while (you can also think of speech recognition, as an application potentially affected by this). The main reason for this is that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not support examples of multiple lengths. So, after padding one must assume that all examples have the same size.\r\n\r\nThis, by itself, is not a big issue. The problems arise when one uses **bidirectional LSTMs** and the length of the sequences varies a lot within each batch.\r\n\r\nNotice that the **left-to-right LSTM has to deal with lots of zeros in the input at the end of the sequence**, while the **right-to-left LSTM presents this problem at the start of the sequence**. Centering the examples within the batch mitigates this problem.\r\n\r\nIn terms of the loss (CTC), I just ignore the fact that I padded the sequences. In practice, this means that the model has to learn to be invariant to the amount of \"zeros\" at the beginning *and* the end of the examples.\r\n\r\nIdeally, I would like that all recurrent operations support examples of multiples sizes within the batch, but that requires a significant effort from many other developers. In any case, I found that this simple trick of centering the features within the batch works quite well in practice, so it's a good thing to have this option available if some recurrent op does not support multiple layers.\r\n\r\nRegarding the PaddedBatchDataset operation in TF, we could add an `output_offset` attribute, similar to `output_types` and `output_shapes` that contains the offset of each element within the batch. The default (left-aligned padding) would be a zeros tensor. This, and the `output_shapes` would be enough if the user wants to take into account the padding for any masking in later ops."}