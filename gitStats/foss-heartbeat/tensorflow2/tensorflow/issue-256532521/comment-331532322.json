{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331532322", "html_url": "https://github.com/tensorflow/tensorflow/issues/12948#issuecomment-331532322", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12948", "id": 331532322, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTUzMjMyMg==", "user": {"login": "ahundt", "id": 55744, "node_id": "MDQ6VXNlcjU1NzQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/55744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahundt", "html_url": "https://github.com/ahundt", "followers_url": "https://api.github.com/users/ahundt/followers", "following_url": "https://api.github.com/users/ahundt/following{/other_user}", "gists_url": "https://api.github.com/users/ahundt/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahundt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahundt/subscriptions", "organizations_url": "https://api.github.com/users/ahundt/orgs", "repos_url": "https://api.github.com/users/ahundt/repos", "events_url": "https://api.github.com/users/ahundt/events{/privacy}", "received_events_url": "https://api.github.com/users/ahundt/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-22T18:56:18Z", "updated_at": "2017-12-02T18:56:58Z", "author_association": "NONE", "body_html": "<p>I'd like to provide some additional motivation for this feature request.</p>\n<p>I'm able to train a Resnet50 based architecture for my problem with a batch size of 32 on the older 12GB Titan X GPU. However, a DenseNet based architecture that should actually be fairly small, with 40 layers and 500k parameters can only fit a batch size of 6, produces some tf memory warnings, and runs 8x slower during training, leading a day of training to take over a week, and smaller batch sizes were even slower. Obviously this isn't a perfect 1:1 comparison since I was using the naive implementation, but the authors indicate they are able to create networks that outperform ResNet in training and evaluation performance on pytorch. To my knowledge, I don't think TF supports any easy to use operations that could be used to re-create this memory efficient implementation.</p>\n<p>DenseNet won the 2017 CVPR best paper award, so I think support for the capabilities necessary to implement DenseNet efficiently seems particularly worthwhile for TF. I'd look into implementing it myself but I've already used up my free cycles for infrastructure on a number of other contributions here and in Keras, so I'm sticking to ResNet based solutions at this time. Thanks for your consideration!</p>", "body_text": "I'd like to provide some additional motivation for this feature request.\nI'm able to train a Resnet50 based architecture for my problem with a batch size of 32 on the older 12GB Titan X GPU. However, a DenseNet based architecture that should actually be fairly small, with 40 layers and 500k parameters can only fit a batch size of 6, produces some tf memory warnings, and runs 8x slower during training, leading a day of training to take over a week, and smaller batch sizes were even slower. Obviously this isn't a perfect 1:1 comparison since I was using the naive implementation, but the authors indicate they are able to create networks that outperform ResNet in training and evaluation performance on pytorch. To my knowledge, I don't think TF supports any easy to use operations that could be used to re-create this memory efficient implementation.\nDenseNet won the 2017 CVPR best paper award, so I think support for the capabilities necessary to implement DenseNet efficiently seems particularly worthwhile for TF. I'd look into implementing it myself but I've already used up my free cycles for infrastructure on a number of other contributions here and in Keras, so I'm sticking to ResNet based solutions at this time. Thanks for your consideration!", "body": "I'd like to provide some additional motivation for this feature request. \r\n\r\nI'm able to train a Resnet50 based architecture for my problem with a batch size of 32 on the older 12GB Titan X GPU. However, a DenseNet based architecture that should actually be fairly small, with 40 layers and 500k parameters can only fit a batch size of 6, produces some tf memory warnings, and runs 8x slower during training, leading a day of training to take over a week, and smaller batch sizes were even slower. Obviously this isn't a perfect 1:1 comparison since I was using the naive implementation, but the authors indicate they are able to create networks that outperform ResNet in training and evaluation performance on pytorch. To my knowledge, I don't think TF supports any easy to use operations that could be used to re-create this memory efficient implementation.\r\n\r\nDenseNet won the 2017 CVPR best paper award, so I think support for the capabilities necessary to implement DenseNet efficiently seems particularly worthwhile for TF. I'd look into implementing it myself but I've already used up my free cycles for infrastructure on a number of other contributions here and in Keras, so I'm sticking to ResNet based solutions at this time. Thanks for your consideration!\r\n"}