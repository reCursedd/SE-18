{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12948", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12948/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12948/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12948/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12948", "id": 256532521, "node_id": "MDU6SXNzdWUyNTY1MzI1MjE=", "number": 12948, "title": "feature request: shared memory concat with new allocation for subsequent operations", "user": {"login": "ahundt", "id": 55744, "node_id": "MDQ6VXNlcjU1NzQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/55744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahundt", "html_url": "https://github.com/ahundt", "followers_url": "https://api.github.com/users/ahundt/followers", "following_url": "https://api.github.com/users/ahundt/following{/other_user}", "gists_url": "https://api.github.com/users/ahundt/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahundt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahundt/subscriptions", "organizations_url": "https://api.github.com/users/ahundt/orgs", "repos_url": "https://api.github.com/users/ahundt/repos", "events_url": "https://api.github.com/users/ahundt/events{/privacy}", "received_events_url": "https://api.github.com/users/ahundt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 29, "created_at": "2017-09-10T19:13:20Z", "updated_at": "2018-11-16T19:08:18Z", "closed_at": "2018-11-10T22:16:09Z", "author_association": "NONE", "body_html": "<p>DenseNet is an effective network design that relies on applying nn layers on recursive concatenations of data along the channel axis. Unfortunately, this has the side effect of quadratic memory growth in TensorFlow as completely new blocks of memory are allocated after each concat operation, resulting in poor performance during all phases of execution.</p>\n<p>This is a feature request for a new <code>allocation='shared'</code> option for operations such as <code>tf.concat(allocation='shared')</code> which works seamlessly with later operations that might modify the data such as BatchNorm, which might also need to share memory. This would make it is possible to utilize the <a href=\"http://arxiv.org/abs/1707.06990\" rel=\"nofollow\">Memory-Efficient Implementation of DenseNets</a>, a paper which demonstrates that this memory utilization can be dramatically reduced through sharing of allocations. This image from the paper + pytorch implementation illustrates the shared memory approach:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/d3370ec4935b4bc92b736a122bc226abf48988fd/68747470733a2f2f7261772e6769746875622e636f6d2f67706c656973732f656666696369656e745f64656e73656e65745f7079746f7263682f6d61737465722f696d616765732f666f72776172642e706e67\"><img src=\"https://camo.githubusercontent.com/d3370ec4935b4bc92b736a122bc226abf48988fd/68747470733a2f2f7261772e6769746875622e636f6d2f67706c656973732f656666696369656e745f64656e73656e65745f7079746f7263682f6d61737465722f696d616765732f666f72776172642e706e67\" alt=\"densenet shared memory\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li><a href=\"https://github.com/gpleiss/efficient_densenet_pytorch\">Pytorch efficient DenseNet implementation</a></li>\n<li><a href=\"https://github.com/titu1994/keras-contrib/blob/ff47da56fbd54cf6cdc2ac2218529fbdadf99296/keras_contrib/applications/densenet.py\">Keras DenseNet Implementation</a> with \"naive\" allocations, works with TensorFlow backend.</li>\n</ul>\n<p>This functionality would also be useful for any other application or future network design that employs recursive concatenations. Just in case I didn't find it in my search, perhaps a mechanism already exists that can meet these goals?</p>", "body_text": "DenseNet is an effective network design that relies on applying nn layers on recursive concatenations of data along the channel axis. Unfortunately, this has the side effect of quadratic memory growth in TensorFlow as completely new blocks of memory are allocated after each concat operation, resulting in poor performance during all phases of execution.\nThis is a feature request for a new allocation='shared' option for operations such as tf.concat(allocation='shared') which works seamlessly with later operations that might modify the data such as BatchNorm, which might also need to share memory. This would make it is possible to utilize the Memory-Efficient Implementation of DenseNets, a paper which demonstrates that this memory utilization can be dramatically reduced through sharing of allocations. This image from the paper + pytorch implementation illustrates the shared memory approach:\n\n\nPytorch efficient DenseNet implementation\nKeras DenseNet Implementation with \"naive\" allocations, works with TensorFlow backend.\n\nThis functionality would also be useful for any other application or future network design that employs recursive concatenations. Just in case I didn't find it in my search, perhaps a mechanism already exists that can meet these goals?", "body": "DenseNet is an effective network design that relies on applying nn layers on recursive concatenations of data along the channel axis. Unfortunately, this has the side effect of quadratic memory growth in TensorFlow as completely new blocks of memory are allocated after each concat operation, resulting in poor performance during all phases of execution.\r\n\r\nThis is a feature request for a new `allocation='shared'` option for operations such as `tf.concat(allocation='shared')` which works seamlessly with later operations that might modify the data such as BatchNorm, which might also need to share memory. This would make it is possible to utilize the [Memory-Efficient Implementation of DenseNets](http://arxiv.org/abs/1707.06990), a paper which demonstrates that this memory utilization can be dramatically reduced through sharing of allocations. This image from the paper + pytorch implementation illustrates the shared memory approach:\r\n\r\n![densenet shared memory](https://camo.githubusercontent.com/d3370ec4935b4bc92b736a122bc226abf48988fd/68747470733a2f2f7261772e6769746875622e636f6d2f67706c656973732f656666696369656e745f64656e73656e65745f7079746f7263682f6d61737465722f696d616765732f666f72776172642e706e67)\r\n\r\n- [Pytorch efficient DenseNet implementation](https://github.com/gpleiss/efficient_densenet_pytorch)\r\n- [Keras DenseNet Implementation](https://github.com/titu1994/keras-contrib/blob/ff47da56fbd54cf6cdc2ac2218529fbdadf99296/keras_contrib/applications/densenet.py) with \"naive\" allocations, works with TensorFlow backend.\r\n\r\nThis functionality would also be useful for any other application or future network design that employs recursive concatenations. Just in case I didn't find it in my search, perhaps a mechanism already exists that can meet these goals?\r\n"}