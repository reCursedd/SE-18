{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4709", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4709/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4709/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4709/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4709", "id": 180467114, "node_id": "MDU6SXNzdWUxODA0NjcxMTQ=", "number": 4709, "title": "Is tensorflow consuming much more memory than torch?", "user": {"login": "yxchng", "id": 10518587, "node_id": "MDQ6VXNlcjEwNTE4NTg3", "avatar_url": "https://avatars3.githubusercontent.com/u/10518587?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yxchng", "html_url": "https://github.com/yxchng", "followers_url": "https://api.github.com/users/yxchng/followers", "following_url": "https://api.github.com/users/yxchng/following{/other_user}", "gists_url": "https://api.github.com/users/yxchng/gists{/gist_id}", "starred_url": "https://api.github.com/users/yxchng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yxchng/subscriptions", "organizations_url": "https://api.github.com/users/yxchng/orgs", "repos_url": "https://api.github.com/users/yxchng/repos", "events_url": "https://api.github.com/users/yxchng/events{/privacy}", "received_events_url": "https://api.github.com/users/yxchng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2016-10-01T16:53:29Z", "updated_at": "2016-11-11T17:45:55Z", "closed_at": "2016-11-11T17:45:55Z", "author_association": "NONE", "body_html": "<p>I am trying to replicate the stacked hourglass architecture which is implemented in torch, <a href=\"https://github.com/anewell/pose-hg-train/\">https://github.com/anewell/pose-hg-train/</a>. However, I am unable to train it using settings in the opts.lua file which set the train batch size as 6, number of train iteration as 8000, test batch size as 1 and number of test iteration as 1000. I understand that I can reduce the batch size or iteration number to fit the network. However, I just want to know if tensorflow is consuming more memory than torch, if the same architecture and settings can be implemented in tensorflow.</p>", "body_text": "I am trying to replicate the stacked hourglass architecture which is implemented in torch, https://github.com/anewell/pose-hg-train/. However, I am unable to train it using settings in the opts.lua file which set the train batch size as 6, number of train iteration as 8000, test batch size as 1 and number of test iteration as 1000. I understand that I can reduce the batch size or iteration number to fit the network. However, I just want to know if tensorflow is consuming more memory than torch, if the same architecture and settings can be implemented in tensorflow.", "body": "I am trying to replicate the stacked hourglass architecture which is implemented in torch, https://github.com/anewell/pose-hg-train/. However, I am unable to train it using settings in the opts.lua file which set the train batch size as 6, number of train iteration as 8000, test batch size as 1 and number of test iteration as 1000. I understand that I can reduce the batch size or iteration number to fit the network. However, I just want to know if tensorflow is consuming more memory than torch, if the same architecture and settings can be implemented in tensorflow.\n"}