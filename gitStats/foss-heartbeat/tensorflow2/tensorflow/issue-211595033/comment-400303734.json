{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/400303734", "html_url": "https://github.com/tensorflow/tensorflow/issues/8042#issuecomment-400303734", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8042", "id": 400303734, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDMwMzczNA==", "user": {"login": "rodrigofp-cit", "id": 31069458, "node_id": "MDQ6VXNlcjMxMDY5NDU4", "avatar_url": "https://avatars0.githubusercontent.com/u/31069458?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rodrigofp-cit", "html_url": "https://github.com/rodrigofp-cit", "followers_url": "https://api.github.com/users/rodrigofp-cit/followers", "following_url": "https://api.github.com/users/rodrigofp-cit/following{/other_user}", "gists_url": "https://api.github.com/users/rodrigofp-cit/gists{/gist_id}", "starred_url": "https://api.github.com/users/rodrigofp-cit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rodrigofp-cit/subscriptions", "organizations_url": "https://api.github.com/users/rodrigofp-cit/orgs", "repos_url": "https://api.github.com/users/rodrigofp-cit/repos", "events_url": "https://api.github.com/users/rodrigofp-cit/events{/privacy}", "received_events_url": "https://api.github.com/users/rodrigofp-cit/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-26T13:16:31Z", "updated_at": "2018-06-26T13:16:31Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9796812\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/deepaksuresh\">@deepaksuresh</a> Now I realized what I was doing wrong.</p>\n<p><strong>First</strong>: I was not passing <strong>export_outputs</strong> argument to <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\" rel=\"nofollow\">tf.estimator.EstimatorSpec</a>. Now I'm doing this on my model_fn</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(\n                <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n                <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>prediction_dict,\n                <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>total_loss,\n                <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op,\n                <span class=\"pl-v\">eval_metric_ops</span><span class=\"pl-k\">=</span>eval_metric_ops,\n                <span class=\"pl-v\">export_outputs</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>predict<span class=\"pl-pds\">'</span></span>:tf.estimator.export.PredictOutput(<span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>prediction)})</pre></div>\n<p><strong>Second</strong>: I was trying to create a serving function that not only map what is received to a FixedLenFeature, but also decoded it to an image (in my case). This was causing trouble for me, so I decided to just make the mapping. This is my code;</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">get_serving_fn</span>():        \n        feature_spec <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>raw_bytes<span class=\"pl-pds\">'</span></span>:tf.FixedLenFeature(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.string,<span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>())}       \n        <span class=\"pl-k\">return</span>  tf.estimator.export.build_parsing_serving_input_receiver_fn(<span class=\"pl-v\">feature_spec</span><span class=\"pl-k\">=</span>feature_spec)</pre></div>\n<p>And on my model_fn I check whether what is received as <strong>features</strong> is a dict(export mode) or just a Tensor(eval or train)</p>\n<div class=\"highlight highlight-source-python\"><pre> <span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">params</span>):            \n            <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">!=</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">PREDICT</span>:\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> unfortunately, when exporting a saved model, features is a dict of Tensors, instead of a Tensor itself                                </span>\n                image_batch <span class=\"pl-k\">=</span> tf.map_fn(<span class=\"pl-c1\">self</span>.preproc_fn,features,<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n                tf.logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_fn called for training, evaluation or prediction, not export<span class=\"pl-pds\">'</span></span>)\n                tf.logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Input shape not export: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(features.get_shape()))\n            <span class=\"pl-k\">else</span>:\n                image_batch <span class=\"pl-k\">=</span> tf.map_fn(<span class=\"pl-c1\">self</span>.preproc_fn,features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>raw_bytes<span class=\"pl-pds\">'</span></span>],<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n                tf.logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_fn called to export saved_model<span class=\"pl-pds\">'</span></span>)\n                tf.logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Input shape export: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(image_batch.get_shape()))</pre></div>\n<p>And export the saved model with</p>\n<div class=\"highlight highlight-source-python\"><pre>estimator.export_savedmodel(<span class=\"pl-v\">export_dir_base</span><span class=\"pl-k\">=</span>output_dir,\n                                <span class=\"pl-v\">serving_input_receiver_fn</span><span class=\"pl-k\">=</span>get_serving_fn())</pre></div>\n<p>Well, to conclude. I've fixed the bug.</p>", "body_text": "@deepaksuresh Now I realized what I was doing wrong.\nFirst: I was not passing export_outputs argument to tf.estimator.EstimatorSpec. Now I'm doing this on my model_fn\nreturn tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=prediction_dict,\n                loss=total_loss,\n                train_op=train_op,\n                eval_metric_ops=eval_metric_ops,\n                export_outputs={'predict':tf.estimator.export.PredictOutput(outputs=prediction)})\nSecond: I was trying to create a serving function that not only map what is received to a FixedLenFeature, but also decoded it to an image (in my case). This was causing trouble for me, so I decided to just make the mapping. This is my code;\ndef get_serving_fn():        \n        feature_spec = {'raw_bytes':tf.FixedLenFeature(dtype=tf.string,default_value=\"\",shape=())}       \n        return  tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec=feature_spec)\nAnd on my model_fn I check whether what is received as features is a dict(export mode) or just a Tensor(eval or train)\n def model_fn(features, labels, mode, params):            \n            if mode != tf.estimator.ModeKeys.PREDICT:\n                # unfortunately, when exporting a saved model, features is a dict of Tensors, instead of a Tensor itself                                \n                image_batch = tf.map_fn(self.preproc_fn,features,dtype=tf.float32)\n                tf.logging.info('model_fn called for training, evaluation or prediction, not export')\n                tf.logging.info('Input shape not export: {}'.format(features.get_shape()))\n            else:\n                image_batch = tf.map_fn(self.preproc_fn,features['raw_bytes'],dtype=tf.float32)\n                tf.logging.info('model_fn called to export saved_model')\n                tf.logging.info('Input shape export: {}'.format(image_batch.get_shape()))\nAnd export the saved model with\nestimator.export_savedmodel(export_dir_base=output_dir,\n                                serving_input_receiver_fn=get_serving_fn())\nWell, to conclude. I've fixed the bug.", "body": "@deepaksuresh Now I realized what I was doing wrong.\r\n\r\n**First**: I was not passing **export_outputs** argument to [tf.estimator.EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec). Now I'm doing this on my model_fn\r\n\r\n```python\r\nreturn tf.estimator.EstimatorSpec(\r\n                mode=mode,\r\n                predictions=prediction_dict,\r\n                loss=total_loss,\r\n                train_op=train_op,\r\n                eval_metric_ops=eval_metric_ops,\r\n                export_outputs={'predict':tf.estimator.export.PredictOutput(outputs=prediction)})\r\n```\r\n\r\n**Second**: I was trying to create a serving function that not only map what is received to a FixedLenFeature, but also decoded it to an image (in my case). This was causing trouble for me, so I decided to just make the mapping. This is my code;\r\n\r\n```python\r\ndef get_serving_fn():        \r\n        feature_spec = {'raw_bytes':tf.FixedLenFeature(dtype=tf.string,default_value=\"\",shape=())}       \r\n        return  tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec=feature_spec)\r\n```\r\n\r\nAnd on my model_fn I check whether what is received as **features** is a dict(export mode) or just a Tensor(eval or train)\r\n\r\n```python\r\n def model_fn(features, labels, mode, params):            \r\n            if mode != tf.estimator.ModeKeys.PREDICT:\r\n                # unfortunately, when exporting a saved model, features is a dict of Tensors, instead of a Tensor itself                                \r\n                image_batch = tf.map_fn(self.preproc_fn,features,dtype=tf.float32)\r\n                tf.logging.info('model_fn called for training, evaluation or prediction, not export')\r\n                tf.logging.info('Input shape not export: {}'.format(features.get_shape()))\r\n            else:\r\n                image_batch = tf.map_fn(self.preproc_fn,features['raw_bytes'],dtype=tf.float32)\r\n                tf.logging.info('model_fn called to export saved_model')\r\n                tf.logging.info('Input shape export: {}'.format(image_batch.get_shape()))\r\n```\r\n\r\nAnd export the saved model with\r\n\r\n```python\r\nestimator.export_savedmodel(export_dir_base=output_dir,\r\n                                serving_input_receiver_fn=get_serving_fn())\r\n```\r\n\r\nWell, to conclude. I've fixed the bug."}