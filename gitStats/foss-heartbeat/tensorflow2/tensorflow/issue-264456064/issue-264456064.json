{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13621", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13621/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13621/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13621/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13621", "id": 264456064, "node_id": "MDU6SXNzdWUyNjQ0NTYwNjQ=", "number": 13621, "title": "\u201cNone\u201d value for gradient of Tensorflow variables which is used in the network", "user": {"login": "alisahaf70", "id": 2054666, "node_id": "MDQ6VXNlcjIwNTQ2NjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/2054666?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alisahaf70", "html_url": "https://github.com/alisahaf70", "followers_url": "https://api.github.com/users/alisahaf70/followers", "following_url": "https://api.github.com/users/alisahaf70/following{/other_user}", "gists_url": "https://api.github.com/users/alisahaf70/gists{/gist_id}", "starred_url": "https://api.github.com/users/alisahaf70/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alisahaf70/subscriptions", "organizations_url": "https://api.github.com/users/alisahaf70/orgs", "repos_url": "https://api.github.com/users/alisahaf70/repos", "events_url": "https://api.github.com/users/alisahaf70/events{/privacy}", "received_events_url": "https://api.github.com/users/alisahaf70/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-11T04:45:03Z", "updated_at": "2017-10-11T19:42:21Z", "closed_at": "2017-10-11T19:42:20Z", "author_association": "NONE", "body_html": "<p>I want to develop a custom Seq2Seq in tensorflow and I've created this part of network</p>\n<p>`wflat0 = tf.get_variable(\"wflat0\", shape=(1024, 1024),<br>\ninitializer=LinearInitializer)<br>\nbflat0 = tf.get_variable(\"bflat0\", shape=1024, initializer=BiasInitializer)</p>\n<p>l0flat = selu(tf.matmul(x, wflat0) + bflat0)<br>\nx=tf.reshape(l0flat,[shape[0],shape[1],1024])<br>\nx = tf.unstack(x, constant.Lstm_cell, 1)<br>\nlstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)<br>\noutputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)</p>\n<p>decoder = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)</p>\n<p>y = tf.unstack(tf.reshape(self.y_,[tf.shape(self.y_)[0],tf.shape(self.y_)[1],1]), constant.Lstm_cell, 1)<br>\ndecoutputs, decstates = tf.contrib.rnn.static_rnn(decoder, y, dtype=tf.float32)</p>\n<p>wflat1 = tf.get_variable(\"wflat1\", shape=(LSTMHiddenSize, 128),<br>\ninitializer=LinearInitializer)<br>\nbflat1 = tf.get_variable(\"bflat1\", shape=128, initializer=BiasInitializer)<br>\nshapes = tf.shape(decoutputs)<br>\ndecoutputs=tf.transpose(decoutputs, perm=[1, 0, 2])<br>\ndecoutputs=tf.reshape(decoutputs,(shapes[0]*shapes[1],shapes[2]))<br>\nself.sss=decoutputs<br>\nl1flat = selu(tf.matmul(decoutputs, wflat1) + bflat1)<br>\nwflat2 = tf.get_variable(\"wflat2\", shape=(128, 1), initializer=LinearInitializer)<br>\nbflat2 = tf.get_variable(\"bflat2\", shape=1, initializer=BiasInitializer)<br>\nl2flat = tf.matmul(l1flat, wflat2) + bflat2<br>\nl2flat = tf.reshape(l2flat,(constant.batchsize , constant.Lstm_cell))<br>\nself.sigout=tf.nn.softmax(l2flat)<br>\nself.out=l2flat<br>\ncost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.out, labels=self.y_))`</p>\n<p>and this network create NAN output after first update iteration for large sequence and it just works for small sequence so I've decided to add gradient clipping and when I was developing that I've found that most of the variables have a none gradient as you can see in this <a href=\"https://i.stack.imgur.com/XtkY0.png\" rel=\"nofollow\">picture Imag</a>e of computed Gradient how could it possible that some variables which take participate in loss function have a none gradient is this a bug?</p>", "body_text": "I want to develop a custom Seq2Seq in tensorflow and I've created this part of network\n`wflat0 = tf.get_variable(\"wflat0\", shape=(1024, 1024),\ninitializer=LinearInitializer)\nbflat0 = tf.get_variable(\"bflat0\", shape=1024, initializer=BiasInitializer)\nl0flat = selu(tf.matmul(x, wflat0) + bflat0)\nx=tf.reshape(l0flat,[shape[0],shape[1],1024])\nx = tf.unstack(x, constant.Lstm_cell, 1)\nlstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)\noutputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\ndecoder = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)\ny = tf.unstack(tf.reshape(self.y_,[tf.shape(self.y_)[0],tf.shape(self.y_)[1],1]), constant.Lstm_cell, 1)\ndecoutputs, decstates = tf.contrib.rnn.static_rnn(decoder, y, dtype=tf.float32)\nwflat1 = tf.get_variable(\"wflat1\", shape=(LSTMHiddenSize, 128),\ninitializer=LinearInitializer)\nbflat1 = tf.get_variable(\"bflat1\", shape=128, initializer=BiasInitializer)\nshapes = tf.shape(decoutputs)\ndecoutputs=tf.transpose(decoutputs, perm=[1, 0, 2])\ndecoutputs=tf.reshape(decoutputs,(shapes[0]*shapes[1],shapes[2]))\nself.sss=decoutputs\nl1flat = selu(tf.matmul(decoutputs, wflat1) + bflat1)\nwflat2 = tf.get_variable(\"wflat2\", shape=(128, 1), initializer=LinearInitializer)\nbflat2 = tf.get_variable(\"bflat2\", shape=1, initializer=BiasInitializer)\nl2flat = tf.matmul(l1flat, wflat2) + bflat2\nl2flat = tf.reshape(l2flat,(constant.batchsize , constant.Lstm_cell))\nself.sigout=tf.nn.softmax(l2flat)\nself.out=l2flat\ncost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.out, labels=self.y_))`\nand this network create NAN output after first update iteration for large sequence and it just works for small sequence so I've decided to add gradient clipping and when I was developing that I've found that most of the variables have a none gradient as you can see in this picture Image of computed Gradient how could it possible that some variables which take participate in loss function have a none gradient is this a bug?", "body": "I want to develop a custom Seq2Seq in tensorflow and I've created this part of network\r\n\r\n`wflat0 = tf.get_variable(\"wflat0\", shape=(1024, 1024), \r\ninitializer=LinearInitializer)\r\nbflat0 = tf.get_variable(\"bflat0\", shape=1024, initializer=BiasInitializer)\r\n\r\nl0flat = selu(tf.matmul(x, wflat0) + bflat0)\r\nx=tf.reshape(l0flat,[shape[0],shape[1],1024])\r\nx = tf.unstack(x, constant.Lstm_cell, 1)\r\nlstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)\r\noutputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n\r\ndecoder = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)\r\n\r\ny = tf.unstack(tf.reshape(self.y_,[tf.shape(self.y_)[0],tf.shape(self.y_)[1],1]), constant.Lstm_cell, 1)\r\ndecoutputs, decstates = tf.contrib.rnn.static_rnn(decoder, y, dtype=tf.float32)\r\n\r\nwflat1 = tf.get_variable(\"wflat1\", shape=(LSTMHiddenSize, 128),\r\n                                     initializer=LinearInitializer)\r\nbflat1 = tf.get_variable(\"bflat1\", shape=128, initializer=BiasInitializer)\r\nshapes = tf.shape(decoutputs)\r\ndecoutputs=tf.transpose(decoutputs, perm=[1, 0, 2])\r\ndecoutputs=tf.reshape(decoutputs,(shapes[0]*shapes[1],shapes[2]))\r\nself.sss=decoutputs\r\nl1flat = selu(tf.matmul(decoutputs, wflat1) + bflat1)\r\nwflat2 = tf.get_variable(\"wflat2\", shape=(128, 1), initializer=LinearInitializer)\r\nbflat2 = tf.get_variable(\"bflat2\", shape=1, initializer=BiasInitializer)\r\nl2flat = tf.matmul(l1flat, wflat2) + bflat2\r\nl2flat = tf.reshape(l2flat,(constant.batchsize , constant.Lstm_cell))\r\nself.sigout=tf.nn.softmax(l2flat)\r\nself.out=l2flat\r\ncost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.out, labels=self.y_))`\r\n\r\nand this network create NAN output after first update iteration for large sequence and it just works for small sequence so I've decided to add gradient clipping and when I was developing that I've found that most of the variables have a none gradient as you can see in this [picture Imag](https://i.stack.imgur.com/XtkY0.png)e of computed Gradient how could it possible that some variables which take participate in loss function have a none gradient is this a bug?"}