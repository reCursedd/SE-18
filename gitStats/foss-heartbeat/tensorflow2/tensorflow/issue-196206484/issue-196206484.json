{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6374", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6374/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6374/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6374/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6374", "id": 196206484, "node_id": "MDU6SXNzdWUxOTYyMDY0ODQ=", "number": 6374, "title": "BUG when trainning with multiple ps-server", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-12-17T06:43:11Z", "updated_at": "2017-04-10T22:21:05Z", "closed_at": "2017-01-16T03:41:37Z", "author_association": "NONE", "body_html": "<p>I post a issue previous <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"195728676\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6326\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6326/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6326\">#6326</a></p>\n<p>The original problem is solved by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>  with add <code>sharded=True</code> to Saver's <strong>init</strong> .</p>\n<h4>BUT , One more problem occurs:</h4>\n<ul>\n<li>when I training with 5 workers and <strong>m (m=1)</strong> ps-server, then training process works well.</li>\n<li>BUT when training with 5 workers and <strong>m (m&gt;1)</strong> ps-servers, then training process crash with error <code>NotFoundError (see above for traceback): ./supervisor/model.ckpt-0_temp_994ae96906954e838fc2f3481ce8f296/part-00001-of-00002.data-00000-of-00001</code></li>\n</ul>\n<p>It seems that when have multiple ps-servers , the checkpoint-save has some weird bugs.</p>\n<p>The detail error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"distributed_deepcake.py\", line 441, in &lt;module&gt;\n    exit(1)\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\n    yield\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\n    self.run_loop()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\n    global_step=self._sv.global_step)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\n    {self.saver_def.filename_tensor_name: checkpoint_file})\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'save/SaveV2_4', defined at:\n  File \"distributed_deepcake.py\", line 293, in &lt;module&gt;\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\n    tensors)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\n    tensors=tensors, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>Only find one issue and it's myself . <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"195728676\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6326\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6326/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6326\">#6326</a></p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\ncentos 6.5 ,glibc 2.17, gcc6.2, Python 2.7.12, <code>tensorflow newest version 0.12rc1 with cpu</code></p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n    # Read TFRecords files for training\n    filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.train),\n        num_epochs=epoch_number)\n    serialized_example = read_and_decode(filename_queue)\n    batch_serialized_example = tf.train.shuffle_batch(\n        [serialized_example],\n        batch_size=batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    features = tf.parse_example(\n        batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    batch_labels = features[\"label\"]\n    batch_ids = features[\"ids\"]\n    batch_values = features[\"values\"]\n\n    # Read TFRecords file for validatioin\n    validate_filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.eval),\n        num_epochs=epoch_number)\n    validate_serialized_example = read_and_decode(validate_filename_queue)\n    validate_batch_serialized_example = tf.train.shuffle_batch(\n        [validate_serialized_example],\n        batch_size=validate_batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    validate_features = tf.parse_example(\n        validate_batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    validate_batch_labels = features[\"label\"]\n    validate_batch_ids = features[\"ids\"]\n    validate_batch_values = features[\"values\"]\n    logits = inference(batch_ids, batch_values)\n    batch_labels = tf.to_int64(batch_labels)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n                                                                   batch_labels)\n    loss = tf.reduce_mean(cross_entropy, name='loss')\n\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\n\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n\n    # Initialize saver and summary\n    steps_to_validate = FLAGS.steps_to_validate\n    init_op = tf.initialize_all_variables()\n\n    saver = tf.train.Saver(max_to_keep = 2)\n    keys_placeholder = tf.placeholder(\"float\")\n    keys = tf.identity(keys_placeholder)\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\n                                                'softmax': inference_softmax.name,\n                                                'prediction': inference_op.name}))\n\n    summary_op = tf.merge_all_summaries()\n\n\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                         logdir=\"./supervisor/\",\n                         init_op=init_op,\n                         summary_op=summary_op,\n                         saver=saver,\n                         global_step=global_step,\n                         save_model_secs=60)\n\nwith sv.managed_session(server.target) as sess:\n\n    while not sv.should_stop():\n        # Get coordinator and run queues to read data\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        try:\n            while not coord.should_stop():\n                _, loss_value, step = sess.run([train_op, loss, global_step])\n\n        except tf.errors.OutOfRangeError:\n            print(\"Done training after reading all data\")\n        finally:\n            coord.request_stop()\n\n        coord.join(threads)\n</code></pre>", "body_text": "I post a issue previous #6326\nThe original problem is solved by @yaroslavvb  with add sharded=True to Saver's init .\nBUT , One more problem occurs:\n\nwhen I training with 5 workers and m (m=1) ps-server, then training process works well.\nBUT when training with 5 workers and m (m>1) ps-servers, then training process crash with error NotFoundError (see above for traceback): ./supervisor/model.ckpt-0_temp_994ae96906954e838fc2f3481ce8f296/part-00001-of-00002.data-00000-of-00001\n\nIt seems that when have multiple ps-servers , the checkpoint-save has some weird bugs.\nThe detail error:\nTraceback (most recent call last):\n  File \"distributed_deepcake.py\", line 441, in <module>\n    exit(1)\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\n    yield\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\n    self.run_loop()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\n    global_step=self._sv.global_step)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\n    {self.saver_def.filename_tensor_name: checkpoint_file})\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'save/SaveV2_4', defined at:\n  File \"distributed_deepcake.py\", line 293, in <module>\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\n    tensors)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\n    tensors=tensors, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nOnly find one issue and it's myself . #6326\nEnvironment info\nOperating System:\ncentos 6.5 ,glibc 2.17, gcc6.2, Python 2.7.12, tensorflow newest version 0.12rc1 with cpu\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nwith tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n    # Read TFRecords files for training\n    filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.train),\n        num_epochs=epoch_number)\n    serialized_example = read_and_decode(filename_queue)\n    batch_serialized_example = tf.train.shuffle_batch(\n        [serialized_example],\n        batch_size=batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    features = tf.parse_example(\n        batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    batch_labels = features[\"label\"]\n    batch_ids = features[\"ids\"]\n    batch_values = features[\"values\"]\n\n    # Read TFRecords file for validatioin\n    validate_filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.eval),\n        num_epochs=epoch_number)\n    validate_serialized_example = read_and_decode(validate_filename_queue)\n    validate_batch_serialized_example = tf.train.shuffle_batch(\n        [validate_serialized_example],\n        batch_size=validate_batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    validate_features = tf.parse_example(\n        validate_batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    validate_batch_labels = features[\"label\"]\n    validate_batch_ids = features[\"ids\"]\n    validate_batch_values = features[\"values\"]\n    logits = inference(batch_ids, batch_values)\n    batch_labels = tf.to_int64(batch_labels)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n                                                                   batch_labels)\n    loss = tf.reduce_mean(cross_entropy, name='loss')\n\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\n\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n\n    # Initialize saver and summary\n    steps_to_validate = FLAGS.steps_to_validate\n    init_op = tf.initialize_all_variables()\n\n    saver = tf.train.Saver(max_to_keep = 2)\n    keys_placeholder = tf.placeholder(\"float\")\n    keys = tf.identity(keys_placeholder)\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\n                                                'softmax': inference_softmax.name,\n                                                'prediction': inference_op.name}))\n\n    summary_op = tf.merge_all_summaries()\n\n\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                         logdir=\"./supervisor/\",\n                         init_op=init_op,\n                         summary_op=summary_op,\n                         saver=saver,\n                         global_step=global_step,\n                         save_model_secs=60)\n\nwith sv.managed_session(server.target) as sess:\n\n    while not sv.should_stop():\n        # Get coordinator and run queues to read data\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        try:\n            while not coord.should_stop():\n                _, loss_value, step = sess.run([train_op, loss, global_step])\n\n        except tf.errors.OutOfRangeError:\n            print(\"Done training after reading all data\")\n        finally:\n            coord.request_stop()\n\n        coord.join(threads)", "body": "I post a issue previous https://github.com/tensorflow/tensorflow/issues/6326\r\n\r\nThe original problem is solved by @yaroslavvb  with add `sharded=True` to Saver's __init__ .\r\n\r\n#### BUT , One more problem occurs: \r\n\r\n* when I training with 5 workers and **m (m=1)** ps-server, then training process works well.\r\n* BUT when training with 5 workers and **m (m>1)** ps-servers, then training process crash with error `NotFoundError (see above for traceback): ./supervisor/model.ckpt-0_temp_994ae96906954e838fc2f3481ce8f296/part-00001-of-00002.data-00000-of-00001`\r\n\r\nIt seems that when have multiple ps-servers , the checkpoint-save has some weird bugs.\r\n\r\nThe detail error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"distributed_deepcake.py\", line 441, in <module>\r\n    exit(1)\r\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\r\n    yield\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\r\n    self.run_loop()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\r\n    global_step=self._sv.global_step)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'save/SaveV2_4', defined at:\r\n  File \"distributed_deepcake.py\", line 293, in <module>\r\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\r\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\r\n    tensors)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\r\n    tensors=tensors, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nOnly find one issue and it's myself . https://github.com/tensorflow/tensorflow/issues/6326\r\n\r\n### Environment info\r\nOperating System:\r\ncentos 6.5 ,glibc 2.17, gcc6.2, Python 2.7.12, `tensorflow newest version 0.12rc1 with cpu`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nwith tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n    # Read TFRecords files for training\r\n    filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.train),\r\n        num_epochs=epoch_number)\r\n    serialized_example = read_and_decode(filename_queue)\r\n    batch_serialized_example = tf.train.shuffle_batch(\r\n        [serialized_example],\r\n        batch_size=batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    features = tf.parse_example(\r\n        batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    batch_labels = features[\"label\"]\r\n    batch_ids = features[\"ids\"]\r\n    batch_values = features[\"values\"]\r\n\r\n    # Read TFRecords file for validatioin\r\n    validate_filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.eval),\r\n        num_epochs=epoch_number)\r\n    validate_serialized_example = read_and_decode(validate_filename_queue)\r\n    validate_batch_serialized_example = tf.train.shuffle_batch(\r\n        [validate_serialized_example],\r\n        batch_size=validate_batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    validate_features = tf.parse_example(\r\n        validate_batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    validate_batch_labels = features[\"label\"]\r\n    validate_batch_ids = features[\"ids\"]\r\n    validate_batch_values = features[\"values\"]\r\n    logits = inference(batch_ids, batch_values)\r\n    batch_labels = tf.to_int64(batch_labels)\r\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\r\n                                                                   batch_labels)\r\n    loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\r\n\r\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\r\n\r\n    global_step = tf.Variable(0, name='global_step', trainable=False)\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n    # Initialize saver and summary\r\n    steps_to_validate = FLAGS.steps_to_validate\r\n    init_op = tf.initialize_all_variables()\r\n\r\n    saver = tf.train.Saver(max_to_keep = 2)\r\n    keys_placeholder = tf.placeholder(\"float\")\r\n    keys = tf.identity(keys_placeholder)\r\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\r\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\r\n                                                'softmax': inference_softmax.name,\r\n                                                'prediction': inference_op.name}))\r\n\r\n    summary_op = tf.merge_all_summaries()\r\n\r\n\r\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                         logdir=\"./supervisor/\",\r\n                         init_op=init_op,\r\n                         summary_op=summary_op,\r\n                         saver=saver,\r\n                         global_step=global_step,\r\n                         save_model_secs=60)\r\n\r\nwith sv.managed_session(server.target) as sess:\r\n\r\n    while not sv.should_stop():\r\n        # Get coordinator and run queues to read data\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n        try:\r\n            while not coord.should_stop():\r\n                _, loss_value, step = sess.run([train_op, loss, global_step])\r\n\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done training after reading all data\")\r\n        finally:\r\n            coord.request_stop()\r\n\r\n        coord.join(threads)\r\n```\r\n"}