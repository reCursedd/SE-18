{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417885688", "html_url": "https://github.com/tensorflow/tensorflow/issues/21389#issuecomment-417885688", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21389", "id": 417885688, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzg4NTY4OA==", "user": {"login": "lgeiger", "id": 13285808, "node_id": "MDQ6VXNlcjEzMjg1ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/13285808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lgeiger", "html_url": "https://github.com/lgeiger", "followers_url": "https://api.github.com/users/lgeiger/followers", "following_url": "https://api.github.com/users/lgeiger/following{/other_user}", "gists_url": "https://api.github.com/users/lgeiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/lgeiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lgeiger/subscriptions", "organizations_url": "https://api.github.com/users/lgeiger/orgs", "repos_url": "https://api.github.com/users/lgeiger/repos", "events_url": "https://api.github.com/users/lgeiger/events{/privacy}", "received_events_url": "https://api.github.com/users/lgeiger/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-01T20:40:33Z", "updated_at": "2018-09-01T20:40:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=32556631\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anj-s\">@anj-s</a> Thanks for pushing a fix. A quick test revealed a sneaky issue, propably related to <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/distributed_training_utils.py#L238\">incorrect flattening</a>.</p>\n<p>While it works when the input names are alphabetically sorted, it fails when they're not. Checkout the following code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">from</span> tensorflow.contrib.distribute.python <span class=\"pl-k\">import</span> mirrored_strategy\n<span class=\"pl-k\">from</span> tensorflow.python <span class=\"pl-k\">import</span> keras\n<span class=\"pl-k\">from</span> tensorflow.python.data.ops <span class=\"pl-k\">import</span> dataset_ops\n<span class=\"pl-k\">from</span> tensorflow.python.platform <span class=\"pl-k\">import</span> test\n<span class=\"pl-k\">from</span> tensorflow.python.training <span class=\"pl-k\">import</span> gradient_descent\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TrainingTest</span>(<span class=\"pl-e\">test</span>.<span class=\"pl-e\">TestCase</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_multi_input_model</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">self</span>.test_session():\n      a <span class=\"pl-k\">=</span> keras.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">3</span>,), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>aa_input_a<span class=\"pl-pds\">'</span></span>)\n      b <span class=\"pl-k\">=</span> keras.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>,), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>zz_input_b<span class=\"pl-pds\">'</span></span>)\n      x <span class=\"pl-k\">=</span> keras.layers.concatenate([a, b])\n      y <span class=\"pl-k\">=</span> keras.layers.Dense(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>dense<span class=\"pl-pds\">'</span></span>)(x)\n\n      optimizer <span class=\"pl-k\">=</span> gradient_descent.GradientDescentOptimizer(<span class=\"pl-c1\">0.001</span>)\n      distribute <span class=\"pl-k\">=</span> mirrored_strategy.MirroredStrategy([<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/device:GPU:1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/device:CPU:0<span class=\"pl-pds\">'</span></span>])\n\n      model <span class=\"pl-k\">=</span> keras.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>[a, b], <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>[y])\n      model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>mse<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mae<span class=\"pl-pds\">'</span></span>], <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>optimizer, <span class=\"pl-v\">distribute</span><span class=\"pl-k\">=</span>distribute)\n\n      input_a <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>))\n      input_b <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">4</span>))\n      targets <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">5</span>))\n      dataset <span class=\"pl-k\">=</span> dataset_ops.Dataset.from_tensor_slices(({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>aa_input_a<span class=\"pl-pds\">'</span></span>: input_a, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>zz_input_b<span class=\"pl-pds\">'</span></span>: input_b}, targets))\n      dataset <span class=\"pl-k\">=</span> dataset.repeat(<span class=\"pl-c1\">100</span>)\n      dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">10</span>)\n\n      model.fit(dataset, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_multi_input_model_non_alphabetic</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">self</span>.test_session():\n      a <span class=\"pl-k\">=</span> keras.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">3</span>,), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>zz_input_a<span class=\"pl-pds\">'</span></span>)\n      b <span class=\"pl-k\">=</span> keras.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>,), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>aa_input_b<span class=\"pl-pds\">'</span></span>)\n      x <span class=\"pl-k\">=</span> keras.layers.concatenate([a, b])\n      y <span class=\"pl-k\">=</span> keras.layers.Dense(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>dense<span class=\"pl-pds\">'</span></span>)(x)\n\n      optimizer <span class=\"pl-k\">=</span> gradient_descent.GradientDescentOptimizer(<span class=\"pl-c1\">0.001</span>)\n      distribute <span class=\"pl-k\">=</span> mirrored_strategy.MirroredStrategy([<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/device:GPU:1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/device:CPU:0<span class=\"pl-pds\">'</span></span>])\n\n      model <span class=\"pl-k\">=</span> keras.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>[a, b], <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>[y])\n      model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>mse<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mae<span class=\"pl-pds\">'</span></span>], <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>optimizer, <span class=\"pl-v\">distribute</span><span class=\"pl-k\">=</span>distribute)\n\n      input_a <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>))\n      input_b <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">4</span>))\n      targets <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">5</span>))\n      dataset <span class=\"pl-k\">=</span> dataset_ops.Dataset.from_tensor_slices(({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>zz_input_a<span class=\"pl-pds\">'</span></span>: input_a, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>aa_input_b<span class=\"pl-pds\">'</span></span>: input_b}, targets))\n      dataset <span class=\"pl-k\">=</span> dataset.repeat(<span class=\"pl-c1\">100</span>)\n      dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">10</span>)\n\n      model.fit(dataset, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  test.main()</pre></div>", "body_text": "@anj-s Thanks for pushing a fix. A quick test revealed a sneaky issue, propably related to incorrect flattening.\nWhile it works when the input names are alphabetically sorted, it fails when they're not. Checkout the following code:\nimport numpy as np\n\nfrom tensorflow.contrib.distribute.python import mirrored_strategy\nfrom tensorflow.python import keras\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training import gradient_descent\n\n\nclass TrainingTest(test.TestCase):\n  def test_multi_input_model(self):\n    with self.test_session():\n      a = keras.layers.Input(shape=(3,), name='aa_input_a')\n      b = keras.layers.Input(shape=(4,), name='zz_input_b')\n      x = keras.layers.concatenate([a, b])\n      y = keras.layers.Dense(5, name='dense')(x)\n\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\n      distribute = mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])\n\n      model = keras.Model(inputs=[a, b], outputs=[y])\n      model.compile(loss='mse', metrics=['mae'], optimizer=optimizer, distribute=distribute)\n\n      input_a = np.zeros((10, 3))\n      input_b = np.zeros((10, 4))\n      targets = np.zeros((10, 5))\n      dataset = dataset_ops.Dataset.from_tensor_slices(({'aa_input_a': input_a, 'zz_input_b': input_b}, targets))\n      dataset = dataset.repeat(100)\n      dataset = dataset.batch(10)\n\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\n\n  def test_multi_input_model_non_alphabetic(self):\n    with self.test_session():\n      a = keras.layers.Input(shape=(3,), name='zz_input_a')\n      b = keras.layers.Input(shape=(4,), name='aa_input_b')\n      x = keras.layers.concatenate([a, b])\n      y = keras.layers.Dense(5, name='dense')(x)\n\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\n      distribute = mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])\n\n      model = keras.Model(inputs=[a, b], outputs=[y])\n      model.compile(loss='mse', metrics=['mae'], optimizer=optimizer, distribute=distribute)\n\n      input_a = np.zeros((10, 3))\n      input_b = np.zeros((10, 4))\n      targets = np.zeros((10, 5))\n      dataset = dataset_ops.Dataset.from_tensor_slices(({'zz_input_a': input_a, 'aa_input_b': input_b}, targets))\n      dataset = dataset.repeat(100)\n      dataset = dataset.batch(10)\n\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\n\nif __name__ == '__main__':\n  test.main()", "body": "@anj-s Thanks for pushing a fix. A quick test revealed a sneaky issue, propably related to [incorrect flattening](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/distributed_training_utils.py#L238).\r\n\r\nWhile it works when the input names are alphabetically sorted, it fails when they're not. Checkout the following code:\r\n```python\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.distribute.python import mirrored_strategy\r\nfrom tensorflow.python import keras\r\nfrom tensorflow.python.data.ops import dataset_ops\r\nfrom tensorflow.python.platform import test\r\nfrom tensorflow.python.training import gradient_descent\r\n\r\n\r\nclass TrainingTest(test.TestCase):\r\n  def test_multi_input_model(self):\r\n    with self.test_session():\r\n      a = keras.layers.Input(shape=(3,), name='aa_input_a')\r\n      b = keras.layers.Input(shape=(4,), name='zz_input_b')\r\n      x = keras.layers.concatenate([a, b])\r\n      y = keras.layers.Dense(5, name='dense')(x)\r\n\r\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\r\n      distribute = mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])\r\n\r\n      model = keras.Model(inputs=[a, b], outputs=[y])\r\n      model.compile(loss='mse', metrics=['mae'], optimizer=optimizer, distribute=distribute)\r\n\r\n      input_a = np.zeros((10, 3))\r\n      input_b = np.zeros((10, 4))\r\n      targets = np.zeros((10, 5))\r\n      dataset = dataset_ops.Dataset.from_tensor_slices(({'aa_input_a': input_a, 'zz_input_b': input_b}, targets))\r\n      dataset = dataset.repeat(100)\r\n      dataset = dataset.batch(10)\r\n\r\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n\r\n  def test_multi_input_model_non_alphabetic(self):\r\n    with self.test_session():\r\n      a = keras.layers.Input(shape=(3,), name='zz_input_a')\r\n      b = keras.layers.Input(shape=(4,), name='aa_input_b')\r\n      x = keras.layers.concatenate([a, b])\r\n      y = keras.layers.Dense(5, name='dense')(x)\r\n\r\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\r\n      distribute = mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])\r\n\r\n      model = keras.Model(inputs=[a, b], outputs=[y])\r\n      model.compile(loss='mse', metrics=['mae'], optimizer=optimizer, distribute=distribute)\r\n\r\n      input_a = np.zeros((10, 3))\r\n      input_b = np.zeros((10, 4))\r\n      targets = np.zeros((10, 5))\r\n      dataset = dataset_ops.Dataset.from_tensor_slices(({'zz_input_a': input_a, 'aa_input_b': input_b}, targets))\r\n      dataset = dataset.repeat(100)\r\n      dataset = dataset.batch(10)\r\n\r\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n\r\nif __name__ == '__main__':\r\n  test.main()\r\n```"}