{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22633", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22633/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22633/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22633/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22633", "id": 365209697, "node_id": "MDU6SXNzdWUzNjUyMDk2OTc=", "number": 22633, "title": "calibration for tensorRT INT8 in tensorflow failed?", "user": {"login": "tilaba", "id": 31062802, "node_id": "MDQ6VXNlcjMxMDYyODAy", "avatar_url": "https://avatars0.githubusercontent.com/u/31062802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tilaba", "html_url": "https://github.com/tilaba", "followers_url": "https://api.github.com/users/tilaba/followers", "following_url": "https://api.github.com/users/tilaba/following{/other_user}", "gists_url": "https://api.github.com/users/tilaba/gists{/gist_id}", "starred_url": "https://api.github.com/users/tilaba/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tilaba/subscriptions", "organizations_url": "https://api.github.com/users/tilaba/orgs", "repos_url": "https://api.github.com/users/tilaba/repos", "events_url": "https://api.github.com/users/tilaba/events{/privacy}", "received_events_url": "https://api.github.com/users/tilaba/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-09-30T09:37:38Z", "updated_at": "2018-11-01T07:20:35Z", "closed_at": "2018-10-16T22:47:57Z", "author_association": "NONE", "body_html": "<p>when I use the resnetV150_frozen.pb model\uff0c I can calibrate the INT8 graph.     However, I try to use another pb model I download from web, I find the INT8 calibration does not work.   I just don't know why?  I doubt the problem may lie in the memory allocation. And when I use data to calibrate the INT8 graph, it always reports the following error.  \u201cERROR:tensorflow:Not a calib graph. Doesn't seem to contain any calibration nodes.\u201d  please help me!</p>\n<p>if f.INT8:<br>\ncalibGraph=getINT8CalibGraph(f.batch_size,wsize)</p>\n<pre><code>timings,comp,_,mdstats=timeGraph(calibGraph,1,1,dummy_input)\nint8Graph=getINT8InferenceGraph(calibGraph)\ndel calibGraph\ntimings,comp,valint8,mdstats=timeGraph(int8Graph,f.batch_size,\n                               f.num_loops,dummy_input)\n</code></pre>\n<p>vals=[valnative,valfp32,valfp16,valint8]</p>\n<p>def timeGraph(gdef,batch_size=128,num_loops=100,dummy_input=None,timelineName=None):<br>\ntf.logging.info(\"Starting execution\")<br>\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)<br>\ntf.reset_default_graph()<br>\ng = tf.Graph()<br>\nif dummy_input is None:<br>\ndummy_input = np.random.random_sample((batch_size,3,224,224))<br>\noutlist=[]<br>\nwith g.as_default():</p>\n<pre><code>next_element=tf.constant(dummy_input, dtype= tf.float32)\nout = tf.import_graph_def(\n  graph_def=gdef,\n  input_map={f.input_node:next_element},\n  return_elements=[f.output_node]\n)\nout = out[0].outputs[0]\noutlist.append(out)\n</code></pre>\n<p>timings=[]</p>\n<p>with tf.Session(graph=g,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:<br>\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)<br>\nrun_metadata = tf.RunMetadata()<br>\ntf.logging.info(\"Starting Warmup cycle\")</p>\n<pre><code>num_iters=1\nfor i in range(num_loops):\n  tstart=time.time()\n  for k in range(num_iters):\n    val = sess.run(outlist)\n  timings.append((time.time()-tstart)/float(num_iters))\n  print(\"iter \",i,\" \",timings[-1])\ncomp=sess.run(tf.reduce_all(tf.equal(val[0],val[0])))\nprint(\"Comparison=\",comp)\nsess.close()\ntf.logging.info(\"Timing loop done!\")\nreturn timings,comp,val[0],None\n</code></pre>\n<p>def getINT8InferenceGraph(calibGraph):<br>\ntrt_graph=trt.calib_graph_to_infer_graph(calibGraph)<br>\nreturn trt_graph</p>\n<p>def getINT8CalibGraph(batch_size=128,workspace_size=1&lt;&lt;25):<br>\ntrt_graph = trt.create_inference_graph(getResnet50(f.frozen_graph), [ f.output_node],<br>\nmax_batch_size=batch_size,<br>\nmax_workspace_size_bytes=workspace_size,<br>\nprecision_mode=\"INT8\"<br>\n)</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2537426/model.zip\">model.zip</a></p>", "body_text": "when I use the resnetV150_frozen.pb model\uff0c I can calibrate the INT8 graph.     However, I try to use another pb model I download from web, I find the INT8 calibration does not work.   I just don't know why?  I doubt the problem may lie in the memory allocation. And when I use data to calibrate the INT8 graph, it always reports the following error.  \u201cERROR:tensorflow:Not a calib graph. Doesn't seem to contain any calibration nodes.\u201d  please help me!\nif f.INT8:\ncalibGraph=getINT8CalibGraph(f.batch_size,wsize)\ntimings,comp,_,mdstats=timeGraph(calibGraph,1,1,dummy_input)\nint8Graph=getINT8InferenceGraph(calibGraph)\ndel calibGraph\ntimings,comp,valint8,mdstats=timeGraph(int8Graph,f.batch_size,\n                               f.num_loops,dummy_input)\n\nvals=[valnative,valfp32,valfp16,valint8]\ndef timeGraph(gdef,batch_size=128,num_loops=100,dummy_input=None,timelineName=None):\ntf.logging.info(\"Starting execution\")\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\ntf.reset_default_graph()\ng = tf.Graph()\nif dummy_input is None:\ndummy_input = np.random.random_sample((batch_size,3,224,224))\noutlist=[]\nwith g.as_default():\nnext_element=tf.constant(dummy_input, dtype= tf.float32)\nout = tf.import_graph_def(\n  graph_def=gdef,\n  input_map={f.input_node:next_element},\n  return_elements=[f.output_node]\n)\nout = out[0].outputs[0]\noutlist.append(out)\n\ntimings=[]\nwith tf.Session(graph=g,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\ntf.logging.info(\"Starting Warmup cycle\")\nnum_iters=1\nfor i in range(num_loops):\n  tstart=time.time()\n  for k in range(num_iters):\n    val = sess.run(outlist)\n  timings.append((time.time()-tstart)/float(num_iters))\n  print(\"iter \",i,\" \",timings[-1])\ncomp=sess.run(tf.reduce_all(tf.equal(val[0],val[0])))\nprint(\"Comparison=\",comp)\nsess.close()\ntf.logging.info(\"Timing loop done!\")\nreturn timings,comp,val[0],None\n\ndef getINT8InferenceGraph(calibGraph):\ntrt_graph=trt.calib_graph_to_infer_graph(calibGraph)\nreturn trt_graph\ndef getINT8CalibGraph(batch_size=128,workspace_size=1<<25):\ntrt_graph = trt.create_inference_graph(getResnet50(f.frozen_graph), [ f.output_node],\nmax_batch_size=batch_size,\nmax_workspace_size_bytes=workspace_size,\nprecision_mode=\"INT8\"\n)\nmodel.zip", "body": "when I use the resnetV150_frozen.pb model\uff0c I can calibrate the INT8 graph.     However, I try to use another pb model I download from web, I find the INT8 calibration does not work.   I just don't know why?  I doubt the problem may lie in the memory allocation. And when I use data to calibrate the INT8 graph, it always reports the following error.  \u201cERROR:tensorflow:Not a calib graph. Doesn't seem to contain any calibration nodes.\u201d  please help me!\r\n\r\nif f.INT8:\r\n    calibGraph=getINT8CalibGraph(f.batch_size,wsize)\r\n    \r\n    timings,comp,_,mdstats=timeGraph(calibGraph,1,1,dummy_input)\r\n    int8Graph=getINT8InferenceGraph(calibGraph)\r\n    del calibGraph\r\n    timings,comp,valint8,mdstats=timeGraph(int8Graph,f.batch_size,\r\n                                   f.num_loops,dummy_input)\r\n    \r\n  vals=[valnative,valfp32,valfp16,valint8]\r\n\r\n\r\ndef timeGraph(gdef,batch_size=128,num_loops=100,dummy_input=None,timelineName=None):\r\n  tf.logging.info(\"Starting execution\")\r\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\n  tf.reset_default_graph()\r\n  g = tf.Graph()\r\n  if dummy_input is None:\r\n    dummy_input = np.random.random_sample((batch_size,3,224,224))\r\n  outlist=[]\r\n  with g.as_default():\r\n\r\n    next_element=tf.constant(dummy_input, dtype= tf.float32)\r\n    out = tf.import_graph_def(\r\n      graph_def=gdef,\r\n      input_map={f.input_node:next_element},\r\n      return_elements=[f.output_node]\r\n    )\r\n    out = out[0].outputs[0]\r\n    outlist.append(out)\r\n    \r\n  timings=[]\r\n  \r\n  with tf.Session(graph=g,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    tf.logging.info(\"Starting Warmup cycle\")\r\n    \r\n    \r\n\r\n\r\n\r\n    num_iters=1\r\n    for i in range(num_loops):\r\n      tstart=time.time()\r\n      for k in range(num_iters):\r\n        val = sess.run(outlist)\r\n      timings.append((time.time()-tstart)/float(num_iters))\r\n      print(\"iter \",i,\" \",timings[-1])\r\n    comp=sess.run(tf.reduce_all(tf.equal(val[0],val[0])))\r\n    print(\"Comparison=\",comp)\r\n    sess.close()\r\n    tf.logging.info(\"Timing loop done!\")\r\n    return timings,comp,val[0],None\r\n\r\n\r\ndef getINT8InferenceGraph(calibGraph):\r\n  trt_graph=trt.calib_graph_to_infer_graph(calibGraph)\r\n  return trt_graph\r\n\r\ndef getINT8CalibGraph(batch_size=128,workspace_size=1<<25):\r\n  trt_graph = trt.create_inference_graph(getResnet50(f.frozen_graph), [ f.output_node],\r\n                                         max_batch_size=batch_size,\r\n                                         max_workspace_size_bytes=workspace_size,\r\n                                         precision_mode=\"INT8\"\r\n                                       )  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/2537426/model.zip)\r\n"}