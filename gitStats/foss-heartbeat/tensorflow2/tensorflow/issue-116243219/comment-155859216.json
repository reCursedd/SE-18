{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155859216", "html_url": "https://github.com/tensorflow/tensorflow/pull/113#issuecomment-155859216", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/113", "id": 155859216, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTg1OTIxNg==", "user": {"login": "Yangqing", "id": 551151, "node_id": "MDQ6VXNlcjU1MTE1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/551151?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yangqing", "html_url": "https://github.com/Yangqing", "followers_url": "https://api.github.com/users/Yangqing/followers", "following_url": "https://api.github.com/users/Yangqing/following{/other_user}", "gists_url": "https://api.github.com/users/Yangqing/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yangqing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yangqing/subscriptions", "organizations_url": "https://api.github.com/users/Yangqing/orgs", "repos_url": "https://api.github.com/users/Yangqing/repos", "events_url": "https://api.github.com/users/Yangqing/events{/privacy}", "received_events_url": "https://api.github.com/users/Yangqing/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-11T17:47:26Z", "updated_at": "2015-11-11T17:47:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Jeff - it turns out that there is an implicit memcpy going on when the session is run. Basically, when session.run(target) is called, we also fetch all the targets into numpy arrays (which is a feature that is not very well documented...). Since the gradients are on GPU and the numpy arrays are going to be on CPU, a memcpy is triggered that causes a nontrivial amount time.</p>\n<p>The reason we started to see this performance hit after adding FC layers is - as one may expect - because FC layers. I've added a proposed change to the code for experimentation on your side.</p>\n<p>I'll let the guys know and add these notes to the documentation. Thanks for digging into this!</p>", "body_text": "Hi Jeff - it turns out that there is an implicit memcpy going on when the session is run. Basically, when session.run(target) is called, we also fetch all the targets into numpy arrays (which is a feature that is not very well documented...). Since the gradients are on GPU and the numpy arrays are going to be on CPU, a memcpy is triggered that causes a nontrivial amount time.\nThe reason we started to see this performance hit after adding FC layers is - as one may expect - because FC layers. I've added a proposed change to the code for experimentation on your side.\nI'll let the guys know and add these notes to the documentation. Thanks for digging into this!", "body": "Hi Jeff - it turns out that there is an implicit memcpy going on when the session is run. Basically, when session.run(target) is called, we also fetch all the targets into numpy arrays (which is a feature that is not very well documented...). Since the gradients are on GPU and the numpy arrays are going to be on CPU, a memcpy is triggered that causes a nontrivial amount time.\n\nThe reason we started to see this performance hit after adding FC layers is - as one may expect - because FC layers. I've added a proposed change to the code for experimentation on your side.\n\nI'll let the guys know and add these notes to the documentation. Thanks for digging into this!\n"}