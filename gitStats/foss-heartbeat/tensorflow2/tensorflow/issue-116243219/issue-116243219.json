{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/113", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/113/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/113/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/113/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/113", "id": 116243219, "node_id": "MDExOlB1bGxSZXF1ZXN0NTAzMzc3NzA=", "number": 113, "title": "AlexNet with FC layers: backward is very slow?", "user": {"login": "jeffdonahue", "id": 965190, "node_id": "MDQ6VXNlcjk2NTE5MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/965190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffdonahue", "html_url": "https://github.com/jeffdonahue", "followers_url": "https://api.github.com/users/jeffdonahue/followers", "following_url": "https://api.github.com/users/jeffdonahue/following{/other_user}", "gists_url": "https://api.github.com/users/jeffdonahue/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffdonahue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffdonahue/subscriptions", "organizations_url": "https://api.github.com/users/jeffdonahue/orgs", "repos_url": "https://api.github.com/users/jeffdonahue/repos", "events_url": "https://api.github.com/users/jeffdonahue/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffdonahue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2015-11-11T01:38:25Z", "updated_at": "2015-11-11T21:07:47Z", "closed_at": "2015-11-11T20:54:19Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/113", "html_url": "https://github.com/tensorflow/tensorflow/pull/113", "diff_url": "https://github.com/tensorflow/tensorflow/pull/113.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/113.patch"}, "body_html": "<p>(I know PRs are not the way to contribute to TensorFlow -- I'm just posting this for discussion, though I'm happy to submit this patch via the official means if desired.)</p>\n<p>I modified <code>alexnet_benchmark.py</code> to compute activations for the fully-connected layers (fc6-fc8) where the original only computes through <code>pool5</code>.  Using a Titan X with the original code that only goes through <code>pool5</code>, I see roughly the expected result:</p>\n<pre><code>2015-11-10 16:29:54.760793: step 10, duration = 0.098\n2015-11-10 16:29:55.746638: step 20, duration = 0.099\n2015-11-10 16:29:56.734163: step 30, duration = 0.099\n2015-11-10 16:29:57.720769: step 40, duration = 0.098\n2015-11-10 16:29:58.708402: step 50, duration = 0.099\n2015-11-10 16:29:59.697522: step 60, duration = 0.100\n2015-11-10 16:30:00.687530: step 70, duration = 0.099\n2015-11-10 16:30:01.677052: step 80, duration = 0.099\n2015-11-10 16:30:02.669677: step 90, duration = 0.099\n2015-11-10 16:30:03.563315: Forward across 100 steps, 0.098 +/- 0.010 sec / batch\n2015-11-10 16:30:10.385117: step 10, duration = 0.321\n2015-11-10 16:30:13.613230: step 20, duration = 0.322\n2015-11-10 16:30:16.840772: step 30, duration = 0.322\n2015-11-10 16:30:20.068794: step 40, duration = 0.322\n2015-11-10 16:30:23.308556: step 50, duration = 0.326\n2015-11-10 16:30:26.558616: step 60, duration = 0.323\n2015-11-10 16:30:30.260454: step 70, duration = 0.325\n2015-11-10 16:30:33.939707: step 80, duration = 0.324\n2015-11-10 16:30:37.797578: step 90, duration = 0.326\n2015-11-10 16:30:41.159575: Forward-backward across 100 steps, 0.340 +/- 0.052 sec / batch\n</code></pre>\n<p>With this modified version, I get expected computation speed for the forward-only version (only epsilon longer than the pool5 version), but very slow speeds doing forward-backward (stopped after 60 iterations due to slowness):</p>\n<pre><code>2015-11-10 15:54:39.231156: step 10, duration = 0.101\n2015-11-10 15:54:40.235104: step 20, duration = 0.100\n2015-11-10 15:54:41.238807: step 30, duration = 0.100\n2015-11-10 15:54:42.241662: step 40, duration = 0.100\n2015-11-10 15:54:43.245950: step 50, duration = 0.100\n2015-11-10 15:54:44.251555: step 60, duration = 0.100\n2015-11-10 15:54:45.255560: step 70, duration = 0.100\n2015-11-10 15:54:46.260286: step 80, duration = 0.101\n2015-11-10 15:54:47.268687: step 90, duration = 0.101\n2015-11-10 15:54:48.172288: Forward across 100 steps, 0.099 +/- 0.010 sec / batch\n2015-11-10 15:55:30.047371: step 10, duration = 13.337\n2015-11-10 15:58:34.855597: step 20, duration = 11.968\n2015-11-10 16:01:40.070674: step 30, duration = 19.707\n2015-11-10 16:04:30.698742: step 40, duration = 13.459\n2015-11-10 16:07:15.900770: step 50, duration = 20.028\n2015-11-10 16:10:11.997452: step 60, duration = 12.357\n^C^C^CTraceback (most recent call last):\n</code></pre>\n<p>During the forward-backward part of the benchmark, looking at GPU utilization using <code>nvidia-smi</code> while running this shows 0% utilization most of the time.</p>\n<p>For reference, I ran more or less the equivalent timing on the same GPU using out-of-the-box Caffe (<em>no</em> CuDNN) using <a href=\"https://gist.github.com/jeffdonahue/9193b3c88e4262128fe8\">this prototxt</a>, and see Forward-Backward times of ~413 ms.  (And for the record the comparison is still a little unfair to Caffe as <code>caffe time</code>, unlike <code>caffe train</code>, computes all gradients, including w.r.t. the input image.)</p>\n<pre><code>$ caffe time -model alexnet_dummydata_nolrn.prototxt -gpu 0\n[...]\nI1110 16:37:19.770344  6275 caffe.cpp:366] Average Forward pass: 176.546 ms.\nI1110 16:37:19.770350  6275 caffe.cpp:368] Average Backward pass: 237.147 ms.\nI1110 16:37:19.770356  6275 caffe.cpp:370] Average Forward-Backward: 413.773 ms.\n</code></pre>\n<p>Please let me know if anyone has any idea what might be going on, or if something is wrong with my implementation (which I wouldn't doubt!).  Thanks!</p>", "body_text": "(I know PRs are not the way to contribute to TensorFlow -- I'm just posting this for discussion, though I'm happy to submit this patch via the official means if desired.)\nI modified alexnet_benchmark.py to compute activations for the fully-connected layers (fc6-fc8) where the original only computes through pool5.  Using a Titan X with the original code that only goes through pool5, I see roughly the expected result:\n2015-11-10 16:29:54.760793: step 10, duration = 0.098\n2015-11-10 16:29:55.746638: step 20, duration = 0.099\n2015-11-10 16:29:56.734163: step 30, duration = 0.099\n2015-11-10 16:29:57.720769: step 40, duration = 0.098\n2015-11-10 16:29:58.708402: step 50, duration = 0.099\n2015-11-10 16:29:59.697522: step 60, duration = 0.100\n2015-11-10 16:30:00.687530: step 70, duration = 0.099\n2015-11-10 16:30:01.677052: step 80, duration = 0.099\n2015-11-10 16:30:02.669677: step 90, duration = 0.099\n2015-11-10 16:30:03.563315: Forward across 100 steps, 0.098 +/- 0.010 sec / batch\n2015-11-10 16:30:10.385117: step 10, duration = 0.321\n2015-11-10 16:30:13.613230: step 20, duration = 0.322\n2015-11-10 16:30:16.840772: step 30, duration = 0.322\n2015-11-10 16:30:20.068794: step 40, duration = 0.322\n2015-11-10 16:30:23.308556: step 50, duration = 0.326\n2015-11-10 16:30:26.558616: step 60, duration = 0.323\n2015-11-10 16:30:30.260454: step 70, duration = 0.325\n2015-11-10 16:30:33.939707: step 80, duration = 0.324\n2015-11-10 16:30:37.797578: step 90, duration = 0.326\n2015-11-10 16:30:41.159575: Forward-backward across 100 steps, 0.340 +/- 0.052 sec / batch\n\nWith this modified version, I get expected computation speed for the forward-only version (only epsilon longer than the pool5 version), but very slow speeds doing forward-backward (stopped after 60 iterations due to slowness):\n2015-11-10 15:54:39.231156: step 10, duration = 0.101\n2015-11-10 15:54:40.235104: step 20, duration = 0.100\n2015-11-10 15:54:41.238807: step 30, duration = 0.100\n2015-11-10 15:54:42.241662: step 40, duration = 0.100\n2015-11-10 15:54:43.245950: step 50, duration = 0.100\n2015-11-10 15:54:44.251555: step 60, duration = 0.100\n2015-11-10 15:54:45.255560: step 70, duration = 0.100\n2015-11-10 15:54:46.260286: step 80, duration = 0.101\n2015-11-10 15:54:47.268687: step 90, duration = 0.101\n2015-11-10 15:54:48.172288: Forward across 100 steps, 0.099 +/- 0.010 sec / batch\n2015-11-10 15:55:30.047371: step 10, duration = 13.337\n2015-11-10 15:58:34.855597: step 20, duration = 11.968\n2015-11-10 16:01:40.070674: step 30, duration = 19.707\n2015-11-10 16:04:30.698742: step 40, duration = 13.459\n2015-11-10 16:07:15.900770: step 50, duration = 20.028\n2015-11-10 16:10:11.997452: step 60, duration = 12.357\n^C^C^CTraceback (most recent call last):\n\nDuring the forward-backward part of the benchmark, looking at GPU utilization using nvidia-smi while running this shows 0% utilization most of the time.\nFor reference, I ran more or less the equivalent timing on the same GPU using out-of-the-box Caffe (no CuDNN) using this prototxt, and see Forward-Backward times of ~413 ms.  (And for the record the comparison is still a little unfair to Caffe as caffe time, unlike caffe train, computes all gradients, including w.r.t. the input image.)\n$ caffe time -model alexnet_dummydata_nolrn.prototxt -gpu 0\n[...]\nI1110 16:37:19.770344  6275 caffe.cpp:366] Average Forward pass: 176.546 ms.\nI1110 16:37:19.770350  6275 caffe.cpp:368] Average Backward pass: 237.147 ms.\nI1110 16:37:19.770356  6275 caffe.cpp:370] Average Forward-Backward: 413.773 ms.\n\nPlease let me know if anyone has any idea what might be going on, or if something is wrong with my implementation (which I wouldn't doubt!).  Thanks!", "body": "(I know PRs are not the way to contribute to TensorFlow -- I'm just posting this for discussion, though I'm happy to submit this patch via the official means if desired.)\n\nI modified `alexnet_benchmark.py` to compute activations for the fully-connected layers (fc6-fc8) where the original only computes through `pool5`.  Using a Titan X with the original code that only goes through `pool5`, I see roughly the expected result:\n\n```\n2015-11-10 16:29:54.760793: step 10, duration = 0.098\n2015-11-10 16:29:55.746638: step 20, duration = 0.099\n2015-11-10 16:29:56.734163: step 30, duration = 0.099\n2015-11-10 16:29:57.720769: step 40, duration = 0.098\n2015-11-10 16:29:58.708402: step 50, duration = 0.099\n2015-11-10 16:29:59.697522: step 60, duration = 0.100\n2015-11-10 16:30:00.687530: step 70, duration = 0.099\n2015-11-10 16:30:01.677052: step 80, duration = 0.099\n2015-11-10 16:30:02.669677: step 90, duration = 0.099\n2015-11-10 16:30:03.563315: Forward across 100 steps, 0.098 +/- 0.010 sec / batch\n2015-11-10 16:30:10.385117: step 10, duration = 0.321\n2015-11-10 16:30:13.613230: step 20, duration = 0.322\n2015-11-10 16:30:16.840772: step 30, duration = 0.322\n2015-11-10 16:30:20.068794: step 40, duration = 0.322\n2015-11-10 16:30:23.308556: step 50, duration = 0.326\n2015-11-10 16:30:26.558616: step 60, duration = 0.323\n2015-11-10 16:30:30.260454: step 70, duration = 0.325\n2015-11-10 16:30:33.939707: step 80, duration = 0.324\n2015-11-10 16:30:37.797578: step 90, duration = 0.326\n2015-11-10 16:30:41.159575: Forward-backward across 100 steps, 0.340 +/- 0.052 sec / batch\n```\n\nWith this modified version, I get expected computation speed for the forward-only version (only epsilon longer than the pool5 version), but very slow speeds doing forward-backward (stopped after 60 iterations due to slowness):\n\n```\n2015-11-10 15:54:39.231156: step 10, duration = 0.101\n2015-11-10 15:54:40.235104: step 20, duration = 0.100\n2015-11-10 15:54:41.238807: step 30, duration = 0.100\n2015-11-10 15:54:42.241662: step 40, duration = 0.100\n2015-11-10 15:54:43.245950: step 50, duration = 0.100\n2015-11-10 15:54:44.251555: step 60, duration = 0.100\n2015-11-10 15:54:45.255560: step 70, duration = 0.100\n2015-11-10 15:54:46.260286: step 80, duration = 0.101\n2015-11-10 15:54:47.268687: step 90, duration = 0.101\n2015-11-10 15:54:48.172288: Forward across 100 steps, 0.099 +/- 0.010 sec / batch\n2015-11-10 15:55:30.047371: step 10, duration = 13.337\n2015-11-10 15:58:34.855597: step 20, duration = 11.968\n2015-11-10 16:01:40.070674: step 30, duration = 19.707\n2015-11-10 16:04:30.698742: step 40, duration = 13.459\n2015-11-10 16:07:15.900770: step 50, duration = 20.028\n2015-11-10 16:10:11.997452: step 60, duration = 12.357\n^C^C^CTraceback (most recent call last):\n```\n\nDuring the forward-backward part of the benchmark, looking at GPU utilization using `nvidia-smi` while running this shows 0% utilization most of the time.\n\nFor reference, I ran more or less the equivalent timing on the same GPU using out-of-the-box Caffe (_no_ CuDNN) using [this prototxt](https://gist.github.com/jeffdonahue/9193b3c88e4262128fe8), and see Forward-Backward times of ~413 ms.  (And for the record the comparison is still a little unfair to Caffe as `caffe time`, unlike `caffe train`, computes all gradients, including w.r.t. the input image.)\n\n```\n$ caffe time -model alexnet_dummydata_nolrn.prototxt -gpu 0\n[...]\nI1110 16:37:19.770344  6275 caffe.cpp:366] Average Forward pass: 176.546 ms.\nI1110 16:37:19.770350  6275 caffe.cpp:368] Average Backward pass: 237.147 ms.\nI1110 16:37:19.770356  6275 caffe.cpp:370] Average Forward-Backward: 413.773 ms.\n```\n\nPlease let me know if anyone has any idea what might be going on, or if something is wrong with my implementation (which I wouldn't doubt!).  Thanks!\n"}