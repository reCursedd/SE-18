{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/367860655", "html_url": "https://github.com/tensorflow/tensorflow/issues/17159#issuecomment-367860655", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17159", "id": 367860655, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Nzg2MDY1NQ==", "user": {"login": "dacox", "id": 3887682, "node_id": "MDQ6VXNlcjM4ODc2ODI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3887682?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dacox", "html_url": "https://github.com/dacox", "followers_url": "https://api.github.com/users/dacox/followers", "following_url": "https://api.github.com/users/dacox/following{/other_user}", "gists_url": "https://api.github.com/users/dacox/gists{/gist_id}", "starred_url": "https://api.github.com/users/dacox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dacox/subscriptions", "organizations_url": "https://api.github.com/users/dacox/orgs", "repos_url": "https://api.github.com/users/dacox/repos", "events_url": "https://api.github.com/users/dacox/events{/privacy}", "received_events_url": "https://api.github.com/users/dacox/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T23:40:17Z", "updated_at": "2018-02-22T23:42:15Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19293677\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ispirmustafa\">@ispirmustafa</a> Sure!</p>\n<p>I've trimmed it down a bit by using a pre-canned estimator, the default optimizer, and not providing any <code>vars_to_warm_start</code>.</p>\n<p><strong>Example Script</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\ntf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n\n\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">unused_args</span>):\n    model_dir <span class=\"pl-k\">=</span> os.path.join(<span class=\"pl-c1\">FLAGS</span>.model_dir, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>my_model<span class=\"pl-pds\">'</span></span>)\n\n    X_train <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">30000</span>, <span class=\"pl-c1\">500</span>)\n    y_train <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">30000</span>, <span class=\"pl-c1\">2048</span>)\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Train data shape...<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">print</span>(X_train.shape)\n    <span class=\"pl-c1\">print</span>(y_train.shape)\n\n    X_dim <span class=\"pl-k\">=</span> X_train.shape[<span class=\"pl-c1\">1</span>]\n    y_dim <span class=\"pl-k\">=</span> y_train.shape[<span class=\"pl-c1\">1</span>]\n\n    feature_columns <span class=\"pl-k\">=</span> [\n        tf.feature_column.numeric_column(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>X_dim)\n    ]\n\n    warm_start_from <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.warm_start:\n        warm_start_from <span class=\"pl-k\">=</span> tf.estimator.WarmStartSettings(\n            <span class=\"pl-v\">ckpt_to_initialize_from</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.warm_start,\n        )\n\n    model <span class=\"pl-k\">=</span> tf.estimator.DNNRegressor(\n        <span class=\"pl-v\">hidden_units</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">500</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2048</span>],\n        <span class=\"pl-v\">feature_columns</span><span class=\"pl-k\">=</span>feature_columns,\n        <span class=\"pl-v\">label_dimension</span><span class=\"pl-k\">=</span>y_dim,\n        <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span>model_dir,\n        <span class=\"pl-v\">warm_start_from</span><span class=\"pl-k\">=</span>warm_start_from\n    )\n\n    train_input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n        <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: X_train},\n        <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>y_train,\n        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n        <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>,\n        <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>\n    )\n\n    model.train(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>train_input_fn)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n    parser.add_argument(\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_dir<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-v\">nargs</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>?<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>output<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Directory to save resulting model(s).<span class=\"pl-pds\">'</span></span>\n    )\n    parser.add_argument(\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--warm_start<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Checkpoint to initialize weights from for fine-tuning.<span class=\"pl-pds\">'</span></span>\n    )\n\n    <span class=\"pl-c1\">FLAGS</span>, unparsed <span class=\"pl-k\">=</span> parser.parse_known_args()\n\n    tf.app.run(<span class=\"pl-v\">main</span><span class=\"pl-k\">=</span>main, <span class=\"pl-v\">argv</span><span class=\"pl-k\">=</span>[sys.argv[<span class=\"pl-c1\">0</span>]] <span class=\"pl-k\">+</span> unparsed)</pre></div>\n<p><strong>Script Usage</strong></p>\n<div class=\"highlight highlight-source-shell\"><pre>$ mkdir -p output\n$ python example.py output/\n$ mv output/my_model output/my_checkpoint\n$ python example.py --warm_start output/my_checkpoint/ output/</pre></div>\n<p><strong>First Epoch</strong></p>\n<div class=\"highlight highlight-source-shell\"><pre>/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from <span class=\"pl-s\"><span class=\"pl-pds\">`</span>float<span class=\"pl-pds\">`</span></span> to <span class=\"pl-s\"><span class=\"pl-pds\">`</span>np.floating<span class=\"pl-pds\">`</span></span> is deprecated. In future, it will be treated as <span class=\"pl-s\"><span class=\"pl-pds\">`</span>np.float64 == np.dtype(float).type<span class=\"pl-pds\">`</span></span>.\n  from ._conv import register_converters as _register_converters\nTrain data shape...\n(30000, 500)\n(30000, 2048)\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_secs<span class=\"pl-pds\">'</span></span>: 600, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_session_config<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_max<span class=\"pl-pds\">'</span></span>: 5, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_task_type<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_global_id_in_cluster<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_is_chief<span class=\"pl-pds\">'</span></span>: True, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_cluster_spec<span class=\"pl-pds\">'</span></span>: <span class=\"pl-k\">&lt;</span>tensorflow.python.training.server_lib.ClusterSpec object at 0x7f56c3d6f9d<span class=\"pl-k\">0&gt;</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_eval</span>\n<span class=\"pl-s\">uation_master<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_steps<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_every_n_hours<span class=\"pl-pds\">'</span></span>: 10000, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_service<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_num_ps_replicas<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_tf_random_seed<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_master<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_num_worker_replicas<span class=\"pl-pds\">'</span></span>: 1, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_task_id<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_log_step_count_steps<span class=\"pl-pds\">'</span></span>: 100, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_model_dir<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>output/my_model<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save</span>\n<span class=\"pl-s\">_summary_steps<span class=\"pl-pds\">'</span></span>: 100}\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\n2018-02-22 23:37:26.110761: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-02-22 23:37:26.207601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-02-22 23:37:26.207969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\npciBusID: 0000:01:00.0\ntotalMemory: 5.93GiB freeMemory: 3.06GiB\n2018-02-22 23:37:26.207991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2018-02-22 23:37:26.358043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2783 MB memory) -<span class=\"pl-k\">&gt;</span> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints <span class=\"pl-k\">for</span> 1 into output/my_model/model.ckpt.\nINFO:tensorflow:loss = 71532.54, step = 1\nINFO:tensorflow:global_step/sec: 176.009\nINFO:tensorflow:loss = 17626.25, step = 101 (0.568 sec)\nINFO:tensorflow:global_step/sec: 181.392\nINFO:tensorflow:loss = 17433.3, step = 201 (0.551 sec)\nINFO:tensorflow:global_step/sec: 182.943\nINFO:tensorflow:loss = 17426.316, step = 301 (0.547 sec)\nINFO:tensorflow:global_step/sec: 183.914\nINFO:tensorflow:loss = 17366.844, step = 401 (0.544 sec)\nINFO:tensorflow:global_step/sec: 181.319\nINFO:tensorflow:loss = 17432.586, step = 501 (0.552 sec)\nINFO:tensorflow:global_step/sec: 183.824\nINFO:tensorflow:loss = 17425.293, step = 601 (0.544 sec)\nINFO:tensorflow:global_step/sec: 184.213\nINFO:tensorflow:loss = 17454.996, step = 701 (0.543 sec)\nINFO:tensorflow:global_step/sec: 184.085\nINFO:tensorflow:loss = 17382.533, step = 801 (0.543 sec)\nINFO:tensorflow:global_step/sec: 183.105\nINFO:tensorflow:loss = 17416.637, step = 901 (0.546 sec)\nINFO:tensorflow:global_step/sec: 182.509\nINFO:tensorflow:loss = 17479.277, step = 1001 (0.548 sec)\nINFO:tensorflow:global_step/sec: 183.757\nINFO:tensorflow:loss = 17386.068, step = 1101 (0.544 sec)\nINFO:tensorflow:global_step/sec: 182.909\nINFO:tensorflow:loss = 17392.65, step = 1201 (0.547 sec)\nINFO:tensorflow:global_step/sec: 182.233\nINFO:tensorflow:loss = 17397.05, step = 1301 (0.549 sec)\nINFO:tensorflow:global_step/sec: 183.525\nINFO:tensorflow:loss = 17380.441, step = 1401 (0.545 sec)\nINFO:tensorflow:Saving checkpoints <span class=\"pl-k\">for</span> 1500 into output/my_model/model.ckpt.\nINFO:tensorflow:Loss <span class=\"pl-k\">for</span> final step: 17338.143.</pre></div>\n<p><strong>Second Epoch (with warm-start)</strong></p>\n<div class=\"highlight highlight-source-shell\"><pre>/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from <span class=\"pl-s\"><span class=\"pl-pds\">`</span>float<span class=\"pl-pds\">`</span></span> to <span class=\"pl-s\"><span class=\"pl-pds\">`</span>np.floating<span class=\"pl-pds\">`</span></span> is deprecated. In future, it will be treated as <span class=\"pl-s\"><span class=\"pl-pds\">`</span>np.float64 == np.dtype(float).type<span class=\"pl-pds\">`</span></span>.\n  from ._conv import register_converters as _register_converters\nTrain data shape...\n(30000, 500)\n(30000, 2048)\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_secs<span class=\"pl-pds\">'</span></span>: 600, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_session_config<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_max<span class=\"pl-pds\">'</span></span>: 5, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_task_type<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_global_id_in_cluster<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_is_chief<span class=\"pl-pds\">'</span></span>: True, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_cluster_spec<span class=\"pl-pds\">'</span></span>: <span class=\"pl-k\">&lt;</span>tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb55742fa<span class=\"pl-k\">10&gt;</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_evaluation_master<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_checkpoints_steps<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_keep_checkpoint_every_n_hours<span class=\"pl-pds\">'</span></span>: 10000, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_service<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_num_ps_replicas<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_tf_random_seed<span class=\"pl-pds\">'</span></span>: None, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_master<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_num_worker_replicas<span class=\"pl-pds\">'</span></span>: 1, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_task_id<span class=\"pl-pds\">'</span></span>: 0, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_log_step_count_steps<span class=\"pl-pds\">'</span></span>: 100, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_model_dir<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>output/my_model<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_save_summary_steps<span class=\"pl-pds\">'</span></span>: 100}\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>output/my_checkpoint/<span class=\"pl-pds\">'</span></span>, vars_to_warm_start=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.*<span class=\"pl-pds\">'</span></span>, var_name_to_vocab_info={}, var_name_to_prev_var_name={})\nINFO:tensorflow:Warm-starting from: (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>output/my_checkpoint/<span class=\"pl-pds\">'</span></span>,)\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/kernel<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/kernel\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/bias<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/bias\nINFO:tensorflow:Warm-starting variable: dnn/logits/kernel<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/logits/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/kernel\nINFO:tensorflow:Warm-starting variable: dnn/logits/bias<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/logits/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/bias\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/kernel<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/kernel\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/bias<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/bias\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/kernel<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/kernel\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/bias<span class=\"pl-k\">;</span> prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/bias\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\n2018-02-22 23:37:49.783077: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-02-22 23:37:49.883252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-02-22 23:37:49.883612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\npciBusID: 0000:01:00.0\ntotalMemory: 5.93GiB freeMemory: 3.08GiB\n2018-02-22 23:37:49.883628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2018-02-22 23:37:50.032547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2795 MB memory) -<span class=\"pl-k\">&gt;</span> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints <span class=\"pl-k\">for</span> 1 into output/my_model/model.ckpt.\nINFO:tensorflow:loss = 17359.91, step = 1\nINFO:tensorflow:global_step/sec: 68.0301\nINFO:tensorflow:loss = 17201.738, step = 101 (1.470 sec)\nINFO:tensorflow:global_step/sec: 69.0413\nINFO:tensorflow:loss = 17168.336, step = 201 (1.448 sec)\nINFO:tensorflow:global_step/sec: 69.6031\nINFO:tensorflow:loss = 17170.893, step = 301 (1.437 sec)\nINFO:tensorflow:global_step/sec: 51.401\nINFO:tensorflow:loss = 17156.967, step = 401 (1.947 sec)\nINFO:tensorflow:global_step/sec: 64.7499\nINFO:tensorflow:loss = 17082.166, step = 501 (1.543 sec)\nINFO:tensorflow:global_step/sec: 68.8794\nINFO:tensorflow:loss = 17137.012, step = 601 (1.452 sec)\nINFO:tensorflow:global_step/sec: 66.9792\nINFO:tensorflow:loss = 17190.42, step = 701 (1.493 sec)\nINFO:tensorflow:global_step/sec: 69.3956\nINFO:tensorflow:loss = 17167.111, step = 801 (1.441 sec)\nINFO:tensorflow:global_step/sec: 63.2036\nINFO:tensorflow:loss = 17156.027, step = 901 (1.584 sec)\nINFO:tensorflow:global_step/sec: 66.5333\nINFO:tensorflow:loss = 17144.354, step = 1001 (1.501 sec)\nINFO:tensorflow:global_step/sec: 68.6058\nINFO:tensorflow:loss = 17067.762, step = 1101 (1.458 sec)\nINFO:tensorflow:global_step/sec: 67.3146\nINFO:tensorflow:loss = 17135.996, step = 1201 (1.486 sec)\nINFO:tensorflow:global_step/sec: 64.1376\nINFO:tensorflow:loss = 17125.902, step = 1301 (1.559 sec)\nINFO:tensorflow:global_step/sec: 68.5196\nINFO:tensorflow:loss = 17054.58, step = 1401 (1.459 sec)\nINFO:tensorflow:Saving checkpoints <span class=\"pl-k\">for</span> 1500 into output/my_model/model.ckpt.\nINFO:tensorflow:Loss <span class=\"pl-k\">for</span> final step: 17118.121.</pre></div>", "body_text": "@ispirmustafa Sure!\nI've trimmed it down a bit by using a pre-canned estimator, the default optimizer, and not providing any vars_to_warm_start.\nExample Script\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nFLAGS = None\n\n\ndef main(unused_args):\n    model_dir = os.path.join(FLAGS.model_dir, 'my_model')\n\n    X_train = np.random.rand(30000, 500)\n    y_train = np.random.rand(30000, 2048)\n\n    print('Train data shape...')\n    print(X_train.shape)\n    print(y_train.shape)\n\n    X_dim = X_train.shape[1]\n    y_dim = y_train.shape[1]\n\n    feature_columns = [\n        tf.feature_column.numeric_column('x', shape=X_dim)\n    ]\n\n    warm_start_from = None\n    if FLAGS.warm_start:\n        warm_start_from = tf.estimator.WarmStartSettings(\n            ckpt_to_initialize_from=FLAGS.warm_start,\n        )\n\n    model = tf.estimator.DNNRegressor(\n        hidden_units=[500, 1000, 2048],\n        feature_columns=feature_columns,\n        label_dimension=y_dim,\n        model_dir=model_dir,\n        warm_start_from=warm_start_from\n    )\n\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={'x': X_train},\n        y=y_train,\n        batch_size=100,\n        num_epochs=5,\n        shuffle=True\n    )\n\n    model.train(input_fn=train_input_fn)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'model_dir',\n        nargs='?',\n        default='output',\n        help='Directory to save resulting model(s).'\n    )\n    parser.add_argument(\n        '--warm_start',\n        help='Checkpoint to initialize weights from for fine-tuning.'\n    )\n\n    FLAGS, unparsed = parser.parse_known_args()\n\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\nScript Usage\n$ mkdir -p output\n$ python example.py output/\n$ mv output/my_model output/my_checkpoint\n$ python example.py --warm_start output/my_checkpoint/ output/\nFirst Epoch\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nTrain data shape...\n(30000, 500)\n(30000, 2048)\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f56c3d6f9d0>, '_eval\nuation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save\n_summary_steps': 100}\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\n2018-02-22 23:37:26.110761: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-02-22 23:37:26.207601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-02-22 23:37:26.207969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\npciBusID: 0000:01:00.0\ntotalMemory: 5.93GiB freeMemory: 3.06GiB\n2018-02-22 23:37:26.207991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2018-02-22 23:37:26.358043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2783 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\nINFO:tensorflow:loss = 71532.54, step = 1\nINFO:tensorflow:global_step/sec: 176.009\nINFO:tensorflow:loss = 17626.25, step = 101 (0.568 sec)\nINFO:tensorflow:global_step/sec: 181.392\nINFO:tensorflow:loss = 17433.3, step = 201 (0.551 sec)\nINFO:tensorflow:global_step/sec: 182.943\nINFO:tensorflow:loss = 17426.316, step = 301 (0.547 sec)\nINFO:tensorflow:global_step/sec: 183.914\nINFO:tensorflow:loss = 17366.844, step = 401 (0.544 sec)\nINFO:tensorflow:global_step/sec: 181.319\nINFO:tensorflow:loss = 17432.586, step = 501 (0.552 sec)\nINFO:tensorflow:global_step/sec: 183.824\nINFO:tensorflow:loss = 17425.293, step = 601 (0.544 sec)\nINFO:tensorflow:global_step/sec: 184.213\nINFO:tensorflow:loss = 17454.996, step = 701 (0.543 sec)\nINFO:tensorflow:global_step/sec: 184.085\nINFO:tensorflow:loss = 17382.533, step = 801 (0.543 sec)\nINFO:tensorflow:global_step/sec: 183.105\nINFO:tensorflow:loss = 17416.637, step = 901 (0.546 sec)\nINFO:tensorflow:global_step/sec: 182.509\nINFO:tensorflow:loss = 17479.277, step = 1001 (0.548 sec)\nINFO:tensorflow:global_step/sec: 183.757\nINFO:tensorflow:loss = 17386.068, step = 1101 (0.544 sec)\nINFO:tensorflow:global_step/sec: 182.909\nINFO:tensorflow:loss = 17392.65, step = 1201 (0.547 sec)\nINFO:tensorflow:global_step/sec: 182.233\nINFO:tensorflow:loss = 17397.05, step = 1301 (0.549 sec)\nINFO:tensorflow:global_step/sec: 183.525\nINFO:tensorflow:loss = 17380.441, step = 1401 (0.545 sec)\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\nINFO:tensorflow:Loss for final step: 17338.143.\nSecond Epoch (with warm-start)\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nTrain data shape...\n(30000, 500)\n(30000, 2048)\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb55742fa10>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save_summary_steps': 100}\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='output/my_checkpoint/', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\nINFO:tensorflow:Warm-starting from: ('output/my_checkpoint/',)\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/kernel\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/bias; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/bias\nINFO:tensorflow:Warm-starting variable: dnn/logits/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/logits/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/kernel\nINFO:tensorflow:Warm-starting variable: dnn/logits/bias; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/logits/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/bias\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/kernel\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/bias; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/bias\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/kernel\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/bias; prev_var_name: Unchanged\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/bias\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\n2018-02-22 23:37:49.783077: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-02-22 23:37:49.883252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-02-22 23:37:49.883612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\npciBusID: 0000:01:00.0\ntotalMemory: 5.93GiB freeMemory: 3.08GiB\n2018-02-22 23:37:49.883628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n2018-02-22 23:37:50.032547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2795 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\nINFO:tensorflow:loss = 17359.91, step = 1\nINFO:tensorflow:global_step/sec: 68.0301\nINFO:tensorflow:loss = 17201.738, step = 101 (1.470 sec)\nINFO:tensorflow:global_step/sec: 69.0413\nINFO:tensorflow:loss = 17168.336, step = 201 (1.448 sec)\nINFO:tensorflow:global_step/sec: 69.6031\nINFO:tensorflow:loss = 17170.893, step = 301 (1.437 sec)\nINFO:tensorflow:global_step/sec: 51.401\nINFO:tensorflow:loss = 17156.967, step = 401 (1.947 sec)\nINFO:tensorflow:global_step/sec: 64.7499\nINFO:tensorflow:loss = 17082.166, step = 501 (1.543 sec)\nINFO:tensorflow:global_step/sec: 68.8794\nINFO:tensorflow:loss = 17137.012, step = 601 (1.452 sec)\nINFO:tensorflow:global_step/sec: 66.9792\nINFO:tensorflow:loss = 17190.42, step = 701 (1.493 sec)\nINFO:tensorflow:global_step/sec: 69.3956\nINFO:tensorflow:loss = 17167.111, step = 801 (1.441 sec)\nINFO:tensorflow:global_step/sec: 63.2036\nINFO:tensorflow:loss = 17156.027, step = 901 (1.584 sec)\nINFO:tensorflow:global_step/sec: 66.5333\nINFO:tensorflow:loss = 17144.354, step = 1001 (1.501 sec)\nINFO:tensorflow:global_step/sec: 68.6058\nINFO:tensorflow:loss = 17067.762, step = 1101 (1.458 sec)\nINFO:tensorflow:global_step/sec: 67.3146\nINFO:tensorflow:loss = 17135.996, step = 1201 (1.486 sec)\nINFO:tensorflow:global_step/sec: 64.1376\nINFO:tensorflow:loss = 17125.902, step = 1301 (1.559 sec)\nINFO:tensorflow:global_step/sec: 68.5196\nINFO:tensorflow:loss = 17054.58, step = 1401 (1.459 sec)\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\nINFO:tensorflow:Loss for final step: 17118.121.", "body": "@ispirmustafa Sure!\r\n\r\nI've trimmed it down a bit by using a pre-canned estimator, the default optimizer, and not providing any `vars_to_warm_start`.\r\n\r\n**Example Script**\r\n\r\n```py\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport sys\r\nimport argparse\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n\r\nFLAGS = None\r\n\r\n\r\ndef main(unused_args):\r\n    model_dir = os.path.join(FLAGS.model_dir, 'my_model')\r\n\r\n    X_train = np.random.rand(30000, 500)\r\n    y_train = np.random.rand(30000, 2048)\r\n\r\n    print('Train data shape...')\r\n    print(X_train.shape)\r\n    print(y_train.shape)\r\n\r\n    X_dim = X_train.shape[1]\r\n    y_dim = y_train.shape[1]\r\n\r\n    feature_columns = [\r\n        tf.feature_column.numeric_column('x', shape=X_dim)\r\n    ]\r\n\r\n    warm_start_from = None\r\n    if FLAGS.warm_start:\r\n        warm_start_from = tf.estimator.WarmStartSettings(\r\n            ckpt_to_initialize_from=FLAGS.warm_start,\r\n        )\r\n\r\n    model = tf.estimator.DNNRegressor(\r\n        hidden_units=[500, 1000, 2048],\r\n        feature_columns=feature_columns,\r\n        label_dimension=y_dim,\r\n        model_dir=model_dir,\r\n        warm_start_from=warm_start_from\r\n    )\r\n\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={'x': X_train},\r\n        y=y_train,\r\n        batch_size=100,\r\n        num_epochs=5,\r\n        shuffle=True\r\n    )\r\n\r\n    model.train(input_fn=train_input_fn)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        'model_dir',\r\n        nargs='?',\r\n        default='output',\r\n        help='Directory to save resulting model(s).'\r\n    )\r\n    parser.add_argument(\r\n        '--warm_start',\r\n        help='Checkpoint to initialize weights from for fine-tuning.'\r\n    )\r\n\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n\r\n**Script Usage**\r\n\r\n```sh\r\n$ mkdir -p output\r\n$ python example.py output/\r\n$ mv output/my_model output/my_checkpoint\r\n$ python example.py --warm_start output/my_checkpoint/ output/\r\n```\r\n**First Epoch**\r\n\r\n```sh\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTrain data shape...\r\n(30000, 500)\r\n(30000, 2048)\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f56c3d6f9d0>, '_eval\r\nuation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save\r\n_summary_steps': 100}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-22 23:37:26.110761: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-02-22 23:37:26.207601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-22 23:37:26.207969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 3.06GiB\r\n2018-02-22 23:37:26.207991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-22 23:37:26.358043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2783 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\r\nINFO:tensorflow:loss = 71532.54, step = 1\r\nINFO:tensorflow:global_step/sec: 176.009\r\nINFO:tensorflow:loss = 17626.25, step = 101 (0.568 sec)\r\nINFO:tensorflow:global_step/sec: 181.392\r\nINFO:tensorflow:loss = 17433.3, step = 201 (0.551 sec)\r\nINFO:tensorflow:global_step/sec: 182.943\r\nINFO:tensorflow:loss = 17426.316, step = 301 (0.547 sec)\r\nINFO:tensorflow:global_step/sec: 183.914\r\nINFO:tensorflow:loss = 17366.844, step = 401 (0.544 sec)\r\nINFO:tensorflow:global_step/sec: 181.319\r\nINFO:tensorflow:loss = 17432.586, step = 501 (0.552 sec)\r\nINFO:tensorflow:global_step/sec: 183.824\r\nINFO:tensorflow:loss = 17425.293, step = 601 (0.544 sec)\r\nINFO:tensorflow:global_step/sec: 184.213\r\nINFO:tensorflow:loss = 17454.996, step = 701 (0.543 sec)\r\nINFO:tensorflow:global_step/sec: 184.085\r\nINFO:tensorflow:loss = 17382.533, step = 801 (0.543 sec)\r\nINFO:tensorflow:global_step/sec: 183.105\r\nINFO:tensorflow:loss = 17416.637, step = 901 (0.546 sec)\r\nINFO:tensorflow:global_step/sec: 182.509\r\nINFO:tensorflow:loss = 17479.277, step = 1001 (0.548 sec)\r\nINFO:tensorflow:global_step/sec: 183.757\r\nINFO:tensorflow:loss = 17386.068, step = 1101 (0.544 sec)\r\nINFO:tensorflow:global_step/sec: 182.909\r\nINFO:tensorflow:loss = 17392.65, step = 1201 (0.547 sec)\r\nINFO:tensorflow:global_step/sec: 182.233\r\nINFO:tensorflow:loss = 17397.05, step = 1301 (0.549 sec)\r\nINFO:tensorflow:global_step/sec: 183.525\r\nINFO:tensorflow:loss = 17380.441, step = 1401 (0.545 sec)\r\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 17338.143.\r\n```\r\n\r\n**Second Epoch (with warm-start)**\r\n\r\n```sh\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTrain data shape...\r\n(30000, 500)\r\n(30000, 2048)\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb55742fa10>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'output/my_model', '_save_summary_steps': 100}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='output/my_checkpoint/', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('output/my_checkpoint/',)\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/bias\r\nINFO:tensorflow:Warm-starting variable: dnn/logits/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/logits/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/logits/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/logits/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/logits/bias\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_2/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_2/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_2/bias\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_0/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_0/kernel/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_0/kernel\r\nINFO:tensorflow:Warm-starting variable: dnn/hiddenlayer_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable dnn/hiddenlayer_1/bias/part_0:0 from checkpoint output/my_checkpoint/ with dnn/hiddenlayer_1/bias\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-02-22 23:37:49.783077: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-02-22 23:37:49.883252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-22 23:37:49.883612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 3.08GiB\r\n2018-02-22 23:37:49.883628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-22 23:37:50.032547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2795 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into output/my_model/model.ckpt.\r\nINFO:tensorflow:loss = 17359.91, step = 1\r\nINFO:tensorflow:global_step/sec: 68.0301\r\nINFO:tensorflow:loss = 17201.738, step = 101 (1.470 sec)\r\nINFO:tensorflow:global_step/sec: 69.0413\r\nINFO:tensorflow:loss = 17168.336, step = 201 (1.448 sec)\r\nINFO:tensorflow:global_step/sec: 69.6031\r\nINFO:tensorflow:loss = 17170.893, step = 301 (1.437 sec)\r\nINFO:tensorflow:global_step/sec: 51.401\r\nINFO:tensorflow:loss = 17156.967, step = 401 (1.947 sec)\r\nINFO:tensorflow:global_step/sec: 64.7499\r\nINFO:tensorflow:loss = 17082.166, step = 501 (1.543 sec)\r\nINFO:tensorflow:global_step/sec: 68.8794\r\nINFO:tensorflow:loss = 17137.012, step = 601 (1.452 sec)\r\nINFO:tensorflow:global_step/sec: 66.9792\r\nINFO:tensorflow:loss = 17190.42, step = 701 (1.493 sec)\r\nINFO:tensorflow:global_step/sec: 69.3956\r\nINFO:tensorflow:loss = 17167.111, step = 801 (1.441 sec)\r\nINFO:tensorflow:global_step/sec: 63.2036\r\nINFO:tensorflow:loss = 17156.027, step = 901 (1.584 sec)\r\nINFO:tensorflow:global_step/sec: 66.5333\r\nINFO:tensorflow:loss = 17144.354, step = 1001 (1.501 sec)\r\nINFO:tensorflow:global_step/sec: 68.6058\r\nINFO:tensorflow:loss = 17067.762, step = 1101 (1.458 sec)\r\nINFO:tensorflow:global_step/sec: 67.3146\r\nINFO:tensorflow:loss = 17135.996, step = 1201 (1.486 sec)\r\nINFO:tensorflow:global_step/sec: 64.1376\r\nINFO:tensorflow:loss = 17125.902, step = 1301 (1.559 sec)\r\nINFO:tensorflow:global_step/sec: 68.5196\r\nINFO:tensorflow:loss = 17054.58, step = 1401 (1.459 sec)\r\nINFO:tensorflow:Saving checkpoints for 1500 into output/my_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 17118.121.\r\n```"}