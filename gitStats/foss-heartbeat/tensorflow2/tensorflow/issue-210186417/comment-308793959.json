{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308793959", "html_url": "https://github.com/tensorflow/tensorflow/issues/7868#issuecomment-308793959", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7868", "id": 308793959, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODc5Mzk1OQ==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-15T16:24:48Z", "updated_at": "2017-06-15T16:24:48Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1184671\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xiejw\">@xiejw</a> for additional comments. A hyperopt tuner would be very cool.</p>\n<p>You need <code>tf.Server</code> etc. only when you want each experiment to run on several machines. If you have one machine per experiment, you can distribute the python code and run it on each machine independently. You just have to make sure that your Tuner distributes different hyperparameter values to different workers.</p>", "body_text": "@xiejw for additional comments. A hyperopt tuner would be very cool.\nYou need tf.Server etc. only when you want each experiment to run on several machines. If you have one machine per experiment, you can distribute the python code and run it on each machine independently. You just have to make sure that your Tuner distributes different hyperparameter values to different workers.", "body": "@xiejw for additional comments. A hyperopt tuner would be very cool.\r\n\r\nYou need `tf.Server` etc. only when you want each experiment to run on several machines. If you have one machine per experiment, you can distribute the python code and run it on each machine independently. You just have to make sure that your Tuner distributes different hyperparameter values to different workers."}