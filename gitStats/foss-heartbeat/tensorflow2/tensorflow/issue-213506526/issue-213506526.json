{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8299", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8299/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8299/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8299/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8299", "id": 213506526, "node_id": "MDU6SXNzdWUyMTM1MDY1MjY=", "number": 8299, "title": "tf.contrib.seq2seq.prepare_attention doesn't allow decoder states and attention states to be different lengths", "user": {"login": "abisee", "id": 14880223, "node_id": "MDQ6VXNlcjE0ODgwMjIz", "avatar_url": "https://avatars3.githubusercontent.com/u/14880223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abisee", "html_url": "https://github.com/abisee", "followers_url": "https://api.github.com/users/abisee/followers", "following_url": "https://api.github.com/users/abisee/following{/other_user}", "gists_url": "https://api.github.com/users/abisee/gists{/gist_id}", "starred_url": "https://api.github.com/users/abisee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abisee/subscriptions", "organizations_url": "https://api.github.com/users/abisee/orgs", "repos_url": "https://api.github.com/users/abisee/repos", "events_url": "https://api.github.com/users/abisee/events{/privacy}", "received_events_url": "https://api.github.com/users/abisee/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-11T05:14:53Z", "updated_at": "2017-03-11T20:18:45Z", "closed_at": "2017-03-11T20:18:45Z", "author_association": "NONE", "body_html": "<p>I'm using the new <code>tf.contrib.seq2seq.prepare_attention</code> with <code>tf.contrib.seq2seq.attention_decoder_fn_train</code> and <code>tf.contrib.seq2seq.dynamic_rnn_decoder</code> to do dynamic decoding with attention.</p>\n<p>If we are using e.g. <code>attention_option=\"bahdanau\"</code>, then <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py#L408\">this line</a> implements the standard attention equation:</p>\n<p>score<sub>i</sub> = v<sup>T</sup> tanh(W<sub>h</sub> h<sub>i</sub> + W<sub>q</sub> q)</p>\n<p>where</p>\n<ul>\n<li>h<sub>i</sub> is the ith attention state, a vector length <code>num_units</code></li>\n<li>W<sub>h</sub> is a weight matrix shape [<code>num_units</code>, <code>num_units</code>]</li>\n<li>q is the query (i.e. current decoder hidden state), a vector length <code>num_units</code></li>\n<li>W<sub>q</sub> is a weight matrix shape [<code>num_units</code>, <code>num_units</code>]</li>\n<li>v is a weight vector length <code>num_units</code></li>\n</ul>\n<p>In particular, the code assumes that:</p>\n<ol>\n<li>Decoder hidden states q and attention states h<sub>i</sub> are the same size (which isn't true if e.g. you want different size hidden vectors for your encoder and decoder, or you want to use bidirectional RNN for encoder but not decoder)</li>\n<li>v, W<sub>h</sub> h<sub>i</sub> and W<sub>q</sub> q must also be same length <code>num_units</code></li>\n</ol>\n<p>In particular assumption 1 is very limiting. I think it would be better to allow:</p>\n<ul>\n<li>h<sub>i</sub> is the ith attention state, a vector length <code>attn_size</code></li>\n<li>W<sub>h</sub> is a weight matrix shape [<code>num_units</code>, <code>attn_size</code>]</li>\n<li>q is the query (i.e. current decoder hidden state), a vector length <code>query_size</code></li>\n<li>W<sub>q</sub> is a weight matrix shape [<code>num_units</code>, <code>query_size</code>]</li>\n<li>v is a weight vector length <code>num_units</code></li>\n</ul>\n<p>where the user can define <code>num_units</code>, <code>attn_size</code> and <code>query_size</code>. From what I can see this would be fairly uncomplicated.</p>", "body_text": "I'm using the new tf.contrib.seq2seq.prepare_attention with tf.contrib.seq2seq.attention_decoder_fn_train and tf.contrib.seq2seq.dynamic_rnn_decoder to do dynamic decoding with attention.\nIf we are using e.g. attention_option=\"bahdanau\", then this line implements the standard attention equation:\nscorei = vT tanh(Wh hi + Wq q)\nwhere\n\nhi is the ith attention state, a vector length num_units\nWh is a weight matrix shape [num_units, num_units]\nq is the query (i.e. current decoder hidden state), a vector length num_units\nWq is a weight matrix shape [num_units, num_units]\nv is a weight vector length num_units\n\nIn particular, the code assumes that:\n\nDecoder hidden states q and attention states hi are the same size (which isn't true if e.g. you want different size hidden vectors for your encoder and decoder, or you want to use bidirectional RNN for encoder but not decoder)\nv, Wh hi and Wq q must also be same length num_units\n\nIn particular assumption 1 is very limiting. I think it would be better to allow:\n\nhi is the ith attention state, a vector length attn_size\nWh is a weight matrix shape [num_units, attn_size]\nq is the query (i.e. current decoder hidden state), a vector length query_size\nWq is a weight matrix shape [num_units, query_size]\nv is a weight vector length num_units\n\nwhere the user can define num_units, attn_size and query_size. From what I can see this would be fairly uncomplicated.", "body": "I'm using the new `tf.contrib.seq2seq.prepare_attention` with `tf.contrib.seq2seq.attention_decoder_fn_train` and `tf.contrib.seq2seq.dynamic_rnn_decoder` to do dynamic decoding with attention. \r\n\r\nIf we are using e.g. `attention_option=\"bahdanau\"`, then [this line](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py#L408) implements the standard attention equation:\r\n\r\nscore<sub>i</sub> = v<sup>T</sup> tanh(W<sub>h</sub> h<sub>i</sub> + W<sub>q</sub> q)\r\n\r\nwhere \r\n- h<sub>i</sub> is the ith attention state, a vector length `num_units`\r\n- W<sub>h</sub> is a weight matrix shape [`num_units`, `num_units`]\r\n- q is the query (i.e. current decoder hidden state), a vector length `num_units`\r\n- W<sub>q</sub> is a weight matrix shape [`num_units`, `num_units`]\r\n- v is a weight vector length `num_units`\r\n\r\nIn particular, the code assumes that:\r\n1. Decoder hidden states q and attention states h<sub>i</sub> are the same size (which isn't true if e.g. you want different size hidden vectors for your encoder and decoder, or you want to use bidirectional RNN for encoder but not decoder)\r\n2. v, W<sub>h</sub> h<sub>i</sub> and W<sub>q</sub> q must also be same length `num_units`\r\n\r\nIn particular assumption 1 is very limiting. I think it would be better to allow:\r\n- h<sub>i</sub> is the ith attention state, a vector length `attn_size`\r\n- W<sub>h</sub> is a weight matrix shape [`num_units`, `attn_size`]\r\n- q is the query (i.e. current decoder hidden state), a vector length `query_size`\r\n- W<sub>q</sub> is a weight matrix shape [`num_units`, `query_size`]\r\n- v is a weight vector length `num_units`\r\n\r\nwhere the user can define `num_units`, `attn_size` and `query_size`. From what I can see this would be fairly uncomplicated."}