{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318072321", "html_url": "https://github.com/tensorflow/tensorflow/issues/7031#issuecomment-318072321", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7031", "id": 318072321, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODA3MjMyMQ==", "user": {"login": "tiru1930", "id": 12211287, "node_id": "MDQ6VXNlcjEyMjExMjg3", "avatar_url": "https://avatars2.githubusercontent.com/u/12211287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tiru1930", "html_url": "https://github.com/tiru1930", "followers_url": "https://api.github.com/users/tiru1930/followers", "following_url": "https://api.github.com/users/tiru1930/following{/other_user}", "gists_url": "https://api.github.com/users/tiru1930/gists{/gist_id}", "starred_url": "https://api.github.com/users/tiru1930/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tiru1930/subscriptions", "organizations_url": "https://api.github.com/users/tiru1930/orgs", "repos_url": "https://api.github.com/users/tiru1930/repos", "events_url": "https://api.github.com/users/tiru1930/events{/privacy}", "received_events_url": "https://api.github.com/users/tiru1930/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T14:35:23Z", "updated_at": "2017-07-26T14:35:23Z", "author_association": "NONE", "body_html": "<p>from <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import time<br>\nimport logging</p>\n<p>import numpy as np</p>\n<h1>from six.moves import xrange</h1>\n<p>import tensorflow as tf<br>\nfrom tensorflow.python import debug as tf_debug</p>\n<p>logging.basicConfig(level=logging.INFO)</p>\n<p>class Encoder(object):</p>\n<pre><code>def __init__(self,hidden_size,initializer=lambda:None):\n    self.hidden_size=hidden_size\n    self.initializer=initializer\n\ndef encode(self,inputs,span):\n\n    document,answer = inputs\n    start_idx,end_idx = span\n\n\n\n\n    with tf.variable_scope('encode'):\n        # weights = tf.Variable(tf.random_normal([2,3], stddev=0.35),name=\"weights\")\n        # biases = tf.Variable(tf.zeros([1]), name=\"biases\")\n        # label  = tf.Variable(tf.random_normal([2,3],stddev=0.35),name='lable')\n        # weights = tf.Variable(shape=[None,None],name=\"weights\")\n        # biases = tf.Variable(tf.zeros([None]), name=\"biases\")\n        # label  = tf.Variable(shape=[None,None],name='lable')\n        print(document.get_shape())\n        document_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)\n        document_vector=document_vector_orig\n        for i in range(tf.shape(document_vector)[0]):\n            document_vector[i]=document_vector[i][start_idx:end_idx]\n        annotation_sequence=tf.concat(document_vector,answer,axis=2, name='annotation_sequence')\n        answer_vector=self.bidirectional_LSTM(annotation_sequence,self.hidden_size,self.initializer,output_sequence=False)\n\n        r=tf.matmul(lable,answer_vector)+(tf.reduce_sum(document_vector_orig,2,keep_dims=True))\n        intial_state=tf.tanh(tf.matmul(r,weights)+biases)\n\n    return document_vector_orig,answer_vector ,intial_state\n</code></pre>\n<p>def bidirectional_LSTM(input, hidden_state_dimension, initializer, sequence_length=None, output_sequence=True):</p>\n<pre><code>with tf.variable_scope(\"bidirectional_LSTM\"):\n    initializer=None\n    print(sequence_length)\n    if sequence_length == None:\n        batch_size = 1\n        sequence_length = tf.shape(input)[1]\n        sequence_length = tf.expand_dims(sequence_length, axis=0, name='sequence_length')\n    else:\n        batch_size = tf.shape(sequence_length)[0]\n\n    lstm_cell = {}\n    initial_state = {}\n    for direction in [\"forward\", \"backward\"]:\n        with tf.variable_scope(direction):\n            # LSTM cell\n            lstm_cell[direction] = tf.contrib.rnn.CoupledInputForgetGateLSTMCell(hidden_state_dimension, \\\n                                    forget_bias=1.0, initializer=initializer, state_is_tuple=True)\n            # initial state: http://stackoverflow.com/questions/38441589/tensorflow-rnn-initial-state\n            initial_cell_state = tf.get_variable(\"initial_cell_state\", shape=[1, hidden_state_dimension], \\\n                                                dtype=tf.float32, initializer=initializer)\n            initial_output_state = tf.get_variable(\"initial_output_state\", shape=[1, hidden_state_dimension],\\\n                                                 dtype=tf.float32, initializer=initializer)\n            c_states = tf.tile(initial_cell_state, tf.stack([batch_size, 1]))\n            h_states = tf.tile(initial_output_state, tf.stack([batch_size, 1]))\n            initial_state[direction] = tf.nn.rnn_cell.LSTMStateTuple(c_states, h_states)\n\n    # sequence_length must be provided for tf.nn.bidirectional_dynamic_rnn due to internal bug\n    outputs, final_states = tf.nn.bidirectional_dynamic_rnn(lstm_cell[\"forward\"],\n                                                                lstm_cell[\"backward\"],\n                                                                input,\n                                                                dtype=tf.float32,\n                                                                sequence_length=sequence_length,\n                                                                initial_state_fw=initial_state[\"forward\"],\n                                                                initial_state_bw=initial_state[\"backward\"])\n    if output_sequence == True:\n        outputs_forward, outputs_backward = outputs\n        print(outputs)\n        print(outputs_forward.get_shape())\n        print(outputs_backward.get_shape())\n        # print(outputs_forward[0][1][1])\n        output = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\n    else:\n\n        final_states_forward, final_states_backward = final_states\n        output = tf.concat([final_states_forward[1], final_states_backward[1]], 1, name='output')\n\nreturn output\n</code></pre>\n<p>def test():</p>\n<pre><code>hidden_size=100\nencoder=Encoder(hidden_size)\ndocument_placeholder=tf.placeholder(tf.float32,(None,2,4),name='Document_placeholder')\nanswer_paceholder=tf.placeholder(tf.float32,(None,2,3),name='Answer_paceholder')\nstart_id=tf.placeholder(tf.float32,(None),name='start_id')\nend_id=tf.placeholder(tf.float32,(None),name='end_id')\n\nparagraph =[[0,1],[1,0],[0,2],[5,3]]\n\nquestion = [[3,3],[5,5],[1,1]]\nfunc = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    HQ,HP,state = sess.run(func, feed_dict = {document_placeholder :paragraph, answer_paceholder : question,start_id : 1, \\\n        end_id : 2}) \nprint(HQ.get_shape().as_list())\nprint(HP.get_shape().as_list())\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\ntest()</p>\n<p>I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally<br>\n(?, 2, 4)<br>\nNone<br>\n(&lt;tf.Tensor 'encode/bidirectional_LSTM/BiRNN/FW/FW/transpose:0' shape=(1, 2, 100) dtype=float32&gt;, &lt;tf.Tensor 'encode/bidirectional_LSTM/ReverseSequence:0' shape=(1, 2, 100) dtype=float32&gt;)<br>\n(1, 2, 100)<br>\n(1, 2, 100)<br>\nTraceback (most recent call last):<br>\nFile \"q_gen_model.py\", line 132, in <br>\ntest()<br>\nFile \"q_gen_model.py\", line 120, in test<br>\nfunc = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])<br>\nFile \"q_gen_model.py\", line 38, in encode<br>\ndocument_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)<br>\nFile \"q_gen_model.py\", line 95, in bidirectional_LSTM<br>\noutput = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1075, in concat<br>\ndtype=dtypes.int32).get_shape(<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor<br>\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function<br>\nreturn constant(v, dtype=dtype, name=name)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant<br>\ntensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto<br>\n_AssertCompatible(values, dtype)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible<br>\n(dtype.name, repr(mismatch), type(mismatch).<strong>name</strong>))<br>\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.</p>", "body_text": "from future import absolute_import\nfrom future import division\nfrom future import print_function\nimport time\nimport logging\nimport numpy as np\nfrom six.moves import xrange\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\nlogging.basicConfig(level=logging.INFO)\nclass Encoder(object):\ndef __init__(self,hidden_size,initializer=lambda:None):\n    self.hidden_size=hidden_size\n    self.initializer=initializer\n\ndef encode(self,inputs,span):\n\n    document,answer = inputs\n    start_idx,end_idx = span\n\n\n\n\n    with tf.variable_scope('encode'):\n        # weights = tf.Variable(tf.random_normal([2,3], stddev=0.35),name=\"weights\")\n        # biases = tf.Variable(tf.zeros([1]), name=\"biases\")\n        # label  = tf.Variable(tf.random_normal([2,3],stddev=0.35),name='lable')\n        # weights = tf.Variable(shape=[None,None],name=\"weights\")\n        # biases = tf.Variable(tf.zeros([None]), name=\"biases\")\n        # label  = tf.Variable(shape=[None,None],name='lable')\n        print(document.get_shape())\n        document_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)\n        document_vector=document_vector_orig\n        for i in range(tf.shape(document_vector)[0]):\n            document_vector[i]=document_vector[i][start_idx:end_idx]\n        annotation_sequence=tf.concat(document_vector,answer,axis=2, name='annotation_sequence')\n        answer_vector=self.bidirectional_LSTM(annotation_sequence,self.hidden_size,self.initializer,output_sequence=False)\n\n        r=tf.matmul(lable,answer_vector)+(tf.reduce_sum(document_vector_orig,2,keep_dims=True))\n        intial_state=tf.tanh(tf.matmul(r,weights)+biases)\n\n    return document_vector_orig,answer_vector ,intial_state\n\ndef bidirectional_LSTM(input, hidden_state_dimension, initializer, sequence_length=None, output_sequence=True):\nwith tf.variable_scope(\"bidirectional_LSTM\"):\n    initializer=None\n    print(sequence_length)\n    if sequence_length == None:\n        batch_size = 1\n        sequence_length = tf.shape(input)[1]\n        sequence_length = tf.expand_dims(sequence_length, axis=0, name='sequence_length')\n    else:\n        batch_size = tf.shape(sequence_length)[0]\n\n    lstm_cell = {}\n    initial_state = {}\n    for direction in [\"forward\", \"backward\"]:\n        with tf.variable_scope(direction):\n            # LSTM cell\n            lstm_cell[direction] = tf.contrib.rnn.CoupledInputForgetGateLSTMCell(hidden_state_dimension, \\\n                                    forget_bias=1.0, initializer=initializer, state_is_tuple=True)\n            # initial state: http://stackoverflow.com/questions/38441589/tensorflow-rnn-initial-state\n            initial_cell_state = tf.get_variable(\"initial_cell_state\", shape=[1, hidden_state_dimension], \\\n                                                dtype=tf.float32, initializer=initializer)\n            initial_output_state = tf.get_variable(\"initial_output_state\", shape=[1, hidden_state_dimension],\\\n                                                 dtype=tf.float32, initializer=initializer)\n            c_states = tf.tile(initial_cell_state, tf.stack([batch_size, 1]))\n            h_states = tf.tile(initial_output_state, tf.stack([batch_size, 1]))\n            initial_state[direction] = tf.nn.rnn_cell.LSTMStateTuple(c_states, h_states)\n\n    # sequence_length must be provided for tf.nn.bidirectional_dynamic_rnn due to internal bug\n    outputs, final_states = tf.nn.bidirectional_dynamic_rnn(lstm_cell[\"forward\"],\n                                                                lstm_cell[\"backward\"],\n                                                                input,\n                                                                dtype=tf.float32,\n                                                                sequence_length=sequence_length,\n                                                                initial_state_fw=initial_state[\"forward\"],\n                                                                initial_state_bw=initial_state[\"backward\"])\n    if output_sequence == True:\n        outputs_forward, outputs_backward = outputs\n        print(outputs)\n        print(outputs_forward.get_shape())\n        print(outputs_backward.get_shape())\n        # print(outputs_forward[0][1][1])\n        output = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\n    else:\n\n        final_states_forward, final_states_backward = final_states\n        output = tf.concat([final_states_forward[1], final_states_backward[1]], 1, name='output')\n\nreturn output\n\ndef test():\nhidden_size=100\nencoder=Encoder(hidden_size)\ndocument_placeholder=tf.placeholder(tf.float32,(None,2,4),name='Document_placeholder')\nanswer_paceholder=tf.placeholder(tf.float32,(None,2,3),name='Answer_paceholder')\nstart_id=tf.placeholder(tf.float32,(None),name='start_id')\nend_id=tf.placeholder(tf.float32,(None),name='end_id')\n\nparagraph =[[0,1],[1,0],[0,2],[5,3]]\n\nquestion = [[3,3],[5,5],[1,1]]\nfunc = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    HQ,HP,state = sess.run(func, feed_dict = {document_placeholder :paragraph, answer_paceholder : question,start_id : 1, \\\n        end_id : 2}) \nprint(HQ.get_shape().as_list())\nprint(HP.get_shape().as_list())\n\nif name == 'main':\ntest()\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n(?, 2, 4)\nNone\n(<tf.Tensor 'encode/bidirectional_LSTM/BiRNN/FW/FW/transpose:0' shape=(1, 2, 100) dtype=float32>, <tf.Tensor 'encode/bidirectional_LSTM/ReverseSequence:0' shape=(1, 2, 100) dtype=float32>)\n(1, 2, 100)\n(1, 2, 100)\nTraceback (most recent call last):\nFile \"q_gen_model.py\", line 132, in \ntest()\nFile \"q_gen_model.py\", line 120, in test\nfunc = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])\nFile \"q_gen_model.py\", line 38, in encode\ndocument_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)\nFile \"q_gen_model.py\", line 95, in bidirectional_LSTM\noutput = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1075, in concat\ndtype=dtypes.int32).get_shape(\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\nreturn constant(v, dtype=dtype, name=name)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\ntensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\n_AssertCompatible(values, dtype)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\n(dtype.name, repr(mismatch), type(mismatch).name))\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.", "body": "from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport time\r\nimport logging\r\n\r\nimport numpy as np\r\n# from six.moves import xrange \r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\n\r\nclass Encoder(object):\r\n\r\n    def __init__(self,hidden_size,initializer=lambda:None):\r\n        self.hidden_size=hidden_size\r\n        self.initializer=initializer\r\n\r\n    def encode(self,inputs,span):\r\n\r\n        document,answer = inputs\r\n        start_idx,end_idx = span\r\n\r\n\r\n\r\n\r\n        with tf.variable_scope('encode'):\r\n            # weights = tf.Variable(tf.random_normal([2,3], stddev=0.35),name=\"weights\")\r\n            # biases = tf.Variable(tf.zeros([1]), name=\"biases\")\r\n            # label  = tf.Variable(tf.random_normal([2,3],stddev=0.35),name='lable')\r\n            # weights = tf.Variable(shape=[None,None],name=\"weights\")\r\n            # biases = tf.Variable(tf.zeros([None]), name=\"biases\")\r\n            # label  = tf.Variable(shape=[None,None],name='lable')\r\n            print(document.get_shape())\r\n            document_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)\r\n            document_vector=document_vector_orig\r\n            for i in range(tf.shape(document_vector)[0]):\r\n                document_vector[i]=document_vector[i][start_idx:end_idx]\r\n            annotation_sequence=tf.concat(document_vector,answer,axis=2, name='annotation_sequence')\r\n            answer_vector=self.bidirectional_LSTM(annotation_sequence,self.hidden_size,self.initializer,output_sequence=False)\r\n\r\n            r=tf.matmul(lable,answer_vector)+(tf.reduce_sum(document_vector_orig,2,keep_dims=True))\r\n            intial_state=tf.tanh(tf.matmul(r,weights)+biases)\r\n\r\n        return document_vector_orig,answer_vector ,intial_state\r\n\r\n            \r\n\r\n\r\ndef bidirectional_LSTM(input, hidden_state_dimension, initializer, sequence_length=None, output_sequence=True):\r\n\r\n    with tf.variable_scope(\"bidirectional_LSTM\"):\r\n        initializer=None\r\n        print(sequence_length)\r\n        if sequence_length == None:\r\n            batch_size = 1\r\n            sequence_length = tf.shape(input)[1]\r\n            sequence_length = tf.expand_dims(sequence_length, axis=0, name='sequence_length')\r\n        else:\r\n            batch_size = tf.shape(sequence_length)[0]\r\n\r\n        lstm_cell = {}\r\n        initial_state = {}\r\n        for direction in [\"forward\", \"backward\"]:\r\n            with tf.variable_scope(direction):\r\n                # LSTM cell\r\n                lstm_cell[direction] = tf.contrib.rnn.CoupledInputForgetGateLSTMCell(hidden_state_dimension, \\\r\n                                        forget_bias=1.0, initializer=initializer, state_is_tuple=True)\r\n                # initial state: http://stackoverflow.com/questions/38441589/tensorflow-rnn-initial-state\r\n                initial_cell_state = tf.get_variable(\"initial_cell_state\", shape=[1, hidden_state_dimension], \\\r\n                                                    dtype=tf.float32, initializer=initializer)\r\n                initial_output_state = tf.get_variable(\"initial_output_state\", shape=[1, hidden_state_dimension],\\\r\n                                                     dtype=tf.float32, initializer=initializer)\r\n                c_states = tf.tile(initial_cell_state, tf.stack([batch_size, 1]))\r\n                h_states = tf.tile(initial_output_state, tf.stack([batch_size, 1]))\r\n                initial_state[direction] = tf.nn.rnn_cell.LSTMStateTuple(c_states, h_states)\r\n\r\n        # sequence_length must be provided for tf.nn.bidirectional_dynamic_rnn due to internal bug\r\n        outputs, final_states = tf.nn.bidirectional_dynamic_rnn(lstm_cell[\"forward\"],\r\n                                                                    lstm_cell[\"backward\"],\r\n                                                                    input,\r\n                                                                    dtype=tf.float32,\r\n                                                                    sequence_length=sequence_length,\r\n                                                                    initial_state_fw=initial_state[\"forward\"],\r\n                                                                    initial_state_bw=initial_state[\"backward\"])\r\n        if output_sequence == True:\r\n            outputs_forward, outputs_backward = outputs\r\n            print(outputs)\r\n            print(outputs_forward.get_shape())\r\n            print(outputs_backward.get_shape())\r\n            # print(outputs_forward[0][1][1])\r\n            output = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\r\n        else:\r\n\r\n            final_states_forward, final_states_backward = final_states\r\n            output = tf.concat([final_states_forward[1], final_states_backward[1]], 1, name='output')\r\n\r\n    return output\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef test():\r\n\r\n    hidden_size=100\r\n    encoder=Encoder(hidden_size)\r\n    document_placeholder=tf.placeholder(tf.float32,(None,2,4),name='Document_placeholder')\r\n    answer_paceholder=tf.placeholder(tf.float32,(None,2,3),name='Answer_paceholder')\r\n    start_id=tf.placeholder(tf.float32,(None),name='start_id')\r\n    end_id=tf.placeholder(tf.float32,(None),name='end_id')\r\n\r\n    paragraph =[[0,1],[1,0],[0,2],[5,3]]\r\n\r\n    question = [[3,3],[5,5],[1,1]]\r\n    func = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])\r\n\r\n    init = tf.global_variables_initializer()\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        HQ,HP,state = sess.run(func, feed_dict = {document_placeholder :paragraph, answer_paceholder : question,start_id : 1, \\\r\n            end_id : 2}) \r\n    print(HQ.get_shape().as_list())\r\n    print(HP.get_shape().as_list())\r\n\r\n\r\nif __name__ == '__main__':\r\n    test()\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n(?, 2, 4)\r\nNone\r\n(<tf.Tensor 'encode/bidirectional_LSTM/BiRNN/FW/FW/transpose:0' shape=(1, 2, 100) dtype=float32>, <tf.Tensor 'encode/bidirectional_LSTM/ReverseSequence:0' shape=(1, 2, 100) dtype=float32>)\r\n(1, 2, 100)\r\n(1, 2, 100)\r\nTraceback (most recent call last):\r\n  File \"q_gen_model.py\", line 132, in <module>\r\n    test()\r\n  File \"q_gen_model.py\", line 120, in test\r\n    func = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])\r\n  File \"q_gen_model.py\", line 38, in encode\r\n    document_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)\r\n  File \"q_gen_model.py\", line 95, in bidirectional_LSTM\r\n    output = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1075, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n"}