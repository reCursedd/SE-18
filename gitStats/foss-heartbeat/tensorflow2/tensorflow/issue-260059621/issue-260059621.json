{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13269", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13269/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13269/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13269/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13269", "id": 260059621, "node_id": "MDU6SXNzdWUyNjAwNTk2MjE=", "number": 13269, "title": "TensorFlow 1.0 and 1.2 behave differently on MultiRNNCell.", "user": {"login": "lhg912", "id": 12981081, "node_id": "MDQ6VXNlcjEyOTgxMDgx", "avatar_url": "https://avatars3.githubusercontent.com/u/12981081?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhg912", "html_url": "https://github.com/lhg912", "followers_url": "https://api.github.com/users/lhg912/followers", "following_url": "https://api.github.com/users/lhg912/following{/other_user}", "gists_url": "https://api.github.com/users/lhg912/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhg912/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhg912/subscriptions", "organizations_url": "https://api.github.com/users/lhg912/orgs", "repos_url": "https://api.github.com/users/lhg912/repos", "events_url": "https://api.github.com/users/lhg912/events{/privacy}", "received_events_url": "https://api.github.com/users/lhg912/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-24T05:33:15Z", "updated_at": "2017-09-26T00:02:38Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>The following code works well on TF 1.0.1, but doesn't work on TF 1.2 and above, leaving error massage,</p>\n<p>File \"pb_OE_column_on_the_spot.py\", line 318, in <br>\noutputs1, _states = tf.nn.dynamic_rnn(_multi_cells, tf.one_hot(_X, data_dim), dtype=tf.float32)<br>\nValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (256, 512) and found shape (132, 512).</p>\n<p>I followed the change after version 1.2: <a href=\"url\">https://github.com/tensorflow/tensorflow/releases</a>,<br>\nwhere states: To get 5 layers each with their own parameters, write: MultiRNNCell([LSTMCell(...) for _ in range(5)])</p>\n<p>and I also reviewed similar posts regarding same issue: <a href=\"url\">https://github.com/udacity/deep-learning/issues/132</a></p>\n<pre><code>seq_length = 6\ndata_dim =4\nhidden_dim = 12\nX = tf.placeholder(tf.int32, [None, seq_length])\nY = tf.placeholder(tf.int32, [None]) \nkeep_prob = tf.placeholder(tf.float32) \n\nX_one_hot = tf.one_hot(X, data_dim)\nY_one_hot = tf.one_hot(Y, 2) \n\ndef lstm_cell():\n    lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n    return drop\n\nmulti_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)], state_is_tuple=True)\noutputs1, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n</code></pre>\n<p>Why does this error happen?<br>\n.<br>\n.<br>\n.<br>\nby the way, would 12 for the number of units in the LSTM cell be too small?</p>", "body_text": "The following code works well on TF 1.0.1, but doesn't work on TF 1.2 and above, leaving error massage,\nFile \"pb_OE_column_on_the_spot.py\", line 318, in \noutputs1, _states = tf.nn.dynamic_rnn(_multi_cells, tf.one_hot(_X, data_dim), dtype=tf.float32)\nValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (256, 512) and found shape (132, 512).\nI followed the change after version 1.2: https://github.com/tensorflow/tensorflow/releases,\nwhere states: To get 5 layers each with their own parameters, write: MultiRNNCell([LSTMCell(...) for _ in range(5)])\nand I also reviewed similar posts regarding same issue: https://github.com/udacity/deep-learning/issues/132\nseq_length = 6\ndata_dim =4\nhidden_dim = 12\nX = tf.placeholder(tf.int32, [None, seq_length])\nY = tf.placeholder(tf.int32, [None]) \nkeep_prob = tf.placeholder(tf.float32) \n\nX_one_hot = tf.one_hot(X, data_dim)\nY_one_hot = tf.one_hot(Y, 2) \n\ndef lstm_cell():\n    lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n    return drop\n\nmulti_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)], state_is_tuple=True)\noutputs1, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n\nWhy does this error happen?\n.\n.\n.\nby the way, would 12 for the number of units in the LSTM cell be too small?", "body": "The following code works well on TF 1.0.1, but doesn't work on TF 1.2 and above, leaving error massage, \r\n\r\nFile \"pb_OE_column_on_the_spot.py\", line 318, in <module>\r\n    outputs1, _states = tf.nn.dynamic_rnn(_multi_cells, tf.one_hot(_X, data_dim), dtype=tf.float32)\r\nValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (256, 512) and found shape (132, 512).\r\n\r\nI followed the change after version 1.2: [https://github.com/tensorflow/tensorflow/releases](url),\r\nwhere states: To get 5 layers each with their own parameters, write: MultiRNNCell([LSTMCell(...) for _ in range(5)])\r\n\r\nand I also reviewed similar posts regarding same issue: [https://github.com/udacity/deep-learning/issues/132](url)\r\n\r\n    seq_length = 6\r\n    data_dim =4\r\n    hidden_dim = 12\r\n    X = tf.placeholder(tf.int32, [None, seq_length])\r\n    Y = tf.placeholder(tf.int32, [None]) \r\n    keep_prob = tf.placeholder(tf.float32) \r\n\r\n    X_one_hot = tf.one_hot(X, data_dim)\r\n    Y_one_hot = tf.one_hot(Y, 2) \r\n\r\n    def lstm_cell():\r\n        lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\r\n        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\r\n        return drop\r\n\r\n    multi_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)], state_is_tuple=True)\r\n    outputs1, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\r\n\r\nWhy does this error happen?\r\n.\r\n.\r\n.\r\nby the way, would 12 for the number of units in the LSTM cell be too small?"}