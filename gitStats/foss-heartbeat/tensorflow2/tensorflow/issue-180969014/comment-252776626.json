{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/252776626", "html_url": "https://github.com/tensorflow/tensorflow/pull/4761#issuecomment-252776626", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4761", "id": 252776626, "node_id": "MDEyOklzc3VlQ29tbWVudDI1Mjc3NjYyNg==", "user": {"login": "ethancaballero", "id": 5994634, "node_id": "MDQ6VXNlcjU5OTQ2MzQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5994634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ethancaballero", "html_url": "https://github.com/ethancaballero", "followers_url": "https://api.github.com/users/ethancaballero/followers", "following_url": "https://api.github.com/users/ethancaballero/following{/other_user}", "gists_url": "https://api.github.com/users/ethancaballero/gists{/gist_id}", "starred_url": "https://api.github.com/users/ethancaballero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ethancaballero/subscriptions", "organizations_url": "https://api.github.com/users/ethancaballero/orgs", "repos_url": "https://api.github.com/users/ethancaballero/repos", "events_url": "https://api.github.com/users/ethancaballero/events{/privacy}", "received_events_url": "https://api.github.com/users/ethancaballero/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-10T23:57:31Z", "updated_at": "2016-10-11T00:53:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12167999\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alrojo\">@alrojo</a> Yes, although I would instead use the Attention based GRU from equation 11 of <a href=\"https://arxiv.org/pdf/1603.01417.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1603.01417.pdf</a> to obtain contextual vector c_t that then updates episode memory m_t\u22121 (via a vanilla GRU) to produce m_t. There's an example tensorflow implementation of Attention based GRU here:<br>\n<a href=\"https://github.com/therne/dmn-tensorflow/blob/master/utils/attn_gru.py\">https://github.com/therne/dmn-tensorflow/blob/master/utils/attn_gru.py</a><br>\nAlso, this method might require a second encoder layer (referred to as input fusion layer in 1603.01417) on top of the normal word_level encoder to work correctly.</p>\n<p>Alternatively, one could use a weighted sum as a soft attention to compute contextual vector c_t during each hop (which is more similar to the current tf.nn.seq2seq api), but this weighted sum method loses positional &amp; ordering information and as a result is significantly less accurate for use cases that require more than 2 hops.</p>", "body_text": "@alrojo Yes, although I would instead use the Attention based GRU from equation 11 of https://arxiv.org/pdf/1603.01417.pdf to obtain contextual vector c_t that then updates episode memory m_t\u22121 (via a vanilla GRU) to produce m_t. There's an example tensorflow implementation of Attention based GRU here:\nhttps://github.com/therne/dmn-tensorflow/blob/master/utils/attn_gru.py\nAlso, this method might require a second encoder layer (referred to as input fusion layer in 1603.01417) on top of the normal word_level encoder to work correctly.\nAlternatively, one could use a weighted sum as a soft attention to compute contextual vector c_t during each hop (which is more similar to the current tf.nn.seq2seq api), but this weighted sum method loses positional & ordering information and as a result is significantly less accurate for use cases that require more than 2 hops.", "body": "@alrojo Yes, although I would instead use the Attention based GRU from equation 11 of https://arxiv.org/pdf/1603.01417.pdf to obtain contextual vector c_t that then updates episode memory m_t\u22121 (via a vanilla GRU) to produce m_t. There's an example tensorflow implementation of Attention based GRU here: \nhttps://github.com/therne/dmn-tensorflow/blob/master/utils/attn_gru.py\nAlso, this method might require a second encoder layer (referred to as input fusion layer in 1603.01417) on top of the normal word_level encoder to work correctly.\n\nAlternatively, one could use a weighted sum as a soft attention to compute contextual vector c_t during each hop (which is more similar to the current tf.nn.seq2seq api), but this weighted sum method loses positional & ordering information and as a result is significantly less accurate for use cases that require more than 2 hops.\n"}