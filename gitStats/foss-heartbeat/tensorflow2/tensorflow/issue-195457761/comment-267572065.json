{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267572065", "html_url": "https://github.com/tensorflow/tensorflow/pull/6299#issuecomment-267572065", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6299", "id": 267572065, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzU3MjA2NQ==", "user": {"login": "AnishShah", "id": 3175743, "node_id": "MDQ6VXNlcjMxNzU3NDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/3175743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnishShah", "html_url": "https://github.com/AnishShah", "followers_url": "https://api.github.com/users/AnishShah/followers", "following_url": "https://api.github.com/users/AnishShah/following{/other_user}", "gists_url": "https://api.github.com/users/AnishShah/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnishShah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnishShah/subscriptions", "organizations_url": "https://api.github.com/users/AnishShah/orgs", "repos_url": "https://api.github.com/users/AnishShah/repos", "events_url": "https://api.github.com/users/AnishShah/events{/privacy}", "received_events_url": "https://api.github.com/users/AnishShah/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-16T11:11:33Z", "updated_at": "2016-12-16T11:11:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=486336\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aam-at\">@aam-at</a> Also, I would like to know what is wrong?<br>\nFor the max pool operation, the gradient is passed to the one who has the maximum value inside the kernel window. So, the second order derivative should be zero. This is <strong>kind of</strong> like ReLUs, isn't it? Can you please explain? It would help me understand better. :)</p>", "body_text": "@aam-at Also, I would like to know what is wrong?\nFor the max pool operation, the gradient is passed to the one who has the maximum value inside the kernel window. So, the second order derivative should be zero. This is kind of like ReLUs, isn't it? Can you please explain? It would help me understand better. :)", "body": "@aam-at Also, I would like to know what is wrong?\r\nFor the max pool operation, the gradient is passed to the one who has the maximum value inside the kernel window. So, the second order derivative should be zero. This is **kind of** like ReLUs, isn't it? Can you please explain? It would help me understand better. :)"}