{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311226240", "html_url": "https://github.com/tensorflow/tensorflow/issues/7089#issuecomment-311226240", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7089", "id": 311226240, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTIyNjI0MA==", "user": {"login": "simaoh", "id": 14142516, "node_id": "MDQ6VXNlcjE0MTQyNTE2", "avatar_url": "https://avatars0.githubusercontent.com/u/14142516?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simaoh", "html_url": "https://github.com/simaoh", "followers_url": "https://api.github.com/users/simaoh/followers", "following_url": "https://api.github.com/users/simaoh/following{/other_user}", "gists_url": "https://api.github.com/users/simaoh/gists{/gist_id}", "starred_url": "https://api.github.com/users/simaoh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simaoh/subscriptions", "organizations_url": "https://api.github.com/users/simaoh/orgs", "repos_url": "https://api.github.com/users/simaoh/repos", "events_url": "https://api.github.com/users/simaoh/events{/privacy}", "received_events_url": "https://api.github.com/users/simaoh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-27T01:23:26Z", "updated_at": "2017-06-27T01:24:25Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38796628\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dandelionmane\">@dandelionmane</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5580724\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/BarzinM\">@BarzinM</a>,  Can we do this inside tensorflow's  tf.contrib.learn.Estimator?<br>\nContrib.learn abstracts the concept of both tf.graph, and tf.session, so we actually cannot run</p>\n<div class=\"highlight highlight-source-python\"><pre>    summary <span class=\"pl-k\">=</span> session.run(write_op, {log_var: random.rand()})\n    writer_1.add_summary(summary, i)</pre></div>\n<p>I have been trying to use  tf.train.SummarySaverHook without success.<br>\nAs an example how can I plot loss and loss2 in the same graph for the example below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">cnn_model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>,<span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">params</span>):\n    \n    <span class=\"pl-c1\">LEARNING_RATE</span> <span class=\"pl-k\">=</span> params[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning_rate<span class=\"pl-pds\">'</span></span>]\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>input</span>\n    \n    input_layer<span class=\"pl-k\">=</span>tf.reshape(features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>],<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">1</span>])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>conv1</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>with tf.variable_scope(\"conv2d\"):</span>\n    conv1<span class=\"pl-k\">=</span>tf.layers.conv2d(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>input_layer,\n                           <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>,\n                           <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>],\n                           <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>,\n                           <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu)\n    \n    pool1<span class=\"pl-k\">=</span>tf.layers.max_pooling2d(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>conv1, <span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>conv2</span>\n    conv2<span class=\"pl-k\">=</span>tf.layers.conv2d(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>pool1,\n                           <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n                           <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>],\n                           <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>,\n                           <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>my_conv2<span class=\"pl-pds\">'</span></span>)\n    pool2<span class=\"pl-k\">=</span>tf.layers.max_pooling2d(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>conv2, <span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>fc1</span>\n    flat<span class=\"pl-k\">=</span>tf.reshape(pool2, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">7</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">7</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">64</span>])\n    \n    batch_norm<span class=\"pl-k\">=</span>tf.contrib.layers.batch_norm(flat, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_norm<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>trainable</span>\n    dense1<span class=\"pl-k\">=</span>tf.layers.dense(batch_norm, <span class=\"pl-c1\">1024</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu)\n    \n    dropout <span class=\"pl-k\">=</span> tf.layers.dropout(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>dense1, <span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.4</span>, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>mode <span class=\"pl-k\">==</span> learn.ModeKeys.<span class=\"pl-c1\">TRAIN</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>fc2</span>\n    logits<span class=\"pl-k\">=</span>tf.layers.dense(dropout, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu)\n    loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    train_op <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    \n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>loss</span>\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">!=</span> learn.ModeKeys.<span class=\"pl-c1\">INFER</span>:\n        onehot_labels<span class=\"pl-k\">=</span>tf.one_hot(<span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>tf.cast(labels, tf.int32),<span class=\"pl-v\">depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n        loss<span class=\"pl-k\">=</span>tf.losses.softmax_cross_entropy(<span class=\"pl-v\">onehot_labels</span><span class=\"pl-k\">=</span>onehot_labels, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n        loss2<span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(<span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>onehot_labels, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>optimizer</span>\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> learn.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        train_op <span class=\"pl-k\">=</span> tf.contrib.layers.optimize_loss(\n                        <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss,\n                        <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.contrib.framework.get_global_step(),\n                        <span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LEARNING_RATE</span>,\n                        <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Adam<span class=\"pl-pds\">\"</span></span>)\n        \n    predictions<span class=\"pl-k\">=</span>{\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>classes<span class=\"pl-pds\">'</span></span>: tf.argmax(logits, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>) ,\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>predictions<span class=\"pl-pds\">'</span></span>: tf.nn.softmax(logits,<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax_tensor<span class=\"pl-pds\">\"</span></span>)           \n    }\n    \n    <span class=\"pl-k\">return</span> model_fn_lib.ModelFnOps(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss, <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op)\n\n\noriginal_model_dir<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/tmp/bnorm/<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Instantiate an estimator</span>\n<span class=\"pl-c1\">LEARNING_RATE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0001</span>\nmy_params <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning_rate<span class=\"pl-pds\">'</span></span>:<span class=\"pl-c1\">LEARNING_RATE</span>}\n\n<span class=\"pl-k\">if</span> os.path.isdir(original_model_dir):\n    shutil.rmtree(original_model_dir)\nrunConfig <span class=\"pl-k\">=</span> learn.RunConfig(<span class=\"pl-v\">save_summary_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>, <span class=\"pl-v\">save_checkpoints_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, \n                            <span class=\"pl-v\">save_checkpoints_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\nclassifier<span class=\"pl-k\">=</span>learn.Estimator(<span class=\"pl-v\">model_fn</span><span class=\"pl-k\">=</span>cnn_model_fn, <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span>original_model_dir,<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>runConfig, <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span> my_params)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Configure the accuracy metric for evaluation</span>\nmetrics <span class=\"pl-k\">=</span> {\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>:\n      learn.MetricSpec(\n          <span class=\"pl-v\">metric_fn</span><span class=\"pl-k\">=</span>tf.metrics.accuracy, <span class=\"pl-v\">prediction_key</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>classes<span class=\"pl-pds\">\"</span></span>),\n}\n\nexperiment<span class=\"pl-k\">=</span>learn.Experiment(<span class=\"pl-v\">estimator</span><span class=\"pl-k\">=</span>classifier, \n                <span class=\"pl-v\">train_input_fn</span><span class=\"pl-k\">=</span>train_input_fn,\n                <span class=\"pl-v\">eval_input_fn</span><span class=\"pl-k\">=</span>eval_input_fn,\n                <span class=\"pl-v\">eval_metrics</span><span class=\"pl-k\">=</span>metrics,\n                <span class=\"pl-v\">train_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>)\nexperiment.train_and_evaluate()</pre></div>", "body_text": "@dandelionmane @BarzinM,  Can we do this inside tensorflow's  tf.contrib.learn.Estimator?\nContrib.learn abstracts the concept of both tf.graph, and tf.session, so we actually cannot run\n    summary = session.run(write_op, {log_var: random.rand()})\n    writer_1.add_summary(summary, i)\nI have been trying to use  tf.train.SummarySaverHook without success.\nAs an example how can I plot loss and loss2 in the same graph for the example below:\ndef cnn_model_fn(features, labels,mode, params):\n    \n    LEARNING_RATE = params['learning_rate']\n    \n    #input\n    \n    input_layer=tf.reshape(features['x'],shape=[-1,28,28,1])\n\n    #conv1\n    #with tf.variable_scope(\"conv2d\"):\n    conv1=tf.layers.conv2d(inputs=input_layer,\n                           filters=32,\n                           kernel_size=[5, 5],\n                           padding='same',\n                           activation=tf.nn.relu)\n    \n    pool1=tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2)\n\n    #conv2\n    conv2=tf.layers.conv2d(inputs=pool1,\n                           filters=64,\n                           kernel_size=[5,5],\n                           padding='same',\n                           activation=tf.nn.relu, name='my_conv2')\n    pool2=tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\n\n    #fc1\n    flat=tf.reshape(pool2, [-1, 7*7*64])\n    \n    batch_norm=tf.contrib.layers.batch_norm(flat, scale=True, scope='batch_norm')\n    #trainable\n    dense1=tf.layers.dense(batch_norm, 1024, activation=tf.nn.relu)\n    \n    dropout = tf.layers.dropout(inputs=dense1, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n\n    #fc2\n    logits=tf.layers.dense(dropout, 10, activation=tf.nn.relu)\n    loss = None\n    train_op = None\n    \n    \n    #loss\n    if mode != learn.ModeKeys.INFER:\n        onehot_labels=tf.one_hot(indices=tf.cast(labels, tf.int32),depth=10)\n        loss=tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n        loss2= tf.losses.sigmoid_cross_entropy(multi_class_labels=onehot_labels, logits=logits)\n    \n    #optimizer\n    if mode == learn.ModeKeys.TRAIN:\n        train_op = tf.contrib.layers.optimize_loss(\n                        loss=loss,\n                        global_step=tf.contrib.framework.get_global_step(),\n                        learning_rate=LEARNING_RATE,\n                        optimizer=\"Adam\")\n        \n    predictions={\n            'classes': tf.argmax(logits, axis=1) ,\n            'predictions': tf.nn.softmax(logits,name=\"softmax_tensor\")           \n    }\n    \n    return model_fn_lib.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n\n\noriginal_model_dir=\"/tmp/bnorm/\"\n\n#Instantiate an estimator\nLEARNING_RATE = 0.0001\nmy_params = {'learning_rate':LEARNING_RATE}\n\nif os.path.isdir(original_model_dir):\n    shutil.rmtree(original_model_dir)\nrunConfig = learn.RunConfig(save_summary_steps=20, save_checkpoints_steps=100, \n                            save_checkpoints_secs=None)\nclassifier=learn.Estimator(model_fn=cnn_model_fn, model_dir=original_model_dir,config=runConfig, params= my_params)\n\n# Configure the accuracy metric for evaluation\nmetrics = {\n  \"accuracy\":\n      learn.MetricSpec(\n          metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),\n}\n\nexperiment=learn.Experiment(estimator=classifier, \n                train_input_fn=train_input_fn,\n                eval_input_fn=eval_input_fn,\n                eval_metrics=metrics,\n                train_steps=200)\nexperiment.train_and_evaluate()", "body": "@dandelionmane @BarzinM,  Can we do this inside tensorflow's  tf.contrib.learn.Estimator?\r\nContrib.learn abstracts the concept of both tf.graph, and tf.session, so we actually cannot run\r\n```python\r\n    summary = session.run(write_op, {log_var: random.rand()})\r\n    writer_1.add_summary(summary, i)\r\n```\r\n\r\nI have been trying to use  tf.train.SummarySaverHook without success.\r\nAs an example how can I plot loss and loss2 in the same graph for the example below:\r\n\r\n```python\r\ndef cnn_model_fn(features, labels,mode, params):\r\n    \r\n    LEARNING_RATE = params['learning_rate']\r\n    \r\n    #input\r\n    \r\n    input_layer=tf.reshape(features['x'],shape=[-1,28,28,1])\r\n\r\n    #conv1\r\n    #with tf.variable_scope(\"conv2d\"):\r\n    conv1=tf.layers.conv2d(inputs=input_layer,\r\n                           filters=32,\r\n                           kernel_size=[5, 5],\r\n                           padding='same',\r\n                           activation=tf.nn.relu)\r\n    \r\n    pool1=tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2)\r\n\r\n    #conv2\r\n    conv2=tf.layers.conv2d(inputs=pool1,\r\n                           filters=64,\r\n                           kernel_size=[5,5],\r\n                           padding='same',\r\n                           activation=tf.nn.relu, name='my_conv2')\r\n    pool2=tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\r\n\r\n    #fc1\r\n    flat=tf.reshape(pool2, [-1, 7*7*64])\r\n    \r\n    batch_norm=tf.contrib.layers.batch_norm(flat, scale=True, scope='batch_norm')\r\n    #trainable\r\n    dense1=tf.layers.dense(batch_norm, 1024, activation=tf.nn.relu)\r\n    \r\n    dropout = tf.layers.dropout(inputs=dense1, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\r\n\r\n    #fc2\r\n    logits=tf.layers.dense(dropout, 10, activation=tf.nn.relu)\r\n    loss = None\r\n    train_op = None\r\n    \r\n    \r\n    #loss\r\n    if mode != learn.ModeKeys.INFER:\r\n        onehot_labels=tf.one_hot(indices=tf.cast(labels, tf.int32),depth=10)\r\n        loss=tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\r\n        loss2= tf.losses.sigmoid_cross_entropy(multi_class_labels=onehot_labels, logits=logits)\r\n    \r\n    #optimizer\r\n    if mode == learn.ModeKeys.TRAIN:\r\n        train_op = tf.contrib.layers.optimize_loss(\r\n                        loss=loss,\r\n                        global_step=tf.contrib.framework.get_global_step(),\r\n                        learning_rate=LEARNING_RATE,\r\n                        optimizer=\"Adam\")\r\n        \r\n    predictions={\r\n            'classes': tf.argmax(logits, axis=1) ,\r\n            'predictions': tf.nn.softmax(logits,name=\"softmax_tensor\")           \r\n    }\r\n    \r\n    return model_fn_lib.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)\r\n\r\n\r\noriginal_model_dir=\"/tmp/bnorm/\"\r\n\r\n#Instantiate an estimator\r\nLEARNING_RATE = 0.0001\r\nmy_params = {'learning_rate':LEARNING_RATE}\r\n\r\nif os.path.isdir(original_model_dir):\r\n    shutil.rmtree(original_model_dir)\r\nrunConfig = learn.RunConfig(save_summary_steps=20, save_checkpoints_steps=100, \r\n                            save_checkpoints_secs=None)\r\nclassifier=learn.Estimator(model_fn=cnn_model_fn, model_dir=original_model_dir,config=runConfig, params= my_params)\r\n\r\n# Configure the accuracy metric for evaluation\r\nmetrics = {\r\n  \"accuracy\":\r\n      learn.MetricSpec(\r\n          metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),\r\n}\r\n\r\nexperiment=learn.Experiment(estimator=classifier, \r\n                train_input_fn=train_input_fn,\r\n                eval_input_fn=eval_input_fn,\r\n                eval_metrics=metrics,\r\n                train_steps=200)\r\nexperiment.train_and_evaluate()\r\n```\r\n\r\n"}