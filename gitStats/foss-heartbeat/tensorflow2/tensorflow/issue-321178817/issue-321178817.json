{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19145", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19145/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19145/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19145/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19145", "id": 321178817, "node_id": "MDU6SXNzdWUzMjExNzg4MTc=", "number": 19145, "title": "Linalg.LinearOperators do not give any performance improvement", "user": {"login": "sanket-kamthe", "id": 15108821, "node_id": "MDQ6VXNlcjE1MTA4ODIx", "avatar_url": "https://avatars2.githubusercontent.com/u/15108821?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanket-kamthe", "html_url": "https://github.com/sanket-kamthe", "followers_url": "https://api.github.com/users/sanket-kamthe/followers", "following_url": "https://api.github.com/users/sanket-kamthe/following{/other_user}", "gists_url": "https://api.github.com/users/sanket-kamthe/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanket-kamthe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanket-kamthe/subscriptions", "organizations_url": "https://api.github.com/users/sanket-kamthe/orgs", "repos_url": "https://api.github.com/users/sanket-kamthe/repos", "events_url": "https://api.github.com/users/sanket-kamthe/events{/privacy}", "received_events_url": "https://api.github.com/users/sanket-kamthe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-05-08T13:07:45Z", "updated_at": "2018-05-16T11:28:09Z", "closed_at": "2018-05-16T11:28:08Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>The performance gaurantees are not visible for the Linalg.LinearOperators (e.g., <a href=\"https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorDiag\" rel=\"nofollow\">DiagOperator</a>)   I have implemented a basic case below as per the documentation. Is there any reason why the performance of <code>diag</code> operator is same as full matrix ?</p>\n<p>Cuda and other versions not included as the issue is reproducible across GPU and CPU modes.</p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n</ul>\n<p>Yes, but using only preliminary Linalg operations.</p>\n<ul>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux xxxx-xxxop 4.10.0-28-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115947948\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/32\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/32/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/32\">#32</a>~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux<br>\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"<br>\nVERSION_ID=\"16.04\"<br>\nVERSION_CODENAME=xenial</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBinary</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\n== tensorflow import ============================================<br>\ntf.VERSION = 1.8.0<br>\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072<br>\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072<br>\nSanity check: array([1], dtype=int32)</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00)</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\nCheck the source code.</p>\n</li>\n</ul>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nrng <span class=\"pl-k\">=</span> np.random.RandomState(<span class=\"pl-c1\">1</span>)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Data</span>:\n    num_data <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    num_ind <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\n    D_in <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n    D_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n\n    Xmu <span class=\"pl-k\">=</span> rng.randn(num_data, D_in)\n    Xcov <span class=\"pl-k\">=</span> rng.randn(num_data, D_in, D_in)\n    Xcov <span class=\"pl-k\">=</span> Xcov <span class=\"pl-k\">@</span> np.transpose(Xcov, (<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>))\n    Z <span class=\"pl-k\">=</span> rng.randn(num_ind, D_in)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre>N <span class=\"pl-k\">=</span> Data.num_data\nXmu <span class=\"pl-k\">=</span> tf.convert_to_tensor(Data.Xmu)\nXcov <span class=\"pl-k\">=</span> tf.convert_to_tensor(Data.Xcov)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre>C <span class=\"pl-k\">=</span> tf.cholesky(Xcov)\n\nZ_tiled <span class=\"pl-k\">=</span> tf.tile(tf.expand_dims(tf.transpose(Data.Z), <span class=\"pl-c1\">0</span>), [N, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>])</pre></div>\n<div class=\"highlight highlight-source-python\"><pre>C_operator <span class=\"pl-k\">=</span> tf.linalg.LinearOperatorLowerTriangular(C)\nC_diag_operator <span class=\"pl-k\">=</span> tf.linalg.LinearOperatorDiag(tf.matrix_diag_part(C))</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">compute_matrix_solve</span>(<span class=\"pl-smi\">C</span>, <span class=\"pl-smi\">Z_tiled</span>):\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>     return tf.matrix_triangular_solve(C, Z_tiled, lower=True)  # NxDxM</span>\n    <span class=\"pl-k\">return</span> tf.matrix_solve(C, Z_tiled)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> NxDxM</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">compute_matrix_matmul</span>(<span class=\"pl-smi\">C</span>, <span class=\"pl-smi\">Z_tiled</span>):\n    mat <span class=\"pl-k\">=</span> tf.matmul(C, Z_tiled)\n    <span class=\"pl-k\">return</span> mat \n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">compute_operator_solve</span>(<span class=\"pl-smi\">C_operator</span>, <span class=\"pl-smi\">Z_tiled</span>):\n    operator_value <span class=\"pl-k\">=</span> C_operator.solve(Z_tiled)\n    <span class=\"pl-k\">return</span> operator_value\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">compute_operator_matmul</span>(<span class=\"pl-smi\">C_operator</span>, <span class=\"pl-smi\">Z_tiled</span>):\n    operator_value <span class=\"pl-k\">=</span> C_operator.matmul(Z_tiled)\n    <span class=\"pl-k\">return</span> operator_value</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    mat_solve <span class=\"pl-k\">=</span> sess.run(compute_matrix_solve(C, Z_tiled))\n    op_solve <span class=\"pl-k\">=</span> sess.run(compute_operator_solve(C_operator, Z_tiled))\n    \n    tf_mat_mul <span class=\"pl-k\">=</span> sess.run(compute_matrix_matmul(C, Z_tiled))\n    op_matmul <span class=\"pl-k\">=</span> sess.run(compute_operator_matmul(C_operator, Z_tiled))\n\nnp.testing.assert_allclose(mat_solve, op_solve)\nnp.testing.assert_allclose(tf_mat_mul, op_matmul)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-k\">%</span>timeit sess.run(compute_matrix_solve(C, Z_tiled))\n    <span class=\"pl-k\">%</span>timeit sess.run(compute_operator_solve(C_operator, Z_tiled))\n    <span class=\"pl-k\">%</span>timeit sess.run(compute_operator_solve(C_diag_operator, Z_tiled))</pre></div>\n<pre><code>69.1 ms \u00b1 1.33 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n69.4 ms \u00b1 515 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n73.7 ms \u00b1 1.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-k\">%</span>timeit sess.run(compute_matrix_matmul(C, Z_tiled))\n    <span class=\"pl-k\">%</span>timeit sess.run(compute_operator_matmul(C_operator, Z_tiled))\n    <span class=\"pl-k\">%</span>timeit sess.run(compute_operator_matmul(C_diag_operator, Z_tiled))\n    </pre></div>\n<pre><code>58.5 ms \u00b1 447 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n60.8 ms \u00b1 808 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n63.8 ms \u00b1 1.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre>", "body_text": "Describe the problem\nThe performance gaurantees are not visible for the Linalg.LinearOperators (e.g., DiagOperator)   I have implemented a basic case below as per the documentation. Is there any reason why the performance of diag operator is same as full matrix ?\nCuda and other versions not included as the issue is reproducible across GPU and CPU modes.\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nYes, but using only preliminary Linalg operations.\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux xxxx-xxxop 4.10.0-28-generic #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n\nTensorFlow installed from (source or binary):\nBinary\n\n\nTensorFlow version (use command below):\n== tensorflow import ============================================\ntf.VERSION = 1.8.0\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\nSanity check: array([1], dtype=int32)\n\n\nPython version:\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00)\n\n\nBazel version (if compiling from source):\n\n\nGCC/Compiler version (if compiling from source):\n\n\nCUDA/cuDNN version:\n\n\nGPU model and memory:\n\n\nExact command to reproduce:\nCheck the source code.\n\n\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nrng = np.random.RandomState(1)\nclass Data:\n    num_data = 10\n    num_ind = 50\n    D_in = 100\n    D_out = 2\n\n    Xmu = rng.randn(num_data, D_in)\n    Xcov = rng.randn(num_data, D_in, D_in)\n    Xcov = Xcov @ np.transpose(Xcov, (0, 2, 1))\n    Z = rng.randn(num_ind, D_in)\nN = Data.num_data\nXmu = tf.convert_to_tensor(Data.Xmu)\nXcov = tf.convert_to_tensor(Data.Xcov)\nC = tf.cholesky(Xcov)\n\nZ_tiled = tf.tile(tf.expand_dims(tf.transpose(Data.Z), 0), [N, 1, 1])\nC_operator = tf.linalg.LinearOperatorLowerTriangular(C)\nC_diag_operator = tf.linalg.LinearOperatorDiag(tf.matrix_diag_part(C))\ndef compute_matrix_solve(C, Z_tiled):\n#     return tf.matrix_triangular_solve(C, Z_tiled, lower=True)  # NxDxM\n    return tf.matrix_solve(C, Z_tiled)  # NxDxM\n\ndef compute_matrix_matmul(C, Z_tiled):\n    mat = tf.matmul(C, Z_tiled)\n    return mat \n\ndef compute_operator_solve(C_operator, Z_tiled):\n    operator_value = C_operator.solve(Z_tiled)\n    return operator_value\n\ndef compute_operator_matmul(C_operator, Z_tiled):\n    operator_value = C_operator.matmul(Z_tiled)\n    return operator_value\nwith tf.Session() as sess:\n    mat_solve = sess.run(compute_matrix_solve(C, Z_tiled))\n    op_solve = sess.run(compute_operator_solve(C_operator, Z_tiled))\n    \n    tf_mat_mul = sess.run(compute_matrix_matmul(C, Z_tiled))\n    op_matmul = sess.run(compute_operator_matmul(C_operator, Z_tiled))\n\nnp.testing.assert_allclose(mat_solve, op_solve)\nnp.testing.assert_allclose(tf_mat_mul, op_matmul)\nwith tf.Session() as sess:\n    %timeit sess.run(compute_matrix_solve(C, Z_tiled))\n    %timeit sess.run(compute_operator_solve(C_operator, Z_tiled))\n    %timeit sess.run(compute_operator_solve(C_diag_operator, Z_tiled))\n69.1 ms \u00b1 1.33 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n69.4 ms \u00b1 515 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n73.7 ms \u00b1 1.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nwith tf.Session() as sess:\n    %timeit sess.run(compute_matrix_matmul(C, Z_tiled))\n    %timeit sess.run(compute_operator_matmul(C_operator, Z_tiled))\n    %timeit sess.run(compute_operator_matmul(C_diag_operator, Z_tiled))\n    \n58.5 ms \u00b1 447 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n60.8 ms \u00b1 808 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n63.8 ms \u00b1 1.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)", "body": "### Describe the problem\r\n\r\nThe performance gaurantees are not visible for the Linalg.LinearOperators (e.g., [DiagOperator](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorDiag))   I have implemented a basic case below as per the documentation. Is there any reason why the performance of `diag` operator is same as full matrix ? \r\n\r\nCuda and other versions not included as the issue is reproducible across GPU and CPU modes.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes, but using only preliminary Linalg operations. \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux xxxx-xxxop 4.10.0-28-generic #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n\r\n- **Python version**: \r\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) \r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nCheck the source code.\r\n\r\n\r\n### Source code / logs\r\n\r\n\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nrng = np.random.RandomState(1)\r\n```\r\n```python\r\nclass Data:\r\n    num_data = 10\r\n    num_ind = 50\r\n    D_in = 100\r\n    D_out = 2\r\n\r\n    Xmu = rng.randn(num_data, D_in)\r\n    Xcov = rng.randn(num_data, D_in, D_in)\r\n    Xcov = Xcov @ np.transpose(Xcov, (0, 2, 1))\r\n    Z = rng.randn(num_ind, D_in)\r\n```\r\n\r\n\r\n```python\r\nN = Data.num_data\r\nXmu = tf.convert_to_tensor(Data.Xmu)\r\nXcov = tf.convert_to_tensor(Data.Xcov)\r\n```\r\n```python\r\nC = tf.cholesky(Xcov)\r\n\r\nZ_tiled = tf.tile(tf.expand_dims(tf.transpose(Data.Z), 0), [N, 1, 1])\r\n```\r\n```python\r\nC_operator = tf.linalg.LinearOperatorLowerTriangular(C)\r\nC_diag_operator = tf.linalg.LinearOperatorDiag(tf.matrix_diag_part(C))\r\n```\r\n\r\n```python\r\ndef compute_matrix_solve(C, Z_tiled):\r\n#     return tf.matrix_triangular_solve(C, Z_tiled, lower=True)  # NxDxM\r\n    return tf.matrix_solve(C, Z_tiled)  # NxDxM\r\n\r\ndef compute_matrix_matmul(C, Z_tiled):\r\n    mat = tf.matmul(C, Z_tiled)\r\n    return mat \r\n\r\ndef compute_operator_solve(C_operator, Z_tiled):\r\n    operator_value = C_operator.solve(Z_tiled)\r\n    return operator_value\r\n\r\ndef compute_operator_matmul(C_operator, Z_tiled):\r\n    operator_value = C_operator.matmul(Z_tiled)\r\n    return operator_value\r\n```\r\n\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n    mat_solve = sess.run(compute_matrix_solve(C, Z_tiled))\r\n    op_solve = sess.run(compute_operator_solve(C_operator, Z_tiled))\r\n    \r\n    tf_mat_mul = sess.run(compute_matrix_matmul(C, Z_tiled))\r\n    op_matmul = sess.run(compute_operator_matmul(C_operator, Z_tiled))\r\n\r\nnp.testing.assert_allclose(mat_solve, op_solve)\r\nnp.testing.assert_allclose(tf_mat_mul, op_matmul)\r\n```\r\n\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n    %timeit sess.run(compute_matrix_solve(C, Z_tiled))\r\n    %timeit sess.run(compute_operator_solve(C_operator, Z_tiled))\r\n    %timeit sess.run(compute_operator_solve(C_diag_operator, Z_tiled))\r\n```\r\n\r\n    69.1 ms \u00b1 1.33 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    69.4 ms \u00b1 515 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    73.7 ms \u00b1 1.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n    %timeit sess.run(compute_matrix_matmul(C, Z_tiled))\r\n    %timeit sess.run(compute_operator_matmul(C_operator, Z_tiled))\r\n    %timeit sess.run(compute_operator_matmul(C_diag_operator, Z_tiled))\r\n    \r\n```\r\n\r\n    58.5 ms \u00b1 447 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    60.8 ms \u00b1 808 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    63.8 ms \u00b1 1.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n\r\n\r\n\r\n"}