{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14353", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14353/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14353/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14353/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14353", "id": 272109585, "node_id": "MDU6SXNzdWUyNzIxMDk1ODU=", "number": 14353, "title": "variance goes negative when set layers.batch_norm reuse=True", "user": {"login": "coderXiangLi", "id": 8342490, "node_id": "MDQ6VXNlcjgzNDI0OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8342490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/coderXiangLi", "html_url": "https://github.com/coderXiangLi", "followers_url": "https://api.github.com/users/coderXiangLi/followers", "following_url": "https://api.github.com/users/coderXiangLi/following{/other_user}", "gists_url": "https://api.github.com/users/coderXiangLi/gists{/gist_id}", "starred_url": "https://api.github.com/users/coderXiangLi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/coderXiangLi/subscriptions", "organizations_url": "https://api.github.com/users/coderXiangLi/orgs", "repos_url": "https://api.github.com/users/coderXiangLi/repos", "events_url": "https://api.github.com/users/coderXiangLi/events{/privacy}", "received_events_url": "https://api.github.com/users/coderXiangLi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "joel-shor", "id": 6020988, "node_id": "MDQ6VXNlcjYwMjA5ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6020988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joel-shor", "html_url": "https://github.com/joel-shor", "followers_url": "https://api.github.com/users/joel-shor/followers", "following_url": "https://api.github.com/users/joel-shor/following{/other_user}", "gists_url": "https://api.github.com/users/joel-shor/gists{/gist_id}", "starred_url": "https://api.github.com/users/joel-shor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joel-shor/subscriptions", "organizations_url": "https://api.github.com/users/joel-shor/orgs", "repos_url": "https://api.github.com/users/joel-shor/repos", "events_url": "https://api.github.com/users/joel-shor/events{/privacy}", "received_events_url": "https://api.github.com/users/joel-shor/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "joel-shor", "id": 6020988, "node_id": "MDQ6VXNlcjYwMjA5ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6020988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joel-shor", "html_url": "https://github.com/joel-shor", "followers_url": "https://api.github.com/users/joel-shor/followers", "following_url": "https://api.github.com/users/joel-shor/following{/other_user}", "gists_url": "https://api.github.com/users/joel-shor/gists{/gist_id}", "starred_url": "https://api.github.com/users/joel-shor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joel-shor/subscriptions", "organizations_url": "https://api.github.com/users/joel-shor/orgs", "repos_url": "https://api.github.com/users/joel-shor/repos", "events_url": "https://api.github.com/users/joel-shor/events{/privacy}", "received_events_url": "https://api.github.com/users/joel-shor/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2017-11-08T08:21:05Z", "updated_at": "2018-03-29T21:13:46Z", "closed_at": "2018-03-29T21:13:32Z", "author_association": "NONE", "body_html": "<h4>TF 1.2.1 running on cpu &amp; distributed version</h4>\n<h4>layers.batch_norm set reuse=True</h4>\n<p>I use a sequence of items features which contains n (feature length) * N (sequence length) real-value features. And do batch_norm on each item of the sequence, then do some full_connected, etc. Finally concat them as dnn input.<br>\nHere is a simple code of the this. In order to make variance quickly go negative, I set decay=0.6.</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib import layers\nimport numpy as np\n\nseq_length = 1000\nbatch = 100\nlength = 3\nplace_holders = []\nseq_raw_f = []\nfor i in range(seq_length):\n  x = True if i != 0 else None\n  sequence_place_holder = tf.placeholder(dtype=tf.float32, shape=[None, length])\n  place_holders.append(sequence_place_holder)\n  sequence = layers.batch_norm(inputs=sequence_place_holder, scope=\"bn\", reuse=x, decay=0.6, scale=True,\n                               # updates_collections=None,\n                               zero_debias_moving_mean=True)\n  seq_raw_f.append(sequence)\n\ninput = tf.concat(seq_raw_f, axis=0)\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nplace_holder = {}\nfor i in range(seq_length):\n  place_holder[place_holders[i]] = np.random.rand(batch, length)\n\nfor i in range(10000):\n  sess.run(update_ops, place_holder)\n  # sess.run(input, place_holder)\n  print sess.run(\"bn/moving_variance:0\")\n</code></pre>\n<h4>variance can be :</h4>\n<pre><code>[ 0.08063364  0.08680637  0.08229597]\n[ 0.09141719  0.08313672  0.08208766]\n[ 0.07279671  0.08088712  0.08174741]\n[-0.1192904  -0.13598624 -0.31083935]\n[ 0.24966593  0.26548591  0.16420737]\n[ 0.07931737  0.07920683  0.0833094 ]\n[ 0.08559028  0.08299339  0.08146621]\n[ 0.08117087  0.08168832  0.08265808]\n</code></pre>\n<h4>Reason probably the code below.</h4>\n<p>When reuse=True, this code will subtract update_delta many times according to reuse times. Then the formula goes wrong. <code>a * m_v - (1-a) v -&gt; a * m_v -N * (1-a) v.</code></p>\n<pre><code>update_delta = (variable - value) * decay\nreturn state_ops.assign_sub(variable, update_delta, name=scope)\n</code></pre>\n<p>when, updates_collections=None,  to force update variance, it still happened.</p>\n<h4>temporary fix</h4>\n<pre><code> if not zero_debias:\n   # variable * decay + value * (1 - decay)\n   state_ops.assign(variable, variable * decay + value * (1 - decay))\n</code></pre>", "body_text": "TF 1.2.1 running on cpu & distributed version\nlayers.batch_norm set reuse=True\nI use a sequence of items features which contains n (feature length) * N (sequence length) real-value features. And do batch_norm on each item of the sequence, then do some full_connected, etc. Finally concat them as dnn input.\nHere is a simple code of the this. In order to make variance quickly go negative, I set decay=0.6.\nimport tensorflow as tf\nfrom tensorflow.contrib import layers\nimport numpy as np\n\nseq_length = 1000\nbatch = 100\nlength = 3\nplace_holders = []\nseq_raw_f = []\nfor i in range(seq_length):\n  x = True if i != 0 else None\n  sequence_place_holder = tf.placeholder(dtype=tf.float32, shape=[None, length])\n  place_holders.append(sequence_place_holder)\n  sequence = layers.batch_norm(inputs=sequence_place_holder, scope=\"bn\", reuse=x, decay=0.6, scale=True,\n                               # updates_collections=None,\n                               zero_debias_moving_mean=True)\n  seq_raw_f.append(sequence)\n\ninput = tf.concat(seq_raw_f, axis=0)\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nplace_holder = {}\nfor i in range(seq_length):\n  place_holder[place_holders[i]] = np.random.rand(batch, length)\n\nfor i in range(10000):\n  sess.run(update_ops, place_holder)\n  # sess.run(input, place_holder)\n  print sess.run(\"bn/moving_variance:0\")\n\nvariance can be :\n[ 0.08063364  0.08680637  0.08229597]\n[ 0.09141719  0.08313672  0.08208766]\n[ 0.07279671  0.08088712  0.08174741]\n[-0.1192904  -0.13598624 -0.31083935]\n[ 0.24966593  0.26548591  0.16420737]\n[ 0.07931737  0.07920683  0.0833094 ]\n[ 0.08559028  0.08299339  0.08146621]\n[ 0.08117087  0.08168832  0.08265808]\n\nReason probably the code below.\nWhen reuse=True, this code will subtract update_delta many times according to reuse times. Then the formula goes wrong. a * m_v - (1-a) v -> a * m_v -N * (1-a) v.\nupdate_delta = (variable - value) * decay\nreturn state_ops.assign_sub(variable, update_delta, name=scope)\n\nwhen, updates_collections=None,  to force update variance, it still happened.\ntemporary fix\n if not zero_debias:\n   # variable * decay + value * (1 - decay)\n   state_ops.assign(variable, variable * decay + value * (1 - decay))", "body": "#### TF 1.2.1 running on cpu & distributed version\r\n\r\n#### layers.batch_norm set reuse=True\r\nI use a sequence of items features which contains n (feature length) * N (sequence length) real-value features. And do batch_norm on each item of the sequence, then do some full_connected, etc. Finally concat them as dnn input.\r\nHere is a simple code of the this. In order to make variance quickly go negative, I set decay=0.6.\r\n\r\n\timport tensorflow as tf\r\n\tfrom tensorflow.contrib import layers\r\n\timport numpy as np\r\n\t\r\n\tseq_length = 1000\r\n\tbatch = 100\r\n\tlength = 3\r\n\tplace_holders = []\r\n\tseq_raw_f = []\r\n\tfor i in range(seq_length):\r\n\t  x = True if i != 0 else None\r\n\t  sequence_place_holder = tf.placeholder(dtype=tf.float32, shape=[None, length])\r\n\t  place_holders.append(sequence_place_holder)\r\n\t  sequence = layers.batch_norm(inputs=sequence_place_holder, scope=\"bn\", reuse=x, decay=0.6, scale=True,\r\n\t                               # updates_collections=None,\r\n\t                               zero_debias_moving_mean=True)\r\n\t  seq_raw_f.append(sequence)\r\n\t\r\n\tinput = tf.concat(seq_raw_f, axis=0)\r\n\t\r\n\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\r\n\tsess = tf.Session()\r\n\tsess.run(tf.initialize_all_variables())\r\n\t\r\n\tplace_holder = {}\r\n\tfor i in range(seq_length):\r\n\t  place_holder[place_holders[i]] = np.random.rand(batch, length)\r\n\t\r\n\tfor i in range(10000):\r\n\t  sess.run(update_ops, place_holder)\r\n\t  # sess.run(input, place_holder)\r\n\t  print sess.run(\"bn/moving_variance:0\")\r\n\t  \r\n#### variance can be :\r\n\r\n\t[ 0.08063364  0.08680637  0.08229597]\r\n\t[ 0.09141719  0.08313672  0.08208766]\r\n\t[ 0.07279671  0.08088712  0.08174741]\r\n\t[-0.1192904  -0.13598624 -0.31083935]\r\n\t[ 0.24966593  0.26548591  0.16420737]\r\n\t[ 0.07931737  0.07920683  0.0833094 ]\r\n\t[ 0.08559028  0.08299339  0.08146621]\r\n\t[ 0.08117087  0.08168832  0.08265808]\r\n\t\r\n\r\n#### Reason probably the code below. \r\nWhen reuse=True, this code will subtract update_delta many times according to reuse times. Then the formula goes wrong. `a * m_v - (1-a) v -> a * m_v -N * (1-a) v.`\r\n\t\r\n\tupdate_delta = (variable - value) * decay\r\n\treturn state_ops.assign_sub(variable, update_delta, name=scope)\r\n\t\r\nwhen, updates_collections=None,  to force update variance, it still happened.\r\n\r\n#### temporary fix\r\n     \r\n     if not zero_debias:\r\n       # variable * decay + value * (1 - decay)\r\n       state_ops.assign(variable, variable * decay + value * (1 - decay))\r\n\r\n\t\r\n\r\n"}