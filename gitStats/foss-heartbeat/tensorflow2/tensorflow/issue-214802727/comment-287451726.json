{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287451726", "html_url": "https://github.com/tensorflow/tensorflow/issues/8478#issuecomment-287451726", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8478", "id": 287451726, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NzQ1MTcyNg==", "user": {"login": "MaxGubin", "id": 2595108, "node_id": "MDQ6VXNlcjI1OTUxMDg=", "avatar_url": "https://avatars0.githubusercontent.com/u/2595108?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MaxGubin", "html_url": "https://github.com/MaxGubin", "followers_url": "https://api.github.com/users/MaxGubin/followers", "following_url": "https://api.github.com/users/MaxGubin/following{/other_user}", "gists_url": "https://api.github.com/users/MaxGubin/gists{/gist_id}", "starred_url": "https://api.github.com/users/MaxGubin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MaxGubin/subscriptions", "organizations_url": "https://api.github.com/users/MaxGubin/orgs", "repos_url": "https://api.github.com/users/MaxGubin/repos", "events_url": "https://api.github.com/users/MaxGubin/events{/privacy}", "received_events_url": "https://api.github.com/users/MaxGubin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-17T19:37:52Z", "updated_at": "2017-03-17T19:37:52Z", "author_association": "NONE", "body_html": "<p>I've been thinking about it a bit.<br>\nFirst \"pre-heat\" I meant to tell the runtime that during a run over the graph all pages must be in memory. I think that \"synchronous\" is ambiguous because an OS not necessary have to load everything during the call but before the first usage of the tensor.<br>\nA generic implementation can simply touch all pages, but for a specific OS a special flag can be specified that effectively does the same.</p>\n<p>Returning to the initial idea, I like the optimization, but I'd like to have some optimization/control and not switch all memmapped tensors to the new mode. Yesterday I thought that a good option is to create a <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/graph_optimizer.h\">GraphOptimizer</a> that sets to ImmutableConst the \"pre-heat\" attribute based on some optimization parameters (e.g. amount of memory). But it might be an over-kill, a simpler approach would be a flag in the script that adds ImmutableConstants that are specified by a name pattern that they need to be pre-heated. How about this solution?</p>", "body_text": "I've been thinking about it a bit.\nFirst \"pre-heat\" I meant to tell the runtime that during a run over the graph all pages must be in memory. I think that \"synchronous\" is ambiguous because an OS not necessary have to load everything during the call but before the first usage of the tensor.\nA generic implementation can simply touch all pages, but for a specific OS a special flag can be specified that effectively does the same.\nReturning to the initial idea, I like the optimization, but I'd like to have some optimization/control and not switch all memmapped tensors to the new mode. Yesterday I thought that a good option is to create a GraphOptimizer that sets to ImmutableConst the \"pre-heat\" attribute based on some optimization parameters (e.g. amount of memory). But it might be an over-kill, a simpler approach would be a flag in the script that adds ImmutableConstants that are specified by a name pattern that they need to be pre-heated. How about this solution?", "body": "I've been thinking about it a bit. \r\nFirst \"pre-heat\" I meant to tell the runtime that during a run over the graph all pages must be in memory. I think that \"synchronous\" is ambiguous because an OS not necessary have to load everything during the call but before the first usage of the tensor. \r\nA generic implementation can simply touch all pages, but for a specific OS a special flag can be specified that effectively does the same.\r\n\r\nReturning to the initial idea, I like the optimization, but I'd like to have some optimization/control and not switch all memmapped tensors to the new mode. Yesterday I thought that a good option is to create a [GraphOptimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/graph_optimizer.h) that sets to ImmutableConst the \"pre-heat\" attribute based on some optimization parameters (e.g. amount of memory). But it might be an over-kill, a simpler approach would be a flag in the script that adds ImmutableConstants that are specified by a name pattern that they need to be pre-heated. How about this solution?\r\n\r\n"}