{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20253", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20253/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20253/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20253/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20253", "id": 335135045, "node_id": "MDU6SXNzdWUzMzUxMzUwNDU=", "number": 20253, "title": "Contrib Loss Function Bug", "user": {"login": "jhorowitz", "id": 5096318, "node_id": "MDQ6VXNlcjUwOTYzMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5096318?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhorowitz", "html_url": "https://github.com/jhorowitz", "followers_url": "https://api.github.com/users/jhorowitz/followers", "following_url": "https://api.github.com/users/jhorowitz/following{/other_user}", "gists_url": "https://api.github.com/users/jhorowitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhorowitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhorowitz/subscriptions", "organizations_url": "https://api.github.com/users/jhorowitz/orgs", "repos_url": "https://api.github.com/users/jhorowitz/repos", "events_url": "https://api.github.com/users/jhorowitz/events{/privacy}", "received_events_url": "https://api.github.com/users/jhorowitz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-06-23T22:20:05Z", "updated_at": "2018-07-11T14:58:36Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>I've reproduced this problem on multiple systems including two with CPU only and three with GPU. The specs below represent one such system.</p>\n<p>Have I written custom code<br>\nYes</p>\n<p>OS Platform and Distribution<br>\nMac OS X 10.13.2<br>\n17.3.0 Darwin Kernel 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64</p>\n<p>TensorFlow installed from<br>\nPip (python3)</p>\n<p>TensorFlow version<br>\n1.8.0</p>\n<p>Bazel version<br>\nN/A</p>\n<p>CUDA/cuDNN version<br>\nN/A</p>\n<p>GPU model and memory<br>\nN/A</p>\n<p>Exact command to reproduce<br>\nPlease see code below</p>\n<h3>Describe the problem</h3>\n<p>There are several conditions that cause triplet_semihard_loss from tf.contrib to return nan as the loss. This totally messes up the gradient and causes divergence (at least in my build). Below is a minimal reproduction of one way to cause nan which is to have a batch of all different classes which causes there to be no positive for any chosen anchor.</p>\n<p>The lack of a negative does not result in nan, just a super high loss which while not as bad, is still not what I'd expect.</p>\n<p>When running backprop, I would expect to have no gradient if there are no valid triplets to train off of.</p>\n<p>I'm happy to contribute to fixes if you guys agree this is a bug.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.losses <span class=\"pl-k\">import</span> metric_learning\n\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>)\nembeddings <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">128</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>embeddings<span class=\"pl-pds\">'</span></span>)\n\nloss_ph <span class=\"pl-k\">=</span> metric_learning.triplet_semihard_loss(labels, embeddings)\n\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nloss <span class=\"pl-k\">=</span> sess.run(loss_ph, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n    embeddings: np.random.rand(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">128</span>),\n    labels: np.array([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n})\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss<span class=\"pl-pds\">\"</span></span>, loss)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> loss nan</span></pre></div>", "body_text": "System information\nI've reproduced this problem on multiple systems including two with CPU only and three with GPU. The specs below represent one such system.\nHave I written custom code\nYes\nOS Platform and Distribution\nMac OS X 10.13.2\n17.3.0 Darwin Kernel 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64\nTensorFlow installed from\nPip (python3)\nTensorFlow version\n1.8.0\nBazel version\nN/A\nCUDA/cuDNN version\nN/A\nGPU model and memory\nN/A\nExact command to reproduce\nPlease see code below\nDescribe the problem\nThere are several conditions that cause triplet_semihard_loss from tf.contrib to return nan as the loss. This totally messes up the gradient and causes divergence (at least in my build). Below is a minimal reproduction of one way to cause nan which is to have a batch of all different classes which causes there to be no positive for any chosen anchor.\nThe lack of a negative does not result in nan, just a super high loss which while not as bad, is still not what I'd expect.\nWhen running backprop, I would expect to have no gradient if there are no valid triplets to train off of.\nI'm happy to contribute to fixes if you guys agree this is a bug.\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.losses import metric_learning\n\nlabels = tf.placeholder(tf.int32, [None], name='labels')\nembeddings = tf.placeholder(tf.float32, [None, 128], name='embeddings')\n\nloss_ph = metric_learning.triplet_semihard_loss(labels, embeddings)\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nloss = sess.run(loss_ph, feed_dict={\n    embeddings: np.random.rand(2, 128),\n    labels: np.array([1, 2])\n})\n\nprint(\"loss\", loss)\n# loss nan", "body": "### System information\r\nI've reproduced this problem on multiple systems including two with CPU only and three with GPU. The specs below represent one such system.\r\n\r\nHave I written custom code\r\nYes\r\n\r\nOS Platform and Distribution\r\nMac OS X 10.13.2\r\n17.3.0 Darwin Kernel 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64\r\n\r\nTensorFlow installed from\r\nPip (python3)\r\n\r\nTensorFlow version\r\n1.8.0\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\nN/A\r\n\r\nGPU model and memory\r\nN/A\r\n\r\nExact command to reproduce\r\nPlease see code below\r\n\r\n### Describe the problem\r\nThere are several conditions that cause triplet_semihard_loss from tf.contrib to return nan as the loss. This totally messes up the gradient and causes divergence (at least in my build). Below is a minimal reproduction of one way to cause nan which is to have a batch of all different classes which causes there to be no positive for any chosen anchor. \r\n\r\nThe lack of a negative does not result in nan, just a super high loss which while not as bad, is still not what I'd expect.\r\n\r\nWhen running backprop, I would expect to have no gradient if there are no valid triplets to train off of.\r\n\r\nI'm happy to contribute to fixes if you guys agree this is a bug.\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.losses import metric_learning\r\n\r\nlabels = tf.placeholder(tf.int32, [None], name='labels')\r\nembeddings = tf.placeholder(tf.float32, [None, 128], name='embeddings')\r\n\r\nloss_ph = metric_learning.triplet_semihard_loss(labels, embeddings)\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nloss = sess.run(loss_ph, feed_dict={\r\n    embeddings: np.random.rand(2, 128),\r\n    labels: np.array([1, 2])\r\n})\r\n\r\nprint(\"loss\", loss)\r\n# loss nan\r\n```"}