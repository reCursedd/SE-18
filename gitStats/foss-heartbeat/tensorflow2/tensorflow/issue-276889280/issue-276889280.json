{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14897", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14897/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14897/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14897/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14897", "id": 276889280, "node_id": "MDU6SXNzdWUyNzY4ODkyODA=", "number": 14897, "title": "A bug in tensorflow r1.4 when applying  MultiRNNCell", "user": {"login": "pangzhan27", "id": 30332036, "node_id": "MDQ6VXNlcjMwMzMyMDM2", "avatar_url": "https://avatars1.githubusercontent.com/u/30332036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pangzhan27", "html_url": "https://github.com/pangzhan27", "followers_url": "https://api.github.com/users/pangzhan27/followers", "following_url": "https://api.github.com/users/pangzhan27/following{/other_user}", "gists_url": "https://api.github.com/users/pangzhan27/gists{/gist_id}", "starred_url": "https://api.github.com/users/pangzhan27/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pangzhan27/subscriptions", "organizations_url": "https://api.github.com/users/pangzhan27/orgs", "repos_url": "https://api.github.com/users/pangzhan27/repos", "events_url": "https://api.github.com/users/pangzhan27/events{/privacy}", "received_events_url": "https://api.github.com/users/pangzhan27/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-11-27T03:16:15Z", "updated_at": "2018-04-28T03:23:35Z", "closed_at": "2017-11-28T02:45:31Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>**TensorFlow installed from source :</li>\n<li>**TensorFlow version: r1.4</li>\n<li>**Python version: 3.5.4</li>\n<li>**Bazel version: 0.5.4</li>\n<li>**GCC/Compiler version 5.4.0</li>\n<li>**CUDA/cuDNN version: 9.0 &amp;5.0</li>\n<li>*<em>GPU model and memory</em>: GeForce GTX 1080</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>when applying the MultiRNNCell as below, an error occurs. The code went well in tensorflow r1.3</p>\n<h1>Source code</h1>\n<p>input_list is a list of tensor with shape[None, 8]<br>\nn_hidden = 32<br>\nlstm = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)<br>\nstacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm]*2)<br>\noutputs, states = tf.nn.static_rnn(stacked_lstm, input_list, dtype=tf.float32)</p>\n<h1>error</h1>\n<p>ValueError: Dimensions must be equal, but are 64 and 40 for 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,64], [40,128].</p>\n<p>However when only applying one single lstm, i works well.</p>\n<p>opinions:<br>\nwhen calculating, the basiclstmcell will be called, where a class named Linear will be initialized, as an example, in my case, the variable self.weight in this class will be initialized as <a href=\"%5B32+8,32*4%5D\">40,128</a>.<br>\n*** code from rnn_cell_impl.py***<br>\nif self._linear is None:<br>\nself._linear = _Linear([inputs, h], 4 * self._num_units, True)</p>\n<p>But, when MultiRNNCell is the case, for example,  a 2 layers lstm. in the second layer, the weight should be [64,128]('h' in last layer (32)+'o' in  last layer(32)). Disappointingly, the weight will only be initialized once and stay with the shape [40,128] due to the sentence \"if self._linear is None:\". So that the reason why such error occurs.</p>\n<p>i try to comment out this sentence, but since share variable mechanism is related. it dosen't work, and induces other problem.</p>\n<p>ValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (64, 128) and found shape (40, 128).</p>\n<p>Any idea how to solve this problem efficiently?</p>", "body_text": "System information\n\n**TensorFlow installed from source :\n**TensorFlow version: r1.4\n**Python version: 3.5.4\n**Bazel version: 0.5.4\n**GCC/Compiler version 5.4.0\n**CUDA/cuDNN version: 9.0 &5.0\n*GPU model and memory: GeForce GTX 1080\n\nDescribe the problem\nwhen applying the MultiRNNCell as below, an error occurs. The code went well in tensorflow r1.3\nSource code\ninput_list is a list of tensor with shape[None, 8]\nn_hidden = 32\nlstm = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\nstacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm]*2)\noutputs, states = tf.nn.static_rnn(stacked_lstm, input_list, dtype=tf.float32)\nerror\nValueError: Dimensions must be equal, but are 64 and 40 for 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,64], [40,128].\nHowever when only applying one single lstm, i works well.\nopinions:\nwhen calculating, the basiclstmcell will be called, where a class named Linear will be initialized, as an example, in my case, the variable self.weight in this class will be initialized as 40,128.\n*** code from rnn_cell_impl.py***\nif self._linear is None:\nself._linear = _Linear([inputs, h], 4 * self._num_units, True)\nBut, when MultiRNNCell is the case, for example,  a 2 layers lstm. in the second layer, the weight should be [64,128]('h' in last layer (32)+'o' in  last layer(32)). Disappointingly, the weight will only be initialized once and stay with the shape [40,128] due to the sentence \"if self._linear is None:\". So that the reason why such error occurs.\ni try to comment out this sentence, but since share variable mechanism is related. it dosen't work, and induces other problem.\nValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (64, 128) and found shape (40, 128).\nAny idea how to solve this problem efficiently?", "body": "\r\n### System information\r\n- **TensorFlow installed from source :\r\n- **TensorFlow version: r1.4\r\n- **Python version: 3.5.4\r\n- **Bazel version: 0.5.4\r\n- **GCC/Compiler version 5.4.0\r\n- **CUDA/cuDNN version: 9.0 &5.0\r\n- **GPU model and memory*: GeForce GTX 1080\r\n\r\n### Describe the problem\r\nwhen applying the MultiRNNCell as below, an error occurs. The code went well in tensorflow r1.3\r\n# Source code\r\ninput_list is a list of tensor with shape[None, 8]\r\nn_hidden = 32\r\nlstm = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\r\nstacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm]*2)\r\noutputs, states = tf.nn.static_rnn(stacked_lstm, input_list, dtype=tf.float32)\r\n# error\r\nValueError: Dimensions must be equal, but are 64 and 40 for 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,64], [40,128].\r\n\r\nHowever when only applying one single lstm, i works well.\r\n\r\nopinions:\r\nwhen calculating, the basiclstmcell will be called, where a class named Linear will be initialized, as an example, in my case, the variable self.weight in this class will be initialized as [40,128]([32+8,32*4]). \r\n*** code from rnn_cell_impl.py***\r\nif self._linear is None:\r\n    self._linear = _Linear([inputs, h], 4 * self._num_units, True)\r\n\r\nBut, when MultiRNNCell is the case, for example,  a 2 layers lstm. in the second layer, the weight should be [64,128]('h' in last layer (32)+'o' in  last layer(32)). Disappointingly, the weight will only be initialized once and stay with the shape [40,128] due to the sentence \"if self._linear is None:\". So that the reason why such error occurs.\r\n\r\ni try to comment out this sentence, but since share variable mechanism is related. it dosen't work, and induces other problem.\r\n\r\nValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (64, 128) and found shape (40, 128).\r\n\r\nAny idea how to solve this problem efficiently?\r\n\r\n\r\n\r\n"}