{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23684", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23684/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23684/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23684/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23684", "id": 379784003, "node_id": "MDU6SXNzdWUzNzk3ODQwMDM=", "number": 23684, "title": "How to freeze layers that located at the last of the whole graph  when fine-tune", "user": {"login": "ztwe", "id": 7899459, "node_id": "MDQ6VXNlcjc4OTk0NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7899459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ztwe", "html_url": "https://github.com/ztwe", "followers_url": "https://api.github.com/users/ztwe/followers", "following_url": "https://api.github.com/users/ztwe/following{/other_user}", "gists_url": "https://api.github.com/users/ztwe/gists{/gist_id}", "starred_url": "https://api.github.com/users/ztwe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ztwe/subscriptions", "organizations_url": "https://api.github.com/users/ztwe/orgs", "repos_url": "https://api.github.com/users/ztwe/repos", "events_url": "https://api.github.com/users/ztwe/events{/privacy}", "received_events_url": "https://api.github.com/users/ztwe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-12T13:47:03Z", "updated_at": "2018-11-19T20:12:03Z", "closed_at": "2018-11-19T20:12:03Z", "author_association": "NONE", "body_html": "<p>The grpah is A-&gt;B-&gt;C-&gt;loss, and I want to freeze A and C when fine-tune. A is a pretrained network. It's easy to freeze A. B contains LSTM and fc ops. C is variables or tuple tensors which are the results of fc. C stores one tensor  for one time because the tensor is hidden state of LSTM. Actually C will stores 6 tensors for a iteration.<br>\nIt seems there are several solutions:</p>\n<ol>\n<li>C is variables and \"trainable=False\"           (this results in \"no gradient\" error)</li>\n<li>tf.stop_gradient()          (not try yet)</li>\n<li>add var_list to function optimize_loss()         (not try yet)</li>\n</ol>\n<p>Which one should I try? Or are there new solutions?</p>", "body_text": "The grpah is A->B->C->loss, and I want to freeze A and C when fine-tune. A is a pretrained network. It's easy to freeze A. B contains LSTM and fc ops. C is variables or tuple tensors which are the results of fc. C stores one tensor  for one time because the tensor is hidden state of LSTM. Actually C will stores 6 tensors for a iteration.\nIt seems there are several solutions:\n\nC is variables and \"trainable=False\"           (this results in \"no gradient\" error)\ntf.stop_gradient()          (not try yet)\nadd var_list to function optimize_loss()         (not try yet)\n\nWhich one should I try? Or are there new solutions?", "body": "The grpah is A->B->C->loss, and I want to freeze A and C when fine-tune. A is a pretrained network. It's easy to freeze A. B contains LSTM and fc ops. C is variables or tuple tensors which are the results of fc. C stores one tensor  for one time because the tensor is hidden state of LSTM. Actually C will stores 6 tensors for a iteration.\r\nIt seems there are several solutions:\r\n1. C is variables and \"trainable=False\"           (this results in \"no gradient\" error)\r\n2. tf.stop_gradient()          (not try yet)\r\n3.  add var_list to function optimize_loss()         (not try yet)\r\n\r\nWhich one should I try? Or are there new solutions? \r\n"}