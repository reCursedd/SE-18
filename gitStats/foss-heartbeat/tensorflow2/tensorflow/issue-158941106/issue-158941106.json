{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2706", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2706/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2706/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2706/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2706", "id": 158941106, "node_id": "MDU6SXNzdWUxNTg5NDExMDY=", "number": 2706, "title": "While loop gradient descent error", "user": {"login": "HanyuGuo", "id": 14354442, "node_id": "MDQ6VXNlcjE0MzU0NDQy", "avatar_url": "https://avatars2.githubusercontent.com/u/14354442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HanyuGuo", "html_url": "https://github.com/HanyuGuo", "followers_url": "https://api.github.com/users/HanyuGuo/followers", "following_url": "https://api.github.com/users/HanyuGuo/following{/other_user}", "gists_url": "https://api.github.com/users/HanyuGuo/gists{/gist_id}", "starred_url": "https://api.github.com/users/HanyuGuo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HanyuGuo/subscriptions", "organizations_url": "https://api.github.com/users/HanyuGuo/orgs", "repos_url": "https://api.github.com/users/HanyuGuo/repos", "events_url": "https://api.github.com/users/HanyuGuo/events{/privacy}", "received_events_url": "https://api.github.com/users/HanyuGuo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2016-06-07T14:42:54Z", "updated_at": "2017-04-20T01:37:22Z", "closed_at": "2016-06-22T16:33:58Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Centos 7</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\nCPU version</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.</li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\".</li>\n</ol>\n<p>0.8.0</p>\n<h3>Steps to reproduce</h3>\n<p>Example 1 for Tensorflow 0.8</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> rnn\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">model</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        error_position <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n        vocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\n        embedding_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Word2vec</span>\n        W <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n            <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.story <span class=\"pl-k\">=</span> []\n        story_embedded <span class=\"pl-k\">=</span> []\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Embedding</span>\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">4</span>):\n            <span class=\"pl-c1\">self</span>.story.append(tf.placeholder(tf.int32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>]))\n            story_embedded.append(tf.nn.embedding_lookup(W, <span class=\"pl-c1\">self</span>.story[i]))\n\n        <span class=\"pl-c1\">self</span>.answer <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">3</span>, tf.int64)\n        answer_weights <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([embedding_size, vocab_size], <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-c1\">0.1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>answer_weights<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> w2v to sentence2vec for story and question</span>\n        scell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array <span class=\"pl-k\">=</span> []\n\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">4</span>):\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tt<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span> <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>):\n                _, story_state <span class=\"pl-k\">=</span> rnn.rnn(scell, tf.unpack(tf.reshape(story_embedded[i],[<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">5</span>]), <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n                story_state_array.append(story_state)\n        storys <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">0</span>,story_state_array)\n\n\n        mem_weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mem_weights<span class=\"pl-pds\">\"</span></span>, [embedding_size, embedding_size], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer())\n        l1_weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>l1_weights<span class=\"pl-pds\">\"</span></span>, [embedding_size, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer())\n\n        episodic_gate_unpacked <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">body</span>(<span class=\"pl-smi\">mem_state_previous</span>, <span class=\"pl-smi\">hops</span>):\n            mem_state_current <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(mem_state_previous, mem_weights))\n            hops <span class=\"pl-k\">=</span> tf.add(hops,<span class=\"pl-c1\">1</span>)\n            <span class=\"pl-k\">return</span>  mem_state_current, hops\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">condition</span>(<span class=\"pl-smi\">mem_state_previous</span>, <span class=\"pl-smi\">hops</span>):\n            z <span class=\"pl-k\">=</span> tf.mul(storys, mem_state_previous)\n            e_reshaped <span class=\"pl-k\">=</span> tf.reshape(tf.matmul(z , l1_weights) , [<span class=\"pl-c1\">1</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>e_reshaped<span class=\"pl-pds\">\"</span></span>)\n            e_gate <span class=\"pl-k\">=</span> tf.nn.softmax(e_reshaped)\n            e_unpacked <span class=\"pl-k\">=</span> tf.unpack( tf.reshape(e_gate, [<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">1</span>]))\n            argmax_e <span class=\"pl-k\">=</span> tf.to_int32(tf.argmax(e_gate, <span class=\"pl-c1\">1</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span>should be 1</span>\n            <span class=\"pl-k\">return</span> tf.logical_and(tf.less(argmax_e[<span class=\"pl-c1\">0</span>], error_position),tf.less(hops,<span class=\"pl-c1\">5</span>))\n\n        init_state <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">1.0</span>,<span class=\"pl-c1\">1.0</span>,<span class=\"pl-c1\">1.0</span>,<span class=\"pl-c1\">1.0</span>,<span class=\"pl-c1\">1.0</span>,]])\n        initial_hops <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>)\n        a_state, _ <span class=\"pl-k\">=</span> tf.while_loop(condition,body,[init_state, initial_hops], <span class=\"pl-v\">back_prop</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> answer</span>\n        predicted_answer <span class=\"pl-k\">=</span> tf.matmul(a_state, answer_weights)\n        answer <span class=\"pl-k\">=</span> tf.reshape(tf.one_hot(<span class=\"pl-c1\">self</span>.answer, vocab_size, <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">0.0</span>), [<span class=\"pl-c1\">1</span>,vocab_size])\n        <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> gradient</span>\n        params <span class=\"pl-k\">=</span> tf.trainable_variables()\n        <span class=\"pl-c1\">self</span>.gradient_norms <span class=\"pl-k\">=</span> []\n        <span class=\"pl-c1\">self</span>.updates <span class=\"pl-k\">=</span> []\n        optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.05</span>)\n        gradients <span class=\"pl-k\">=</span> tf.gradients(<span class=\"pl-c1\">self</span>.loss, params)\n        <span class=\"pl-c1\">self</span>.updates <span class=\"pl-k\">=</span> optimizer.apply_gradients(\n            <span class=\"pl-c1\">zip</span>(gradients, params))\n        <span class=\"pl-c1\">self</span>.saver <span class=\"pl-k\">=</span> tf.train.Saver(tf.all_variables())\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">sess</span>):\n        feed <span class=\"pl-k\">=</span>{}\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">0</span>].name] <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>]\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">1</span>].name] <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">6</span>,<span class=\"pl-c1\">5</span>]\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">2</span>].name] <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">7</span>,<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">9</span>,<span class=\"pl-c1\">8</span>]\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">3</span>].name] <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">5</span>]\n\n        <span class=\"pl-c1\">print</span> sess.run([<span class=\"pl-c1\">self</span>.loss,<span class=\"pl-c1\">self</span>.updates],feed)\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        test_model <span class=\"pl-k\">=</span> model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n</pre></div>\n<p>Example 2 for Tensorflow 0.9</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> rnn\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">model</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        error_position <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n        vocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\n        embedding_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Word2vec</span>\n        W <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n                <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.story <span class=\"pl-k\">=</span> []\n        story_embedded <span class=\"pl-k\">=</span> []\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Embedding</span>\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">4</span>):\n            <span class=\"pl-c1\">self</span>.story.append(tf.placeholder(tf.int32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>,<span class=\"pl-c1\">None</span>]))\n            story_embedded.append(tf.nn.embedding_lookup(W, <span class=\"pl-c1\">self</span>.story[i]))\n\n        <span class=\"pl-c1\">self</span>.question <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>,<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Question<span class=\"pl-pds\">\"</span></span>)\n        question_embedded <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(W, <span class=\"pl-c1\">self</span>.question)\n\n        <span class=\"pl-c1\">self</span>.answer <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">3</span>, tf.int64)\n        answer_weights <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([embedding_size, vocab_size], <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-c1\">0.1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>answer_weights<span class=\"pl-pds\">\"</span></span>)   \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> w2v to sentence2vec for story and question</span>\n        scell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array <span class=\"pl-k\">=</span> []\n\n        q_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(embedding_size)\n        _, question_state <span class=\"pl-k\">=</span> tf.nn.rnn(q_cell, tf.unpack(question_embedded, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">4</span>):\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tt<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span> <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>):\n                _, story_state <span class=\"pl-k\">=</span> tf.nn.rnn(scell, tf.unpack(story_embedded[i], <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n                story_state_array.append(story_state)   \n\n        stories <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">0</span>,story_state_array)\n\n\n        mem_weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mem_weights<span class=\"pl-pds\">\"</span></span>, [embedding_size <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, embedding_size], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer())\n        l1_weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>l1_weights<span class=\"pl-pds\">\"</span></span>, [embedding_size, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer())\n\n        e_unpacked <span class=\"pl-k\">=</span> [] \n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">body</span>(<span class=\"pl-smi\">mem_state_previous</span>, <span class=\"pl-smi\">hops</span>):\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> context = MGRU(facts_, e_unpacked)</span>\n            mem_state_current <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(tf.concat(<span class=\"pl-c1\">1</span>,[mem_state_previous, question_state]), mem_weights))\n\n            hops <span class=\"pl-k\">=</span> tf.add(hops,<span class=\"pl-c1\">1</span>)\n            <span class=\"pl-k\">return</span>  mem_state_current, hops\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">condition</span>(<span class=\"pl-smi\">mem_state_previous</span>, <span class=\"pl-smi\">hops</span>):    \n            z <span class=\"pl-k\">=</span> tf.mul(stories, mem_state_previous)\n            e_reshaped <span class=\"pl-k\">=</span> tf.reshape(tf.matmul(z , l1_weights) , [<span class=\"pl-c1\">1</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>e_reshaped<span class=\"pl-pds\">\"</span></span>)\n            e_gate <span class=\"pl-k\">=</span> tf.nn.softmax(e_reshaped)\n            e_unpacked <span class=\"pl-k\">=</span> tf.unpack( tf.reshape(e_gate, [<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">1</span>]))  \n            argmax_e <span class=\"pl-k\">=</span> tf.to_int32(tf.argmax(e_gate, <span class=\"pl-c1\">1</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span>should be 1</span>\n            <span class=\"pl-k\">return</span> tf.logical_and(tf.less(argmax_e[<span class=\"pl-c1\">0</span>], error_position),tf.less(hops,<span class=\"pl-c1\">5</span>))\n\n\n        initial_hops <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>)\n        a_state, _ <span class=\"pl-k\">=</span> tf.while_loop(condition,body,[question_state, initial_hops], <span class=\"pl-v\">back_prop</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)   \n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> answer</span>\n        predicted_answer <span class=\"pl-k\">=</span> tf.matmul(a_state, answer_weights)\n        answer <span class=\"pl-k\">=</span> tf.reshape(tf.one_hot(<span class=\"pl-c1\">self</span>.answer, vocab_size, <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">0.0</span>), [<span class=\"pl-c1\">1</span>,vocab_size])\n        <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> gradient</span>\n        params <span class=\"pl-k\">=</span> tf.trainable_variables()   \n        <span class=\"pl-c1\">self</span>.gradient_norms <span class=\"pl-k\">=</span> []\n        <span class=\"pl-c1\">self</span>.updates <span class=\"pl-k\">=</span> []\n        optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.05</span>)\n        gradients <span class=\"pl-k\">=</span> tf.gradients(<span class=\"pl-c1\">self</span>.loss, params) \n        <span class=\"pl-c1\">self</span>.updates <span class=\"pl-k\">=</span> optimizer.apply_gradients(\n            <span class=\"pl-c1\">zip</span>(gradients, params))\n        <span class=\"pl-c1\">self</span>.saver <span class=\"pl-k\">=</span> tf.train.Saver(tf.all_variables())\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">sess</span>):    \n        feed <span class=\"pl-k\">=</span>{}\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">0</span>].name] <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">1</span>],[<span class=\"pl-c1\">2</span>],[<span class=\"pl-c1\">3</span>],[<span class=\"pl-c1\">4</span>]]\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">1</span>].name] <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">4</span>],[<span class=\"pl-c1\">5</span>],[<span class=\"pl-c1\">6</span>],[<span class=\"pl-c1\">5</span>]]\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">2</span>].name] <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">7</span>],[<span class=\"pl-c1\">8</span>],[<span class=\"pl-c1\">9</span>],[<span class=\"pl-c1\">8</span>]]\n        feed[<span class=\"pl-c1\">self</span>.story[<span class=\"pl-c1\">3</span>].name] <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">0</span>],[<span class=\"pl-c1\">2</span>],[<span class=\"pl-c1\">3</span>],[<span class=\"pl-c1\">5</span>]]\n        feed[<span class=\"pl-c1\">self</span>.question.name] <span class=\"pl-k\">=</span> [[<span class=\"pl-c1\">1</span>],[<span class=\"pl-c1\">4</span>],[<span class=\"pl-c1\">5</span>],[<span class=\"pl-c1\">5</span>]]\n\n        <span class=\"pl-c1\">print</span> sess.run([<span class=\"pl-c1\">self</span>.loss,<span class=\"pl-c1\">self</span>.updates],feed)\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        test_model <span class=\"pl-k\">=</span> model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n</pre></div>\n<h3>What have you tried?</h3>\n<p>In my <code>tf.while_loop()</code>, it calculates attention in each loop. If the attention is beyond certain threshold, it would terminate the <code>while_loop</code>. But it does not work as I expected.</p>\n<p>In this two situations, it works</p>\n<ol>\n<li>If I turn <code>back_prop=False</code> in <code>while_loop</code>, it works.</li>\n<li>If <code>argmax_ep_gate[0]</code> always <code>&lt; error_position</code>, it works.</li>\n</ol>\n<p><strong>Hence, the error might appear after several trails</strong></p>\n<p>Also, we found that in <code>z = tf.mul(storys, mem_state_previous)</code>, if <code>storys</code> does not generated after <code>tf.concat(0, story_state_array)</code> such as a single sentence2vec from <code>_, story_state = rnn.dynamic_rnn(scell, story_embedded[i], dtype=tf.float32)</code>, it works.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>Below log is generated <strong>before</strong> shrinking the example for both Tensorflow 0.8 and 0.9</p>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">111</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    test_model.step(sess)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">106</span>, <span class=\"pl-k\">in</span> step\n    <span class=\"pl-c1\">print</span> sess.run([<span class=\"pl-c1\">self</span>.loss, <span class=\"pl-c1\">self</span>.gradient_norms],feed)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">340</span>, <span class=\"pl-k\">in</span> run\n    run_metadata_ptr)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">564</span>, <span class=\"pl-k\">in</span> _run\n    feed_dict_string, options, run_metadata)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">637</span>, <span class=\"pl-k\">in</span> _do_run\n    target_list, options, run_metadata)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">659</span>, <span class=\"pl-k\">in</span> _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Inputs to operation gradients<span class=\"pl-k\">/</span>AddN of <span class=\"pl-c1\">type</span> AddN must have the same size <span class=\"pl-k\">and</span> shape.  Input <span class=\"pl-c1\">0</span>: [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">5</span>] <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">input</span> <span class=\"pl-c1\">1</span>: []\n     [[Node: gradients<span class=\"pl-k\">/</span>AddN = AddN[N=<span class=\"pl-c1\">2</span>, T=<span class=\"pl-c1\">DT_FLOAT</span>, _device=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/job:localhost/replica:0/task:0/cpu:0<span class=\"pl-pds\">\"</span></span>](gradients<span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>Enter_grad<span class=\"pl-k\">/</span>Exit, gradients<span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>Mul<span class=\"pl-k\">/</span>Enter_1_grad<span class=\"pl-k\">/</span>b_acc_3)]]\nCaused by op <span class=\"pl-s\"><span class=\"pl-k\">u</span><span class=\"pl-pds\">'</span>gradients/AddN<span class=\"pl-pds\">'</span></span>, defined at:\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">109</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    test_model <span class=\"pl-k\">=</span> model()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">89</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    gradients <span class=\"pl-k\">=</span> tf.gradients(<span class=\"pl-c1\">self</span>.loss, params)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">431</span>, <span class=\"pl-k\">in</span> gradients\n    out_grads <span class=\"pl-k\">=</span> _AggregatedGrads(grads, op, loop_state, aggregation_method)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">676</span>, <span class=\"pl-k\">in</span> _AggregatedGrads\n    out_grads[i] <span class=\"pl-k\">=</span> math_ops.add_n(out_grad)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">58</span>, <span class=\"pl-k\">in</span> add_n\n    <span class=\"pl-k\">return</span> _op_def_lib.apply_op(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>AddN<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>inputs, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">655</span>, <span class=\"pl-k\">in</span> apply_op\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2154</span>, <span class=\"pl-k\">in</span> create_op\n    original_op<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._default_original_op, op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1154</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">self</span>._traceback <span class=\"pl-k\">=</span> _extract_stack()\n</pre></div>\n<p>Below log is generated <strong>after</strong> shrinking the example, fixed after 0.9</p>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">82</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    test_model.step(sess)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">77</span>, <span class=\"pl-k\">in</span> step\n    <span class=\"pl-c1\">print</span> sess.run([<span class=\"pl-c1\">self</span>.loss,<span class=\"pl-c1\">self</span>.updates],feed)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">340</span>, <span class=\"pl-k\">in</span> run\n    run_metadata_ptr)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">564</span>, <span class=\"pl-k\">in</span> _run\n    feed_dict_string, options, run_metadata)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">637</span>, <span class=\"pl-k\">in</span> _do_run\n    target_list, options, run_metadata)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">659</span>, <span class=\"pl-k\">in</span> _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: var <span class=\"pl-k\">and</span> grad do <span class=\"pl-k\">not</span> have the same shape[<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>] []\n     [[Node: Adam<span class=\"pl-k\">/</span>update_mem_weights<span class=\"pl-k\">/</span>ApplyAdam = ApplyAdam[T=<span class=\"pl-c1\">DT_FLOAT</span>, _class=[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loc:@mem_weights<span class=\"pl-pds\">\"</span></span>], use_locking=false, _device=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/job:localhost/replica:0/task:0/cpu:0<span class=\"pl-pds\">\"</span></span>](mem_weights, mem_weights<span class=\"pl-k\">/</span>Adam, mem_weights<span class=\"pl-k\">/</span>Adam_1, beta1_power<span class=\"pl-k\">/</span>read, beta2_power<span class=\"pl-k\">/</span>read, Adam<span class=\"pl-k\">/</span>learning_rate, Adam<span class=\"pl-k\">/</span>beta1, Adam<span class=\"pl-k\">/</span>beta2, Adam<span class=\"pl-k\">/</span>epsilon, gradients<span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>MatMul_1<span class=\"pl-k\">/</span>Enter_grad<span class=\"pl-k\">/</span>b_acc_3)]]\nCaused by op <span class=\"pl-s\"><span class=\"pl-k\">u</span><span class=\"pl-pds\">'</span>Adam/update_mem_weights/ApplyAdam<span class=\"pl-pds\">'</span></span>, defined at:\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">80</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    test_model <span class=\"pl-k\">=</span> model()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>github_issue.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">67</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">zip</span>(gradients, params))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">299</span>, <span class=\"pl-k\">in</span> apply_gradients\n    update_ops.append(<span class=\"pl-c1\">self</span>._apply_dense(grad, var))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/training/adam.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">129</span>, <span class=\"pl-k\">in</span> _apply_dense\n    <span class=\"pl-c1\">self</span>._epsilon_t, grad, use_locking<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._use_locking).op\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">120</span>, <span class=\"pl-k\">in</span> apply_adam\n    use_locking<span class=\"pl-k\">=</span>use_locking, name<span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">655</span>, <span class=\"pl-k\">in</span> apply_op\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2154</span>, <span class=\"pl-k\">in</span> create_op\n    original_op<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._default_original_op, op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1154</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">self</span>._traceback <span class=\"pl-k\">=</span> _extract_stack()\n</pre></div>", "body_text": "Environment info\nOperating System: Centos 7\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nCPU version\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\n\n0.8.0\nSteps to reproduce\nExample 1 for Tensorflow 0.8\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn\n\nclass model(object):\n    def __init__(self):\n        error_position = 1\n        vocab_size = 20\n        embedding_size = 5\n        # Word2vec\n        W = tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n            trainable=False, name=\"W\")\n        self.story = []\n        story_embedded = []\n        # Embedding\n        for i in range(4):\n            self.story.append(tf.placeholder(tf.int32, shape=[None]))\n            story_embedded.append(tf.nn.embedding_lookup(W, self.story[i]))\n\n        self.answer = tf.constant(3, tf.int64)\n        answer_weights = tf.Variable(tf.truncated_normal([embedding_size, vocab_size], -0.1, 0.1), name=\"answer_weights\")\n        # w2v to sentence2vec for story and question\n        scell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array = []\n\n        for i in range(0,4):\n            with tf.variable_scope(\"tt\", reuse=True if i > 0 else None):\n                _, story_state = rnn.rnn(scell, tf.unpack(tf.reshape(story_embedded[i],[4,1,5]), 4), dtype=tf.float32)\n                story_state_array.append(story_state)\n        storys = tf.concat(0,story_state_array)\n\n\n        mem_weights = tf.get_variable(\"mem_weights\", [embedding_size, embedding_size], initializer=tf.random_normal_initializer())\n        l1_weights = tf.get_variable(\"l1_weights\", [embedding_size, 1], initializer=tf.random_normal_initializer())\n\n        episodic_gate_unpacked = []\n        def body(mem_state_previous, hops):\n            mem_state_current = tf.nn.relu(tf.matmul(mem_state_previous, mem_weights))\n            hops = tf.add(hops,1)\n            return  mem_state_current, hops\n        def condition(mem_state_previous, hops):\n            z = tf.mul(storys, mem_state_previous)\n            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name=\"e_reshaped\")\n            e_gate = tf.nn.softmax(e_reshaped)\n            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))\n            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1\n            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))\n\n        init_state = tf.constant([[1.0,1.0,1.0,1.0,1.0,]])\n        initial_hops = tf.constant(0)\n        a_state, _ = tf.while_loop(condition,body,[init_state, initial_hops], back_prop=True)\n\n        # answer\n        predicted_answer = tf.matmul(a_state, answer_weights)\n        answer = tf.reshape(tf.one_hot(self.answer, vocab_size, 1.0, 0.0), [1,vocab_size])\n        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), name='loss')\n\n        # gradient\n        params = tf.trainable_variables()\n        self.gradient_norms = []\n        self.updates = []\n        optimizer = tf.train.AdamOptimizer(0.05)\n        gradients = tf.gradients(self.loss, params)\n        self.updates = optimizer.apply_gradients(\n            zip(gradients, params))\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self,sess):\n        feed ={}\n        feed[self.story[0].name] = [1,2,3,4]\n        feed[self.story[1].name] = [4,5,6,5]\n        feed[self.story[2].name] = [7,8,9,8]\n        feed[self.story[3].name] = [0,2,3,5]\n\n        print sess.run([self.loss,self.updates],feed)\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        test_model = model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n\nExample 2 for Tensorflow 0.9\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn\n\nclass model(object):\n    def __init__(self):\n        error_position = 2\n        vocab_size = 20\n        embedding_size = 5\n        # Word2vec\n        W = tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n                trainable=False, name=\"W\")\n        self.story = []\n        story_embedded = []\n        # Embedding\n        for i in range(4):\n            self.story.append(tf.placeholder(tf.int32, shape=[None,None]))\n            story_embedded.append(tf.nn.embedding_lookup(W, self.story[i]))\n\n        self.question = tf.placeholder(tf.int32, shape=[None,None], name=\"Question\")\n        question_embedded = tf.nn.embedding_lookup(W, self.question)\n\n        self.answer = tf.constant(3, tf.int64)\n        answer_weights = tf.Variable(tf.truncated_normal([embedding_size, vocab_size], -0.1, 0.1), name=\"answer_weights\")   \n        # w2v to sentence2vec for story and question\n        scell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array = []\n\n        q_cell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        _, question_state = tf.nn.rnn(q_cell, tf.unpack(question_embedded, 4), dtype=tf.float32)\n\n        for i in range(0,4):\n            with tf.variable_scope(\"tt\", reuse=True if i > 0 else None):\n                _, story_state = tf.nn.rnn(scell, tf.unpack(story_embedded[i], 4), dtype=tf.float32)\n                story_state_array.append(story_state)   \n\n        stories = tf.concat(0,story_state_array)\n\n\n        mem_weights = tf.get_variable(\"mem_weights\", [embedding_size * 2, embedding_size], initializer=tf.random_normal_initializer())\n        l1_weights = tf.get_variable(\"l1_weights\", [embedding_size, 1], initializer=tf.random_normal_initializer())\n\n        e_unpacked = [] \n        def body(mem_state_previous, hops):\n            # context = MGRU(facts_, e_unpacked)\n            mem_state_current = tf.nn.relu(tf.matmul(tf.concat(1,[mem_state_previous, question_state]), mem_weights))\n\n            hops = tf.add(hops,1)\n            return  mem_state_current, hops\n        def condition(mem_state_previous, hops):    \n            z = tf.mul(stories, mem_state_previous)\n            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name=\"e_reshaped\")\n            e_gate = tf.nn.softmax(e_reshaped)\n            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))  \n            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1\n            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))\n\n\n        initial_hops = tf.constant(0)\n        a_state, _ = tf.while_loop(condition,body,[question_state, initial_hops], back_prop=True)   \n\n        # answer\n        predicted_answer = tf.matmul(a_state, answer_weights)\n        answer = tf.reshape(tf.one_hot(self.answer, vocab_size, 1.0, 0.0), [1,vocab_size])\n        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), name='loss')\n\n        # gradient\n        params = tf.trainable_variables()   \n        self.gradient_norms = []\n        self.updates = []\n        optimizer = tf.train.AdamOptimizer(0.05)\n        gradients = tf.gradients(self.loss, params) \n        self.updates = optimizer.apply_gradients(\n            zip(gradients, params))\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self,sess):    \n        feed ={}\n        feed[self.story[0].name] = [[1],[2],[3],[4]]\n        feed[self.story[1].name] = [[4],[5],[6],[5]]\n        feed[self.story[2].name] = [[7],[8],[9],[8]]\n        feed[self.story[3].name] = [[0],[2],[3],[5]]\n        feed[self.question.name] = [[1],[4],[5],[5]]\n\n        print sess.run([self.loss,self.updates],feed)\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        test_model = model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n\nWhat have you tried?\nIn my tf.while_loop(), it calculates attention in each loop. If the attention is beyond certain threshold, it would terminate the while_loop. But it does not work as I expected.\nIn this two situations, it works\n\nIf I turn back_prop=False in while_loop, it works.\nIf argmax_ep_gate[0] always < error_position, it works.\n\nHence, the error might appear after several trails\nAlso, we found that in z = tf.mul(storys, mem_state_previous), if storys does not generated after tf.concat(0, story_state_array) such as a single sentence2vec from _, story_state = rnn.dynamic_rnn(scell, story_embedded[i], dtype=tf.float32), it works.\nLogs or other output that would be helpful\nBelow log is generated before shrinking the example for both Tensorflow 0.8 and 0.9\nTraceback (most recent call last):\n  File \"github_issue.py\", line 111, in <module>\n    test_model.step(sess)\n  File \"github_issue.py\", line 106, in step\n    print sess.run([self.loss, self.gradient_norms],feed)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Inputs to operation gradients/AddN of type AddN must have the same size and shape.  Input 0: [1,5] != input 1: []\n     [[Node: gradients/AddN = AddN[N=2, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/while/Enter_grad/Exit, gradients/while/Mul/Enter_1_grad/b_acc_3)]]\nCaused by op u'gradients/AddN', defined at:\n  File \"github_issue.py\", line 109, in <module>\n    test_model = model()\n  File \"github_issue.py\", line 89, in __init__\n    gradients = tf.gradients(self.loss, params)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 431, in gradients\n    out_grads = _AggregatedGrads(grads, op, loop_state, aggregation_method)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 676, in _AggregatedGrads\n    out_grads[i] = math_ops.add_n(out_grad)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 58, in add_n\n    return _op_def_lib.apply_op(\"AddN\", inputs=inputs, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\nBelow log is generated after shrinking the example, fixed after 0.9\nTraceback (most recent call last):\n  File \"github_issue.py\", line 82, in <module>\n    test_model.step(sess)\n  File \"github_issue.py\", line 77, in step\n    print sess.run([self.loss,self.updates],feed)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: var and grad do not have the same shape[5,5] []\n     [[Node: Adam/update_mem_weights/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@mem_weights\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mem_weights, mem_weights/Adam, mem_weights/Adam_1, beta1_power/read, beta2_power/read, Adam/learning_rate, Adam/beta1, Adam/beta2, Adam/epsilon, gradients/while/MatMul_1/Enter_grad/b_acc_3)]]\nCaused by op u'Adam/update_mem_weights/ApplyAdam', defined at:\n  File \"github_issue.py\", line 80, in <module>\n    test_model = model()\n  File \"github_issue.py\", line 67, in __init__\n    zip(gradients, params))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 299, in apply_gradients\n    update_ops.append(self._apply_dense(grad, var))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 129, in _apply_dense\n    self._epsilon_t, grad, use_locking=self._use_locking).op\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py\", line 120, in apply_adam\n    use_locking=use_locking, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()", "body": "### Environment info\n\nOperating System: Centos 7\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCPU version\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n0.8.0\n### Steps to reproduce\n\nExample 1 for Tensorflow 0.8\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn\n\nclass model(object):\n    def __init__(self):\n        error_position = 1\n        vocab_size = 20\n        embedding_size = 5\n        # Word2vec\n        W = tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n            trainable=False, name=\"W\")\n        self.story = []\n        story_embedded = []\n        # Embedding\n        for i in range(4):\n            self.story.append(tf.placeholder(tf.int32, shape=[None]))\n            story_embedded.append(tf.nn.embedding_lookup(W, self.story[i]))\n\n        self.answer = tf.constant(3, tf.int64)\n        answer_weights = tf.Variable(tf.truncated_normal([embedding_size, vocab_size], -0.1, 0.1), name=\"answer_weights\")\n        # w2v to sentence2vec for story and question\n        scell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array = []\n\n        for i in range(0,4):\n            with tf.variable_scope(\"tt\", reuse=True if i > 0 else None):\n                _, story_state = rnn.rnn(scell, tf.unpack(tf.reshape(story_embedded[i],[4,1,5]), 4), dtype=tf.float32)\n                story_state_array.append(story_state)\n        storys = tf.concat(0,story_state_array)\n\n\n        mem_weights = tf.get_variable(\"mem_weights\", [embedding_size, embedding_size], initializer=tf.random_normal_initializer())\n        l1_weights = tf.get_variable(\"l1_weights\", [embedding_size, 1], initializer=tf.random_normal_initializer())\n\n        episodic_gate_unpacked = []\n        def body(mem_state_previous, hops):\n            mem_state_current = tf.nn.relu(tf.matmul(mem_state_previous, mem_weights))\n            hops = tf.add(hops,1)\n            return  mem_state_current, hops\n        def condition(mem_state_previous, hops):\n            z = tf.mul(storys, mem_state_previous)\n            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name=\"e_reshaped\")\n            e_gate = tf.nn.softmax(e_reshaped)\n            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))\n            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1\n            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))\n\n        init_state = tf.constant([[1.0,1.0,1.0,1.0,1.0,]])\n        initial_hops = tf.constant(0)\n        a_state, _ = tf.while_loop(condition,body,[init_state, initial_hops], back_prop=True)\n\n        # answer\n        predicted_answer = tf.matmul(a_state, answer_weights)\n        answer = tf.reshape(tf.one_hot(self.answer, vocab_size, 1.0, 0.0), [1,vocab_size])\n        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), name='loss')\n\n        # gradient\n        params = tf.trainable_variables()\n        self.gradient_norms = []\n        self.updates = []\n        optimizer = tf.train.AdamOptimizer(0.05)\n        gradients = tf.gradients(self.loss, params)\n        self.updates = optimizer.apply_gradients(\n            zip(gradients, params))\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self,sess):\n        feed ={}\n        feed[self.story[0].name] = [1,2,3,4]\n        feed[self.story[1].name] = [4,5,6,5]\n        feed[self.story[2].name] = [7,8,9,8]\n        feed[self.story[3].name] = [0,2,3,5]\n\n        print sess.run([self.loss,self.updates],feed)\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        test_model = model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n\n```\n\nExample 2 for Tensorflow 0.9\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import rnn\n\nclass model(object):\n    def __init__(self):\n        error_position = 2\n        vocab_size = 20\n        embedding_size = 5\n        # Word2vec\n        W = tf.Variable(tf.random_normal([vocab_size, embedding_size]),\n                trainable=False, name=\"W\")\n        self.story = []\n        story_embedded = []\n        # Embedding\n        for i in range(4):\n            self.story.append(tf.placeholder(tf.int32, shape=[None,None]))\n            story_embedded.append(tf.nn.embedding_lookup(W, self.story[i]))\n\n        self.question = tf.placeholder(tf.int32, shape=[None,None], name=\"Question\")\n        question_embedded = tf.nn.embedding_lookup(W, self.question)\n\n        self.answer = tf.constant(3, tf.int64)\n        answer_weights = tf.Variable(tf.truncated_normal([embedding_size, vocab_size], -0.1, 0.1), name=\"answer_weights\")   \n        # w2v to sentence2vec for story and question\n        scell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        story_state_array = []\n\n        q_cell = tf.nn.rnn_cell.GRUCell(embedding_size)\n        _, question_state = tf.nn.rnn(q_cell, tf.unpack(question_embedded, 4), dtype=tf.float32)\n\n        for i in range(0,4):\n            with tf.variable_scope(\"tt\", reuse=True if i > 0 else None):\n                _, story_state = tf.nn.rnn(scell, tf.unpack(story_embedded[i], 4), dtype=tf.float32)\n                story_state_array.append(story_state)   \n\n        stories = tf.concat(0,story_state_array)\n\n\n        mem_weights = tf.get_variable(\"mem_weights\", [embedding_size * 2, embedding_size], initializer=tf.random_normal_initializer())\n        l1_weights = tf.get_variable(\"l1_weights\", [embedding_size, 1], initializer=tf.random_normal_initializer())\n\n        e_unpacked = [] \n        def body(mem_state_previous, hops):\n            # context = MGRU(facts_, e_unpacked)\n            mem_state_current = tf.nn.relu(tf.matmul(tf.concat(1,[mem_state_previous, question_state]), mem_weights))\n\n            hops = tf.add(hops,1)\n            return  mem_state_current, hops\n        def condition(mem_state_previous, hops):    \n            z = tf.mul(stories, mem_state_previous)\n            e_reshaped = tf.reshape(tf.matmul(z , l1_weights) , [1,-1], name=\"e_reshaped\")\n            e_gate = tf.nn.softmax(e_reshaped)\n            e_unpacked = tf.unpack( tf.reshape(e_gate, [4,1]))  \n            argmax_e = tf.to_int32(tf.argmax(e_gate, 1)) #should be 1\n            return tf.logical_and(tf.less(argmax_e[0], error_position),tf.less(hops,5))\n\n\n        initial_hops = tf.constant(0)\n        a_state, _ = tf.while_loop(condition,body,[question_state, initial_hops], back_prop=True)   \n\n        # answer\n        predicted_answer = tf.matmul(a_state, answer_weights)\n        answer = tf.reshape(tf.one_hot(self.answer, vocab_size, 1.0, 0.0), [1,vocab_size])\n        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predicted_answer, answer), name='loss')\n\n        # gradient\n        params = tf.trainable_variables()   \n        self.gradient_norms = []\n        self.updates = []\n        optimizer = tf.train.AdamOptimizer(0.05)\n        gradients = tf.gradients(self.loss, params) \n        self.updates = optimizer.apply_gradients(\n            zip(gradients, params))\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self,sess):    \n        feed ={}\n        feed[self.story[0].name] = [[1],[2],[3],[4]]\n        feed[self.story[1].name] = [[4],[5],[6],[5]]\n        feed[self.story[2].name] = [[7],[8],[9],[8]]\n        feed[self.story[3].name] = [[0],[2],[3],[5]]\n        feed[self.question.name] = [[1],[4],[5],[5]]\n\n        print sess.run([self.loss,self.updates],feed)\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        test_model = model()\n        sess.run(tf.initialize_all_variables())\n        test_model.step(sess)\n\n```\n### What have you tried?\n\nIn my `tf.while_loop()`, it calculates attention in each loop. If the attention is beyond certain threshold, it would terminate the `while_loop`. But it does not work as I expected.\n\nIn this two situations, it works\n1. If I turn `back_prop=False` in `while_loop`, it works.\n2. If `argmax_ep_gate[0]` always `< error_position`, it works.\n\n**Hence, the error might appear after several trails** \n\nAlso, we found that in `z = tf.mul(storys, mem_state_previous)`, if `storys` does not generated after `tf.concat(0, story_state_array)` such as a single sentence2vec from `_, story_state = rnn.dynamic_rnn(scell, story_embedded[i], dtype=tf.float32)`, it works.\n### Logs or other output that would be helpful\n\nBelow log is generated **before** shrinking the example for both Tensorflow 0.8 and 0.9\n\n``` python\nTraceback (most recent call last):\n  File \"github_issue.py\", line 111, in <module>\n    test_model.step(sess)\n  File \"github_issue.py\", line 106, in step\n    print sess.run([self.loss, self.gradient_norms],feed)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Inputs to operation gradients/AddN of type AddN must have the same size and shape.  Input 0: [1,5] != input 1: []\n     [[Node: gradients/AddN = AddN[N=2, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/while/Enter_grad/Exit, gradients/while/Mul/Enter_1_grad/b_acc_3)]]\nCaused by op u'gradients/AddN', defined at:\n  File \"github_issue.py\", line 109, in <module>\n    test_model = model()\n  File \"github_issue.py\", line 89, in __init__\n    gradients = tf.gradients(self.loss, params)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 431, in gradients\n    out_grads = _AggregatedGrads(grads, op, loop_state, aggregation_method)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 676, in _AggregatedGrads\n    out_grads[i] = math_ops.add_n(out_grad)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 58, in add_n\n    return _op_def_lib.apply_op(\"AddN\", inputs=inputs, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n```\n\nBelow log is generated **after** shrinking the example, fixed after 0.9\n\n``` python\nTraceback (most recent call last):\n  File \"github_issue.py\", line 82, in <module>\n    test_model.step(sess)\n  File \"github_issue.py\", line 77, in step\n    print sess.run([self.loss,self.updates],feed)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: var and grad do not have the same shape[5,5] []\n     [[Node: Adam/update_mem_weights/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@mem_weights\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mem_weights, mem_weights/Adam, mem_weights/Adam_1, beta1_power/read, beta2_power/read, Adam/learning_rate, Adam/beta1, Adam/beta2, Adam/epsilon, gradients/while/MatMul_1/Enter_grad/b_acc_3)]]\nCaused by op u'Adam/update_mem_weights/ApplyAdam', defined at:\n  File \"github_issue.py\", line 80, in <module>\n    test_model = model()\n  File \"github_issue.py\", line 67, in __init__\n    zip(gradients, params))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 299, in apply_gradients\n    update_ops.append(self._apply_dense(grad, var))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 129, in _apply_dense\n    self._epsilon_t, grad, use_locking=self._use_locking).op\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py\", line 120, in apply_adam\n    use_locking=use_locking, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n```\n"}