{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18874", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18874/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18874/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18874/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18874", "id": 317781917, "node_id": "MDU6SXNzdWUzMTc3ODE5MTc=", "number": 18874, "title": "Faulty numpy randomness when using GPU", "user": {"login": "nikonikolov", "id": 11044035, "node_id": "MDQ6VXNlcjExMDQ0MDM1", "avatar_url": "https://avatars3.githubusercontent.com/u/11044035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikonikolov", "html_url": "https://github.com/nikonikolov", "followers_url": "https://api.github.com/users/nikonikolov/followers", "following_url": "https://api.github.com/users/nikonikolov/following{/other_user}", "gists_url": "https://api.github.com/users/nikonikolov/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikonikolov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikonikolov/subscriptions", "organizations_url": "https://api.github.com/users/nikonikolov/orgs", "repos_url": "https://api.github.com/users/nikonikolov/repos", "events_url": "https://api.github.com/users/nikonikolov/events{/privacy}", "received_events_url": "https://api.github.com/users/nikonikolov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-04-25T20:28:39Z", "updated_at": "2018-11-14T19:17:52Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Have I written custom code: Yes</li>\n<li>OS Platform and Distribution: Tested on Slackware Linux 14.2 and Ubuntu 16.04</li>\n<li>TensorFlow installed from: binary</li>\n<li>TensorFlow version: tested on both 1.4 and 1.6</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 8.0 for TF 1.4 and 9.0 for TF 1.6</li>\n<li>Bazel version: N/A</li>\n<li>GPU model and memory: tested on GTX 960M and GTX 1080</li>\n<li>Exact command to reproduce: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am unable to reproduce random numbers generated from numpy when I use it in combination with TF. In the beginning of all my tests, I set</p>\n<pre><code>tf.set_random_seed(seed)\nnp.random.seed(seed)\n</code></pre>\n<p>I have been debugging, and when I use numpy and no TF, all results are reproducible. When I add the TF code, the random numbers stop being reproducible. When I use both TF and numpy, I get the following results:</p>\n<ol>\n<li>TF variables are initialized to the same value every time (OK)</li>\n<li>When I use <code>np.random.RandomState()</code> with a set seed instead of direct calls to <code>np.random.uniform()</code>, <code>np.random.normal()</code>, etc, results are reproducible (OK)</li>\n<li>When I use direct calls to <code>np.random.uniform()</code>, <code>np.random.normal()</code>, etc, results are reproducible on CPU but not on GPU (NOT OK)<br>\n1080<br>\nSince the results are reproducible when using CPU but not GPU, it made me think that this might be a possible bug.</li>\n</ol>\n<p>I am not using any threads so the problem is definitely not caused by race conditions. I am monitoring reproducibility of results only by the random numbers which are generated, which are not in any way affected by the training results from the TF neural net. What is really strange is that the piece of code that seems to be affecting the results is the part about computing and backpropagating gradients. I do not expect that this uses any random numbers generated by numpy in the backend. Furthermore, even if it did, the order of my calls to <code>np.random</code> and to <code>sess.run</code> is always deterministic, so the same random numbers should be observed between separate runs.</p>\n<p>My code is somewhat too big at the moment to post. I can try to compile some simple example where the issue occurs, but I first wanted to make sure that this is indeed not the expected behavior.</p>", "body_text": "System information\n\nHave I written custom code: Yes\nOS Platform and Distribution: Tested on Slackware Linux 14.2 and Ubuntu 16.04\nTensorFlow installed from: binary\nTensorFlow version: tested on both 1.4 and 1.6\nPython version: 3.6\nCUDA/cuDNN version: 8.0 for TF 1.4 and 9.0 for TF 1.6\nBazel version: N/A\nGPU model and memory: tested on GTX 960M and GTX 1080\nExact command to reproduce: N/A\n\nDescribe the problem\nI am unable to reproduce random numbers generated from numpy when I use it in combination with TF. In the beginning of all my tests, I set\ntf.set_random_seed(seed)\nnp.random.seed(seed)\n\nI have been debugging, and when I use numpy and no TF, all results are reproducible. When I add the TF code, the random numbers stop being reproducible. When I use both TF and numpy, I get the following results:\n\nTF variables are initialized to the same value every time (OK)\nWhen I use np.random.RandomState() with a set seed instead of direct calls to np.random.uniform(), np.random.normal(), etc, results are reproducible (OK)\nWhen I use direct calls to np.random.uniform(), np.random.normal(), etc, results are reproducible on CPU but not on GPU (NOT OK)\n1080\nSince the results are reproducible when using CPU but not GPU, it made me think that this might be a possible bug.\n\nI am not using any threads so the problem is definitely not caused by race conditions. I am monitoring reproducibility of results only by the random numbers which are generated, which are not in any way affected by the training results from the TF neural net. What is really strange is that the piece of code that seems to be affecting the results is the part about computing and backpropagating gradients. I do not expect that this uses any random numbers generated by numpy in the backend. Furthermore, even if it did, the order of my calls to np.random and to sess.run is always deterministic, so the same random numbers should be observed between separate runs.\nMy code is somewhat too big at the moment to post. I can try to compile some simple example where the issue occurs, but I first wanted to make sure that this is indeed not the expected behavior.", "body": "### System information\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Tested on Slackware Linux 14.2 and Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: tested on both 1.4 and 1.6\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 8.0 for TF 1.4 and 9.0 for TF 1.6\r\n- Bazel version: N/A\r\n- GPU model and memory: tested on GTX 960M and GTX 1080\r\n- Exact command to reproduce: N/A\r\n\r\n\r\n### Describe the problem\r\nI am unable to reproduce random numbers generated from numpy when I use it in combination with TF. In the beginning of all my tests, I set\r\n```\r\ntf.set_random_seed(seed)\r\nnp.random.seed(seed)\r\n```\r\n\r\nI have been debugging, and when I use numpy and no TF, all results are reproducible. When I add the TF code, the random numbers stop being reproducible. When I use both TF and numpy, I get the following results:\r\n\r\n1. TF variables are initialized to the same value every time (OK)\r\n2. When I use `np.random.RandomState()` with a set seed instead of direct calls to `np.random.uniform()`, `np.random.normal()`, etc, results are reproducible (OK)\r\n3. When I use direct calls to `np.random.uniform()`, `np.random.normal()`, etc, results are reproducible on CPU but not on GPU (NOT OK)\r\n1080\r\nSince the results are reproducible when using CPU but not GPU, it made me think that this might be a possible bug. \r\n\r\nI am not using any threads so the problem is definitely not caused by race conditions. I am monitoring reproducibility of results only by the random numbers which are generated, which are not in any way affected by the training results from the TF neural net. What is really strange is that the piece of code that seems to be affecting the results is the part about computing and backpropagating gradients. I do not expect that this uses any random numbers generated by numpy in the backend. Furthermore, even if it did, the order of my calls to `np.random` and to `sess.run` is always deterministic, so the same random numbers should be observed between separate runs. \r\n\r\nMy code is somewhat too big at the moment to post. I can try to compile some simple example where the issue occurs, but I first wanted to make sure that this is indeed not the expected behavior.\r\n"}