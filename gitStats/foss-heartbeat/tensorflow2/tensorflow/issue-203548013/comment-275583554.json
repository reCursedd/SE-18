{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275583554", "html_url": "https://github.com/tensorflow/tensorflow/issues/7106#issuecomment-275583554", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7106", "id": 275583554, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTU4MzU1NA==", "user": {"login": "act65", "id": 6046380, "node_id": "MDQ6VXNlcjYwNDYzODA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6046380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/act65", "html_url": "https://github.com/act65", "followers_url": "https://api.github.com/users/act65/followers", "following_url": "https://api.github.com/users/act65/following{/other_user}", "gists_url": "https://api.github.com/users/act65/gists{/gist_id}", "starred_url": "https://api.github.com/users/act65/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/act65/subscriptions", "organizations_url": "https://api.github.com/users/act65/orgs", "repos_url": "https://api.github.com/users/act65/repos", "events_url": "https://api.github.com/users/act65/events{/privacy}", "received_events_url": "https://api.github.com/users/act65/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-27T03:52:50Z", "updated_at": "2017-01-27T03:52:50Z", "author_association": "NONE", "body_html": "<p>We would like audio summaries to work with the <a href=\"https://www.tensorflow.org/how_tos/embedding_viz/\" rel=\"nofollow\">embedding visualiser</a>. Much like there are thumbnails of each mnist image (see link), we would like to be able to click on an embedded datapoint; to listen to it (currently we are just using the spectrograms as image thumbnails) and to figure out where it came from (which part of which audio recording).</p>", "body_text": "We would like audio summaries to work with the embedding visualiser. Much like there are thumbnails of each mnist image (see link), we would like to be able to click on an embedded datapoint; to listen to it (currently we are just using the spectrograms as image thumbnails) and to figure out where it came from (which part of which audio recording).", "body": "We would like audio summaries to work with the [embedding visualiser](https://www.tensorflow.org/how_tos/embedding_viz/). Much like there are thumbnails of each mnist image (see link), we would like to be able to click on an embedded datapoint; to listen to it (currently we are just using the spectrograms as image thumbnails) and to figure out where it came from (which part of which audio recording)."}