{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8244", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8244/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8244/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8244/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8244", "id": 213109516, "node_id": "MDU6SXNzdWUyMTMxMDk1MTY=", "number": 8244, "title": "[Java API] Tensor.create() slow for large arrays", "user": {"login": "mdymczyk", "id": 1208615, "node_id": "MDQ6VXNlcjEyMDg2MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1208615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mdymczyk", "html_url": "https://github.com/mdymczyk", "followers_url": "https://api.github.com/users/mdymczyk/followers", "following_url": "https://api.github.com/users/mdymczyk/following{/other_user}", "gists_url": "https://api.github.com/users/mdymczyk/gists{/gist_id}", "starred_url": "https://api.github.com/users/mdymczyk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mdymczyk/subscriptions", "organizations_url": "https://api.github.com/users/mdymczyk/orgs", "repos_url": "https://api.github.com/users/mdymczyk/repos", "events_url": "https://api.github.com/users/mdymczyk/events{/privacy}", "received_events_url": "https://api.github.com/users/mdymczyk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2017-03-09T17:29:30Z", "updated_at": "2018-09-22T14:55:54Z", "closed_at": "2017-03-10T04:28:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The current Java API's <code>Tensor.create(Object)</code> is really slow - for a batch of 128 images of size 224x224x3 it's taking around 1.5seconds. To put this into perspective <code>runner.run()</code> with that data and an InceptionV3 graph took below 1second so data prep is x1.5 of the runtime here (for a batch of 32 images it's around 0.35-0.45sec).</p>\n<p>Is this working as intended? When running the Python code (using simple <code>sess.run(fetches, feed_dict=feed_dict)</code>) with which the graph meta file was generated (TF 1.0.1) and feeding a Python array I don't see such hiccups, the speed is the same as the Java <code>runner.run()</code>.</p>\n<p>Might it be because of build flags used, maybe I'm missing some optimizations?</p>\n<p>For now this small part is killing the whole performance, bringing it down from 130obs/sec (<code>runner.run()</code> time) to about ~45obs/sec (Tensor.create+run()).</p>\n<p>A bit of a sidenote, the performance page states:</p>\n<blockquote>\n<p>This will result in poor performance.<br>\nsess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})</p>\n</blockquote>\n<p>But currently there's no other way to feed data from the Java API, right? A queue (able to read from a file and from memory, i.e. from a Java structure) would be amazing.</p>\n<h3>Jar build command</h3>\n<pre><code>export CC=\"/usr/bin/gcc\"\nexport CXX=\"/usr/bin/g++\"\nexport TF_NEED_CUDA=1\nexport GCC_HOST_COMPILER_PATH=$CC\nexport BUILDFLAGS=\"--config=cuda --copt=-m64 --linkopt=-m64 --copt=-march=native\"\n\nbazel build -c opt \\\n  //tensorflow/java:tensorflow \\\n  //tensorflow/java:libtensorflow_jni \\\n  $BUILDFLAGS --spawn_strategy=standalone --genrule_strategy=standalone\n</code></pre>\n<h3>Environment info</h3>\n<p><strong>OS:</strong> Ubuntu 16.04<br>\n<strong>GPU:</strong> GPU TITAN X (Pascal) 12GB<br>\n<strong>CPU:</strong> Intel\u00ae Xeon\u00ae Processor E5-2630 v4 10core<br>\n<strong>GPU Drivers:</strong><br>\nNVidia CUDA Driver Version: 375.39<br>\nCUDNN 5.1.5<br>\nCUDA 8<br>\n<strong>Tensorflow version:</strong> JAR file built from current master (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/c25ecb53ae0a8b3ebe4b30238433a7588dbe8537/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/c25ecb53ae0a8b3ebe4b30238433a7588dbe8537\"><tt>c25ecb5</tt></a>)</p>\n<h3>Example</h3>\n<div class=\"highlight highlight-source-java\"><pre><span class=\"pl-k\">public</span> <span class=\"pl-k\">void</span> test() {\n  <span class=\"pl-smi\">Random</span> r <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">Random</span>();\n  <span class=\"pl-k\">int</span> imageSize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">224</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">224</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>;\n  <span class=\"pl-k\">int</span> batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>;\n  <span class=\"pl-k\">float</span>[][] input <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">float</span>[batch][imageSize];\n  <span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>; i <span class=\"pl-k\">&lt;</span> batch; i<span class=\"pl-k\">++</span>) {\n    <span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> j <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>; j <span class=\"pl-k\">&lt;</span> imageSize; j<span class=\"pl-k\">++</span>) {\n      input[i][j] <span class=\"pl-k\">=</span> r<span class=\"pl-k\">.</span>nextFloat();\n    }\n  }\n\n  <span class=\"pl-k\">long</span> start <span class=\"pl-k\">=</span> <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>nanoTime();\n  <span class=\"pl-smi\">Tensor</span><span class=\"pl-k\">.</span>create(input);\n  <span class=\"pl-k\">long</span> end <span class=\"pl-k\">=</span> <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>nanoTime();\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> Around 1.5sec</span>\n  <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>out<span class=\"pl-k\">.</span>println(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Took: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> (end <span class=\"pl-k\">-</span> start));\n}</pre></div>", "body_text": "The current Java API's Tensor.create(Object) is really slow - for a batch of 128 images of size 224x224x3 it's taking around 1.5seconds. To put this into perspective runner.run() with that data and an InceptionV3 graph took below 1second so data prep is x1.5 of the runtime here (for a batch of 32 images it's around 0.35-0.45sec).\nIs this working as intended? When running the Python code (using simple sess.run(fetches, feed_dict=feed_dict)) with which the graph meta file was generated (TF 1.0.1) and feeding a Python array I don't see such hiccups, the speed is the same as the Java runner.run().\nMight it be because of build flags used, maybe I'm missing some optimizations?\nFor now this small part is killing the whole performance, bringing it down from 130obs/sec (runner.run() time) to about ~45obs/sec (Tensor.create+run()).\nA bit of a sidenote, the performance page states:\n\nThis will result in poor performance.\nsess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\nBut currently there's no other way to feed data from the Java API, right? A queue (able to read from a file and from memory, i.e. from a Java structure) would be amazing.\nJar build command\nexport CC=\"/usr/bin/gcc\"\nexport CXX=\"/usr/bin/g++\"\nexport TF_NEED_CUDA=1\nexport GCC_HOST_COMPILER_PATH=$CC\nexport BUILDFLAGS=\"--config=cuda --copt=-m64 --linkopt=-m64 --copt=-march=native\"\n\nbazel build -c opt \\\n  //tensorflow/java:tensorflow \\\n  //tensorflow/java:libtensorflow_jni \\\n  $BUILDFLAGS --spawn_strategy=standalone --genrule_strategy=standalone\n\nEnvironment info\nOS: Ubuntu 16.04\nGPU: GPU TITAN X (Pascal) 12GB\nCPU: Intel\u00ae Xeon\u00ae Processor E5-2630 v4 10core\nGPU Drivers:\nNVidia CUDA Driver Version: 375.39\nCUDNN 5.1.5\nCUDA 8\nTensorflow version: JAR file built from current master (c25ecb5)\nExample\npublic void test() {\n  Random r = new Random();\n  int imageSize = 224 * 224 * 3;\n  int batch = 128;\n  float[][] input = new float[batch][imageSize];\n  for(int i = 0; i < batch; i++) {\n    for(int j = 0; j < imageSize; j++) {\n      input[i][j] = r.nextFloat();\n    }\n  }\n\n  long start = System.nanoTime();\n  Tensor.create(input);\n  long end = System.nanoTime();\n  // Around 1.5sec\n  System.out.println(\"Took: \" + (end - start));\n}", "body": "The current Java API's `Tensor.create(Object)` is really slow - for a batch of 128 images of size 224x224x3 it's taking around 1.5seconds. To put this into perspective `runner.run()` with that data and an InceptionV3 graph took below 1second so data prep is x1.5 of the runtime here (for a batch of 32 images it's around 0.35-0.45sec).\r\n\r\nIs this working as intended? When running the Python code (using simple `sess.run(fetches, feed_dict=feed_dict)`) with which the graph meta file was generated (TF 1.0.1) and feeding a Python array I don't see such hiccups, the speed is the same as the Java `runner.run()`.\r\n\r\nMight it be because of build flags used, maybe I'm missing some optimizations?\r\n\r\nFor now this small part is killing the whole performance, bringing it down from 130obs/sec (`runner.run()` time) to about ~45obs/sec (Tensor.create+run()).\r\n\r\nA bit of a sidenote, the performance page states:\r\n\r\n> This will result in poor performance.\r\n> sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\n\r\nBut currently there's no other way to feed data from the Java API, right? A queue (able to read from a file and from memory, i.e. from a Java structure) would be amazing.\r\n\r\n### Jar build command\r\n```\r\nexport CC=\"/usr/bin/gcc\"\r\nexport CXX=\"/usr/bin/g++\"\r\nexport TF_NEED_CUDA=1\r\nexport GCC_HOST_COMPILER_PATH=$CC\r\nexport BUILDFLAGS=\"--config=cuda --copt=-m64 --linkopt=-m64 --copt=-march=native\"\r\n\r\nbazel build -c opt \\\r\n  //tensorflow/java:tensorflow \\\r\n  //tensorflow/java:libtensorflow_jni \\\r\n  $BUILDFLAGS --spawn_strategy=standalone --genrule_strategy=standalone\r\n```\r\n\r\n### Environment info\r\n**OS:** Ubuntu 16.04\r\n**GPU:** GPU TITAN X (Pascal) 12GB\r\n**CPU:** Intel\u00ae Xeon\u00ae Processor E5-2630 v4 10core\r\n**GPU Drivers:** \r\nNVidia CUDA Driver Version: 375.39\r\nCUDNN 5.1.5\r\nCUDA 8\r\n**Tensorflow version:** JAR file built from current master (c25ecb53)\r\n\r\n### Example\r\n\r\n```java\r\npublic void test() {\r\n  Random r = new Random();\r\n  int imageSize = 224 * 224 * 3;\r\n  int batch = 128;\r\n  float[][] input = new float[batch][imageSize];\r\n  for(int i = 0; i < batch; i++) {\r\n    for(int j = 0; j < imageSize; j++) {\r\n      input[i][j] = r.nextFloat();\r\n    }\r\n  }\r\n\r\n  long start = System.nanoTime();\r\n  Tensor.create(input);\r\n  long end = System.nanoTime();\r\n  // Around 1.5sec\r\n  System.out.println(\"Took: \" + (end - start));\r\n}\r\n```"}