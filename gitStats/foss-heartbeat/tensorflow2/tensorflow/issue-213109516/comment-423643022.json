{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423643022", "html_url": "https://github.com/tensorflow/tensorflow/issues/8244#issuecomment-423643022", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8244", "id": 423643022, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzY0MzAyMg==", "user": {"login": "AndreyFilippov", "id": 7122489, "node_id": "MDQ6VXNlcjcxMjI0ODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/7122489?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreyFilippov", "html_url": "https://github.com/AndreyFilippov", "followers_url": "https://api.github.com/users/AndreyFilippov/followers", "following_url": "https://api.github.com/users/AndreyFilippov/following{/other_user}", "gists_url": "https://api.github.com/users/AndreyFilippov/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreyFilippov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreyFilippov/subscriptions", "organizations_url": "https://api.github.com/users/AndreyFilippov/orgs", "repos_url": "https://api.github.com/users/AndreyFilippov/repos", "events_url": "https://api.github.com/users/AndreyFilippov/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreyFilippov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-21T19:14:03Z", "updated_at": "2018-09-21T19:14:03Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a>, is there any way to reuse tensors of the same shape by just copying data without reallocating memory (maybe handling endiness outside)? Eventually we would like to feed directly from the non-tensorflow GPU memory (that seems recently to become possible - <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/a1d6179adb1ca6208281ed955860c319525edf75/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/a1d6179adb1ca6208281ed955860c319525edf75\"><tt>a1d6179</tt></a>) but for now we would like to eliminate all unnecessary memory copying - the source float[] data is created by a multi-threaded application. Without it the inference takes less time than source data copying (even in Python).</p>", "body_text": "@asimshankar, is there any way to reuse tensors of the same shape by just copying data without reallocating memory (maybe handling endiness outside)? Eventually we would like to feed directly from the non-tensorflow GPU memory (that seems recently to become possible - a1d6179) but for now we would like to eliminate all unnecessary memory copying - the source float[] data is created by a multi-threaded application. Without it the inference takes less time than source data copying (even in Python).", "body": "@asimshankar, is there any way to reuse tensors of the same shape by just copying data without reallocating memory (maybe handling endiness outside)? Eventually we would like to feed directly from the non-tensorflow GPU memory (that seems recently to become possible - https://github.com/tensorflow/tensorflow/commit/a1d6179adb1ca6208281ed955860c319525edf75) but for now we would like to eliminate all unnecessary memory copying - the source float[] data is created by a multi-threaded application. Without it the inference takes less time than source data copying (even in Python)."}