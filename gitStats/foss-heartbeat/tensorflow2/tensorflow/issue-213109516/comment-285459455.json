{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285459455", "html_url": "https://github.com/tensorflow/tensorflow/issues/8244#issuecomment-285459455", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8244", "id": 285459455, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTQ1OTQ1NQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-09T19:44:25Z", "updated_at": "2017-03-09T20:11:07Z", "author_association": "MEMBER", "body_html": "<p>Thanks for the detailed description and the sample code, it is very much appreciated!</p>\n<p>The <code>create(Object)</code> method call involves use of reflection to determine the shape and copy things over one array at a time, so it is pretty slow, especially as you add dimensions. The <a href=\"https://goo.gl/aGbcSB\" rel=\"nofollow\"><code>create(shape, FloatBuffer)</code></a> method would be an order-of-magnitude faster. For example:</p>\n<div class=\"highlight highlight-source-java\"><pre><span class=\"pl-k\">public</span> <span class=\"pl-k\">void</span> test() {\n    <span class=\"pl-smi\">Random</span> r <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">Random</span>();\n    <span class=\"pl-k\">int</span> imageSize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">224</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">224</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>;\n    <span class=\"pl-k\">int</span> batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>;\n    <span class=\"pl-k\">long</span>[] shape <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">long</span>[] {batch, imageSize};\n    <span class=\"pl-smi\">FloatBuffer</span> buf <span class=\"pl-k\">=</span> <span class=\"pl-smi\">FloatBuffer</span><span class=\"pl-k\">.</span>allocate(imageSize <span class=\"pl-k\">*</span> batch);\n    <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>; i <span class=\"pl-k\">&lt;</span> imageSize <span class=\"pl-k\">*</span> batch; <span class=\"pl-k\">++</span>i) {\n      buf<span class=\"pl-k\">.</span>put(r<span class=\"pl-k\">.</span>nextFloat());\n    }\n    buf<span class=\"pl-k\">.</span>flip();\n\n    <span class=\"pl-k\">long</span> start <span class=\"pl-k\">=</span> <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>nanoTime();\n    <span class=\"pl-smi\">Tensor</span><span class=\"pl-k\">.</span>create(shape, buf);\n    <span class=\"pl-k\">long</span> end <span class=\"pl-k\">=</span> <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>nanoTime();\n    <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>out<span class=\"pl-k\">.</span>println(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Took: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> (end <span class=\"pl-k\">-</span> start));\n}</pre></div>\n<p>This is still slower than I'd want it to be, have to dig into that, but hopefully it is enough to satisfy your current needs (and session execution should be faster than Python).</p>\n<p>Regarding your other question: Yes, using feeds is slower than getting input from queues. While I believe we do have the primitives to enable use of queues from any language, it is admittedly not too easy (as you have to figure out what is being done in python - e.g., start threads that run the enqueue op - and duplicate that). Note that, as per the proposal in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"210920003\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7951\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7951\">#7951</a> - investing into queues in other languages might not be worth while at this stage.</p>\n<p>Do let me know if using the <code>FloatBuffer</code> suffices for now (and I can close this issue, while we look into general performance improvements for the Java API).</p>", "body_text": "Thanks for the detailed description and the sample code, it is very much appreciated!\nThe create(Object) method call involves use of reflection to determine the shape and copy things over one array at a time, so it is pretty slow, especially as you add dimensions. The create(shape, FloatBuffer) method would be an order-of-magnitude faster. For example:\npublic void test() {\n    Random r = new Random();\n    int imageSize = 224 * 224 * 3;\n    int batch = 128;\n    long[] shape = new long[] {batch, imageSize};\n    FloatBuffer buf = FloatBuffer.allocate(imageSize * batch);\n    for (int i = 0; i < imageSize * batch; ++i) {\n      buf.put(r.nextFloat());\n    }\n    buf.flip();\n\n    long start = System.nanoTime();\n    Tensor.create(shape, buf);\n    long end = System.nanoTime();\n    System.out.println(\"Took: \" + (end - start));\n}\nThis is still slower than I'd want it to be, have to dig into that, but hopefully it is enough to satisfy your current needs (and session execution should be faster than Python).\nRegarding your other question: Yes, using feeds is slower than getting input from queues. While I believe we do have the primitives to enable use of queues from any language, it is admittedly not too easy (as you have to figure out what is being done in python - e.g., start threads that run the enqueue op - and duplicate that). Note that, as per the proposal in #7951 - investing into queues in other languages might not be worth while at this stage.\nDo let me know if using the FloatBuffer suffices for now (and I can close this issue, while we look into general performance improvements for the Java API).", "body": "Thanks for the detailed description and the sample code, it is very much appreciated!\r\n\r\nThe `create(Object)` method call involves use of reflection to determine the shape and copy things over one array at a time, so it is pretty slow, especially as you add dimensions. The [`create(shape, FloatBuffer)`](https://goo.gl/aGbcSB) method would be an order-of-magnitude faster. For example:\r\n\r\n```java\r\npublic void test() {\r\n    Random r = new Random();\r\n    int imageSize = 224 * 224 * 3;\r\n    int batch = 128;\r\n    long[] shape = new long[] {batch, imageSize};\r\n    FloatBuffer buf = FloatBuffer.allocate(imageSize * batch);\r\n    for (int i = 0; i < imageSize * batch; ++i) {\r\n      buf.put(r.nextFloat());\r\n    }\r\n    buf.flip();\r\n\r\n    long start = System.nanoTime();\r\n    Tensor.create(shape, buf);\r\n    long end = System.nanoTime();\r\n    System.out.println(\"Took: \" + (end - start));\r\n}\r\n```\r\n\r\nThis is still slower than I'd want it to be, have to dig into that, but hopefully it is enough to satisfy your current needs (and session execution should be faster than Python).\r\n\r\nRegarding your other question: Yes, using feeds is slower than getting input from queues. While I believe we do have the primitives to enable use of queues from any language, it is admittedly not too easy (as you have to figure out what is being done in python - e.g., start threads that run the enqueue op - and duplicate that). Note that, as per the proposal in #7951 - investing into queues in other languages might not be worth while at this stage.\r\n\r\nDo let me know if using the `FloatBuffer` suffices for now (and I can close this issue, while we look into general performance improvements for the Java API)."}