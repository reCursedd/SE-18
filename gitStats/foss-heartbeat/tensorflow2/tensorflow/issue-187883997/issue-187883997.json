{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5470", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5470/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5470/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5470/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5470", "id": 187883997, "node_id": "MDU6SXNzdWUxODc4ODM5OTc=", "number": 5470, "title": "Cannot import graph_def for 8-bit Quantized cnn model", "user": {"login": "ResByte", "id": 4814781, "node_id": "MDQ6VXNlcjQ4MTQ3ODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/4814781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ResByte", "html_url": "https://github.com/ResByte", "followers_url": "https://api.github.com/users/ResByte/followers", "following_url": "https://api.github.com/users/ResByte/following{/other_user}", "gists_url": "https://api.github.com/users/ResByte/gists{/gist_id}", "starred_url": "https://api.github.com/users/ResByte/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ResByte/subscriptions", "organizations_url": "https://api.github.com/users/ResByte/orgs", "repos_url": "https://api.github.com/users/ResByte/repos", "events_url": "https://api.github.com/users/ResByte/events{/privacy}", "received_events_url": "https://api.github.com/users/ResByte/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2016-11-08T02:15:35Z", "updated_at": "2016-11-21T16:46:10Z", "closed_at": "2016-11-21T16:46:10Z", "author_association": "NONE", "body_html": "<p>Following steps I did:</p>\n<ol>\n<li>Train a CNN model on MNIST using tensorflow tutorials(Deep MNIST for Experts)(CONV+CONV+FC). Saved this model as binary protobuf(.pb).</li>\n<li>Using 8-bit Quantization api, converted previously saved model to 8-bit model as binary proto(.pb).</li>\n<li>Read 8-bit binary-proto to tensorflow, by first creating graph_def and importing graph def to tf.Session()</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> sess.as_default() :\n   <span class=\"pl-k\">with</span> tf.Graph().as_default():\n       graph_def <span class=\"pl-k\">=</span> tf.GraphDef()\n             <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(input_file_name, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n                 proto_b <span class=\"pl-k\">=</span> f.read()\n                 graph_def.ParseFromString(proto_b)\t\n                 _ <span class=\"pl-k\">=</span> tf.import_graph_def(graph_def, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>However, this gives error on tf.import_graph_def as<br>\n<code>ValueError: graph_def is invalid at node u'concat_eightbit_reshape_concat/values_0': Input tensor 'concat/values_0:0' Cannot convert a tensor of type int32 to an input of type float32.</code></p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 16.04 + tensorflow 0.11.0rc2</p>\n<p>Installed version of CUDA and cuDNN: No</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed: <a href=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl</a></li>\n</ol>\n<h3>What other attempted solutions have you tried?</h3>\n<ol>\n<li>I am able to correctly import quantized graph def for pre-trained inception model(.pb).</li>\n<li>When I define Multi-layer perceptron model for MNIST, it can import quantized graph_def.</li>\n<li>All the quantization headers are imported correctly as</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> tensorflow.contrib.quantization <span class=\"pl-k\">import</span> load_quantized_ops_so\n<span class=\"pl-k\">from</span> tensorflow.contrib.quantization.kernels <span class=\"pl-k\">import</span> load_quantized_kernels_so\nload_quantized_ops_so.Load()\nload_quantized_kernels_so.Load()</pre></div>", "body_text": "Following steps I did:\n\nTrain a CNN model on MNIST using tensorflow tutorials(Deep MNIST for Experts)(CONV+CONV+FC). Saved this model as binary protobuf(.pb).\nUsing 8-bit Quantization api, converted previously saved model to 8-bit model as binary proto(.pb).\nRead 8-bit binary-proto to tensorflow, by first creating graph_def and importing graph def to tf.Session()\n\nwith sess.as_default() :\n   with tf.Graph().as_default():\n       graph_def = tf.GraphDef()\n             with open(input_file_name, 'rb') as f:\n                 proto_b = f.read()\n                 graph_def.ParseFromString(proto_b)\t\n                 _ = tf.import_graph_def(graph_def, name=\"\")\nHowever, this gives error on tf.import_graph_def as\nValueError: graph_def is invalid at node u'concat_eightbit_reshape_concat/values_0': Input tensor 'concat/values_0:0' Cannot convert a tensor of type int32 to an input of type float32.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System: Ubuntu 16.04 + tensorflow 0.11.0rc2\nInstalled version of CUDA and cuDNN: No\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\n\nWhat other attempted solutions have you tried?\n\nI am able to correctly import quantized graph def for pre-trained inception model(.pb).\nWhen I define Multi-layer perceptron model for MNIST, it can import quantized graph_def.\nAll the quantization headers are imported correctly as\n\nfrom tensorflow.contrib.quantization import load_quantized_ops_so\nfrom tensorflow.contrib.quantization.kernels import load_quantized_kernels_so\nload_quantized_ops_so.Load()\nload_quantized_kernels_so.Load()", "body": "Following steps I did:\r\n1. Train a CNN model on MNIST using tensorflow tutorials(Deep MNIST for Experts)(CONV+CONV+FC). Saved this model as binary protobuf(.pb).\r\n2. Using 8-bit Quantization api, converted previously saved model to 8-bit model as binary proto(.pb). \r\n3. Read 8-bit binary-proto to tensorflow, by first creating graph_def and importing graph def to tf.Session()\r\n```python\r\nwith sess.as_default() :\r\n   with tf.Graph().as_default():\r\n       graph_def = tf.GraphDef()\r\n             with open(input_file_name, 'rb') as f:\r\n                 proto_b = f.read()\r\n                 graph_def.ParseFromString(proto_b)\t\r\n                 _ = tf.import_graph_def(graph_def, name=\"\")\r\n``` \r\n\r\nHowever, this gives error on tf.import_graph_def as\r\n`ValueError: graph_def is invalid at node u'concat_eightbit_reshape_concat/values_0': Input tensor 'concat/values_0:0' Cannot convert a tensor of type int32 to an input of type float32.`\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04 + tensorflow 0.11.0rc2\r\n\r\nInstalled version of CUDA and cuDNN: No\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\r\n\r\n\r\n\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n1. I am able to correctly import quantized graph def for pre-trained inception model(.pb).\r\n2. When I define Multi-layer perceptron model for MNIST, it can import quantized graph_def. \r\n3. All the quantization headers are imported correctly as\r\n``` python\r\nfrom tensorflow.contrib.quantization import load_quantized_ops_so\r\nfrom tensorflow.contrib.quantization.kernels import load_quantized_kernels_so\r\nload_quantized_ops_so.Load()\r\nload_quantized_kernels_so.Load()\r\n``` \r\n\r\n"}