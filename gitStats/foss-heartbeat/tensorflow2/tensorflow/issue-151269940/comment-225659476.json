{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225659476", "html_url": "https://github.com/tensorflow/tensorflow/issues/2121#issuecomment-225659476", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2121", "id": 225659476, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTY1OTQ3Ng==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-13T18:00:07Z", "updated_at": "2016-06-13T18:00:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4404828\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/liuyipei\">@liuyipei</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=501890\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yosinski\">@yosinski</a>, in order to debug this problem, it is very useful to list all threads information, even better with call stacks, at least for the unique ones. Also please run the same experiment with environmental variable \"CUDA_LAUNCH_BLOCKING=1\" set.</p>\n<p>That would help us pin-point where the hang is happening.</p>\n<p>There are a number of problems described here. Not sure all of them are the same one. But on the surface, \"query event: CUDA_ERROR_LAUNCH_TIMEOUT\" implies a \"cuEventQuery\" call fails with a \"CUDA_ERROR_LAUNCH_TIMEOUT\" status. Since \"cuEventQuery\" itself is non-blocking, it is quite strange that it could fail with timeout. So another possibility is some other kernel is failing, and cuEventQuery is the one that detects that. The \"CUDA_LAUNCH_BLOCKING=1\" should tell us which Cuda calls is making the trouble.</p>\n<p>One possibility is that we are deadlock within the Cuda library itself. Since we don't have source code, debugging that would be more difficult.</p>", "body_text": "@ppwwyyxx, @liuyipei, @yosinski, in order to debug this problem, it is very useful to list all threads information, even better with call stacks, at least for the unique ones. Also please run the same experiment with environmental variable \"CUDA_LAUNCH_BLOCKING=1\" set.\nThat would help us pin-point where the hang is happening.\nThere are a number of problems described here. Not sure all of them are the same one. But on the surface, \"query event: CUDA_ERROR_LAUNCH_TIMEOUT\" implies a \"cuEventQuery\" call fails with a \"CUDA_ERROR_LAUNCH_TIMEOUT\" status. Since \"cuEventQuery\" itself is non-blocking, it is quite strange that it could fail with timeout. So another possibility is some other kernel is failing, and cuEventQuery is the one that detects that. The \"CUDA_LAUNCH_BLOCKING=1\" should tell us which Cuda calls is making the trouble.\nOne possibility is that we are deadlock within the Cuda library itself. Since we don't have source code, debugging that would be more difficult.", "body": "@ppwwyyxx, @liuyipei, @yosinski, in order to debug this problem, it is very useful to list all threads information, even better with call stacks, at least for the unique ones. Also please run the same experiment with environmental variable \"CUDA_LAUNCH_BLOCKING=1\" set. \n\nThat would help us pin-point where the hang is happening. \n\nThere are a number of problems described here. Not sure all of them are the same one. But on the surface, \"query event: CUDA_ERROR_LAUNCH_TIMEOUT\" implies a \"cuEventQuery\" call fails with a \"CUDA_ERROR_LAUNCH_TIMEOUT\" status. Since \"cuEventQuery\" itself is non-blocking, it is quite strange that it could fail with timeout. So another possibility is some other kernel is failing, and cuEventQuery is the one that detects that. The \"CUDA_LAUNCH_BLOCKING=1\" should tell us which Cuda calls is making the trouble. \n\nOne possibility is that we are deadlock within the Cuda library itself. Since we don't have source code, debugging that would be more difficult. \n"}