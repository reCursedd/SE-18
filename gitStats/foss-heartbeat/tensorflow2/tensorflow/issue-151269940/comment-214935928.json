{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/214935928", "html_url": "https://github.com/tensorflow/tensorflow/issues/2121#issuecomment-214935928", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2121", "id": 214935928, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNDkzNTkyOA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-27T01:11:53Z", "updated_at": "2016-04-27T01:11:53Z", "author_association": "MEMBER", "body_html": "<p>This kind of problem is very hard to diagnose.  Given that we have a long track record of running inception variants on TensorFlow, it's unlikely to be a simple logic error in the TensorFlow source.  Are you running any Ops you've defined yourself, or custom kernels?  If not, I would suspect either a mutual problem between TF and CUDA 7.5 or something in your hardware environment.  To diagnose that, you probably need to use the NVIDIA tools and also anything else that might indicate problems with the PCIe bus(es) to which your GPUs are attached.</p>\n<p>How many GPUs is your model using?<br>\nWhen a GPU freezes like this, is it always the same one?  If so, try running the model without that one, just using the others.<br>\nTake a look at the full<br>\nnvidia-smi -q<br>\noutput.  It might be that the memory utilization number is bogus.<br>\nIf you can try using CUDA 7.0, that might be worth a shot.</p>\n<p>The first thread that you've traced is just the python thread that's issued a session run request.  The model executes within the C++/CUDA binary.  However, if the GPU has gotten into some kind of infinite loop, you won't see much indication of why in any of the thread stacks: you'll likely be able to see that an executor thread is waiting for some Op to terminate, but not which one.  Although it's unlikely to be of much help, if you're willing to modify the source and recompile you can log the names of ops before/after execution by modifying common_runtime/executor.cc.</p>", "body_text": "This kind of problem is very hard to diagnose.  Given that we have a long track record of running inception variants on TensorFlow, it's unlikely to be a simple logic error in the TensorFlow source.  Are you running any Ops you've defined yourself, or custom kernels?  If not, I would suspect either a mutual problem between TF and CUDA 7.5 or something in your hardware environment.  To diagnose that, you probably need to use the NVIDIA tools and also anything else that might indicate problems with the PCIe bus(es) to which your GPUs are attached.\nHow many GPUs is your model using?\nWhen a GPU freezes like this, is it always the same one?  If so, try running the model without that one, just using the others.\nTake a look at the full\nnvidia-smi -q\noutput.  It might be that the memory utilization number is bogus.\nIf you can try using CUDA 7.0, that might be worth a shot.\nThe first thread that you've traced is just the python thread that's issued a session run request.  The model executes within the C++/CUDA binary.  However, if the GPU has gotten into some kind of infinite loop, you won't see much indication of why in any of the thread stacks: you'll likely be able to see that an executor thread is waiting for some Op to terminate, but not which one.  Although it's unlikely to be of much help, if you're willing to modify the source and recompile you can log the names of ops before/after execution by modifying common_runtime/executor.cc.", "body": "This kind of problem is very hard to diagnose.  Given that we have a long track record of running inception variants on TensorFlow, it's unlikely to be a simple logic error in the TensorFlow source.  Are you running any Ops you've defined yourself, or custom kernels?  If not, I would suspect either a mutual problem between TF and CUDA 7.5 or something in your hardware environment.  To diagnose that, you probably need to use the NVIDIA tools and also anything else that might indicate problems with the PCIe bus(es) to which your GPUs are attached.\n\nHow many GPUs is your model using?\nWhen a GPU freezes like this, is it always the same one?  If so, try running the model without that one, just using the others.\nTake a look at the full\n  nvidia-smi -q\noutput.  It might be that the memory utilization number is bogus.  \nIf you can try using CUDA 7.0, that might be worth a shot.\n\nThe first thread that you've traced is just the python thread that's issued a session run request.  The model executes within the C++/CUDA binary.  However, if the GPU has gotten into some kind of infinite loop, you won't see much indication of why in any of the thread stacks: you'll likely be able to see that an executor thread is waiting for some Op to terminate, but not which one.  Although it's unlikely to be of much help, if you're willing to modify the source and recompile you can log the names of ops before/after execution by modifying common_runtime/executor.cc.\n"}