{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2121", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2121/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2121/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2121/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2121", "id": 151269940, "node_id": "MDU6SXNzdWUxNTEyNjk5NDA=", "number": 2121, "title": "TF freezes while running.", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284463744, "node_id": "MDU6TGFiZWwyODQ0NjM3NDQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cuda", "name": "cuda", "color": "f7c6c7", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2016-04-27T00:26:20Z", "updated_at": "2016-11-02T18:57:04Z", "closed_at": "2016-08-16T00:29:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Operating System: CentOS7<br>\nInstalled version of CUDA and cuDNN: 7.5.18; 4.0.7<br>\nPackage: Python2 GPU nightly built on Apr 23.</p>\n<p>While I'm training an inception-like model on multi GPUs, the training totally freezed after hours.<br>\nThen I could see one of the GPUs stay at a 99~100% GPU utilization and a 0% GPU memory utilization. Others stayed at 0%:</p>\n<pre><code>$ nvidia-smi --query-gpu=temperature.gpu,clocks.current.sm,power.draw,utilization.gpu,utilization.memory,memory.free --format=csv | column -t -s ,\ntemperature.gpu   clocks.current.sm [MHz]   power.draw [W]   utilization.gpu [%]   utilization.memory [%]   memory.free [MiB]\n53                987 MHz                   71.80 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.33 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.81 W          0 %                   0 %                      38 MiB\n29                324 MHz                   14.34 W          0 %                   0 %                      38 MiB\n60                1151 MHz                  87.82 W          100 %                 0 %                      39 MiB\n54                987 MHz                   74.47 W          0 %                   0 %                      39 MiB\n28                324 MHz                   14.16 W          0 %                   0 %                      12264 MiB\n29                324 MHz                   14.06 W          0 %                   0 %                      12264 MiB\n</code></pre>\n<p>This problem happened to me several times, but still hard to reproduce (happen after hours of training).</p>\n<p>I attached the process with <code>gdb -p [proc]</code> and did stack trace on thread 1 (there are a lot other threads):</p>\n<pre><code>#0  0x00007fda5a3156d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x00007fda506ee9ec in std::condition_variable::wait(std::unique_lock&lt;std::mutex&gt;&amp;) () from /lib64/libstdc++.so.6\n#2  0x00007fda33a1ed13 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, long long) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fda33a28634 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&amp;, std::vector&lt;std::pair&lt;std::string, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::string, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*) () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fda33a2a8e2 in tensorflow::DirectSession::Run(std::vector&lt;std::pair&lt;std::string, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::string, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fda33c2ff87 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fda33c303d1 in TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fda32da7855 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector&lt;std::pair&lt;char const*, tagPyArrayObject*&gt;, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fda32da7ec1 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector&lt;std::pair&lt;char const*, tagPyArrayObject*&gt;, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fda32d93d17 in _wrap_TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fda5a606aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#11 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#12 0x00007fda5a594f68 in function_call () from /lib64/libpython2.7.so.1.0\n#13 0x00007fda5a5700b3 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#14 0x00007fda5a6032f7 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#15 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#16 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#17 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#18 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#19 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#20 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#21 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#22 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#23 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#24 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#25 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#26 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#27 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#28 0x00007fda5a6081c2 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#29 0x00007fda5a6215ff in run_mod () from /lib64/libpython2.7.so.1.0\n#30 0x00007fda5a6227be in PyRun_FileExFlags () from /lib64/libpython2.7.so.1.0\n#31 0x00007fda5a623a49 in PyRun_SimpleFileExFlags () from /lib64/libpython2.7.so.1.0\n#32 0x00007fda5a634b9f in Py_Main () from /lib64/libpython2.7.so.1.0\n#33 0x00007fda59861b15 in __libc_start_main () from /lib64/libc.so.6\n#34 0x0000000000400721 in _start ()\n</code></pre>\n<p>The process doesn't respond to Ctrl-C (SIGINT), but can be killed by SIGTERM.<br>\nI'm happy to try other methods to gather more information about the problem.</p>", "body_text": "Operating System: CentOS7\nInstalled version of CUDA and cuDNN: 7.5.18; 4.0.7\nPackage: Python2 GPU nightly built on Apr 23.\nWhile I'm training an inception-like model on multi GPUs, the training totally freezed after hours.\nThen I could see one of the GPUs stay at a 99~100% GPU utilization and a 0% GPU memory utilization. Others stayed at 0%:\n$ nvidia-smi --query-gpu=temperature.gpu,clocks.current.sm,power.draw,utilization.gpu,utilization.memory,memory.free --format=csv | column -t -s ,\ntemperature.gpu   clocks.current.sm [MHz]   power.draw [W]   utilization.gpu [%]   utilization.memory [%]   memory.free [MiB]\n53                987 MHz                   71.80 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.33 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.81 W          0 %                   0 %                      38 MiB\n29                324 MHz                   14.34 W          0 %                   0 %                      38 MiB\n60                1151 MHz                  87.82 W          100 %                 0 %                      39 MiB\n54                987 MHz                   74.47 W          0 %                   0 %                      39 MiB\n28                324 MHz                   14.16 W          0 %                   0 %                      12264 MiB\n29                324 MHz                   14.06 W          0 %                   0 %                      12264 MiB\n\nThis problem happened to me several times, but still hard to reproduce (happen after hours of training).\nI attached the process with gdb -p [proc] and did stack trace on thread 1 (there are a lot other threads):\n#0  0x00007fda5a3156d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x00007fda506ee9ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\n#2  0x00007fda33a1ed13 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, long long) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fda33a28634 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fda33a2a8e2 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fda33c2ff87 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fda33c303d1 in TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fda32da7855 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fda32da7ec1 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fda32d93d17 in _wrap_TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fda5a606aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#11 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#12 0x00007fda5a594f68 in function_call () from /lib64/libpython2.7.so.1.0\n#13 0x00007fda5a5700b3 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#14 0x00007fda5a6032f7 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#15 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#16 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#17 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#18 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#19 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#20 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#21 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#22 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#23 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#24 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#25 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#26 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#27 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#28 0x00007fda5a6081c2 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#29 0x00007fda5a6215ff in run_mod () from /lib64/libpython2.7.so.1.0\n#30 0x00007fda5a6227be in PyRun_FileExFlags () from /lib64/libpython2.7.so.1.0\n#31 0x00007fda5a623a49 in PyRun_SimpleFileExFlags () from /lib64/libpython2.7.so.1.0\n#32 0x00007fda5a634b9f in Py_Main () from /lib64/libpython2.7.so.1.0\n#33 0x00007fda59861b15 in __libc_start_main () from /lib64/libc.so.6\n#34 0x0000000000400721 in _start ()\n\nThe process doesn't respond to Ctrl-C (SIGINT), but can be killed by SIGTERM.\nI'm happy to try other methods to gather more information about the problem.", "body": "Operating System: CentOS7\nInstalled version of CUDA and cuDNN: 7.5.18; 4.0.7\nPackage: Python2 GPU nightly built on Apr 23.\n\nWhile I'm training an inception-like model on multi GPUs, the training totally freezed after hours.\nThen I could see one of the GPUs stay at a 99~100% GPU utilization and a 0% GPU memory utilization. Others stayed at 0%:\n\n```\n$ nvidia-smi --query-gpu=temperature.gpu,clocks.current.sm,power.draw,utilization.gpu,utilization.memory,memory.free --format=csv | column -t -s ,\ntemperature.gpu   clocks.current.sm [MHz]   power.draw [W]   utilization.gpu [%]   utilization.memory [%]   memory.free [MiB]\n53                987 MHz                   71.80 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.33 W          0 %                   0 %                      38 MiB\n30                324 MHz                   14.81 W          0 %                   0 %                      38 MiB\n29                324 MHz                   14.34 W          0 %                   0 %                      38 MiB\n60                1151 MHz                  87.82 W          100 %                 0 %                      39 MiB\n54                987 MHz                   74.47 W          0 %                   0 %                      39 MiB\n28                324 MHz                   14.16 W          0 %                   0 %                      12264 MiB\n29                324 MHz                   14.06 W          0 %                   0 %                      12264 MiB\n```\n\nThis problem happened to me several times, but still hard to reproduce (happen after hours of training).\n\nI attached the process with `gdb -p [proc]` and did stack trace on thread 1 (there are a lot other threads):\n\n```\n#0  0x00007fda5a3156d5 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x00007fda506ee9ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\n#2  0x00007fda33a1ed13 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, long long) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fda33a28634 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fda33a2a8e2 in tensorflow::DirectSession::Run(std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fda33c2ff87 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fda33c303d1 in TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fda32da7855 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fda32da7ec1 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00007fda32d93d17 in _wrap_TF_Run () from /home/wyx/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00007fda5a606aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#11 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#12 0x00007fda5a594f68 in function_call () from /lib64/libpython2.7.so.1.0\n#13 0x00007fda5a5700b3 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#14 0x00007fda5a6032f7 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#15 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#16 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#17 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#18 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#19 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#20 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#21 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#22 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#23 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#24 0x00007fda5a606860 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#25 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#26 0x00007fda5a60676f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#27 0x00007fda5a6080bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#28 0x00007fda5a6081c2 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#29 0x00007fda5a6215ff in run_mod () from /lib64/libpython2.7.so.1.0\n#30 0x00007fda5a6227be in PyRun_FileExFlags () from /lib64/libpython2.7.so.1.0\n#31 0x00007fda5a623a49 in PyRun_SimpleFileExFlags () from /lib64/libpython2.7.so.1.0\n#32 0x00007fda5a634b9f in Py_Main () from /lib64/libpython2.7.so.1.0\n#33 0x00007fda59861b15 in __libc_start_main () from /lib64/libc.so.6\n#34 0x0000000000400721 in _start ()\n```\n\nThe process doesn't respond to Ctrl-C (SIGINT), but can be killed by SIGTERM.\nI'm happy to try other methods to gather more information about the problem.\n"}