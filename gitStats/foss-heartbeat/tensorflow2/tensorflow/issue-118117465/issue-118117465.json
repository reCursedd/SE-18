{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/313", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/313/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/313/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/313/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/313", "id": 118117465, "node_id": "MDU6SXNzdWUxMTgxMTc0NjU=", "number": 313, "title": "Allow learning rates of specific variables to be scaled", "user": {"login": "alexatknit", "id": 15474222, "node_id": "MDQ6VXNlcjE1NDc0MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/15474222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexatknit", "html_url": "https://github.com/alexatknit", "followers_url": "https://api.github.com/users/alexatknit/followers", "following_url": "https://api.github.com/users/alexatknit/following{/other_user}", "gists_url": "https://api.github.com/users/alexatknit/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexatknit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexatknit/subscriptions", "organizations_url": "https://api.github.com/users/alexatknit/orgs", "repos_url": "https://api.github.com/users/alexatknit/repos", "events_url": "https://api.github.com/users/alexatknit/events{/privacy}", "received_events_url": "https://api.github.com/users/alexatknit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2015-11-20T20:12:08Z", "updated_at": "2016-01-22T06:21:51Z", "closed_at": "2015-11-24T19:43:56Z", "author_association": "NONE", "body_html": "<p>When repurposing a network, it can be useful to set layer-wise learning rates so that the final layer (which has random weights) does most of the learning initially. A useful pattern for applying learning rate scaling would be tf.stop_gradient(input, name=None), which can be seen as effectively scaling the learning rate by a factor of zero (obviously it also stops computation).</p>", "body_text": "When repurposing a network, it can be useful to set layer-wise learning rates so that the final layer (which has random weights) does most of the learning initially. A useful pattern for applying learning rate scaling would be tf.stop_gradient(input, name=None), which can be seen as effectively scaling the learning rate by a factor of zero (obviously it also stops computation).", "body": "When repurposing a network, it can be useful to set layer-wise learning rates so that the final layer (which has random weights) does most of the learning initially. A useful pattern for applying learning rate scaling would be tf.stop_gradient(input, name=None), which can be seen as effectively scaling the learning rate by a factor of zero (obviously it also stops computation).\n"}