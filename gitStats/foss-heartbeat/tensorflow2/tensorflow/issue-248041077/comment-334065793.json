{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/334065793", "html_url": "https://github.com/tensorflow/tensorflow/issues/12040#issuecomment-334065793", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12040", "id": 334065793, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDA2NTc5Mw==", "user": {"login": "sdschulze", "id": 30729315, "node_id": "MDQ6VXNlcjMwNzI5MzE1", "avatar_url": "https://avatars0.githubusercontent.com/u/30729315?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sdschulze", "html_url": "https://github.com/sdschulze", "followers_url": "https://api.github.com/users/sdschulze/followers", "following_url": "https://api.github.com/users/sdschulze/following{/other_user}", "gists_url": "https://api.github.com/users/sdschulze/gists{/gist_id}", "starred_url": "https://api.github.com/users/sdschulze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sdschulze/subscriptions", "organizations_url": "https://api.github.com/users/sdschulze/orgs", "repos_url": "https://api.github.com/users/sdschulze/repos", "events_url": "https://api.github.com/users/sdschulze/events{/privacy}", "received_events_url": "https://api.github.com/users/sdschulze/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-04T06:55:09Z", "updated_at": "2017-10-04T06:55:09Z", "author_association": "NONE", "body_html": "<p>I originally encountered this problem using real data; I just changed the example to random data to make it simpler and more portable, so other people can check if they can reproduce it.</p>\n<p>Whether or not large kernel sizes are useful is arguable, but they certainly should not trigger an NaN error.  I am pretty certain that this is a bug, as the error occurs spontaneously after very few iterations, and neither the variables nor the gradients explode, and I cannot think of a sensible reason why setting k &lt; 14 fixes it completely.  Even more suspiciously, setting use_locking=True in the AdamOptimizer seems to make the error appear a little later.  I am just not sure if it is a bug in Tensorflow itself or perhaps some CUDA library, so I would like to know if anyone else can reproduce it.</p>", "body_text": "I originally encountered this problem using real data; I just changed the example to random data to make it simpler and more portable, so other people can check if they can reproduce it.\nWhether or not large kernel sizes are useful is arguable, but they certainly should not trigger an NaN error.  I am pretty certain that this is a bug, as the error occurs spontaneously after very few iterations, and neither the variables nor the gradients explode, and I cannot think of a sensible reason why setting k < 14 fixes it completely.  Even more suspiciously, setting use_locking=True in the AdamOptimizer seems to make the error appear a little later.  I am just not sure if it is a bug in Tensorflow itself or perhaps some CUDA library, so I would like to know if anyone else can reproduce it.", "body": "I originally encountered this problem using real data; I just changed the example to random data to make it simpler and more portable, so other people can check if they can reproduce it.\r\n\r\nWhether or not large kernel sizes are useful is arguable, but they certainly should not trigger an NaN error.  I am pretty certain that this is a bug, as the error occurs spontaneously after very few iterations, and neither the variables nor the gradients explode, and I cannot think of a sensible reason why setting k < 14 fixes it completely.  Even more suspiciously, setting use_locking=True in the AdamOptimizer seems to make the error appear a little later.  I am just not sure if it is a bug in Tensorflow itself or perhaps some CUDA library, so I would like to know if anyone else can reproduce it."}