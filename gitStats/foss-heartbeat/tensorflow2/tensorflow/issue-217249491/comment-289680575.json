{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289680575", "html_url": "https://github.com/tensorflow/tensorflow/issues/8753#issuecomment-289680575", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8753", "id": 289680575, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTY4MDU3NQ==", "user": {"login": "hqqxyy", "id": 3648878, "node_id": "MDQ6VXNlcjM2NDg4Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3648878?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hqqxyy", "html_url": "https://github.com/hqqxyy", "followers_url": "https://api.github.com/users/hqqxyy/followers", "following_url": "https://api.github.com/users/hqqxyy/following{/other_user}", "gists_url": "https://api.github.com/users/hqqxyy/gists{/gist_id}", "starred_url": "https://api.github.com/users/hqqxyy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hqqxyy/subscriptions", "organizations_url": "https://api.github.com/users/hqqxyy/orgs", "repos_url": "https://api.github.com/users/hqqxyy/repos", "events_url": "https://api.github.com/users/hqqxyy/events{/privacy}", "received_events_url": "https://api.github.com/users/hqqxyy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-28T06:55:44Z", "updated_at": "2017-03-28T06:56:40Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16824702\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/caisq\">@caisq</a><br>\nHi, I try to reproduce this issue in the mnist example. It also wrong in line 928 of debug_data.py. But it is wrong with</p>\n<blockquote>\n<p>ValueError: Node name 'accuracy/correct_prediction/ArgMax/dimension' is not found in partition graphs.</p>\n</blockquote>\n<p>My code is as follows, I only change the mnist example's data_reader with <code>tf.train.batch</code></p>\n<pre><code># Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Demo of the tfdbg curses CLI: Locating the source of bad numerical values.\n\nThe neural network in this demo is larged based on the tutorial at:\n  tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n\nBut modifications are made so that problematic numerical values (infs and nans)\nappear in nodes of the graph during training.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.python import debug as tf_debug\n\n\nIMAGE_SIZE = 28\nHIDDEN_SIZE = 500\nNUM_LABELS = 10\nRAND_SEED = 42\n\n\ndef test_pyfunc(x):\n    return x\n\n\ndef main(_):\n    # Import data\n\n    with tf.Graph().as_default():\n        mnist = input_data.read_data_sets(FLAGS.data_dir,\n                                          one_hot=True,\n                                          fake_data=FLAGS.fake_data)\n\n        # def feed_dict(train):\n        #   if train or FLAGS.fake_data:\n        #     xs, ys = mnist.train.next_batch(FLAGS.train_batch_size,\n        #                                     fake_data=FLAGS.fake_data)\n        #   else:\n        #     xs, ys = mnist.test.images, mnist.test.labels\n        #\n        #   return {x: xs, y_: ys}\n\n        images, labels = mnist.train.images, mnist.train.labels\n\n        def get_image_label():\n            ind = np.random.randint(len(images))\n            return images[ind].astype(np.float32), labels[ind].astype(np.float32)\n\n        image_tensor, label_tensor, = tf.py_func(get_image_label, [], [tf.float32, tf.float32])\n        image_tensor.set_shape((IMAGE_SIZE ** 2, ))\n        label_tensor.set_shape((NUM_LABELS, ))\n\n        x, y_ = tf.train.batch([image_tensor, label_tensor],\n                               batch_size=FLAGS.train_batch_size,\n                               num_threads=2)\n\n\n        # Create the MNIST neural network graph.\n\n        # # Input placeholders.\n        # with tf.name_scope(\"input\"):\n        #   x = tf.placeholder(\n        #       tf.float32, [None, IMAGE_SIZE * IMAGE_SIZE], name=\"x-input\")\n        #   y_ = tf.placeholder(tf.float32, [None, NUM_LABELS], name=\"y-input\")\n\n\n        def weight_variable(shape):\n            \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n            initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\n            return tf.Variable(initial)\n\n        def bias_variable(shape):\n            \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n            initial = tf.constant(0.1, shape=shape)\n            return tf.Variable(initial)\n\n        def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n            \"\"\"Reusable code for making a simple neural net layer.\"\"\"\n            # Adding a name scope ensures logical grouping of the layers in the graph.\n            with tf.name_scope(layer_name):\n                # This Variable will hold the state of the weights for the layer\n                with tf.name_scope(\"weights\"):\n                    weights = weight_variable([input_dim, output_dim])\n                with tf.name_scope(\"biases\"):\n                    biases = bias_variable([output_dim])\n                with tf.name_scope(\"Wx_plus_b\"):\n                    preactivate = tf.matmul(input_tensor, weights) + biases\n\n                activations = act(preactivate)\n                return activations\n\n        hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\n        y = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"softmax\", act=tf.nn.softmax)\n\n        with tf.name_scope(\"cross_entropy\"):\n            # The following line is the culprit of the bad numerical values that appear\n            # during training of this graph. Log of zero gives inf, which is first seen\n            # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\n            # call. A multiplication of the inf values with zeros leads to nans,\n            # which is first in \"cross_entropy/mul:0\".\n            #\n            # You can use clipping to fix this issue, e.g.,\n            #   diff = y_ * tf.log(tf.clip_by_value(y, 1e-8, 1.0))\n\n            diff = y_ * tf.log(y)\n            with tf.name_scope(\"total\"):\n                cross_entropy = -tf.reduce_mean(diff)\n\n        with tf.name_scope(\"train\"):\n            train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n                cross_entropy)\n\n        with tf.name_scope(\"accuracy\"):\n            with tf.name_scope(\"correct_prediction\"):\n                correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n            with tf.name_scope(\"accuracy\"):\n                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        sess = tf.Session(config=tf.ConfigProto(\n            allow_soft_placement=True))\n        sess.run(tf.global_variables_initializer())\n\n        if FLAGS.debug:\n            sess = tf_debug.LocalCLIDebugWrapperSession(sess, ui_type=FLAGS.ui_type)\n            sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n\n        tf.train.start_queue_runners(sess=sess)\n\n        # Add this point, sess is a debug wrapper around the actual Session if\n        # FLAGS.debug is true. In that case, calling run() will launch the CLI.\n        for i in range(FLAGS.max_steps):\n            acc = sess.run(accuracy)\n            print(\"Accuracy at step %d: %s\" % (i, acc))\n\n            sess.run(train_step)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n    parser.add_argument(\n        \"--max_steps\",\n        type=int,\n        default=10,\n        help=\"Number of steps to run trainer.\")\n    parser.add_argument(\n        \"--train_batch_size\",\n        type=int,\n        default=100,\n        help=\"Batch size used during training.\")\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=0.025,\n        help=\"Initial learning rate.\")\n    parser.add_argument(\n        \"--data_dir\",\n        type=str,\n        default=\"/tmp/mnist_data\",\n        help=\"Directory for storing data\")\n    parser.add_argument(\n        \"--ui_type\",\n        type=str,\n        default=\"curses\",\n        help=\"Command-line user interface type (curses | readline)\")\n    parser.add_argument(\n        \"--fake_data\",\n        type=\"bool\",\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"Use fake MNIST data for unit testing\")\n    parser.add_argument(\n        \"--debug\",\n        type=\"bool\",\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"Use debugger to track down bad values during training\")\n    FLAGS, unparsed = parser.parse_known_args()\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n</code></pre>", "body_text": "@caisq\nHi, I try to reproduce this issue in the mnist example. It also wrong in line 928 of debug_data.py. But it is wrong with\n\nValueError: Node name 'accuracy/correct_prediction/ArgMax/dimension' is not found in partition graphs.\n\nMy code is as follows, I only change the mnist example's data_reader with tf.train.batch\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Demo of the tfdbg curses CLI: Locating the source of bad numerical values.\n\nThe neural network in this demo is larged based on the tutorial at:\n  tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n\nBut modifications are made so that problematic numerical values (infs and nans)\nappear in nodes of the graph during training.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.python import debug as tf_debug\n\n\nIMAGE_SIZE = 28\nHIDDEN_SIZE = 500\nNUM_LABELS = 10\nRAND_SEED = 42\n\n\ndef test_pyfunc(x):\n    return x\n\n\ndef main(_):\n    # Import data\n\n    with tf.Graph().as_default():\n        mnist = input_data.read_data_sets(FLAGS.data_dir,\n                                          one_hot=True,\n                                          fake_data=FLAGS.fake_data)\n\n        # def feed_dict(train):\n        #   if train or FLAGS.fake_data:\n        #     xs, ys = mnist.train.next_batch(FLAGS.train_batch_size,\n        #                                     fake_data=FLAGS.fake_data)\n        #   else:\n        #     xs, ys = mnist.test.images, mnist.test.labels\n        #\n        #   return {x: xs, y_: ys}\n\n        images, labels = mnist.train.images, mnist.train.labels\n\n        def get_image_label():\n            ind = np.random.randint(len(images))\n            return images[ind].astype(np.float32), labels[ind].astype(np.float32)\n\n        image_tensor, label_tensor, = tf.py_func(get_image_label, [], [tf.float32, tf.float32])\n        image_tensor.set_shape((IMAGE_SIZE ** 2, ))\n        label_tensor.set_shape((NUM_LABELS, ))\n\n        x, y_ = tf.train.batch([image_tensor, label_tensor],\n                               batch_size=FLAGS.train_batch_size,\n                               num_threads=2)\n\n\n        # Create the MNIST neural network graph.\n\n        # # Input placeholders.\n        # with tf.name_scope(\"input\"):\n        #   x = tf.placeholder(\n        #       tf.float32, [None, IMAGE_SIZE * IMAGE_SIZE], name=\"x-input\")\n        #   y_ = tf.placeholder(tf.float32, [None, NUM_LABELS], name=\"y-input\")\n\n\n        def weight_variable(shape):\n            \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n            initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\n            return tf.Variable(initial)\n\n        def bias_variable(shape):\n            \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n            initial = tf.constant(0.1, shape=shape)\n            return tf.Variable(initial)\n\n        def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n            \"\"\"Reusable code for making a simple neural net layer.\"\"\"\n            # Adding a name scope ensures logical grouping of the layers in the graph.\n            with tf.name_scope(layer_name):\n                # This Variable will hold the state of the weights for the layer\n                with tf.name_scope(\"weights\"):\n                    weights = weight_variable([input_dim, output_dim])\n                with tf.name_scope(\"biases\"):\n                    biases = bias_variable([output_dim])\n                with tf.name_scope(\"Wx_plus_b\"):\n                    preactivate = tf.matmul(input_tensor, weights) + biases\n\n                activations = act(preactivate)\n                return activations\n\n        hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\n        y = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"softmax\", act=tf.nn.softmax)\n\n        with tf.name_scope(\"cross_entropy\"):\n            # The following line is the culprit of the bad numerical values that appear\n            # during training of this graph. Log of zero gives inf, which is first seen\n            # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\n            # call. A multiplication of the inf values with zeros leads to nans,\n            # which is first in \"cross_entropy/mul:0\".\n            #\n            # You can use clipping to fix this issue, e.g.,\n            #   diff = y_ * tf.log(tf.clip_by_value(y, 1e-8, 1.0))\n\n            diff = y_ * tf.log(y)\n            with tf.name_scope(\"total\"):\n                cross_entropy = -tf.reduce_mean(diff)\n\n        with tf.name_scope(\"train\"):\n            train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n                cross_entropy)\n\n        with tf.name_scope(\"accuracy\"):\n            with tf.name_scope(\"correct_prediction\"):\n                correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n            with tf.name_scope(\"accuracy\"):\n                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        sess = tf.Session(config=tf.ConfigProto(\n            allow_soft_placement=True))\n        sess.run(tf.global_variables_initializer())\n\n        if FLAGS.debug:\n            sess = tf_debug.LocalCLIDebugWrapperSession(sess, ui_type=FLAGS.ui_type)\n            sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n\n        tf.train.start_queue_runners(sess=sess)\n\n        # Add this point, sess is a debug wrapper around the actual Session if\n        # FLAGS.debug is true. In that case, calling run() will launch the CLI.\n        for i in range(FLAGS.max_steps):\n            acc = sess.run(accuracy)\n            print(\"Accuracy at step %d: %s\" % (i, acc))\n\n            sess.run(train_step)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n    parser.add_argument(\n        \"--max_steps\",\n        type=int,\n        default=10,\n        help=\"Number of steps to run trainer.\")\n    parser.add_argument(\n        \"--train_batch_size\",\n        type=int,\n        default=100,\n        help=\"Batch size used during training.\")\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=0.025,\n        help=\"Initial learning rate.\")\n    parser.add_argument(\n        \"--data_dir\",\n        type=str,\n        default=\"/tmp/mnist_data\",\n        help=\"Directory for storing data\")\n    parser.add_argument(\n        \"--ui_type\",\n        type=str,\n        default=\"curses\",\n        help=\"Command-line user interface type (curses | readline)\")\n    parser.add_argument(\n        \"--fake_data\",\n        type=\"bool\",\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"Use fake MNIST data for unit testing\")\n    parser.add_argument(\n        \"--debug\",\n        type=\"bool\",\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"Use debugger to track down bad values during training\")\n    FLAGS, unparsed = parser.parse_known_args()\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)", "body": "@caisq \r\nHi, I try to reproduce this issue in the mnist example. It also wrong in line 928 of debug_data.py. But it is wrong with \r\n\r\n> ValueError: Node name 'accuracy/correct_prediction/ArgMax/dimension' is not found in partition graphs.\r\n\r\nMy code is as follows, I only change the mnist example's data_reader with `tf.train.batch`\r\n\r\n```\r\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"Demo of the tfdbg curses CLI: Locating the source of bad numerical values.\r\n\r\nThe neural network in this demo is larged based on the tutorial at:\r\n  tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n\r\nBut modifications are made so that problematic numerical values (infs and nans)\r\nappear in nodes of the graph during training.\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\n\r\nIMAGE_SIZE = 28\r\nHIDDEN_SIZE = 500\r\nNUM_LABELS = 10\r\nRAND_SEED = 42\r\n\r\n\r\ndef test_pyfunc(x):\r\n    return x\r\n\r\n\r\ndef main(_):\r\n    # Import data\r\n\r\n    with tf.Graph().as_default():\r\n        mnist = input_data.read_data_sets(FLAGS.data_dir,\r\n                                          one_hot=True,\r\n                                          fake_data=FLAGS.fake_data)\r\n\r\n        # def feed_dict(train):\r\n        #   if train or FLAGS.fake_data:\r\n        #     xs, ys = mnist.train.next_batch(FLAGS.train_batch_size,\r\n        #                                     fake_data=FLAGS.fake_data)\r\n        #   else:\r\n        #     xs, ys = mnist.test.images, mnist.test.labels\r\n        #\r\n        #   return {x: xs, y_: ys}\r\n\r\n        images, labels = mnist.train.images, mnist.train.labels\r\n\r\n        def get_image_label():\r\n            ind = np.random.randint(len(images))\r\n            return images[ind].astype(np.float32), labels[ind].astype(np.float32)\r\n\r\n        image_tensor, label_tensor, = tf.py_func(get_image_label, [], [tf.float32, tf.float32])\r\n        image_tensor.set_shape((IMAGE_SIZE ** 2, ))\r\n        label_tensor.set_shape((NUM_LABELS, ))\r\n\r\n        x, y_ = tf.train.batch([image_tensor, label_tensor],\r\n                               batch_size=FLAGS.train_batch_size,\r\n                               num_threads=2)\r\n\r\n\r\n        # Create the MNIST neural network graph.\r\n\r\n        # # Input placeholders.\r\n        # with tf.name_scope(\"input\"):\r\n        #   x = tf.placeholder(\r\n        #       tf.float32, [None, IMAGE_SIZE * IMAGE_SIZE], name=\"x-input\")\r\n        #   y_ = tf.placeholder(tf.float32, [None, NUM_LABELS], name=\"y-input\")\r\n\r\n\r\n        def weight_variable(shape):\r\n            \"\"\"Create a weight variable with appropriate initialization.\"\"\"\r\n            initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\r\n            return tf.Variable(initial)\r\n\r\n        def bias_variable(shape):\r\n            \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n            initial = tf.constant(0.1, shape=shape)\r\n            return tf.Variable(initial)\r\n\r\n        def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\r\n            \"\"\"Reusable code for making a simple neural net layer.\"\"\"\r\n            # Adding a name scope ensures logical grouping of the layers in the graph.\r\n            with tf.name_scope(layer_name):\r\n                # This Variable will hold the state of the weights for the layer\r\n                with tf.name_scope(\"weights\"):\r\n                    weights = weight_variable([input_dim, output_dim])\r\n                with tf.name_scope(\"biases\"):\r\n                    biases = bias_variable([output_dim])\r\n                with tf.name_scope(\"Wx_plus_b\"):\r\n                    preactivate = tf.matmul(input_tensor, weights) + biases\r\n\r\n                activations = act(preactivate)\r\n                return activations\r\n\r\n        hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\r\n        y = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"softmax\", act=tf.nn.softmax)\r\n\r\n        with tf.name_scope(\"cross_entropy\"):\r\n            # The following line is the culprit of the bad numerical values that appear\r\n            # during training of this graph. Log of zero gives inf, which is first seen\r\n            # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\r\n            # call. A multiplication of the inf values with zeros leads to nans,\r\n            # which is first in \"cross_entropy/mul:0\".\r\n            #\r\n            # You can use clipping to fix this issue, e.g.,\r\n            #   diff = y_ * tf.log(tf.clip_by_value(y, 1e-8, 1.0))\r\n\r\n            diff = y_ * tf.log(y)\r\n            with tf.name_scope(\"total\"):\r\n                cross_entropy = -tf.reduce_mean(diff)\r\n\r\n        with tf.name_scope(\"train\"):\r\n            train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\r\n                cross_entropy)\r\n\r\n        with tf.name_scope(\"accuracy\"):\r\n            with tf.name_scope(\"correct_prediction\"):\r\n                correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n            with tf.name_scope(\"accuracy\"):\r\n                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n        sess = tf.Session(config=tf.ConfigProto(\r\n            allow_soft_placement=True))\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        if FLAGS.debug:\r\n            sess = tf_debug.LocalCLIDebugWrapperSession(sess, ui_type=FLAGS.ui_type)\r\n            sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\n\r\n        tf.train.start_queue_runners(sess=sess)\r\n\r\n        # Add this point, sess is a debug wrapper around the actual Session if\r\n        # FLAGS.debug is true. In that case, calling run() will launch the CLI.\r\n        for i in range(FLAGS.max_steps):\r\n            acc = sess.run(accuracy)\r\n            print(\"Accuracy at step %d: %s\" % (i, acc))\r\n\r\n            sess.run(train_step)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n    parser.add_argument(\r\n        \"--max_steps\",\r\n        type=int,\r\n        default=10,\r\n        help=\"Number of steps to run trainer.\")\r\n    parser.add_argument(\r\n        \"--train_batch_size\",\r\n        type=int,\r\n        default=100,\r\n        help=\"Batch size used during training.\")\r\n    parser.add_argument(\r\n        \"--learning_rate\",\r\n        type=float,\r\n        default=0.025,\r\n        help=\"Initial learning rate.\")\r\n    parser.add_argument(\r\n        \"--data_dir\",\r\n        type=str,\r\n        default=\"/tmp/mnist_data\",\r\n        help=\"Directory for storing data\")\r\n    parser.add_argument(\r\n        \"--ui_type\",\r\n        type=str,\r\n        default=\"curses\",\r\n        help=\"Command-line user interface type (curses | readline)\")\r\n    parser.add_argument(\r\n        \"--fake_data\",\r\n        type=\"bool\",\r\n        nargs=\"?\",\r\n        const=True,\r\n        default=False,\r\n        help=\"Use fake MNIST data for unit testing\")\r\n    parser.add_argument(\r\n        \"--debug\",\r\n        type=\"bool\",\r\n        nargs=\"?\",\r\n        const=True,\r\n        default=False,\r\n        help=\"Use debugger to track down bad values during training\")\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n"}