{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/351692924", "html_url": "https://github.com/tensorflow/tensorflow/issues/15363#issuecomment-351692924", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15363", "id": 351692924, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTY5MjkyNA==", "user": {"login": "Tony-Hou", "id": 26059901, "node_id": "MDQ6VXNlcjI2MDU5OTAx", "avatar_url": "https://avatars3.githubusercontent.com/u/26059901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tony-Hou", "html_url": "https://github.com/Tony-Hou", "followers_url": "https://api.github.com/users/Tony-Hou/followers", "following_url": "https://api.github.com/users/Tony-Hou/following{/other_user}", "gists_url": "https://api.github.com/users/Tony-Hou/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tony-Hou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tony-Hou/subscriptions", "organizations_url": "https://api.github.com/users/Tony-Hou/orgs", "repos_url": "https://api.github.com/users/Tony-Hou/repos", "events_url": "https://api.github.com/users/Tony-Hou/events{/privacy}", "received_events_url": "https://api.github.com/users/Tony-Hou/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-14T12:06:56Z", "updated_at": "2017-12-14T12:06:56Z", "author_association": "NONE", "body_html": "<p>#!/usr/bin/python<br>\n#-<em>- coding: utf-8 -</em>-</p>\n<p>import tensorflow as tf<br>\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step<br>\nfrom tensorflow.python.platform import tf_logging as logging<br>\nimport inception_preprocessing<br>\nfrom inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope<br>\nimport os<br>\nimport time<br>\nslim = tf.contrib.slim</p>\n<p>import numpy as np</p>\n<p>config  = tf.ConfigProto()<br>\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9</p>\n<p>\"\"\"<br>\nconfig = tf.ConfigProto()<br>\nconfig.gpu_options.allow_growth = True</p>\n<p>\"\"\"<br>\n#================ DATASET INFORMATION ======================<br>\n#State dataset directory where the tfrecord files are located<br>\ndataset_dir = '.'</p>\n<p>#State where your log file is at. If it doesn't exist, create it.<br>\nlog_dir = './log'</p>\n<p>#State where your checkpoint file is<br>\ncheckpoint_file = './inception_resnet_v2_2016_08_30.ckpt'</p>\n<p>#State the image size you're resizing your images to. We will use the default inception size of 299.<br>\nimg_width = 800<br>\nimg_height = 600</p>\n<p>file_pattern = 'estate_%s_*.tfrecord'<br>\n#State the number of classes to predict:<br>\nnum_classes = 6</p>\n<p>#State the labels file and read it<br>\nlabels_file = './labels.txt'<br>\nlabels = open(labels_file, 'r')</p>\n<p>#Create a dictionary to refer each label to their string name<br>\nlabels_to_name = {}<br>\nfor line in labels:<br>\nlabel, string_name = line.split(':')<br>\nstring_name = string_name[:-1] #Remove newline<br>\nlabels_to_name[int(label)] = string_name</p>\n<p>#Create the file pattern of your TFRecord files so that it could be recognized later on<br>\nfile_pattern = 'estate_%s_*.tfrecord'</p>\n<p>#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.</p>\n<p>items_to_descriptions = {<br>\n'image': 'A 3-channel RGB coloured real estate image that is either bathroom, bedroom, floorplan, kitchen, or livingroom, other.',<br>\n'label': 'A label that is as such -- 0:bathroom, 1:bedroom, 2:floorplan, 3:kitchen, 4:livingroom, 5:other'<br>\n}</p>\n<p>#================= TRAINING INFORMATION ==================<br>\n#State the number of epochs to train<br>\nnum_epochs = 1</p>\n<p>#State your batch size<br>\nbatch_size = 1</p>\n<p>#Learning rate information and configuration (Up to you to experiment)<br>\ninitial_learning_rate = 0.0002<br>\nlearning_rate_decay_factor = 0.7<br>\nnum_epochs_before_decay = 2</p>\n<p>#iteration<br>\ntotal_iterations = 0<br>\n#============== DATASET LOADING ======================<br>\n#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.<br>\ndef get_split(split_name, dataset_dir, file_pattern=file_pattern, file_pattern_for_counting='estate'):<br>\n'''<br>\nObtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will<br>\nset up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.<br>\nYour file_pattern is very important in locating the files later.</p>\n<pre><code>INPUTS:\n- split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n- dataset_dir(str): the dataset directory where the tfrecord files are located\n- file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n- file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n\nOUTPUTS:\n- dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n'''\n\n#First check whether the split_name is train or validation\nif split_name not in ['train', 'validation']:\n    raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n\n#Create the full path for a general file_pattern to locate the tfrecord_files\nfile_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n\n#Count the total number of examples in all of these shard\nnum_samples = 0\nfile_pattern_for_counting = file_pattern_for_counting + '_' + split_name\ntfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\nfor tfrecord_file in tfrecords_to_count:\n    for record in tf.python_io.tf_record_iterator(tfrecord_file):\n        num_samples += 1\n\n#Create a reader, which must be a TFRecord reader in this case\nreader = tf.TFRecordReader\n\n#Create the keys_to_features dictionary for the decoder\nkeys_to_features = {\n  'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n  'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n  'image/class/label': tf.FixedLenFeature(\n      [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n}\n\n#Create the items_to_handlers dictionary for the decoder.\nitems_to_handlers = {\n'image': slim.tfexample_decoder.Image(),\n'label': slim.tfexample_decoder.Tensor('image/class/label'),\n}\n\n#Start to create the decoder\ndecoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n#Create the labels_to_name file\nlabels_to_name_dict = labels_to_name\n\n #Actually create the dataset\ndataset = slim.dataset.Dataset(\n    data_sources = file_pattern_path,\n    decoder = decoder,\n    reader = reader,\n    num_readers = 4,\n    num_samples = num_samples,\n    num_classes = num_classes,\n    labels_to_name = labels_to_name_dict,\n    items_to_descriptions = items_to_descriptions)\nreturn dataset\n</code></pre>\n<p>def load_batch(dataset, batch_size, height=img_height, width=img_width, is_training=True):<br>\n'''<br>\nLoads a batch for training.</p>\n<pre><code>INPUTS:\n- dataset(Dataset): a Dataset class object that is created from the get_split function\n- batch_size(int): determines how big of a batch to train\n- height(int): the height of the image to resize to during preprocessing\n- width(int): the width of the image to resize to during preprocessing\n- is_training(bool): to determine whether to perform a training or evaluation preprocessing\n\nOUTPUTS:\n- images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n- labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n\n'''\n#First create the data_provider object\ndata_provider = slim.dataset_data_provider.DatasetDataProvider(\n    dataset,\n    common_queue_capacity = 24 + 3 * batch_size,\n    common_queue_min = 24)\n\n#Obtain the raw image using the get method\nraw_image, label = data_provider.get(['image', 'label'])\n\n#Perform the correct preprocessing for this image depending if it is training or evaluating\nimage = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n\n#As for the raw images, we just do a simple reshape to batch it up\nraw_image = tf.expand_dims(raw_image, 0)\nraw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n#modify due to data type\nraw_image = tf.cast(raw_image, tf.float32)\nraw_image = tf.squeeze(raw_image)\n\n#Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\nimages, raw_images, labels = tf.train.batch(\n    [image, raw_image, label],\n    batch_size = batch_size,\n    num_threads = 1,\n    capacity = 4 * batch_size,\n    allow_smaller_final_batch = True)\n\nreturn images, raw_images, labels\n</code></pre>\n<p>sess = tf.Session()</p>\n<pre><code>    #Create the log directory here. Must be done here otherwise import will activate this unneededly.\n</code></pre>\n<p>if not os.path.exists(log_dir):<br>\nos.mkdir(log_dir)<br>\n#======================= TRAINING PROCESS =========================<br>\n#Now we start to construct the graph and build our model<br>\ntf.logging.set_verbosity(tf.logging.INFO) #Set the verbosity to INFO level<br>\ndataset = get_split('train', dataset_dir, file_pattern=file_pattern)<br>\n#First create the dataset and load one batch<br>\nx = tf.placeholder(tf.float32, shape=[None, img_height, img_width,3], name='x')<br>\n#y_true = tf.placeholder(tf.int32, shape=[None, num_classes], name='y_true')<br>\ny_true = tf.placeholder(tf.int32, shape=[num_classes], name='y_true')<br>\n#images = tf.reshape(x, [-1, 800, 600, 1])</p>\n<p>#Know the number steps to take before decaying the learning rate and batches per epoch<br>\nnum_batches_per_epoch = int(dataset.num_samples / batch_size)<br>\nnum_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed<br>\ndecay_steps = int(num_epochs_before_decay * num_steps_per_epoch)</p>\n<p>#Create the model inference<br>\nwith slim.arg_scope(inception_resnet_v2_arg_scope()):<br>\nlogits, end_points = inception_resnet_v2(x, num_classes = dataset.num_classes, is_training = True)</p>\n<p>y_pred = tf.nn.softmax(logits, name='y_pred')<br>\n#Define the scopes that you want to exclude for restoration<br>\nexclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']<br>\nvariables_to_restore = slim.get_variables_to_restore(exclude = exclude)</p>\n<p>#Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)<br>\none_hot_labels = slim.one_hot_encoding(y_true, dataset.num_classes)</p>\n<p>#Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks<br>\nloss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)<br>\ntotal_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well</p>\n<p>#Create the global step for monitoring the learning_rate and training.<br>\nglobal_step = get_or_create_global_step()</p>\n<p>#Define your exponentially decaying learning rate<br>\nlr = tf.train.exponential_decay(<br>\nlearning_rate = initial_learning_rate,<br>\nglobal_step = global_step,<br>\ndecay_steps = decay_steps,<br>\ndecay_rate = learning_rate_decay_factor,<br>\nstaircase = True)</p>\n<p>#Now we can define the optimizer that takes on the learning rate<br>\noptimizer = tf.train.AdamOptimizer(learning_rate = lr)</p>\n<p>#Create the train_op.<br>\ntrain_op = slim.learning.create_train_op(total_loss, optimizer)</p>\n<p>#State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.<br>\npredictions = tf.argmax(end_points['Predictions'], 1)<br>\nprobabilities = end_points['Predictions']<br>\naccuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, y_true)<br>\nmetrics_op = tf.group(accuracy_update, probabilities)</p>\n<p>sess.run(tf.global_variables_initializer())</p>\n<p>#Now finally create all the summaries you need to monitor and group them into one summary op.<br>\ntf.summary.scalar('losses/Total_Loss', total_loss)<br>\ntf.summary.scalar('accuracy', accuracy)<br>\ntf.summary.scalar('learning_rate', lr)<br>\nmy_summary_op = tf.summary.merge_all()</p>\n<p>#Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.<br>\ndef train_step(sess, train_op, global_step):<br>\n'''<br>\nSimply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step<br>\n'''<br>\n#Check the time for each sess run<br>\nstart_time = time.time()<br>\ntotal_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])<br>\ntime_elapsed = time.time() - start_time</p>\n<pre><code>#Run the logging to print some results\n#if global_step_count % 10 == 0:\nlogging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\nreturn total_loss, global_step_count\n</code></pre>\n<p>#Now we create a saver function that actually restores the variables from a checkpoint file in a sess<br>\nsaver = tf.train.Saver(variables_to_restore)<br>\nsaver.restore(sess, checkpoint_file)</p>\n<p>def train(num_iteration):<br>\nglobal total_iterations</p>\n<pre><code>for i in range(total_iterations,\n               total_iterations + num_iteration):\n    images, _, labels = load_batch(dataset, batch_size=batch_size)\n    sess.run(train_op, feed_dict={x: images, y_true: labels})\n    saver.save(sess,  global_step = global_step)\ntotal_iterations += num_iteration\n</code></pre>\n<p>train(num_iteration=24000)</p>", "body_text": "#!/usr/bin/python\n#-- coding: utf-8 --\nimport tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\nfrom tensorflow.python.platform import tf_logging as logging\nimport inception_preprocessing\nfrom inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\nimport os\nimport time\nslim = tf.contrib.slim\nimport numpy as np\nconfig  = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\n\"\"\"\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\"\"\"\n#================ DATASET INFORMATION ======================\n#State dataset directory where the tfrecord files are located\ndataset_dir = '.'\n#State where your log file is at. If it doesn't exist, create it.\nlog_dir = './log'\n#State where your checkpoint file is\ncheckpoint_file = './inception_resnet_v2_2016_08_30.ckpt'\n#State the image size you're resizing your images to. We will use the default inception size of 299.\nimg_width = 800\nimg_height = 600\nfile_pattern = 'estate_%s_*.tfrecord'\n#State the number of classes to predict:\nnum_classes = 6\n#State the labels file and read it\nlabels_file = './labels.txt'\nlabels = open(labels_file, 'r')\n#Create a dictionary to refer each label to their string name\nlabels_to_name = {}\nfor line in labels:\nlabel, string_name = line.split(':')\nstring_name = string_name[:-1] #Remove newline\nlabels_to_name[int(label)] = string_name\n#Create the file pattern of your TFRecord files so that it could be recognized later on\nfile_pattern = 'estate_%s_*.tfrecord'\n#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\nitems_to_descriptions = {\n'image': 'A 3-channel RGB coloured real estate image that is either bathroom, bedroom, floorplan, kitchen, or livingroom, other.',\n'label': 'A label that is as such -- 0:bathroom, 1:bedroom, 2:floorplan, 3:kitchen, 4:livingroom, 5:other'\n}\n#================= TRAINING INFORMATION ==================\n#State the number of epochs to train\nnum_epochs = 1\n#State your batch size\nbatch_size = 1\n#Learning rate information and configuration (Up to you to experiment)\ninitial_learning_rate = 0.0002\nlearning_rate_decay_factor = 0.7\nnum_epochs_before_decay = 2\n#iteration\ntotal_iterations = 0\n#============== DATASET LOADING ======================\n#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\ndef get_split(split_name, dataset_dir, file_pattern=file_pattern, file_pattern_for_counting='estate'):\n'''\nObtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\nset up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\nYour file_pattern is very important in locating the files later.\nINPUTS:\n- split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n- dataset_dir(str): the dataset directory where the tfrecord files are located\n- file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n- file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n\nOUTPUTS:\n- dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n'''\n\n#First check whether the split_name is train or validation\nif split_name not in ['train', 'validation']:\n    raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n\n#Create the full path for a general file_pattern to locate the tfrecord_files\nfile_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n\n#Count the total number of examples in all of these shard\nnum_samples = 0\nfile_pattern_for_counting = file_pattern_for_counting + '_' + split_name\ntfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\nfor tfrecord_file in tfrecords_to_count:\n    for record in tf.python_io.tf_record_iterator(tfrecord_file):\n        num_samples += 1\n\n#Create a reader, which must be a TFRecord reader in this case\nreader = tf.TFRecordReader\n\n#Create the keys_to_features dictionary for the decoder\nkeys_to_features = {\n  'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n  'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n  'image/class/label': tf.FixedLenFeature(\n      [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n}\n\n#Create the items_to_handlers dictionary for the decoder.\nitems_to_handlers = {\n'image': slim.tfexample_decoder.Image(),\n'label': slim.tfexample_decoder.Tensor('image/class/label'),\n}\n\n#Start to create the decoder\ndecoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n#Create the labels_to_name file\nlabels_to_name_dict = labels_to_name\n\n #Actually create the dataset\ndataset = slim.dataset.Dataset(\n    data_sources = file_pattern_path,\n    decoder = decoder,\n    reader = reader,\n    num_readers = 4,\n    num_samples = num_samples,\n    num_classes = num_classes,\n    labels_to_name = labels_to_name_dict,\n    items_to_descriptions = items_to_descriptions)\nreturn dataset\n\ndef load_batch(dataset, batch_size, height=img_height, width=img_width, is_training=True):\n'''\nLoads a batch for training.\nINPUTS:\n- dataset(Dataset): a Dataset class object that is created from the get_split function\n- batch_size(int): determines how big of a batch to train\n- height(int): the height of the image to resize to during preprocessing\n- width(int): the width of the image to resize to during preprocessing\n- is_training(bool): to determine whether to perform a training or evaluation preprocessing\n\nOUTPUTS:\n- images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n- labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n\n'''\n#First create the data_provider object\ndata_provider = slim.dataset_data_provider.DatasetDataProvider(\n    dataset,\n    common_queue_capacity = 24 + 3 * batch_size,\n    common_queue_min = 24)\n\n#Obtain the raw image using the get method\nraw_image, label = data_provider.get(['image', 'label'])\n\n#Perform the correct preprocessing for this image depending if it is training or evaluating\nimage = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n\n#As for the raw images, we just do a simple reshape to batch it up\nraw_image = tf.expand_dims(raw_image, 0)\nraw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n#modify due to data type\nraw_image = tf.cast(raw_image, tf.float32)\nraw_image = tf.squeeze(raw_image)\n\n#Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\nimages, raw_images, labels = tf.train.batch(\n    [image, raw_image, label],\n    batch_size = batch_size,\n    num_threads = 1,\n    capacity = 4 * batch_size,\n    allow_smaller_final_batch = True)\n\nreturn images, raw_images, labels\n\nsess = tf.Session()\n    #Create the log directory here. Must be done here otherwise import will activate this unneededly.\n\nif not os.path.exists(log_dir):\nos.mkdir(log_dir)\n#======================= TRAINING PROCESS =========================\n#Now we start to construct the graph and build our model\ntf.logging.set_verbosity(tf.logging.INFO) #Set the verbosity to INFO level\ndataset = get_split('train', dataset_dir, file_pattern=file_pattern)\n#First create the dataset and load one batch\nx = tf.placeholder(tf.float32, shape=[None, img_height, img_width,3], name='x')\n#y_true = tf.placeholder(tf.int32, shape=[None, num_classes], name='y_true')\ny_true = tf.placeholder(tf.int32, shape=[num_classes], name='y_true')\n#images = tf.reshape(x, [-1, 800, 600, 1])\n#Know the number steps to take before decaying the learning rate and batches per epoch\nnum_batches_per_epoch = int(dataset.num_samples / batch_size)\nnum_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed\ndecay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n#Create the model inference\nwith slim.arg_scope(inception_resnet_v2_arg_scope()):\nlogits, end_points = inception_resnet_v2(x, num_classes = dataset.num_classes, is_training = True)\ny_pred = tf.nn.softmax(logits, name='y_pred')\n#Define the scopes that you want to exclude for restoration\nexclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\nvariables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n#Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\none_hot_labels = slim.one_hot_encoding(y_true, dataset.num_classes)\n#Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\nloss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\ntotal_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well\n#Create the global step for monitoring the learning_rate and training.\nglobal_step = get_or_create_global_step()\n#Define your exponentially decaying learning rate\nlr = tf.train.exponential_decay(\nlearning_rate = initial_learning_rate,\nglobal_step = global_step,\ndecay_steps = decay_steps,\ndecay_rate = learning_rate_decay_factor,\nstaircase = True)\n#Now we can define the optimizer that takes on the learning rate\noptimizer = tf.train.AdamOptimizer(learning_rate = lr)\n#Create the train_op.\ntrain_op = slim.learning.create_train_op(total_loss, optimizer)\n#State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\npredictions = tf.argmax(end_points['Predictions'], 1)\nprobabilities = end_points['Predictions']\naccuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, y_true)\nmetrics_op = tf.group(accuracy_update, probabilities)\nsess.run(tf.global_variables_initializer())\n#Now finally create all the summaries you need to monitor and group them into one summary op.\ntf.summary.scalar('losses/Total_Loss', total_loss)\ntf.summary.scalar('accuracy', accuracy)\ntf.summary.scalar('learning_rate', lr)\nmy_summary_op = tf.summary.merge_all()\n#Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\ndef train_step(sess, train_op, global_step):\n'''\nSimply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n'''\n#Check the time for each sess run\nstart_time = time.time()\ntotal_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\ntime_elapsed = time.time() - start_time\n#Run the logging to print some results\n#if global_step_count % 10 == 0:\nlogging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\nreturn total_loss, global_step_count\n\n#Now we create a saver function that actually restores the variables from a checkpoint file in a sess\nsaver = tf.train.Saver(variables_to_restore)\nsaver.restore(sess, checkpoint_file)\ndef train(num_iteration):\nglobal total_iterations\nfor i in range(total_iterations,\n               total_iterations + num_iteration):\n    images, _, labels = load_batch(dataset, batch_size=batch_size)\n    sess.run(train_op, feed_dict={x: images, y_true: labels})\n    saver.save(sess,  global_step = global_step)\ntotal_iterations += num_iteration\n\ntrain(num_iteration=24000)", "body": "#!/usr/bin/python\r\n#-*- coding: utf-8 -*- \r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\r\nfrom tensorflow.python.platform import tf_logging as logging\r\nimport inception_preprocessing\r\nfrom inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\r\nimport os\r\nimport time\r\nslim = tf.contrib.slim\r\n\r\nimport numpy as np\r\n\r\nconfig  = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\r\n\r\n\r\n\"\"\"\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\n\"\"\"\r\n#================ DATASET INFORMATION ======================\r\n#State dataset directory where the tfrecord files are located\r\ndataset_dir = '.'\r\n\r\n#State where your log file is at. If it doesn't exist, create it.\r\nlog_dir = './log'\r\n\r\n#State where your checkpoint file is\r\ncheckpoint_file = './inception_resnet_v2_2016_08_30.ckpt'\r\n\r\n#State the image size you're resizing your images to. We will use the default inception size of 299.\r\nimg_width = 800\r\nimg_height = 600\r\n\r\nfile_pattern = 'estate_%s_*.tfrecord'\r\n#State the number of classes to predict:\r\nnum_classes = 6\r\n\r\n#State the labels file and read it\r\nlabels_file = './labels.txt'\r\nlabels = open(labels_file, 'r')\r\n\r\n#Create a dictionary to refer each label to their string name\r\nlabels_to_name = {}\r\nfor line in labels:\r\n    label, string_name = line.split(':')\r\n    string_name = string_name[:-1] #Remove newline\r\n    labels_to_name[int(label)] = string_name\r\n\r\n#Create the file pattern of your TFRecord files so that it could be recognized later on\r\nfile_pattern = 'estate_%s_*.tfrecord'\r\n\r\n#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\r\n\r\nitems_to_descriptions = {\r\n    'image': 'A 3-channel RGB coloured real estate image that is either bathroom, bedroom, floorplan, kitchen, or livingroom, other.',\r\n    'label': 'A label that is as such -- 0:bathroom, 1:bedroom, 2:floorplan, 3:kitchen, 4:livingroom, 5:other'\r\n}\r\n\r\n\r\n#================= TRAINING INFORMATION ==================\r\n#State the number of epochs to train\r\nnum_epochs = 1\r\n\r\n#State your batch size\r\nbatch_size = 1\r\n\r\n#Learning rate information and configuration (Up to you to experiment)\r\ninitial_learning_rate = 0.0002\r\nlearning_rate_decay_factor = 0.7\r\nnum_epochs_before_decay = 2\r\n\r\n#iteration \r\ntotal_iterations = 0\r\n#============== DATASET LOADING ======================\r\n#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\r\ndef get_split(split_name, dataset_dir, file_pattern=file_pattern, file_pattern_for_counting='estate'):\r\n    '''\r\n    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\r\n    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\r\n    Your file_pattern is very important in locating the files later. \r\n\r\n    INPUTS:\r\n    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\r\n    - dataset_dir(str): the dataset directory where the tfrecord files are located\r\n    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\r\n    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\r\n\r\n    OUTPUTS:\r\n    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\r\n    '''\r\n\r\n    #First check whether the split_name is train or validation\r\n    if split_name not in ['train', 'validation']:\r\n        raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\r\n\r\n    #Create the full path for a general file_pattern to locate the tfrecord_files\r\n    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\r\n\r\n    #Count the total number of examples in all of these shard\r\n    num_samples = 0\r\n    file_pattern_for_counting = file_pattern_for_counting + '_' + split_name\r\n    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\r\n    for tfrecord_file in tfrecords_to_count:\r\n        for record in tf.python_io.tf_record_iterator(tfrecord_file):\r\n            num_samples += 1\r\n\r\n    #Create a reader, which must be a TFRecord reader in this case\r\n    reader = tf.TFRecordReader\r\n\r\n    #Create the keys_to_features dictionary for the decoder\r\n    keys_to_features = {\r\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\r\n      'image/class/label': tf.FixedLenFeature(\r\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\r\n    }\r\n\r\n    #Create the items_to_handlers dictionary for the decoder.\r\n    items_to_handlers = {\r\n    'image': slim.tfexample_decoder.Image(),\r\n    'label': slim.tfexample_decoder.Tensor('image/class/label'),\r\n    }\r\n\r\n    #Start to create the decoder\r\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\r\n\r\n    #Create the labels_to_name file\r\n    labels_to_name_dict = labels_to_name\r\n\r\n   \t #Actually create the dataset\r\n    dataset = slim.dataset.Dataset(\r\n        data_sources = file_pattern_path,\r\n        decoder = decoder,\r\n        reader = reader,\r\n        num_readers = 4,\r\n        num_samples = num_samples,\r\n        num_classes = num_classes,\r\n        labels_to_name = labels_to_name_dict,\r\n        items_to_descriptions = items_to_descriptions)\r\n    return dataset\r\n\r\n\r\ndef load_batch(dataset, batch_size, height=img_height, width=img_width, is_training=True):\r\n    '''\r\n    Loads a batch for training.\r\n\r\n    INPUTS:\r\n    - dataset(Dataset): a Dataset class object that is created from the get_split function\r\n    - batch_size(int): determines how big of a batch to train\r\n    - height(int): the height of the image to resize to during preprocessing\r\n    - width(int): the width of the image to resize to during preprocessing\r\n    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\r\n\r\n    OUTPUTS:\r\n    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\r\n    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\r\n\r\n    '''\r\n    #First create the data_provider object\r\n    data_provider = slim.dataset_data_provider.DatasetDataProvider(\r\n        dataset,\r\n        common_queue_capacity = 24 + 3 * batch_size,\r\n        common_queue_min = 24)\r\n\r\n    #Obtain the raw image using the get method\r\n    raw_image, label = data_provider.get(['image', 'label'])\r\n\r\n    #Perform the correct preprocessing for this image depending if it is training or evaluating\r\n    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\r\n\r\n    #As for the raw images, we just do a simple reshape to batch it up\r\n    raw_image = tf.expand_dims(raw_image, 0)\r\n    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\r\n    #modify due to data type\r\n    raw_image = tf.cast(raw_image, tf.float32)\r\n    raw_image = tf.squeeze(raw_image)\r\n\r\n    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\r\n    images, raw_images, labels = tf.train.batch(\r\n        [image, raw_image, label],\r\n        batch_size = batch_size,\r\n        num_threads = 1,\r\n        capacity = 4 * batch_size,\r\n        allow_smaller_final_batch = True)\r\n    \r\n    return images, raw_images, labels\r\n\r\n\r\nsess = tf.Session()\r\n    \r\n        #Create the log directory here. Must be done here otherwise import will activate this unneededly.\r\nif not os.path.exists(log_dir):\r\n    os.mkdir(log_dir)\r\n#======================= TRAINING PROCESS =========================\r\n#Now we start to construct the graph and build our model\r\ntf.logging.set_verbosity(tf.logging.INFO) #Set the verbosity to INFO level\r\ndataset = get_split('train', dataset_dir, file_pattern=file_pattern)\r\n#First create the dataset and load one batch\r\nx = tf.placeholder(tf.float32, shape=[None, img_height, img_width,3], name='x')\r\n#y_true = tf.placeholder(tf.int32, shape=[None, num_classes], name='y_true')\r\ny_true = tf.placeholder(tf.int32, shape=[num_classes], name='y_true')\r\n#images = tf.reshape(x, [-1, 800, 600, 1])\r\n\r\n#Know the number steps to take before decaying the learning rate and batches per epoch\r\nnum_batches_per_epoch = int(dataset.num_samples / batch_size)\r\nnum_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed\r\ndecay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\r\n\r\n#Create the model inference\r\nwith slim.arg_scope(inception_resnet_v2_arg_scope()):\r\n    logits, end_points = inception_resnet_v2(x, num_classes = dataset.num_classes, is_training = True)\r\n\r\ny_pred = tf.nn.softmax(logits, name='y_pred')\r\n#Define the scopes that you want to exclude for restoration\r\nexclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\r\nvariables_to_restore = slim.get_variables_to_restore(exclude = exclude)\r\n\r\n#Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\r\none_hot_labels = slim.one_hot_encoding(y_true, dataset.num_classes)\r\n\r\n#Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\r\nloss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\r\ntotal_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well\r\n\r\n#Create the global step for monitoring the learning_rate and training.\r\nglobal_step = get_or_create_global_step()\r\n\r\n#Define your exponentially decaying learning rate\r\nlr = tf.train.exponential_decay(\r\n    learning_rate = initial_learning_rate,\r\n    global_step = global_step,\r\n    decay_steps = decay_steps,\r\n    decay_rate = learning_rate_decay_factor,\r\n    staircase = True)\r\n\r\n#Now we can define the optimizer that takes on the learning rate\r\noptimizer = tf.train.AdamOptimizer(learning_rate = lr)\r\n\r\n#Create the train_op.\r\ntrain_op = slim.learning.create_train_op(total_loss, optimizer)\r\n\r\n#State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\r\npredictions = tf.argmax(end_points['Predictions'], 1)\r\nprobabilities = end_points['Predictions']\r\naccuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, y_true)\r\nmetrics_op = tf.group(accuracy_update, probabilities)\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\n#Now finally create all the summaries you need to monitor and group them into one summary op.\r\ntf.summary.scalar('losses/Total_Loss', total_loss)\r\ntf.summary.scalar('accuracy', accuracy)\r\ntf.summary.scalar('learning_rate', lr)\r\nmy_summary_op = tf.summary.merge_all()\r\n\r\n#Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\r\ndef train_step(sess, train_op, global_step):\r\n    '''\r\n    Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\r\n    '''\r\n    #Check the time for each sess run\r\n    start_time = time.time()\r\n    total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\r\n    time_elapsed = time.time() - start_time\r\n\r\n    #Run the logging to print some results\r\n    #if global_step_count % 10 == 0:\r\n    logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\r\n    return total_loss, global_step_count\r\n\r\n#Now we create a saver function that actually restores the variables from a checkpoint file in a sess\r\nsaver = tf.train.Saver(variables_to_restore)\r\nsaver.restore(sess, checkpoint_file)\r\n\r\ndef train(num_iteration):\r\n    global total_iterations\r\n    \r\n    for i in range(total_iterations,\r\n                   total_iterations + num_iteration):\r\n        images, _, labels = load_batch(dataset, batch_size=batch_size)\r\n        sess.run(train_op, feed_dict={x: images, y_true: labels})\r\n        saver.save(sess,  global_step = global_step)\r\n    total_iterations += num_iteration\r\n\r\ntrain(num_iteration=24000)\r\n\r\n                \r\n\r\n"}