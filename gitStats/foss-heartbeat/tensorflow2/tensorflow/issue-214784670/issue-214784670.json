{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8476", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8476/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8476/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8476/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8476", "id": 214784670, "node_id": "MDU6SXNzdWUyMTQ3ODQ2NzA=", "number": 8476, "title": "Distributed TensorFlow running in parallel and Session Running time problem.", "user": {"login": "TianweiXing", "id": 17885178, "node_id": "MDQ6VXNlcjE3ODg1MTc4", "avatar_url": "https://avatars1.githubusercontent.com/u/17885178?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TianweiXing", "html_url": "https://github.com/TianweiXing", "followers_url": "https://api.github.com/users/TianweiXing/followers", "following_url": "https://api.github.com/users/TianweiXing/following{/other_user}", "gists_url": "https://api.github.com/users/TianweiXing/gists{/gist_id}", "starred_url": "https://api.github.com/users/TianweiXing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TianweiXing/subscriptions", "organizations_url": "https://api.github.com/users/TianweiXing/orgs", "repos_url": "https://api.github.com/users/TianweiXing/repos", "events_url": "https://api.github.com/users/TianweiXing/events{/privacy}", "received_events_url": "https://api.github.com/users/TianweiXing/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-16T17:41:17Z", "updated_at": "2017-03-17T00:36:27Z", "closed_at": "2017-03-17T00:35:47Z", "author_association": "NONE", "body_html": "<p>I 'm trying to using tensorflow to do the inference task in a distributed way. I follow the doc here(<a href=\"https://www.tensorflow.org/deploy/distributed\" rel=\"nofollow\">https://www.tensorflow.org/deploy/distributed</a>), and I'm using the example naive MNIST network.<br>\n(codes shown below.)</p>\n<p>I use different workers on local machine. And since the network is small, I put different node on different workers.<br>\n<strong>What I want to know is:</strong><br>\n<strong>1. If I divide the graph into several parts, can they run in parallel?</strong><br>\nI know this graph is sequential/linear, (one part of the computation is dependent on the result of previous part, so if there's only 1 batch of data, it has to run sequentially.)<br>\nBut I want to know, if I have multiple batches of data, can they run parallelly in a pipeline fashion? (like Part 1 finished computing batch i data, send it to Part 2, and continue working on batch i+1 data)  ??</p>\n<p><strong>2. Like shown in the code, I want to output the computation for each part of the graph.</strong><br>\nHowever after running it, I noticed that, the output time is only the time for \"constructing the node/graph\", instead of running the graph. All the running is in the \"sess.run()\". And in this case, I cannot know the computation time for each part of the graph...</p>\n<p><strong>I 'm wondering if there's a way to show the computation time of each part of the graph?(or each node maybe?)</strong></p>\n<p>The code:</p>\n<pre><code>from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nimport time\nimport tensorflow as tf\n\nstart=time.time()\n\nx = tf.placeholder(tf.float32, [None, 784], name=\"input\")\n\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\", \"localhost:2224\"]})\n\nwith tf.device(\"/job:local/task:0\"):\n    W = tf.Variable(tf.zeros([784, 10]), name = \"w1\")\n    b = tf.Variable(tf.zeros([10]), name = \"b1\")\n\ntime1=time.time()\n#print (\"Time1: \"+str(time1-start)+\" seconds\")\n\nwith tf.device(\"/job:local/task:1\"):\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\n    y_ = tf.placeholder(tf.float32, [None, 10], name = \"output\")\n\ntime2=time.time()\n#print (\"Time2: \"+str(time2-time1)+\" seconds\")\nwith tf.device(\"/job:local/task:2\"):\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\ntime3=time.time()\n#print (\"Time3: \"+str(time3-time2)+\" seconds\")\n\n# swd--save the Checkpoint file\nsaver = tf.train.Saver()\n\n#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\nwith tf.Session(\"grpc://localhost:2224\") as sess:\n    # swd save ckpt\n    saver.restore(sess, \"saved_model/model.ckpt\")\n    print(\"\\nModel restored.\\n\")\n\n    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n    \ntime4=time.time()\n#print (\"Time4: \"+str(time4-time3)+\" seconds\")\n\nend=time.time()\nprint (\"Computing time: \"+str(end-start)+\" seconds\")\n</code></pre>", "body_text": "I 'm trying to using tensorflow to do the inference task in a distributed way. I follow the doc here(https://www.tensorflow.org/deploy/distributed), and I'm using the example naive MNIST network.\n(codes shown below.)\nI use different workers on local machine. And since the network is small, I put different node on different workers.\nWhat I want to know is:\n1. If I divide the graph into several parts, can they run in parallel?\nI know this graph is sequential/linear, (one part of the computation is dependent on the result of previous part, so if there's only 1 batch of data, it has to run sequentially.)\nBut I want to know, if I have multiple batches of data, can they run parallelly in a pipeline fashion? (like Part 1 finished computing batch i data, send it to Part 2, and continue working on batch i+1 data)  ??\n2. Like shown in the code, I want to output the computation for each part of the graph.\nHowever after running it, I noticed that, the output time is only the time for \"constructing the node/graph\", instead of running the graph. All the running is in the \"sess.run()\". And in this case, I cannot know the computation time for each part of the graph...\nI 'm wondering if there's a way to show the computation time of each part of the graph?(or each node maybe?)\nThe code:\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nimport time\nimport tensorflow as tf\n\nstart=time.time()\n\nx = tf.placeholder(tf.float32, [None, 784], name=\"input\")\n\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\", \"localhost:2224\"]})\n\nwith tf.device(\"/job:local/task:0\"):\n    W = tf.Variable(tf.zeros([784, 10]), name = \"w1\")\n    b = tf.Variable(tf.zeros([10]), name = \"b1\")\n\ntime1=time.time()\n#print (\"Time1: \"+str(time1-start)+\" seconds\")\n\nwith tf.device(\"/job:local/task:1\"):\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\n    y_ = tf.placeholder(tf.float32, [None, 10], name = \"output\")\n\ntime2=time.time()\n#print (\"Time2: \"+str(time2-time1)+\" seconds\")\nwith tf.device(\"/job:local/task:2\"):\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\ntime3=time.time()\n#print (\"Time3: \"+str(time3-time2)+\" seconds\")\n\n# swd--save the Checkpoint file\nsaver = tf.train.Saver()\n\n#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\nwith tf.Session(\"grpc://localhost:2224\") as sess:\n    # swd save ckpt\n    saver.restore(sess, \"saved_model/model.ckpt\")\n    print(\"\\nModel restored.\\n\")\n\n    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n    \ntime4=time.time()\n#print (\"Time4: \"+str(time4-time3)+\" seconds\")\n\nend=time.time()\nprint (\"Computing time: \"+str(end-start)+\" seconds\")", "body": "I 'm trying to using tensorflow to do the inference task in a distributed way. I follow the doc here(https://www.tensorflow.org/deploy/distributed), and I'm using the example naive MNIST network. \r\n(codes shown below.)\r\n\r\nI use different workers on local machine. And since the network is small, I put different node on different workers.\r\n**What I want to know is:**\r\n**1. If I divide the graph into several parts, can they run in parallel?** \r\nI know this graph is sequential/linear, (one part of the computation is dependent on the result of previous part, so if there's only 1 batch of data, it has to run sequentially.) \r\nBut I want to know, if I have multiple batches of data, can they run parallelly in a pipeline fashion? (like Part 1 finished computing batch i data, send it to Part 2, and continue working on batch i+1 data)  ??\r\n\r\n**2. Like shown in the code, I want to output the computation for each part of the graph.** \r\nHowever after running it, I noticed that, the output time is only the time for \"constructing the node/graph\", instead of running the graph. All the running is in the \"sess.run()\". And in this case, I cannot know the computation time for each part of the graph...\r\n\r\n**I 'm wondering if there's a way to show the computation time of each part of the graph?(or each node maybe?)**\r\n\r\nThe code:\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\nimport time\r\nimport tensorflow as tf\r\n\r\nstart=time.time()\r\n\r\nx = tf.placeholder(tf.float32, [None, 784], name=\"input\")\r\n\r\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\", \"localhost:2224\"]})\r\n\r\nwith tf.device(\"/job:local/task:0\"):\r\n    W = tf.Variable(tf.zeros([784, 10]), name = \"w1\")\r\n    b = tf.Variable(tf.zeros([10]), name = \"b1\")\r\n\r\ntime1=time.time()\r\n#print (\"Time1: \"+str(time1-start)+\" seconds\")\r\n\r\nwith tf.device(\"/job:local/task:1\"):\r\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\r\n    y_ = tf.placeholder(tf.float32, [None, 10], name = \"output\")\r\n\r\ntime2=time.time()\r\n#print (\"Time2: \"+str(time2-time1)+\" seconds\")\r\nwith tf.device(\"/job:local/task:2\"):\r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\ntime3=time.time()\r\n#print (\"Time3: \"+str(time3-time2)+\" seconds\")\r\n\r\n# swd--save the Checkpoint file\r\nsaver = tf.train.Saver()\r\n\r\n#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\r\nwith tf.Session(\"grpc://localhost:2224\") as sess:\r\n    # swd save ckpt\r\n    saver.restore(sess, \"saved_model/model.ckpt\")\r\n    print(\"\\nModel restored.\\n\")\r\n\r\n    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\r\n    \r\ntime4=time.time()\r\n#print (\"Time4: \"+str(time4-time3)+\" seconds\")\r\n\r\nend=time.time()\r\nprint (\"Computing time: \"+str(end-start)+\" seconds\")\r\n```"}