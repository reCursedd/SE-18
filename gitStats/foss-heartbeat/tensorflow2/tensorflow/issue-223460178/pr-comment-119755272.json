{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/119755272", "pull_request_review_id": 41665143, "id": 119755272, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExOTc1NTI3Mg==", "diff_hunk": "@@ -27,83 +27,114 @@ namespace tensorflow {\n \n typedef Eigen::GpuDevice GPUDevice;\n \n-template <typename T, typename Tindices, bool ADJ_A, bool ADJ_B>\n+template <typename T, typename Tindices, bool ADJ_A, bool ADJ_B, int NDIM>\n __global__ void SparseTensorDenseMatMulKernel(int nnz, int m, int b_rows,\n                                               int b_cols, int p,\n                                               const Tindices* a_indices,\n                                               const T* a_values, const T* b,\n-                                              T* out) {\n+                                              T* out, const int* skips) {\n   // out_{ij} = sum_k {a_ik b_kj}\n   // out = A * B', out_{ij} = sum_k {a_ik (b')_kj}; b'_{kj} = b_{jk}\n   const int n = (ADJ_B) ? b_cols : b_rows;\n   CUDA_1D_KERNEL_LOOP(index, nnz * p) {\n     const int a_ix = index / p;\n     const int j = index % p;\n-    const int i = ldg(a_indices + 2 * a_ix + ((ADJ_A) ? 1 : 0));\n-    const int k = ldg(a_indices + 2 * a_ix + ((ADJ_A) ? 0 : 1));\n+    const Tindices* a_index = a_indices + NDIM * a_ix;\n+    const int i = ldg(a_index + ((ADJ_A) ? NDIM - 1 : NDIM - 2));\n+    const int k = ldg(a_index + ((ADJ_A) ? NDIM - 2 : NDIM - 1));\n+    // matrices to skip\n+    int skip = 0;\n+    for (int ind = 0; ind < NDIM - 2; ++ind) {\n+      skip += ldg(a_index + ind) * skips[ind];\n+    }\n     if (!FastBoundsCheck(i, m)) {\n       continue;  // Nowhere to signal an error :(\n     }\n-    // out[i, j]\n-    T* out_location = out + i * p + j;\n+    // out[..., i, j]\n+    T* out_location = out + skip * m * p + i * p + j;\n     if (!FastBoundsCheck(k, n)) {\n       CudaAtomicAdd(out_location, std::numeric_limits<T>::quiet_NaN());\n       continue;\n     }\n \n-    // a_value == (ADJ_A) ? a[k, i] : a[i, k]\n+    // a_value == (ADJ_A) ? a[..., k, i] : a[..., i, k]\n     const T a_value = ldg(a_values + a_ix);\n \n-    // b_value == (ADJ_B) ? b[j, k] : b[k, j]\n-    const T b_value = ldg(b + ((ADJ_B) ? j * b_cols + k : k * b_cols + j));\n+    // b_value == (ADJ_B) ? b[..., j, k] : b[..., k, j]\n+    const T b_value = ldg(b\n+                          + skip * b_rows * b_cols\n+                          + ((ADJ_B) ? j * b_cols + k : k * b_cols + j));\n     CudaAtomicAdd(out_location, a_value * b_value);\n   }\n }\n \n namespace functor {\n \n-template <typename T, typename Tindices, bool ADJ_A, bool ADJ_B>\n-struct SparseTensorDenseMatMulFunctor<GPUDevice, T, Tindices, ADJ_A, ADJ_B> {\n-  static EIGEN_ALWAYS_INLINE Status\n-  Compute(const GPUDevice& d, typename TTypes<T>::Matrix out,\n-          typename TTypes<Tindices>::ConstMatrix a_indices,\n-          typename TTypes<T>::ConstVec a_values,\n-          typename TTypes<T>::ConstMatrix b) {\n+template <typename T, typename Tindices, bool ADJ_A, bool ADJ_B, int NDIM>\n+struct SparseTensorDenseMatMulFunctor<GPUDevice, T, Tindices, ADJ_A, ADJ_B, NDIM> {\n+  static EIGEN_ALWAYS_INLINE Status Compute(\n+      const GPUDevice& d,\n+      typename TTypes<T, NDIM>::Tensor out,\n+      typename TTypes<Tindices>::ConstMatrix a_indices,\n+      typename TTypes<T>::ConstVec a_values,\n+      typename TTypes<T, NDIM>::ConstTensor b) {\n     out.device(d) = out.constant(T(0));\n     int nnz = a_values.size();\n-    // out = A * B, A is [m x n] and B is [n x p], out is [m x p]\n-    int m = out.dimension(0);\n-    int p = out.dimension(1);\n-    int b_rows = b.dimension(0);\n-    int b_cols = b.dimension(1);\n+    // out = A * B, A is [... x m x n] and B is [... x n x p], out is [... x m x p]\n+    int m = out.dimension(NDIM-2);\n+    int p = out.dimension(NDIM-1);\n+    int b_rows = b.dimension(NDIM-2);\n+    int b_cols = b.dimension(NDIM-1);\n+    \n+    // stores matrices to skip:\n+    // e.g.: if out.dims = {6, 5, 3, 4}, then skips = {5, 1, 1}\n+    //       if a_index = {4, 3, 2, 1}, then matrices to skip = 4*5 + 3*1\n+    int* skips = new int[NDIM-1];\n+    skips[NDIM-2] = 1;\n+    if (NDIM != 2) {\n+      skips[NDIM-3] = 1;\n+    }\n+    for (int i = NDIM - 4; i >= 0; --i) {\n+      skips[i] = skips[i+1] * out.dimension(i+1);\n+    }\n \n     // TODO(ebrevdo): Should this be alpha * nnz instead of\n     // out.size()?  Perhaps p * nnz ?\n     CudaLaunchConfig config = GetCudaLaunchConfig(p * nnz, d);\n \n-    SparseTensorDenseMatMulKernel<T, Tindices, ADJ_A, ADJ_B>\n+    SparseTensorDenseMatMulKernel<T, Tindices, ADJ_A, ADJ_B, NDIM>\n         <<<config.block_count, config.thread_per_block, 0, d.stream()>>>(\n             nnz, m, b_rows, b_cols, p, a_indices.data(), a_values.data(),\n-            b.data(), out.data());\n+            b.data(), out.data(), skips);\n+            \n+    delete [] skips;\n \n     return Status::OK();\n   }\n };\n \n }  // namespace functor\n \n-#define DEFINE(T, Tindices)                                \\\n-  template struct functor::SparseTensorDenseMatMulFunctor< \\\n-      GPUDevice, T, Tindices, false, false>;               \\\n-  template struct functor::SparseTensorDenseMatMulFunctor< \\\n-      GPUDevice, T, Tindices, false, true>;                \\\n-  template struct functor::SparseTensorDenseMatMulFunctor< \\\n-      GPUDevice, T, Tindices, true, false>;                \\\n-  template struct functor::SparseTensorDenseMatMulFunctor< \\\n-      GPUDevice, T, Tindices, true, true>;\n-\n-DEFINE(float, int32);\n-DEFINE(float, int64);\n+#define DEFINE(T, Tindices, NDIM)                                              \\", "path": "tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc", "position": null, "original_position": 127, "commit_id": "b365ec3d21be7959f5c237f11a0d334b1d6f597e", "original_commit_id": "9bdda292f53108021a7fb7c26a0f94dc5b1195d4", "user": {"login": "zycdragonball", "id": 26096551, "node_id": "MDQ6VXNlcjI2MDk2NTUx", "avatar_url": "https://avatars0.githubusercontent.com/u/26096551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zycdragonball", "html_url": "https://github.com/zycdragonball", "followers_url": "https://api.github.com/users/zycdragonball/followers", "following_url": "https://api.github.com/users/zycdragonball/following{/other_user}", "gists_url": "https://api.github.com/users/zycdragonball/gists{/gist_id}", "starred_url": "https://api.github.com/users/zycdragonball/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zycdragonball/subscriptions", "organizations_url": "https://api.github.com/users/zycdragonball/orgs", "repos_url": "https://api.github.com/users/zycdragonball/repos", "events_url": "https://api.github.com/users/zycdragonball/events{/privacy}", "received_events_url": "https://api.github.com/users/zycdragonball/received_events", "type": "User", "site_admin": false}, "body": "Done.", "created_at": "2017-06-01T23:47:01Z", "updated_at": "2017-07-23T22:46:49Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9373#discussion_r119755272", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9373", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/119755272"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9373#discussion_r119755272"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9373"}}, "body_html": "<p>Done.</p>", "body_text": "Done.", "in_reply_to_id": 119233009}