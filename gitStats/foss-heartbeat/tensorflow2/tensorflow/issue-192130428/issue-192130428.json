{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5914", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5914/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5914/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5914/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5914", "id": 192130428, "node_id": "MDU6SXNzdWUxOTIxMzA0Mjg=", "number": 5914, "title": "No documentation for sending input parameters of request to TensorFlow Serving ", "user": {"login": "vasantivmahajan", "id": 17990840, "node_id": "MDQ6VXNlcjE3OTkwODQw", "avatar_url": "https://avatars0.githubusercontent.com/u/17990840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vasantivmahajan", "html_url": "https://github.com/vasantivmahajan", "followers_url": "https://api.github.com/users/vasantivmahajan/followers", "following_url": "https://api.github.com/users/vasantivmahajan/following{/other_user}", "gists_url": "https://api.github.com/users/vasantivmahajan/gists{/gist_id}", "starred_url": "https://api.github.com/users/vasantivmahajan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vasantivmahajan/subscriptions", "organizations_url": "https://api.github.com/users/vasantivmahajan/orgs", "repos_url": "https://api.github.com/users/vasantivmahajan/repos", "events_url": "https://api.github.com/users/vasantivmahajan/events{/privacy}", "received_events_url": "https://api.github.com/users/vasantivmahajan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-11-28T22:07:48Z", "updated_at": "2016-12-07T21:34:38Z", "closed_at": "2016-12-07T21:34:38Z", "author_association": "NONE", "body_html": "<p>I am running the sample iris program in TensorFlow Serving. Since it is a TF.Learn model, I am exporting the model using the following <code>classifier.export(export_dir=model_dir,signature_fn=my_classification_signature_fn)</code></p>\n<p>and the signature_fn is defined as shown below:</p>\n<pre><code>def my_classification_signature_fn(examples, unused_features, predictions):\n  \"\"\"Creates classification signature from given examples and predictions.\n  Args:\n    examples: `Tensor`.\n    unused_features: `dict` of `Tensor`s.\n    predictions: `Tensor` or dict of tensors that contains the classes tensor\n      as in {'classes': `Tensor`}.\n  Returns:\n    Tuple of default classification signature and empty named signatures.\n  Raises:\n    ValueError: If examples is `None`.\n  \"\"\"\n  if examples is None:\n    raise ValueError('examples cannot be None when using this signature fn.')\n\n  if isinstance(predictions, dict):\n    default_signature = exporter.classification_signature(\n        examples, classes_tensor=predictions['classes'])\n\n  else:\n\n    default_signature = exporter.classification_signature(\n        examples, classes_tensor=predictions)\n  named_graph_signatures={\n        'inputs': exporter.generic_signature({'x_values': examples}),\n        'outputs': exporter.generic_signature({'preds': predictions})}    \n  return default_signature, named_graph_signatures\n</code></pre>\n<p>The model gets successfully exported using the following piece of code.</p>\n<p>I have created a client which makes real-time predictions using TensorFlow Serving.</p>\n<p>The following is the code for the client:</p>\n<pre><code>flags.DEFINE_string(\"model_dir\", \"/tmp/iris_model_dir\", \"Base directory for output models.\")\ntf.app.flags.DEFINE_integer('concurrency', 1,\n                            'maximum number of concurrent inference requests')\ntf.app.flags.DEFINE_string('server', '', 'PredictionService host:port')\n\n#connection\nhost, port = FLAGS.server.split(':')\nchannel = implementations.insecure_channel(host, int(port))\nstub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n\n# Classify two new flower samples.\nnew_samples = np.array([5.8, 3.1, 5.0, 1.7], dtype=float)\n\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = 'iris'\n\nrequest.inputs[\"x_values\"].CopyFrom(\n        tf.contrib.util.make_tensor_proto(new_samples))\n\nresult = stub.Predict(request, 10.0)  # 10 secs timeout\n</code></pre>\n<p>However, on making the predictions, the following error is displayed:</p>\n<p><code>grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type string for node _recv_input_example_tensor_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=2016246895612781641, tensor_name=\"input_example_tensor:0\", tensor_type=DT_STRING, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")</code></p>\n<p>Here is the entire stack trace.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/17990840/20688020/fac6285a-b58c-11e6-8a93-1fed6d78b8de.png\"><img src=\"https://cloud.githubusercontent.com/assets/17990840/20688020/fac6285a-b58c-11e6-8a93-1fed6d78b8de.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>The iris model is defined in the following manner:</p>\n<pre><code># Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=3, model_dir=model_dir)\n\n# Fit model.\nclassifier.fit(x=training_set.data, \n               y=training_set.target, \n               steps=2000)\n</code></pre>\n<p>Can someone provide some documentation for creating the client program which sends the request to TensorFlow Serving.</p>", "body_text": "I am running the sample iris program in TensorFlow Serving. Since it is a TF.Learn model, I am exporting the model using the following classifier.export(export_dir=model_dir,signature_fn=my_classification_signature_fn)\nand the signature_fn is defined as shown below:\ndef my_classification_signature_fn(examples, unused_features, predictions):\n  \"\"\"Creates classification signature from given examples and predictions.\n  Args:\n    examples: `Tensor`.\n    unused_features: `dict` of `Tensor`s.\n    predictions: `Tensor` or dict of tensors that contains the classes tensor\n      as in {'classes': `Tensor`}.\n  Returns:\n    Tuple of default classification signature and empty named signatures.\n  Raises:\n    ValueError: If examples is `None`.\n  \"\"\"\n  if examples is None:\n    raise ValueError('examples cannot be None when using this signature fn.')\n\n  if isinstance(predictions, dict):\n    default_signature = exporter.classification_signature(\n        examples, classes_tensor=predictions['classes'])\n\n  else:\n\n    default_signature = exporter.classification_signature(\n        examples, classes_tensor=predictions)\n  named_graph_signatures={\n        'inputs': exporter.generic_signature({'x_values': examples}),\n        'outputs': exporter.generic_signature({'preds': predictions})}    \n  return default_signature, named_graph_signatures\n\nThe model gets successfully exported using the following piece of code.\nI have created a client which makes real-time predictions using TensorFlow Serving.\nThe following is the code for the client:\nflags.DEFINE_string(\"model_dir\", \"/tmp/iris_model_dir\", \"Base directory for output models.\")\ntf.app.flags.DEFINE_integer('concurrency', 1,\n                            'maximum number of concurrent inference requests')\ntf.app.flags.DEFINE_string('server', '', 'PredictionService host:port')\n\n#connection\nhost, port = FLAGS.server.split(':')\nchannel = implementations.insecure_channel(host, int(port))\nstub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n\n# Classify two new flower samples.\nnew_samples = np.array([5.8, 3.1, 5.0, 1.7], dtype=float)\n\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = 'iris'\n\nrequest.inputs[\"x_values\"].CopyFrom(\n        tf.contrib.util.make_tensor_proto(new_samples))\n\nresult = stub.Predict(request, 10.0)  # 10 secs timeout\n\nHowever, on making the predictions, the following error is displayed:\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type string for node _recv_input_example_tensor_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=2016246895612781641, tensor_name=\"input_example_tensor:0\", tensor_type=DT_STRING, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")\nHere is the entire stack trace.\n\nThe iris model is defined in the following manner:\n# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=3, model_dir=model_dir)\n\n# Fit model.\nclassifier.fit(x=training_set.data, \n               y=training_set.target, \n               steps=2000)\n\nCan someone provide some documentation for creating the client program which sends the request to TensorFlow Serving.", "body": "I am running the sample iris program in TensorFlow Serving. Since it is a TF.Learn model, I am exporting the model using the following `classifier.export(export_dir=model_dir,signature_fn=my_classification_signature_fn)`\r\n\r\nand the signature_fn is defined as shown below:\r\n\r\n```\r\ndef my_classification_signature_fn(examples, unused_features, predictions):\r\n  \"\"\"Creates classification signature from given examples and predictions.\r\n  Args:\r\n    examples: `Tensor`.\r\n    unused_features: `dict` of `Tensor`s.\r\n    predictions: `Tensor` or dict of tensors that contains the classes tensor\r\n      as in {'classes': `Tensor`}.\r\n  Returns:\r\n    Tuple of default classification signature and empty named signatures.\r\n  Raises:\r\n    ValueError: If examples is `None`.\r\n  \"\"\"\r\n  if examples is None:\r\n    raise ValueError('examples cannot be None when using this signature fn.')\r\n\r\n  if isinstance(predictions, dict):\r\n    default_signature = exporter.classification_signature(\r\n        examples, classes_tensor=predictions['classes'])\r\n\r\n  else:\r\n\r\n    default_signature = exporter.classification_signature(\r\n        examples, classes_tensor=predictions)\r\n  named_graph_signatures={\r\n        'inputs': exporter.generic_signature({'x_values': examples}),\r\n        'outputs': exporter.generic_signature({'preds': predictions})}    \r\n  return default_signature, named_graph_signatures\r\n```\r\nThe model gets successfully exported using the following piece of code.\r\n\r\nI have created a client which makes real-time predictions using TensorFlow Serving.\r\n\r\nThe following is the code for the client:\r\n\r\n```\r\nflags.DEFINE_string(\"model_dir\", \"/tmp/iris_model_dir\", \"Base directory for output models.\")\r\ntf.app.flags.DEFINE_integer('concurrency', 1,\r\n                            'maximum number of concurrent inference requests')\r\ntf.app.flags.DEFINE_string('server', '', 'PredictionService host:port')\r\n\r\n#connection\r\nhost, port = FLAGS.server.split(':')\r\nchannel = implementations.insecure_channel(host, int(port))\r\nstub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n\r\n# Classify two new flower samples.\r\nnew_samples = np.array([5.8, 3.1, 5.0, 1.7], dtype=float)\r\n\r\nrequest = predict_pb2.PredictRequest()\r\nrequest.model_spec.name = 'iris'\r\n\r\nrequest.inputs[\"x_values\"].CopyFrom(\r\n        tf.contrib.util.make_tensor_proto(new_samples))\r\n\r\nresult = stub.Predict(request, 10.0)  # 10 secs timeout\r\n```\r\n\r\nHowever, on making the predictions, the following error is displayed:\r\n\r\n`grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details=\"Output 0 of type double does not match declared output type string for node _recv_input_example_tensor_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=2016246895612781641, tensor_name=\"input_example_tensor:0\", tensor_type=DT_STRING, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")`\r\n\r\nHere is the entire stack trace.\r\n![image](https://cloud.githubusercontent.com/assets/17990840/20688020/fac6285a-b58c-11e6-8a93-1fed6d78b8de.png)\r\n\r\nThe iris model is defined in the following manner:\r\n\r\n```\r\n# Specify that all features have real-value data\r\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\r\n\r\n# Build 3 layer DNN with 10, 20, 10 units respectively.\r\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\r\n                                            hidden_units=[10, 20, 10],\r\n                                            n_classes=3, model_dir=model_dir)\r\n\r\n# Fit model.\r\nclassifier.fit(x=training_set.data, \r\n               y=training_set.target, \r\n               steps=2000)\r\n```\r\n\r\nCan someone provide some documentation for creating the client program which sends the request to TensorFlow Serving. "}