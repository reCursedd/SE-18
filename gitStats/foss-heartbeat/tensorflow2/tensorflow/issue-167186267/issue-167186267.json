{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3478", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3478/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3478/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3478/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3478", "id": 167186267, "node_id": "MDU6SXNzdWUxNjcxODYyNjc=", "number": 3478, "title": "How to restore a distributed model and continue to train it with more workers?", "user": {"login": "perhapszzy", "id": 7953637, "node_id": "MDQ6VXNlcjc5NTM2Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7953637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/perhapszzy", "html_url": "https://github.com/perhapszzy", "followers_url": "https://api.github.com/users/perhapszzy/followers", "following_url": "https://api.github.com/users/perhapszzy/following{/other_user}", "gists_url": "https://api.github.com/users/perhapszzy/gists{/gist_id}", "starred_url": "https://api.github.com/users/perhapszzy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/perhapszzy/subscriptions", "organizations_url": "https://api.github.com/users/perhapszzy/orgs", "repos_url": "https://api.github.com/users/perhapszzy/repos", "events_url": "https://api.github.com/users/perhapszzy/events{/privacy}", "received_events_url": "https://api.github.com/users/perhapszzy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jmchen-g", "id": 15792374, "node_id": "MDQ6VXNlcjE1NzkyMzc0", "avatar_url": "https://avatars3.githubusercontent.com/u/15792374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmchen-g", "html_url": "https://github.com/jmchen-g", "followers_url": "https://api.github.com/users/jmchen-g/followers", "following_url": "https://api.github.com/users/jmchen-g/following{/other_user}", "gists_url": "https://api.github.com/users/jmchen-g/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmchen-g/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmchen-g/subscriptions", "organizations_url": "https://api.github.com/users/jmchen-g/orgs", "repos_url": "https://api.github.com/users/jmchen-g/repos", "events_url": "https://api.github.com/users/jmchen-g/events{/privacy}", "received_events_url": "https://api.github.com/users/jmchen-g/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jmchen-g", "id": 15792374, "node_id": "MDQ6VXNlcjE1NzkyMzc0", "avatar_url": "https://avatars3.githubusercontent.com/u/15792374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmchen-g", "html_url": "https://github.com/jmchen-g", "followers_url": "https://api.github.com/users/jmchen-g/followers", "following_url": "https://api.github.com/users/jmchen-g/following{/other_user}", "gists_url": "https://api.github.com/users/jmchen-g/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmchen-g/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmchen-g/subscriptions", "organizations_url": "https://api.github.com/users/jmchen-g/orgs", "repos_url": "https://api.github.com/users/jmchen-g/repos", "events_url": "https://api.github.com/users/jmchen-g/events{/privacy}", "received_events_url": "https://api.github.com/users/jmchen-g/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-07-23T14:21:46Z", "updated_at": "2017-01-23T23:11:38Z", "closed_at": "2017-01-23T23:11:38Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>Running distributed tensorflow on kubernetes.</p>\n<p>Installed version of CUDA and cuDNN:  No</p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Train a model with 3 workers</li>\n<li>restore the model with 4 workers</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [4] rhs shape= [3]\n     [[Node: save/Assign_103 = Assign[T=DT_INT64, _class=[\"loc:@local_steps\"], use_locking=true, validate_shape=true, _device=\"/job:ps/replica:0/task:0/cpu:0\"](local_steps, save/restore_slice_103)]]\n     [[Node: save/restore_all/NoOp_S6 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=6655275555388235088, tensor_name=\"edge_9844_save/restore_all/NoOp\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<p>It seems that we need to have the same number of workers to restore a model. Is it possible to increase the number of workers without retraining the model from beginning?</p>", "body_text": "Environment info\nRunning distributed tensorflow on kubernetes.\nInstalled version of CUDA and cuDNN:  No\nSteps to reproduce\n\nTrain a model with 3 workers\nrestore the model with 4 workers\n\nLogs or other output that would be helpful\ntensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [4] rhs shape= [3]\n     [[Node: save/Assign_103 = Assign[T=DT_INT64, _class=[\"loc:@local_steps\"], use_locking=true, validate_shape=true, _device=\"/job:ps/replica:0/task:0/cpu:0\"](local_steps, save/restore_slice_103)]]\n     [[Node: save/restore_all/NoOp_S6 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=6655275555388235088, tensor_name=\"edge_9844_save/restore_all/NoOp\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nIt seems that we need to have the same number of workers to restore a model. Is it possible to increase the number of workers without retraining the model from beginning?", "body": "### Environment info\n\nRunning distributed tensorflow on kubernetes.\n\nInstalled version of CUDA and cuDNN:  No\n### Steps to reproduce\n1. Train a model with 3 workers\n2. restore the model with 4 workers\n### Logs or other output that would be helpful\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [4] rhs shape= [3]\n     [[Node: save/Assign_103 = Assign[T=DT_INT64, _class=[\"loc:@local_steps\"], use_locking=true, validate_shape=true, _device=\"/job:ps/replica:0/task:0/cpu:0\"](local_steps, save/restore_slice_103)]]\n     [[Node: save/restore_all/NoOp_S6 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=6655275555388235088, tensor_name=\"edge_9844_save/restore_all/NoOp\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n```\n\nIt seems that we need to have the same number of workers to restore a model. Is it possible to increase the number of workers without retraining the model from beginning?\n"}