{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264966005", "html_url": "https://github.com/tensorflow/tensorflow/pull/5954#issuecomment-264966005", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5954", "id": 264966005, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDk2NjAwNQ==", "user": {"login": "philwo", "id": 504652, "node_id": "MDQ6VXNlcjUwNDY1Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/504652?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philwo", "html_url": "https://github.com/philwo", "followers_url": "https://api.github.com/users/philwo/followers", "following_url": "https://api.github.com/users/philwo/following{/other_user}", "gists_url": "https://api.github.com/users/philwo/gists{/gist_id}", "starred_url": "https://api.github.com/users/philwo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philwo/subscriptions", "organizations_url": "https://api.github.com/users/philwo/orgs", "repos_url": "https://api.github.com/users/philwo/repos", "events_url": "https://api.github.com/users/philwo/events{/privacy}", "received_events_url": "https://api.github.com/users/philwo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-05T20:22:50Z", "updated_at": "2016-12-05T20:22:50Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7946809\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gunan\">@gunan</a>,</p>\n<p>sorry for the delay! I will try to help you out. :) But I don't know where to get the build time data / logs regarding your builds and what to compare them to (what does \"twice\" mean? Also, I only see a failed build for \"Linux GPU\", so how can we compare performance to a usually succeeding build here?).</p>\n<p>Here are my best guesses:</p>\n<ol>\n<li>Regarding the build failure: The only thing I can see related to something called \"Linux GPU\" is a failed build mentioned above in the GitHub merge thingy. There is an error in the console log of that build:</li>\n</ol>\n<p>process-wrapper: execvp(\"external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\", ...): No such file or directory</p>\n<p>(from <a href=\"https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2801/consoleFull\" rel=\"nofollow\">https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2801/consoleFull</a>)</p>\n<p>My guess it that this Skylark file: <a href=\"https://github.com/tensorflow/tensorflow/blob/48fb73a1c94ee2409382225428063d3496dc651e/third_party/gpus/cuda_configure.bzl\">https://github.com/tensorflow/tensorflow/blob/48fb73a1c94ee2409382225428063d3496dc651e/third_party/gpus/cuda_configure.bzl</a> somehow doesn't add that \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" file to the input files required when building with Cuda. This means it won't be included in the sandbox for actions using Cuda, but will appear to work just fine when not using the standalone strategy. The fix is to add that file (+ its dependencies) to the Skylark repository you're building in that script.</p>\n<ol start=\"2\">\n<li>Regarding the build performance, if you have many input files and a slow disk with high latency (and cloud based VMs have in my experience a ridiculously high disk latency when compared to real hardware, even when they're using \"SSDs\") then you will very probably hit the currently only known performance issue with the sandbox, which is that you're limited by your disk / filesystem when building the symlink tree. The long-term fix for this will very probably involve using FUSE to generate the sandbox directory without actually doing any I/O. A short-term fix is something I'm currently working on. I hope I can get it done this or next week. From benchmarks run by one of our users who suffered from a similar issue, this fully resolves the performance problem.</li>\n</ol>\n<p>Hope that helps,<br>\nPhilipp</p>", "body_text": "Hi @gunan,\nsorry for the delay! I will try to help you out. :) But I don't know where to get the build time data / logs regarding your builds and what to compare them to (what does \"twice\" mean? Also, I only see a failed build for \"Linux GPU\", so how can we compare performance to a usually succeeding build here?).\nHere are my best guesses:\n\nRegarding the build failure: The only thing I can see related to something called \"Linux GPU\" is a failed build mentioned above in the GitHub merge thingy. There is an error in the console log of that build:\n\nprocess-wrapper: execvp(\"external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\", ...): No such file or directory\n(from https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2801/consoleFull)\nMy guess it that this Skylark file: https://github.com/tensorflow/tensorflow/blob/48fb73a1c94ee2409382225428063d3496dc651e/third_party/gpus/cuda_configure.bzl somehow doesn't add that \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" file to the input files required when building with Cuda. This means it won't be included in the sandbox for actions using Cuda, but will appear to work just fine when not using the standalone strategy. The fix is to add that file (+ its dependencies) to the Skylark repository you're building in that script.\n\nRegarding the build performance, if you have many input files and a slow disk with high latency (and cloud based VMs have in my experience a ridiculously high disk latency when compared to real hardware, even when they're using \"SSDs\") then you will very probably hit the currently only known performance issue with the sandbox, which is that you're limited by your disk / filesystem when building the symlink tree. The long-term fix for this will very probably involve using FUSE to generate the sandbox directory without actually doing any I/O. A short-term fix is something I'm currently working on. I hope I can get it done this or next week. From benchmarks run by one of our users who suffered from a similar issue, this fully resolves the performance problem.\n\nHope that helps,\nPhilipp", "body": "Hi @gunan,\r\n\r\nsorry for the delay! I will try to help you out. :) But I don't know where to get the build time data / logs regarding your builds and what to compare them to (what does \"twice\" mean? Also, I only see a failed build for \"Linux GPU\", so how can we compare performance to a usually succeeding build here?).\r\n\r\nHere are my best guesses:\r\n\r\n1) Regarding the build failure: The only thing I can see related to something called \"Linux GPU\" is a failed build mentioned above in the GitHub merge thingy. There is an error in the console log of that build:\r\n\r\nprocess-wrapper: execvp(\"external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\", ...): No such file or directory\r\n\r\n(from https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2801/consoleFull)\r\n\r\nMy guess it that this Skylark file: https://github.com/tensorflow/tensorflow/blob/48fb73a1c94ee2409382225428063d3496dc651e/third_party/gpus/cuda_configure.bzl somehow doesn't add that \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" file to the input files required when building with Cuda. This means it won't be included in the sandbox for actions using Cuda, but will appear to work just fine when not using the standalone strategy. The fix is to add that file (+ its dependencies) to the Skylark repository you're building in that script.\r\n\r\n2) Regarding the build performance, if you have many input files and a slow disk with high latency (and cloud based VMs have in my experience a ridiculously high disk latency when compared to real hardware, even when they're using \"SSDs\") then you will very probably hit the currently only known performance issue with the sandbox, which is that you're limited by your disk / filesystem when building the symlink tree. The long-term fix for this will very probably involve using FUSE to generate the sandbox directory without actually doing any I/O. A short-term fix is something I'm currently working on. I hope I can get it done this or next week. From benchmarks run by one of our users who suffered from a similar issue, this fully resolves the performance problem.\r\n\r\nHope that helps,\r\nPhilipp"}