{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5252", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5252/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5252/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5252/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5252", "id": 185833247, "node_id": "MDU6SXNzdWUxODU4MzMyNDc=", "number": 5252, "title": "multigpu gradient averaging if grad_op is IndexedSlices; race condition in apply_gradients?", "user": {"login": "MInner", "id": 5229267, "node_id": "MDQ6VXNlcjUyMjkyNjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5229267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MInner", "html_url": "https://github.com/MInner", "followers_url": "https://api.github.com/users/MInner/followers", "following_url": "https://api.github.com/users/MInner/following{/other_user}", "gists_url": "https://api.github.com/users/MInner/gists{/gist_id}", "starred_url": "https://api.github.com/users/MInner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MInner/subscriptions", "organizations_url": "https://api.github.com/users/MInner/orgs", "repos_url": "https://api.github.com/users/MInner/repos", "events_url": "https://api.github.com/users/MInner/events{/privacy}", "received_events_url": "https://api.github.com/users/MInner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-10-28T03:55:02Z", "updated_at": "2018-08-13T09:00:59Z", "closed_at": "2017-06-16T18:07:10Z", "author_association": "NONE", "body_html": "<p>If one follows the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py\"><code>cifar10_multi_gpu_train.py</code></a> example (build two towers that share weights, feed data independently, collect gradients, average them, apply then) and passes gradients to the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L110\"><code>average_gradients()</code></a> function, but graph uses <code>tf.gather</code> consequently, the resulting gradient op will be <code>IndexedSlices</code> that has many <code>-1</code> values in <code>indices</code> field (from my experience, not sure about why this happens), so when tensorflow will attempt to apply <code>tensorflow.python.ops.gradients._IndexedSlicesToTensor</code> to convert it to Tensor (to apply <code>tf.expand_dims</code> on it), it will fail on <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients.py#L92\"><code>unsorted_segment_sum</code></a> , because <code>-1</code>s are indeed not in the indexes range. I have explicitly printed <code>w_grad_op.indices</code> and they looked like:</p>\n<pre><code>[0 1 2 3 6 4 5 6 7 8 9 13 12 11 15 16 ... 230 231 -1 -1 ... -1 280 -1 -1 265 -1 -1 ... -1]\n</code></pre>\n<p>- so the <code>average_gradients()</code> indeed fails on these examples (people have been reporting similar problems <a href=\"http://stackoverflow.com/questions/37074077/fail-to-average-indexedslices-on-porting-ptb-word-lm-py-to-multi-gpu-towers\" rel=\"nofollow\">[1]</a>, <a href=\"http://stackoverflow.com/questions/39017896/tensorflow-how-to-average-several-indexedslicesvalue\" rel=\"nofollow\">[2]</a> .. 2-3 more around the web).</p>\n<p>One way of dealing with (solution 1) it is just applying gradients consequentially (with lower learning rate) instead of averaging them first:</p>\n<pre><code> train_op = tf.group(*[gd.apply_gradients(grad) for grad in tower_grads])\n</code></pre>\n<p>-  this works great and scales nicely, but (in my case) led to some weird convergence issues (eventually leading to <code>nan</code>s popping here and there), possibly because of some sort of race conditions (?), while applying gradients (<code>gd.apply_gradients()</code> is probably not very atomic). One possible workaround (solution 2) might be to use <code>tf.control_dependencies</code>:</p>\n<pre><code>def rigid_op_sequence(op_lambdas):\n    ## equivalent to:\n    ## op = op1()\n    ## with.control_dependencies([op]):\n    ##    op = op2()\n    ##    with.control_dependencies([op]):\n    ##        op = op3()\n    ##        ...\n    ## return op\n\n    with ExitStack() as stack:\n        for op_func in op_lambdas:\n            op = op_func()\n            context = tf.control_dependencies([op])\n            stack.enter_context(context)\n    return op\n\ngrad_update_lambdas = [(lambda: gd.apply_gradients(grad)) for grad in tower_grads]\ntrain_op = rigid_op_sequence(grad_update_lambdas)\n</code></pre>\n<p>however this solution seems to somewhat significantly degenerate performance, because of that \"blocking\" (GPU load dropped from ~90% to ~60%).</p>\n<p>Several fundamental questions:</p>\n<ol>\n<li>What is officially recommended way of dealing with this \"multigpu IndexedSlices\" case? (could not find any; people keep asking)</li>\n<li>Does (solution 1) indeed might experience race condition or tensorflow handles it somehow and my convergences issues were not related to that? (and we all can just use solution 1)</li>\n</ol>", "body_text": "If one follows the cifar10_multi_gpu_train.py example (build two towers that share weights, feed data independently, collect gradients, average them, apply then) and passes gradients to the average_gradients() function, but graph uses tf.gather consequently, the resulting gradient op will be IndexedSlices that has many -1 values in indices field (from my experience, not sure about why this happens), so when tensorflow will attempt to apply tensorflow.python.ops.gradients._IndexedSlicesToTensor to convert it to Tensor (to apply tf.expand_dims on it), it will fail on unsorted_segment_sum , because -1s are indeed not in the indexes range. I have explicitly printed w_grad_op.indices and they looked like:\n[0 1 2 3 6 4 5 6 7 8 9 13 12 11 15 16 ... 230 231 -1 -1 ... -1 280 -1 -1 265 -1 -1 ... -1]\n\n- so the average_gradients() indeed fails on these examples (people have been reporting similar problems [1], [2] .. 2-3 more around the web).\nOne way of dealing with (solution 1) it is just applying gradients consequentially (with lower learning rate) instead of averaging them first:\n train_op = tf.group(*[gd.apply_gradients(grad) for grad in tower_grads])\n\n-  this works great and scales nicely, but (in my case) led to some weird convergence issues (eventually leading to nans popping here and there), possibly because of some sort of race conditions (?), while applying gradients (gd.apply_gradients() is probably not very atomic). One possible workaround (solution 2) might be to use tf.control_dependencies:\ndef rigid_op_sequence(op_lambdas):\n    ## equivalent to:\n    ## op = op1()\n    ## with.control_dependencies([op]):\n    ##    op = op2()\n    ##    with.control_dependencies([op]):\n    ##        op = op3()\n    ##        ...\n    ## return op\n\n    with ExitStack() as stack:\n        for op_func in op_lambdas:\n            op = op_func()\n            context = tf.control_dependencies([op])\n            stack.enter_context(context)\n    return op\n\ngrad_update_lambdas = [(lambda: gd.apply_gradients(grad)) for grad in tower_grads]\ntrain_op = rigid_op_sequence(grad_update_lambdas)\n\nhowever this solution seems to somewhat significantly degenerate performance, because of that \"blocking\" (GPU load dropped from ~90% to ~60%).\nSeveral fundamental questions:\n\nWhat is officially recommended way of dealing with this \"multigpu IndexedSlices\" case? (could not find any; people keep asking)\nDoes (solution 1) indeed might experience race condition or tensorflow handles it somehow and my convergences issues were not related to that? (and we all can just use solution 1)", "body": "If one follows the [`cifar10_multi_gpu_train.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py) example (build two towers that share weights, feed data independently, collect gradients, average them, apply then) and passes gradients to the [`average_gradients()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L110) function, but graph uses `tf.gather` consequently, the resulting gradient op will be `IndexedSlices` that has many `-1` values in `indices` field (from my experience, not sure about why this happens), so when tensorflow will attempt to apply `tensorflow.python.ops.gradients._IndexedSlicesToTensor` to convert it to Tensor (to apply `tf.expand_dims` on it), it will fail on [`unsorted_segment_sum`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients.py#L92) , because `-1`s are indeed not in the indexes range. I have explicitly printed `w_grad_op.indices` and they looked like: \n\n```\n[0 1 2 3 6 4 5 6 7 8 9 13 12 11 15 16 ... 230 231 -1 -1 ... -1 280 -1 -1 265 -1 -1 ... -1]\n```\n\n\\- so the `average_gradients()` indeed fails on these examples (people have been reporting similar problems [[1]](http://stackoverflow.com/questions/37074077/fail-to-average-indexedslices-on-porting-ptb-word-lm-py-to-multi-gpu-towers), [[2]](http://stackoverflow.com/questions/39017896/tensorflow-how-to-average-several-indexedslicesvalue) .. 2-3 more around the web).\n\nOne way of dealing with (solution 1) it is just applying gradients consequentially (with lower learning rate) instead of averaging them first:\n\n```\n train_op = tf.group(*[gd.apply_gradients(grad) for grad in tower_grads])\n```\n\n\\-  this works great and scales nicely, but (in my case) led to some weird convergence issues (eventually leading to `nan`s popping here and there), possibly because of some sort of race conditions (?), while applying gradients (`gd.apply_gradients()` is probably not very atomic). One possible workaround (solution 2) might be to use `tf.control_dependencies`:\n\n```\ndef rigid_op_sequence(op_lambdas):\n    ## equivalent to:\n    ## op = op1()\n    ## with.control_dependencies([op]):\n    ##    op = op2()\n    ##    with.control_dependencies([op]):\n    ##        op = op3()\n    ##        ...\n    ## return op\n\n    with ExitStack() as stack:\n        for op_func in op_lambdas:\n            op = op_func()\n            context = tf.control_dependencies([op])\n            stack.enter_context(context)\n    return op\n\ngrad_update_lambdas = [(lambda: gd.apply_gradients(grad)) for grad in tower_grads]\ntrain_op = rigid_op_sequence(grad_update_lambdas)\n```\n\nhowever this solution seems to somewhat significantly degenerate performance, because of that \"blocking\" (GPU load dropped from ~90% to ~60%).\n\nSeveral fundamental questions:\n1. What is officially recommended way of dealing with this \"multigpu IndexedSlices\" case? (could not find any; people keep asking)\n2. Does (solution 1) indeed might experience race condition or tensorflow handles it somehow and my convergences issues were not related to that? (and we all can just use solution 1)\n"}