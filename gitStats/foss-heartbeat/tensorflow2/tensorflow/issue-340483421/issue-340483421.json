{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20727", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20727/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20727/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20727/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20727", "id": 340483421, "node_id": "MDU6SXNzdWUzNDA0ODM0MjE=", "number": 20727, "title": "tf.contrib.seq2seq.AttentionWrapperState TypeError: __new__() missing 4 required positional arguments: 'time', 'alignments', 'alignment_history', and 'attention_state'", "user": {"login": "lvjiujin", "id": 17509092, "node_id": "MDQ6VXNlcjE3NTA5MDky", "avatar_url": "https://avatars3.githubusercontent.com/u/17509092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lvjiujin", "html_url": "https://github.com/lvjiujin", "followers_url": "https://api.github.com/users/lvjiujin/followers", "following_url": "https://api.github.com/users/lvjiujin/following{/other_user}", "gists_url": "https://api.github.com/users/lvjiujin/gists{/gist_id}", "starred_url": "https://api.github.com/users/lvjiujin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lvjiujin/subscriptions", "organizations_url": "https://api.github.com/users/lvjiujin/orgs", "repos_url": "https://api.github.com/users/lvjiujin/repos", "events_url": "https://api.github.com/users/lvjiujin/events{/privacy}", "received_events_url": "https://api.github.com/users/lvjiujin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-07-12T04:15:32Z", "updated_at": "2018-09-07T05:17:15Z", "closed_at": "2018-07-23T18:01:19Z", "author_association": "NONE", "body_html": "<p>in the tensorflow r1.8:</p>\n<pre><code> tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)\n</code></pre>\n<p>in the tensorflow r1.2</p>\n<pre><code> tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)\n</code></pre>\n<p>my code is r1.2, but now i want to run it in the r1.8.</p>\n<pre><code> initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n                                                             _zero_state_tensors(rnn_size, \n                                                                                 batch_size, \n                                                                                 tf.float32)) \n with tf.variable_scope(\"decode\"):\n        training_logits = training_decoding_layer(dec_embed_input, \n                                                  summary_length, \n                                                  dec_cell, \n                                                  initial_state,\n                                                  output_layer,\n                                                  vocab_size, \n                                                  max_summary_length)\n    with tf.variable_scope(\"decode\", reuse=True):\n        inference_logits = inference_decoding_layer(embeddings,  \n                                                    vocab_to_int['&lt;GO&gt;'], \n                                                    vocab_to_int['&lt;EOS&gt;'],\n                                                    dec_cell, \n                                                    initial_state, \n                                                    output_layer,\n                                                    max_summary_length,\n                                                    batch_size)\n</code></pre>\n<p>who can help me ?<br>\nbecause the number of the argument of the AttentionWrapperState in the r1.2 is the same to the r1.8 , why it occurs error in the r1.8 , but well in the r1.2?</p>", "body_text": "in the tensorflow r1.8:\n tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)\n\nin the tensorflow r1.2\n tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)\n\nmy code is r1.2, but now i want to run it in the r1.8.\n initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n                                                             _zero_state_tensors(rnn_size, \n                                                                                 batch_size, \n                                                                                 tf.float32)) \n with tf.variable_scope(\"decode\"):\n        training_logits = training_decoding_layer(dec_embed_input, \n                                                  summary_length, \n                                                  dec_cell, \n                                                  initial_state,\n                                                  output_layer,\n                                                  vocab_size, \n                                                  max_summary_length)\n    with tf.variable_scope(\"decode\", reuse=True):\n        inference_logits = inference_decoding_layer(embeddings,  \n                                                    vocab_to_int['<GO>'], \n                                                    vocab_to_int['<EOS>'],\n                                                    dec_cell, \n                                                    initial_state, \n                                                    output_layer,\n                                                    max_summary_length,\n                                                    batch_size)\n\nwho can help me ?\nbecause the number of the argument of the AttentionWrapperState in the r1.2 is the same to the r1.8 , why it occurs error in the r1.8 , but well in the r1.2?", "body": "in the tensorflow r1.8:\r\n```\r\n tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)\r\n```\r\nin the tensorflow r1.2 \r\n```\r\n tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)\r\n```\r\n\r\nmy code is r1.2, but now i want to run it in the r1.8.\r\n```\r\n initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\r\n                                                             _zero_state_tensors(rnn_size, \r\n                                                                                 batch_size, \r\n                                                                                 tf.float32)) \r\n with tf.variable_scope(\"decode\"):\r\n        training_logits = training_decoding_layer(dec_embed_input, \r\n                                                  summary_length, \r\n                                                  dec_cell, \r\n                                                  initial_state,\r\n                                                  output_layer,\r\n                                                  vocab_size, \r\n                                                  max_summary_length)\r\n    with tf.variable_scope(\"decode\", reuse=True):\r\n        inference_logits = inference_decoding_layer(embeddings,  \r\n                                                    vocab_to_int['<GO>'], \r\n                                                    vocab_to_int['<EOS>'],\r\n                                                    dec_cell, \r\n                                                    initial_state, \r\n                                                    output_layer,\r\n                                                    max_summary_length,\r\n                                                    batch_size)\r\n```\r\nwho can help me ?\r\nbecause the number of the argument of the AttentionWrapperState in the r1.2 is the same to the r1.8 , why it occurs error in the r1.8 , but well in the r1.2?\r\n"}