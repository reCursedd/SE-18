{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/256718844", "html_url": "https://github.com/tensorflow/tensorflow/issues/4807#issuecomment-256718844", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4807", "id": 256718844, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NjcxODg0NA==", "user": {"login": "mkolod", "id": 476135, "node_id": "MDQ6VXNlcjQ3NjEzNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/476135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mkolod", "html_url": "https://github.com/mkolod", "followers_url": "https://api.github.com/users/mkolod/followers", "following_url": "https://api.github.com/users/mkolod/following{/other_user}", "gists_url": "https://api.github.com/users/mkolod/gists{/gist_id}", "starred_url": "https://api.github.com/users/mkolod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mkolod/subscriptions", "organizations_url": "https://api.github.com/users/mkolod/orgs", "repos_url": "https://api.github.com/users/mkolod/repos", "events_url": "https://api.github.com/users/mkolod/events{/privacy}", "received_events_url": "https://api.github.com/users/mkolod/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-27T17:46:59Z", "updated_at": "2016-10-27T17:48:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=170179\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jhseu\">@jhseu</a> I'll try to look into making turbo optional in the Bazel build, right now I have it on for all cases. Meanwhile, I wanted to share some perf figures of how this would impact decoding. The base case here is Caffe and CNTK, which use OpenCV and get about 1k ImageNet images decoded per CPU core on a CPU the same as mine (6-core Core i7-5930K 3.50GHz). I used the standard ImageNet protobufs generated using the ImageNet preprocessing script from the Inception code base. The figures are as follows, on a per core basis:</p>\n<p>base case: 219 images/sec</p>\n<p>base + Fast IDCT (see issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"184050195\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5072\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/5072/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/5072\">#5072</a>): 233 im/sec</p>\n<p>base + Fast IDCT + libjpeg-turbo: 356 im/sec</p>\n<p>base + Fast IDCT + libjpeg-turbo + pre-resized images (to 256x256, can crop it down to 227x227 for AlexNet at runtime with random crops, etc.): 700 im/sec</p>\n<p>The Caffe/CNTK perf is at 1k im/sec, but that's also in benchmarks with pre-resized images (one shouldn't have to do anything else than cropping at runtime, definitely not padding, etc.). For lighter networks like AlexNet, the image decode is definitely a bottleneck. For heavier networks like Inception, it may not be an issue, but again, it could be an issue for more CPU-challenged platforms like ARM, even for inference.</p>\n<p><a href=\"https://github.com/mkolod/tf_perf_eval/blob/master/io/only_decode_benchmark.py\">Here</a> is the script I used for this benchmark. I'll post the Bazel integration with the libjpeg-turbo build shortly. For making turbo optional, I wonder if it shouldn't be a configure option, like the CUDA/CUDNN/HDFS support right now? If not, what's another option for conditional inclusion into the build?</p>", "body_text": "@jhseu I'll try to look into making turbo optional in the Bazel build, right now I have it on for all cases. Meanwhile, I wanted to share some perf figures of how this would impact decoding. The base case here is Caffe and CNTK, which use OpenCV and get about 1k ImageNet images decoded per CPU core on a CPU the same as mine (6-core Core i7-5930K 3.50GHz). I used the standard ImageNet protobufs generated using the ImageNet preprocessing script from the Inception code base. The figures are as follows, on a per core basis:\nbase case: 219 images/sec\nbase + Fast IDCT (see issue #5072): 233 im/sec\nbase + Fast IDCT + libjpeg-turbo: 356 im/sec\nbase + Fast IDCT + libjpeg-turbo + pre-resized images (to 256x256, can crop it down to 227x227 for AlexNet at runtime with random crops, etc.): 700 im/sec\nThe Caffe/CNTK perf is at 1k im/sec, but that's also in benchmarks with pre-resized images (one shouldn't have to do anything else than cropping at runtime, definitely not padding, etc.). For lighter networks like AlexNet, the image decode is definitely a bottleneck. For heavier networks like Inception, it may not be an issue, but again, it could be an issue for more CPU-challenged platforms like ARM, even for inference.\nHere is the script I used for this benchmark. I'll post the Bazel integration with the libjpeg-turbo build shortly. For making turbo optional, I wonder if it shouldn't be a configure option, like the CUDA/CUDNN/HDFS support right now? If not, what's another option for conditional inclusion into the build?", "body": "@jhseu I'll try to look into making turbo optional in the Bazel build, right now I have it on for all cases. Meanwhile, I wanted to share some perf figures of how this would impact decoding. The base case here is Caffe and CNTK, which use OpenCV and get about 1k ImageNet images decoded per CPU core on a CPU the same as mine (6-core Core i7-5930K 3.50GHz). I used the standard ImageNet protobufs generated using the ImageNet preprocessing script from the Inception code base. The figures are as follows, on a per core basis:\n\nbase case: 219 images/sec\n\nbase + Fast IDCT (see issue #5072): 233 im/sec\n\nbase + Fast IDCT + libjpeg-turbo: 356 im/sec\n\nbase + Fast IDCT + libjpeg-turbo + pre-resized images (to 256x256, can crop it down to 227x227 for AlexNet at runtime with random crops, etc.): 700 im/sec\n\nThe Caffe/CNTK perf is at 1k im/sec, but that's also in benchmarks with pre-resized images (one shouldn't have to do anything else than cropping at runtime, definitely not padding, etc.). For lighter networks like AlexNet, the image decode is definitely a bottleneck. For heavier networks like Inception, it may not be an issue, but again, it could be an issue for more CPU-challenged platforms like ARM, even for inference.\n\n[Here](https://github.com/mkolod/tf_perf_eval/blob/master/io/only_decode_benchmark.py) is the script I used for this benchmark. I'll post the Bazel integration with the libjpeg-turbo build shortly. For making turbo optional, I wonder if it shouldn't be a configure option, like the CUDA/CUDNN/HDFS support right now? If not, what's another option for conditional inclusion into the build?\n"}