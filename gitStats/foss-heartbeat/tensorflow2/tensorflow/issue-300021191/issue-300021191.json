{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17257", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17257/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17257/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17257/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17257", "id": 300021191, "node_id": "MDU6SXNzdWUzMDAwMjExOTE=", "number": 17257, "title": "Saver: Don't fail on restoring variables not present in a checkpoint.", "user": {"login": "sleighsoft", "id": 9438971, "node_id": "MDQ6VXNlcjk0Mzg5NzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/9438971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sleighsoft", "html_url": "https://github.com/sleighsoft", "followers_url": "https://api.github.com/users/sleighsoft/followers", "following_url": "https://api.github.com/users/sleighsoft/following{/other_user}", "gists_url": "https://api.github.com/users/sleighsoft/gists{/gist_id}", "starred_url": "https://api.github.com/users/sleighsoft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sleighsoft/subscriptions", "organizations_url": "https://api.github.com/users/sleighsoft/orgs", "repos_url": "https://api.github.com/users/sleighsoft/repos", "events_url": "https://api.github.com/users/sleighsoft/events{/privacy}", "received_events_url": "https://api.github.com/users/sleighsoft/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 30, "created_at": "2018-02-25T12:12:37Z", "updated_at": "2018-08-22T16:04:29Z", "closed_at": "2018-04-04T16:19:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is a feature request.</p>\n<p>Let's consider a scenario where one trains multiple models and uses them in combination (like it is done in GANs).</p>\n<p>To simplify the process of saving and restoring variables that are partly shared across the models (Pretraining Model, Training Model, Evaluation Model, Infer Model) one could instantiate the whole graph, containing all operations and variables, and save it.</p>\n<p>Then in order to do pre-training only the subset of graph elements that is required for pretraining is used.</p>\n<p>This results in the overhead of having to build the whole model (which might consist of multiple sub models) the first time the model is run even though only a smaller part (e.g. Pretraining Model) is required.</p>\n<p>Another issue arises when using different optimizers in different training stages (e.g. SGD first then Adam). As Adam creates additional variables Adam has to be instantiated during the first training stage so restoring from a checkpoint does not fail when restoring with Adam instead of SGD.</p>\n<p>This restriction of having to build everything despite parts not being required results in more complicated code. If it would be possible to silently fail when a variable is not found in a checkpoint, so it can be initialized with <code>tf.global_variables_initializer()</code> instead, would allow better structuring of code.</p>\n<p>I have looked through all current issues regarding this problem and I have found a couple that face a similar problem and where a <code>QuietlyFailRestoringSaver</code> could solve this problem:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"247946712\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12032\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/12032/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/12032\">#12032</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"294537159\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/16781\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/16781/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/16781\">#16781</a></p>\n<p>I might consider building this if there is enough support for it. I am open for feedback.</p>", "body_text": "This is a feature request.\nLet's consider a scenario where one trains multiple models and uses them in combination (like it is done in GANs).\nTo simplify the process of saving and restoring variables that are partly shared across the models (Pretraining Model, Training Model, Evaluation Model, Infer Model) one could instantiate the whole graph, containing all operations and variables, and save it.\nThen in order to do pre-training only the subset of graph elements that is required for pretraining is used.\nThis results in the overhead of having to build the whole model (which might consist of multiple sub models) the first time the model is run even though only a smaller part (e.g. Pretraining Model) is required.\nAnother issue arises when using different optimizers in different training stages (e.g. SGD first then Adam). As Adam creates additional variables Adam has to be instantiated during the first training stage so restoring from a checkpoint does not fail when restoring with Adam instead of SGD.\nThis restriction of having to build everything despite parts not being required results in more complicated code. If it would be possible to silently fail when a variable is not found in a checkpoint, so it can be initialized with tf.global_variables_initializer() instead, would allow better structuring of code.\nI have looked through all current issues regarding this problem and I have found a couple that face a similar problem and where a QuietlyFailRestoringSaver could solve this problem:\n#12032\n#16781\nI might consider building this if there is enough support for it. I am open for feedback.", "body": "This is a feature request.\r\n\r\nLet's consider a scenario where one trains multiple models and uses them in combination (like it is done in GANs).\r\n\r\nTo simplify the process of saving and restoring variables that are partly shared across the models (Pretraining Model, Training Model, Evaluation Model, Infer Model) one could instantiate the whole graph, containing all operations and variables, and save it.\r\n\r\nThen in order to do pre-training only the subset of graph elements that is required for pretraining is used.\r\n\r\nThis results in the overhead of having to build the whole model (which might consist of multiple sub models) the first time the model is run even though only a smaller part (e.g. Pretraining Model) is required.\r\n\r\nAnother issue arises when using different optimizers in different training stages (e.g. SGD first then Adam). As Adam creates additional variables Adam has to be instantiated during the first training stage so restoring from a checkpoint does not fail when restoring with Adam instead of SGD.\r\n\r\nThis restriction of having to build everything despite parts not being required results in more complicated code. If it would be possible to silently fail when a variable is not found in a checkpoint, so it can be initialized with `tf.global_variables_initializer()` instead, would allow better structuring of code.\r\n\r\nI have looked through all current issues regarding this problem and I have found a couple that face a similar problem and where a `QuietlyFailRestoringSaver` could solve this problem:\r\nhttps://github.com/tensorflow/tensorflow/issues/12032\r\nhttps://github.com/tensorflow/tensorflow/issues/16781\r\n\r\nI might consider building this if there is enough support for it. I am open for feedback."}