{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281543333", "html_url": "https://github.com/tensorflow/tensorflow/issues/7026#issuecomment-281543333", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7026", "id": 281543333, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTU0MzMzMw==", "user": {"login": "rizar", "id": 654434, "node_id": "MDQ6VXNlcjY1NDQzNA==", "avatar_url": "https://avatars0.githubusercontent.com/u/654434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rizar", "html_url": "https://github.com/rizar", "followers_url": "https://api.github.com/users/rizar/followers", "following_url": "https://api.github.com/users/rizar/following{/other_user}", "gists_url": "https://api.github.com/users/rizar/gists{/gist_id}", "starred_url": "https://api.github.com/users/rizar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rizar/subscriptions", "organizations_url": "https://api.github.com/users/rizar/orgs", "repos_url": "https://api.github.com/users/rizar/repos", "events_url": "https://api.github.com/users/rizar/events{/privacy}", "received_events_url": "https://api.github.com/users/rizar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-22T01:54:36Z", "updated_at": "2017-02-22T01:54:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a> , I can not confirm. I tried and without any changes to the code <code>scatter_nd</code> is definitely done on cpu.</p>\n<p>I also tried adding the following diff:</p>\n<div class=\"highlight highlight-source-c++\"><pre>--- a/tensorflow/core/kernels/scatter_nd_op.cc\n+++ b/tensorflow/core/kernels/scatter_nd_op.cc\n@@ -<span class=\"pl-c1\">374</span>,<span class=\"pl-c1\">8</span> +<span class=\"pl-c1\">374</span>,<span class=\"pl-c1\">12</span> @@ TF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_CPU);\n #<span class=\"pl-k\">define</span> <span class=\"pl-en\">REGISTER_SCATTER_ND_UPDATE_GPU</span>(<span class=\"pl-v\">type</span>) \\\n   <span class=\"pl-en\">REGISTER_SCATTER_ND_UPDATE</span>(type, GPU);\n \n+#define <span class=\"pl-en\">REGISTER_SCATTER_ND_GPU</span>(type) \\\n+  REGISTER_SCATTER_ND(type, GPU);\n+\n <span class=\"pl-en\">TF_CALL_GPU_NUMBER_TYPES_NO_HALF</span>(REGISTER_SCATTER_ND_ADD_SUB_GPU);\n <span class=\"pl-en\">TF_CALL_GPU_NUMBER_TYPES_NO_HALF</span>(REGISTER_SCATTER_ND_UPDATE_GPU);\n+TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_GPU);\n \n <span class=\"pl-c\"><span class=\"pl-c\">//</span> Forward declarations of the functor specializations for GPU.</span>\n <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">functor</span> {</pre></div>\n<p>As a result, <code>scatter_nd</code> is placed on the GPU but eventually my test code crushes. My test code is listed below:</p>\n<div class=\"highlight highlight-source-python\"><pre>config <span class=\"pl-k\">=</span> config_pb2.ConfigProto(<span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> graph, tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n\n    indices <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))\n\n    z <span class=\"pl-k\">=</span> tf.scatter_nd(indices, tf.ones(<span class=\"pl-c1\">2</span>), <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">100</span>))\n    s <span class=\"pl-k\">=</span> tf.reduce_sum(z)\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n        <span class=\"pl-c1\">print</span>(sess.run(s, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{indices: [(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>), (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)]}))</pre></div>", "body_text": "@drpngx , I can not confirm. I tried and without any changes to the code scatter_nd is definitely done on cpu.\nI also tried adding the following diff:\n--- a/tensorflow/core/kernels/scatter_nd_op.cc\n+++ b/tensorflow/core/kernels/scatter_nd_op.cc\n@@ -374,8 +374,12 @@ TF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_CPU);\n #define REGISTER_SCATTER_ND_UPDATE_GPU(type) \\\n   REGISTER_SCATTER_ND_UPDATE(type, GPU);\n \n+#define REGISTER_SCATTER_ND_GPU(type) \\\n+  REGISTER_SCATTER_ND(type, GPU);\n+\n TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_ADD_SUB_GPU);\n TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_UPDATE_GPU);\n+TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_GPU);\n \n // Forward declarations of the functor specializations for GPU.\n namespace functor {\nAs a result, scatter_nd is placed on the GPU but eventually my test code crushes. My test code is listed below:\nconfig = config_pb2.ConfigProto(log_device_placement=True)\n\nwith tf.Graph().as_default() as graph, tf.Session(config=config) as sess:\n\n    indices = tf.placeholder(tf.int32, shape=(2, 2))\n\n    z = tf.scatter_nd(indices, tf.ones(2), shape=(10, 100))\n    s = tf.reduce_sum(z)\n\n    for i in range(10):\n        print(sess.run(s, feed_dict={indices: [(0, 0), (1, 1)]}))", "body": "@drpngx , I can not confirm. I tried and without any changes to the code `scatter_nd` is definitely done on cpu.\r\n\r\nI also tried adding the following diff:\r\n\r\n```cpp\r\n--- a/tensorflow/core/kernels/scatter_nd_op.cc\r\n+++ b/tensorflow/core/kernels/scatter_nd_op.cc\r\n@@ -374,8 +374,12 @@ TF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_CPU);\r\n #define REGISTER_SCATTER_ND_UPDATE_GPU(type) \\\r\n   REGISTER_SCATTER_ND_UPDATE(type, GPU);\r\n \r\n+#define REGISTER_SCATTER_ND_GPU(type) \\\r\n+  REGISTER_SCATTER_ND(type, GPU);\r\n+\r\n TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_ADD_SUB_GPU);\r\n TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_UPDATE_GPU);\r\n+TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_GPU);\r\n \r\n // Forward declarations of the functor specializations for GPU.\r\n namespace functor {\r\n```\r\n\r\nAs a result, `scatter_nd` is placed on the GPU but eventually my test code crushes. My test code is listed below:\r\n\r\n```python\r\nconfig = config_pb2.ConfigProto(log_device_placement=True)\r\n\r\nwith tf.Graph().as_default() as graph, tf.Session(config=config) as sess:\r\n\r\n    indices = tf.placeholder(tf.int32, shape=(2, 2))\r\n\r\n    z = tf.scatter_nd(indices, tf.ones(2), shape=(10, 100))\r\n    s = tf.reduce_sum(z)\r\n\r\n    for i in range(10):\r\n        print(sess.run(s, feed_dict={indices: [(0, 0), (1, 1)]}))\r\n```"}