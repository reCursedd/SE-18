{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436703156", "html_url": "https://github.com/tensorflow/tensorflow/issues/23312#issuecomment-436703156", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23312", "id": 436703156, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjcwMzE1Ng==", "user": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T17:15:28Z", "updated_at": "2018-11-07T17:15:28Z", "author_association": "MEMBER", "body_html": "<p>A simple cumsum of numbers [0 .. 5793] already starts having a significant error due to precision.</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>numpy.float32(16776528 + 5793)<br>\n16782320.0<br>\nnumpy.float64(16776528 + 5793)<br>\n16782321.0</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>It might be that other implementation (non Eigen) are smarter and do accumulation in 64 bit, so the error does not accumulate. Note you can use K.set_floatx('float64') to get the necessary precision</p>", "body_text": "A simple cumsum of numbers [0 .. 5793] already starts having a significant error due to precision.\n\n\n\nnumpy.float32(16776528 + 5793)\n16782320.0\nnumpy.float64(16776528 + 5793)\n16782321.0\n\n\n\nIt might be that other implementation (non Eigen) are smarter and do accumulation in 64 bit, so the error does not accumulate. Note you can use K.set_floatx('float64') to get the necessary precision", "body": "A simple cumsum of numbers [0 .. 5793] already starts having a significant error due to precision.\r\n>>> numpy.float32(16776528 + 5793)\r\n16782320.0\r\n>>> numpy.float64(16776528 + 5793)\r\n16782321.0\r\n\r\nIt might be that other implementation (non Eigen) are smarter and do accumulation in 64 bit, so the error does not accumulate. Note you can use K.set_floatx('float64') to get the necessary precision\r\n"}