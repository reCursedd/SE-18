{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266083540", "html_url": "https://github.com/tensorflow/tensorflow/issues/6072#issuecomment-266083540", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6072", "id": 266083540, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjA4MzU0MA==", "user": {"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-09T18:18:47Z", "updated_at": "2016-12-09T18:18:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=18507467\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/robotnc\">@robotnc</a>, as you see, the return values of params_to_canonical are weights and biases; they are stored in a layer-by-layer order.</p>\n<p>For example, for LSTM, there are 8 matrices in each layer and the first 8 weight matrices stored in weights belong to the first layer; out of these 8 matrices, the first 4 matrices are applied to the input from the previous layer, and the second 4 matrices are applied to the recurrent input. Biases are stored in the same order.</p>\n<p>You can write your canonical_to_params to take in weights and biases and output whatever data format you want for your model.</p>\n<p>Please let me if anything is not clear.</p>", "body_text": "@robotnc, as you see, the return values of params_to_canonical are weights and biases; they are stored in a layer-by-layer order.\nFor example, for LSTM, there are 8 matrices in each layer and the first 8 weight matrices stored in weights belong to the first layer; out of these 8 matrices, the first 4 matrices are applied to the input from the previous layer, and the second 4 matrices are applied to the recurrent input. Biases are stored in the same order.\nYou can write your canonical_to_params to take in weights and biases and output whatever data format you want for your model.\nPlease let me if anything is not clear.", "body": "@robotnc, as you see, the return values of params_to_canonical are weights and biases; they are stored in a layer-by-layer order.\r\n\r\nFor example, for LSTM, there are 8 matrices in each layer and the first 8 weight matrices stored in weights belong to the first layer; out of these 8 matrices, the first 4 matrices are applied to the input from the previous layer, and the second 4 matrices are applied to the recurrent input. Biases are stored in the same order.\r\n\r\nYou can write your canonical_to_params to take in weights and biases and output whatever data format you want for your model.\r\n\r\nPlease let me if anything is not clear."}