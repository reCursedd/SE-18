{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/274590896", "html_url": "https://github.com/tensorflow/tensorflow/issues/2358#issuecomment-274590896", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2358", "id": 274590896, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDU5MDg5Ng==", "user": {"login": "AdityaGudimella", "id": 5364789, "node_id": "MDQ6VXNlcjUzNjQ3ODk=", "avatar_url": "https://avatars0.githubusercontent.com/u/5364789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AdityaGudimella", "html_url": "https://github.com/AdityaGudimella", "followers_url": "https://api.github.com/users/AdityaGudimella/followers", "following_url": "https://api.github.com/users/AdityaGudimella/following{/other_user}", "gists_url": "https://api.github.com/users/AdityaGudimella/gists{/gist_id}", "starred_url": "https://api.github.com/users/AdityaGudimella/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AdityaGudimella/subscriptions", "organizations_url": "https://api.github.com/users/AdityaGudimella/orgs", "repos_url": "https://api.github.com/users/AdityaGudimella/repos", "events_url": "https://api.github.com/users/AdityaGudimella/events{/privacy}", "received_events_url": "https://api.github.com/users/AdityaGudimella/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-23T19:26:57Z", "updated_at": "2017-01-23T19:30:27Z", "author_association": "NONE", "body_html": "<p>Would this work? Since this is just using TensorFlow ops under the hood, it propogates gradients too.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">scatter_add_tensor</span>(<span class=\"pl-smi\">ref</span>, <span class=\"pl-smi\">indices</span>, <span class=\"pl-smi\">updates</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Adds sparse updates to a variable reference.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    This operation outputs ref after the update is done. This makes it easier to chain operations that need to use the</span>\n<span class=\"pl-s\">    reset value.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Duplicate indices are handled correctly: if multiple indices reference the same location, their contributions add.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Requires updates.shape = indices.shape + ref.shape[1:].</span>\n<span class=\"pl-s\">    :param ref: A Tensor. Must be one of the following types: float32, float64, int64, int32, uint8, uint16,</span>\n<span class=\"pl-s\">        int16, int8, complex64, complex128, qint8, quint8, qint32, half.</span>\n<span class=\"pl-s\">    :param indices: A Tensor. Must be one of the following types: int32, int64. A tensor of indices into the first</span>\n<span class=\"pl-s\">        dimension of ref.</span>\n<span class=\"pl-s\">    :param updates: A Tensor. Must have the same dtype as ref. A tensor of updated values to add to ref</span>\n<span class=\"pl-s\">    :param name: A name for the operation (optional).</span>\n<span class=\"pl-s\">    :return: Same as ref. Returned as a convenience for operations that want to use the updated values after the update</span>\n<span class=\"pl-s\">        is done.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.name_scope(name, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>scatter_add_tensor<span class=\"pl-pds\">'</span></span>, [ref, indices, updates]) <span class=\"pl-k\">as</span> scope:\n        ref <span class=\"pl-k\">=</span> tf.convert_to_tensor(ref, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>ref<span class=\"pl-pds\">'</span></span>)\n        indices <span class=\"pl-k\">=</span> tf.convert_to_tensor(indices, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>indices<span class=\"pl-pds\">'</span></span>)\n        updates <span class=\"pl-k\">=</span> tf.convert_to_tensor(updates, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>updates<span class=\"pl-pds\">'</span></span>)\n        ref_shape <span class=\"pl-k\">=</span> tf.shape(ref, <span class=\"pl-v\">out_type</span><span class=\"pl-k\">=</span>indices.dtype, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>ref_shape<span class=\"pl-pds\">'</span></span>)\n        scattered_updates <span class=\"pl-k\">=</span> tf.scatter_nd(indices, updates, ref_shape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>scattered_updates<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">with</span> tf.control_dependencies([tf.assert_equal(ref_shape, tf.shape(scattered_updates, <span class=\"pl-v\">out_type</span><span class=\"pl-k\">=</span>indices.dtype))]):\n            output <span class=\"pl-k\">=</span> tf.add(ref, scattered_updates, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope)\n        <span class=\"pl-k\">return</span> output</pre></div>", "body_text": "Would this work? Since this is just using TensorFlow ops under the hood, it propogates gradients too.\ndef scatter_add_tensor(ref, indices, updates, name=None):\n    \"\"\"\n    Adds sparse updates to a variable reference.\n\n    This operation outputs ref after the update is done. This makes it easier to chain operations that need to use the\n    reset value.\n\n    Duplicate indices are handled correctly: if multiple indices reference the same location, their contributions add.\n\n    Requires updates.shape = indices.shape + ref.shape[1:].\n    :param ref: A Tensor. Must be one of the following types: float32, float64, int64, int32, uint8, uint16,\n        int16, int8, complex64, complex128, qint8, quint8, qint32, half.\n    :param indices: A Tensor. Must be one of the following types: int32, int64. A tensor of indices into the first\n        dimension of ref.\n    :param updates: A Tensor. Must have the same dtype as ref. A tensor of updated values to add to ref\n    :param name: A name for the operation (optional).\n    :return: Same as ref. Returned as a convenience for operations that want to use the updated values after the update\n        is done.\n    \"\"\"\n    with tf.name_scope(name, 'scatter_add_tensor', [ref, indices, updates]) as scope:\n        ref = tf.convert_to_tensor(ref, name='ref')\n        indices = tf.convert_to_tensor(indices, name='indices')\n        updates = tf.convert_to_tensor(updates, name='updates')\n        ref_shape = tf.shape(ref, out_type=indices.dtype, name='ref_shape')\n        scattered_updates = tf.scatter_nd(indices, updates, ref_shape, name='scattered_updates')\n        with tf.control_dependencies([tf.assert_equal(ref_shape, tf.shape(scattered_updates, out_type=indices.dtype))]):\n            output = tf.add(ref, scattered_updates, name=scope)\n        return output", "body": "Would this work? Since this is just using TensorFlow ops under the hood, it propogates gradients too.\r\n\r\n```python\r\ndef scatter_add_tensor(ref, indices, updates, name=None):\r\n    \"\"\"\r\n    Adds sparse updates to a variable reference.\r\n\r\n    This operation outputs ref after the update is done. This makes it easier to chain operations that need to use the\r\n    reset value.\r\n\r\n    Duplicate indices are handled correctly: if multiple indices reference the same location, their contributions add.\r\n\r\n    Requires updates.shape = indices.shape + ref.shape[1:].\r\n    :param ref: A Tensor. Must be one of the following types: float32, float64, int64, int32, uint8, uint16,\r\n        int16, int8, complex64, complex128, qint8, quint8, qint32, half.\r\n    :param indices: A Tensor. Must be one of the following types: int32, int64. A tensor of indices into the first\r\n        dimension of ref.\r\n    :param updates: A Tensor. Must have the same dtype as ref. A tensor of updated values to add to ref\r\n    :param name: A name for the operation (optional).\r\n    :return: Same as ref. Returned as a convenience for operations that want to use the updated values after the update\r\n        is done.\r\n    \"\"\"\r\n    with tf.name_scope(name, 'scatter_add_tensor', [ref, indices, updates]) as scope:\r\n        ref = tf.convert_to_tensor(ref, name='ref')\r\n        indices = tf.convert_to_tensor(indices, name='indices')\r\n        updates = tf.convert_to_tensor(updates, name='updates')\r\n        ref_shape = tf.shape(ref, out_type=indices.dtype, name='ref_shape')\r\n        scattered_updates = tf.scatter_nd(indices, updates, ref_shape, name='scattered_updates')\r\n        with tf.control_dependencies([tf.assert_equal(ref_shape, tf.shape(scattered_updates, out_type=indices.dtype))]):\r\n            output = tf.add(ref, scattered_updates, name=scope)\r\n        return output\r\n```"}