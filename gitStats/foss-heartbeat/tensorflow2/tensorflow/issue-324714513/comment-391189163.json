{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/391189163", "html_url": "https://github.com/tensorflow/tensorflow/issues/19420#issuecomment-391189163", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19420", "id": 391189163, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTE4OTE2Mw==", "user": {"login": "hmorimitsu", "id": 24420973, "node_id": "MDQ6VXNlcjI0NDIwOTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/24420973?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hmorimitsu", "html_url": "https://github.com/hmorimitsu", "followers_url": "https://api.github.com/users/hmorimitsu/followers", "following_url": "https://api.github.com/users/hmorimitsu/following{/other_user}", "gists_url": "https://api.github.com/users/hmorimitsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/hmorimitsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hmorimitsu/subscriptions", "organizations_url": "https://api.github.com/users/hmorimitsu/orgs", "repos_url": "https://api.github.com/users/hmorimitsu/repos", "events_url": "https://api.github.com/users/hmorimitsu/events{/privacy}", "received_events_url": "https://api.github.com/users/hmorimitsu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T01:13:41Z", "updated_at": "2018-05-23T01:13:41Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710264\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jcRisch\">@jcRisch</a>,</p>\n<p>I also struggled with this a bit before and I found out that tf.keras.layers.Dropout supports a training argument in its call function. Therefore, you could write:</p>\n<div class=\"highlight highlight-source-python\"><pre>dropout <span class=\"pl-k\">=</span> tf.keras.layers.Dropout(<span class=\"pl-c1\">0.2</span>, <span class=\"pl-v\">noise_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)(dense, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)</pre></div>\n<p>That also applies to tf.keras.layers.BatchNormalization (and possibly other layers).</p>\n<p>I hope that helps.</p>", "body_text": "Hi @jcRisch,\nI also struggled with this a bit before and I found out that tf.keras.layers.Dropout supports a training argument in its call function. Therefore, you could write:\ndropout = tf.keras.layers.Dropout(0.2, noise_shape=None, seed=None)(dense, training=is_training)\nThat also applies to tf.keras.layers.BatchNormalization (and possibly other layers).\nI hope that helps.", "body": "Hi @jcRisch,\r\n\r\nI also struggled with this a bit before and I found out that tf.keras.layers.Dropout supports a training argument in its call function. Therefore, you could write:\r\n\r\n```python\r\ndropout = tf.keras.layers.Dropout(0.2, noise_shape=None, seed=None)(dense, training=is_training)\r\n```\r\n\r\nThat also applies to tf.keras.layers.BatchNormalization (and possibly other layers).\r\n\r\nI hope that helps."}