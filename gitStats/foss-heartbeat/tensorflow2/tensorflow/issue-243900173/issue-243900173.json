{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11597", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11597/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11597/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11597/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11597", "id": 243900173, "node_id": "MDU6SXNzdWUyNDM5MDAxNzM=", "number": 11597, "title": " Not found: Key <variable_name> not found in checkpoint even though it exists in meta graph", "user": {"login": "kevinlu1211", "id": 12663823, "node_id": "MDQ6VXNlcjEyNjYzODIz", "avatar_url": "https://avatars0.githubusercontent.com/u/12663823?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kevinlu1211", "html_url": "https://github.com/kevinlu1211", "followers_url": "https://api.github.com/users/kevinlu1211/followers", "following_url": "https://api.github.com/users/kevinlu1211/following{/other_user}", "gists_url": "https://api.github.com/users/kevinlu1211/gists{/gist_id}", "starred_url": "https://api.github.com/users/kevinlu1211/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kevinlu1211/subscriptions", "organizations_url": "https://api.github.com/users/kevinlu1211/orgs", "repos_url": "https://api.github.com/users/kevinlu1211/repos", "events_url": "https://api.github.com/users/kevinlu1211/events{/privacy}", "received_events_url": "https://api.github.com/users/kevinlu1211/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-07-19T01:53:00Z", "updated_at": "2017-07-25T01:29:18Z", "closed_at": "2017-07-25T01:29:18Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>Python version: 3.6<br>\nTensorflow version: 1.1</p>\n<p>Here are the commands needed to reproduce the error:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n    \ntf.reset_default_graph()\n\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\nx_data = np.random.rand(100).astype(np.float32)\ny_data = x_data * 0.1 + 0.3\n\n# Try to find values for W and b that compute y_data = W * x_data + b\n# (We know that W should be 0.1 and b 0.3, but TensorFlow will\n# figure that out for us.)\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='W')\nb = tf.Variable(tf.zeros([1]), name='b')\ny = W * x_data + b\n\n# Minimize the mean squared errors.\nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\nopt_op = optimizer.minimize(loss)\n\n# Track the moving averages of all trainable variables.\nema = tf.train.ExponentialMovingAverage(decay=0.9999)\nvariables = tf.trainable_variables()\nprint(variables)\naverages_op = ema.apply(tf.trainable_variables())\nwith tf.control_dependencies([opt_op]):\n    train_op = tf.group(averages_op)\n\n# Before starting, initialize the variables.  We will 'run' this first.\ninit = tf.global_variables_initializer()\n\nsaver = tf.train.Saver(tf.trainable_variables())\n\n# Launch the graph.\nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.\nfor _ in range(201):\n    sess.run(train_op)\n\nw_reference = sess.run('W/ExponentialMovingAverage:0')\nb_reference = sess.run('b/ExponentialMovingAverage:0')\n\nsaver.save(sess, os.path.join(\"model_ex1\"))\n\ntf.reset_default_graph()\n\ntf.train.import_meta_graph(\"model_ex1.meta\")\nsess = tf.Session()\n\nprint('------------------------------------------------------')\nfor var in tf.global_variables():\n    print('all variables: ' + var.op.name)\nfor var in tf.trainable_variables():\n    print('normal variable: ' + var.op.name)\nfor var in tf.moving_average_variables():\n    print('ema variable: ' + var.op.name)\nprint('------------------------------------------------------')\n\n\nrestore_vars = {}\n\nema = tf.train.ExponentialMovingAverage(1.0)\nfor var in tf.trainable_variables():\n    print('%s: %s' % (ema.average_name(var), var.op.name))\n    restore_vars[ema.average_name(var)] = var\n\nsaver = tf.train.Saver(restore_vars, name='ema_restore')\n\nsaver.restore(sess, os.path.join(\"model_ex1\"))\n\nw_restored = sess.run('W:0')\nb_restored = sess.run('b:0')\n\nprint(w_reference)\nprint(w_restored)\nprint(b_reference)\nprint(b_restored)\n\n</code></pre>", "body_text": "System information\nPython version: 3.6\nTensorflow version: 1.1\nHere are the commands needed to reproduce the error:\nimport tensorflow as tf\nimport numpy as np\n\n    \ntf.reset_default_graph()\n\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\nx_data = np.random.rand(100).astype(np.float32)\ny_data = x_data * 0.1 + 0.3\n\n# Try to find values for W and b that compute y_data = W * x_data + b\n# (We know that W should be 0.1 and b 0.3, but TensorFlow will\n# figure that out for us.)\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='W')\nb = tf.Variable(tf.zeros([1]), name='b')\ny = W * x_data + b\n\n# Minimize the mean squared errors.\nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\nopt_op = optimizer.minimize(loss)\n\n# Track the moving averages of all trainable variables.\nema = tf.train.ExponentialMovingAverage(decay=0.9999)\nvariables = tf.trainable_variables()\nprint(variables)\naverages_op = ema.apply(tf.trainable_variables())\nwith tf.control_dependencies([opt_op]):\n    train_op = tf.group(averages_op)\n\n# Before starting, initialize the variables.  We will 'run' this first.\ninit = tf.global_variables_initializer()\n\nsaver = tf.train.Saver(tf.trainable_variables())\n\n# Launch the graph.\nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.\nfor _ in range(201):\n    sess.run(train_op)\n\nw_reference = sess.run('W/ExponentialMovingAverage:0')\nb_reference = sess.run('b/ExponentialMovingAverage:0')\n\nsaver.save(sess, os.path.join(\"model_ex1\"))\n\ntf.reset_default_graph()\n\ntf.train.import_meta_graph(\"model_ex1.meta\")\nsess = tf.Session()\n\nprint('------------------------------------------------------')\nfor var in tf.global_variables():\n    print('all variables: ' + var.op.name)\nfor var in tf.trainable_variables():\n    print('normal variable: ' + var.op.name)\nfor var in tf.moving_average_variables():\n    print('ema variable: ' + var.op.name)\nprint('------------------------------------------------------')\n\n\nrestore_vars = {}\n\nema = tf.train.ExponentialMovingAverage(1.0)\nfor var in tf.trainable_variables():\n    print('%s: %s' % (ema.average_name(var), var.op.name))\n    restore_vars[ema.average_name(var)] = var\n\nsaver = tf.train.Saver(restore_vars, name='ema_restore')\n\nsaver.restore(sess, os.path.join(\"model_ex1\"))\n\nw_restored = sess.run('W:0')\nb_restored = sess.run('b:0')\n\nprint(w_reference)\nprint(w_restored)\nprint(b_reference)\nprint(b_restored)", "body": "### System information\r\nPython version: 3.6\r\nTensorflow version: 1.1\r\n\r\nHere are the commands needed to reproduce the error:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n    \r\ntf.reset_default_graph()\r\n\r\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\r\nx_data = np.random.rand(100).astype(np.float32)\r\ny_data = x_data * 0.1 + 0.3\r\n\r\n# Try to find values for W and b that compute y_data = W * x_data + b\r\n# (We know that W should be 0.1 and b 0.3, but TensorFlow will\r\n# figure that out for us.)\r\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='W')\r\nb = tf.Variable(tf.zeros([1]), name='b')\r\ny = W * x_data + b\r\n\r\n# Minimize the mean squared errors.\r\nloss = tf.reduce_mean(tf.square(y - y_data))\r\noptimizer = tf.train.GradientDescentOptimizer(0.5)\r\nopt_op = optimizer.minimize(loss)\r\n\r\n# Track the moving averages of all trainable variables.\r\nema = tf.train.ExponentialMovingAverage(decay=0.9999)\r\nvariables = tf.trainable_variables()\r\nprint(variables)\r\naverages_op = ema.apply(tf.trainable_variables())\r\nwith tf.control_dependencies([opt_op]):\r\n    train_op = tf.group(averages_op)\r\n\r\n# Before starting, initialize the variables.  We will 'run' this first.\r\ninit = tf.global_variables_initializer()\r\n\r\nsaver = tf.train.Saver(tf.trainable_variables())\r\n\r\n# Launch the graph.\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# Fit the line.\r\nfor _ in range(201):\r\n    sess.run(train_op)\r\n\r\nw_reference = sess.run('W/ExponentialMovingAverage:0')\r\nb_reference = sess.run('b/ExponentialMovingAverage:0')\r\n\r\nsaver.save(sess, os.path.join(\"model_ex1\"))\r\n\r\ntf.reset_default_graph()\r\n\r\ntf.train.import_meta_graph(\"model_ex1.meta\")\r\nsess = tf.Session()\r\n\r\nprint('------------------------------------------------------')\r\nfor var in tf.global_variables():\r\n    print('all variables: ' + var.op.name)\r\nfor var in tf.trainable_variables():\r\n    print('normal variable: ' + var.op.name)\r\nfor var in tf.moving_average_variables():\r\n    print('ema variable: ' + var.op.name)\r\nprint('------------------------------------------------------')\r\n\r\n\r\nrestore_vars = {}\r\n\r\nema = tf.train.ExponentialMovingAverage(1.0)\r\nfor var in tf.trainable_variables():\r\n    print('%s: %s' % (ema.average_name(var), var.op.name))\r\n    restore_vars[ema.average_name(var)] = var\r\n\r\nsaver = tf.train.Saver(restore_vars, name='ema_restore')\r\n\r\nsaver.restore(sess, os.path.join(\"model_ex1\"))\r\n\r\nw_restored = sess.run('W:0')\r\nb_restored = sess.run('b:0')\r\n\r\nprint(w_reference)\r\nprint(w_restored)\r\nprint(b_reference)\r\nprint(b_restored)\r\n\r\n```"}