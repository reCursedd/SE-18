{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133249873", "pull_request_review_id": 56405047, "id": 133249873, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMzI0OTg3Mw==", "diff_hunk": "@@ -0,0 +1,693 @@\n+# 2017 Contrib.\n+# ==============================================================================\n+\n+\"\"\"Synchronize replicas for model average training.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import time\n+import six\n+\n+from tensorflow.core.framework import node_def_pb2\n+from tensorflow.python.framework import device as pydev\n+from tensorflow.python.training import server_lib\n+from tensorflow.python.training import device_setter\n+from tensorflow.core.framework import types_pb2\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import data_flow_ops\n+from tensorflow.python.ops import init_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.ops import variables\n+from tensorflow.python.ops import variable_scope\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.training import queue_runner\n+from tensorflow.python.training import session_manager\n+from tensorflow.python.training import session_run_hook\n+\n+class _ModelAverageDeviceChooser(object):\n+  \"\"\"Class to choose devices for Ops in a model average training setup.\n+\n+  This class is not to be used directly by users.  See instead\n+  `model_average_device_setter()` below.\n+  \"\"\"\n+\n+  def __init__(self, ps_tasks, ps_device, worker_device, ps_ops,\n+               ps_strategy):\n+    \"\"\"Create a new `_ReplicaDeviceChooser`.\n+\n+    Args:\n+      ps_tasks: Number of tasks in the `ps` job.\n+      ps_device: String.  Name of the `ps` job.\n+      worker_device: String.  Name of the `worker` job.\n+      ps_ops: List of strings representing `Operation` types that need to be\n+        placed on `ps` devices.\n+      ps_strategy: A callable invoked for every ps `Operation` (i.e. matched by\n+        `ps_ops`), that takes the `Operation` and returns the ps task index to\n+        use.\n+    \"\"\"\n+    self._ps_tasks = ps_tasks\n+    self._ps_device = ps_device\n+    self._worker_device = worker_device\n+    self._ps_ops = ps_ops\n+    self._ps_strategy = ps_strategy\n+\n+  def device_function(self, op):\n+    \"\"\"Choose a device for `op`.\n+\n+    Args:\n+      op: an `Operation`.\n+\n+    Returns:\n+      The device to use for the `Operation`.\n+    \"\"\"\n+\n+    current_device = pydev.DeviceSpec.from_string(op.device or \"\")\n+\n+    node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n+    # TODO(chen meng): only global variables should be placed on ps. Now we just\n+    # do this according to substring in var name.\n+    if self._ps_tasks and self._ps_device and node_def.op in self._ps_ops \\\n+       and node_def.name.find('modelAverage') != -1:\n+      ps_device = pydev.DeviceSpec.from_string(self._ps_device)\n+\n+      current_job, ps_job = current_device.job, ps_device.job\n+      if ps_job and (not current_job or current_job == ps_job):\n+        ps_device.task = self._ps_strategy(op)\n+\n+      ps_device.merge_from(current_device)\n+      return ps_device.to_string()\n+\n+    worker_device = pydev.DeviceSpec.from_string(self._worker_device or \"\")\n+    worker_device.merge_from(current_device)\n+    return worker_device.to_string()\n+\n+def model_average_device_setter(ps_tasks=0, ps_device=\"/job:ps\",\n+                          worker_device=\"/job:worker\",\n+                          cluster=None, ps_ops=None, ps_strategy=None):\n+  \"\"\"Return a `device function` to use when building a Graph for model average.\n+\n+  There is only one difference between model_average_device_setter and\n+  replica_device_setter : replica_device_setter placed all variables\n+  (including global/local variables) on ps, while in model average, each worker\n+  own its local variables (local model parameters), these local variables\n+  should be placed in each worker.\n+  Args:\n+    ps_tasks: Number of tasks in the `ps` job.  Ignored if `cluster` is\n+      provided.\n+    ps_device: String.  Device of the `ps` job.  If empty no `ps` job is used.\n+      Defaults to `ps`.\n+    worker_device: String.  Device of the `worker` job.  If empty no `worker`\n+      job is used.\n+    cluster: `ClusterDef` proto or `ClusterSpec`.\n+    ps_ops: List of strings representing `Operation` types that need to be\n+      placed on `ps` devices.  If `None`, defaults to `[\"Variable\"]`.\n+    ps_strategy: A callable invoked for every ps `Operation` (i.e. matched by\n+      `ps_ops`), that takes the `Operation` and returns the ps task index to\n+      use.  If `None`, defaults to a round-robin strategy across all `ps`\n+      devices.\n+\n+  Returns:\n+    A function to pass to `tf.device()`.\n+\n+  Raises:\n+    TypeError if `cluster` is not a dictionary or `ClusterDef` protocol buffer,\n+    or if `ps_strategy` is provided but not a callable.\n+  \"\"\"\n+  if cluster is not None:\n+    if isinstance(cluster, server_lib.ClusterSpec):\n+      cluster_spec = cluster.as_dict()\n+    else:\n+      cluster_spec = server_lib.ClusterSpec(cluster).as_dict()\n+    # Get ps_job_name from ps_device by striping \"/job:\".\n+    ps_job_name = pydev.DeviceSpec.from_string(ps_device).job\n+    if ps_job_name not in cluster_spec or cluster_spec[ps_job_name] is None:\n+      return None\n+    ps_tasks = len(cluster_spec[ps_job_name])\n+\n+  if ps_tasks == 0:\n+    return None\n+\n+  if ps_ops is None:\n+    ps_ops = [\"Variable\", \"VariableV2\", \"VarHandleOp\"]\n+\n+  if ps_strategy is None:\n+    ps_strategy = device_setter._RoundRobinStrategy(ps_tasks)\n+  if not six.callable(ps_strategy):\n+    raise TypeError(\"ps_strategy must be callable\")\n+  chooser = _ModelAverageDeviceChooser(\n+      ps_tasks, ps_device, worker_device, ps_ops, ps_strategy)\n+  return chooser.device_function\n+\n+class ModelAverageOptimizer(object):\n+  \"\"\"Class to synchronize, aggregate model params.\n+\n+  In a typical synchronous training environment (N-replica synchronous training)\n+  , gradients will be averaged each step, and then applying them to the\n+  variables in one shot, after which replicas can fetch the new variables and\n+  continue. In a model average training environment, model variables will be\n+  averaged (or with momentum) every 'interval_steps' steps, and then fetch the\n+  new variables and continue training in local worker. In the interval between\n+  two \"average operation\", there are no data transfer at all, which can\n+  accerlate training.\n+\n+  The following accumulators/queue are created:\n+  <empty line>\n+  * N `model-variable accumulators`, one per variable for train model. local\n+  variables are pushed to them and the chief worker will wait until enough\n+  variables are collected and then average them. The accumulator will drop all\n+  stale variables (more details in the accumulator op).\n+  * 1 `token` queue where the optimizer pushes the new global_step value after\n+    all variables are updated.\n+\n+  The following local variable is created:\n+  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n+    each accumulator to check for staleness of the variables.\n+\n+  The optimizer adds nodes to the graph to collect local variables and pause\n+  the trainers until variables are updated.\n+  For the Parameter Server job:\n+  <empty line>\n+  1. An accumulator is created for each variable, and each replica pushes the\n+     local variables into the accumulators.\n+  2. Each accumulator averages once enough variables (replicas_to_aggregate)\n+     have been accumulated.\n+  3. apply the averaged variables to global variables.\n+  4. Only after all variables have been updated, increment the global step.\n+  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n+     each worker replica. The workers can now fetch the global step, use it to\n+     update its local_step variable and start the next batch.\n+\n+  For the replicas:\n+  <empty line>\n+  1. Start a training block: fetch variables, finish \"interval_steps\" steps\n+     training.\n+  2. Once current training block has been finished, push local variables into\n+     accumulators. Each accumulator will check the staleness and drop the\n+     stale ones.\n+  3. After pushing all the variables, dequeue an updated value of global_step\n+     from the token queue and record that step to its local_step variable. Note\n+     that this is effectively a barrier.\n+  4. fetch new variables, Start the next block.\n+\n+  ### Usage\n+\n+  ```python\n+  # Create any optimizer to update the variables, say a simple SGD:\n+  opt = GradientDescentOptimizer(learning_rate=0.1)\n+\n+  # Create a ModelAverageOptimizer to update the global variables:\n+  # Note that if you want to have 2 backup replicas, you can change\n+  # total_num_replicas=52 and make sure this number matches how many physical\n+  # replicas you started in your job.\n+  ma = tf.contrib.model_average.ModelAverageOptimizer(replicas_to_aggregate=50,\n+                                                      interval_steps=100)\n+\n+  # You can create the hook which handles model average operations.\n+  ma_hook = ma.make_ma_run_hook()\n+  # And also, create the hook which handles initialization and queues.\n+  ma_replicas_hook = ma.make_session_run_hook(is_chief)\n+  ```\n+\n+  In the training program, every worker will run the train_op as if not\n+  model_average or synchronized. Note that if you want to run other ops like\n+  test op, you should use common session instead of monitoredSession:\n+\n+  ```python\n+  with training.MonitoredTrainingSession(\n+      master=workers[worker_id].target, is_chief=is_chief,\n+      hooks=[ma_replicas_hook, ma_hook]) as mon_sess:\n+    while not mon_sess.should_stop():\n+      mon_sess.run(training_op)\n+\n+      sess = mon_sess._tf_sess()\n+      sess.run(testing_op)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               replicas_to_aggregate,\n+               interval_steps,\n+               total_num_replicas=None,\n+               block_momentum_rate=0.0,\n+               use_nesterov=True,\n+               block_learning_rate=1.0):\n+    \"\"\"Construct a model_average optimizer.\n+\n+    Args:\n+      replicas_to_aggregate: number of replicas to aggregate for each variable\n+        update.\n+      interval_steps: number of steps between two \"average op\", which specifies\n+        how frequent a model synchronization is performed.\n+      total_num_replicas: Total number of tasks/workers/replicas, could be\n+        different from replicas_to_aggregate.\n+        total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n+        replicas_to_aggregate.\n+      block_momentum_rate: It brings in the historical blockwise gradients.\n+        The default value is 0.0. When using default value, the naive\n+        ModelAverage method is applied, and the original learning rate of\n+        local optimizer should be multiply by num_of_workers.\n+        If using BMUF algorithm, the block momentum_rate is usually set\n+        according to the number of workers: block_momentum_rate =\n+        1.0 - 1.0/num_of_workers, the learning rate of local optimizer can be\n+        unchanged.\n+        For details, see Ref: K. Chen and Q. Huo, \"Scalable training of deep\n+        learning machines by incremental block training with intra-block\n+        parallel optimization and blockwise model-update filtering,\" in\n+        Proceedings of ICASSP, 2016.\n+      use_nesterov: means the Nesterov-style momentum update is applied on the\n+        block level. The default value is true. This can accelerate training\n+        with non-zero block_momentum_rate.\n+      block_learning_rate: block_learning_rate is always 1.0 or slightly higher\n+        than 1.0\n+    \"\"\"\n+    if total_num_replicas is None:\n+      total_num_replicas = replicas_to_aggregate\n+    logging.info(\n+        \"ModelAverageV1: replicas_to_aggregate=%s; total_num_replicas=%s\",\n+        replicas_to_aggregate, total_num_replicas)\n+    self._replicas_to_aggregate = replicas_to_aggregate\n+    self._block_momentum_rate = block_momentum_rate\n+    self._block_learning_rate = block_learning_rate\n+    self._interval_steps = interval_steps\n+    self._use_nesterov = use_nesterov\n+    self._gradients_applied = False\n+    self._total_num_replicas = total_num_replicas\n+    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n+    self._global_step = None\n+    self._sync_token_queue = None\n+\n+    # The synchronization op will be executed in a queue runner which should\n+    # only be executed by one of the replicas (usually the chief).\n+    self._chief_queue_runner = None\n+\n+    # Remember which accumulator is on which device to set the initial step in\n+    # the accumulator to be global step. This list contains list of the\n+    # following format: (accumulator, device).\n+    self._accumulator_list = []\n+    # ModelAverageHook should be called before ReplicasHook.\n+    self._ma_run_hook = False\n+    # name: string. Name of the global variables and related operation on ps.\n+    self._name = 'modelAverage'\n+\n+    self.generate_local_and_global_variables()\n+\n+  def generate_local_and_global_variables(self):\n+    # Change all variables to local variables.\n+    for v in variables.global_variables():\n+      if v.op.name.find(self._name) != -1:\n+        raise AssertionError('%s: cannot use \\'%s\\' as a substr of any name for'\n+                             ' ops or variables when calling an '\n+                             'ModelAverageOptimizer.' % (v.op.name, self._name))\n+      ops.add_to_collection(ops.GraphKeys.LOCAL_VARIABLES, v)\n+    # Clear global_variables list.\n+    ops.get_default_graph().clear_collection(ops.GraphKeys.GLOBAL_VARIABLES)\n+    # Generate new global variables dependent on trainable variables.\n+    for i, v in enumerate(variables.trainable_variables()):\n+      if v.op.name.find(self._name) != -1:\n+        raise AssertionError('%s: cannot use \\'%s\\' as a substr of a name for '\n+                             'any ops or variables when calling an '\n+                             'ModelAverageOptimizer.' % (v.op.name, self._name))\n+      # v_g is the global-variable version of each user-defined trainable\n+      # variable.  They are supposed to be placed on PS device. v_g is used to\n+      # store averaged model parameters.\n+      v_g = variable_scope.get_variable(name='%s_g%d' % (self._name, i),\n+                                  initializer=v.initialized_value(),\n+                                  trainable=False,\n+                                  collections = [ops.GraphKeys.GLOBAL_VARIABLES,\n+                                                 'global_model'])\n+      v_block_grad = variable_scope.get_variable(\n+                  name='%s_block_grad%d' % (self._name, i), shape=v.get_shape(),\n+                  initializer=init_ops.constant_initializer(0.0, dtype=v.dtype),\n+                  trainable=False,\n+                  collections = [ops.GraphKeys.GLOBAL_VARIABLES, 'block_grads'])\n+      v_nesterov = variable_scope.get_variable(\n+                  name='%s_nesterov%d' % (self._name, i),\n+                  initializer=v_g.initialized_value(),\n+                  trainable=False,\n+                  collections = [ops.GraphKeys.GLOBAL_VARIABLES, 'nesterov'])\n+    self._super_global_step = variables.Variable(0,\n+                  name=\"%s_global_step\" % self._name, trainable=False)\n+    self._num_of_global_variables = len(variables.global_variables())\n+    self._num_of_trainable_variables = len(variables.trainable_variables())\n+\n+  def apply_model_average(self,\n+                          lvars_and_gvars,\n+                          global_vars,\n+                          v_vars,\n+                          nesterov_vars,\n+                          global_step=None,\n+                          name=None):\n+    \"\"\"Apply local weights to global variables.\n+\n+    Args:\n+      lvars_and_gvars: List of (local_vars, global_vars) pairs.\n+      global_step: Optional Variable to increment by one after the\n+        variables have been updated.\n+      name: Optional name for the returned operation.  Default to the\n+        name passed to the Optimizer constructor.\n+\n+    Returns:\n+      train_op: The op to dequeue a token so the replicas can exit this batch\n+      and start the next one. This is executed by each replica.\n+\n+    Raises:\n+      ValueError: If the lvars_and_gvars is empty.\n+      ValueError: If global step is not provided, the staleness cannot be\n+        checked.\n+    \"\"\"\n+    if not lvars_and_gvars:\n+      raise ValueError(\"Must supply at least one variable\")\n+\n+    if global_step is None:\n+      raise ValueError(\"Global step is required to check staleness\")\n+\n+    self._global_step = global_step\n+    train_ops = []\n+    aggregated_lvars = []\n+    var_list = []\n+\n+    model_reassign_ops = []\n+    nesterov_reassign_ops = []\n+    v_reassign_ops = []\n+\n+\n+    self._local_step = variables.Variable(\n+        initial_value=0,\n+        trainable=False,\n+        collections=[ops.GraphKeys.LOCAL_VARIABLES],\n+        dtype=global_step.dtype.base_dtype,\n+        name=\"ma_local_step\")\n+    self.local_step_init_op = state_ops.assign(self._local_step, global_step)\n+    chief_init_ops = [self.local_step_init_op]\n+    self.ready_for_local_init_op = variables.report_uninitialized_variables(\n+        variables.global_variables())\n+\n+    with ops.name_scope(None, self._name):\n+      for lvar, var in lvars_and_gvars:\n+        lvar = ops.convert_to_tensor(lvar)\n+        var_list.append(var)\n+        with ops.device(var.device):\n+          if lvar is None:\n+            aggregated_lvars.append(None)  # pass-through.\n+            continue\n+          elif isinstance(lvar, ops.Tensor):\n+            lvar_accum = data_flow_ops.ConditionalAccumulator(\n+                lvar.dtype,\n+                shape=var.get_shape(),\n+                shared_name=var.name + \"/lvar_accum\")\n+            train_ops.append(lvar_accum.apply_grad(\n+                lvar, local_step=self._local_step))\n+            aggregated_lvars.append(lvar_accum.take_grad(\n+                self._replicas_to_aggregate))\n+          else:\n+            if not isinstance(lvar, ops.IndexedSlices):\n+              raise ValueError(\"Unknown model variable type!\")\n+            lvar_accum = data_flow_ops.SparseConditionalAccumulator(\n+                                 lvar.dtype, shape=(),\n+                                 shared_name=var.name + \"/model_variable_accum\")\n+            train_ops.append(lvar_accum.apply_indexed_slices_grad(\n+                lvar, local_step=self._local_step))\n+            aggregated_lvars.append(lvar_accum.take_indexed_slices_grad(\n+                self._replicas_to_aggregate))\n+\n+          self._accumulator_list.append((lvar_accum, var.device))\n+\n+      aggregated_lvars_and_gvars = zip(aggregated_lvars, var_list)\n+\n+      # sync_op will be assigned to the same device as the global step.\n+      with ops.device(global_step.device), ops.name_scope(\"\"):\n+        for (avg_var, init_var), v in zip(aggregated_lvars_and_gvars, v_vars):\n+          gk_avg = math_ops.subtract(init_var, avg_var)\n+          gk_new = gk_avg\n+          block_grad = math_ops.multiply(self._block_learning_rate, gk_new)\n+          his_v = math_ops.multiply(self._block_momentum_rate, v)\n+          v_new = math_ops.add(his_v, block_grad)\n+          v_reassign_ops.append(state_ops.assign(v, v_new))\n+        v_op = control_flow_ops.group(*(v_reassign_ops))\n+        with ops.control_dependencies([v_op]):\n+          for global_var, v in zip(global_vars, v_vars):\n+            model_reassign_ops.append(state_ops.assign_sub(global_var, v))\n+          g_update_op = control_flow_ops.group(*(model_reassign_ops))\n+        with ops.control_dependencies([g_update_op]):\n+          for n_var, global_var, v in zip(nesterov_vars, global_vars, v_vars):\n+            momentum = math_ops.multiply(self._block_momentum_rate, v)\n+            nesterov = math_ops.subtract(global_var, momentum)\n+            nesterov_reassign_ops.append(state_ops.assign(n_var, nesterov))\n+          nesterov_reassign_ops.append(state_ops.assign_add(global_step, 1))\n+          update_op = control_flow_ops.group(*(nesterov_reassign_ops))\n+\n+\n+      self._local_step = self._local_step\n+      self._train_ops = train_ops\n+      # Create token queue.\n+      with ops.device(global_step.device), ops.name_scope(\"\"):\n+        sync_token_queue = (\n+            data_flow_ops.FIFOQueue(-1,\n+                                    global_step.dtype.base_dtype,\n+                                    shapes=(),\n+                                    name=\"sync_token_q\",\n+                                    shared_name=\"sync_token_q\"))\n+        self._sync_token_queue = sync_token_queue\n+\n+        # dummy_queue is passed to the queue runner. Don't use the real queues\n+        # because the queue runner doesn't automatically reopen it once it\n+        # closed queues in PS devices.\n+        dummy_queue = (\n+            data_flow_ops.FIFOQueue(1,\n+                                    types_pb2.DT_INT32,\n+                                    shapes=(),\n+                                    name=\"dummy_queue\",\n+                                    shared_name=\"dummy_queue\"))\n+\n+      with ops.device(global_step.device), ops.name_scope(\"\"):\n+        # Replicas have to wait until they can get a token from the token queue.\n+        with ops.control_dependencies(train_ops):\n+          token = sync_token_queue.dequeue()\n+        train_op = state_ops.assign(self._local_step, token)\n+\n+        with ops.control_dependencies([update_op]):\n+          # Sync_op needs to insert tokens to the token queue at the end of the\n+          # step so the replicas can fetch them to start the next step.\n+          tokens = array_ops.fill([self._tokens_per_step], global_step)\n+          sync_op = sync_token_queue.enqueue_many((tokens,))\n+\n+        self._chief_queue_runner = queue_runner.QueueRunner(dummy_queue,\n+                                                            [sync_op])\n+      for accum, dev in self._accumulator_list:\n+        with ops.device(dev):\n+          chief_init_ops.append(\n+              accum.set_global_step(\n+                  global_step, name=\"SetGlobalStep\"))\n+      self.chief_init_op = control_flow_ops.group(*(chief_init_ops))\n+      self._gradients_applied = True\n+      return train_op\n+\n+  def apply_refresh_local_vars(self, local_vars, global_vars):\n+    reassign_ops = []\n+    for local_var, global_var in zip(local_vars, global_vars):\n+      reassign_ops.append(state_ops.assign(local_var, global_var))\n+    refresh_ops = control_flow_ops.group(*(reassign_ops))\n+    return refresh_ops\n+\n+\n+  def get_chief_queue_runner(self):\n+    \"\"\"Returns the QueueRunner for the chief to execute.\n+\n+    This includes the operations to synchronize replicas: local weights,\n+    apply to variables, increment global step, insert tokens to token queue.\n+\n+    Note that this can only be called after calling apply_gradients() which\n+    actually generates this queuerunner.\n+\n+    Returns:\n+      A `QueueRunner` for chief to execute.\n+\n+    Raises:\n+      ValueError: If this is called before apply_gradients().\n+    \"\"\"\n+    if self._gradients_applied is False:\n+      raise ValueError(\"Should be called after apply_gradients().\")\n+\n+    return self._chief_queue_runner\n+\n+  def get_init_tokens_op(self, num_tokens=0):\n+    \"\"\"Returns the op to fill the sync_token_queue with the tokens.\n+\n+    This is supposed to be executed in the beginning of the chief/sync thread\n+    so that even if the total_num_replicas is less than replicas_to_aggregate,\n+    the model can still proceed as the replicas can compute multiple steps per\n+    variable update. Make sure:\n+    `num_tokens >= replicas_to_aggregate - total_num_replicas`.\n+\n+    Args:\n+      num_tokens: Number of tokens to add to the queue.\n+\n+    Returns:\n+      An op for the chief/sync replica to fill the token queue.\n+\n+    Raises:\n+      ValueError: If this is called before apply_gradients().\n+      ValueError: If num_tokens are smaller than replicas_to_aggregate -\n+        total_num_replicas.\n+    \"\"\"\n+    if self._gradients_applied is False:\n+      raise ValueError(\n+          \"get_init_tokens_op() should be called after apply_gradients().\")\n+\n+    tokens_needed = self._replicas_to_aggregate - self._total_num_replicas\n+    if num_tokens == -1:\n+      num_tokens = self._replicas_to_aggregate\n+    elif num_tokens < tokens_needed:\n+      raise ValueError(\n+          \"Too few tokens to finish the first step: %d (given) vs %d (needed)\" %\n+          (num_tokens, tokens_needed))\n+\n+    if num_tokens > 0:\n+      with ops.device(self._global_step.device), ops.name_scope(\"\"):\n+        tokens = array_ops.fill([num_tokens], self._global_step)\n+        init_tokens = self._sync_token_queue.enqueue_many((tokens,))\n+    else:\n+      init_tokens = control_flow_ops.no_op(name=\"no_init_tokens\")\n+\n+    return init_tokens\n+\n+  def make_ma_run_hook(self):\n+    local_vars = variables.trainable_variables()\n+    global_vars = ops.get_collection_ref('global_model')\n+    v_vars = ops.get_collection_ref('block_grads')\n+    nesterov_vars = ops.get_collection_ref('nesterov')\n+\n+    if self._use_nesterov:\n+      self._apply_refresh_local_vars_op = self.apply_refresh_local_vars(\n+                                                                  local_vars,\n+                                                                  nesterov_vars)\n+      local_and_init_vars = list(zip(local_vars, nesterov_vars))\n+      self._apply_init_nesterov_vars_op = self.apply_refresh_local_vars(\n+                                                                  nesterov_vars,\n+                                                                  global_vars)\n+    else:\n+      self._apply_refresh_local_vars_op = self.apply_refresh_local_vars(\n+                                                                  local_vars,\n+                                                                  global_vars)\n+      local_and_init_vars = list(zip(local_vars, global_vars))\n+      self._apply_init_nesterov_vars_op = None\n+\n+    self._apply_ma_op = self.apply_model_average(local_and_init_vars,\n+                                           global_vars,\n+                                           v_vars,\n+                                           nesterov_vars,\n+                                           global_step=self._super_global_step)\n+    self._ma_run_hook = True\n+\n+    return self._ModelAverageHook(self, self._apply_refresh_local_vars_op,\n+                                    self._apply_init_nesterov_vars_op,\n+                                    self._apply_ma_op,\n+                                    self._interval_steps,\n+                                    self._super_global_step)\n+\n+  def make_session_run_hook(self, is_chief, num_tokens=0):\n+    \"\"\"Creates a hook to handle ReplicasHook ops such as initialization.\"\"\"\n+    if self._ma_run_hook is False:\n+      raise ValueError(\"make_session_run_hook Should be \"\n+                       \"called after make_ma_run_hook.\")\n+\n+    if is_chief:\n+      return self._ReplicasHook(self.chief_init_op,\n+                                    self.ready_for_local_init_op,\n+                                    self.get_chief_queue_runner(),\n+                                    self.get_init_tokens_op(num_tokens))\n+\n+    return self._ReplicasHook(self.local_step_init_op,\n+                                  self.ready_for_local_init_op, None, None)\n+\n+  class _ModelAverageHook(session_run_hook.SessionRunHook):\n+    def __init__(self, parent, local_refresh_op, nesterov_init_op, apply_ma_op,\n+                 interval_steps, super_global_step):\n+      self._local_refresh_op = local_refresh_op", "path": "tensorflow/contrib/opt/python/training/model_average_optimizer.py", "position": null, "original_position": 611, "commit_id": "86498c37a589fe38a9464b15b2f39b1576b8cbec", "original_commit_id": "9e35501bf08632630016efb4167a361b4c6234f7", "user": {"login": "ry", "id": 80, "node_id": "MDQ6VXNlcjgw", "avatar_url": "https://avatars1.githubusercontent.com/u/80?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ry", "html_url": "https://github.com/ry", "followers_url": "https://api.github.com/users/ry/followers", "following_url": "https://api.github.com/users/ry/following{/other_user}", "gists_url": "https://api.github.com/users/ry/gists{/gist_id}", "starred_url": "https://api.github.com/users/ry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ry/subscriptions", "organizations_url": "https://api.github.com/users/ry/orgs", "repos_url": "https://api.github.com/users/ry/repos", "events_url": "https://api.github.com/users/ry/events{/privacy}", "received_events_url": "https://api.github.com/users/ry/received_events", "type": "User", "site_admin": false}, "body": "Use the same naming throughout. If I'm reading this correctly, elsewhere this is called _apply_refresh_local_vars_op", "created_at": "2017-08-15T17:26:31Z", "updated_at": "2017-09-28T04:17:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r133249873", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133249873"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r133249873"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581"}}, "body_html": "<p>Use the same naming throughout. If I'm reading this correctly, elsewhere this is called _apply_refresh_local_vars_op</p>", "body_text": "Use the same naming throughout. If I'm reading this correctly, elsewhere this is called _apply_refresh_local_vars_op"}