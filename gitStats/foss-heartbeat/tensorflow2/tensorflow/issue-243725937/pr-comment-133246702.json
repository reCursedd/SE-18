{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133246702", "pull_request_review_id": 56405047, "id": 133246702, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMzI0NjcwMg==", "diff_hunk": "@@ -0,0 +1,693 @@\n+# 2017 Contrib.\n+# ==============================================================================\n+\n+\"\"\"Synchronize replicas for model average training.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import time\n+import six\n+\n+from tensorflow.core.framework import node_def_pb2\n+from tensorflow.python.framework import device as pydev\n+from tensorflow.python.training import server_lib\n+from tensorflow.python.training import device_setter\n+from tensorflow.core.framework import types_pb2\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import data_flow_ops\n+from tensorflow.python.ops import init_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.ops import variables\n+from tensorflow.python.ops import variable_scope\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.training import queue_runner\n+from tensorflow.python.training import session_manager\n+from tensorflow.python.training import session_run_hook\n+\n+class _ModelAverageDeviceChooser(object):\n+  \"\"\"Class to choose devices for Ops in a model average training setup.\n+\n+  This class is not to be used directly by users.  See instead\n+  `model_average_device_setter()` below.\n+  \"\"\"\n+\n+  def __init__(self, ps_tasks, ps_device, worker_device, ps_ops,\n+               ps_strategy):\n+    \"\"\"Create a new `_ReplicaDeviceChooser`.\n+\n+    Args:\n+      ps_tasks: Number of tasks in the `ps` job.\n+      ps_device: String.  Name of the `ps` job.\n+      worker_device: String.  Name of the `worker` job.\n+      ps_ops: List of strings representing `Operation` types that need to be\n+        placed on `ps` devices.\n+      ps_strategy: A callable invoked for every ps `Operation` (i.e. matched by\n+        `ps_ops`), that takes the `Operation` and returns the ps task index to\n+        use.\n+    \"\"\"\n+    self._ps_tasks = ps_tasks\n+    self._ps_device = ps_device\n+    self._worker_device = worker_device\n+    self._ps_ops = ps_ops\n+    self._ps_strategy = ps_strategy\n+\n+  def device_function(self, op):\n+    \"\"\"Choose a device for `op`.\n+\n+    Args:\n+      op: an `Operation`.\n+\n+    Returns:\n+      The device to use for the `Operation`.\n+    \"\"\"\n+\n+    current_device = pydev.DeviceSpec.from_string(op.device or \"\")\n+\n+    node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n+    # TODO(chen meng): only global variables should be placed on ps. Now we just\n+    # do this according to substring in var name.\n+    if self._ps_tasks and self._ps_device and node_def.op in self._ps_ops \\\n+       and node_def.name.find('modelAverage') != -1:\n+      ps_device = pydev.DeviceSpec.from_string(self._ps_device)\n+\n+      current_job, ps_job = current_device.job, ps_device.job\n+      if ps_job and (not current_job or current_job == ps_job):\n+        ps_device.task = self._ps_strategy(op)\n+\n+      ps_device.merge_from(current_device)\n+      return ps_device.to_string()\n+\n+    worker_device = pydev.DeviceSpec.from_string(self._worker_device or \"\")\n+    worker_device.merge_from(current_device)\n+    return worker_device.to_string()\n+\n+def model_average_device_setter(ps_tasks=0, ps_device=\"/job:ps\",\n+                          worker_device=\"/job:worker\",\n+                          cluster=None, ps_ops=None, ps_strategy=None):\n+  \"\"\"Return a `device function` to use when building a Graph for model average.\n+\n+  There is only one difference between model_average_device_setter and\n+  replica_device_setter : replica_device_setter placed all variables\n+  (including global/local variables) on ps, while in model average, each worker\n+  own its local variables (local model parameters), these local variables\n+  should be placed in each worker.\n+  Args:\n+    ps_tasks: Number of tasks in the `ps` job.  Ignored if `cluster` is\n+      provided.\n+    ps_device: String.  Device of the `ps` job.  If empty no `ps` job is used.\n+      Defaults to `ps`.\n+    worker_device: String.  Device of the `worker` job.  If empty no `worker`\n+      job is used.\n+    cluster: `ClusterDef` proto or `ClusterSpec`.\n+    ps_ops: List of strings representing `Operation` types that need to be\n+      placed on `ps` devices.  If `None`, defaults to `[\"Variable\"]`.\n+    ps_strategy: A callable invoked for every ps `Operation` (i.e. matched by\n+      `ps_ops`), that takes the `Operation` and returns the ps task index to\n+      use.  If `None`, defaults to a round-robin strategy across all `ps`\n+      devices.\n+\n+  Returns:\n+    A function to pass to `tf.device()`.\n+\n+  Raises:\n+    TypeError if `cluster` is not a dictionary or `ClusterDef` protocol buffer,\n+    or if `ps_strategy` is provided but not a callable.\n+  \"\"\"\n+  if cluster is not None:\n+    if isinstance(cluster, server_lib.ClusterSpec):\n+      cluster_spec = cluster.as_dict()\n+    else:\n+      cluster_spec = server_lib.ClusterSpec(cluster).as_dict()\n+    # Get ps_job_name from ps_device by striping \"/job:\".\n+    ps_job_name = pydev.DeviceSpec.from_string(ps_device).job\n+    if ps_job_name not in cluster_spec or cluster_spec[ps_job_name] is None:\n+      return None\n+    ps_tasks = len(cluster_spec[ps_job_name])\n+\n+  if ps_tasks == 0:\n+    return None\n+\n+  if ps_ops is None:\n+    ps_ops = [\"Variable\", \"VariableV2\", \"VarHandleOp\"]\n+\n+  if ps_strategy is None:\n+    ps_strategy = device_setter._RoundRobinStrategy(ps_tasks)\n+  if not six.callable(ps_strategy):\n+    raise TypeError(\"ps_strategy must be callable\")\n+  chooser = _ModelAverageDeviceChooser(\n+      ps_tasks, ps_device, worker_device, ps_ops, ps_strategy)\n+  return chooser.device_function\n+\n+class ModelAverageOptimizer(object):\n+  \"\"\"Class to synchronize, aggregate model params.\n+\n+  In a typical synchronous training environment (N-replica synchronous training)\n+  , gradients will be averaged each step, and then applying them to the\n+  variables in one shot, after which replicas can fetch the new variables and\n+  continue. In a model average training environment, model variables will be\n+  averaged (or with momentum) every 'interval_steps' steps, and then fetch the\n+  new variables and continue training in local worker. In the interval between\n+  two \"average operation\", there are no data transfer at all, which can\n+  accerlate training.\n+\n+  The following accumulators/queue are created:\n+  <empty line>\n+  * N `model-variable accumulators`, one per variable for train model. local\n+  variables are pushed to them and the chief worker will wait until enough\n+  variables are collected and then average them. The accumulator will drop all\n+  stale variables (more details in the accumulator op).\n+  * 1 `token` queue where the optimizer pushes the new global_step value after\n+    all variables are updated.\n+\n+  The following local variable is created:\n+  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n+    each accumulator to check for staleness of the variables.\n+\n+  The optimizer adds nodes to the graph to collect local variables and pause\n+  the trainers until variables are updated.\n+  For the Parameter Server job:\n+  <empty line>\n+  1. An accumulator is created for each variable, and each replica pushes the\n+     local variables into the accumulators.\n+  2. Each accumulator averages once enough variables (replicas_to_aggregate)\n+     have been accumulated.\n+  3. apply the averaged variables to global variables.\n+  4. Only after all variables have been updated, increment the global step.\n+  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n+     each worker replica. The workers can now fetch the global step, use it to\n+     update its local_step variable and start the next batch.\n+\n+  For the replicas:\n+  <empty line>\n+  1. Start a training block: fetch variables, finish \"interval_steps\" steps\n+     training.\n+  2. Once current training block has been finished, push local variables into\n+     accumulators. Each accumulator will check the staleness and drop the\n+     stale ones.\n+  3. After pushing all the variables, dequeue an updated value of global_step\n+     from the token queue and record that step to its local_step variable. Note\n+     that this is effectively a barrier.\n+  4. fetch new variables, Start the next block.\n+\n+  ### Usage\n+\n+  ```python\n+  # Create any optimizer to update the variables, say a simple SGD:\n+  opt = GradientDescentOptimizer(learning_rate=0.1)\n+\n+  # Create a ModelAverageOptimizer to update the global variables:\n+  # Note that if you want to have 2 backup replicas, you can change\n+  # total_num_replicas=52 and make sure this number matches how many physical\n+  # replicas you started in your job.\n+  ma = tf.contrib.model_average.ModelAverageOptimizer(replicas_to_aggregate=50,\n+                                                      interval_steps=100)\n+\n+  # You can create the hook which handles model average operations.\n+  ma_hook = ma.make_ma_run_hook()\n+  # And also, create the hook which handles initialization and queues.\n+  ma_replicas_hook = ma.make_session_run_hook(is_chief)\n+  ```\n+\n+  In the training program, every worker will run the train_op as if not\n+  model_average or synchronized. Note that if you want to run other ops like\n+  test op, you should use common session instead of monitoredSession:\n+\n+  ```python\n+  with training.MonitoredTrainingSession(\n+      master=workers[worker_id].target, is_chief=is_chief,\n+      hooks=[ma_replicas_hook, ma_hook]) as mon_sess:\n+    while not mon_sess.should_stop():\n+      mon_sess.run(training_op)\n+\n+      sess = mon_sess._tf_sess()\n+      sess.run(testing_op)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               replicas_to_aggregate,\n+               interval_steps,\n+               total_num_replicas=None,\n+               block_momentum_rate=0.0,\n+               use_nesterov=True,\n+               block_learning_rate=1.0):\n+    \"\"\"Construct a model_average optimizer.\n+\n+    Args:\n+      replicas_to_aggregate: number of replicas to aggregate for each variable\n+        update.\n+      interval_steps: number of steps between two \"average op\", which specifies\n+        how frequent a model synchronization is performed.\n+      total_num_replicas: Total number of tasks/workers/replicas, could be\n+        different from replicas_to_aggregate.\n+        total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n+        replicas_to_aggregate.\n+      block_momentum_rate: It brings in the historical blockwise gradients.\n+        The default value is 0.0. When using default value, the naive\n+        ModelAverage method is applied, and the original learning rate of\n+        local optimizer should be multiply by num_of_workers.\n+        If using BMUF algorithm, the block momentum_rate is usually set\n+        according to the number of workers: block_momentum_rate =\n+        1.0 - 1.0/num_of_workers, the learning rate of local optimizer can be\n+        unchanged.\n+        For details, see Ref: K. Chen and Q. Huo, \"Scalable training of deep\n+        learning machines by incremental block training with intra-block\n+        parallel optimization and blockwise model-update filtering,\" in\n+        Proceedings of ICASSP, 2016.\n+      use_nesterov: means the Nesterov-style momentum update is applied on the\n+        block level. The default value is true. This can accelerate training\n+        with non-zero block_momentum_rate.\n+      block_learning_rate: block_learning_rate is always 1.0 or slightly higher\n+        than 1.0\n+    \"\"\"\n+    if total_num_replicas is None:\n+      total_num_replicas = replicas_to_aggregate\n+    logging.info(\n+        \"ModelAverageV1: replicas_to_aggregate=%s; total_num_replicas=%s\",\n+        replicas_to_aggregate, total_num_replicas)\n+    self._replicas_to_aggregate = replicas_to_aggregate\n+    self._block_momentum_rate = block_momentum_rate\n+    self._block_learning_rate = block_learning_rate\n+    self._interval_steps = interval_steps\n+    self._use_nesterov = use_nesterov\n+    self._gradients_applied = False\n+    self._total_num_replicas = total_num_replicas\n+    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n+    self._global_step = None\n+    self._sync_token_queue = None\n+\n+    # The synchronization op will be executed in a queue runner which should\n+    # only be executed by one of the replicas (usually the chief).\n+    self._chief_queue_runner = None\n+\n+    # Remember which accumulator is on which device to set the initial step in\n+    # the accumulator to be global step. This list contains list of the\n+    # following format: (accumulator, device).\n+    self._accumulator_list = []\n+    # ModelAverageHook should be called before ReplicasHook.\n+    self._ma_run_hook = False\n+    # name: string. Name of the global variables and related operation on ps.\n+    self._name = 'modelAverage'\n+\n+    self.generate_local_and_global_variables()\n+\n+  def generate_local_and_global_variables(self):\n+    # Change all variables to local variables.\n+    for v in variables.global_variables():\n+      if v.op.name.find(self._name) != -1:\n+        raise AssertionError('%s: cannot use \\'%s\\' as a substr of any name for'\n+                             ' ops or variables when calling an '\n+                             'ModelAverageOptimizer.' % (v.op.name, self._name))\n+      ops.add_to_collection(ops.GraphKeys.LOCAL_VARIABLES, v)\n+    # Clear global_variables list.\n+    ops.get_default_graph().clear_collection(ops.GraphKeys.GLOBAL_VARIABLES)\n+    # Generate new global variables dependent on trainable variables.\n+    for i, v in enumerate(variables.trainable_variables()):\n+      if v.op.name.find(self._name) != -1:\n+        raise AssertionError('%s: cannot use \\'%s\\' as a substr of a name for '\n+                             'any ops or variables when calling an '\n+                             'ModelAverageOptimizer.' % (v.op.name, self._name))\n+      # v_g is the global-variable version of each user-defined trainable\n+      # variable.  They are supposed to be placed on PS device. v_g is used to\n+      # store averaged model parameters.\n+      v_g = variable_scope.get_variable(name='%s_g%d' % (self._name, i),\n+                                  initializer=v.initialized_value(),\n+                                  trainable=False,\n+                                  collections = [ops.GraphKeys.GLOBAL_VARIABLES,\n+                                                 'global_model'])\n+      v_block_grad = variable_scope.get_variable(\n+                  name='%s_block_grad%d' % (self._name, i), shape=v.get_shape(),\n+                  initializer=init_ops.constant_initializer(0.0, dtype=v.dtype),\n+                  trainable=False,\n+                  collections = [ops.GraphKeys.GLOBAL_VARIABLES, 'block_grads'])\n+      v_nesterov = variable_scope.get_variable(\n+                  name='%s_nesterov%d' % (self._name, i),\n+                  initializer=v_g.initialized_value(),\n+                  trainable=False,\n+                  collections = [ops.GraphKeys.GLOBAL_VARIABLES, 'nesterov'])\n+    self._super_global_step = variables.Variable(0,\n+                  name=\"%s_global_step\" % self._name, trainable=False)\n+    self._num_of_global_variables = len(variables.global_variables())\n+    self._num_of_trainable_variables = len(variables.trainable_variables())\n+\n+  def apply_model_average(self,", "path": "tensorflow/contrib/opt/python/training/model_average_optimizer.py", "position": null, "original_position": 338, "commit_id": "86498c37a589fe38a9464b15b2f39b1576b8cbec", "original_commit_id": "9e35501bf08632630016efb4167a361b4c6234f7", "user": {"login": "ry", "id": 80, "node_id": "MDQ6VXNlcjgw", "avatar_url": "https://avatars1.githubusercontent.com/u/80?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ry", "html_url": "https://github.com/ry", "followers_url": "https://api.github.com/users/ry/followers", "following_url": "https://api.github.com/users/ry/following{/other_user}", "gists_url": "https://api.github.com/users/ry/gists{/gist_id}", "starred_url": "https://api.github.com/users/ry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ry/subscriptions", "organizations_url": "https://api.github.com/users/ry/orgs", "repos_url": "https://api.github.com/users/ry/repos", "events_url": "https://api.github.com/users/ry/events{/privacy}", "received_events_url": "https://api.github.com/users/ry/received_events", "type": "User", "site_admin": false}, "body": "Private methods should have a leading underscore.", "created_at": "2017-08-15T17:13:47Z", "updated_at": "2017-09-28T04:17:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r133246702", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133246702"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r133246702"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581"}}, "body_html": "<p>Private methods should have a leading underscore.</p>", "body_text": "Private methods should have a leading underscore."}