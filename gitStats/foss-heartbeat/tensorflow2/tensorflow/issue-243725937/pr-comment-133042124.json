{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133042124", "pull_request_review_id": 56177155, "id": 133042124, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMzA0MjEyNA==", "diff_hunk": "@@ -0,0 +1,581 @@\n+# 2017 Contrib.\n+# ==============================================================================\n+\n+\"\"\"Synchronize replicas for model average training.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import time\n+\n+from tensorflow.core.framework import types_pb2\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import data_flow_ops\n+from tensorflow.python.ops import init_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.ops import variables\n+from tensorflow.python.ops import variable_scope\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.training import queue_runner\n+from tensorflow.python.training import session_manager\n+from tensorflow.python.training import session_run_hook\n+\n+class ModelAverageOptimizer(object):\n+  \"\"\"Class to synchronize, aggregate model params.\n+\n+  In a typical synchronous training environment (N-replica synchronous training)\n+  , gradients will be averaged each step, and then applying them to the\n+  variables in one shot, after which replicas can fetch the new variables and\n+  continue. In a model average training environment, model variables will be\n+  averaged (or with momentum) every 'ma_intervals' steps, and then fetch the new\n+  variables and continue training in local worker. In the interval between two\n+  \"average operation\", there are no data transfer at all, which can accerlate\n+  training.\n+\n+  The following accumulators/queue are created:\n+  <empty line>\n+  * N `model-variable accumulators`, one per variable for train model. local\n+  variables are pushed to them and the chief worker will wait until enough\n+  variables are collected and then average them. The accumulator will drop all\n+  stale variables (more details in the accumulator op).\n+  * 1 `token` queue where the optimizer pushes the new global_step value after\n+    all variables are updated.\n+\n+  The following local variable is created:\n+  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n+    each accumulator to check for staleness of the variables.\n+\n+  The optimizer adds nodes to the graph to collect local variables and pause\n+  the trainers until variables are updated.\n+  For the Parameter Server job:\n+  <empty line>\n+  1. An accumulator is created for each variable, and each replica pushes the\n+     local variables into the accumulators.\n+  2. Each accumulator averages once enough variables (replicas_to_aggregate)\n+     have been accumulated.\n+  3. apply the averaged variables to global variables.\n+  4. Only after all variables have been updated, increment the global step.\n+  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n+     each worker replica. The workers can now fetch the global step, use it to\n+     update its local_step variable and start the next batch.\n+\n+  For the replicas:\n+  <empty line>\n+  1. Start a training block: fetch variables, finish \"ma_intervals\" steps\n+     training.\n+  2. Once current training block has been finished, push local variables into\n+     accumulators. Each accumulator will check the staleness and drop the\n+     stale ones.\n+  3. After pushing all the variables, dequeue an updated value of global_step\n+     from the token queue and record that step to its local_step variable. Note\n+     that this is effectively a barrier.\n+  4. fetch new variables, Start the next block.\n+\n+  ### Usage\n+\n+  ```python\n+  # Create any optimizer to update the variables, say a simple SGD:\n+  opt = GradientDescentOptimizer(learning_rate=0.1)\n+\n+  # Create a ModelAverageOptimizer to update the global variables:\n+  # Note that if you want to have 2 backup replicas, you can change\n+  # total_num_replicas=52 and make sure this number matches how many physical\n+  # replicas you started in your job.\n+  ma = tf.contrib.model_average.ModelAverageOptimizer(replicas_to_aggregate=50,\n+                                                      ma_intervals=100)\n+\n+  # You can create the hook which handles model average operations.\n+  ma_replicas_hook = ma.make_ma_run_hook()\n+  # And also, create the hook which handles initialization and queues.\n+  ma_hook = ma.make_session_run_hook(is_chief)\n+  ```\n+\n+  In the training program, every worker will run the train_op as if not\n+  model_average or synchronized. Note that if you want to run other ops like\n+  test op, you should use common session instead of monitoredSession:\n+\n+  ```python\n+  with training.MonitoredTrainingSession(\n+      master=workers[worker_id].target, is_chief=is_chief,\n+      hooks=[ma_replicas_hook, ma_hook]) as mon_sess:\n+    while not mon_sess.should_stop():\n+      mon_sess.run(training_op)\n+\n+      sess = mon_sess._tf_sess()\n+      sess.run(testing_op)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               replicas_to_aggregate,\n+               ma_intervals,\n+               total_num_replicas=None,\n+               block_momentum_rate=0.0,\n+               use_Nesterov=True,\n+               block_learning_rate=1.0):\n+    \"\"\"Construct a model_average optimizer.\n+\n+    Args:\n+      replicas_to_aggregate: number of replicas to aggregate for each variable\n+        update.\n+      ma_intervals: number of steps between two \"average op\", which specifies\n+        how frequent a model synchronization is performed.\n+      total_num_replicas: Total number of tasks/workers/replicas, could be\n+        different from replicas_to_aggregate.\n+        total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n+        replicas_to_aggregate.\n+      block_momentum_rate: It brings in the historical blockwise gradients.\n+        The default value is 0.0. When using default value, the naive\n+        ModelAverage method is applied, and the original learning rate of\n+        local optimizer should be multiply by num_of_workers.\n+        If using BMUF algorithm, the block momentum_rate is usually set\n+        according to the number of workers: block_momentum_rate =\n+        1.0 - 1.0/num_of_workers, the learning rate of local optimizer can be\n+        unchanged.\n+        For details, see Ref: K. Chen and Q. Huo, \"Scalable training of deep\n+        learning machines by incremental block training with intra-block\n+        parallel optimization and blockwise model-update filtering,\" in\n+        Proceedings of ICASSP, 2016.\n+      use_Nesterov: means the Nesterov-style momentum update is applied on the\n+        block level. The default value is true. This can accelerate training\n+        with non-zero block_momentum_rate.\n+      block_learning_rate: block_learning_rate is always 1.0 or slightly higher\n+        than 1.0\n+    \"\"\"\n+    if total_num_replicas is None:\n+      total_num_replicas = replicas_to_aggregate\n+    logging.info(\n+        \"ModelAverageV1: replicas_to_aggregate=%s; total_num_replicas=%s\",\n+        replicas_to_aggregate, total_num_replicas)\n+    self._replicas_to_aggregate = replicas_to_aggregate\n+    self._block_momentum_rate = block_momentum_rate\n+    self._block_learning_rate = block_learning_rate\n+    self._ma_intervals = ma_intervals\n+    self._use_Nesterov = use_Nesterov\n+    self._gradients_applied = False\n+    self._total_num_replicas = total_num_replicas\n+    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n+    self._global_step = None\n+    self._sync_token_queue = None\n+\n+    # The synchronization op will be executed in a queue runner which should\n+    # only be executed by one of the replicas (usually the chief).\n+    self._chief_queue_runner = None\n+\n+    # Remember which accumulator is on which device to set the initial step in\n+    # the accumulator to be global step. This list contains list of the\n+    # following format: (accumulator, device).\n+    self._accumulator_list = []\n+    # ma_run_hook should be called before SyncReplicasHook\n+    self._ma_run_hook = False\n+    # name: string. Name of the global variables and related operation on ps.\n+    self._name = 'modelAverage'\n+\n+    self.add_variables_for_betweenGraph()\n+\n+  def add_variables_for_betweenGraph(self):\n+    # change all variables to local variables\n+    for v in variables.global_variables():\n+      if v.op.name.find(self._name) != -1:\n+        raise AssertionError('%s: cannot use \\'%s\\' as a substr of any name for'\n+                             ' ops or variables when calling an '\n+                             'ModelAverageOptimizer.' % (v.op.name, self._name))\n+      ops.add_to_collection(ops.GraphKeys.LOCAL_VARIABLES, v)\n+    # clear global_variables list\n+    ops.get_default_graph().clear_collection(ops.GraphKeys.GLOBAL_VARIABLES)\n+    # generate new global variables dependent on trainable variables\n+    i = 0\n+    for v in variables.trainable_variables():", "path": "tensorflow/contrib/opt/python/training/model_average_optimizer.py", "position": null, "original_position": 192, "commit_id": "86498c37a589fe38a9464b15b2f39b1576b8cbec", "original_commit_id": "9f7b2fe99e5d63ae14f03d0ab02bbe3839d6021a", "user": {"login": "ry", "id": 80, "node_id": "MDQ6VXNlcjgw", "avatar_url": "https://avatars1.githubusercontent.com/u/80?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ry", "html_url": "https://github.com/ry", "followers_url": "https://api.github.com/users/ry/followers", "following_url": "https://api.github.com/users/ry/following{/other_user}", "gists_url": "https://api.github.com/users/ry/gists{/gist_id}", "starred_url": "https://api.github.com/users/ry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ry/subscriptions", "organizations_url": "https://api.github.com/users/ry/orgs", "repos_url": "https://api.github.com/users/ry/repos", "events_url": "https://api.github.com/users/ry/events{/privacy}", "received_events_url": "https://api.github.com/users/ry/received_events", "type": "User", "site_admin": false}, "body": "for i, v in enumerate(variables.trainable_variables())", "created_at": "2017-08-14T19:39:19Z", "updated_at": "2017-09-28T04:17:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r133042124", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/133042124"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r133042124"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581"}}, "body_html": "<p>for i, v in enumerate(variables.trainable_variables())</p>", "body_text": "for i, v in enumerate(variables.trainable_variables())"}