{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/134398226", "pull_request_review_id": 57682525, "id": 134398226, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNDM5ODIyNg==", "diff_hunk": "@@ -0,0 +1,696 @@\n+# 2017 Contrib.\n+# ==============================================================================\n+\n+\"\"\"Synchronize replicas for model average training.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import time\n+import six\n+\n+from tensorflow.core.framework import node_def_pb2\n+from tensorflow.python.framework import device as pydev\n+from tensorflow.python.training import server_lib\n+from tensorflow.python.training import device_setter\n+from tensorflow.core.framework import types_pb2\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import data_flow_ops\n+from tensorflow.python.ops import init_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.ops import variables\n+from tensorflow.python.ops import variable_scope\n+from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.training import queue_runner\n+from tensorflow.python.training import session_manager\n+from tensorflow.python.training import session_run_hook\n+\n+class _ModelAverageDeviceChooser(object):\n+  \"\"\"Class to choose devices for Ops in a model average training setup.\n+\n+  This class is not to be used directly by users.  See instead\n+  `model_average_device_setter()` below.\n+  \"\"\"\n+\n+  def __init__(self, ps_tasks, ps_device, worker_device, ps_ops,\n+               ps_strategy):\n+    \"\"\"Create a new `_ReplicaDeviceChooser`.\n+\n+    Args:\n+      ps_tasks: Number of tasks in the `ps` job.\n+      ps_device: String.  Name of the `ps` job.\n+      worker_device: String.  Name of the `worker` job.\n+      ps_ops: List of strings representing `Operation` types that need to be\n+        placed on `ps` devices.\n+      ps_strategy: A callable invoked for every ps `Operation` (i.e. matched by\n+        `ps_ops`), that takes the `Operation` and returns the ps task index to\n+        use.\n+    \"\"\"\n+    self._ps_tasks = ps_tasks\n+    self._ps_device = ps_device\n+    self._worker_device = worker_device\n+    self._ps_ops = ps_ops\n+    self._ps_strategy = ps_strategy\n+\n+  def device_function(self, op):\n+    \"\"\"Choose a device for `op`.\n+\n+    Args:\n+      op: an `Operation`.\n+\n+    Returns:\n+      The device to use for the `Operation`.\n+    \"\"\"\n+\n+    current_device = pydev.DeviceSpec.from_string(op.device or \"\")\n+\n+    node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n+    # TODO(chen meng): only global variables should be placed on ps. Now we just\n+    # do this according to substring in var name.\n+    if self._ps_tasks and self._ps_device and node_def.op in self._ps_ops \\\n+       and node_def.name.find('modelAverage') != -1:\n+      ps_device = pydev.DeviceSpec.from_string(self._ps_device)\n+\n+      current_job, ps_job = current_device.job, ps_device.job\n+      if ps_job and (not current_job or current_job == ps_job):\n+        ps_device.task = self._ps_strategy(op)\n+\n+      ps_device.merge_from(current_device)\n+      return ps_device.to_string()\n+\n+    worker_device = pydev.DeviceSpec.from_string(self._worker_device or \"\")\n+    worker_device.merge_from(current_device)\n+    return worker_device.to_string()\n+\n+def model_average_device_setter(ps_tasks=0, ps_device=\"/job:ps\",\n+                                worker_device=\"/job:worker\",\n+                                cluster=None, ps_ops=None, ps_strategy=None):\n+  \"\"\"Return a `device function` to use when building a Graph for model average.\n+\n+  There is only one difference between model_average_device_setter and\n+  replica_device_setter : replica_device_setter placed all variables\n+  (including global/local variables) on ps, while in model average, each worker\n+  own its local variables (local model parameters), these local variables\n+  should be placed in each worker.\n+  Args:\n+    ps_tasks: Number of tasks in the `ps` job.  Ignored if `cluster` is\n+      provided.\n+    ps_device: String.  Device of the `ps` job.  If empty no `ps` job is used.\n+      Defaults to `ps`.\n+    worker_device: String.  Device of the `worker` job.  If empty no `worker`\n+      job is used.\n+    cluster: `ClusterDef` proto or `ClusterSpec`.\n+    ps_ops: List of strings representing `Operation` types that need to be\n+      placed on `ps` devices.  If `None`, defaults to `[\"Variable\"]`.\n+    ps_strategy: A callable invoked for every ps `Operation` (i.e. matched by\n+      `ps_ops`), that takes the `Operation` and returns the ps task index to\n+      use.  If `None`, defaults to a round-robin strategy across all `ps`\n+      devices.\n+\n+  Returns:\n+    A function to pass to `tf.device()`.\n+\n+  Raises:\n+    TypeError if `cluster` is not a dictionary or `ClusterDef` protocol buffer,\n+    or if `ps_strategy` is provided but not a callable.\n+  \"\"\"\n+  if cluster is not None:\n+    if isinstance(cluster, server_lib.ClusterSpec):\n+      cluster_spec = cluster.as_dict()\n+    else:\n+      cluster_spec = server_lib.ClusterSpec(cluster).as_dict()\n+    # Get ps_job_name from ps_device by striping \"/job:\".\n+    ps_job_name = pydev.DeviceSpec.from_string(ps_device).job\n+    if ps_job_name not in cluster_spec or cluster_spec[ps_job_name] is None:\n+      return None\n+    ps_tasks = len(cluster_spec[ps_job_name])\n+\n+  if ps_tasks == 0:\n+    return None\n+\n+  if ps_ops is None:\n+    ps_ops = [\"Variable\", \"VariableV2\", \"VarHandleOp\"]\n+\n+  if ps_strategy is None:\n+    # pylint: disable=protected-access\n+    ps_strategy = device_setter._RoundRobinStrategy(ps_tasks)\n+  if not six.callable(ps_strategy):\n+    raise TypeError(\"ps_strategy must be callable\")\n+  chooser = _ModelAverageDeviceChooser(\n+      ps_tasks, ps_device, worker_device, ps_ops, ps_strategy)\n+  return chooser.device_function\n+\n+class ModelAverageOptimizer(object):\n+  \"\"\"Class to synchronize, aggregate model params.\n+\n+  In a typical synchronous training environment (N-replica synchronous training)\n+  , gradients will be averaged each step, and then applying them to the\n+  variables in one shot, after which replicas can fetch the new variables and\n+  continue. In a model average training environment, model variables will be\n+  averaged (or with momentum) every 'interval_steps' steps, and then fetch the\n+  new variables and continue training in local worker. In the interval between\n+  two \"average operation\", there are no data transfer at all, which can\n+  accerlate training.\n+\n+  The following accumulators/queue are created:\n+  <empty line>\n+  * N `model-variable accumulators`, one per variable for train model. local\n+  variables are pushed to them and the chief worker will wait until enough\n+  variables are collected and then average them. The accumulator will drop all\n+  stale variables (more details in the accumulator op).\n+  * 1 `token` queue where the optimizer pushes the new global_step value after\n+    all variables are updated.\n+\n+  The following local variable is created:\n+  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n+    each accumulator to check for staleness of the variables.\n+\n+  The optimizer adds nodes to the graph to collect local variables and pause\n+  the trainers until variables are updated.\n+  For the Parameter Server job:\n+  <empty line>\n+  1. An accumulator is created for each variable, and each replica pushes the\n+     local variables into the accumulators.\n+  2. Each accumulator averages once enough variables (replicas_to_aggregate)\n+     have been accumulated.\n+  3. apply the averaged variables to global variables.\n+  4. Only after all variables have been updated, increment the global step.\n+  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n+     each worker replica. The workers can now fetch the global step, use it to\n+     update its local_step variable and start the next batch.\n+\n+  For the replicas:\n+  <empty line>\n+  1. Start a training block: fetch variables, finish \"interval_steps\" steps\n+     training.\n+  2. Once current training block has been finished, push local variables into\n+     accumulators. Each accumulator will check the staleness and drop the\n+     stale ones.\n+  3. After pushing all the variables, dequeue an updated value of global_step\n+     from the token queue and record that step to its local_step variable. Note\n+     that this is effectively a barrier.\n+  4. fetch new variables, Start the next block.\n+\n+  ### Usage\n+\n+  ```python\n+  # Create any optimizer to update the variables, say a simple SGD:\n+  opt = GradientDescentOptimizer(learning_rate=0.1)\n+\n+  # Create a ModelAverageOptimizer to update the global variables:\n+  # Note that if you want to have 2 backup replicas, you can change\n+  # total_num_replicas=52 and make sure this number matches how many physical\n+  # replicas you started in your job.\n+  ma = tf.contrib.model_average.ModelAverageOptimizer(replicas_to_aggregate=50,\n+                                                      interval_steps=100)\n+\n+  # You can create the hook which handles model average operations.\n+  ma_hook = ma.make_ma_run_hook()\n+  # And also, create the hook which handles initialization and queues.\n+  ma_replicas_hook = ma.make_session_run_hook(is_chief)\n+  ```\n+\n+  In the training program, every worker will run the train_op as if not\n+  model_average or synchronized. Note that if you want to run other ops like\n+  test op, you should use common session instead of monitoredSession:\n+\n+  ```python\n+  with training.MonitoredTrainingSession(\n+      master=workers[worker_id].target, is_chief=is_chief,\n+      hooks=[ma_replicas_hook, ma_hook]) as mon_sess:\n+    while not mon_sess.should_stop():\n+      mon_sess.run(training_op)\n+\n+      sess = mon_sess._tf_sess()\n+      sess.run(testing_op)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               replicas_to_aggregate,\n+               interval_steps,\n+               total_num_replicas=None,\n+               block_momentum_rate=0.0,\n+               use_nesterov=True,\n+               block_learning_rate=1.0):\n+    \"\"\"Construct a model_average optimizer.\n+\n+    Args:\n+      replicas_to_aggregate: number of replicas to aggregate for each variable\n+        update.\n+      interval_steps: number of steps between two \"average op\", which specifies\n+        how frequent a model synchronization is performed.\n+      total_num_replicas: Total number of tasks/workers/replicas, could be\n+        different from replicas_to_aggregate.\n+        total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n+        replicas_to_aggregate.\n+      block_momentum_rate: It brings in the historical blockwise gradients.\n+        The default value is 0.0. When using default value, the naive\n+        ModelAverage method is applied, and the original learning rate of\n+        local optimizer should be multiply by num_of_workers.\n+        If using BMUF algorithm, the block momentum_rate is usually set\n+        according to the number of workers: block_momentum_rate =\n+        1.0 - 1.0/num_of_workers, the learning rate of local optimizer can be\n+        unchanged.\n+        For details, see Ref: K. Chen and Q. Huo, \"Scalable training of deep\n+        learning machines by incremental block training with intra-block\n+        parallel optimization and blockwise model-update filtering,\" in\n+        Proceedings of ICASSP, 2016.\n+      use_nesterov: means the Nesterov-style momentum update is applied on the\n+        block level. The default value is true. This can accelerate training\n+        with non-zero block_momentum_rate.\n+      block_learning_rate: block_learning_rate is always 1.0 or slightly higher\n+        than 1.0\n+    \"\"\"\n+    if total_num_replicas is None:\n+      total_num_replicas = replicas_to_aggregate\n+    logging.info(\n+        \"ModelAverageV1: replicas_to_aggregate=%s; total_num_replicas=%s\",\n+        replicas_to_aggregate, total_num_replicas)\n+    self._replicas_to_aggregate = replicas_to_aggregate\n+    self._block_momentum_rate = block_momentum_rate\n+    self._block_learning_rate = block_learning_rate\n+    self._interval_steps = interval_steps\n+    self._use_nesterov = use_nesterov\n+    self._gradients_applied = False\n+    self._total_num_replicas = total_num_replicas\n+    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n+    self._global_step = None\n+    self._sync_token_queue = None\n+\n+    # The synchronization op will be executed in a queue runner which should\n+    # only be executed by one of the replicas (usually the chief).\n+    self._chief_queue_runner = None\n+\n+    # Remember which accumulator is on which device to set the initial step in\n+    # the accumulator to be global step. This list contains list of the\n+    # following format: (accumulator, device).\n+    self._accumulator_list = []\n+    # ModelAverageHook should be called before ReplicasHook.\n+    self._ma_run_hook = False\n+    # name: string. Name of the global variables and related operation on ps.\n+    self._name = 'modelAverage'\n+\n+    self._generate_local_and_global_variables()\n+\n+  def _generate_local_and_global_variables(self):\n+    \"\"\"Change all variables to local variables and generate a global-version\n+       placed on ps for each.\n+    \"\"\"\n+    # Change all variables to local variables.\n+    for v in variables.global_variables():\n+      if v.op.name.find(self._name) != -1:\n+        raise AssertionError('%s: cannot use \\'%s\\' as a substr of any name for'\n+                             ' ops or variables when calling an '\n+                             'ModelAverageOptimizer.' % (v.op.name, self._name))\n+      ops.add_to_collection(ops.GraphKeys.LOCAL_VARIABLES, v)\n+    # Clear global_variables list.\n+    ops.get_default_graph().clear_collection(ops.GraphKeys.GLOBAL_VARIABLES)\n+    # Generate new global variables dependent on trainable variables.\n+    for i, v in enumerate(variables.trainable_variables()):\n+      if v.op.name.find(self._name) != -1:\n+        raise AssertionError('%s: cannot use \\'%s\\' as a substr of a name for '\n+                             'any ops or variables when calling an '\n+                             'ModelAverageOptimizer.' % (v.op.name, self._name))\n+      # v_g is the global-variable version of each user-defined trainable\n+      # variable.  They are supposed to be placed on PS device. v_g is used to\n+      # store averaged model parameters.\n+      v_g = variable_scope.get_variable(\n+          name='%s_g%d' % (self._name, i),\n+          initializer=v.initialized_value(),\n+          trainable=False,\n+          collections=[ops.GraphKeys.GLOBAL_VARIABLES, 'global_model'])\n+      v_block_grad = variable_scope.get_variable(\n+          name='%s_block_grad%d' % (self._name, i), shape=v.get_shape(),\n+          initializer=init_ops.constant_initializer(0.0, dtype=v.dtype),\n+          trainable=False,\n+          collections=[ops.GraphKeys.GLOBAL_VARIABLES, 'block_grad'])\n+      v_nesterov = variable_scope.get_variable(\n+          name='%s_nesterov%d' % (self._name, i),\n+          initializer=v_g.initialized_value(),\n+          trainable=False,\n+          collections=[ops.GraphKeys.GLOBAL_VARIABLES, 'nesterov'])\n+    self._super_global_step = variables.Variable(0, name=\"%s_global_step\" %\n+                                                 self._name, trainable=False)\n+    self._num_of_global_variables = len(variables.global_variables())\n+    self._num_of_trainable_variables = len(variables.trainable_variables())\n+\n+  def _apply_model_average(self,\n+                           lvars_and_gvars,\n+                           global_vars,\n+                           block_grad_vars,\n+                           nesterov_vars,\n+                           global_step=None,\n+                           name=None):\n+    \"\"\"Apply local weights to global variables.\n+\n+    Args:\n+      lvars_and_gvars: List of (local_vars, global_vars) pairs.\n+      global_vars: The averaged weights.\n+      block_grad_vars: The historical blockwise gradients.\n+      nesterov_vars: The Nesterov-style momentum updated weights.\n+      global_step: Optional Variable to increment by one after the\n+        variables have been updated.\n+      name: Optional name for the returned operation.  Default to the\n+        name passed to the Optimizer constructor.\n+\n+    Returns:\n+      train_op: The op to dequeue a token so the replicas can exit this batch\n+      and start the next one. This is executed by each replica.\n+\n+    Raises:\n+      ValueError: If the lvars_and_gvars is empty.\n+      ValueError: If global step is not provided, the staleness cannot be\n+        checked.\n+    \"\"\"\n+    if not lvars_and_gvars:\n+      raise ValueError(\"Must supply at least one variable\")\n+\n+    if global_step is None:\n+      raise ValueError(\"Global step is required to check staleness\")\n+\n+    self._global_step = global_step\n+    train_ops = []\n+    aggregated_lvars = []\n+    var_list = []\n+\n+    model_reassign_ops = []\n+    nesterov_reassign_ops = []\n+    bg_reassign_ops = []\n+\n+\n+    self._local_step = variables.Variable(\n+        initial_value=0,\n+        trainable=False,\n+        collections=[ops.GraphKeys.LOCAL_VARIABLES],\n+        dtype=global_step.dtype.base_dtype,\n+        name=\"ma_local_step\")\n+    self.local_step_init_op = state_ops.assign(self._local_step, global_step)\n+    chief_init_ops = [self.local_step_init_op]\n+    self.ready_for_local_init_op = variables.report_uninitialized_variables(\n+        variables.global_variables())\n+\n+    with ops.name_scope(None, self._name):\n+      for lvar, var in lvars_and_gvars:\n+        lvar = ops.convert_to_tensor(lvar)\n+        var_list.append(var)\n+        with ops.device(var.device):\n+          if lvar is None:\n+            aggregated_lvars.append(None)  # pass-through.\n+            continue\n+          elif isinstance(lvar, ops.Tensor):\n+            lvar_accum = data_flow_ops.ConditionalAccumulator(\n+                lvar.dtype,\n+                shape=var.get_shape(),\n+                shared_name=var.name + \"/lvar_accum\")\n+            train_ops.append(lvar_accum.apply_grad(\n+                lvar, local_step=self._local_step))\n+            aggregated_lvars.append(lvar_accum.take_grad(\n+                self._replicas_to_aggregate))\n+          else:\n+            if not isinstance(lvar, ops.IndexedSlices):\n+              raise ValueError(\"Unknown model variable type!\")\n+            lvar_accum = data_flow_ops.SparseConditionalAccumulator(\n+                lvar.dtype, shape=(),\n+                shared_name=var.name + \"/model_variable_accum\")\n+            train_ops.append(lvar_accum.apply_indexed_slices_grad(\n+                lvar, local_step=self._local_step))\n+            aggregated_lvars.append(lvar_accum.take_indexed_slices_grad(\n+                self._replicas_to_aggregate))\n+\n+          self._accumulator_list.append((lvar_accum, var.device))\n+\n+      aggregated_lvars_and_gvars = zip(aggregated_lvars, var_list)\n+\n+      # sync_op will be assigned to the same device as the global step.\n+      with ops.device(global_step.device), ops.name_scope(\"\"):\n+        for (avg_var, init_var), bg_var in zip(aggregated_lvars_and_gvars,\n+                                               block_grad_vars):\n+          gk_avg = math_ops.subtract(init_var, avg_var)\n+          gk_new = gk_avg\n+          block_grad = math_ops.multiply(self._block_learning_rate, gk_new)\n+          his_bg = math_ops.multiply(self._block_momentum_rate, bg_var)\n+          bg_new = math_ops.add(his_bg, block_grad)\n+          bg_reassign_ops.append(state_ops.assign(bg_var, bg_new))\n+        bg_op = control_flow_ops.group(*(bg_reassign_ops))\n+        with ops.control_dependencies([bg_op]):\n+          for global_var, bg_var in zip(global_vars, block_grad_vars):\n+            model_reassign_ops.append(state_ops.assign_sub(global_var, bg_var))\n+          g_update_op = control_flow_ops.group(*(model_reassign_ops))\n+        with ops.control_dependencies([g_update_op]):\n+          for n_var, global_var, bg_var in zip(nesterov_vars, global_vars,\n+                                               block_grad_vars):\n+            momentum = math_ops.multiply(self._block_momentum_rate, bg_var)\n+            nesterov = math_ops.subtract(global_var, momentum)\n+            nesterov_reassign_ops.append(state_ops.assign(n_var, nesterov))", "path": "tensorflow/contrib/opt/python/training/model_average_optimizer.py", "position": 451, "original_position": 449, "commit_id": "86498c37a589fe38a9464b15b2f39b1576b8cbec", "original_commit_id": "4bb973323e87b2a442dcedd710fe3f14106d6182", "user": {"login": "abenmao", "id": 29789552, "node_id": "MDQ6VXNlcjI5Nzg5NTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/29789552?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abenmao", "html_url": "https://github.com/abenmao", "followers_url": "https://api.github.com/users/abenmao/followers", "following_url": "https://api.github.com/users/abenmao/following{/other_user}", "gists_url": "https://api.github.com/users/abenmao/gists{/gist_id}", "starred_url": "https://api.github.com/users/abenmao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abenmao/subscriptions", "organizations_url": "https://api.github.com/users/abenmao/orgs", "repos_url": "https://api.github.com/users/abenmao/repos", "events_url": "https://api.github.com/users/abenmao/events{/privacy}", "received_events_url": "https://api.github.com/users/abenmao/received_events", "type": "User", "site_admin": false}, "body": "Yes, this should be executed even if use_nesterov=False.", "created_at": "2017-08-22T06:58:34Z", "updated_at": "2017-09-28T04:17:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r134398226", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/134398226"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11581#discussion_r134398226"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11581"}}, "body_html": "<p>Yes, this should be executed even if use_nesterov=False.</p>", "body_text": "Yes, this should be executed even if use_nesterov=False.", "in_reply_to_id": 133991511}