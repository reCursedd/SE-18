{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/393078218", "html_url": "https://github.com/tensorflow/tensorflow/pull/11581#issuecomment-393078218", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11581", "id": 393078218, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzA3ODIxOA==", "user": {"login": "yunyin", "id": 9065977, "node_id": "MDQ6VXNlcjkwNjU5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9065977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yunyin", "html_url": "https://github.com/yunyin", "followers_url": "https://api.github.com/users/yunyin/followers", "following_url": "https://api.github.com/users/yunyin/following{/other_user}", "gists_url": "https://api.github.com/users/yunyin/gists{/gist_id}", "starred_url": "https://api.github.com/users/yunyin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yunyin/subscriptions", "organizations_url": "https://api.github.com/users/yunyin/orgs", "repos_url": "https://api.github.com/users/yunyin/repos", "events_url": "https://api.github.com/users/yunyin/events{/privacy}", "received_events_url": "https://api.github.com/users/yunyin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T08:36:03Z", "updated_at": "2018-05-30T08:36:03Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=80\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ry\">@ry</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=29789552\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/abenmao\">@abenmao</a><br>\nI still have problems with <code>model_average_model</code></p>\n<p>I use <code>tensorflow_gpu==1.8.0</code> in <code>anaconda2.5</code>, <code>model_average_optimizer_test.py</code> can run easily, however some errors occured with my own code.</p>\n<p>Also, I use <code>tensorflow.contrib.opt.python.training.model_average_optimizer</code>, other errors comes: <a href=\"url\">https://github.com/tensorflow/tensorflow/issues/19617</a></p>\n<p>errors are shown below.</p>\n<pre><code>InvalidArgumentError (see above for traceback): Cannot colocate nodes 'modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0' and 'modelAverage/g/v1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'\n[[Node: modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0 = Switch[T=DT_FLOAT, _class=[\"loc:@modelAverage/g/v1\"], _device=\"/job:worker/task:1\"](modelAverage/nesterov/v1/cond_1/Merge, cond_3/pred_id)]]\n</code></pre>\n<p>The run commad is</p>\n<pre><code>~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=0\n~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=1\n~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=server --task_id=0\n</code></pre>\n<p>and code is</p>\n<pre><code>import os\nimport time\n \nimport json\nimport copy\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nimport model_average_optimizer\n \nflags = tf.flags\nflags.DEFINE_string(\"server_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"job_name\", \"\", \"Either 'server' of 'worker'\")\nflags.DEFINE_integer(\"task_id\", 0, \"Task Id for Each workers\")\n \nFLAGS = flags.FLAGS\ntf.logging.set_verbosity(tf.logging.INFO)\n \ndef workers_ps_creator(args):\n    ps_hosts = args.server_hosts.split(\",\")\n    worker_hosts = args.worker_hosts.split(\",\")\n    num_workers = len(worker_hosts)\n \n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts,\"worker\": worker_hosts})\n    gpu_options = tf.GPUOptions(allocator_type='BFC', allow_growth=True)\n    if args.job_name == \"server\":\n        server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\n                job_name='ps',\n                task_index=args.task_id,\n                default_session_config=tf.ConfigProto(gpu_options=gpu_options, device_count={\"GPU\":0}),\n                protocol=\"grpc\")\n    elif args.job_name == \"worker\":\n        server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\n                job_name=\"worker\",\n                task_index=args.task_id,\n                default_session_config = tf.ConfigProto(gpu_options=gpu_options),\n                protocol=\"grpc\")\n    server = tf.train.Server(server_def)\n    return server, cluster, num_workers, gpu_options\n \ndef Model(sync_opt):\n    if FLAGS.task_id == 0:\n        var_0 = tf.get_variable(initializer = 0.0, name = 'v0')\n        var_1 = tf.get_variable(initializer = 1.0, name = 'v1')\n        grads_0 = constant_op.constant(-1.0)\n        grads_1 = constant_op.constant(-1.0)\n    else:\n        var_0 = tf.get_variable(initializer = 7.0, name = 'v0')\n        var_1 = tf.get_variable(initializer = 8.0, name = 'v1')\n        grads_0 = constant_op.constant(-2.0)\n        grads_1 = constant_op.constant(-2.0)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = sync_opt.apply_gradients([[grads_0, var_0], [grads_1, var_1]],\n            global_step = tf.train.get_or_create_global_step())\n    return train_op\n \ndef test():\n    server, cluster, num_workers, gpu_options = workers_ps_creator(FLAGS)\n \n    if FLAGS.job_name == \"server\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        is_chief = (FLAGS.task_id == 0)\n        #Between-graph replication\n        worker_device = \"/job:worker/task:%d\" % (FLAGS.task_id)\n        with tf.device(model_average_optimizer.model_average_device_setter(\n                worker_device=worker_device, cluster=cluster)):\n            #create model\n            lr = tf.Variable(1, trainable=False)\n            opt = tf.train.GradientDescentOptimizer(lr)\n            train_model = Model(opt)\n            ma_opt = model_average_optimizer.ModelAverageOptimizer(num_workers, 3)\n\t\n            ## store local variables\n            collection_key = tf.GraphKeys.SAVERS\n            saver = tf.train.Saver(tf.local_variables(), save_relative_paths=True)\n            if saver is not None: tf.add_to_collection(collection_key, saver)\n\t\n        sess_config = tf.ConfigProto(gpu_options=gpu_options)\n        sess_config.log_device_placement = False\n        sess_config.allow_soft_placement = True\n\t\n        ma_hook = ma_opt.make_ma_run_hook()\n        ma_replica_hook = ma_opt.make_session_run_hook(is_chief)\n        all_hooks = [ma_replica_hook, ma_hook]\n\t\n        tf.logging.info('Start Sess')\n        with tf.train.MonitoredTrainingSession(master=server.target,\n                    is_chief=is_chief,\n                    hooks=all_hooks) as sess:\n            tf.logging.info(\"is chief: %s, len: %s\", is_chief, num_workers)\n            for i in range(4):\n                sess.run(train_op)\n                pp1 = sess.run(tf.get_default_graph().get_tensor_by_name('v0:0'))\n                pp2 = sess.run(tf.get_default_graph().get_tensor_by_name('v1:0'))\n                tf.logging.info(\"%d %.2f %.2f\" % (FLAGS.task_id, pp1, pp2))\n        sv.stop()\n        tf.logging.info(\"done\")\n\t\ndef main(_):\n    test()\n\t\nif __name__ == '__main__':\n    tf.app.run()\n</code></pre>", "body_text": "@ry @abenmao\nI still have problems with model_average_model\nI use tensorflow_gpu==1.8.0 in anaconda2.5, model_average_optimizer_test.py can run easily, however some errors occured with my own code.\nAlso, I use tensorflow.contrib.opt.python.training.model_average_optimizer, other errors comes: https://github.com/tensorflow/tensorflow/issues/19617\nerrors are shown below.\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0' and 'modelAverage/g/v1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'\n[[Node: modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0 = Switch[T=DT_FLOAT, _class=[\"loc:@modelAverage/g/v1\"], _device=\"/job:worker/task:1\"](modelAverage/nesterov/v1/cond_1/Merge, cond_3/pred_id)]]\n\nThe run commad is\n~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=0\n~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=1\n~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=server --task_id=0\n\nand code is\nimport os\nimport time\n \nimport json\nimport copy\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nimport model_average_optimizer\n \nflags = tf.flags\nflags.DEFINE_string(\"server_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"job_name\", \"\", \"Either 'server' of 'worker'\")\nflags.DEFINE_integer(\"task_id\", 0, \"Task Id for Each workers\")\n \nFLAGS = flags.FLAGS\ntf.logging.set_verbosity(tf.logging.INFO)\n \ndef workers_ps_creator(args):\n    ps_hosts = args.server_hosts.split(\",\")\n    worker_hosts = args.worker_hosts.split(\",\")\n    num_workers = len(worker_hosts)\n \n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts,\"worker\": worker_hosts})\n    gpu_options = tf.GPUOptions(allocator_type='BFC', allow_growth=True)\n    if args.job_name == \"server\":\n        server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\n                job_name='ps',\n                task_index=args.task_id,\n                default_session_config=tf.ConfigProto(gpu_options=gpu_options, device_count={\"GPU\":0}),\n                protocol=\"grpc\")\n    elif args.job_name == \"worker\":\n        server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\n                job_name=\"worker\",\n                task_index=args.task_id,\n                default_session_config = tf.ConfigProto(gpu_options=gpu_options),\n                protocol=\"grpc\")\n    server = tf.train.Server(server_def)\n    return server, cluster, num_workers, gpu_options\n \ndef Model(sync_opt):\n    if FLAGS.task_id == 0:\n        var_0 = tf.get_variable(initializer = 0.0, name = 'v0')\n        var_1 = tf.get_variable(initializer = 1.0, name = 'v1')\n        grads_0 = constant_op.constant(-1.0)\n        grads_1 = constant_op.constant(-1.0)\n    else:\n        var_0 = tf.get_variable(initializer = 7.0, name = 'v0')\n        var_1 = tf.get_variable(initializer = 8.0, name = 'v1')\n        grads_0 = constant_op.constant(-2.0)\n        grads_1 = constant_op.constant(-2.0)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = sync_opt.apply_gradients([[grads_0, var_0], [grads_1, var_1]],\n            global_step = tf.train.get_or_create_global_step())\n    return train_op\n \ndef test():\n    server, cluster, num_workers, gpu_options = workers_ps_creator(FLAGS)\n \n    if FLAGS.job_name == \"server\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        is_chief = (FLAGS.task_id == 0)\n        #Between-graph replication\n        worker_device = \"/job:worker/task:%d\" % (FLAGS.task_id)\n        with tf.device(model_average_optimizer.model_average_device_setter(\n                worker_device=worker_device, cluster=cluster)):\n            #create model\n            lr = tf.Variable(1, trainable=False)\n            opt = tf.train.GradientDescentOptimizer(lr)\n            train_model = Model(opt)\n            ma_opt = model_average_optimizer.ModelAverageOptimizer(num_workers, 3)\n\t\n            ## store local variables\n            collection_key = tf.GraphKeys.SAVERS\n            saver = tf.train.Saver(tf.local_variables(), save_relative_paths=True)\n            if saver is not None: tf.add_to_collection(collection_key, saver)\n\t\n        sess_config = tf.ConfigProto(gpu_options=gpu_options)\n        sess_config.log_device_placement = False\n        sess_config.allow_soft_placement = True\n\t\n        ma_hook = ma_opt.make_ma_run_hook()\n        ma_replica_hook = ma_opt.make_session_run_hook(is_chief)\n        all_hooks = [ma_replica_hook, ma_hook]\n\t\n        tf.logging.info('Start Sess')\n        with tf.train.MonitoredTrainingSession(master=server.target,\n                    is_chief=is_chief,\n                    hooks=all_hooks) as sess:\n            tf.logging.info(\"is chief: %s, len: %s\", is_chief, num_workers)\n            for i in range(4):\n                sess.run(train_op)\n                pp1 = sess.run(tf.get_default_graph().get_tensor_by_name('v0:0'))\n                pp2 = sess.run(tf.get_default_graph().get_tensor_by_name('v1:0'))\n                tf.logging.info(\"%d %.2f %.2f\" % (FLAGS.task_id, pp1, pp2))\n        sv.stop()\n        tf.logging.info(\"done\")\n\t\ndef main(_):\n    test()\n\t\nif __name__ == '__main__':\n    tf.app.run()", "body": "@ry @abenmao \r\nI still have problems with `model_average_model`\r\n\r\nI use `tensorflow_gpu==1.8.0` in `anaconda2.5`, `model_average_optimizer_test.py` can run easily, however some errors occured with my own code.\r\n\r\nAlso, I use `tensorflow.contrib.opt.python.training.model_average_optimizer`, other errors comes: [https://github.com/tensorflow/tensorflow/issues/19617](url)\r\n\r\nerrors are shown below.\r\n\r\n\tInvalidArgumentError (see above for traceback): Cannot colocate nodes 'modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0' and 'modelAverage/g/v1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'\r\n\t[[Node: modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0 = Switch[T=DT_FLOAT, _class=[\"loc:@modelAverage/g/v1\"], _device=\"/job:worker/task:1\"](modelAverage/nesterov/v1/cond_1/Merge, cond_3/pred_id)]]\r\n\r\nThe run commad is\r\n\r\n\t~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=0\r\n\t~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=1\r\n\t~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=server --task_id=0\r\n\r\nand code is \r\n\r\n    import os\r\n    import time\r\n     \r\n    import json\r\n    import copy\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.python.framework import constant_op\r\n    import model_average_optimizer\r\n     \r\n    flags = tf.flags\r\n    flags.DEFINE_string(\"server_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n    flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n    flags.DEFINE_string(\"job_name\", \"\", \"Either 'server' of 'worker'\")\r\n    flags.DEFINE_integer(\"task_id\", 0, \"Task Id for Each workers\")\r\n     \r\n    FLAGS = flags.FLAGS\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n     \r\n    def workers_ps_creator(args):\r\n        ps_hosts = args.server_hosts.split(\",\")\r\n        worker_hosts = args.worker_hosts.split(\",\")\r\n        num_workers = len(worker_hosts)\r\n     \r\n        cluster = tf.train.ClusterSpec({\"ps\": ps_hosts,\"worker\": worker_hosts})\r\n        gpu_options = tf.GPUOptions(allocator_type='BFC', allow_growth=True)\r\n        if args.job_name == \"server\":\r\n            server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\r\n                    job_name='ps',\r\n                    task_index=args.task_id,\r\n                    default_session_config=tf.ConfigProto(gpu_options=gpu_options, device_count={\"GPU\":0}),\r\n                    protocol=\"grpc\")\r\n        elif args.job_name == \"worker\":\r\n            server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\r\n                    job_name=\"worker\",\r\n                    task_index=args.task_id,\r\n                    default_session_config = tf.ConfigProto(gpu_options=gpu_options),\r\n                    protocol=\"grpc\")\r\n        server = tf.train.Server(server_def)\r\n        return server, cluster, num_workers, gpu_options\r\n     \r\n    def Model(sync_opt):\r\n        if FLAGS.task_id == 0:\r\n            var_0 = tf.get_variable(initializer = 0.0, name = 'v0')\r\n            var_1 = tf.get_variable(initializer = 1.0, name = 'v1')\r\n            grads_0 = constant_op.constant(-1.0)\r\n            grads_1 = constant_op.constant(-1.0)\r\n        else:\r\n            var_0 = tf.get_variable(initializer = 7.0, name = 'v0')\r\n            var_1 = tf.get_variable(initializer = 8.0, name = 'v1')\r\n            grads_0 = constant_op.constant(-2.0)\r\n            grads_1 = constant_op.constant(-2.0)\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(update_ops):\r\n            train_op = sync_opt.apply_gradients([[grads_0, var_0], [grads_1, var_1]],\r\n                global_step = tf.train.get_or_create_global_step())\r\n        return train_op\r\n     \r\n    def test():\r\n        server, cluster, num_workers, gpu_options = workers_ps_creator(FLAGS)\r\n     \r\n        if FLAGS.job_name == \"server\":\r\n            server.join()\r\n        elif FLAGS.job_name == \"worker\":\r\n            is_chief = (FLAGS.task_id == 0)\r\n            #Between-graph replication\r\n            worker_device = \"/job:worker/task:%d\" % (FLAGS.task_id)\r\n            with tf.device(model_average_optimizer.model_average_device_setter(\r\n                    worker_device=worker_device, cluster=cluster)):\r\n                #create model\r\n                lr = tf.Variable(1, trainable=False)\r\n                opt = tf.train.GradientDescentOptimizer(lr)\r\n                train_model = Model(opt)\r\n                ma_opt = model_average_optimizer.ModelAverageOptimizer(num_workers, 3)\r\n    \t\r\n                ## store local variables\r\n                collection_key = tf.GraphKeys.SAVERS\r\n                saver = tf.train.Saver(tf.local_variables(), save_relative_paths=True)\r\n                if saver is not None: tf.add_to_collection(collection_key, saver)\r\n    \t\r\n            sess_config = tf.ConfigProto(gpu_options=gpu_options)\r\n            sess_config.log_device_placement = False\r\n            sess_config.allow_soft_placement = True\r\n    \t\r\n            ma_hook = ma_opt.make_ma_run_hook()\r\n            ma_replica_hook = ma_opt.make_session_run_hook(is_chief)\r\n            all_hooks = [ma_replica_hook, ma_hook]\r\n    \t\r\n            tf.logging.info('Start Sess')\r\n            with tf.train.MonitoredTrainingSession(master=server.target,\r\n                        is_chief=is_chief,\r\n                        hooks=all_hooks) as sess:\r\n                tf.logging.info(\"is chief: %s, len: %s\", is_chief, num_workers)\r\n                for i in range(4):\r\n                    sess.run(train_op)\r\n                    pp1 = sess.run(tf.get_default_graph().get_tensor_by_name('v0:0'))\r\n                    pp2 = sess.run(tf.get_default_graph().get_tensor_by_name('v1:0'))\r\n                    tf.logging.info(\"%d %.2f %.2f\" % (FLAGS.task_id, pp1, pp2))\r\n            sv.stop()\r\n            tf.logging.info(\"done\")\r\n    \t\r\n    def main(_):\r\n        test()\r\n    \t\r\n    if __name__ == '__main__':\r\n        tf.app.run()\r\n"}