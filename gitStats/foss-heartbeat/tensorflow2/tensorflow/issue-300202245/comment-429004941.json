{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/429004941", "html_url": "https://github.com/tensorflow/tensorflow/issues/17272#issuecomment-429004941", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17272", "id": 429004941, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTAwNDk0MQ==", "user": {"login": "ricvo", "id": 9975354, "node_id": "MDQ6VXNlcjk5NzUzNTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9975354?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ricvo", "html_url": "https://github.com/ricvo", "followers_url": "https://api.github.com/users/ricvo/followers", "following_url": "https://api.github.com/users/ricvo/following{/other_user}", "gists_url": "https://api.github.com/users/ricvo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ricvo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ricvo/subscriptions", "organizations_url": "https://api.github.com/users/ricvo/orgs", "repos_url": "https://api.github.com/users/ricvo/repos", "events_url": "https://api.github.com/users/ricvo/events{/privacy}", "received_events_url": "https://api.github.com/users/ricvo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-11T15:41:37Z", "updated_at": "2018-10-11T15:43:11Z", "author_association": "NONE", "body_html": "<p>I am having the same issue, is it possible some workaround or to understand what this is due to?<br>\ntf version == 1.11.0</p>\n<p>I tried to use all three possible MonitoredSessions (that I know of..) and the result is always the same, that is why I think it is a problem of the CheckpointSaverHook<br>\nIt seems like more processes are writing on the event for saving the graph maybe.<br>\nI am not sure how many processes are launched under the hood, but only the chief should write the tf.summaries I think, isn't this the default behaviour or what is the suspected issue here?</p>\n<p>For all the 3 following cases the model is saved, and together with is the graph in a tb event I guess...<br>\nwhen I read the respective folder with tensorboard I obtain:</p>\n<pre><code>W1011 18:27:17.685001 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\nW1011 18:27:17.685001 140617855026944 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\nW1011 18:27:17.920676 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\nW1011 18:27:17.920676 140617855026944 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n</code></pre>\n<p>A simple code presenting the issue:</p>\n<pre><code>hooks = [\n    tf.train.StopAtStepHook(last_step=300)\n]\n\nsessioncode = args.session\nSAVE_STEPS = 50\n\nif sessioncode == 'mts':\n    print(\"I am using a MonitoredTrainingSession!!\")\n    basedir = \"tempMonitoredTrainingSession\"\n    checkpoint_dir = basedir\n \n    sess = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n                                            save_checkpoint_steps = SAVE_STEPS,\n                                            hooks=hooks,\n                                            config=sess_config)\n\nelif sessioncode == 'ms':\n    print(\"I am using a MonitoredSession!!\")\n    basedir = \"tempMonitoredSession\"\n    checkpoint_dir = basedir\n    hooks.append(tf.train.CheckpointSaverHook(checkpoint_dir, save_steps = SAVE_STEPS))\n    \n    chiefsess_creator = tf.train.ChiefSessionCreator(config=sess_config, checkpoint_dir=checkpoint_dir)\n    sess = tf.train.MonitoredSession(session_creator=chiefsess_creator, hooks=hooks)\n\nelif sessioncode == 'sms':\n    print(\"I am using a SingularMonitoredSession!!\")\n    basedir = \"tempSingularMonitoredSession\"\n    checkpoint_dir = basedir\n    hooks.append(tf.train.CheckpointSaverHook(checkpoint_dir, save_steps = SAVE_STEPS))\n    \n    sess = tf.train.SingularMonitoredSession(checkpoint_dir=checkpoint_dir,\n                                            hooks=hooks,\n                                            config=sess_config)\n\nelse:\n    raise ValueError(\"the session code passed is not contemplated!\")\n\n\nwith sess:\n    while not sess.should_stop():\n        _, g_step = sess.run([training_op, global_step])\n        print(\"global step: \", g_step)\n\n</code></pre>", "body_text": "I am having the same issue, is it possible some workaround or to understand what this is due to?\ntf version == 1.11.0\nI tried to use all three possible MonitoredSessions (that I know of..) and the result is always the same, that is why I think it is a problem of the CheckpointSaverHook\nIt seems like more processes are writing on the event for saving the graph maybe.\nI am not sure how many processes are launched under the hood, but only the chief should write the tf.summaries I think, isn't this the default behaviour or what is the suspected issue here?\nFor all the 3 following cases the model is saved, and together with is the graph in a tb event I guess...\nwhen I read the respective folder with tensorboard I obtain:\nW1011 18:27:17.685001 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\nW1011 18:27:17.685001 140617855026944 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\nW1011 18:27:17.920676 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\nW1011 18:27:17.920676 140617855026944 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n\nA simple code presenting the issue:\nhooks = [\n    tf.train.StopAtStepHook(last_step=300)\n]\n\nsessioncode = args.session\nSAVE_STEPS = 50\n\nif sessioncode == 'mts':\n    print(\"I am using a MonitoredTrainingSession!!\")\n    basedir = \"tempMonitoredTrainingSession\"\n    checkpoint_dir = basedir\n \n    sess = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n                                            save_checkpoint_steps = SAVE_STEPS,\n                                            hooks=hooks,\n                                            config=sess_config)\n\nelif sessioncode == 'ms':\n    print(\"I am using a MonitoredSession!!\")\n    basedir = \"tempMonitoredSession\"\n    checkpoint_dir = basedir\n    hooks.append(tf.train.CheckpointSaverHook(checkpoint_dir, save_steps = SAVE_STEPS))\n    \n    chiefsess_creator = tf.train.ChiefSessionCreator(config=sess_config, checkpoint_dir=checkpoint_dir)\n    sess = tf.train.MonitoredSession(session_creator=chiefsess_creator, hooks=hooks)\n\nelif sessioncode == 'sms':\n    print(\"I am using a SingularMonitoredSession!!\")\n    basedir = \"tempSingularMonitoredSession\"\n    checkpoint_dir = basedir\n    hooks.append(tf.train.CheckpointSaverHook(checkpoint_dir, save_steps = SAVE_STEPS))\n    \n    sess = tf.train.SingularMonitoredSession(checkpoint_dir=checkpoint_dir,\n                                            hooks=hooks,\n                                            config=sess_config)\n\nelse:\n    raise ValueError(\"the session code passed is not contemplated!\")\n\n\nwith sess:\n    while not sess.should_stop():\n        _, g_step = sess.run([training_op, global_step])\n        print(\"global step: \", g_step)", "body": "I am having the same issue, is it possible some workaround or to understand what this is due to?\r\ntf version == 1.11.0\r\n\r\nI tried to use all three possible MonitoredSessions (that I know of..) and the result is always the same, that is why I think it is a problem of the CheckpointSaverHook\r\nIt seems like more processes are writing on the event for saving the graph maybe.\r\nI am not sure how many processes are launched under the hood, but only the chief should write the tf.summaries I think, isn't this the default behaviour or what is the suspected issue here?\r\n\r\nFor all the 3 following cases the model is saved, and together with is the graph in a tb event I guess...\r\nwhen I read the respective folder with tensorboard I obtain:\r\n```\r\nW1011 18:27:17.685001 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nW1011 18:27:17.685001 140617855026944 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nW1011 18:27:17.920676 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nW1011 18:27:17.920676 140617855026944 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\n```\r\n\r\nA simple code presenting the issue:\r\n```\r\nhooks = [\r\n    tf.train.StopAtStepHook(last_step=300)\r\n]\r\n\r\nsessioncode = args.session\r\nSAVE_STEPS = 50\r\n\r\nif sessioncode == 'mts':\r\n    print(\"I am using a MonitoredTrainingSession!!\")\r\n    basedir = \"tempMonitoredTrainingSession\"\r\n    checkpoint_dir = basedir\r\n \r\n    sess = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\r\n                                            save_checkpoint_steps = SAVE_STEPS,\r\n                                            hooks=hooks,\r\n                                            config=sess_config)\r\n\r\nelif sessioncode == 'ms':\r\n    print(\"I am using a MonitoredSession!!\")\r\n    basedir = \"tempMonitoredSession\"\r\n    checkpoint_dir = basedir\r\n    hooks.append(tf.train.CheckpointSaverHook(checkpoint_dir, save_steps = SAVE_STEPS))\r\n    \r\n    chiefsess_creator = tf.train.ChiefSessionCreator(config=sess_config, checkpoint_dir=checkpoint_dir)\r\n    sess = tf.train.MonitoredSession(session_creator=chiefsess_creator, hooks=hooks)\r\n\r\nelif sessioncode == 'sms':\r\n    print(\"I am using a SingularMonitoredSession!!\")\r\n    basedir = \"tempSingularMonitoredSession\"\r\n    checkpoint_dir = basedir\r\n    hooks.append(tf.train.CheckpointSaverHook(checkpoint_dir, save_steps = SAVE_STEPS))\r\n    \r\n    sess = tf.train.SingularMonitoredSession(checkpoint_dir=checkpoint_dir,\r\n                                            hooks=hooks,\r\n                                            config=sess_config)\r\n\r\nelse:\r\n    raise ValueError(\"the session code passed is not contemplated!\")\r\n\r\n\r\nwith sess:\r\n    while not sess.should_stop():\r\n        _, g_step = sess.run([training_op, global_step])\r\n        print(\"global step: \", g_step)\r\n\r\n```"}