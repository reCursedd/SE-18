{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5350", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5350/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5350/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5350/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5350", "id": 186749207, "node_id": "MDU6SXNzdWUxODY3NDkyMDc=", "number": 5350, "title": "Sampled and regular softmax should use the same weight matrix shape", "user": {"login": "DavidNemeskey", "id": 690386, "node_id": "MDQ6VXNlcjY5MDM4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/690386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNemeskey", "html_url": "https://github.com/DavidNemeskey", "followers_url": "https://api.github.com/users/DavidNemeskey/followers", "following_url": "https://api.github.com/users/DavidNemeskey/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNemeskey/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNemeskey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNemeskey/subscriptions", "organizations_url": "https://api.github.com/users/DavidNemeskey/orgs", "repos_url": "https://api.github.com/users/DavidNemeskey/repos", "events_url": "https://api.github.com/users/DavidNemeskey/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNemeskey/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2016-11-02T09:24:43Z", "updated_at": "2018-02-08T00:12:52Z", "closed_at": "2018-02-08T00:12:52Z", "author_association": "NONE", "body_html": "<p>As it is now, the softmax samplers <a href=\"https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss\" rel=\"nofollow\">sampled_softmax_loss</a> and <a href=\"https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#nce_loss\" rel=\"nofollow\">nce_loss</a> require a <code>|C|x|H|</code> weight matrix, while the logits from <a href=\"https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#softmax\" rel=\"nofollow\">softmax</a> come from a <code>|H|x|C|</code>-sized one. This discrepancy is problematic on two levels:</p>\n<ol>\n<li>it makes for an inconsistent API;</li>\n<li>it results in a performance hit, because if one uses a sampled method for training and softmax for testing (as is usual when the number of classes is huge), one has to call <code>tf.transpose</code> somewhere, which does not work well with sparse input (see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"174407019\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4138\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4138/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4138\">#4138</a>).</li>\n</ol>\n<p>In my case, I can choose between 21150 / 18300 or 22350 / 11900 train / test wps, depending on whether the <code>tf.transpose</code> happens on the sampled softmax loss during training or on regular softmax during testing. In either case, the performance is suboptimal.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>As mentioned above, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"174407019\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4138\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4138/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4138\">#4138</a>. The fix there, however, is in the client code, not the API in question.</p>\n<h3>Environment info</h3>\n<p>Operating System: <code>Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u3 (2016-01-17) x86_64 GNU/Linux</code></p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n<code>/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44</code>; I don't have cuDNN at the moment</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed: the official GPU install for Python 3.4</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>: <code>0.11.0rc1</code></li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>I cannot do that right now, but one can experiment with e.g. the PTB example.</p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>N/A</p>", "body_text": "As it is now, the softmax samplers sampled_softmax_loss and nce_loss require a |C|x|H| weight matrix, while the logits from softmax come from a |H|x|C|-sized one. This discrepancy is problematic on two levels:\n\nit makes for an inconsistent API;\nit results in a performance hit, because if one uses a sampled method for training and softmax for testing (as is usual when the number of classes is huge), one has to call tf.transpose somewhere, which does not work well with sparse input (see #4138).\n\nIn my case, I can choose between 21150 / 18300 or 22350 / 11900 train / test wps, depending on whether the tf.transpose happens on the sampled softmax loss during training or on regular softmax during testing. In either case, the performance is suboptimal.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nAs mentioned above, #4138. The fix there, however, is in the client code, not the API in question.\nEnvironment info\nOperating System: Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u3 (2016-01-17) x86_64 GNU/Linux\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44; I don't have cuDNN at the moment\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: the official GPU install for Python 3.4\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\": 0.11.0rc1\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nI cannot do that right now, but one can experiment with e.g. the PTB example.\nWhat other attempted solutions have you tried?\nN/A", "body": "As it is now, the softmax samplers [sampled_softmax_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss) and [nce_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#nce_loss) require a `|C|x|H|` weight matrix, while the logits from [softmax](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#softmax) come from a `|H|x|C|`-sized one. This discrepancy is problematic on two levels:\r\n1. it makes for an inconsistent API;\r\n1. it results in a performance hit, because if one uses a sampled method for training and softmax for testing (as is usual when the number of classes is huge), one has to call `tf.transpose` somewhere, which does not work well with sparse input (see #4138).\r\n\r\nIn my case, I can choose between 21150 / 18300 or 22350 / 11900 train / test wps, depending on whether the `tf.transpose` happens on the sampled softmax loss during training or on regular softmax during testing. In either case, the performance is suboptimal.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nAs mentioned above, #4138. The fix there, however, is in the client code, not the API in question.\r\n\r\n### Environment info\r\nOperating System: `Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u3 (2016-01-17) x86_64 GNU/Linux`\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n`/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44`; I don't have cuDNN at the moment\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: the official GPU install for Python 3.4\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: `0.11.0rc1`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI cannot do that right now, but one can experiment with e.g. the PTB example.\r\n\r\n### What other attempted solutions have you tried?\r\nN/A"}