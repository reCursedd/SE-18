{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263283501", "html_url": "https://github.com/tensorflow/tensorflow/issues/5350#issuecomment-263283501", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5350", "id": 263283501, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzI4MzUwMQ==", "user": {"login": "vince62s", "id": 15141326, "node_id": "MDQ6VXNlcjE1MTQxMzI2", "avatar_url": "https://avatars3.githubusercontent.com/u/15141326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vince62s", "html_url": "https://github.com/vince62s", "followers_url": "https://api.github.com/users/vince62s/followers", "following_url": "https://api.github.com/users/vince62s/following{/other_user}", "gists_url": "https://api.github.com/users/vince62s/gists{/gist_id}", "starred_url": "https://api.github.com/users/vince62s/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vince62s/subscriptions", "organizations_url": "https://api.github.com/users/vince62s/orgs", "repos_url": "https://api.github.com/users/vince62s/repos", "events_url": "https://api.github.com/users/vince62s/events{/privacy}", "received_events_url": "https://api.github.com/users/vince62s/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-28T14:24:44Z", "updated_at": "2016-11-28T14:24:44Z", "author_association": "NONE", "body_html": "<p>Hey, I also encountered the issue  cf <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"191891573\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5890\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5890/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/5890\">#5890</a><br>\nI really think this is a major issue since once you decide to use sampled_softmax for training (to be quick), of course you need the full softmax to be quick too at inference time.<br>\nAny update on this at the API level ?<br>\nMany thanks.</p>", "body_text": "Hey, I also encountered the issue  cf #5890\nI really think this is a major issue since once you decide to use sampled_softmax for training (to be quick), of course you need the full softmax to be quick too at inference time.\nAny update on this at the API level ?\nMany thanks.", "body": "Hey, I also encountered the issue  cf https://github.com/tensorflow/tensorflow/issues/5890\r\nI really think this is a major issue since once you decide to use sampled_softmax for training (to be quick), of course you need the full softmax to be quick too at inference time.\r\nAny update on this at the API level ?\r\nMany thanks.\r\n"}