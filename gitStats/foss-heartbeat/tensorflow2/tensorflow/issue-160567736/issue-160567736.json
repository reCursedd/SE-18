{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2895", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2895/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2895/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2895/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2895", "id": 160567736, "node_id": "MDU6SXNzdWUxNjA1Njc3MzY=", "number": 2895, "title": "fatal issue with _reverse_seq in rnn.py", "user": {"login": "yanghoonkim", "id": 9985986, "node_id": "MDQ6VXNlcjk5ODU5ODY=", "avatar_url": "https://avatars2.githubusercontent.com/u/9985986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yanghoonkim", "html_url": "https://github.com/yanghoonkim", "followers_url": "https://api.github.com/users/yanghoonkim/followers", "following_url": "https://api.github.com/users/yanghoonkim/following{/other_user}", "gists_url": "https://api.github.com/users/yanghoonkim/gists{/gist_id}", "starred_url": "https://api.github.com/users/yanghoonkim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yanghoonkim/subscriptions", "organizations_url": "https://api.github.com/users/yanghoonkim/orgs", "repos_url": "https://api.github.com/users/yanghoonkim/repos", "events_url": "https://api.github.com/users/yanghoonkim/events{/privacy}", "received_events_url": "https://api.github.com/users/yanghoonkim/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-06-16T03:02:19Z", "updated_at": "2016-07-09T03:33:08Z", "closed_at": "2016-07-09T03:33:08Z", "author_association": "NONE", "body_html": "<p>I was trying to code bidirectional seq2seq on my own. I made a <code>cell with a embedding wrapper</code>,<br>\nfollowing is the code of _reverse_seq which is from tensorflow ver r0.7(r0.9 doesn't work either)</p>\n<div class=\"highlight highlight-source-js\"><pre>def <span class=\"pl-en\">_reverse_seq</span>(input_seq, lengths)<span class=\"pl-k\">:</span>\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-ii\">Reverse a list of Tensors up to specified lengths.</span></span>\n<span class=\"pl-s\"><span class=\"pl-ii\">  Args:</span></span>\n<span class=\"pl-s\"><span class=\"pl-ii\">    input_seq: Sequence of seq_len tensors of dimension (batch_size, depth)</span></span>\n<span class=\"pl-s\"><span class=\"pl-ii\">    lengths:   A tensor of dimension batch_size, containing lengths for each</span></span>\n<span class=\"pl-s\">               sequence in the batch. If <span class=\"pl-pds\">\"</span></span>None<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-ii\"> is specified, simply reverses</span></span>\n<span class=\"pl-s\"><span class=\"pl-ii\">               the list.</span></span>\n<span class=\"pl-s\"><span class=\"pl-ii\">  Returns:</span></span>\n<span class=\"pl-s\"><span class=\"pl-ii\">    time-reversed sequence</span></span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n  <span class=\"pl-k\">if</span> lengths is None<span class=\"pl-k\">:</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-en\">list</span>(<span class=\"pl-en\">reversed</span>(input_seq))\n\n  <span class=\"pl-k\">for</span> input_ <span class=\"pl-k\">in</span> input_seq<span class=\"pl-k\">:</span>\n    <span class=\"pl-smi\">input_</span>.<span class=\"pl-en\">set_shape</span>(<span class=\"pl-smi\">input_</span>.<span class=\"pl-en\">get_shape</span>().<span class=\"pl-en\">with_rank</span>(<span class=\"pl-c1\">2</span>))\n\n\n  # Join <span class=\"pl-en\">into</span> (time, batch_size, depth)\n  s_joined <span class=\"pl-k\">=</span> <span class=\"pl-smi\">array_ops</span>.<span class=\"pl-en\">pack</span>(input_seq)\n\n  # Reverse along dimension <span class=\"pl-c1\">0</span>\n  s_reversed <span class=\"pl-k\">=</span> <span class=\"pl-smi\">array_ops</span>.<span class=\"pl-en\">reverse_sequence</span>(s_joined, lengths, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n  # Split again into list\n  result <span class=\"pl-k\">=</span> <span class=\"pl-smi\">array_ops</span>.<span class=\"pl-en\">unpack</span>(s_reversed)\n  <span class=\"pl-k\">return</span> result</pre></div>\n<p>it requires input_seq which is <code>sequence of seq_len tensors of dimension (batch_size, depth)</code><br>\nHowever, pay attention to a part of <code>def bidirectional_rnn</code> :</p>\n<div class=\"highlight highlight-source-js\"><pre><span class=\"pl-k\">with</span> <span class=\"pl-smi\">vs</span>.<span class=\"pl-en\">variable_scope</span>(name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_FW<span class=\"pl-pds\">\"</span></span>)<span class=\"pl-k\">:</span>\n    output_fw, _ <span class=\"pl-k\">=</span> <span class=\"pl-en\">rnn</span>(cell_fw, inputs, initial_state_fw, dtype,\n                       None) # modified\n  #<span class=\"pl-en\">print</span> (inputs[<span class=\"pl-c1\">0</span>].<span class=\"pl-en\">get_shape</span>())\n  #<span class=\"pl-en\">print</span> (<span class=\"pl-smi\">sequence_length</span>.<span class=\"pl-en\">get_shape</span>())\n\n  # Backward direction\n  <span class=\"pl-k\">with</span> <span class=\"pl-smi\">vs</span>.<span class=\"pl-en\">variable_scope</span>(name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_BW<span class=\"pl-pds\">\"</span></span>)<span class=\"pl-k\">:</span>\n    tmp, _ <span class=\"pl-k\">=</span> <span class=\"pl-en\">rnn</span>(cell_bw, <span class=\"pl-en\">_reverse_seq</span>(inputs, sequence_length),\n                 initial_state_bw, dtype, None) #modified\n  output_bw <span class=\"pl-k\">=</span> <span class=\"pl-en\">_reverse_seq</span>(tmp, sequence_length)</pre></div>\n<p>just because I use a cell with a embedding wrapper, those inputs i feed are not embedded vector but one-hot encodded, and it has confliction with the requirement of _reverse_seq input.</p>", "body_text": "I was trying to code bidirectional seq2seq on my own. I made a cell with a embedding wrapper,\nfollowing is the code of _reverse_seq which is from tensorflow ver r0.7(r0.9 doesn't work either)\ndef _reverse_seq(input_seq, lengths):\n  \"\"\"Reverse a list of Tensors up to specified lengths.\n  Args:\n    input_seq: Sequence of seq_len tensors of dimension (batch_size, depth)\n    lengths:   A tensor of dimension batch_size, containing lengths for each\n               sequence in the batch. If \"None\" is specified, simply reverses\n               the list.\n  Returns:\n    time-reversed sequence\n  \"\"\"\n  if lengths is None:\n    return list(reversed(input_seq))\n\n  for input_ in input_seq:\n    input_.set_shape(input_.get_shape().with_rank(2))\n\n\n  # Join into (time, batch_size, depth)\n  s_joined = array_ops.pack(input_seq)\n\n  # Reverse along dimension 0\n  s_reversed = array_ops.reverse_sequence(s_joined, lengths, 0, 1)\n  # Split again into list\n  result = array_ops.unpack(s_reversed)\n  return result\nit requires input_seq which is sequence of seq_len tensors of dimension (batch_size, depth)\nHowever, pay attention to a part of def bidirectional_rnn :\nwith vs.variable_scope(name + \"_FW\"):\n    output_fw, _ = rnn(cell_fw, inputs, initial_state_fw, dtype,\n                       None) # modified\n  #print (inputs[0].get_shape())\n  #print (sequence_length.get_shape())\n\n  # Backward direction\n  with vs.variable_scope(name + \"_BW\"):\n    tmp, _ = rnn(cell_bw, _reverse_seq(inputs, sequence_length),\n                 initial_state_bw, dtype, None) #modified\n  output_bw = _reverse_seq(tmp, sequence_length)\njust because I use a cell with a embedding wrapper, those inputs i feed are not embedded vector but one-hot encodded, and it has confliction with the requirement of _reverse_seq input.", "body": "I was trying to code bidirectional seq2seq on my own. I made a `cell with a embedding wrapper`, \nfollowing is the code of _reverse_seq which is from tensorflow ver r0.7(r0.9 doesn't work either)\n\n``` javascript\ndef _reverse_seq(input_seq, lengths):\n  \"\"\"Reverse a list of Tensors up to specified lengths.\n  Args:\n    input_seq: Sequence of seq_len tensors of dimension (batch_size, depth)\n    lengths:   A tensor of dimension batch_size, containing lengths for each\n               sequence in the batch. If \"None\" is specified, simply reverses\n               the list.\n  Returns:\n    time-reversed sequence\n  \"\"\"\n  if lengths is None:\n    return list(reversed(input_seq))\n\n  for input_ in input_seq:\n    input_.set_shape(input_.get_shape().with_rank(2))\n\n\n  # Join into (time, batch_size, depth)\n  s_joined = array_ops.pack(input_seq)\n\n  # Reverse along dimension 0\n  s_reversed = array_ops.reverse_sequence(s_joined, lengths, 0, 1)\n  # Split again into list\n  result = array_ops.unpack(s_reversed)\n  return result\n```\n\nit requires input_seq which is `sequence of seq_len tensors of dimension (batch_size, depth)`\nHowever, pay attention to a part of `def bidirectional_rnn` : \n\n``` javascript\nwith vs.variable_scope(name + \"_FW\"):\n    output_fw, _ = rnn(cell_fw, inputs, initial_state_fw, dtype,\n                       None) # modified\n  #print (inputs[0].get_shape())\n  #print (sequence_length.get_shape())\n\n  # Backward direction\n  with vs.variable_scope(name + \"_BW\"):\n    tmp, _ = rnn(cell_bw, _reverse_seq(inputs, sequence_length),\n                 initial_state_bw, dtype, None) #modified\n  output_bw = _reverse_seq(tmp, sequence_length)\n```\n\njust because I use a cell with a embedding wrapper, those inputs i feed are not embedded vector but one-hot encodded, and it has confliction with the requirement of _reverse_seq input.\n"}