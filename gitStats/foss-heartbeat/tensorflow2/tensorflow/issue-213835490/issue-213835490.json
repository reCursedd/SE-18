{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8362", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8362/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8362/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8362/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8362", "id": 213835490, "node_id": "MDU6SXNzdWUyMTM4MzU0OTA=", "number": 8362, "title": "Memory Allocation at each call to Frozen Graph?", "user": {"login": "richfwebb", "id": 11443554, "node_id": "MDQ6VXNlcjExNDQzNTU0", "avatar_url": "https://avatars2.githubusercontent.com/u/11443554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/richfwebb", "html_url": "https://github.com/richfwebb", "followers_url": "https://api.github.com/users/richfwebb/followers", "following_url": "https://api.github.com/users/richfwebb/following{/other_user}", "gists_url": "https://api.github.com/users/richfwebb/gists{/gist_id}", "starred_url": "https://api.github.com/users/richfwebb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/richfwebb/subscriptions", "organizations_url": "https://api.github.com/users/richfwebb/orgs", "repos_url": "https://api.github.com/users/richfwebb/repos", "events_url": "https://api.github.com/users/richfwebb/events{/privacy}", "received_events_url": "https://api.github.com/users/richfwebb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-03-13T17:12:07Z", "updated_at": "2017-03-29T08:53:04Z", "closed_at": "2017-03-28T18:15:19Z", "author_association": "NONE", "body_html": "<p>Operating System: Linux, CPU<br>\nTF: 0.11.0</p>\n<p>It looks like a memory allocation is occurring during each call to my frozen graph.<br>\nI suspect I'm using the code incorrectly rather than this is a bug.<br>\nPlease delete if this does not belong here.<br>\nI posted on <a href=\"http://stackoverflow.com/questions/42769464/bad-memory-allocation-when-using-frozen-graph\" rel=\"nofollow\">StackOverflow</a>.</p>\n<hr>\n<p>My model was running too slow doing inference.<br>\nI froze the graph and converted variables to constants to improve the speed.</p>\n<p>Normally I load the graph and weights (which runs OK):</p>\n<pre><code>with tf.Graph().as_default(), tf.Session() as session:\n\t...\n\tsaver.restore(session, ckpt_path)\n\tfor i in range(5):\n\t    result = session.run(...)\n</code></pre>\n<p>However, when I load the frozen graph:</p>\n<pre><code># Save the graph\n\n# output_node_names - equivalent to the results that are requested in the session.run() calls\noutput_node_names = [...] \nsaver = tf.train.import_meta_graph(ckpt + '.meta')\ngraph = tf.get_default_graph()\ninput_graph_def = graph.as_graph_def()\n\nwith tf.Session() as sess:\n    saver.restore(sess, ckpt)\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess, input_graph_def, output_node_names) \n\n     with tf.gfile.GFile(output_graph, \"wb\") as f:\n         f.write(output_graph_def.SerializeToString())\n\n\n# Load the graph\n\ndef load_graph(filename):\n\twith tf.gfile.GFile(filename, \"rb\") as f:\n\t       graph_def = tf.GraphDef()\n\t       graph_def.ParseFromString(f.read())\n\n\t with tf.Graph().as_default() as graph:\n\t        tf.import_graph_def(graph_def, input_map=None, \n\t        \treturn_elements=None, name=\"prefix\", op_dict=None, \n                        producer_op_list=None)\n\treturn graph \n\ngraph = load_graph(\"model.pb\")\nsession = tf.Session(graph=graph)\n\t\nfor i in range(5):\n\tresult = session.run(...)\n</code></pre>\n<p>After loading the full graph, inference takes ~1 second.<br>\nAfter loading the frozen graph, inference takes ~4 seconds.<br>\nSo it seems like something may not be loaded entirely in my frozen graph?<br>\nThe first call to session.run() successfully produces a result (although very slow), but <strong>the second call to run() throws an error</strong>.</p>\n<p><strong><code>W tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[300,100000]</code></strong></p>\n<p>Since the graph internally has an embedding table (this is not what is being returned by session.run()), it seems this is what is being re-allocated:<br>\n<code>embedding = tf.get_variable(\"embedding\", [300, 100000])</code></p>\n<p>Also the error message printed 3 times, hopefully that means 3 allocation attempts, and not 3 instances being allocated?<br>\nBut why is this allocation on every call to session.run()?</p>\n<p>Edit:<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17184992\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/MicaelCarvalho\">@MicaelCarvalho</a><br>\nHere is the format of my input for Session.run():</p>\n<pre><code>x = [0.0]\nx_placeholder = graph.get_tensor_by_name('prefix/x_placeholder:0')\ny = graph.get_tensor_by_name('prefix/y:0')\nfeed_dict = {x_placeholder: x}\nto_return = [y]\nresult = sess.run(to_return, feed_dict=feed_dict)\n</code></pre>", "body_text": "Operating System: Linux, CPU\nTF: 0.11.0\nIt looks like a memory allocation is occurring during each call to my frozen graph.\nI suspect I'm using the code incorrectly rather than this is a bug.\nPlease delete if this does not belong here.\nI posted on StackOverflow.\n\nMy model was running too slow doing inference.\nI froze the graph and converted variables to constants to improve the speed.\nNormally I load the graph and weights (which runs OK):\nwith tf.Graph().as_default(), tf.Session() as session:\n\t...\n\tsaver.restore(session, ckpt_path)\n\tfor i in range(5):\n\t    result = session.run(...)\n\nHowever, when I load the frozen graph:\n# Save the graph\n\n# output_node_names - equivalent to the results that are requested in the session.run() calls\noutput_node_names = [...] \nsaver = tf.train.import_meta_graph(ckpt + '.meta')\ngraph = tf.get_default_graph()\ninput_graph_def = graph.as_graph_def()\n\nwith tf.Session() as sess:\n    saver.restore(sess, ckpt)\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess, input_graph_def, output_node_names) \n\n     with tf.gfile.GFile(output_graph, \"wb\") as f:\n         f.write(output_graph_def.SerializeToString())\n\n\n# Load the graph\n\ndef load_graph(filename):\n\twith tf.gfile.GFile(filename, \"rb\") as f:\n\t       graph_def = tf.GraphDef()\n\t       graph_def.ParseFromString(f.read())\n\n\t with tf.Graph().as_default() as graph:\n\t        tf.import_graph_def(graph_def, input_map=None, \n\t        \treturn_elements=None, name=\"prefix\", op_dict=None, \n                        producer_op_list=None)\n\treturn graph \n\ngraph = load_graph(\"model.pb\")\nsession = tf.Session(graph=graph)\n\t\nfor i in range(5):\n\tresult = session.run(...)\n\nAfter loading the full graph, inference takes ~1 second.\nAfter loading the frozen graph, inference takes ~4 seconds.\nSo it seems like something may not be loaded entirely in my frozen graph?\nThe first call to session.run() successfully produces a result (although very slow), but the second call to run() throws an error.\nW tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[300,100000]\nSince the graph internally has an embedding table (this is not what is being returned by session.run()), it seems this is what is being re-allocated:\nembedding = tf.get_variable(\"embedding\", [300, 100000])\nAlso the error message printed 3 times, hopefully that means 3 allocation attempts, and not 3 instances being allocated?\nBut why is this allocation on every call to session.run()?\nEdit:\n@MicaelCarvalho\nHere is the format of my input for Session.run():\nx = [0.0]\nx_placeholder = graph.get_tensor_by_name('prefix/x_placeholder:0')\ny = graph.get_tensor_by_name('prefix/y:0')\nfeed_dict = {x_placeholder: x}\nto_return = [y]\nresult = sess.run(to_return, feed_dict=feed_dict)", "body": "Operating System: Linux, CPU\r\nTF: 0.11.0\r\n\r\nIt looks like a memory allocation is occurring during each call to my frozen graph.\r\nI suspect I'm using the code incorrectly rather than this is a bug.\r\nPlease delete if this does not belong here. \r\nI posted on [StackOverflow](http://stackoverflow.com/questions/42769464/bad-memory-allocation-when-using-frozen-graph).\r\n\r\n-------\r\n\r\nMy model was running too slow doing inference.\r\nI froze the graph and converted variables to constants to improve the speed. \r\n\r\nNormally I load the graph and weights (which runs OK):\r\n```\r\nwith tf.Graph().as_default(), tf.Session() as session:\r\n\t...\r\n\tsaver.restore(session, ckpt_path)\r\n\tfor i in range(5):\r\n\t    result = session.run(...)\r\n```\r\n\r\nHowever, when I load the frozen graph:\t\r\n```\r\n# Save the graph\r\n\r\n# output_node_names - equivalent to the results that are requested in the session.run() calls\r\noutput_node_names = [...] \r\nsaver = tf.train.import_meta_graph(ckpt + '.meta')\r\ngraph = tf.get_default_graph()\r\ninput_graph_def = graph.as_graph_def()\r\n\r\nwith tf.Session() as sess:\r\n    saver.restore(sess, ckpt)\r\n    output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess, input_graph_def, output_node_names) \r\n\r\n     with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n         f.write(output_graph_def.SerializeToString())\r\n\r\n\r\n# Load the graph\r\n\r\ndef load_graph(filename):\r\n\twith tf.gfile.GFile(filename, \"rb\") as f:\r\n\t       graph_def = tf.GraphDef()\r\n\t       graph_def.ParseFromString(f.read())\r\n\r\n\t with tf.Graph().as_default() as graph:\r\n\t        tf.import_graph_def(graph_def, input_map=None, \r\n\t        \treturn_elements=None, name=\"prefix\", op_dict=None, \r\n                        producer_op_list=None)\r\n\treturn graph \r\n\r\ngraph = load_graph(\"model.pb\")\r\nsession = tf.Session(graph=graph)\r\n\t\r\nfor i in range(5):\r\n\tresult = session.run(...)\r\n```\r\n\r\nAfter loading the full graph, inference takes ~1 second.\r\nAfter loading the frozen graph, inference takes ~4 seconds.\r\nSo it seems like something may not be loaded entirely in my frozen graph?\r\nThe first call to session.run() successfully produces a result (although very slow), but **the second call to run() throws an error**.\r\n\r\n**`W tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[300,100000]`**\r\n\r\nSince the graph internally has an embedding table (this is not what is being returned by session.run()), it seems this is what is being re-allocated:\r\n\t```embedding = tf.get_variable(\"embedding\", [300, 100000])```\r\n\r\nAlso the error message printed 3 times, hopefully that means 3 allocation attempts, and not 3 instances being allocated? \r\nBut why is this allocation on every call to session.run()?\r\n\r\n\r\nEdit:\r\n@MicaelCarvalho \r\nHere is the format of my input for Session.run():\r\n```\r\nx = [0.0]\r\nx_placeholder = graph.get_tensor_by_name('prefix/x_placeholder:0')\r\ny = graph.get_tensor_by_name('prefix/y:0')\r\nfeed_dict = {x_placeholder: x}\r\nto_return = [y]\r\nresult = sess.run(to_return, feed_dict=feed_dict)\r\n```\r\n\r\n"}