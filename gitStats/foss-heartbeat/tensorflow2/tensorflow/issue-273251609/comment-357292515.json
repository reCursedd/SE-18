{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357292515", "html_url": "https://github.com/tensorflow/tensorflow/issues/14502#issuecomment-357292515", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14502", "id": 357292515, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzI5MjUxNQ==", "user": {"login": "xiejw", "id": 1184671, "node_id": "MDQ6VXNlcjExODQ2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1184671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiejw", "html_url": "https://github.com/xiejw", "followers_url": "https://api.github.com/users/xiejw/followers", "following_url": "https://api.github.com/users/xiejw/following{/other_user}", "gists_url": "https://api.github.com/users/xiejw/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiejw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiejw/subscriptions", "organizations_url": "https://api.github.com/users/xiejw/orgs", "repos_url": "https://api.github.com/users/xiejw/repos", "events_url": "https://api.github.com/users/xiejw/events{/privacy}", "received_events_url": "https://api.github.com/users/xiejw/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T16:51:25Z", "updated_at": "2018-01-12T16:51:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi yaroslavvb, thanks for tracking this.</p>\n<p>As the TF_CONFIG contains all the information required to populate the RunConfig properties, we need to be careful about how to support this. For example, the num_worker_replicas is calculated based on the number of the training jobs inside the TF_CONFIG. If we allow sparse cluster spec directly at environment variable TF_CONFIG level, num_workers_replicas will be filled with wrong value. User will be very hard to automatically figure out how to configure the SyncReplicaOptimizer, which replies on the number of the worker in the system.</p>\n<p>One alternative option could be: We keep TF_CONFIG as is, but use an opt-in approach to turn on the sparse cluster spec when bring up the tf.train.Server. To be precise, at Estimator level, we still see the global information for the whole cluster, but when launching the tf.train.Server to connect to each other, we take the advantage of the sparse cluster spec.</p>\n<p>What's your thoughts on this? Are you willing to contribute here?</p>\n<p>If yes, we could work on a plan how to change/test here.</p>", "body_text": "Hi yaroslavvb, thanks for tracking this.\nAs the TF_CONFIG contains all the information required to populate the RunConfig properties, we need to be careful about how to support this. For example, the num_worker_replicas is calculated based on the number of the training jobs inside the TF_CONFIG. If we allow sparse cluster spec directly at environment variable TF_CONFIG level, num_workers_replicas will be filled with wrong value. User will be very hard to automatically figure out how to configure the SyncReplicaOptimizer, which replies on the number of the worker in the system.\nOne alternative option could be: We keep TF_CONFIG as is, but use an opt-in approach to turn on the sparse cluster spec when bring up the tf.train.Server. To be precise, at Estimator level, we still see the global information for the whole cluster, but when launching the tf.train.Server to connect to each other, we take the advantage of the sparse cluster spec.\nWhat's your thoughts on this? Are you willing to contribute here?\nIf yes, we could work on a plan how to change/test here.", "body": "Hi yaroslavvb, thanks for tracking this. \r\n\r\nAs the TF_CONFIG contains all the information required to populate the RunConfig properties, we need to be careful about how to support this. For example, the num_worker_replicas is calculated based on the number of the training jobs inside the TF_CONFIG. If we allow sparse cluster spec directly at environment variable TF_CONFIG level, num_workers_replicas will be filled with wrong value. User will be very hard to automatically figure out how to configure the SyncReplicaOptimizer, which replies on the number of the worker in the system. \r\n\r\nOne alternative option could be: We keep TF_CONFIG as is, but use an opt-in approach to turn on the sparse cluster spec when bring up the tf.train.Server. To be precise, at Estimator level, we still see the global information for the whole cluster, but when launching the tf.train.Server to connect to each other, we take the advantage of the sparse cluster spec. \r\n\r\nWhat's your thoughts on this? Are you willing to contribute here? \r\n\r\nIf yes, we could work on a plan how to change/test here. \r\n\r\n\r\n\r\n\r\n\r\n\r\n"}