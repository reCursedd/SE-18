{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281872138", "html_url": "https://github.com/tensorflow/tensorflow/issues/7782#issuecomment-281872138", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7782", "id": 281872138, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTg3MjEzOA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-23T02:23:58Z", "updated_at": "2017-02-23T02:23:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is a subtle corner of the <code>tf.Variable</code> semantics, which has tripped up a few people. The main thing to note is that when you first <em>read</em> a <code>tf.Variable</code>\u2014in this case, when it is used as part of the argument to <code>tf.concat()</code>\u2014the value read is \"cached\".</p>\n<p>What does it mean for a value to be \"cached\"? In the code, it's fed to a <code>tf.identity()</code>, which implicitly dereferences the ref-typed variable tensor, but then (perhaps surprisingly) returns a value-typed tensor that <em>aliases</em> the buffer used for the variable. This behavior was chosen for distributed (or multi-device) execution, where the aliasing isn't usually noticeable because the reader is typically on a remote device, and the buffer will be copied between devices anyway.</p>\n<p>However, when you assign a tensor of a different shape to a <code>tf.Variable</code>, the snapshot and current variable value can no longer be aliases, because they're buffers of a different size.<br>\n(Aside: If you'd done <code>tf.assign_add(x, x + 1)</code> (or something else that preserved the shape of <code>x</code>) you would see things happen in the order you expected, because everything happens on the same device, and the \"snapshot\" remains an alias of the underlying buffer.) The <code>tf.Print()</code> op gets the old snapshot value, and prints that.</p>\n<p>How can you avoid this? One way is to force an explicit <code>x.read_value()</code>, which forces a new snapshot to be taken, respecting the control dependencies. Changing your program as follows will give the expected output:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> I define a \"shape-able\" Variable                                                                                                                             </span>\nx <span class=\"pl-k\">=</span> tf.Variable([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32, <span class=\"pl-v\">validate_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> I build a new shape and assign it to x                                                                                                                       </span>\nconcat <span class=\"pl-k\">=</span> tf.concat([x, [<span class=\"pl-c1\">0</span>]], <span class=\"pl-c1\">0</span>)\nassign_op <span class=\"pl-k\">=</span> tf.assign(x, concat, <span class=\"pl-v\">validate_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-k\">with</span> tf.control_dependencies([assign_op]):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> I print x after the assignment                                                                                                                         </span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Note that the Print call is on \"x\" and NOT \"assign_op\"                                                                                                 </span>\n  new_x <span class=\"pl-k\">=</span> x.read_value()\n  print_op_dep <span class=\"pl-k\">=</span> tf.Print(new_x, <span class=\"pl-v\">data</span><span class=\"pl-k\">=</span>[new_x], <span class=\"pl-v\">message</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>print_op_dep:<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n  sess.run(tf.global_variables_initializer())\n  <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>):\n    sess.run(print_op_dep)</pre></div>\n<p>The output is:</p>\n<pre><code>I tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0 0]\n</code></pre>", "body_text": "This is a subtle corner of the tf.Variable semantics, which has tripped up a few people. The main thing to note is that when you first read a tf.Variable\u2014in this case, when it is used as part of the argument to tf.concat()\u2014the value read is \"cached\".\nWhat does it mean for a value to be \"cached\"? In the code, it's fed to a tf.identity(), which implicitly dereferences the ref-typed variable tensor, but then (perhaps surprisingly) returns a value-typed tensor that aliases the buffer used for the variable. This behavior was chosen for distributed (or multi-device) execution, where the aliasing isn't usually noticeable because the reader is typically on a remote device, and the buffer will be copied between devices anyway.\nHowever, when you assign a tensor of a different shape to a tf.Variable, the snapshot and current variable value can no longer be aliases, because they're buffers of a different size.\n(Aside: If you'd done tf.assign_add(x, x + 1) (or something else that preserved the shape of x) you would see things happen in the order you expected, because everything happens on the same device, and the \"snapshot\" remains an alias of the underlying buffer.) The tf.Print() op gets the old snapshot value, and prints that.\nHow can you avoid this? One way is to force an explicit x.read_value(), which forces a new snapshot to be taken, respecting the control dependencies. Changing your program as follows will give the expected output:\nimport tensorflow as tf\n\n# I define a \"shape-able\" Variable                                                                                                                             \nx = tf.Variable([], dtype=tf.int32, validate_shape=False, trainable=False)\n# I build a new shape and assign it to x                                                                                                                       \nconcat = tf.concat([x, [0]], 0)\nassign_op = tf.assign(x, concat, validate_shape=False)\n\nwith tf.control_dependencies([assign_op]):\n  # I print x after the assignment                                                                                                                         \n  # Note that the Print call is on \"x\" and NOT \"assign_op\"                                                                                                 \n  new_x = x.read_value()\n  print_op_dep = tf.Print(new_x, data=[new_x], message=\"print_op_dep:\")\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  for i in range(3):\n    sess.run(print_op_dep)\nThe output is:\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0 0]", "body": "This is a subtle corner of the `tf.Variable` semantics, which has tripped up a few people. The main thing to note is that when you first *read* a `tf.Variable`\u2014in this case, when it is used as part of the argument to `tf.concat()`\u2014the value read is \"cached\".\r\n\r\nWhat does it mean for a value to be \"cached\"? In the code, it's fed to a `tf.identity()`, which implicitly dereferences the ref-typed variable tensor, but then (perhaps surprisingly) returns a value-typed tensor that *aliases* the buffer used for the variable. This behavior was chosen for distributed (or multi-device) execution, where the aliasing isn't usually noticeable because the reader is typically on a remote device, and the buffer will be copied between devices anyway.\r\n\r\nHowever, when you assign a tensor of a different shape to a `tf.Variable`, the snapshot and current variable value can no longer be aliases, because they're buffers of a different size. \r\n(Aside: If you'd done `tf.assign_add(x, x + 1)` (or something else that preserved the shape of `x`) you would see things happen in the order you expected, because everything happens on the same device, and the \"snapshot\" remains an alias of the underlying buffer.) The `tf.Print()` op gets the old snapshot value, and prints that.\r\n\r\nHow can you avoid this? One way is to force an explicit `x.read_value()`, which forces a new snapshot to be taken, respecting the control dependencies. Changing your program as follows will give the expected output:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# I define a \"shape-able\" Variable                                                                                                                             \r\nx = tf.Variable([], dtype=tf.int32, validate_shape=False, trainable=False)\r\n# I build a new shape and assign it to x                                                                                                                       \r\nconcat = tf.concat([x, [0]], 0)\r\nassign_op = tf.assign(x, concat, validate_shape=False)\r\n\r\nwith tf.control_dependencies([assign_op]):\r\n  # I print x after the assignment                                                                                                                         \r\n  # Note that the Print call is on \"x\" and NOT \"assign_op\"                                                                                                 \r\n  new_x = x.read_value()\r\n  print_op_dep = tf.Print(new_x, data=[new_x], message=\"print_op_dep:\")\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for i in range(3):\r\n    sess.run(print_op_dep)\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] print_op_dep:[0 0 0]\r\n```\r\n"}