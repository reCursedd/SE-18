{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282063965", "html_url": "https://github.com/tensorflow/tensorflow/issues/7782#issuecomment-282063965", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7782", "id": 282063965, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MjA2Mzk2NQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-23T17:37:44Z", "updated_at": "2017-02-23T17:37:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Glad to hear it!</p>\n<p>As for the performance impact, it's hard to say. At the level of individual assign calls, TensorFlow doesn't do much to optimize your code, so you aren't necessarily missing any optimizations. Concatenating and copying like you do in that code snippet will have quadratic time complexity, but I'm not sure if you're going to be doing that in such a tight loop that it matters :). (If you find yourself concatenating dynamic lists of tensors a lot, you might be interested in <code>tf.TensorArray</code> instead... it was introduced in part to avoid doing quadratic concatenation when accumulating loop state in a <code>tf.while_loop()</code>.)</p>\n<p>It is possible that having varying-shape variables will lead to e.g. more unknowns in shape inference, which could inhibit some nice optimizations that are possible when the shape of a tensor is static, but I assume you have a reason for wanting to change the shape of a variable, so some amount of dynamism is probably necessary.</p>", "body_text": "Glad to hear it!\nAs for the performance impact, it's hard to say. At the level of individual assign calls, TensorFlow doesn't do much to optimize your code, so you aren't necessarily missing any optimizations. Concatenating and copying like you do in that code snippet will have quadratic time complexity, but I'm not sure if you're going to be doing that in such a tight loop that it matters :). (If you find yourself concatenating dynamic lists of tensors a lot, you might be interested in tf.TensorArray instead... it was introduced in part to avoid doing quadratic concatenation when accumulating loop state in a tf.while_loop().)\nIt is possible that having varying-shape variables will lead to e.g. more unknowns in shape inference, which could inhibit some nice optimizations that are possible when the shape of a tensor is static, but I assume you have a reason for wanting to change the shape of a variable, so some amount of dynamism is probably necessary.", "body": "Glad to hear it!\r\n\r\nAs for the performance impact, it's hard to say. At the level of individual assign calls, TensorFlow doesn't do much to optimize your code, so you aren't necessarily missing any optimizations. Concatenating and copying like you do in that code snippet will have quadratic time complexity, but I'm not sure if you're going to be doing that in such a tight loop that it matters :). (If you find yourself concatenating dynamic lists of tensors a lot, you might be interested in `tf.TensorArray` instead... it was introduced in part to avoid doing quadratic concatenation when accumulating loop state in a `tf.while_loop()`.)\r\n\r\nIt is possible that having varying-shape variables will lead to e.g. more unknowns in shape inference, which could inhibit some nice optimizations that are possible when the shape of a tensor is static, but I assume you have a reason for wanting to change the shape of a variable, so some amount of dynamism is probably necessary."}