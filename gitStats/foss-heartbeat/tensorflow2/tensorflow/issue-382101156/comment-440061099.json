{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/440061099", "html_url": "https://github.com/tensorflow/tensorflow/issues/23846#issuecomment-440061099", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23846", "id": 440061099, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDA2MTA5OQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-19T22:14:42Z", "updated_at": "2018-11-19T22:14:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A few questions, since we don't have a way to reproduce your job:</p>\n<ul>\n<li>\n<p>What Python line of code is executing when the process freezes?</p>\n</li>\n<li>\n<p>The <code>inter_op_parallelism_threads</code> and <code>intra_op_parallelism_threads</code> configuration is not having an effect, because you're using a <code>tf.train.Server</code> (which sets those values once for all sessions on the server). Try the following change to server creation:</p>\n<div class=\"highlight highlight-source-python\"><pre>server_config <span class=\"pl-k\">=</span> tf.ConfigProto(<span class=\"pl-v\">inter_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                               <span class=\"pl-v\">intra_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">self</span>.server <span class=\"pl-k\">=</span> tf.train.Server(<span class=\"pl-c1\">self</span>.cluster, <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.job_name,\n                              <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.task_index, <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>server_config)\n</pre></div>\n<p>Can you generate new stack traces with this configuration? (This will cut down on the number of irrelevant threads in the trace.)</p>\n</li>\n<li>\n<p>From the stack traces, all we can see is that the process is blocked waiting for a response from some remote worker. It doesn't appear to be doing anything else. Can you gather a stack trace from the worker(s) in <code>/job:ps</code> that it might be using? If you can reproduce this using a single PS task, it will make it easier to debug.</p>\n</li>\n<li>\n<p>Is the call to <code>self.data_iter()</code> inside a <code>with tf.device(self.param_server_device):</code> block intentional? Usually, each worker would create an iterator on the local CPU device. If you do that instead, does it still hang? (One possibility is that the single PS task that is responsible for all 50 workers' input pipelines is massively oversubscribed, and it starts paging or dropping packets. However, that should not cause an indefinite hang, just very bad performance.)</p>\n</li>\n</ul>", "body_text": "A few questions, since we don't have a way to reproduce your job:\n\n\nWhat Python line of code is executing when the process freezes?\n\n\nThe inter_op_parallelism_threads and intra_op_parallelism_threads configuration is not having an effect, because you're using a tf.train.Server (which sets those values once for all sessions on the server). Try the following change to server creation:\nserver_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n                               intra_op_parallelism_threads=1)\nself.server = tf.train.Server(self.cluster, job_name=self.job_name,\n                              task_index=self.task_index, config=server_config)\n\nCan you generate new stack traces with this configuration? (This will cut down on the number of irrelevant threads in the trace.)\n\n\nFrom the stack traces, all we can see is that the process is blocked waiting for a response from some remote worker. It doesn't appear to be doing anything else. Can you gather a stack trace from the worker(s) in /job:ps that it might be using? If you can reproduce this using a single PS task, it will make it easier to debug.\n\n\nIs the call to self.data_iter() inside a with tf.device(self.param_server_device): block intentional? Usually, each worker would create an iterator on the local CPU device. If you do that instead, does it still hang? (One possibility is that the single PS task that is responsible for all 50 workers' input pipelines is massively oversubscribed, and it starts paging or dropping packets. However, that should not cause an indefinite hang, just very bad performance.)", "body": "A few questions, since we don't have a way to reproduce your job:\r\n\r\n* What Python line of code is executing when the process freezes?\r\n* The `inter_op_parallelism_threads` and `intra_op_parallelism_threads` configuration is not having an effect, because you're using a `tf.train.Server` (which sets those values once for all sessions on the server). Try the following change to server creation:\r\n\r\n    ```python\r\n    server_config = tf.ConfigProto(inter_op_parallelism_threads=1,\r\n                                   intra_op_parallelism_threads=1)\r\n    self.server = tf.train.Server(self.cluster, job_name=self.job_name,\r\n                                  task_index=self.task_index, config=server_config)\r\n\r\n    ```\r\n\r\n  Can you generate new stack traces with this configuration? (This will cut down on the number of irrelevant threads in the trace.)\r\n\r\n* From the stack traces, all we can see is that the process is blocked waiting for a response from some remote worker. It doesn't appear to be doing anything else. Can you gather a stack trace from the worker(s) in `/job:ps` that it might be using? If you can reproduce this using a single PS task, it will make it easier to debug.\r\n* Is the call to `self.data_iter()` inside a `with tf.device(self.param_server_device):` block intentional? Usually, each worker would create an iterator on the local CPU device. If you do that instead, does it still hang? (One possibility is that the single PS task that is responsible for all 50 workers' input pipelines is massively oversubscribed, and it starts paging or dropping packets. However, that should not cause an indefinite hang, just very bad performance.)\r\n"}