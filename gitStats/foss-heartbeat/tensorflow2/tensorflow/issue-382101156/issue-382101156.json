{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23846", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23846/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23846/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23846/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23846", "id": 382101156, "node_id": "MDU6SXNzdWUzODIxMDExNTY=", "number": 23846, "title": "Distributed TensorFlow hangs at the end of one iteration when using DataSet api", "user": {"login": "formath", "id": 6040127, "node_id": "MDQ6VXNlcjYwNDAxMjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/6040127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/formath", "html_url": "https://github.com/formath", "followers_url": "https://api.github.com/users/formath/followers", "following_url": "https://api.github.com/users/formath/following{/other_user}", "gists_url": "https://api.github.com/users/formath/gists{/gist_id}", "starred_url": "https://api.github.com/users/formath/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/formath/subscriptions", "organizations_url": "https://api.github.com/users/formath/orgs", "repos_url": "https://api.github.com/users/formath/repos", "events_url": "https://api.github.com/users/formath/events{/privacy}", "received_events_url": "https://api.github.com/users/formath/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 996845227, "node_id": "MDU6TGFiZWw5OTY4NDUyMjc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:dist-strat", "name": "comp:dist-strat", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-11-19T08:16:56Z", "updated_at": "2018-11-23T01:23:31Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Training epoch by epoch, the program will hangs on some random nodes forever at the end of one iteration, namely the end of one epoch. It appears certainly when using <code>DataSet</code> api, while it is fine when using old <code>Queue</code> based data api. I have set <code>inter_op_parallelism_threads=1, intra_op_parallelism_threads=1</code>. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a></p>\n<p>Env:<br>\nDistributed running on Yarn.<br>\nNode information: CentOS, linux kernel 3.10.0. Only CPU.<br>\nTensorFlow: r1.12, built from source.</p>\n<p>Demo(It's independent of the model. It's easier to reproduce by using more worker nodes. Here, I use 50 worker nodes and 20 ps nodes.)</p>\n<pre><code>flags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\nflags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS(sys.argv)\n\nclass Job(object):\n    def __init__(self):\n        self.ps_hosts = FLAGS.ps_hosts.split(',')\n        self.worker_hosts = FLAGS.worker_hosts.split(',')\n        self.job_name = FLAGS.job_name\n        self.task_index = FLAGS.task_index\n        self.cluster = tf.train.ClusterSpec({'ps': self.ps_hosts, 'worker': self.worker_hosts})\n        self.server = tf.train.Server(self.cluster, job_name=self.job_name, task_index=self.task_index)\n        self.is_chief = (self.task_index == 0 and self.job_name == 'worker')\n        worker_prefix = '/job:worker/task:%s' % self.task_index\n        self.cpu_device = '%s/cpu:0' % worker_prefix\n        self.param_server_device = tf.train.replica_device_setter(\n            worker_device=self.cpu_device, cluster=self.cluster,\n            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(len(self.ps_hosts), tf.contrib.training.byte_size_load_fn))\n        self.num_ps = self.cluster.num_tasks('ps')\n        self.num_worker = self.cluster.num_tasks('worker')\n\n    def data_iter(self, batch_size=1000, file_pattern='./input/part-*'):\n        def _parse_function(examples):\n            features = {}\n            features['label'] = tf.FixedLenFeature([], tf.float32)\n            features['user_id'] = tf.FixedLenFeature([1], tf.int64)\n            features['item_id'] = tf.FixedLenFeature([1], tf.int64)\n            instance = tf.parse_example(examples, features)\n            return instance['label'], instance['user_id'], instance['item_id']\n\n        with tf.name_scope('input'):\n            files = tf.data.Dataset.list_files(file_pattern)\n            dataset = files.apply(tf.contrib.data.parallel_interleave(\n                        lambda file: tf.data.TFRecordDataset(file),\n                        cycle_length=1, sloppy=True))\n            dataset = dataset.prefetch(buffer_size=batch_size*2)\n            dataset = dataset.batch(batch_size)\n            dataset = dataset.map(_parse_function, num_parallel_calls=1)\n            iterator = dataset.make_initializable_iterator()\n            return iterator\n\n    def model(self, user_id, item_id):\n        user_embedding_variable = tf.get_variable('user_emb_var', [1000000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))\n        item_embedding_variable = tf.get_variable('user_emb_var', [500000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))\n        user_embedding = tf.nn.embedding_lookup(user_embedding_variable, user_id)\n        item_embedding = tf.nn.embedding_lookup(item_embedding_variable, item_id)\n        user_embedding = tf.reshape(user_embedding, [-1, 32])\n        item_embedding = tf.reshape(item_embedding, [-1, 32])\n        cross = tf.reduce_sum(user_embedding * item_embedding, 1, keep_dims=True)\n        bias = tf.get_variable('bias', initializer=tf.constant(np.zeros((1), dtype=np.float32)), dtype=tf.float32)\n        layer = cross + bias\n        weight_np = np.zeros((1, 2), dtype=np.float32)\n        weight_np[:, 1] = 1\n        weight = tf.get_variable('weight', initializer=tf.constant(weight_np), dtype=tf.float32, trainable=False)\n        logits = tf.matmul(layer, weight)\n        return logits\n\n    def train(self):\n        if self.job_name == 'ps':\n            with tf.device('/cpu:0'):\n                self.server.join()\n        elif self.job_name == 'worker':\n            with tf.Graph().as_default():\n                with tf.device(self.param_server_device):\n                    train_iterator = self.data_iter()\n                    train_label, train_user_id, train_item_id = train_iterator.get_next()\n                    train_logit = self.model(train_user_id, train_item_id)\n                    train_label = tf.to_int64(train_label)\n                    train_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_logit, labels=train_label)\n                    train_loss = tf.reduce_mean(train_cross_entropy, name='loss')\n                    opt = tf.train.AdamOptimizer(learning_rate=0.001)\n                    train_op = opt.minimize(train_loss)\n                    saver = tf.train.Saver()\n\n                    sess_config = tf.ConfigProto(allow_soft_placement=True,\n                        log_device_placement=False,\n                        device_filters=[\"/job:ps\", \"/job:%s/task:%d\" % (self.job_name, self.task_index)],\n                        operation_timeout_in_ms=60000,\n                        inter_op_parallelism_threads=1,\n                        intra_op_parallelism_threads=1)\n                    with tf.train.MonitoredTrainingSession(master=self.server.target,\n                                                           is_chief=self.is_chief,\n                                                           config=sess_config) as sess:\n                        epoch_num = 0\n                        while epoch_num &lt; 10:\n                            epoch_num += 1\n                            sess.run(train_iterator.initializer)\n                            while True:\n                                try:\n                                    sess.run(train_op)\n                                except tf.errors.OutOfRangeError:\n                                    saver.save(sess=sess._sess._sess._sess._sess,\n                                            save_path=\"some_hdfs_path/model.checkpoint.\"+str(epoch_num),\n                                            latest_filename='checkpoint.'+str(epoch_num))\n                                    break\n\ndef main(_):\n    job = Job()\n    job.train()\n\nif __name__ == '__main__':\n    tf.app.run()\n</code></pre>\n<p>Fully pstack:<br>\n<a href=\"https://github.com/formath/TensorFlow-Bugs/blob/master/39024.pstack\">https://github.com/formath/TensorFlow-Bugs/blob/master/39024.pstack</a></p>", "body_text": "Training epoch by epoch, the program will hangs on some random nodes forever at the end of one iteration, namely the end of one epoch. It appears certainly when using DataSet api, while it is fine when using old Queue based data api. I have set inter_op_parallelism_threads=1, intra_op_parallelism_threads=1. @mrry\nEnv:\nDistributed running on Yarn.\nNode information: CentOS, linux kernel 3.10.0. Only CPU.\nTensorFlow: r1.12, built from source.\nDemo(It's independent of the model. It's easier to reproduce by using more worker nodes. Here, I use 50 worker nodes and 20 ps nodes.)\nflags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\nflags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS(sys.argv)\n\nclass Job(object):\n    def __init__(self):\n        self.ps_hosts = FLAGS.ps_hosts.split(',')\n        self.worker_hosts = FLAGS.worker_hosts.split(',')\n        self.job_name = FLAGS.job_name\n        self.task_index = FLAGS.task_index\n        self.cluster = tf.train.ClusterSpec({'ps': self.ps_hosts, 'worker': self.worker_hosts})\n        self.server = tf.train.Server(self.cluster, job_name=self.job_name, task_index=self.task_index)\n        self.is_chief = (self.task_index == 0 and self.job_name == 'worker')\n        worker_prefix = '/job:worker/task:%s' % self.task_index\n        self.cpu_device = '%s/cpu:0' % worker_prefix\n        self.param_server_device = tf.train.replica_device_setter(\n            worker_device=self.cpu_device, cluster=self.cluster,\n            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(len(self.ps_hosts), tf.contrib.training.byte_size_load_fn))\n        self.num_ps = self.cluster.num_tasks('ps')\n        self.num_worker = self.cluster.num_tasks('worker')\n\n    def data_iter(self, batch_size=1000, file_pattern='./input/part-*'):\n        def _parse_function(examples):\n            features = {}\n            features['label'] = tf.FixedLenFeature([], tf.float32)\n            features['user_id'] = tf.FixedLenFeature([1], tf.int64)\n            features['item_id'] = tf.FixedLenFeature([1], tf.int64)\n            instance = tf.parse_example(examples, features)\n            return instance['label'], instance['user_id'], instance['item_id']\n\n        with tf.name_scope('input'):\n            files = tf.data.Dataset.list_files(file_pattern)\n            dataset = files.apply(tf.contrib.data.parallel_interleave(\n                        lambda file: tf.data.TFRecordDataset(file),\n                        cycle_length=1, sloppy=True))\n            dataset = dataset.prefetch(buffer_size=batch_size*2)\n            dataset = dataset.batch(batch_size)\n            dataset = dataset.map(_parse_function, num_parallel_calls=1)\n            iterator = dataset.make_initializable_iterator()\n            return iterator\n\n    def model(self, user_id, item_id):\n        user_embedding_variable = tf.get_variable('user_emb_var', [1000000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))\n        item_embedding_variable = tf.get_variable('user_emb_var', [500000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))\n        user_embedding = tf.nn.embedding_lookup(user_embedding_variable, user_id)\n        item_embedding = tf.nn.embedding_lookup(item_embedding_variable, item_id)\n        user_embedding = tf.reshape(user_embedding, [-1, 32])\n        item_embedding = tf.reshape(item_embedding, [-1, 32])\n        cross = tf.reduce_sum(user_embedding * item_embedding, 1, keep_dims=True)\n        bias = tf.get_variable('bias', initializer=tf.constant(np.zeros((1), dtype=np.float32)), dtype=tf.float32)\n        layer = cross + bias\n        weight_np = np.zeros((1, 2), dtype=np.float32)\n        weight_np[:, 1] = 1\n        weight = tf.get_variable('weight', initializer=tf.constant(weight_np), dtype=tf.float32, trainable=False)\n        logits = tf.matmul(layer, weight)\n        return logits\n\n    def train(self):\n        if self.job_name == 'ps':\n            with tf.device('/cpu:0'):\n                self.server.join()\n        elif self.job_name == 'worker':\n            with tf.Graph().as_default():\n                with tf.device(self.param_server_device):\n                    train_iterator = self.data_iter()\n                    train_label, train_user_id, train_item_id = train_iterator.get_next()\n                    train_logit = self.model(train_user_id, train_item_id)\n                    train_label = tf.to_int64(train_label)\n                    train_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_logit, labels=train_label)\n                    train_loss = tf.reduce_mean(train_cross_entropy, name='loss')\n                    opt = tf.train.AdamOptimizer(learning_rate=0.001)\n                    train_op = opt.minimize(train_loss)\n                    saver = tf.train.Saver()\n\n                    sess_config = tf.ConfigProto(allow_soft_placement=True,\n                        log_device_placement=False,\n                        device_filters=[\"/job:ps\", \"/job:%s/task:%d\" % (self.job_name, self.task_index)],\n                        operation_timeout_in_ms=60000,\n                        inter_op_parallelism_threads=1,\n                        intra_op_parallelism_threads=1)\n                    with tf.train.MonitoredTrainingSession(master=self.server.target,\n                                                           is_chief=self.is_chief,\n                                                           config=sess_config) as sess:\n                        epoch_num = 0\n                        while epoch_num < 10:\n                            epoch_num += 1\n                            sess.run(train_iterator.initializer)\n                            while True:\n                                try:\n                                    sess.run(train_op)\n                                except tf.errors.OutOfRangeError:\n                                    saver.save(sess=sess._sess._sess._sess._sess,\n                                            save_path=\"some_hdfs_path/model.checkpoint.\"+str(epoch_num),\n                                            latest_filename='checkpoint.'+str(epoch_num))\n                                    break\n\ndef main(_):\n    job = Job()\n    job.train()\n\nif __name__ == '__main__':\n    tf.app.run()\n\nFully pstack:\nhttps://github.com/formath/TensorFlow-Bugs/blob/master/39024.pstack", "body": "Training epoch by epoch, the program will hangs on some random nodes forever at the end of one iteration, namely the end of one epoch. It appears certainly when using `DataSet` api, while it is fine when using old `Queue` based data api. I have set `inter_op_parallelism_threads=1, intra_op_parallelism_threads=1`. @mrry \r\n\r\nEnv:\r\n  Distributed running on Yarn.\r\n  Node information: CentOS, linux kernel 3.10.0. Only CPU.\r\n  TensorFlow: r1.12, built from source.\r\n\r\nDemo(It's independent of the model. It's easier to reproduce by using more worker nodes. Here, I use 50 worker nodes and 20 ps nodes.)\r\n```\r\nflags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\nflags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\n\r\nFLAGS(sys.argv)\r\n\r\nclass Job(object):\r\n    def __init__(self):\r\n        self.ps_hosts = FLAGS.ps_hosts.split(',')\r\n        self.worker_hosts = FLAGS.worker_hosts.split(',')\r\n        self.job_name = FLAGS.job_name\r\n        self.task_index = FLAGS.task_index\r\n        self.cluster = tf.train.ClusterSpec({'ps': self.ps_hosts, 'worker': self.worker_hosts})\r\n        self.server = tf.train.Server(self.cluster, job_name=self.job_name, task_index=self.task_index)\r\n        self.is_chief = (self.task_index == 0 and self.job_name == 'worker')\r\n        worker_prefix = '/job:worker/task:%s' % self.task_index\r\n        self.cpu_device = '%s/cpu:0' % worker_prefix\r\n        self.param_server_device = tf.train.replica_device_setter(\r\n            worker_device=self.cpu_device, cluster=self.cluster,\r\n            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(len(self.ps_hosts), tf.contrib.training.byte_size_load_fn))\r\n        self.num_ps = self.cluster.num_tasks('ps')\r\n        self.num_worker = self.cluster.num_tasks('worker')\r\n\r\n    def data_iter(self, batch_size=1000, file_pattern='./input/part-*'):\r\n        def _parse_function(examples):\r\n            features = {}\r\n            features['label'] = tf.FixedLenFeature([], tf.float32)\r\n            features['user_id'] = tf.FixedLenFeature([1], tf.int64)\r\n            features['item_id'] = tf.FixedLenFeature([1], tf.int64)\r\n            instance = tf.parse_example(examples, features)\r\n            return instance['label'], instance['user_id'], instance['item_id']\r\n\r\n        with tf.name_scope('input'):\r\n            files = tf.data.Dataset.list_files(file_pattern)\r\n            dataset = files.apply(tf.contrib.data.parallel_interleave(\r\n                        lambda file: tf.data.TFRecordDataset(file),\r\n                        cycle_length=1, sloppy=True))\r\n            dataset = dataset.prefetch(buffer_size=batch_size*2)\r\n            dataset = dataset.batch(batch_size)\r\n            dataset = dataset.map(_parse_function, num_parallel_calls=1)\r\n            iterator = dataset.make_initializable_iterator()\r\n            return iterator\r\n\r\n    def model(self, user_id, item_id):\r\n        user_embedding_variable = tf.get_variable('user_emb_var', [1000000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))\r\n        item_embedding_variable = tf.get_variable('user_emb_var', [500000, 32], initializer=tf.random_uniform_initializer(minval=-0.5, maxval=0.5, dtype=tf.float32))\r\n        user_embedding = tf.nn.embedding_lookup(user_embedding_variable, user_id)\r\n        item_embedding = tf.nn.embedding_lookup(item_embedding_variable, item_id)\r\n        user_embedding = tf.reshape(user_embedding, [-1, 32])\r\n        item_embedding = tf.reshape(item_embedding, [-1, 32])\r\n        cross = tf.reduce_sum(user_embedding * item_embedding, 1, keep_dims=True)\r\n        bias = tf.get_variable('bias', initializer=tf.constant(np.zeros((1), dtype=np.float32)), dtype=tf.float32)\r\n        layer = cross + bias\r\n        weight_np = np.zeros((1, 2), dtype=np.float32)\r\n        weight_np[:, 1] = 1\r\n        weight = tf.get_variable('weight', initializer=tf.constant(weight_np), dtype=tf.float32, trainable=False)\r\n        logits = tf.matmul(layer, weight)\r\n        return logits\r\n\r\n    def train(self):\r\n        if self.job_name == 'ps':\r\n            with tf.device('/cpu:0'):\r\n                self.server.join()\r\n        elif self.job_name == 'worker':\r\n            with tf.Graph().as_default():\r\n                with tf.device(self.param_server_device):\r\n                    train_iterator = self.data_iter()\r\n                    train_label, train_user_id, train_item_id = train_iterator.get_next()\r\n                    train_logit = self.model(train_user_id, train_item_id)\r\n                    train_label = tf.to_int64(train_label)\r\n                    train_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_logit, labels=train_label)\r\n                    train_loss = tf.reduce_mean(train_cross_entropy, name='loss')\r\n                    opt = tf.train.AdamOptimizer(learning_rate=0.001)\r\n                    train_op = opt.minimize(train_loss)\r\n                    saver = tf.train.Saver()\r\n\r\n                    sess_config = tf.ConfigProto(allow_soft_placement=True,\r\n                        log_device_placement=False,\r\n                        device_filters=[\"/job:ps\", \"/job:%s/task:%d\" % (self.job_name, self.task_index)],\r\n                        operation_timeout_in_ms=60000,\r\n                        inter_op_parallelism_threads=1,\r\n                        intra_op_parallelism_threads=1)\r\n                    with tf.train.MonitoredTrainingSession(master=self.server.target,\r\n                                                           is_chief=self.is_chief,\r\n                                                           config=sess_config) as sess:\r\n                        epoch_num = 0\r\n                        while epoch_num < 10:\r\n                            epoch_num += 1\r\n                            sess.run(train_iterator.initializer)\r\n                            while True:\r\n                                try:\r\n                                    sess.run(train_op)\r\n                                except tf.errors.OutOfRangeError:\r\n                                    saver.save(sess=sess._sess._sess._sess._sess,\r\n                                            save_path=\"some_hdfs_path/model.checkpoint.\"+str(epoch_num),\r\n                                            latest_filename='checkpoint.'+str(epoch_num))\r\n                                    break\r\n\r\ndef main(_):\r\n    job = Job()\r\n    job.train()\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n```\r\n\r\nFully pstack: \r\n  https://github.com/formath/TensorFlow-Bugs/blob/master/39024.pstack\r\n\r\n"}