{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3221", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3221/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3221/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3221/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3221", "id": 164345152, "node_id": "MDU6SXNzdWUxNjQzNDUxNTI=", "number": 3221, "title": "Distributed tensorflow does not allocate well 'worker' to GPUs", "user": {"login": "hellf", "id": 9377459, "node_id": "MDQ6VXNlcjkzNzc0NTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/9377459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hellf", "html_url": "https://github.com/hellf", "followers_url": "https://api.github.com/users/hellf/followers", "following_url": "https://api.github.com/users/hellf/following{/other_user}", "gists_url": "https://api.github.com/users/hellf/gists{/gist_id}", "starred_url": "https://api.github.com/users/hellf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hellf/subscriptions", "organizations_url": "https://api.github.com/users/hellf/orgs", "repos_url": "https://api.github.com/users/hellf/repos", "events_url": "https://api.github.com/users/hellf/events{/privacy}", "received_events_url": "https://api.github.com/users/hellf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-07-07T15:56:37Z", "updated_at": "2016-07-08T07:15:02Z", "closed_at": "2016-07-08T07:14:42Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04 desktop<br>\nInstalled version of CUDA and cuDNN: 7.5,  5.0.5<br>\ntensorflow 0.9.0.rc0 is installed from source.</p>\n<p>(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n$ ls -l /usr/local/cuda/lib64/libcud*<br>\n-rw-r--r-- 1 root root   322936  8 16  2015 /usr/local/cuda/lib64/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root       16  8 16  2015 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.7.5<br>\nlrwxrwxrwx 1 root root       19  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -&gt; libcudart.so.7.5.18<br>\n-rwxr-xr-x 1 root root   383336  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18<br>\n-rw-r--r-- 1 root root   720192  8 16  2015 /usr/local/cuda/lib64/libcudart_static.a<br>\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so<br>\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5<br>\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5.0.5<br>\n-rwxr--r-- 1 root root 58775484  6 21 15:39 /usr/local/cuda/lib64/libcudnn_static.a</p>\n<p>I'm trying to use distributed tensorflow with 8 Titan-x GPUs in two servers.<br>\n4 GPUs are in one server.</p>\n<p>I separate workers to each GPUs as follows.</p>\n<p>with tf.device(tf.train.replica_device_setter(<br>\nworker_device=\"/gpu:%d\" % (FLAGS.task_id%4), cluster=cluster_spec)):</p>\n<p>When I execute two 'ps' and eight 'worker', nvidia-smi shows next.</p>\n<p>((server1)) workers are allocated on GPU 0,1,2,3<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/9377459/16659014/75379c00-44a3-11e6-957c-845bed33f492.png\"><img src=\"https://cloud.githubusercontent.com/assets/9377459/16659014/75379c00-44a3-11e6-957c-845bed33f492.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>((server2)) workers are allocated on GPU 0<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/9377459/16658977/5748e794-44a3-11e6-94d1-f2eac24639cf.png\"><img src=\"https://cloud.githubusercontent.com/assets/9377459/16658977/5748e794-44a3-11e6-94d1-f2eac24639cf.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>Why the workers are not allocated to GPU 1,2,3 in server1 ?</p>\n<p>please help me.</p>", "body_text": "Environment info\nOperating System: Ubuntu 14.04 desktop\nInstalled version of CUDA and cuDNN: 7.5,  5.0.5\ntensorflow 0.9.0.rc0 is installed from source.\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936  8 16  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16  8 16  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192  8 16  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rwxr--r-- 1 root root 58775484  6 21 15:39 /usr/local/cuda/lib64/libcudnn_static.a\nI'm trying to use distributed tensorflow with 8 Titan-x GPUs in two servers.\n4 GPUs are in one server.\nI separate workers to each GPUs as follows.\nwith tf.device(tf.train.replica_device_setter(\nworker_device=\"/gpu:%d\" % (FLAGS.task_id%4), cluster=cluster_spec)):\nWhen I execute two 'ps' and eight 'worker', nvidia-smi shows next.\n((server1)) workers are allocated on GPU 0,1,2,3\n\n((server2)) workers are allocated on GPU 0\n\nWhy the workers are not allocated to GPU 1,2,3 in server1 ?\nplease help me.", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 desktop\nInstalled version of CUDA and cuDNN: 7.5,  5.0.5\ntensorflow 0.9.0.rc0 is installed from source.\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936  8 16  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16  8 16  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336  8 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192  8 16  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr--r-- 1 root root 59909104  6 21 15:39 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rwxr--r-- 1 root root 58775484  6 21 15:39 /usr/local/cuda/lib64/libcudnn_static.a\n\nI'm trying to use distributed tensorflow with 8 Titan-x GPUs in two servers.\n4 GPUs are in one server.\n\nI separate workers to each GPUs as follows.\n\n with tf.device(tf.train.replica_device_setter(\n      worker_device=\"/gpu:%d\" % (FLAGS.task_id%4), cluster=cluster_spec)):\n\nWhen I execute two 'ps' and eight 'worker', nvidia-smi shows next.\n\n((server1)) workers are allocated on GPU 0,1,2,3\n![image](https://cloud.githubusercontent.com/assets/9377459/16659014/75379c00-44a3-11e6-957c-845bed33f492.png)\n\n((server2)) workers are allocated on GPU 0\n![image](https://cloud.githubusercontent.com/assets/9377459/16658977/5748e794-44a3-11e6-94d1-f2eac24639cf.png)\n\nWhy the workers are not allocated to GPU 1,2,3 in server1 ?\n\nplease help me.\n"}