{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423723884", "html_url": "https://github.com/tensorflow/tensorflow/issues/21081#issuecomment-423723884", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21081", "id": 423723884, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzcyMzg4NA==", "user": {"login": "anhtu812", "id": 24753367, "node_id": "MDQ6VXNlcjI0NzUzMzY3", "avatar_url": "https://avatars0.githubusercontent.com/u/24753367?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anhtu812", "html_url": "https://github.com/anhtu812", "followers_url": "https://api.github.com/users/anhtu812/followers", "following_url": "https://api.github.com/users/anhtu812/following{/other_user}", "gists_url": "https://api.github.com/users/anhtu812/gists{/gist_id}", "starred_url": "https://api.github.com/users/anhtu812/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anhtu812/subscriptions", "organizations_url": "https://api.github.com/users/anhtu812/orgs", "repos_url": "https://api.github.com/users/anhtu812/repos", "events_url": "https://api.github.com/users/anhtu812/events{/privacy}", "received_events_url": "https://api.github.com/users/anhtu812/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-22T07:28:11Z", "updated_at": "2018-09-22T07:37:52Z", "author_association": "NONE", "body_html": "<p>I have same error. If no use tensorflow.contrib.tensorrt.create_inference_graph then no error.</p>\n<p>ResourceExhaustedError (see above for traceback): Requested batch size is not available and engine cache is full<br>\n[[Node: model_1/prior_layer_1/my_trt_op_11 = TRTEngineOp<a href=\"model_1/prior_layer_1/ToFloat\">InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[10], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,2]], max_cached_engines_count=1, output_shapes=[[?,2]], precision_mode=\"FP32\", segment_funcdef_name=\"model_1/prior_layer_1/my_trt_op_11_native_segment\", serialized_segment=\"\\270\\020\\0...00\\000\\000\", static_engine=true, workspace_size_bytes=18292682, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"</a>]]<br>\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>\n<pre><code>     [[Node: while/Switch_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2429_while/Switch_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopwhile/strided_slice_5/stack/_1)]]\n</code></pre>\n<p>Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>\n<p>System:<br>\nubuntu 16.04<br>\ntensorflow v1.10.1 from source<br>\ncuda 9.0, cudnn 7.1<br>\ntensorrt 4.1.2<br>\npython 3.5</p>", "body_text": "I have same error. If no use tensorflow.contrib.tensorrt.create_inference_graph then no error.\nResourceExhaustedError (see above for traceback): Requested batch size is not available and engine cache is full\n[[Node: model_1/prior_layer_1/my_trt_op_11 = TRTEngineOpInT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[10], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,2]], max_cached_engines_count=1, output_shapes=[[?,2]], precision_mode=\"FP32\", segment_funcdef_name=\"model_1/prior_layer_1/my_trt_op_11_native_segment\", serialized_segment=\"\\270\\020\\0...00\\000\\000\", static_engine=true, workspace_size_bytes=18292682, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n     [[Node: while/Switch_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2429_while/Switch_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopwhile/strided_slice_5/stack/_1)]]\n\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\nSystem:\nubuntu 16.04\ntensorflow v1.10.1 from source\ncuda 9.0, cudnn 7.1\ntensorrt 4.1.2\npython 3.5", "body": "I have same error. If no use tensorflow.contrib.tensorrt.create_inference_graph then no error.\r\n\r\nResourceExhaustedError (see above for traceback): Requested batch size is not available and engine cache is full\r\n         [[Node: model_1/prior_layer_1/my_trt_op_11 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[10], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,2]], max_cached_engines_count=1, output_shapes=[[?,2]], precision_mode=\"FP32\", segment_funcdef_name=\"model_1/prior_layer_1/my_trt_op_11_native_segment\", serialized_segment=\"\\270\\020\\0...00\\000\\000\", static_engine=true, workspace_size_bytes=18292682, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_1/prior_layer_1/ToFloat)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[Node: while/Switch_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2429_while/Switch_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopwhile/strided_slice_5/stack/_1)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nSystem:\r\nubuntu 16.04\r\ntensorflow v1.10.1 from source\r\ncuda 9.0, cudnn 7.1\r\ntensorrt 4.1.2\r\npython 3.5"}