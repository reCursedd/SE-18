{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21081", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21081/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21081/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21081/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21081", "id": 343930005, "node_id": "MDU6SXNzdWUzNDM5MzAwMDU=", "number": 21081, "title": "Tensort RT Engine Object Detection", "user": {"login": "snownus", "id": 5620849, "node_id": "MDQ6VXNlcjU2MjA4NDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/5620849?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snownus", "html_url": "https://github.com/snownus", "followers_url": "https://api.github.com/users/snownus/followers", "following_url": "https://api.github.com/users/snownus/following{/other_user}", "gists_url": "https://api.github.com/users/snownus/gists{/gist_id}", "starred_url": "https://api.github.com/users/snownus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snownus/subscriptions", "organizations_url": "https://api.github.com/users/snownus/orgs", "repos_url": "https://api.github.com/users/snownus/repos", "events_url": "https://api.github.com/users/snownus/events{/privacy}", "received_events_url": "https://api.github.com/users/snownus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-24T08:11:59Z", "updated_at": "2018-11-21T19:00:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>tensorflow.python.framework.errors_impl.ResourceExhaustedError: Requested batch size is not available and engine cache is full<br>\n[[Node: import/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,1], [?,1], [?,1], [?,1]], max_cached_engines_count=1, output_shapes=[[?,4]], precision_mode=\"FP32\", segment_funcdef_name=\"my_trt_op_16_native_segment\", serialized_segment=\"\\2007\\000...00\\000\\000\", static_engine=true, workspace_size_bytes=65075264, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](import/MultiClassNonMaxSuppression/ClipToWindow/split, import/MultiClassNonMaxSuppression/ClipToWindow/split:2, import/MultiClassNonMaxSuppression/ClipToWindow/split:1, import/MultiClassNonMaxSuppression/ClipToWindow/split:3)]]<br>\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>\n<pre><code> [[Node: import/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/strided_slice/_89 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1900_...ided_slice\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n</code></pre>\n<p>Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>", "body_text": "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Requested batch size is not available and engine cache is full\n[[Node: import/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,1], [?,1], [?,1], [?,1]], max_cached_engines_count=1, output_shapes=[[?,4]], precision_mode=\"FP32\", segment_funcdef_name=\"my_trt_op_16_native_segment\", serialized_segment=\"\\2007\\000...00\\000\\000\", static_engine=true, workspace_size_bytes=65075264, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](import/MultiClassNonMaxSuppression/ClipToWindow/split, import/MultiClassNonMaxSuppression/ClipToWindow/split:2, import/MultiClassNonMaxSuppression/ClipToWindow/split:1, import/MultiClassNonMaxSuppression/ClipToWindow/split:3)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [[Node: import/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/strided_slice/_89 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1900_...ided_slice\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.", "body": "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Requested batch size is not available and engine cache is full\r\n\t [[Node: import/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,1], [?,1], [?,1], [?,1]], max_cached_engines_count=1, output_shapes=[[?,4]], precision_mode=\"FP32\", segment_funcdef_name=\"my_trt_op_16_native_segment\", serialized_segment=\"\\2007\\000\\...00\\000\\000\", static_engine=true, workspace_size_bytes=65075264, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](import/MultiClassNonMaxSuppression/ClipToWindow/split, import/MultiClassNonMaxSuppression/ClipToWindow/split:2, import/MultiClassNonMaxSuppression/ClipToWindow/split:1, import/MultiClassNonMaxSuppression/ClipToWindow/split:3)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: import/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/strided_slice/_89 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1900_...ided_slice\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info."}