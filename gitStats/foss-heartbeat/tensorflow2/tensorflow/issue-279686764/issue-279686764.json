{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15149", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15149/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15149/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15149/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15149", "id": 279686764, "node_id": "MDU6SXNzdWUyNzk2ODY3NjQ=", "number": 15149, "title": "BatchNorm/gamma not found in checkpoint", "user": {"login": "zuokai", "id": 15143205, "node_id": "MDQ6VXNlcjE1MTQzMjA1", "avatar_url": "https://avatars1.githubusercontent.com/u/15143205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuokai", "html_url": "https://github.com/zuokai", "followers_url": "https://api.github.com/users/zuokai/followers", "following_url": "https://api.github.com/users/zuokai/following{/other_user}", "gists_url": "https://api.github.com/users/zuokai/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuokai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuokai/subscriptions", "organizations_url": "https://api.github.com/users/zuokai/orgs", "repos_url": "https://api.github.com/users/zuokai/repos", "events_url": "https://api.github.com/users/zuokai/events{/privacy}", "received_events_url": "https://api.github.com/users/zuokai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-12-06T09:43:25Z", "updated_at": "2017-12-07T23:55:17Z", "closed_at": "2017-12-07T23:55:17Z", "author_association": "NONE", "body_html": "<p>I trained a model with tensorflow1.4\uff0c and want to finetune in tensorflow 1.3\uff0cfollowing is part of my code:<br>\n1\uff09train code with tf1.4:<br>\nwith slim.arg_scope(inception_v3.inception_v3_arg_scope()):<br>\ny, endpoints = inception_v3.inception_v3(x, CLASSES, True)<br>\n2\uff09test code with master:<br>\nwith slim.arg_scope(inception_v3.inception_v3_arg_scope()):<br>\ny, end_points = inception_v3.inception_v3(x, label_dim, False)<br>\nwith tf.name_scope('train'):<br>\nloss = custom_function.loss_function(y, y_)<br>\ntrain_op = custom_function.train_function(loss, learn_rate)<br>\naccuracy_op = custom_function.accuracy_function(y, y_)<br>\noutput_result = custom_function.output_result(y)<br>\ninit_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())<br>\nsaver = tf.train.Saver()<br>\nsess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)<br>\nsess_config.gpu_options.allow_growth = True<br>\nsess = tf.Session(config=sess_config)<br>\nmerged_summary = tf.summary.merge_all()<br>\ntrain_writer = tf.summary.FileWriter(log_dir, sess.graph)<br>\n#saver = tf.train.import_meta_graph('model_meishi/inception-v3/graph-1205-190241.meta',clear_devices=True)<br>\nsess.run(init_op)<br>\ncoord = tf.train.Coordinator()<br>\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)<br>\n#saver.restore(sess, \"model_meishi/model-6690\")<br>\nsaver.restore(sess, \"model_meishi/inception-v3/model-12042\")<br>\nerror is:NotFoundError (see above for traceback): Key InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma not found in checkpoint<br>\nat first, i think the reason is slim.argscope is not the same in these two version tensorflow and<br>\nI tried to replace the slim of tf1.4 with the slim of master\uff0cbut the error still exist.<br>\nwhat's the problem of my code ?thx</p>", "body_text": "I trained a model with tensorflow1.4\uff0c and want to finetune in tensorflow 1.3\uff0cfollowing is part of my code:\n1\uff09train code with tf1.4:\nwith slim.arg_scope(inception_v3.inception_v3_arg_scope()):\ny, endpoints = inception_v3.inception_v3(x, CLASSES, True)\n2\uff09test code with master:\nwith slim.arg_scope(inception_v3.inception_v3_arg_scope()):\ny, end_points = inception_v3.inception_v3(x, label_dim, False)\nwith tf.name_scope('train'):\nloss = custom_function.loss_function(y, y_)\ntrain_op = custom_function.train_function(loss, learn_rate)\naccuracy_op = custom_function.accuracy_function(y, y_)\noutput_result = custom_function.output_result(y)\ninit_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\nsaver = tf.train.Saver()\nsess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\nsess_config.gpu_options.allow_growth = True\nsess = tf.Session(config=sess_config)\nmerged_summary = tf.summary.merge_all()\ntrain_writer = tf.summary.FileWriter(log_dir, sess.graph)\n#saver = tf.train.import_meta_graph('model_meishi/inception-v3/graph-1205-190241.meta',clear_devices=True)\nsess.run(init_op)\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n#saver.restore(sess, \"model_meishi/model-6690\")\nsaver.restore(sess, \"model_meishi/inception-v3/model-12042\")\nerror is:NotFoundError (see above for traceback): Key InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma not found in checkpoint\nat first, i think the reason is slim.argscope is not the same in these two version tensorflow and\nI tried to replace the slim of tf1.4 with the slim of master\uff0cbut the error still exist.\nwhat's the problem of my code ?thx", "body": "I trained a model with tensorflow1.4\uff0c and want to finetune in tensorflow 1.3\uff0cfollowing is part of my code:\r\n1\uff09train code with tf1.4:\r\n            with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\r\n          y, endpoints = inception_v3.inception_v3(x, CLASSES, True)\r\n2\uff09test code with master:\r\n     with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\r\n        y, end_points = inception_v3.inception_v3(x, label_dim, False)\r\n    with tf.name_scope('train'):\r\n        loss = custom_function.loss_function(y, y_)\r\n        train_op = custom_function.train_function(loss, learn_rate)\r\n        accuracy_op = custom_function.accuracy_function(y, y_)\r\n        output_result = custom_function.output_result(y)\r\n        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n        saver = tf.train.Saver()\r\n        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\n        sess_config.gpu_options.allow_growth = True\r\n        sess = tf.Session(config=sess_config)\r\n        merged_summary = tf.summary.merge_all()\r\n        train_writer = tf.summary.FileWriter(log_dir, sess.graph)\r\n        #saver = tf.train.import_meta_graph('model_meishi/inception-v3/graph-1205-190241.meta',clear_devices=True)\r\n        sess.run(init_op)\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        #saver.restore(sess, \"model_meishi/model-6690\")\r\n        saver.restore(sess, \"model_meishi/inception-v3/model-12042\")\r\nerror is:NotFoundError (see above for traceback): Key InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/gamma not found in checkpoint\r\nat first, i think the reason is slim.argscope is not the same in these two version tensorflow and \r\nI tried to replace the slim of tf1.4 with the slim of master\uff0cbut the error still exist.\r\nwhat's the problem of my code ?thx\r\n     \r\n"}