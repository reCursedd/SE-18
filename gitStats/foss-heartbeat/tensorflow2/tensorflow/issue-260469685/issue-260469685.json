{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13303", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13303/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13303/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13303/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13303", "id": 260469685, "node_id": "MDU6SXNzdWUyNjA0Njk2ODU=", "number": 13303, "title": "[Missing Feature] Input Pipeline for Models with Multi-step Optimization", "user": {"login": "yliu120", "id": 9438093, "node_id": "MDQ6VXNlcjk0MzgwOTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9438093?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yliu120", "html_url": "https://github.com/yliu120", "followers_url": "https://api.github.com/users/yliu120/followers", "following_url": "https://api.github.com/users/yliu120/following{/other_user}", "gists_url": "https://api.github.com/users/yliu120/gists{/gist_id}", "starred_url": "https://api.github.com/users/yliu120/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yliu120/subscriptions", "organizations_url": "https://api.github.com/users/yliu120/orgs", "repos_url": "https://api.github.com/users/yliu120/repos", "events_url": "https://api.github.com/users/yliu120/events{/privacy}", "received_events_url": "https://api.github.com/users/yliu120/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-09-26T02:16:55Z", "updated_at": "2018-01-03T06:01:11Z", "closed_at": "2018-01-03T06:01:11Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>After some searching and reading (e.g. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"210920003\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7951\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7951\">#7951</a>), I found the current input pipeline framework<br>\nis generally lack of support for models with multi-step optimization.</p>\n<p>In most of the models before GAN (General Adversarial Networks), a model almost has one<br>\noptimization function, that is, we optimize over a mini-batch of input data once in a step. While<br>\nstarting from GAN, many models have two or more optimization functions, in other words, they<br>\nsequentially optimize on several functions using the same batch of data (Adversarial Autoencoders).</p>\n<p>The current input pipeline works fine with a unique optimization operation per step, where the <code>tf.Session</code> will pull a batch of data from the input queue once. For multi-step optimization models, if you link all steps with a unique input queue, then the queue will be pulled several times<br>\nif you link all sequential optimization steps with that queue. Apparently, this is completely wrong.</p>\n<p>From my point of view, I think we should add a peek() op in <code>tf.QueueBase</code> for supporting<br>\nmultistep sequential optimization. So for example, for GAN, we can link optimization over discriminator with a peek_many op and link optimization over generator with a dequeue_many op.</p>\n<p>In general, for multiple steps, we can do <code>peek_many</code> -&gt; <code>peek_many</code> -&gt; ... -&gt; <code>dequeue_many</code>.</p>\n<p>For the new tf.data.Dataset API, when we called <code>session.run()</code>, we advance the iterator. So in the new data importing API, we still lack of this feature.</p>\n<p>I think currently a workaround of this would be building a buffer with tf.Variable. and make all subsequent optimization step depends on the snapshot of the buffer. Like,</p>\n<pre><code>buffer = tf.get_variable(\"buffer\",\n                                        shape=(**, **),\n                                        trainable=False,\n                                        initializer=tf.zeros_initializer)\ninput_op = input_function()\nassign_op = tf.assign(buffer, input_op)\n\ncache = buffer.value()\n# based everything on cache for the subsequent step.\n\nwith tf.Session() as sess:\n    sess.run(assign_op)\n    # then run all other steps.\n</code></pre>\n<p>In this case, we have to copy the entire batch of data for each step.</p>", "body_text": "Describe the problem\nAfter some searching and reading (e.g. #7951), I found the current input pipeline framework\nis generally lack of support for models with multi-step optimization.\nIn most of the models before GAN (General Adversarial Networks), a model almost has one\noptimization function, that is, we optimize over a mini-batch of input data once in a step. While\nstarting from GAN, many models have two or more optimization functions, in other words, they\nsequentially optimize on several functions using the same batch of data (Adversarial Autoencoders).\nThe current input pipeline works fine with a unique optimization operation per step, where the tf.Session will pull a batch of data from the input queue once. For multi-step optimization models, if you link all steps with a unique input queue, then the queue will be pulled several times\nif you link all sequential optimization steps with that queue. Apparently, this is completely wrong.\nFrom my point of view, I think we should add a peek() op in tf.QueueBase for supporting\nmultistep sequential optimization. So for example, for GAN, we can link optimization over discriminator with a peek_many op and link optimization over generator with a dequeue_many op.\nIn general, for multiple steps, we can do peek_many -> peek_many -> ... -> dequeue_many.\nFor the new tf.data.Dataset API, when we called session.run(), we advance the iterator. So in the new data importing API, we still lack of this feature.\nI think currently a workaround of this would be building a buffer with tf.Variable. and make all subsequent optimization step depends on the snapshot of the buffer. Like,\nbuffer = tf.get_variable(\"buffer\",\n                                        shape=(**, **),\n                                        trainable=False,\n                                        initializer=tf.zeros_initializer)\ninput_op = input_function()\nassign_op = tf.assign(buffer, input_op)\n\ncache = buffer.value()\n# based everything on cache for the subsequent step.\n\nwith tf.Session() as sess:\n    sess.run(assign_op)\n    # then run all other steps.\n\nIn this case, we have to copy the entire batch of data for each step.", "body": "### Describe the problem\r\n\r\nAfter some searching and reading (e.g. #7951), I found the current input pipeline framework\r\nis generally lack of support for models with multi-step optimization.\r\n\r\nIn most of the models before GAN (General Adversarial Networks), a model almost has one\r\noptimization function, that is, we optimize over a mini-batch of input data once in a step. While\r\nstarting from GAN, many models have two or more optimization functions, in other words, they\r\nsequentially optimize on several functions using the same batch of data (Adversarial Autoencoders). \r\n\r\nThe current input pipeline works fine with a unique optimization operation per step, where the `tf.Session` will pull a batch of data from the input queue once. For multi-step optimization models, if you link all steps with a unique input queue, then the queue will be pulled several times\r\nif you link all sequential optimization steps with that queue. Apparently, this is completely wrong.\r\n\r\nFrom my point of view, I think we should add a peek() op in `tf.QueueBase` for supporting\r\nmultistep sequential optimization. So for example, for GAN, we can link optimization over discriminator with a peek_many op and link optimization over generator with a dequeue_many op.\r\n\r\nIn general, for multiple steps, we can do `peek_many` -> `peek_many` -> ... -> `dequeue_many`.\r\n\r\nFor the new tf.data.Dataset API, when we called `session.run()`, we advance the iterator. So in the new data importing API, we still lack of this feature.\r\n\r\nI think currently a workaround of this would be building a buffer with tf.Variable. and make all subsequent optimization step depends on the snapshot of the buffer. Like,\r\n\r\n```\r\nbuffer = tf.get_variable(\"buffer\",\r\n                                        shape=(**, **),\r\n                                        trainable=False,\r\n                                        initializer=tf.zeros_initializer)\r\ninput_op = input_function()\r\nassign_op = tf.assign(buffer, input_op)\r\n\r\ncache = buffer.value()\r\n# based everything on cache for the subsequent step.\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(assign_op)\r\n    # then run all other steps.\r\n```\r\n\r\nIn this case, we have to copy the entire batch of data for each step."}