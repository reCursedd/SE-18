{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19856", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19856/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19856/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19856/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19856", "id": 330649694, "node_id": "MDU6SXNzdWUzMzA2NDk2OTQ=", "number": 19856, "title": "problem in tf.data.TFRecordDataset", "user": {"login": "haddis3", "id": 21120947, "node_id": "MDQ6VXNlcjIxMTIwOTQ3", "avatar_url": "https://avatars2.githubusercontent.com/u/21120947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/haddis3", "html_url": "https://github.com/haddis3", "followers_url": "https://api.github.com/users/haddis3/followers", "following_url": "https://api.github.com/users/haddis3/following{/other_user}", "gists_url": "https://api.github.com/users/haddis3/gists{/gist_id}", "starred_url": "https://api.github.com/users/haddis3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/haddis3/subscriptions", "organizations_url": "https://api.github.com/users/haddis3/orgs", "repos_url": "https://api.github.com/users/haddis3/repos", "events_url": "https://api.github.com/users/haddis3/events{/privacy}", "received_events_url": "https://api.github.com/users/haddis3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-06-08T13:02:34Z", "updated_at": "2018-06-21T19:28:28Z", "closed_at": "2018-06-21T19:28:28Z", "author_association": "NONE", "body_html": "<p><strong>Describe the problem</strong><br>\nI use the tf.data.TFRecordDataset API to read the tfrecord file. Everything is ok when batch_size is set to 1, but when the batch_size is greater than 1, the code is crashed. the error is below:</p>\n<p>_Traceback (most recent call last):<br>\nFile \"F:/kaggle/JD_Fashion_AI/train.py\", line 125, in <br>\nmain()<br>\nFile \"F:/kaggle/JD_Fashion_AI/train.py\", line 122, in main<br>\nrun_training()<br>\nFile \"F:/kaggle/JD_Fashion_AI/train.py\", line 98, in run_training<br>\nsess.run(label)<br>\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run<br>\nrun_metadata_ptr)<br>\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run<br>\noptions, run_metadata)<br>\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 200704 values, but the requested shape has 150528<br>\n[[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32](DecodeRaw, Reshape/shape)]]<br>\n[[Node: IteratorGetNext = IteratorGetNext<a href=\"Iterator\">output_shapes=[[?,224,224,3], [?,?], [?]], output_types=[DT_UINT8, DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]<br>\n_</p>\n<p>note: i have totally 50000 images, i originally set batch_size to 16, so the num_per_epoch is 3125.</p>\n<p><strong>Source code</strong><br>\n`def _parse_function(record):</p>\n<pre><code>features = {'image': tf.FixedLenFeature([], tf.string), 'label': tf.FixedLenFeature([], tf.string),\n            'id': tf.FixedLenFeature([], tf.string)}\nexample = tf.parse_single_example(record, features=features)\nimage = tf.decode_raw(example['image'], tf.uint8)\nimage = tf.reshape(image, [224, 224, 3])\nlabel = tf.subtract(tf.decode_raw(example['label'], tf.uint8), 48)\nid = tf.cast(example['id'], tf.string)\n\nreturn image, label, id\n</code></pre>\n<p>def run_training():</p>\n<pre><code># learning_rate = tf.placeholder(dtype=tf.float32)\ntrain_filenames = ['I:/JD_fashion_AI/train_tfrecords/train.tfrecords']\nvalidation_filenames = ['I:/JD_fashion_AI/valid_tfrecords/valid.tfrecords']\ntrain_datasets = tf.data.TFRecordDataset(filenames=train_filenames)\nvalidation_datasets = tf.data.TFRecordDataset(filenames=validation_filenames)\ntrain_datasets = train_datasets.map(_parse_function).shuffle(buffer_size=1000).batch(16).repeat()\nvalidation_datasets = validation_datasets.map(_parse_function).batch(1).repeat(1)\niterator = tf.data.Iterator.from_structure(train_datasets.output_types,\n                                           train_datasets.output_shapes)\nimage, label, id = iterator.get_next()\ntraining_init_op = iterator.make_initializer(train_datasets)\nvalidation_init_op = iterator.make_initializer(validation_datasets)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n\n    sess.run(init)\n    step = 0\n    for _ in range(18):\n        sess.run(training_init_op)  # must initialize the iterator to read the data flow\n        for _ in range(3125):\n            \n            sess.run(label)\n            print(step)\n</code></pre>\n<p>`<br>\nanyone else know what happened ?</p>", "body_text": "Describe the problem\nI use the tf.data.TFRecordDataset API to read the tfrecord file. Everything is ok when batch_size is set to 1, but when the batch_size is greater than 1, the code is crashed. the error is below:\n_Traceback (most recent call last):\nFile \"F:/kaggle/JD_Fashion_AI/train.py\", line 125, in \nmain()\nFile \"F:/kaggle/JD_Fashion_AI/train.py\", line 122, in main\nrun_training()\nFile \"F:/kaggle/JD_Fashion_AI/train.py\", line 98, in run_training\nsess.run(label)\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\nrun_metadata_ptr)\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\noptions, run_metadata)\nFile \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 200704 values, but the requested shape has 150528\n[[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32](DecodeRaw, Reshape/shape)]]\n[[Node: IteratorGetNext = IteratorGetNextoutput_shapes=[[?,224,224,3], [?,?], [?]], output_types=[DT_UINT8, DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\n_\nnote: i have totally 50000 images, i originally set batch_size to 16, so the num_per_epoch is 3125.\nSource code\n`def _parse_function(record):\nfeatures = {'image': tf.FixedLenFeature([], tf.string), 'label': tf.FixedLenFeature([], tf.string),\n            'id': tf.FixedLenFeature([], tf.string)}\nexample = tf.parse_single_example(record, features=features)\nimage = tf.decode_raw(example['image'], tf.uint8)\nimage = tf.reshape(image, [224, 224, 3])\nlabel = tf.subtract(tf.decode_raw(example['label'], tf.uint8), 48)\nid = tf.cast(example['id'], tf.string)\n\nreturn image, label, id\n\ndef run_training():\n# learning_rate = tf.placeholder(dtype=tf.float32)\ntrain_filenames = ['I:/JD_fashion_AI/train_tfrecords/train.tfrecords']\nvalidation_filenames = ['I:/JD_fashion_AI/valid_tfrecords/valid.tfrecords']\ntrain_datasets = tf.data.TFRecordDataset(filenames=train_filenames)\nvalidation_datasets = tf.data.TFRecordDataset(filenames=validation_filenames)\ntrain_datasets = train_datasets.map(_parse_function).shuffle(buffer_size=1000).batch(16).repeat()\nvalidation_datasets = validation_datasets.map(_parse_function).batch(1).repeat(1)\niterator = tf.data.Iterator.from_structure(train_datasets.output_types,\n                                           train_datasets.output_shapes)\nimage, label, id = iterator.get_next()\ntraining_init_op = iterator.make_initializer(train_datasets)\nvalidation_init_op = iterator.make_initializer(validation_datasets)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n\n    sess.run(init)\n    step = 0\n    for _ in range(18):\n        sess.run(training_init_op)  # must initialize the iterator to read the data flow\n        for _ in range(3125):\n            \n            sess.run(label)\n            print(step)\n\n`\nanyone else know what happened ?", "body": "**Describe the problem**\r\nI use the tf.data.TFRecordDataset API to read the tfrecord file. Everything is ok when batch_size is set to 1, but when the batch_size is greater than 1, the code is crashed. the error is below:\r\n\r\n_Traceback (most recent call last):\r\n  File \"F:/kaggle/JD_Fashion_AI/train.py\", line 125, in <module>\r\n    main()\r\n  File \"F:/kaggle/JD_Fashion_AI/train.py\", line 122, in main\r\n    run_training()\r\n  File \"F:/kaggle/JD_Fashion_AI/train.py\", line 98, in run_training\r\n    sess.run(label)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 200704 values, but the requested shape has 150528\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32](DecodeRaw, Reshape/shape)]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,224,224,3], [?,?], [?]], output_types=[DT_UINT8, DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n_\r\n\r\nnote: i have totally 50000 images, i originally set batch_size to 16, so the num_per_epoch is 3125.\r\n\r\n**Source code**\r\n`def _parse_function(record):\r\n\r\n    features = {'image': tf.FixedLenFeature([], tf.string), 'label': tf.FixedLenFeature([], tf.string),\r\n                'id': tf.FixedLenFeature([], tf.string)}\r\n    example = tf.parse_single_example(record, features=features)\r\n    image = tf.decode_raw(example['image'], tf.uint8)\r\n    image = tf.reshape(image, [224, 224, 3])\r\n    label = tf.subtract(tf.decode_raw(example['label'], tf.uint8), 48)\r\n    id = tf.cast(example['id'], tf.string)\r\n\r\n    return image, label, id\r\n\r\n\r\ndef run_training():\r\n\r\n    # learning_rate = tf.placeholder(dtype=tf.float32)\r\n    train_filenames = ['I:/JD_fashion_AI/train_tfrecords/train.tfrecords']\r\n    validation_filenames = ['I:/JD_fashion_AI/valid_tfrecords/valid.tfrecords']\r\n    train_datasets = tf.data.TFRecordDataset(filenames=train_filenames)\r\n    validation_datasets = tf.data.TFRecordDataset(filenames=validation_filenames)\r\n    train_datasets = train_datasets.map(_parse_function).shuffle(buffer_size=1000).batch(16).repeat()\r\n    validation_datasets = validation_datasets.map(_parse_function).batch(1).repeat(1)\r\n    iterator = tf.data.Iterator.from_structure(train_datasets.output_types,\r\n                                               train_datasets.output_shapes)\r\n    image, label, id = iterator.get_next()\r\n    training_init_op = iterator.make_initializer(train_datasets)\r\n    validation_init_op = iterator.make_initializer(validation_datasets)\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    with tf.Session() as sess:\r\n\r\n        sess.run(init)\r\n        step = 0\r\n        for _ in range(18):\r\n            sess.run(training_init_op)  # must initialize the iterator to read the data flow\r\n            for _ in range(3125):\r\n                \r\n                sess.run(label)\r\n                print(step)\r\n`\r\nanyone else know what happened ?"}