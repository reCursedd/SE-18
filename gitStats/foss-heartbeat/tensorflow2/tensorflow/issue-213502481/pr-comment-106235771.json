{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106235771", "pull_request_review_id": 27148410, "id": 106235771, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNjIzNTc3MQ==", "diff_hunk": "@@ -0,0 +1,390 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n+\r\n+Licensed under the Apache License, Version 2.0 (the \"License\");\r\n+you may not use this file except in compliance with the License.\r\n+You may obtain a copy of the License at\r\n+\r\n+    http://www.apache.org/licenses/LICENSE-2.0\r\n+\r\n+Unless required by applicable law or agreed to in writing, software\r\n+distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+See the License for the specific language governing permissions and\r\n+limitations under the License.\r\n+==============================================================================*/\r\n+\r\n+// See docs in ../ops/nn_ops.cc.\r\n+#ifdef INTEL_MKL\r\n+\r\n+#include \"tensorflow/core/framework/numeric_op.h\"\r\n+#include \"tensorflow/core/framework/op_kernel.h\"\r\n+#include \"tensorflow/core/framework/register_types.h\"\r\n+#include \"tensorflow/core/framework/tensor.h\"\r\n+#include \"tensorflow/core/lib/core/errors.h\"\r\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n+\r\n+#include \"tensorflow/core/common_runtime/mkl_layer_registry.h\"\r\n+#include \"tensorflow/core/platform/default/logging.h\"\r\n+#include \"tensorflow/core/util/mkl_util.h\"\r\n+#include \"third_party/mkl/include/mkl_dnn.h\"\r\n+#include \"third_party/mkl/include/mkl_dnn_types.h\"\r\n+\r\n+namespace tensorflow {\r\n+\r\n+typedef Eigen::ThreadPoolDevice CPUDevice;\r\n+typedef Eigen::GpuDevice GPUDevice;\r\n+\r\n+struct MklReluHelpers {\r\n+  static void ValidateSameSizeHelper(OpKernelContext* context, const Tensor& g,\r\n+                                     const Tensor& a) {\r\n+    OP_REQUIRES(context, a.IsSameSize(g),\r\n+                errors::InvalidArgument(\"g and a must be the same size\"));\r\n+  }\r\n+  static bool ValidateSameSize(OpKernelContext* context, const Tensor& g,\r\n+                               const Tensor& a) {\r\n+    ValidateSameSizeHelper(context, g, a);\r\n+    return context->status().ok();\r\n+  }\r\n+};\r\n+\r\n+template <typename Device, typename T>\r\n+class MklReluOp : public OpKernel {\r\n+ public:\r\n+  ~MklReluOp() {}\r\n+  explicit MklReluOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n+  void Compute(OpKernelContext* context) override {\r\n+    const Tensor& input = MklGetInput(context, 0);\r\n+    GetMklShape(context, 0, &mkl_params.input_shape);\r\n+    void* user_i = static_cast<void*>(const_cast<T*>(input.flat<T>().data()));\r\n+    bool input_in_mkl_format = mkl_params.input_shape.IsMklTensor();\r\n+    if (!input_in_mkl_format && !input.dims()) {  // handle the case of a scalar\r\n+      const TensorShape& o_shape = input.shape();\r\n+      Tensor* out_tensor = nullptr;\r\n+      mkl_params.output_shape.SetMklTensor(false);\r\n+      AllocateOutputSetMklshape(context, 0, &out_tensor, o_shape,\r\n+                                mkl_params.output_shape);\r\n+      void* out_o = static_cast<void*>(out_tensor->flat<T>().data());\r\n+      (static_cast<T*>(out_o))[0] =\r\n+          std::max((static_cast<T*>(user_i))[0], static_cast<T>(0));\r\n+      return;\r\n+    }\r\n+\r\n+    if (input_in_mkl_format) {\r\n+      mkl_params.in_dims = mkl_params.input_shape.GetDimension();\r\n+      mkl_params.in_sizes = new size_t[mkl_params.in_dims];\r\n+      mkl_params.in_strides = new size_t[mkl_params.in_dims];\r\n+      for (int i = 0; i < mkl_params.in_dims; i++) {\r\n+        mkl_params.in_sizes[i] = mkl_params.input_shape.GetSizes()[i];\r\n+        mkl_params.in_strides[i] = mkl_params.input_shape.GetStrides()[i];\r\n+      }\r\n+    } else {\r\n+      mkl_params.in_dims = input.dims();\r\n+      mkl_params.in_sizes = new size_t[mkl_params.in_dims];\r\n+      mkl_params.in_strides = new size_t[mkl_params.in_dims];\r\n+      for (int i = 0; i < mkl_params.in_dims; i++) {\r\n+        mkl_params.in_sizes[i] = input.dim_size((mkl_params.in_dims - 1) - i);\r\n+      }\r\n+      mkl_params.in_strides[0] = 1;\r\n+      for (int i = 1; i < mkl_params.in_dims; i++) {\r\n+        mkl_params.in_strides[i] = mkl_params.in_strides[i - 1]\r\n+            * mkl_params.in_sizes[i - 1];\r\n+      }\r\n+    }\r\n+\r\n+    float negative_slope = 0.0;\r\n+    MklCreateInputLayouts(context);\r\n+    CHECK_EQ(dnnReLUCreateForward_F32(\r\n+                 &mkl_prim_relu_fwd_, NULL,\r\n+                 mkl_lt_input_,\r\n+                 negative_slope),\r\n+             E_SUCCESS);\r\n+\r\n+    Tensor* output = nullptr;\r\n+\r\n+    if (input_in_mkl_format) {\r\n+      TensorShape tf_shape;\r\n+      mkl_params.output_shape.SetMklTensor(true);\r\n+      mkl_params.output_shape.SetMklLayout(mkl_prim_relu_fwd_, dnnResourceDst);\r\n+      mkl_params.output_shape.SetTfLayout(mkl_params.in_dims,\r\n+          mkl_params.in_sizes, mkl_params.in_strides);\r\n+      tf_shape.AddDim(dnnLayoutGetMemorySize_F32\r\n+          (static_cast<dnnLayout_t>(\r\n+            mkl_params.output_shape.GetMklLayout())) /\r\n+               sizeof(T));\r\n+      AllocateOutputSetMklshape(context, 0, &output,\r\n+          tf_shape, mkl_params.output_shape);\r\n+    } else {\r\n+      const TensorShape& o_shape = input.shape();\r\n+      mkl_params.output_shape.SetMklTensor(false);\r\n+      AllocateOutputSetMklshape(context, 0, &output,\r\n+          o_shape, mkl_params.output_shape);\r\n+    }\r\n+\r\n+    void* user_o = static_cast<void*>(const_cast<T*>(output->flat<T>().data()));\r\n+\r\n+    relu_res[dnnResourceDst] = user_o;\r\n+    relu_res[dnnResourceSrc] = user_i;\r\n+    CHECK_EQ(dnnExecute_F32(mkl_prim_relu_fwd_, relu_res), E_SUCCESS);\r\n+    Mklcleanup();\r\n+  }\r\n+\r\n+ private:\r\n+  typedef struct {\r\n+    int in_dims;\r\n+    size_t *in_sizes;\r\n+    size_t *in_strides;\r\n+    MklShape input_shape, output_shape;\r\n+  } MklReluOpParams_;\r\n+\r\n+  void Mklcleanup() {\r\n+    bool input_in_mkl_format = mkl_params.input_shape.IsMklTensor();\r\n+    if (!input_in_mkl_format) dnnLayoutDelete_F32(mkl_lt_input_);\r\n+    dnnDelete_F32(mkl_prim_relu_fwd_);\r\n+  }\r\n+  void MklCreateInputLayouts(OpKernelContext* context) {\r\n+    bool input_in_mkl_format = mkl_params.input_shape.IsMklTensor();\r\n+    if (!input_in_mkl_format) {\r\n+      CHECK_EQ(dnnLayoutCreate_F32(&mkl_lt_input_,\r\n+          mkl_params.in_dims, mkl_params.in_sizes,\r\n+                            mkl_params.in_strides),\r\n+               E_SUCCESS);\r\n+    } else {\r\n+      mkl_lt_input_ = static_cast<dnnLayout_t>\r\n+          (mkl_params.input_shape.GetCurLayout());\r\n+    }\r\n+  }\r\n+\r\n+  dnnPrimitive_t mkl_prim_relu_fwd_ = nullptr;\r\n+  MklReluOpParams_ mkl_params;\r\n+  void* relu_res[dnnResourceNumber];\r\n+  dnnLayout_t mkl_lt_input_ = nullptr;\r\n+};\r\n+\r\n+template <typename Device, typename T>\r\n+class MklReluGradOp : public OpKernel {\r\n+ public:\r\n+  ~MklReluGradOp() {}\r\n+  explicit MklReluGradOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n+  void Compute(OpKernelContext* context) override;\r\n+\r\n+ private:\r\n+  typedef struct {\r\n+    int in_dims;\r\n+    size_t *in_sizes;\r\n+    size_t *in_strides;\r\n+    MklShape input_shape, grad_shape, output_shape;\r\n+  } MklReluGradOpParams_;\r\n+  MklReluGradOpParams_ mkl_params;\r\n+\r\n+  void MklPrepareReluGradInputs(OpKernelContext* context,\r\n+                                   Tensor* mkl_tmp_grad_buf_tensor,\r\n+                                   Tensor* mkl_tmp_input_buf_tensor) {\r\n+  dnnPrimitive_t cv_user_to_reluB_input = nullptr,\r\n+      cv_user_to_reluB_grad = nullptr;\r\n+  dnnLayout_t mkl_lt_internal_input = nullptr, mkl_lt_internal_grad = nullptr;\r\n+\r\n+  const Tensor& g = MklGetInput(context, 0);\r\n+  const Tensor& a = MklGetInput(context, 1);\r\n+\r\n+  void* user_i = static_cast<void*>(const_cast<T*>(a.flat<T>().data()));\r\n+  void* user_g = static_cast<void*>(const_cast<T*>(g.flat<T>().data()));\r\n+\r\n+  CHECK_EQ(dnnLayoutCreateFromPrimitive_F32(&mkl_lt_internal_grad,\r\n+      mkl_prim_relu_back_,\r\n+     dnnResourceDiffDst),\r\n+           E_SUCCESS);\r\n+\r\n+  CHECK_EQ(dnnLayoutCreateFromPrimitive_F32(&mkl_lt_internal_input,\r\n+      mkl_prim_relu_back_,\r\n+      dnnResourceSrc),\r\n+           E_SUCCESS);\r\n+\r\n+  if (!dnnLayoutCompare_F32(mkl_lt_internal_grad, mkl_lt_grad_)) {\r\n+    AllocTmpBuffer(context, mkl_tmp_grad_buf_tensor, mkl_lt_internal_grad,\r\n+                   &relu_res[dnnResourceDiffDst]);\r\n+    CHECK_EQ(dnnConversionCreate_F32(&cv_user_to_reluB_grad, mkl_lt_grad_,\r\n+                                     mkl_lt_internal_grad),\r\n+             E_SUCCESS);\r\n+  }\r\n+\r\n+  if (!dnnLayoutCompare_F32(mkl_lt_internal_input, mkl_lt_input_)) {\r", "path": "tensorflow/core/kernels/mkl_relu_op.cc", "position": null, "original_position": 210, "commit_id": "3117fe65016b423879b7c944cb5cd5754db42347", "original_commit_id": "84ccc8a79b341584a78d16039279c3f7b44a419f", "user": {"login": "andydavis1", "id": 15696327, "node_id": "MDQ6VXNlcjE1Njk2MzI3", "avatar_url": "https://avatars0.githubusercontent.com/u/15696327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andydavis1", "html_url": "https://github.com/andydavis1", "followers_url": "https://api.github.com/users/andydavis1/followers", "following_url": "https://api.github.com/users/andydavis1/following{/other_user}", "gists_url": "https://api.github.com/users/andydavis1/gists{/gist_id}", "starred_url": "https://api.github.com/users/andydavis1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andydavis1/subscriptions", "organizations_url": "https://api.github.com/users/andydavis1/orgs", "repos_url": "https://api.github.com/users/andydavis1/repos", "events_url": "https://api.github.com/users/andydavis1/events{/privacy}", "received_events_url": "https://api.github.com/users/andydavis1/received_events", "type": "User", "site_admin": false}, "body": "These parameter names are very cryptic. If you can't make them more descriptive, then please add comments explaining each parameter.", "created_at": "2017-03-15T17:45:42Z", "updated_at": "2017-03-23T17:07:46Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8296#discussion_r106235771", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8296", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106235771"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8296#discussion_r106235771"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8296"}}, "body_html": "<p>These parameter names are very cryptic. If you can't make them more descriptive, then please add comments explaining each parameter.</p>", "body_text": "These parameter names are very cryptic. If you can't make them more descriptive, then please add comments explaining each parameter."}