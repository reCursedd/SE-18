{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13244", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13244/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13244/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13244/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13244", "id": 259914370, "node_id": "MDU6SXNzdWUyNTk5MTQzNzA=", "number": 13244, "title": "BUG:Memory leak in tf.string_split", "user": {"login": "sushanthku", "id": 1033383, "node_id": "MDQ6VXNlcjEwMzMzODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1033383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sushanthku", "html_url": "https://github.com/sushanthku", "followers_url": "https://api.github.com/users/sushanthku/followers", "following_url": "https://api.github.com/users/sushanthku/following{/other_user}", "gists_url": "https://api.github.com/users/sushanthku/gists{/gist_id}", "starred_url": "https://api.github.com/users/sushanthku/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sushanthku/subscriptions", "organizations_url": "https://api.github.com/users/sushanthku/orgs", "repos_url": "https://api.github.com/users/sushanthku/repos", "events_url": "https://api.github.com/users/sushanthku/events{/privacy}", "received_events_url": "https://api.github.com/users/sushanthku/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-09-22T19:30:17Z", "updated_at": "2018-01-04T01:59:19Z", "closed_at": "2018-01-04T01:59:19Z", "author_association": "NONE", "body_html": "<p><a href=\"https://github.com/tensorflow/tensorflow/files/1325612/profile_9734.0066.txt\">profile_9734.0066.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1325613/profile_9734.0100.txt\">profile_9734.0100.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1325611/profile_9734.0150.txt\">profile_9734.0150.txt</a></p>\n<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug or a feature request.</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Centos Linux version 3.10.0-229.4.2.el7.x86_64 (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) )</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install tensorflow</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow==1.3.0</li>\n<li><strong>Python version</strong>: Python 2.7.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A (CPU only)</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.<br>\nI am seeing noticeably large memory usage when I use <code>tf.string_split</code>  within the map function of a dataset API.  I have attached a sample code below. I tried to do a heap analysis and I see <code>std::basic_string::_Rep::_S_create</code> constantly growing in size and not freeing up its memory.<br>\nIf i remove the <code>tf.string_split</code> and just return the line as is, there is no memory held over. This issue is a blocker for us to scale up the tensorflow pipeline to large datasets.<br>\nI have attached three output files of pprof over time .<br>\n<code>6447.1  96.8%  96.8%   6447.1  96.8% std::basic_string::_Rep::_S_create</code><br>\n<code>  9765.5  96.5%  96.5%   9765.5  96.5% std::basic_string::_Rep::_S_create</code><br>\n<code> 14704.7  95.5%  95.5%  14704.7  95.5% std::basic_string::_Rep::_S_create</code></p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<pre><code>import tensorflow as tf\n\ndef mapper(line):\n    line = tf.identity(line)\n    tokens = tf.string_split(line, delimiter='\\t')\n    return tokens.indices\n\ndef run():\n    filenames = \"sample.txt\"\n    # Cluster spec\n    cluster = tf.train.ClusterSpec({\n        \"worker\": [\"localhost:2223\"]\n    })\n    server = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\n\n    dataset = tf.contrib.data.TextLineDataset(filenames)\n    dataset = dataset.batch(1000)\n    dataset = dataset.map(mapper, 8).repeat()\n    iterator = dataset.make_one_shot_iterator()\n    next_element = iterator.get_next()\n\n    with tf.Session(target=server.target) as session:\n        while True:\n            try:\n                session.run(next_element)\n            except tf.errors.OutOfRangeError:\n                break\nrun()\n</code></pre>\n<p>#You can run this script by<br>\n<code>LD_PRELOAD=/usr/lib64/libtcmalloc.so.4 HEAPPROFILE=/tmp/profile nohup python -u bug.py &gt; output.log &amp;</code></p>", "body_text": "profile_9734.0066.txt\nprofile_9734.0100.txt\nprofile_9734.0150.txt\nPlease go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug or a feature request.\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Centos Linux version 3.10.0-229.4.2.el7.x86_64 (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) )\nTensorFlow installed from (source or binary): pip install tensorflow\nTensorFlow version (use command below): tensorflow==1.3.0\nPython version: Python 2.7.5\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A (CPU only)\nGPU model and memory: N/A\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nI am seeing noticeably large memory usage when I use tf.string_split  within the map function of a dataset API.  I have attached a sample code below. I tried to do a heap analysis and I see std::basic_string::_Rep::_S_create constantly growing in size and not freeing up its memory.\nIf i remove the tf.string_split and just return the line as is, there is no memory held over. This issue is a blocker for us to scale up the tensorflow pipeline to large datasets.\nI have attached three output files of pprof over time .\n6447.1  96.8%  96.8%   6447.1  96.8% std::basic_string::_Rep::_S_create\n  9765.5  96.5%  96.5%   9765.5  96.5% std::basic_string::_Rep::_S_create\n 14704.7  95.5%  95.5%  14704.7  95.5% std::basic_string::_Rep::_S_create\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nimport tensorflow as tf\n\ndef mapper(line):\n    line = tf.identity(line)\n    tokens = tf.string_split(line, delimiter='\\t')\n    return tokens.indices\n\ndef run():\n    filenames = \"sample.txt\"\n    # Cluster spec\n    cluster = tf.train.ClusterSpec({\n        \"worker\": [\"localhost:2223\"]\n    })\n    server = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\n\n    dataset = tf.contrib.data.TextLineDataset(filenames)\n    dataset = dataset.batch(1000)\n    dataset = dataset.map(mapper, 8).repeat()\n    iterator = dataset.make_one_shot_iterator()\n    next_element = iterator.get_next()\n\n    with tf.Session(target=server.target) as session:\n        while True:\n            try:\n                session.run(next_element)\n            except tf.errors.OutOfRangeError:\n                break\nrun()\n\n#You can run this script by\nLD_PRELOAD=/usr/lib64/libtcmalloc.so.4 HEAPPROFILE=/tmp/profile nohup python -u bug.py > output.log &", "body": "[profile_9734.0066.txt](https://github.com/tensorflow/tensorflow/files/1325612/profile_9734.0066.txt)\r\n[profile_9734.0100.txt](https://github.com/tensorflow/tensorflow/files/1325613/profile_9734.0100.txt)\r\n[profile_9734.0150.txt](https://github.com/tensorflow/tensorflow/files/1325611/profile_9734.0150.txt)\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Centos Linux version 3.10.0-229.4.2.el7.x86_64 (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) )\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n- **TensorFlow version (use command below)**: tensorflow==1.3.0\r\n- **Python version**: Python 2.7.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A (CPU only)\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:  \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI am seeing noticeably large memory usage when I use `tf.string_split`  within the map function of a dataset API.  I have attached a sample code below. I tried to do a heap analysis and I see `std::basic_string::_Rep::_S_create` constantly growing in size and not freeing up its memory. \r\nIf i remove the `tf.string_split` and just return the line as is, there is no memory held over. This issue is a blocker for us to scale up the tensorflow pipeline to large datasets. \r\nI have attached three output files of pprof over time . \r\n`6447.1  96.8%  96.8%   6447.1  96.8% std::basic_string::_Rep::_S_create`\r\n`  9765.5  96.5%  96.5%   9765.5  96.5% std::basic_string::_Rep::_S_create`\r\n` 14704.7  95.5%  95.5%  14704.7  95.5% std::basic_string::_Rep::_S_create`\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n    import tensorflow as tf\r\n\r\n    def mapper(line):\r\n        line = tf.identity(line)\r\n        tokens = tf.string_split(line, delimiter='\\t')\r\n        return tokens.indices\r\n\r\n    def run():\r\n        filenames = \"sample.txt\"\r\n        # Cluster spec\r\n        cluster = tf.train.ClusterSpec({\r\n            \"worker\": [\"localhost:2223\"]\r\n        })\r\n        server = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\r\n\r\n        dataset = tf.contrib.data.TextLineDataset(filenames)\r\n        dataset = dataset.batch(1000)\r\n        dataset = dataset.map(mapper, 8).repeat()\r\n        iterator = dataset.make_one_shot_iterator()\r\n        next_element = iterator.get_next()\r\n\r\n        with tf.Session(target=server.target) as session:\r\n            while True:\r\n                try:\r\n                    session.run(next_element)\r\n                except tf.errors.OutOfRangeError:\r\n                    break\r\n    run()\r\n#You can run this script by \r\n`LD_PRELOAD=/usr/lib64/libtcmalloc.so.4 HEAPPROFILE=/tmp/profile nohup python -u bug.py > output.log &`"}