{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/132026389", "pull_request_review_id": 55064966, "id": 132026389, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMjAyNjM4OQ==", "diff_hunk": "@@ -285,12 +290,12 @@ Status GdrMemoryManager::Init() {\n   }\n \n #if GOOGLE_CUDA\n+  VisitableAllocator::Visitor cuda_alloc_visitor =\n+      std::bind(&GdrMemoryManager::InsertCUDAMemoryRegion, this, _1, _2);", "path": "tensorflow/contrib/gdr/gdr_memory_manager.cc", "position": null, "original_position": 24, "commit_id": "241c020c64410ca16683a7a7f42b223f422e5dae", "original_commit_id": "f9a9d186bfe3c118394f4cb308646b7560b4db91", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "If I understand what you're doing correctly, you're registering the entire GPU memory to have the CU_POINTER_ATTRIBUTE_SYNC_MEMOPS property.  This seems to be a technique that NVIDIA provides because it makes concurrent programming easier, but I would avoid it because it likely will also damage performance considerably.\r\n\r\nIt sounds like you suspect a race condition on either the RDMA read, or some use of the final value.   For background, here's how we generally handle this issue.\r\n\r\nTensorFlow tries hard to use as few synchronous CUDA calls as possible, and to introduce as few sync points as possible.   The techniques used rely on careful use of streams. The general discipline is that 4 streams are created for each GPU, by the [StreamGroupFactory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L182).  All compute Ops (i.e. CUDA kernels) are launched on the single compute stream, and the other three are used for memcpys: one for H2D, one for D2H, and one for D2D copies.  In the normal case where an op relies only on the completion of prior ops in the same stream, it can be launched async without danger or any other temporal dependency. In the case where it relies on prior completion of an op on another stream, we need to introduce a sync dependency, which can be done in more than one way.  If we need to wait for an ordinary compute op prior to e.g. a D2H copy, we can just introduce a wait on the current compute stream prior to the copy, as [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_util.cc#L214).   This will cause the i/o stream to wait for completion of all ops _pending at the time of call_ to complete before launching the next op added to itself.  \r\n\r\nIn the case where we need to wait for a memcpy to terminate on one stream before launching a compute on another, the technique used is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_util.cc#L168).  We don't want the compute stream to stall until the i/o is done, so we wait for the i/o to complete before queuing the compute op.  The wait is accomplished by a call the the EventMgr.\r\n\r\nIn our internal networking which uses GPUDirect, we use the EventMgr to ensure that we don't start a send before the op writing the buffer to be read has completed.  In the other direction, our RPC system provides a callback that executes only after the target buffer area has been written, and we use that callback to trigger subsequent ops that want to read that area.\r\n\r\n", "created_at": "2017-08-08T20:43:10Z", "updated_at": "2017-08-08T20:43:10Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r132026389", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/132026389"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r132026389"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392"}}, "body_html": "<p>If I understand what you're doing correctly, you're registering the entire GPU memory to have the CU_POINTER_ATTRIBUTE_SYNC_MEMOPS property.  This seems to be a technique that NVIDIA provides because it makes concurrent programming easier, but I would avoid it because it likely will also damage performance considerably.</p>\n<p>It sounds like you suspect a race condition on either the RDMA read, or some use of the final value.   For background, here's how we generally handle this issue.</p>\n<p>TensorFlow tries hard to use as few synchronous CUDA calls as possible, and to introduce as few sync points as possible.   The techniques used rely on careful use of streams. The general discipline is that 4 streams are created for each GPU, by the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L182\">StreamGroupFactory</a>.  All compute Ops (i.e. CUDA kernels) are launched on the single compute stream, and the other three are used for memcpys: one for H2D, one for D2H, and one for D2D copies.  In the normal case where an op relies only on the completion of prior ops in the same stream, it can be launched async without danger or any other temporal dependency. In the case where it relies on prior completion of an op on another stream, we need to introduce a sync dependency, which can be done in more than one way.  If we need to wait for an ordinary compute op prior to e.g. a D2H copy, we can just introduce a wait on the current compute stream prior to the copy, as <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_util.cc#L214\">here</a>.   This will cause the i/o stream to wait for completion of all ops <em>pending at the time of call</em> to complete before launching the next op added to itself.</p>\n<p>In the case where we need to wait for a memcpy to terminate on one stream before launching a compute on another, the technique used is <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_util.cc#L168\">here</a>.  We don't want the compute stream to stall until the i/o is done, so we wait for the i/o to complete before queuing the compute op.  The wait is accomplished by a call the the EventMgr.</p>\n<p>In our internal networking which uses GPUDirect, we use the EventMgr to ensure that we don't start a send before the op writing the buffer to be read has completed.  In the other direction, our RPC system provides a callback that executes only after the target buffer area has been written, and we use that callback to trigger subsequent ops that want to read that area.</p>", "body_text": "If I understand what you're doing correctly, you're registering the entire GPU memory to have the CU_POINTER_ATTRIBUTE_SYNC_MEMOPS property.  This seems to be a technique that NVIDIA provides because it makes concurrent programming easier, but I would avoid it because it likely will also damage performance considerably.\nIt sounds like you suspect a race condition on either the RDMA read, or some use of the final value.   For background, here's how we generally handle this issue.\nTensorFlow tries hard to use as few synchronous CUDA calls as possible, and to introduce as few sync points as possible.   The techniques used rely on careful use of streams. The general discipline is that 4 streams are created for each GPU, by the StreamGroupFactory.  All compute Ops (i.e. CUDA kernels) are launched on the single compute stream, and the other three are used for memcpys: one for H2D, one for D2H, and one for D2D copies.  In the normal case where an op relies only on the completion of prior ops in the same stream, it can be launched async without danger or any other temporal dependency. In the case where it relies on prior completion of an op on another stream, we need to introduce a sync dependency, which can be done in more than one way.  If we need to wait for an ordinary compute op prior to e.g. a D2H copy, we can just introduce a wait on the current compute stream prior to the copy, as here.   This will cause the i/o stream to wait for completion of all ops pending at the time of call to complete before launching the next op added to itself.\nIn the case where we need to wait for a memcpy to terminate on one stream before launching a compute on another, the technique used is here.  We don't want the compute stream to stall until the i/o is done, so we wait for the i/o to complete before queuing the compute op.  The wait is accomplished by a call the the EventMgr.\nIn our internal networking which uses GPUDirect, we use the EventMgr to ensure that we don't start a send before the op writing the buffer to be read has completed.  In the other direction, our RPC system provides a callback that executes only after the target buffer area has been written, and we use that callback to trigger subsequent ops that want to read that area."}