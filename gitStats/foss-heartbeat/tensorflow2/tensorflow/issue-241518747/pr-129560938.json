{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392", "id": 129560938, "node_id": "MDExOlB1bGxSZXF1ZXN0MTI5NTYwOTM4", "html_url": "https://github.com/tensorflow/tensorflow/pull/11392", "diff_url": "https://github.com/tensorflow/tensorflow/pull/11392.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/11392.patch", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11392", "number": 11392, "state": "closed", "locked": false, "title": "GPUDirect RDMA Out-of-Band Tensor Transport", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "body": "Introduction\r\n===\r\n\r\nThis PR implements GDR out-of-band transport for TensorFlow distributed runtime, complementary to current gRPC transport. It uses gRPC as control plane to setup rendezvous for each tensor transmission, and utilizes [GPU Direct RDMA](https://developer.nvidia.com/gpudirect) whenever possible to transmit tensors in remote GPU memory through network interface card (NIC), bypassing host memory and CPU entirely. It gracefully falls back to ordinary RDMA or even gRPC when GDR is not available.\r\n\r\nDesign\r\n===\r\n\r\nThe GDR out-of-band transport is designed to avoid any unnecessary memory copies, especially for large tensors (>100MB). That typically requires registration of tensor buffers to NIC on the fly, which is rather slow as described in the design trade-off of the verbs runtime. The verbs runtime thus chooses to manage its own NIC-registered buffers and copy the tensors from/to those buffers for every single tensor transfer.\r\n\r\nWe show that, however, such design trade-off is not always relevant. In this patch, we manage both computation and communication buffers in a unified manner. By pre-registration of large buffers to NIC and allocating small tensors from the buffer pool using a BFC allocator, it is possible to avoid both buffer registration on the fly and memory copies all together.\r\n\r\nFor the actual tensor transport, we rely on gRPC to transmit the remote buffer information. This greatly simplifies our design, and there are only 2 types of RDMA messages: a single READ to retrieve the tensor data (bypassing remote CPU), and another invalidate using WRITE with IMM to release the tensor buffer on the remote side. The remote side will only be polling the invalidate message and `Unref` the tensor buffers that read by its peer.\r\n\r\nEnvironment\r\n===\r\n\r\nTo fully utilize GDR, the target environment has to meet 3 conditions:\r\n\r\n1. There is an RDMA capable device with corresponding [OFED package](https://www.openfabrics.org/index.php/overview.html) installed (detailed information is available from your [Infiniband/RoCE](http://www.mellanox.com/page/products_dyn?product_family=116)/[iWarp](http://www.chelsio.com/gpudirect-rdma/) vendor), which could be verified through `ibv_devinfo`, e.g.\r\n\r\n```\r\n$ ibv_devinfo\r\nhca_id:\tmlx4_0\r\n\ttransport:\t\t\tInfiniBand (0)\r\n\tfw_ver:\t\t\t\t2.40.7000\r\n\tnode_guid:\t\t\t248a:0703:00f6:3370\r\n\tsys_image_guid:\t\t\t248a:0703:00f6:3370\r\n\tvendor_id:\t\t\t0x02c9\r\n\tvendor_part_id:\t\t\t4099\r\n\thw_ver:\t\t\t\t0x1\r\n\tboard_id:\t\t\tMT_1090110023\r\n\tphys_port_cnt:\t\t\t2\r\n\tDevice ports:\r\n\t\tport:\t1\r\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\r\n\t\t\tmax_mtu:\t\t4096 (5)\r\n\t\t\tactive_mtu:\t\t1024 (3)\r\n\t\t\tsm_lid:\t\t\t0\r\n\t\t\tport_lid:\t\t0\r\n\t\t\tport_lmc:\t\t0x00\r\n\t\t\tlink_layer:\t\tEthernet\r\n\r\n\t\tport:\t2\r\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\r\n\t\t\tmax_mtu:\t\t4096 (5)\r\n\t\t\tactive_mtu:\t\t1024 (3)\r\n\t\t\tsm_lid:\t\t\t0\r\n\t\t\tport_lid:\t\t0\r\n\t\t\tport_lmc:\t\t0x00\r\n\t\t\tlink_layer:\t\tEthernet\r\n```\r\n\r\n2. There is a GDR capable GPU, i.e. of Fermi, Kepler or later architecture with [corresponding driver](http://docs.nvidia.com/cuda/gpudirect-rdma/index.html) installed. The PCI-e topology could be confirmed by `nvidia-smi topo -m`. For example, in the following topology, `GPU2` and `GPU3` are adjacent to `mlx4_0`, and tensors on these devices could benefit from GDR in current implementation.\r\n\r\n```\r\n$ nvidia-smi topo -m\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tmlx4_0\tCPU Affinity\r\nGPU0\t X \tPHB\tSOC\tSOC\tSOC\t0-5\r\nGPU1\tPHB\t X \tSOC\tSOC\tSOC\t0-5\r\nGPU2\tSOC\tSOC\t X \tPHB\tPHB\t6-11\r\nGPU3\tSOC\tSOC\tPHB\t X \tPHB\t6-11\r\nmlx4_0\tSOC\tSOC\tPHB\tPHB\t X\r\n\r\nLegend:\r\n\r\n  X   = Self\r\n  SOC  = Connection traversing PCIe as well as the SMP link between CPU sockets(e.g. QPI)\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n3. The [`nv_peer_mem`](https://github.com/Mellanox/nv_peer_memory) kernel module is installed.\r\n\r\nHow to build and run in GDR mode\r\n===\r\n\r\nTo test it out on a GDR capable environment, choose to enable GDR in your configure script.\r\n\r\n```\r\nDo you wish to build TensorFlow with GDR support? [y/N]: y\r\nGDR support will be enabled for TensorFlow.\r\n```\r\n\r\nChange your `protocol` to `grpc+gdr` to enable GDR in your deployment.\r\n\r\n```\r\nserver = tf.train.Server(cluster, job_name=\"local\", task_index=0, protocol='grpc+gdr') # default protocol is 'grpc'\r\n```\r\n\r\nCurrently the out-of-band transport service listens to the same IP and port address as specified in gRPC.\r\n\r\nA successful initialization looks like this:\r\n\r\n```\r\n2017-08-05 19:10:38.601718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0)\r\n2017-08-05 19:10:38.601728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0)\r\n2017-08-05 19:10:38.601736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0)\r\n2017-08-05 19:10:38.601742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0)\r\n2017-08-05 19:10:39.591026: I tensorflow/contrib/gdr/gdr_memory_manager.cc:235] RDMA server is listening on 10.40.2.200:5001\r\n2017-08-05 19:10:39.591071: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cuda_host_bfc\r\n2017-08-05 19:10:39.591083: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_pool\r\n2017-08-05 19:10:39.591095: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_rdma_bfc\r\n2017-08-05 19:10:39.591278: I tensorflow/contrib/gdr/gdr_memory_manager.cc:78] NUMA node for device: mlx4_0 is 1\r\n2017-08-05 19:10:39.740253: I tensorflow/contrib/gdr/gdr_memory_manager.cc:296] Instrumenting GPU allocator with bus_id 2\r\n```\r\n\r\nThe last line suggests that the GPUs with bus id 2 (mapped to pci bus id prefixed 0000:8) will benefit from GDR and host memory bypass, which is `/gpu:2` and `/gpu:3` in this case.\r\n\r\nCaveats\r\n===\r\n\r\nIn current implementation, only tensors that reside in host memory or in GPU memory such that the GPU is adjacent to an RDMA capable NIC will use direct RDMA as its transport. When RDMA is available but not GDR, a temporary tensor copy on host memory will be used as RDMA source/destination (and copied from/to the target device). When there is no RDMA device present, it can even fallback to the original gRPC runtime. While it is theoretically possible to mix GDR enabled TF with non-GDR deployments in the same job, make sure the environment is properly setup so the GDR mode is enabled whenever possible (i.e. do not fall back to gRPC when it is not absolutely necessary).", "created_at": "2017-07-09T11:33:03Z", "updated_at": "2017-10-23T15:13:42Z", "closed_at": "2017-08-09T15:58:13Z", "merged_at": "2017-08-09T15:58:13Z", "merge_commit_sha": "11e2aef14f7f2d862363c350ca1d67b87ea6a57b", "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "requested_reviewers": [], "requested_teams": [], "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "milestone": null, "commits_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392/commits", "review_comments_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392/comments", "review_comment_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11392/comments", "statuses_url": "https://api.github.com/repos/tensorflow/tensorflow/statuses/241c020c64410ca16683a7a7f42b223f422e5dae", "head": {"label": null, "ref": "gpudirect", "sha": "241c020c64410ca16683a7a7f42b223f422e5dae", "user": null, "repo": null}, "base": {"label": "tensorflow:master", "ref": "master", "sha": "6e054dbd4b741d5b8fa8af93fdd7c9b74ae67ce0", "user": {"login": "tensorflow", "id": 15658638, "node_id": "MDEyOk9yZ2FuaXphdGlvbjE1NjU4NjM4", "avatar_url": "https://avatars1.githubusercontent.com/u/15658638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tensorflow", "html_url": "https://github.com/tensorflow", "followers_url": "https://api.github.com/users/tensorflow/followers", "following_url": "https://api.github.com/users/tensorflow/following{/other_user}", "gists_url": "https://api.github.com/users/tensorflow/gists{/gist_id}", "starred_url": "https://api.github.com/users/tensorflow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tensorflow/subscriptions", "organizations_url": "https://api.github.com/users/tensorflow/orgs", "repos_url": "https://api.github.com/users/tensorflow/repos", "events_url": "https://api.github.com/users/tensorflow/events{/privacy}", "received_events_url": "https://api.github.com/users/tensorflow/received_events", "type": "Organization", "site_admin": false}, "repo": {"id": 45717250, "node_id": "MDEwOlJlcG9zaXRvcnk0NTcxNzI1MA==", "name": "tensorflow", "full_name": "tensorflow/tensorflow", "private": false, "owner": {"login": "tensorflow", "id": 15658638, "node_id": "MDEyOk9yZ2FuaXphdGlvbjE1NjU4NjM4", "avatar_url": "https://avatars1.githubusercontent.com/u/15658638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tensorflow", "html_url": "https://github.com/tensorflow", "followers_url": "https://api.github.com/users/tensorflow/followers", "following_url": "https://api.github.com/users/tensorflow/following{/other_user}", "gists_url": "https://api.github.com/users/tensorflow/gists{/gist_id}", "starred_url": "https://api.github.com/users/tensorflow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tensorflow/subscriptions", "organizations_url": "https://api.github.com/users/tensorflow/orgs", "repos_url": "https://api.github.com/users/tensorflow/repos", "events_url": "https://api.github.com/users/tensorflow/events{/privacy}", "received_events_url": "https://api.github.com/users/tensorflow/received_events", "type": "Organization", "site_admin": false}, "html_url": "https://github.com/tensorflow/tensorflow", "description": "An Open Source Machine Learning Framework for Everyone", "fork": false, "url": "https://api.github.com/repos/tensorflow/tensorflow", "forks_url": "https://api.github.com/repos/tensorflow/tensorflow/forks", "keys_url": "https://api.github.com/repos/tensorflow/tensorflow/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/tensorflow/tensorflow/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/tensorflow/tensorflow/teams", "hooks_url": "https://api.github.com/repos/tensorflow/tensorflow/hooks", "issue_events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/events{/number}", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/events", "assignees_url": "https://api.github.com/repos/tensorflow/tensorflow/assignees{/user}", "branches_url": "https://api.github.com/repos/tensorflow/tensorflow/branches{/branch}", "tags_url": "https://api.github.com/repos/tensorflow/tensorflow/tags", "blobs_url": "https://api.github.com/repos/tensorflow/tensorflow/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/tensorflow/tensorflow/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/tensorflow/tensorflow/git/refs{/sha}", "trees_url": "https://api.github.com/repos/tensorflow/tensorflow/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/tensorflow/tensorflow/statuses/{sha}", "languages_url": "https://api.github.com/repos/tensorflow/tensorflow/languages", "stargazers_url": "https://api.github.com/repos/tensorflow/tensorflow/stargazers", "contributors_url": "https://api.github.com/repos/tensorflow/tensorflow/contributors", "subscribers_url": "https://api.github.com/repos/tensorflow/tensorflow/subscribers", "subscription_url": "https://api.github.com/repos/tensorflow/tensorflow/subscription", "commits_url": "https://api.github.com/repos/tensorflow/tensorflow/commits{/sha}", "git_commits_url": "https://api.github.com/repos/tensorflow/tensorflow/git/commits{/sha}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/comments{/number}", "issue_comment_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments{/number}", "contents_url": "https://api.github.com/repos/tensorflow/tensorflow/contents/{+path}", "compare_url": "https://api.github.com/repos/tensorflow/tensorflow/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/tensorflow/tensorflow/merges", "archive_url": "https://api.github.com/repos/tensorflow/tensorflow/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/tensorflow/tensorflow/downloads", "issues_url": "https://api.github.com/repos/tensorflow/tensorflow/issues{/number}", "pulls_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls{/number}", "milestones_url": "https://api.github.com/repos/tensorflow/tensorflow/milestones{/number}", "notifications_url": "https://api.github.com/repos/tensorflow/tensorflow/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/labels{/name}", "releases_url": "https://api.github.com/repos/tensorflow/tensorflow/releases{/id}", "deployments_url": "https://api.github.com/repos/tensorflow/tensorflow/deployments", "created_at": "2015-11-07T01:19:20Z", "updated_at": "2018-11-24T20:25:08Z", "pushed_at": "2018-11-24T18:40:19Z", "git_url": "git://github.com/tensorflow/tensorflow.git", "ssh_url": "git@github.com:tensorflow/tensorflow.git", "clone_url": "https://github.com/tensorflow/tensorflow.git", "svn_url": "https://github.com/tensorflow/tensorflow", "homepage": "https://tensorflow.org", "size": 284546, "stargazers_count": 115177, "watchers_count": 115177, "language": "C++", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "forks_count": 69947, "mirror_url": null, "archived": false, "open_issues_count": 1759, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "forks": 69947, "open_issues": 1759, "watchers": 115177, "default_branch": "master"}}, "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11392"}, "issue": {"href": "https://api.github.com/repos/tensorflow/tensorflow/issues/11392"}, "comments": {"href": "https://api.github.com/repos/tensorflow/tensorflow/issues/11392/comments"}, "review_comments": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392/comments"}, "review_comment": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments{/number}"}, "commits": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392/commits"}, "statuses": {"href": "https://api.github.com/repos/tensorflow/tensorflow/statuses/241c020c64410ca16683a7a7f42b223f422e5dae"}}, "author_association": "CONTRIBUTOR", "body_html": "<h1>Introduction</h1>\n<p>This PR implements GDR out-of-band transport for TensorFlow distributed runtime, complementary to current gRPC transport. It uses gRPC as control plane to setup rendezvous for each tensor transmission, and utilizes <a href=\"https://developer.nvidia.com/gpudirect\" rel=\"nofollow\">GPU Direct RDMA</a> whenever possible to transmit tensors in remote GPU memory through network interface card (NIC), bypassing host memory and CPU entirely. It gracefully falls back to ordinary RDMA or even gRPC when GDR is not available.</p>\n<h1>Design</h1>\n<p>The GDR out-of-band transport is designed to avoid any unnecessary memory copies, especially for large tensors (&gt;100MB). That typically requires registration of tensor buffers to NIC on the fly, which is rather slow as described in the design trade-off of the verbs runtime. The verbs runtime thus chooses to manage its own NIC-registered buffers and copy the tensors from/to those buffers for every single tensor transfer.</p>\n<p>We show that, however, such design trade-off is not always relevant. In this patch, we manage both computation and communication buffers in a unified manner. By pre-registration of large buffers to NIC and allocating small tensors from the buffer pool using a BFC allocator, it is possible to avoid both buffer registration on the fly and memory copies all together.</p>\n<p>For the actual tensor transport, we rely on gRPC to transmit the remote buffer information. This greatly simplifies our design, and there are only 2 types of RDMA messages: a single READ to retrieve the tensor data (bypassing remote CPU), and another invalidate using WRITE with IMM to release the tensor buffer on the remote side. The remote side will only be polling the invalidate message and <code>Unref</code> the tensor buffers that read by its peer.</p>\n<h1>Environment</h1>\n<p>To fully utilize GDR, the target environment has to meet 3 conditions:</p>\n<ol>\n<li>There is an RDMA capable device with corresponding <a href=\"https://www.openfabrics.org/index.php/overview.html\" rel=\"nofollow\">OFED package</a> installed (detailed information is available from your <a href=\"http://www.mellanox.com/page/products_dyn?product_family=116\" rel=\"nofollow\">Infiniband/RoCE</a>/<a href=\"http://www.chelsio.com/gpudirect-rdma/\" rel=\"nofollow\">iWarp</a> vendor), which could be verified through <code>ibv_devinfo</code>, e.g.</li>\n</ol>\n<pre><code>$ ibv_devinfo\nhca_id:\tmlx4_0\n\ttransport:\t\t\tInfiniBand (0)\n\tfw_ver:\t\t\t\t2.40.7000\n\tnode_guid:\t\t\t248a:0703:00f6:3370\n\tsys_image_guid:\t\t\t248a:0703:00f6:3370\n\tvendor_id:\t\t\t0x02c9\n\tvendor_part_id:\t\t\t4099\n\thw_ver:\t\t\t\t0x1\n\tboard_id:\t\t\tMT_1090110023\n\tphys_port_cnt:\t\t\t2\n\tDevice ports:\n\t\tport:\t1\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n\t\t\tmax_mtu:\t\t4096 (5)\n\t\t\tactive_mtu:\t\t1024 (3)\n\t\t\tsm_lid:\t\t\t0\n\t\t\tport_lid:\t\t0\n\t\t\tport_lmc:\t\t0x00\n\t\t\tlink_layer:\t\tEthernet\n\n\t\tport:\t2\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n\t\t\tmax_mtu:\t\t4096 (5)\n\t\t\tactive_mtu:\t\t1024 (3)\n\t\t\tsm_lid:\t\t\t0\n\t\t\tport_lid:\t\t0\n\t\t\tport_lmc:\t\t0x00\n\t\t\tlink_layer:\t\tEthernet\n</code></pre>\n<ol start=\"2\">\n<li>There is a GDR capable GPU, i.e. of Fermi, Kepler or later architecture with <a href=\"http://docs.nvidia.com/cuda/gpudirect-rdma/index.html\" rel=\"nofollow\">corresponding driver</a> installed. The PCI-e topology could be confirmed by <code>nvidia-smi topo -m</code>. For example, in the following topology, <code>GPU2</code> and <code>GPU3</code> are adjacent to <code>mlx4_0</code>, and tensors on these devices could benefit from GDR in current implementation.</li>\n</ol>\n<pre><code>$ nvidia-smi topo -m\n\tGPU0\tGPU1\tGPU2\tGPU3\tmlx4_0\tCPU Affinity\nGPU0\t X \tPHB\tSOC\tSOC\tSOC\t0-5\nGPU1\tPHB\t X \tSOC\tSOC\tSOC\t0-5\nGPU2\tSOC\tSOC\t X \tPHB\tPHB\t6-11\nGPU3\tSOC\tSOC\tPHB\t X \tPHB\t6-11\nmlx4_0\tSOC\tSOC\tPHB\tPHB\t X\n\nLegend:\n\n  X   = Self\n  SOC  = Connection traversing PCIe as well as the SMP link between CPU sockets(e.g. QPI)\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n</code></pre>\n<ol start=\"3\">\n<li>The <a href=\"https://github.com/Mellanox/nv_peer_memory\"><code>nv_peer_mem</code></a> kernel module is installed.</li>\n</ol>\n<h1>How to build and run in GDR mode</h1>\n<p>To test it out on a GDR capable environment, choose to enable GDR in your configure script.</p>\n<pre><code>Do you wish to build TensorFlow with GDR support? [y/N]: y\nGDR support will be enabled for TensorFlow.\n</code></pre>\n<p>Change your <code>protocol</code> to <code>grpc+gdr</code> to enable GDR in your deployment.</p>\n<pre><code>server = tf.train.Server(cluster, job_name=\"local\", task_index=0, protocol='grpc+gdr') # default protocol is 'grpc'\n</code></pre>\n<p>Currently the out-of-band transport service listens to the same IP and port address as specified in gRPC.</p>\n<p>A successful initialization looks like this:</p>\n<pre><code>2017-08-05 19:10:38.601718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0)\n2017-08-05 19:10:38.601728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0)\n2017-08-05 19:10:38.601736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0)\n2017-08-05 19:10:38.601742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0)\n2017-08-05 19:10:39.591026: I tensorflow/contrib/gdr/gdr_memory_manager.cc:235] RDMA server is listening on 10.40.2.200:5001\n2017-08-05 19:10:39.591071: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cuda_host_bfc\n2017-08-05 19:10:39.591083: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_pool\n2017-08-05 19:10:39.591095: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_rdma_bfc\n2017-08-05 19:10:39.591278: I tensorflow/contrib/gdr/gdr_memory_manager.cc:78] NUMA node for device: mlx4_0 is 1\n2017-08-05 19:10:39.740253: I tensorflow/contrib/gdr/gdr_memory_manager.cc:296] Instrumenting GPU allocator with bus_id 2\n</code></pre>\n<p>The last line suggests that the GPUs with bus id 2 (mapped to pci bus id prefixed 0000:8) will benefit from GDR and host memory bypass, which is <code>/gpu:2</code> and <code>/gpu:3</code> in this case.</p>\n<h1>Caveats</h1>\n<p>In current implementation, only tensors that reside in host memory or in GPU memory such that the GPU is adjacent to an RDMA capable NIC will use direct RDMA as its transport. When RDMA is available but not GDR, a temporary tensor copy on host memory will be used as RDMA source/destination (and copied from/to the target device). When there is no RDMA device present, it can even fallback to the original gRPC runtime. While it is theoretically possible to mix GDR enabled TF with non-GDR deployments in the same job, make sure the environment is properly setup so the GDR mode is enabled whenever possible (i.e. do not fall back to gRPC when it is not absolutely necessary).</p>", "body_text": "Introduction\nThis PR implements GDR out-of-band transport for TensorFlow distributed runtime, complementary to current gRPC transport. It uses gRPC as control plane to setup rendezvous for each tensor transmission, and utilizes GPU Direct RDMA whenever possible to transmit tensors in remote GPU memory through network interface card (NIC), bypassing host memory and CPU entirely. It gracefully falls back to ordinary RDMA or even gRPC when GDR is not available.\nDesign\nThe GDR out-of-band transport is designed to avoid any unnecessary memory copies, especially for large tensors (>100MB). That typically requires registration of tensor buffers to NIC on the fly, which is rather slow as described in the design trade-off of the verbs runtime. The verbs runtime thus chooses to manage its own NIC-registered buffers and copy the tensors from/to those buffers for every single tensor transfer.\nWe show that, however, such design trade-off is not always relevant. In this patch, we manage both computation and communication buffers in a unified manner. By pre-registration of large buffers to NIC and allocating small tensors from the buffer pool using a BFC allocator, it is possible to avoid both buffer registration on the fly and memory copies all together.\nFor the actual tensor transport, we rely on gRPC to transmit the remote buffer information. This greatly simplifies our design, and there are only 2 types of RDMA messages: a single READ to retrieve the tensor data (bypassing remote CPU), and another invalidate using WRITE with IMM to release the tensor buffer on the remote side. The remote side will only be polling the invalidate message and Unref the tensor buffers that read by its peer.\nEnvironment\nTo fully utilize GDR, the target environment has to meet 3 conditions:\n\nThere is an RDMA capable device with corresponding OFED package installed (detailed information is available from your Infiniband/RoCE/iWarp vendor), which could be verified through ibv_devinfo, e.g.\n\n$ ibv_devinfo\nhca_id:\tmlx4_0\n\ttransport:\t\t\tInfiniBand (0)\n\tfw_ver:\t\t\t\t2.40.7000\n\tnode_guid:\t\t\t248a:0703:00f6:3370\n\tsys_image_guid:\t\t\t248a:0703:00f6:3370\n\tvendor_id:\t\t\t0x02c9\n\tvendor_part_id:\t\t\t4099\n\thw_ver:\t\t\t\t0x1\n\tboard_id:\t\t\tMT_1090110023\n\tphys_port_cnt:\t\t\t2\n\tDevice ports:\n\t\tport:\t1\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n\t\t\tmax_mtu:\t\t4096 (5)\n\t\t\tactive_mtu:\t\t1024 (3)\n\t\t\tsm_lid:\t\t\t0\n\t\t\tport_lid:\t\t0\n\t\t\tport_lmc:\t\t0x00\n\t\t\tlink_layer:\t\tEthernet\n\n\t\tport:\t2\n\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n\t\t\tmax_mtu:\t\t4096 (5)\n\t\t\tactive_mtu:\t\t1024 (3)\n\t\t\tsm_lid:\t\t\t0\n\t\t\tport_lid:\t\t0\n\t\t\tport_lmc:\t\t0x00\n\t\t\tlink_layer:\t\tEthernet\n\n\nThere is a GDR capable GPU, i.e. of Fermi, Kepler or later architecture with corresponding driver installed. The PCI-e topology could be confirmed by nvidia-smi topo -m. For example, in the following topology, GPU2 and GPU3 are adjacent to mlx4_0, and tensors on these devices could benefit from GDR in current implementation.\n\n$ nvidia-smi topo -m\n\tGPU0\tGPU1\tGPU2\tGPU3\tmlx4_0\tCPU Affinity\nGPU0\t X \tPHB\tSOC\tSOC\tSOC\t0-5\nGPU1\tPHB\t X \tSOC\tSOC\tSOC\t0-5\nGPU2\tSOC\tSOC\t X \tPHB\tPHB\t6-11\nGPU3\tSOC\tSOC\tPHB\t X \tPHB\t6-11\nmlx4_0\tSOC\tSOC\tPHB\tPHB\t X\n\nLegend:\n\n  X   = Self\n  SOC  = Connection traversing PCIe as well as the SMP link between CPU sockets(e.g. QPI)\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n\nThe nv_peer_mem kernel module is installed.\n\nHow to build and run in GDR mode\nTo test it out on a GDR capable environment, choose to enable GDR in your configure script.\nDo you wish to build TensorFlow with GDR support? [y/N]: y\nGDR support will be enabled for TensorFlow.\n\nChange your protocol to grpc+gdr to enable GDR in your deployment.\nserver = tf.train.Server(cluster, job_name=\"local\", task_index=0, protocol='grpc+gdr') # default protocol is 'grpc'\n\nCurrently the out-of-band transport service listens to the same IP and port address as specified in gRPC.\nA successful initialization looks like this:\n2017-08-05 19:10:38.601718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0)\n2017-08-05 19:10:38.601728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0)\n2017-08-05 19:10:38.601736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0)\n2017-08-05 19:10:38.601742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0)\n2017-08-05 19:10:39.591026: I tensorflow/contrib/gdr/gdr_memory_manager.cc:235] RDMA server is listening on 10.40.2.200:5001\n2017-08-05 19:10:39.591071: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cuda_host_bfc\n2017-08-05 19:10:39.591083: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_pool\n2017-08-05 19:10:39.591095: I tensorflow/contrib/gdr/gdr_memory_manager.cc:285] Instrumenting CPU allocator cpu_rdma_bfc\n2017-08-05 19:10:39.591278: I tensorflow/contrib/gdr/gdr_memory_manager.cc:78] NUMA node for device: mlx4_0 is 1\n2017-08-05 19:10:39.740253: I tensorflow/contrib/gdr/gdr_memory_manager.cc:296] Instrumenting GPU allocator with bus_id 2\n\nThe last line suggests that the GPUs with bus id 2 (mapped to pci bus id prefixed 0000:8) will benefit from GDR and host memory bypass, which is /gpu:2 and /gpu:3 in this case.\nCaveats\nIn current implementation, only tensors that reside in host memory or in GPU memory such that the GPU is adjacent to an RDMA capable NIC will use direct RDMA as its transport. When RDMA is available but not GDR, a temporary tensor copy on host memory will be used as RDMA source/destination (and copied from/to the target device). When there is no RDMA device present, it can even fallback to the original gRPC runtime. While it is theoretically possible to mix GDR enabled TF with non-GDR deployments in the same job, make sure the environment is properly setup so the GDR mode is enabled whenever possible (i.e. do not fall back to gRPC when it is not absolutely necessary).", "merged": true, "mergeable": null, "rebaseable": null, "mergeable_state": "unknown", "merged_by": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "comments": 92, "review_comments": 33, "maintainer_can_modify": false, "commits": 0, "additions": 0, "deletions": 0, "changed_files": 0}