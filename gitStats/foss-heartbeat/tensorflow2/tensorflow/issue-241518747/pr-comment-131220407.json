{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131220407", "pull_request_review_id": 54152714, "id": 131220407, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMTIyMDQwNw==", "diff_hunk": "@@ -0,0 +1,608 @@\n+#ifdef TENSORFLOW_USE_GDR\n+\n+#include \"tensorflow/contrib/gdr/gdr_memory_manager.h\"\n+\n+#include <atomic>\n+#include <cerrno>\n+#include <fstream>\n+#include <list>\n+#include <map>\n+\n+#include <fcntl.h>\n+#include <rdma/rdma_cma.h>\n+#include <rdma/rdma_verbs.h>\n+#include <sys/epoll.h>\n+\n+#include \"tensorflow/contrib/gdr/gdr.pb.h\"\n+#include \"tensorflow/core/common_runtime/bfc_allocator.h\"\n+#include \"tensorflow/core/common_runtime/device.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#endif  // GOOGLE_CUDA\n+#include \"tensorflow/core/framework/allocator_registry.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+bool IsGDRAvailable() {\n+  std::ifstream ifs(\"/proc/modules\");\n+  string line;\n+  while (std::getline(ifs, line)) {\n+    auto sep = line.find(' ');\n+    CHECK_NE(sep, std::string::npos);\n+    if (line.substr(0, sep) == \"nv_peer_mem\") {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+int TryToReadNumaNode(ibv_device* device) {\n+  static const int kUnknownNumaNode = -1;\n+\n+  auto filename = string(device->ibdev_path) + \"/device/numa_node\";\n+\n+  std::ifstream ifs(filename.c_str());\n+  string content;\n+  CHECK(std::getline(ifs, content));\n+\n+  int32 value;\n+  if (strings::safe_strto32(content, &value)) {\n+    if (value < 0) {\n+      LOG(INFO) << \"Successful NUMA node read from SysFS had negative value (\"\n+                << value << \"), but there must be at least one NUMA node\"\n+                            \", so returning NUMA node zero\";\n+      return 0;\n+    }\n+    LOG(INFO) << \"NUMA node for device: \" << device->name << \" is \" << value;\n+    return value;\n+  }\n+  return kUnknownNumaNode;\n+}\n+\n+void EndpointDeleter(rdma_cm_id* id) {\n+  if (id) {\n+    rdma_destroy_ep(id);\n+  }\n+}\n+\n+void MRDeleter(ibv_mr* mr) {\n+  if (mr) {\n+    rdma_dereg_mr(mr);\n+  }\n+}\n+\n+using RdmaEndpointPtr = std::unique_ptr<rdma_cm_id, decltype(&EndpointDeleter)>;\n+\n+using MemoryRegionPtr = std::unique_ptr<ibv_mr, decltype(&MRDeleter)>;\n+\n+class GdrMemoryManager : public RemoteMemoryManager {\n+ public:\n+  GdrMemoryManager(const string& host, const string& port);\n+\n+  virtual ~GdrMemoryManager();\n+\n+  virtual Status Init() override;\n+\n+  virtual void Run() override;\n+\n+  virtual void Stop() override;\n+\n+  virtual Status TransportOptionsFromTensor(\n+      ::google::protobuf::Any* mutable_transport_options, const Tensor& tensor,\n+      Device* device, DeviceContext* device_context, bool on_host) override;\n+\n+  virtual Status TensorFromTransportOptions(\n+      Tensor* tensor, const ::google::protobuf::Any& transport_options,\n+      Device* device, DeviceContext* device_context, bool on_host) override;\n+\n+ protected:\n+  Status CreateEndpoint(const string& host, const string& port,\n+                        RdmaEndpointPtr& endpoint);\n+\n+  static bool Comparator(const void* ptr, const MemoryRegionPtr& other) {\n+    return ptr < reinterpret_cast<char*>(other->addr) + other->length;\n+  }\n+\n+  ibv_mr* FindMemoryRegion(void* addr, size_t length);\n+\n+  void InsertMemoryRegion(void* addr, size_t length);\n+\n+  void EvictMemoryRegion(void* addr, size_t length);\n+\n+ private:\n+  const string host_;\n+  const string port_;\n+  RdmaEndpointPtr listening_;\n+  std::atomic<bool> stopped_;\n+  int epfd_;\n+\n+  // Server side endpoints\n+  // Accessed sequentially in Run() so not protected by lock\n+  std::list<RdmaEndpointPtr> server_clients_;\n+\n+  using TensorKey = uint32_t;\n+  std::atomic<TensorKey> next_key_;\n+\n+  // Server side on-the-fly tensor buffers\n+  mutex server_mu_;\n+  std::map<TensorKey, const TensorBuffer*> tensor_buffers_\n+      GUARDED_BY(server_mu_);\n+\n+  // Client side endpoints\n+  mutex client_mu_;\n+  std::map<std::pair<string, string>, RdmaEndpointPtr> clients_\n+      GUARDED_BY(cient_mu_);\n+\n+  // Managed memory regions\n+  mutex alloc_mu_;\n+  std::vector<MemoryRegionPtr> mrs_ GUARDED_BY(alloc_mu_);\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(GdrMemoryManager);\n+};\n+\n+class BasicCPUAllocator : public SubAllocator {\n+ public:\n+  ~BasicCPUAllocator() override {}\n+\n+  void* Alloc(size_t alignment, size_t num_bytes) override {\n+    return port::AlignedMalloc(num_bytes, alignment);\n+  }\n+  void Free(void* ptr, size_t) override { port::AlignedFree(ptr); }\n+};\n+\n+class BFCRdmaAllocator : public BFCAllocator {\n+ public:\n+  BFCRdmaAllocator()\n+      : BFCAllocator(new BasicCPUAllocator(), 1LL << 36, true, \"cpu_rdma_bfc\") {\n+  }\n+};\n+\n+REGISTER_MEM_ALLOCATOR(\"BFCRdmaAllocator\", 300, BFCRdmaAllocator);\n+\n+GdrMemoryManager::GdrMemoryManager(const string& host, const string& port)\n+    : host_(host),\n+      port_(port),\n+      listening_(nullptr, EndpointDeleter),\n+      stopped_(true),\n+      next_key_(0) {}\n+\n+GdrMemoryManager::~GdrMemoryManager() { close(epfd_); }\n+\n+Status GdrMemoryManager::Init() {\n+  epfd_ = epoll_create1(0);\n+  if (epfd_ == -1) {\n+    return errors::Unavailable(strerror(errno), \": \", \"epoll_create\");\n+  }\n+\n+  rdma_addrinfo* addrinfo;\n+  rdma_addrinfo hints = {};\n+  hints.ai_port_space = RDMA_PS_TCP;\n+  hints.ai_flags = RAI_PASSIVE;\n+  if (rdma_getaddrinfo(const_cast<char*>(host_.c_str()),\n+                       const_cast<char*>(port_.c_str()), &hints, &addrinfo)) {\n+    return errors::Unavailable(strerror(errno), \": \", \"cannot resolve rdma://\",\n+                               host_, \":\", port_);\n+  }\n+\n+  ibv_qp_init_attr init_attr = {};\n+  init_attr.qp_type = IBV_QPT_RC;\n+  init_attr.cap.max_recv_wr = 32;\n+  init_attr.cap.max_send_wr = 1;\n+  init_attr.cap.max_recv_sge = 1;\n+  init_attr.cap.max_send_sge = 1;\n+\n+  // Create listening endpoint\n+  rdma_cm_id* id;\n+  if (rdma_create_ep(&id, addrinfo, nullptr, &init_attr)) {\n+    return errors::Unavailable(strerror(errno), \": \", \"cannot bind to rdma://\",\n+                               host_, \":\", port_);\n+  }\n+  listening_.reset(id);\n+  rdma_freeaddrinfo(addrinfo);\n+\n+  // Listen without backlog\n+  if (rdma_listen(listening_.get(), 0)) {\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot listen on rdma://\", host_, \":\", port_);\n+  }\n+  LOG(INFO) << \"RDMA server is listening on \" << host_ << \":\" << port_;\n+\n+  if (listening_->verbs == nullptr) {\n+    return errors::Unimplemented(\n+        \"Unsupported address \", host_, \":\", port_,\n+        \" as it does not bind to a particular RDMA device\");\n+  }\n+\n+  int flags = fcntl(listening_->channel->fd, F_GETFL, 0);\n+  if (fcntl(listening_->channel->fd, F_SETFL, flags | O_NONBLOCK)) {\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot set server to non-blocking mode\");\n+  }\n+\n+  epoll_event event = {};\n+  event.events = EPOLLIN | EPOLLPRI;\n+  event.data.ptr = listening_.get();\n+  if (epoll_ctl(epfd_, EPOLL_CTL_ADD, listening_->channel->fd, &event)) {\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot add server to epoll\");\n+  }\n+\n+  std::set<Allocator*> instrumented_;\n+\n+  Allocator* allocators[] = {\n+#if GOOGLE_CUDA\n+    ProcessState::singleton()->GetCUDAHostAllocator(0),\n+    ProcessState::singleton()->GetCPUAllocator(0),\n+#endif  // GOOGLE_CUDA\n+    cpu_allocator(),\n+  };\n+\n+  using namespace std::placeholders;\n+  VisitableAllocator::Visitor alloc_visitor =\n+      std::bind(&GdrMemoryManager::InsertMemoryRegion, this, _1, _2);\n+  VisitableAllocator::Visitor free_visitor =\n+      std::bind(&GdrMemoryManager::EvictMemoryRegion, this, _1, _2);\n+\n+  // Host memory allocators\n+  for (Allocator* allocator : allocators) {\n+    CHECK(allocator);\n+    auto* visitable_allocator = dynamic_cast<VisitableAllocator*>(allocator);\n+    if (!visitable_allocator) {\n+      LOG(WARNING) << \"Cannot instrument non-visitable CPU allocator \"\n+                   << allocator->Name();\n+    } else if (instrumented_.find(allocator) == std::end(instrumented_)) {\n+      visitable_allocator->AddAllocVisitor(alloc_visitor);\n+      visitable_allocator->AddFreeVisitor(free_visitor);\n+      instrumented_.insert(allocator);\n+      LOG(INFO) << \"Instrumenting CPU allocator \" << allocator->Name();\n+    }\n+  }\n+\n+#if GOOGLE_CUDA\n+  if (IsGDRAvailable()) {\n+    // Note we don't free allocated GPU memory so there is no free visitor\n+    int32_t bus_id = TryToReadNumaNode(listening_->verbs->device) + 1;\n+    VisitableAllocator::Visitor alloc_visitor =\n+        std::bind(&GdrMemoryManager::InsertMemoryRegion, this, _1, _2);\n+    ProcessState::singleton()->AddGPUAllocVisitor(bus_id, alloc_visitor);\n+    LOG(INFO) << \"Instrumenting GPU allocator with bus_id \" << bus_id;\n+  }\n+#endif  // GOOGLE_CUDA\n+\n+  return Status::OK();\n+}\n+\n+void GdrMemoryManager::Run() {\n+  stopped_ = false;\n+  while (!stopped_) {\n+    epoll_event events[32];\n+    int ret = epoll_wait(epfd_, events, 32, 1);\n+    if (ret == -1) {\n+      LOG(ERROR) << \"epoll_wait: \" << strerror(errno);\n+      return;\n+    }\n+    for (int i = 0; i < ret; i++) {\n+      rdma_cm_id* id = static_cast<rdma_cm_id*>(events[i].data.ptr);\n+      if (id == listening_.get()) {\n+        // Accept incoming connections\n+        if (!rdma_get_request(listening_.get(), &id)) {\n+          if (!rdma_accept(id, nullptr)) {\n+            LOG(INFO) << \"Accepted new RDMA connection\";\n+            if (ibv_req_notify_cq(id->recv_cq, 0)) {\n+              LOG(ERROR) << strerror(errno) << \": ibv_req_notify_cq failed\";\n+              EndpointDeleter(id);\n+              continue;\n+            }\n+            for (int i = 0; i < 32; i++) {\n+              if (rdma_post_recvv(id, nullptr, nullptr, 0)) {\n+                LOG(ERROR) << strerror(errno) << \": rdma_post_recvv failed\";\n+                EndpointDeleter(id);\n+                continue;\n+              }\n+            }\n+            int flags = fcntl(id->recv_cq_channel->fd, F_GETFL, 0);\n+            if (fcntl(id->recv_cq_channel->fd, F_SETFL, flags | O_NONBLOCK)) {\n+              LOG(ERROR) << strerror(errno)\n+                         << \": cannot set server_client to non-blocking mode\";\n+              EndpointDeleter(id);\n+              continue;\n+            }\n+            epoll_event event = {};\n+            event.events = EPOLLIN | EPOLLPRI;\n+            event.data.ptr = id;\n+            if (epoll_ctl(epfd_, EPOLL_CTL_ADD, id->recv_cq_channel->fd,\n+                          &event)) {\n+              LOG(ERROR) << strerror(errno)\n+                         << \": cannot add server client to epoll\";\n+              EndpointDeleter(id);\n+              continue;\n+            }\n+            server_clients_.push_back({id, EndpointDeleter});\n+          }\n+        }\n+      } else {\n+        // Polling work completions\n+        ibv_cq* cq;\n+        void* context;\n+        if (!ibv_get_cq_event(id->recv_cq_channel, &cq, &context)) {\n+          ibv_ack_cq_events(id->recv_cq, 1);\n+          if (ibv_req_notify_cq(id->recv_cq, 0)) {\n+            LOG(ERROR) << strerror(errno) << \": ibv_req_notify_cq failed\";\n+            continue;\n+          }\n+          ibv_wc wc[32];\n+          int ret = ibv_poll_cq(id->recv_cq, 32, wc);\n+          if (ret < 0) {\n+            LOG(ERROR) << \"ibv_poll_cq failed\";\n+            continue;\n+          }\n+          for (int i = 0; i < ret; i++) {\n+            if (wc[i].opcode != IBV_WC_RECV_RDMA_WITH_IMM) {\n+              LOG(ERROR) << \"Received unknown operation \" << wc[i].opcode;\n+            }\n+            if (wc[i].status != 0) {\n+              LOG(ERROR) << ibv_wc_status_str(wc[i].status);\n+            }\n+            TensorKey tensor_key = ntohl(wc[i].imm_data);\n+            {\n+              mutex_lock l(server_mu_);\n+              auto iter = tensor_buffers_.find(tensor_key);\n+              if (iter == std::end(tensor_buffers_)) {\n+                LOG(ERROR) << \"Cannot find tensor buffer for tensor key \"\n+                           << tensor_key;\n+              } else {\n+                const TensorBuffer* buffer = iter->second;\n+                buffer->Unref();\n+                tensor_buffers_.erase(iter);\n+              }\n+            }\n+            if (rdma_post_recvv(id, nullptr, nullptr, 0)) {\n+              perror(\"rdma_post_recvv\");\n+              LOG(ERROR) << \"rdma_post_recvv failed\";\n+              continue;\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void GdrMemoryManager::Stop() { stopped_ = true; }\n+\n+Status GdrMemoryManager::TransportOptionsFromTensor(\n+    ::google::protobuf::Any* mutable_transport_options, const Tensor& tensor,\n+    Device* device, DeviceContext* device_context, bool on_host) {\n+  auto buffer = DMAHelper::buffer(&tensor);\n+  void* addr = buffer->data();\n+  size_t length = buffer->size();\n+  if (length == 0) {\n+    return errors::Unavailable(\"Cannot register tensor buffer of size 0\");\n+  }\n+\n+  ibv_mr* mr = FindMemoryRegion(addr, length);\n+\n+  Tensor host_copy;\n+  if (mr == nullptr && device->tensorflow_gpu_device_info() && (!on_host)) {\n+#if GOOGLE_CUDA\n+    Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+    host_copy = Tensor(alloc, tensor.dtype(), tensor.shape());\n+    Status s;\n+    Notification n;\n+    GPUUtil::CopyGPUTensorToCPU(device, device_context, &tensor, &host_copy,\n+                                [&s, &n](const Status& status) {\n+                                  s.Update(status);\n+                                  n.Notify();\n+                                });\n+    n.WaitForNotification();\n+    if (!s.ok()) {\n+      return s;\n+    }\n+    buffer = DMAHelper::buffer(&host_copy);\n+    addr = buffer->data();\n+    length = buffer->size();\n+    mr = FindMemoryRegion(addr, length);\n+#endif\n+  }\n+\n+  if (mr == nullptr) {\n+    return errors::Unavailable(\"Cannot find pinned memory region\");\n+  }\n+\n+  buffer->Ref();\n+  TensorKey tensor_key = next_key_++;\n+  {\n+    mutex_lock l(server_mu_);\n+    tensor_buffers_.insert(std::make_pair(tensor_key, buffer));\n+  }\n+\n+  RemoteMemoryRegion remote_mr;\n+  remote_mr.set_host(host_);\n+  remote_mr.set_port(port_);\n+  remote_mr.set_addr(reinterpret_cast<uint64_t>(addr));\n+  remote_mr.set_rkey(mr->rkey);\n+  remote_mr.set_tensor_key(tensor_key);\n+  mutable_transport_options->PackFrom(remote_mr);\n+\n+  return Status::OK();\n+}\n+\n+Status GdrMemoryManager::TensorFromTransportOptions(\n+    Tensor* tensor, const ::google::protobuf::Any& transport_options,\n+    Device* dst_device, DeviceContext* dst_device_context, bool on_host) {\n+  RemoteMemoryRegion remote_mr;\n+  if (!transport_options.UnpackTo(&remote_mr)) {\n+    return errors::NotFound(\"No RDMA transport options found\");\n+  }\n+\n+  auto buffer = DMAHelper::buffer(tensor);\n+  void* addr = buffer->data();\n+  size_t length = buffer->size();\n+  ibv_mr* mr = FindMemoryRegion(addr, length);\n+\n+  Tensor host_copy;\n+#if GOOGLE_CUDA\n+  if (mr == nullptr && (!on_host)) {\n+    Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+    host_copy = Tensor(alloc, tensor->dtype(), tensor->shape());\n+    buffer = DMAHelper::buffer(&host_copy);\n+    addr = buffer->data();\n+    length = buffer->size();\n+    mr = FindMemoryRegion(addr, length);\n+  }\n+#endif  // GOOGLE_CUDA\n+\n+  if (mr == nullptr) {\n+    return errors::Unavailable(\"Cannot find pinned memory region\");\n+  }\n+\n+  decltype(clients_)::iterator iter;\n+  bool success;\n+  {\n+    mutex_lock l(client_mu_);\n+    std::tie(iter, success) = clients_.insert(\n+        std::make_pair(std::make_pair(remote_mr.host(), remote_mr.port()),\n+                       RdmaEndpointPtr(nullptr, EndpointDeleter)));\n+    if (success || iter->second.get() == nullptr) {\n+      TF_RETURN_IF_ERROR(\n+          CreateEndpoint(remote_mr.host(), remote_mr.port(), iter->second));\n+    }\n+  }\n+  rdma_cm_id* id = iter->second.get();\n+\n+  uint64_t start = Env::Default()->NowMicros();\n+\n+  if (rdma_post_read(id, nullptr, buffer->data(), buffer->size(), mr, 0,\n+                     remote_mr.addr(), remote_mr.rkey())) {\n+    return errors::Unavailable(strerror(errno), \": \", \"rdma_post_read failed\");\n+  }\n+\n+  ibv_send_wr wr = {};\n+  wr.opcode = IBV_WR_RDMA_WRITE_WITH_IMM;\n+  wr.imm_data = htonl(remote_mr.tensor_key());\n+  wr.send_flags = IBV_SEND_FENCE | IBV_SEND_SIGNALED;\n+  ibv_send_wr* bad_wr;\n+  if (ibv_post_send(id->qp, &wr, &bad_wr)) {\n+    return errors::Unavailable(strerror(errno), \": \", \"ibv_post_send failed\");\n+  }\n+\n+  ibv_wc wc = {};\n+  int ret = rdma_get_send_comp(id, &wc);\n+  if (ret < 0 || wc.status) {\n+    return errors::Unavailable(ibv_wc_status_str(wc.status));\n+  }\n+\n+#if GOOGLE_CUDA\n+  if (host_copy.NumElements() > 0) {\n+    Status s;\n+    Notification n;\n+    GPUUtil::CopyCPUTensorToGPU(&host_copy, dst_device_context, dst_device,\n+                                tensor, [&s, &n](const Status& status) {\n+                                  s.Update(status);\n+                                  n.Notify();\n+                                });\n+    n.WaitForNotification();\n+    if (!s.ok()) {\n+      return s;\n+    }\n+  }\n+#endif  // GOOGLE_CUDA\n+\n+  uint64_t end = Env::Default()->NowMicros();\n+\n+  VLOG(2) << \"RDMA from remote memory region \" << remote_mr.rkey()\n+          << \" of size \" << buffer->size() << \" with tensor key \"\n+          << remote_mr.tensor_key() << \" took \" << (end - start) << \" micros\";\n+\n+  return Status::OK();\n+}\n+\n+Status GdrMemoryManager::CreateEndpoint(const string& host, const string& port,\n+                                        RdmaEndpointPtr& endpoint) {\n+  rdma_addrinfo* addrinfo;\n+  rdma_addrinfo hints = {};\n+  hints.ai_port_space = RDMA_PS_TCP;\n+  if (rdma_getaddrinfo(const_cast<char*>(host.c_str()),\n+                       const_cast<char*>(port.c_str()), &hints, &addrinfo)) {\n+    return errors::InvalidArgument(\n+        strerror(errno), \": \", \"cannot connect to rdma://\", host, \":\", port);\n+  }\n+\n+  ibv_qp_init_attr init_attr = {};\n+  init_attr.qp_type = IBV_QPT_RC;\n+  init_attr.cap.max_recv_wr = 1;\n+  init_attr.cap.max_send_wr = 32;\n+  init_attr.cap.max_recv_sge = 1;\n+  init_attr.cap.max_send_sge = 1;\n+\n+  rdma_cm_id* id;\n+  if (rdma_create_ep(&id, addrinfo, nullptr, &init_attr)) {\n+    rdma_freeaddrinfo(addrinfo);\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot create endpoint to rdma://\", host, \":\",\n+                               port);\n+  }\n+  rdma_freeaddrinfo(addrinfo);\n+\n+  if (rdma_connect(id, nullptr)) {\n+    rdma_destroy_ep(id);\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot connect to rdma://\", host, \":\", port);\n+  }\n+\n+  LOG(INFO) << \"RDMA endpoint connected to rdma://\" << host << \":\" << port;\n+  endpoint = RdmaEndpointPtr(id, EndpointDeleter);\n+  return Status::OK();\n+}\n+\n+ibv_mr* GdrMemoryManager::FindMemoryRegion(void* addr, size_t length) {\n+  if (length == 0) return nullptr;\n+  mutex_lock l(alloc_mu_);\n+  auto iter = std::upper_bound(mrs_.begin(), mrs_.end(), addr, &Comparator);\n+  if (iter == std::end(mrs_) || iter->get()->addr > addr) {\n+    return nullptr;\n+  } else {\n+    return iter->get();\n+  }\n+}\n+\n+void GdrMemoryManager::InsertMemoryRegion(void* addr, size_t length) {\n+  if (length == 0) return;\n+  ibv_mr* mr = rdma_reg_read(listening_.get(), addr, length);\n+  if (mr != nullptr) {\n+    mutex_lock l(alloc_mu_);\n+    auto iter = std::upper_bound(mrs_.begin(), mrs_.end(), addr, &Comparator);\n+    mrs_.insert(iter, {mr, &MRDeleter});\n+  } else {\n+    LOG(WARNING) << \"Cannot register memory region\";\n+  }\n+}\n+\n+void GdrMemoryManager::EvictMemoryRegion(void* addr, size_t length) {\n+  if (length == 0) return;\n+  mutex_lock l(alloc_mu_);\n+  auto iter = std::upper_bound(mrs_.begin(), mrs_.end(), addr, &Comparator);\n+  if (iter != std::end(mrs_) && iter->get()->addr == addr) {\n+    mrs_.erase(iter);", "path": "tensorflow/contrib/gdr/gdr_memory_manager.cc", "position": 674, "original_position": 593, "commit_id": "241c020c64410ca16683a7a7f42b223f422e5dae", "original_commit_id": "955764316dd9c796db874ae2034e336a48b1fb46", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "There's no deregistration, right?  In practice we never shrink our memory pools until process termination, so this is probably ok, but might be better to clean up properly.", "created_at": "2017-08-03T18:13:53Z", "updated_at": "2017-08-08T01:02:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r131220407", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131220407"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r131220407"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392"}}, "body_html": "<p>There's no deregistration, right?  In practice we never shrink our memory pools until process termination, so this is probably ok, but might be better to clean up properly.</p>", "body_text": "There's no deregistration, right?  In practice we never shrink our memory pools until process termination, so this is probably ok, but might be better to clean up properly."}