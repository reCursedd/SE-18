{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131781176", "pull_request_review_id": 54793466, "id": 131781176, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMTc4MTE3Ng==", "diff_hunk": "@@ -0,0 +1,122 @@\n+Introduction\n+===\n+\n+This is an implementation of GDR out-of-band transport for TensorFlow distributed runtime, complementary to current gRPC transport. It uses gRPC as control plane to setup rendezvous for each tensor transmission, and utilizes [GPU Direct RDMA](https://developer.nvidia.com/gpudirect) whenever possible to transmit tensors in remote GPU memory through network interface card (NIC), bypassing host memory and CPU entirely. It gracefully falls back to ordinary RDMA or even gRPC when GDR is not available.\n+\n+Design\n+===\n+\n+The GDR out-of-band transport is designed to avoid any unnecessary memory copies, especially for large tensors (>100MB). That typically requires registration of tensor buffers to NIC in an ad-hoc manner, which is rather slow as described in the design trade-off of the verbs runtime. The verbs runtime thus chooses to manager its own NIC-registered buffers and copy the tensors from/to those buffers for every single tensor transfer.\n+\n+We show that, however, such design trade-off is not always relevant. In this patch, we manage both computation and communication buffers in a unified manner. By pre-registration of large buffers to NIC and allocating small tensors from the buffer pool using a BFC allocator, it is possible to avoid both ad-hoc buffer registration and memory copies all together.\n+\n+For the actual tensor transport, we rely on gRPC to transmit the [remote buffer information](gdr.proto). This greatly simplifies our design, and there are only 2 types of RDMA messages: a single READ to retrieve the tensor data (bypassing remote CPU), and another invalidate using WRITE with IMM to release the tensor buffer on the remote side. The remote side will only be polling the invalidate message and `Unref` the tensor buffers that read by its peer.\n+\n+Environment\n+===\n+\n+To fully utilize GDR, the target environment has to meet 3 conditions:\n+\n+1. There is an RDMA capable device with corresponding [OFED package](https://www.openfabrics.org/index.php/overview.html) installed (detailed information is available from your [Infiniband/RoCE](http://www.mellanox.com/page/products_dyn?product_family=116)/[iWarp](http://www.chelsio.com/gpudirect-rdma/) vendor), which could be verified through `ibv_devinfo`, e.g.\n+\n+```\n+$ ibv_devinfo\n+hca_id:\tmlx4_0\n+\ttransport:\t\t\tInfiniBand (0)\n+\tfw_ver:\t\t\t\t2.40.7000\n+\tnode_guid:\t\t\t248a:0703:00f6:3370\n+\tsys_image_guid:\t\t\t248a:0703:00f6:3370\n+\tvendor_id:\t\t\t0x02c9\n+\tvendor_part_id:\t\t\t4099\n+\thw_ver:\t\t\t\t0x1\n+\tboard_id:\t\t\tMT_1090110023\n+\tphys_port_cnt:\t\t\t2\n+\tDevice ports:\n+\t\tport:\t1\n+\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n+\t\t\tmax_mtu:\t\t4096 (5)\n+\t\t\tactive_mtu:\t\t1024 (3)\n+\t\t\tsm_lid:\t\t\t0\n+\t\t\tport_lid:\t\t0\n+\t\t\tport_lmc:\t\t0x00\n+\t\t\tlink_layer:\t\tEthernet\n+\n+\t\tport:\t2\n+\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n+\t\t\tmax_mtu:\t\t4096 (5)\n+\t\t\tactive_mtu:\t\t1024 (3)\n+\t\t\tsm_lid:\t\t\t0\n+\t\t\tport_lid:\t\t0\n+\t\t\tport_lmc:\t\t0x00\n+\t\t\tlink_layer:\t\tEthernet\n+```\n+\n+2. There is a GDR capable GPU, i.e. of Fermi, Kepler or later architecture with [corresponding driver](http://docs.nvidia.com/cuda/gpudirect-rdma/index.html) installed. The PCI-e topology could be confirmed by `nvidia-smi topo -m`. Fore example, in the following topology, `GPU2` and `GPU3` are adjacent to `mlx4_0`, and tensors on these devices could benefit from GDR in current implementation.", "path": "tensorflow/contrib/gdr/README.md", "position": null, "original_position": 54, "commit_id": "241c020c64410ca16683a7a7f42b223f422e5dae", "original_commit_id": "9bb2c8eda776f5a4a464cce56f61cbfcf33564a8", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "s/Fore/For/", "created_at": "2017-08-07T22:35:25Z", "updated_at": "2017-08-08T01:02:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r131781176", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131781176"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r131781176"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392"}}, "body_html": "<p>s/Fore/For/</p>", "body_text": "s/Fore/For/"}