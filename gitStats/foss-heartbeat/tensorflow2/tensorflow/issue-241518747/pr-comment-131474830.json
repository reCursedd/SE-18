{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131474830", "pull_request_review_id": 54464328, "id": 131474830, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMTQ3NDgzMA==", "diff_hunk": "@@ -0,0 +1,608 @@\n+#ifdef TENSORFLOW_USE_GDR\n+\n+#include \"tensorflow/contrib/gdr/gdr_memory_manager.h\"\n+\n+#include <atomic>\n+#include <cerrno>\n+#include <fstream>\n+#include <list>\n+#include <map>\n+\n+#include <fcntl.h>\n+#include <rdma/rdma_cma.h>\n+#include <rdma/rdma_verbs.h>\n+#include <sys/epoll.h>\n+\n+#include \"tensorflow/contrib/gdr/gdr.pb.h\"\n+#include \"tensorflow/core/common_runtime/bfc_allocator.h\"\n+#include \"tensorflow/core/common_runtime/device.h\"\n+#include \"tensorflow/core/common_runtime/dma_helper.h\"\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\n+#endif  // GOOGLE_CUDA\n+#include \"tensorflow/core/framework/allocator_registry.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+bool IsGDRAvailable() {\n+  std::ifstream ifs(\"/proc/modules\");\n+  string line;\n+  while (std::getline(ifs, line)) {\n+    auto sep = line.find(' ');\n+    CHECK_NE(sep, std::string::npos);\n+    if (line.substr(0, sep) == \"nv_peer_mem\") {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+int TryToReadNumaNode(ibv_device* device) {\n+  static const int kUnknownNumaNode = -1;\n+\n+  auto filename = string(device->ibdev_path) + \"/device/numa_node\";\n+\n+  std::ifstream ifs(filename.c_str());\n+  string content;\n+  CHECK(std::getline(ifs, content));\n+\n+  int32 value;\n+  if (strings::safe_strto32(content, &value)) {\n+    if (value < 0) {\n+      LOG(INFO) << \"Successful NUMA node read from SysFS had negative value (\"\n+                << value << \"), but there must be at least one NUMA node\"\n+                            \", so returning NUMA node zero\";\n+      return 0;\n+    }\n+    LOG(INFO) << \"NUMA node for device: \" << device->name << \" is \" << value;\n+    return value;\n+  }\n+  return kUnknownNumaNode;\n+}\n+\n+void EndpointDeleter(rdma_cm_id* id) {\n+  if (id) {\n+    rdma_destroy_ep(id);\n+  }\n+}\n+\n+void MRDeleter(ibv_mr* mr) {\n+  if (mr) {\n+    rdma_dereg_mr(mr);\n+  }\n+}\n+\n+using RdmaEndpointPtr = std::unique_ptr<rdma_cm_id, decltype(&EndpointDeleter)>;\n+\n+using MemoryRegionPtr = std::unique_ptr<ibv_mr, decltype(&MRDeleter)>;\n+\n+class GdrMemoryManager : public RemoteMemoryManager {\n+ public:\n+  GdrMemoryManager(const string& host, const string& port);\n+\n+  virtual ~GdrMemoryManager();\n+\n+  virtual Status Init() override;\n+\n+  virtual void Run() override;\n+\n+  virtual void Stop() override;\n+\n+  virtual Status TransportOptionsFromTensor(\n+      ::google::protobuf::Any* mutable_transport_options, const Tensor& tensor,\n+      Device* device, DeviceContext* device_context, bool on_host) override;\n+\n+  virtual Status TensorFromTransportOptions(\n+      Tensor* tensor, const ::google::protobuf::Any& transport_options,\n+      Device* device, DeviceContext* device_context, bool on_host) override;\n+\n+ protected:\n+  Status CreateEndpoint(const string& host, const string& port,\n+                        RdmaEndpointPtr& endpoint);\n+\n+  static bool Comparator(const void* ptr, const MemoryRegionPtr& other) {\n+    return ptr < reinterpret_cast<char*>(other->addr) + other->length;\n+  }\n+\n+  ibv_mr* FindMemoryRegion(void* addr, size_t length);\n+\n+  void InsertMemoryRegion(void* addr, size_t length);\n+\n+  void EvictMemoryRegion(void* addr, size_t length);\n+\n+ private:\n+  const string host_;\n+  const string port_;\n+  RdmaEndpointPtr listening_;\n+  std::atomic<bool> stopped_;\n+  int epfd_;\n+\n+  // Server side endpoints\n+  // Accessed sequentially in Run() so not protected by lock\n+  std::list<RdmaEndpointPtr> server_clients_;\n+\n+  using TensorKey = uint32_t;\n+  std::atomic<TensorKey> next_key_;\n+\n+  // Server side on-the-fly tensor buffers\n+  mutex server_mu_;\n+  std::map<TensorKey, const TensorBuffer*> tensor_buffers_\n+      GUARDED_BY(server_mu_);\n+\n+  // Client side endpoints\n+  mutex client_mu_;\n+  std::map<std::pair<string, string>, RdmaEndpointPtr> clients_\n+      GUARDED_BY(cient_mu_);\n+\n+  // Managed memory regions\n+  mutex alloc_mu_;\n+  std::vector<MemoryRegionPtr> mrs_ GUARDED_BY(alloc_mu_);\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(GdrMemoryManager);\n+};\n+\n+class BasicCPUAllocator : public SubAllocator {\n+ public:\n+  ~BasicCPUAllocator() override {}\n+\n+  void* Alloc(size_t alignment, size_t num_bytes) override {\n+    return port::AlignedMalloc(num_bytes, alignment);\n+  }\n+  void Free(void* ptr, size_t) override { port::AlignedFree(ptr); }\n+};\n+\n+class BFCRdmaAllocator : public BFCAllocator {\n+ public:\n+  BFCRdmaAllocator()\n+      : BFCAllocator(new BasicCPUAllocator(), 1LL << 36, true, \"cpu_rdma_bfc\") {\n+  }\n+};\n+\n+REGISTER_MEM_ALLOCATOR(\"BFCRdmaAllocator\", 300, BFCRdmaAllocator);\n+\n+GdrMemoryManager::GdrMemoryManager(const string& host, const string& port)\n+    : host_(host),\n+      port_(port),\n+      listening_(nullptr, EndpointDeleter),\n+      stopped_(true),\n+      next_key_(0) {}\n+\n+GdrMemoryManager::~GdrMemoryManager() { close(epfd_); }\n+\n+Status GdrMemoryManager::Init() {\n+  epfd_ = epoll_create1(0);\n+  if (epfd_ == -1) {\n+    return errors::Unavailable(strerror(errno), \": \", \"epoll_create\");\n+  }\n+\n+  rdma_addrinfo* addrinfo;\n+  rdma_addrinfo hints = {};\n+  hints.ai_port_space = RDMA_PS_TCP;\n+  hints.ai_flags = RAI_PASSIVE;\n+  if (rdma_getaddrinfo(const_cast<char*>(host_.c_str()),\n+                       const_cast<char*>(port_.c_str()), &hints, &addrinfo)) {\n+    return errors::Unavailable(strerror(errno), \": \", \"cannot resolve rdma://\",\n+                               host_, \":\", port_);\n+  }\n+\n+  ibv_qp_init_attr init_attr = {};\n+  init_attr.qp_type = IBV_QPT_RC;\n+  init_attr.cap.max_recv_wr = 32;\n+  init_attr.cap.max_send_wr = 1;\n+  init_attr.cap.max_recv_sge = 1;\n+  init_attr.cap.max_send_sge = 1;\n+\n+  // Create listening endpoint\n+  rdma_cm_id* id;\n+  if (rdma_create_ep(&id, addrinfo, nullptr, &init_attr)) {\n+    return errors::Unavailable(strerror(errno), \": \", \"cannot bind to rdma://\",\n+                               host_, \":\", port_);\n+  }\n+  listening_.reset(id);\n+  rdma_freeaddrinfo(addrinfo);\n+\n+  // Listen without backlog\n+  if (rdma_listen(listening_.get(), 0)) {\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot listen on rdma://\", host_, \":\", port_);\n+  }\n+  LOG(INFO) << \"RDMA server is listening on \" << host_ << \":\" << port_;\n+\n+  if (listening_->verbs == nullptr) {\n+    return errors::Unimplemented(\n+        \"Unsupported address \", host_, \":\", port_,\n+        \" as it does not bind to a particular RDMA device\");\n+  }\n+\n+  int flags = fcntl(listening_->channel->fd, F_GETFL, 0);\n+  if (fcntl(listening_->channel->fd, F_SETFL, flags | O_NONBLOCK)) {\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot set server to non-blocking mode\");\n+  }\n+\n+  epoll_event event = {};\n+  event.events = EPOLLIN | EPOLLPRI;\n+  event.data.ptr = listening_.get();\n+  if (epoll_ctl(epfd_, EPOLL_CTL_ADD, listening_->channel->fd, &event)) {\n+    return errors::Unavailable(strerror(errno), \": \",\n+                               \"cannot add server to epoll\");\n+  }\n+\n+  std::set<Allocator*> instrumented_;\n+\n+  Allocator* allocators[] = {\n+#if GOOGLE_CUDA\n+    ProcessState::singleton()->GetCUDAHostAllocator(0),\n+    ProcessState::singleton()->GetCPUAllocator(0),\n+#endif  // GOOGLE_CUDA\n+    cpu_allocator(),\n+  };\n+\n+  using namespace std::placeholders;\n+  VisitableAllocator::Visitor alloc_visitor =\n+      std::bind(&GdrMemoryManager::InsertMemoryRegion, this, _1, _2);\n+  VisitableAllocator::Visitor free_visitor =\n+      std::bind(&GdrMemoryManager::EvictMemoryRegion, this, _1, _2);\n+\n+  // Host memory allocators\n+  for (Allocator* allocator : allocators) {\n+    CHECK(allocator);\n+    auto* visitable_allocator = dynamic_cast<VisitableAllocator*>(allocator);\n+    if (!visitable_allocator) {\n+      LOG(WARNING) << \"Cannot instrument non-visitable CPU allocator \"", "path": "tensorflow/contrib/gdr/gdr_memory_manager.cc", "position": null, "original_position": 258, "commit_id": "241c020c64410ca16683a7a7f42b223f422e5dae", "original_commit_id": "955764316dd9c796db874ae2034e336a48b1fb46", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "body": "I've added a `CHECK(visitable_allocator)` in e841bc80f.", "created_at": "2017-08-04T20:01:18Z", "updated_at": "2017-08-08T01:02:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r131474830", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131474830"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11392#discussion_r131474830"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11392"}}, "body_html": "<p>I've added a <code>CHECK(visitable_allocator)</code> in <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/e841bc80f63c294a2303d543591c8e2f21d13557/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/e841bc80f63c294a2303d543591c8e2f21d13557\"><tt>e841bc8</tt></a>.</p>", "body_text": "I've added a CHECK(visitable_allocator) in e841bc8.", "in_reply_to_id": 131201816}