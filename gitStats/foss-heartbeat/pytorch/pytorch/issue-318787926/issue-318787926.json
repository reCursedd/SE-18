{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7087", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7087/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7087/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7087/events", "html_url": "https://github.com/pytorch/pytorch/issues/7087", "id": 318787926, "node_id": "MDU6SXNzdWUzMTg3ODc5MjY=", "number": 7087, "title": "Advice about the OMP parallel strategy when performing memory limited operations.", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-30T05:18:43Z", "updated_at": "2018-05-03T09:52:43Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>This issue is probably related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"266070488\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3146\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3146/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3146\">#3146</a>. I raise this issue to share some of my experiments results, and hopefully, we could come up with a more CPU-efficient openmp solution for <strong>memory bandwidth</strong> limited operations. For <strong>memory bandwidth</strong> limited operations, I mean level-1 blas, and some util methods such as <code>index_select</code>, with a complexity of <em>O(N)</em>.</p>\n<p>In such case, using too many OMP threads won't benefit much, especially when <em>HyperThreading</em> is enabled. The majority of CPU time is spent on waiting for memory access.<br>\nHere's my exp results on <strong>Intel i7-6800K</strong> (6 cores) when using <code>THTensor_(indexSelect)</code>:</p>\n<div class=\"highlight highlight-source-shell\"><pre> \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=2 python index_select.py\n3.617363452911377s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=4 python index_select.py\n2.15081524848938s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=8 python index_select.py\n2.329015016555786s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=10 python index_select.py\n2.3334872722625732s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=12 python index_select.py\n2.3303682804107666s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=6 python index_select.py\n2.334536075592041s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=4 python index_select.py\n2.149383544921875s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=3 python index_select.py\n2.5756006240844727s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=2 python index_select.py\n3.624565362930298s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c <span class=\"pl-k\">~</span>/Workspace/playground \ue0b0 OMP_NUM_THREADS=1 python index_select.py\n6.745104551315308s</pre></div>\n<p>code snippet:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-c1\">INDEX</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100000</span>\n<span class=\"pl-c1\">NELE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">INDEX</span>, <span class=\"pl-c1\">NELE</span>)\nindex <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">INDEX</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">INDEX</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">8</span>)\nb <span class=\"pl-k\">=</span> torch.from_numpy(index)\n\nres <span class=\"pl-k\">=</span> a.index_select(<span class=\"pl-c1\">0</span>, b)\ntorch.cuda.synchronize()\nstart <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n    res <span class=\"pl-k\">=</span> a.index_select(<span class=\"pl-c1\">0</span>, b)\ntorch.cuda.synchronize()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span>s<span class=\"pl-pds\">'</span></span>.format(time.time() <span class=\"pl-k\">-</span> start))</pre></div>", "body_text": "This issue is probably related to #3146. I raise this issue to share some of my experiments results, and hopefully, we could come up with a more CPU-efficient openmp solution for memory bandwidth limited operations. For memory bandwidth limited operations, I mean level-1 blas, and some util methods such as index_select, with a complexity of O(N).\nIn such case, using too many OMP threads won't benefit much, especially when HyperThreading is enabled. The majority of CPU time is spent on waiting for memory access.\nHere's my exp results on Intel i7-6800K (6 cores) when using THTensor_(indexSelect):\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=2 python index_select.py\n3.617363452911377s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=4 python index_select.py\n2.15081524848938s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=8 python index_select.py\n2.329015016555786s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=10 python index_select.py\n2.3334872722625732s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=12 python index_select.py\n2.3303682804107666s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=6 python index_select.py\n2.334536075592041s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=4 python index_select.py\n2.149383544921875s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=3 python index_select.py\n2.5756006240844727s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=2 python index_select.py\n3.624565362930298s\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=1 python index_select.py\n6.745104551315308s\ncode snippet:\nimport time\n\nimport numpy as np\nimport torch\n\nINDEX = 100000\nNELE = 1000\na = torch.rand(INDEX, NELE)\nindex = np.random.randint(INDEX-1, size=INDEX*8)\nb = torch.from_numpy(index)\n\nres = a.index_select(0, b)\ntorch.cuda.synchronize()\nstart = time.time()\nfor _ in range(10):\n    res = a.index_select(0, b)\ntorch.cuda.synchronize()\nprint('{}s'.format(time.time() - start))", "body": "This issue is probably related to #3146. I raise this issue to share some of my experiments results, and hopefully, we could come up with a more CPU-efficient openmp solution for **memory bandwidth** limited operations. For **memory bandwidth** limited operations, I mean level-1 blas, and some util methods such as `index_select`, with a complexity of *O(N)*.\r\n\r\nIn such case, using too many OMP threads won't benefit much, especially when *HyperThreading* is enabled. The majority of CPU time is spent on waiting for memory access.\r\nHere's my exp results on **Intel i7-6800K** (6 cores) when using `THTensor_(indexSelect)`:\r\n```bash\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=2 python index_select.py\r\n3.617363452911377s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=4 python index_select.py\r\n2.15081524848938s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=8 python index_select.py\r\n2.329015016555786s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=10 python index_select.py\r\n2.3334872722625732s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=12 python index_select.py\r\n2.3303682804107666s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=6 python index_select.py\r\n2.334536075592041s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=4 python index_select.py\r\n2.149383544921875s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=3 python index_select.py\r\n2.5756006240844727s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=2 python index_select.py\r\n3.624565362930298s\r\n \ue73c  (py35) \ue0b1 kys10@banana \ue0b0 \uf07c ~/Workspace/playground \ue0b0 OMP_NUM_THREADS=1 python index_select.py\r\n6.745104551315308s\r\n```\r\n\r\ncode snippet:\r\n```python\r\nimport time\r\n\r\nimport numpy as np\r\nimport torch\r\n\r\nINDEX = 100000\r\nNELE = 1000\r\na = torch.rand(INDEX, NELE)\r\nindex = np.random.randint(INDEX-1, size=INDEX*8)\r\nb = torch.from_numpy(index)\r\n\r\nres = a.index_select(0, b)\r\ntorch.cuda.synchronize()\r\nstart = time.time()\r\nfor _ in range(10):\r\n    res = a.index_select(0, b)\r\ntorch.cuda.synchronize()\r\nprint('{}s'.format(time.time() - start))\r\n```"}