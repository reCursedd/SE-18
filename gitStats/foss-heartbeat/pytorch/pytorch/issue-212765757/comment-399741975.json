{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/399741975", "html_url": "https://github.com/pytorch/pytorch/issues/958#issuecomment-399741975", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/958", "id": 399741975, "node_id": "MDEyOklzc3VlQ29tbWVudDM5OTc0MTk3NQ==", "user": {"login": "he13689", "id": 15154814, "node_id": "MDQ6VXNlcjE1MTU0ODE0", "avatar_url": "https://avatars0.githubusercontent.com/u/15154814?v=4", "gravatar_id": "", "url": "https://api.github.com/users/he13689", "html_url": "https://github.com/he13689", "followers_url": "https://api.github.com/users/he13689/followers", "following_url": "https://api.github.com/users/he13689/following{/other_user}", "gists_url": "https://api.github.com/users/he13689/gists{/gist_id}", "starred_url": "https://api.github.com/users/he13689/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/he13689/subscriptions", "organizations_url": "https://api.github.com/users/he13689/orgs", "repos_url": "https://api.github.com/users/he13689/repos", "events_url": "https://api.github.com/users/he13689/events{/privacy}", "received_events_url": "https://api.github.com/users/he13689/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-24T09:16:33Z", "updated_at": "2018-06-24T11:37:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12710772\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tabibusairam\">@tabibusairam</a> hello, I find this error when I am using pytorch to build GAN with GP, and I have stuck here for 2 days, I have already tried several methods to solve this, but neither works. I really need some help plz.<br>\nthe error is :<br>\n<em>RuntimeError: cuda runtime error (2) : out of memory at xx\\torch\\lib\\thc\\generic/THCStorage.cu:66</em><br>\nwhen i am doing  backward</p>\n<p><em>File \"xxx/train_extractor.py\", line 128, in <br>\ngradient_penalty.backward()<br>\nFile \"xxx\\lib\\site-packages\\torch\\autograd\\variable.py\", line 156, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)<br>\nFile \"xxx\\lib\\site-packages\\torch\\autograd_<em>init</em></em>.py\", line 98, in backward<br>\nvariables, grad_variables, retain_graph)_</p>\n<p>It occurs at the 12th epoch of my training process every time, and I have already reduce the batch_size and the size in my nets.<br>\nThere is no validation procedure.<br>\nhere is a small segment of my code:<br>\nalpha = torch.rand(conf.batch_size,1).expand(X.size())<br>\nx_hat = autograd.Variable(alpha<em>real.data.cpu()+(1-alpha)</em>(real.data.cpu()+0.5*real.data.std()<em>torch.rand(real.size())),requires_grad=True)<br>\nx_hat = x_hat.cuda() if conf.cuda else x_hat<br>\npred_hat,_ = Dis(x_hat)<br>\nlabel = torch.ones(pred_hat.size())<br>\nlabel = label.cuda() if conf.cuda else label<br>\ngradients = autograd.grad(outputs = pred_hat, inputs = x_hat, grad_outputs=label, create_graph=True, retain_graph=True,only_inputs=True)[0]<br>\ngradient_penalty = conf.gp_lambda</em>((gradients.norm(2,dim=1)-1)**2).mean()<br>\n<strong>gradient_penalty.backward()</strong></p>", "body_text": "@apaszke @tabibusairam hello, I find this error when I am using pytorch to build GAN with GP, and I have stuck here for 2 days, I have already tried several methods to solve this, but neither works. I really need some help plz.\nthe error is :\nRuntimeError: cuda runtime error (2) : out of memory at xx\\torch\\lib\\thc\\generic/THCStorage.cu:66\nwhen i am doing  backward\nFile \"xxx/train_extractor.py\", line 128, in \ngradient_penalty.backward()\nFile \"xxx\\lib\\site-packages\\torch\\autograd\\variable.py\", line 156, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"xxx\\lib\\site-packages\\torch\\autograd_init.py\", line 98, in backward\nvariables, grad_variables, retain_graph)_\nIt occurs at the 12th epoch of my training process every time, and I have already reduce the batch_size and the size in my nets.\nThere is no validation procedure.\nhere is a small segment of my code:\nalpha = torch.rand(conf.batch_size,1).expand(X.size())\nx_hat = autograd.Variable(alphareal.data.cpu()+(1-alpha)(real.data.cpu()+0.5*real.data.std()torch.rand(real.size())),requires_grad=True)\nx_hat = x_hat.cuda() if conf.cuda else x_hat\npred_hat,_ = Dis(x_hat)\nlabel = torch.ones(pred_hat.size())\nlabel = label.cuda() if conf.cuda else label\ngradients = autograd.grad(outputs = pred_hat, inputs = x_hat, grad_outputs=label, create_graph=True, retain_graph=True,only_inputs=True)[0]\ngradient_penalty = conf.gp_lambda((gradients.norm(2,dim=1)-1)**2).mean()\ngradient_penalty.backward()", "body": "@apaszke @tabibusairam hello, I find this error when I am using pytorch to build GAN with GP, and I have stuck here for 2 days, I have already tried several methods to solve this, but neither works. I really need some help plz.\r\nthe error is :\r\n_RuntimeError: cuda runtime error (2) : out of memory at xx\\torch\\lib\\thc\\generic/THCStorage.cu:66_\r\nwhen i am doing  backward\r\n\r\n  _File \"xxx/train_extractor.py\", line 128, in <module>\r\n    gradient_penalty.backward()\r\n  File \"xxx\\lib\\site-packages\\torch\\autograd\\variable.py\", line 156, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"xxx\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)_\r\n\r\nIt occurs at the 12th epoch of my training process every time, and I have already reduce the batch_size and the size in my nets.\r\nThere is no validation procedure.\r\nhere is a small segment of my code:\r\n        alpha = torch.rand(conf.batch_size,1).expand(X.size())\r\n        x_hat = autograd.Variable(alpha*real.data.cpu()+(1-alpha)*(real.data.cpu()+0.5*real.data.std()*torch.rand(real.size())),requires_grad=True)\r\n        x_hat = x_hat.cuda() if conf.cuda else x_hat\r\n        pred_hat,_ = Dis(x_hat)\r\n        label = torch.ones(pred_hat.size())\r\n        label = label.cuda() if conf.cuda else label\r\n        gradients = autograd.grad(outputs = pred_hat, inputs = x_hat, grad_outputs=label, create_graph=True, retain_graph=True,only_inputs=True)[0]\r\n        gradient_penalty = conf.gp_lambda*((gradients.norm(2,dim=1)-1)**2).mean()\r\n        **gradient_penalty.backward()**\r\n"}