{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357494674", "html_url": "https://github.com/pytorch/pytorch/issues/958#issuecomment-357494674", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/958", "id": 357494674, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzQ5NDY3NA==", "user": {"login": "tabibusairam", "id": 12710772, "node_id": "MDQ6VXNlcjEyNzEwNzcy", "avatar_url": "https://avatars2.githubusercontent.com/u/12710772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tabibusairam", "html_url": "https://github.com/tabibusairam", "followers_url": "https://api.github.com/users/tabibusairam/followers", "following_url": "https://api.github.com/users/tabibusairam/following{/other_user}", "gists_url": "https://api.github.com/users/tabibusairam/gists{/gist_id}", "starred_url": "https://api.github.com/users/tabibusairam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tabibusairam/subscriptions", "organizations_url": "https://api.github.com/users/tabibusairam/orgs", "repos_url": "https://api.github.com/users/tabibusairam/repos", "events_url": "https://api.github.com/users/tabibusairam/events{/privacy}", "received_events_url": "https://api.github.com/users/tabibusairam/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-14T07:53:24Z", "updated_at": "2018-01-15T16:45:46Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">The computation graph while validation is different as in the train as the\nparameters don't get trained in validation.  Try using the command -\nnvidia-smi to see the gpu memory requirement while validation.\n\nTry reducing the batch size ( if you are working on a single gpu only), The\nmemory requirement is less for smaller batch sizes.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sun, Jan 14, 2018 at 11:17 AM, Chenrui Zhang ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/tabibusairam\">@tabibusairam</a> &lt;<a href=\"https://github.com/tabibusairam\">https://github.com/tabibusairam</a>&gt; I also encountered the\n same issue: the training process worked fine (with 6G cuda memory and my\n GPU has 12G memory) but the evaluating process which goes through the same\n network got a error information as follows:\n\n THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\n Traceback (most recent call last):\n   File \"evaluate.py\", line 132, in &lt;module&gt;\n     evaluate(prednet, args)\n   File \"evaluate.py\", line 94, in evaluate\n     predictions = prednet(X_test, initial_states)\n   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 497, in forward\n     output, hidden_states = self.step(A0, hidden_states)\n   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 377, in step\n     forget_gate = hard_sigmoid(self.conv_layers['f'][lay](inputs))\n   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 28, in hard_sigmoid\n     x = F.threshold(-x, 0, 0)\n   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/functional.py\", line 459, in threshold\n     return _functions.thnn.Threshold.apply(input, threshold, value, inplace)\n   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 174, in forward\n     getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, output, *args)\n RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu:66\n\n Have you worked out? Thanks.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"212765757\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/958\" href=\"https://github.com/pytorch/pytorch/issues/958#issuecomment-357490369\">#958 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AMHzdCQ_jJ9ogDm1jaNSLB6wCbfP08XOks5tKZT8gaJpZM4MW6we\">https://github.com/notifications/unsubscribe-auth/AMHzdCQ_jJ9ogDm1jaNSLB6wCbfP08XOks5tKZT8gaJpZM4MW6we</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "The computation graph while validation is different as in the train as the\nparameters don't get trained in validation.  Try using the command -\nnvidia-smi to see the gpu memory requirement while validation.\n\nTry reducing the batch size ( if you are working on a single gpu only), The\nmemory requirement is less for smaller batch sizes.\n\u2026\nOn Sun, Jan 14, 2018 at 11:17 AM, Chenrui Zhang ***@***.***> wrote:\n @tabibusairam <https://github.com/tabibusairam> I also encountered the\n same issue: the training process worked fine (with 6G cuda memory and my\n GPU has 12G memory) but the evaluating process which goes through the same\n network got a error information as follows:\n\n THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\n Traceback (most recent call last):\n   File \"evaluate.py\", line 132, in <module>\n     evaluate(prednet, args)\n   File \"evaluate.py\", line 94, in evaluate\n     predictions = prednet(X_test, initial_states)\n   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 497, in forward\n     output, hidden_states = self.step(A0, hidden_states)\n   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 377, in step\n     forget_gate = hard_sigmoid(self.conv_layers['f'][lay](inputs))\n   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 28, in hard_sigmoid\n     x = F.threshold(-x, 0, 0)\n   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/functional.py\", line 459, in threshold\n     return _functions.thnn.Threshold.apply(input, threshold, value, inplace)\n   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 174, in forward\n     getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, output, *args)\n RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu:66\n\n Have you worked out? Thanks.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#958 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AMHzdCQ_jJ9ogDm1jaNSLB6wCbfP08XOks5tKZT8gaJpZM4MW6we>\n .", "body": "\r\nThe computation graph while validation is different as in the train as the\r\nparameters don't get trained in validation.  Try using the command -\r\nnvidia-smi to see the gpu memory requirement while validation.\r\n\r\nTry reducing the batch size ( if you are working on a single gpu only), The\r\nmemory requirement is less for smaller batch sizes.\r\n\r\n\r\nOn Sun, Jan 14, 2018 at 11:17 AM, Chenrui Zhang <notifications@github.com>\r\nwrote:\r\n\r\n> @tabibusairam <https://github.com/tabibusairam> I also encountered the\r\n> same issue: the training process worked fine (with 6G cuda memory and my\r\n> GPU has 12G memory) but the evaluating process which goes through the same\r\n> network got a error information as follows:\r\n>\r\n> THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\r\n> Traceback (most recent call last):\r\n>   File \"evaluate.py\", line 132, in <module>\r\n>     evaluate(prednet, args)\r\n>   File \"evaluate.py\", line 94, in evaluate\r\n>     predictions = prednet(X_test, initial_states)\r\n>   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 497, in forward\r\n>     output, hidden_states = self.step(A0, hidden_states)\r\n>   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 377, in step\r\n>     forget_gate = hard_sigmoid(self.conv_layers['f'][lay](inputs))\r\n>   File \"/home/zcrwind/workspace/ijcai2018/predict/zcrPredNet/prednet.py\", line 28, in hard_sigmoid\r\n>     x = F.threshold(-x, 0, 0)\r\n>   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/functional.py\", line 459, in threshold\r\n>     return _functions.thnn.Threshold.apply(input, threshold, value, inplace)\r\n>   File \"/home/zcrwind/.conda/envs/condapython3.6/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\", line 174, in forward\r\n>     getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, output, *args)\r\n> RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THC/generic/THCStorage.cu:66\r\n>\r\n> Have you worked out? Thanks.\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/pytorch/pytorch/issues/958#issuecomment-357490369>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AMHzdCQ_jJ9ogDm1jaNSLB6wCbfP08XOks5tKZT8gaJpZM4MW6we>\r\n> .\r\n>\r\n"}