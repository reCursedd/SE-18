{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360106800", "html_url": "https://github.com/pytorch/pytorch/issues/958#issuecomment-360106800", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/958", "id": 360106800, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDEwNjgwMA==", "user": {"login": "lyakaap", "id": 27487010, "node_id": "MDQ6VXNlcjI3NDg3MDEw", "avatar_url": "https://avatars2.githubusercontent.com/u/27487010?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lyakaap", "html_url": "https://github.com/lyakaap", "followers_url": "https://api.github.com/users/lyakaap/followers", "following_url": "https://api.github.com/users/lyakaap/following{/other_user}", "gists_url": "https://api.github.com/users/lyakaap/gists{/gist_id}", "starred_url": "https://api.github.com/users/lyakaap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lyakaap/subscriptions", "organizations_url": "https://api.github.com/users/lyakaap/orgs", "repos_url": "https://api.github.com/users/lyakaap/repos", "events_url": "https://api.github.com/users/lyakaap/events{/privacy}", "received_events_url": "https://api.github.com/users/lyakaap/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-24T11:48:50Z", "updated_at": "2018-01-24T11:51:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12710772\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tabibusairam\">@tabibusairam</a> Same error occurred to me in the same situation. It was solved by changing \"volatile\" in Variable() when inference. If we set volatile=True, the computational graph will be retained during inference. In inference time, we don't need to retain computational graphs. It's very memory consuming.<br>\nYou can just set flags of volatile to True like this, `Variable(x, volatile=True).</p>", "body_text": "@tabibusairam Same error occurred to me in the same situation. It was solved by changing \"volatile\" in Variable() when inference. If we set volatile=True, the computational graph will be retained during inference. In inference time, we don't need to retain computational graphs. It's very memory consuming.\nYou can just set flags of volatile to True like this, `Variable(x, volatile=True).", "body": "@tabibusairam Same error occurred to me in the same situation. It was solved by changing \"volatile\" in Variable() when inference. If we set volatile=True, the computational graph will be retained during inference. In inference time, we don't need to retain computational graphs. It's very memory consuming.\r\nYou can just set flags of volatile to True like this, `Variable(x, volatile=True)."}