{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/330280491", "html_url": "https://github.com/pytorch/pytorch/pull/2728#issuecomment-330280491", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2728", "id": 330280491, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDI4MDQ5MQ==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-18T16:37:31Z", "updated_at": "2017-09-18T16:37:31Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I preferred int because (a) in CUDA version, the kernel using them has int input parameters and (b) in CPU &amp; CUDA version, the input indices computed from START_IND and END_IND are set to type int. So using long didn't seem necessary to me. Should I stick to using long (now int64_t in the master)?</p>\n</blockquote>\n<p>Good question.  I discussed briefly with a couple of people and my best guess is this:</p>\n<ul>\n<li>In general, we only really care about performance on CUDA.  So the correct and performant thing to do there is to operate on ints if we can, otherwise operate on longs (you see this by searching for <code>canUse32BitIndexMath</code>.  Some of the implementations are lazy about this (or use some knowledge about the grid size) and always use ints.</li>\n<li>On cpu, we usually don't care that much and usually just use <code>long</code>.</li>\n<li>Some of your changes look incorrect.  For example, storing the strides as <code>int</code>s which are then applied as pointer arithmetic would previously work if everything but strides fit into <code>int</code>, but now they are broken.</li>\n</ul>\n<p>In short, without a clear motivation beyond making everything the same type, I'm loathe to change the types.</p>", "body_text": "I preferred int because (a) in CUDA version, the kernel using them has int input parameters and (b) in CPU & CUDA version, the input indices computed from START_IND and END_IND are set to type int. So using long didn't seem necessary to me. Should I stick to using long (now int64_t in the master)?\n\nGood question.  I discussed briefly with a couple of people and my best guess is this:\n\nIn general, we only really care about performance on CUDA.  So the correct and performant thing to do there is to operate on ints if we can, otherwise operate on longs (you see this by searching for canUse32BitIndexMath.  Some of the implementations are lazy about this (or use some knowledge about the grid size) and always use ints.\nOn cpu, we usually don't care that much and usually just use long.\nSome of your changes look incorrect.  For example, storing the strides as ints which are then applied as pointer arithmetic would previously work if everything but strides fit into int, but now they are broken.\n\nIn short, without a clear motivation beyond making everything the same type, I'm loathe to change the types.", "body": "> I preferred int because (a) in CUDA version, the kernel using them has int input parameters and (b) in CPU & CUDA version, the input indices computed from START_IND and END_IND are set to type int. So using long didn't seem necessary to me. Should I stick to using long (now int64_t in the master)?\r\n\r\nGood question.  I discussed briefly with a couple of people and my best guess is this:\r\n- In general, we only really care about performance on CUDA.  So the correct and performant thing to do there is to operate on ints if we can, otherwise operate on longs (you see this by searching for `canUse32BitIndexMath`.  Some of the implementations are lazy about this (or use some knowledge about the grid size) and always use ints.\r\n- On cpu, we usually don't care that much and usually just use `long`.\r\n- Some of your changes look incorrect.  For example, storing the strides as `int`s which are then applied as pointer arithmetic would previously work if everything but strides fit into `int`, but now they are broken.\r\n\r\nIn short, without a clear motivation beyond making everything the same type, I'm loathe to change the types."}