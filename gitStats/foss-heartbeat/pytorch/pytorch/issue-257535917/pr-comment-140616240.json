{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140616240", "pull_request_review_id": 64721506, "id": 140616240, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDYxNjI0MA==", "diff_hunk": "@@ -0,0 +1,173 @@\n+#ifndef THC_GENERIC_FILE\n+#define THC_GENERIC_FILE \"generic/VolumetricAdaptiveAveragePooling.cu\"\n+#else\n+\n+#include \"../common.h\"\n+\n+// 5d tensor B x D x T x H x W\n+\n+void THNN_(VolumetricAdaptiveAveragePooling_updateOutput)(\n+           THCState *state,\n+           THCTensor *input,\n+           THCTensor *output,\n+           int osizeT,\n+           int osizeW,\n+           int osizeH)\n+{\n+  THCUNN_assertSameGPU(state, 2, input, output);\n+\n+  THCUNN_argCheck(state, input->nDimension == 4 || input->nDimension == 5, 2, input,\n+                  \"4D or 5D (batch mode) tensor expected for input, but got: %s\");\n+\n+\n+  real *output_data;\n+  real *input_data;\n+\n+  int64_t sizeD, isizeT, isizeH, isizeW;\n+  int64_t istrideD, istrideT, istrideH, istrideW;\n+  int64_t totalZ;\n+\n+  if (input->nDimension == 4) {\n+    sizeD = input->size[0];\n+    isizeT = input->size[1];\n+    isizeH = input->size[2];\n+    isizeW = input->size[3];\n+\n+    istrideD = input->stride[0];\n+    istrideT = input->stride[1];\n+    istrideH = input->stride[2];\n+    istrideW = input->stride[3];\n+\n+    THCTensor_(resize4d)(state, output, sizeD, osizeT, osizeH, osizeW);\n+\n+    totalZ = sizeD * osizeT;\n+  } else {\n+    input = THCTensor_(newContiguous)(state, input);\n+\n+    int64_t sizeB = input->size[0];\n+    sizeD = input->size[1];\n+    isizeT = input->size[2];\n+    isizeH = input->size[3];\n+    isizeW = input->size[4];\n+\n+    istrideD = input->stride[1];\n+    istrideT = input->stride[2];\n+    istrideH = input->stride[3];\n+    istrideW = input->stride[4];\n+\n+    THCTensor_(resize5d)(state, output, sizeB, sizeD, osizeT, osizeH, osizeW);\n+\n+    totalZ = sizeB * sizeD * osizeT;\n+  }\n+\n+  input_data = THCTensor_(data)(state, input);\n+  output_data = THCTensor_(data)(state, output);\n+\n+  int64_t offsetZ = 0;\n+  dim3 threads(32, 8);\n+  // each H*W plane is processed by blocksH thread blocks\n+  int blocksH = max((int)(16L / totalZ), 1);\n+  while (totalZ > 0) {\n+    dim3 blocks(totalZ > 65535 ? 65535 : totalZ, blocksH);", "path": "torch/lib/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu", "position": 71, "original_position": 71, "commit_id": "24d5909882d7e19e01a6e002cff2a43e92b53724", "original_commit_id": "4cd3d9da4de93804c96d4f21a442013837901878", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "`(blocks,threads)` naming is following the SpatialAdaptiveAveragePooling.\r\nThat blocks is 2d is also following SpatialAdaptiveAveragePooling, where each thread does multiple pixels rather than 1 (`blocksH` blocks together process a H\\*W plane). In VolumetricDilatedMaxPooling, H\\*W/(32\\*8) blocks process a plane. \r\n\r\nI'm not sure if one way is better than the other. But I thought following spatial version should be a reasonable thing to do. :)", "created_at": "2017-09-22T23:39:44Z", "updated_at": "2018-11-23T15:34:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/2728#discussion_r140616240", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2728", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140616240"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2728#discussion_r140616240"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2728"}}, "body_html": "<p><code>(blocks,threads)</code> naming is following the SpatialAdaptiveAveragePooling.<br>\nThat blocks is 2d is also following SpatialAdaptiveAveragePooling, where each thread does multiple pixels rather than 1 (<code>blocksH</code> blocks together process a H*W plane). In VolumetricDilatedMaxPooling, H*W/(32*8) blocks process a plane.</p>\n<p>I'm not sure if one way is better than the other. But I thought following spatial version should be a reasonable thing to do. :)</p>", "body_text": "(blocks,threads) naming is following the SpatialAdaptiveAveragePooling.\nThat blocks is 2d is also following SpatialAdaptiveAveragePooling, where each thread does multiple pixels rather than 1 (blocksH blocks together process a H*W plane). In VolumetricDilatedMaxPooling, H*W/(32*8) blocks process a plane.\nI'm not sure if one way is better than the other. But I thought following spatial version should be a reasonable thing to do. :)", "in_reply_to_id": 140615003}