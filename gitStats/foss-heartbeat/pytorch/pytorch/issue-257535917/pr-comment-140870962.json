{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140870962", "pull_request_review_id": 65004524, "id": 140870962, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDg3MDk2Mg==", "diff_hunk": "@@ -0,0 +1,173 @@\n+#ifndef THC_GENERIC_FILE\n+#define THC_GENERIC_FILE \"generic/VolumetricAdaptiveAveragePooling.cu\"\n+#else\n+\n+#include \"../common.h\"\n+\n+// 5d tensor B x D x T x H x W\n+\n+void THNN_(VolumetricAdaptiveAveragePooling_updateOutput)(\n+           THCState *state,\n+           THCTensor *input,\n+           THCTensor *output,\n+           int osizeT,\n+           int osizeW,\n+           int osizeH)\n+{\n+  THCUNN_assertSameGPU(state, 2, input, output);\n+\n+  THCUNN_argCheck(state, input->nDimension == 4 || input->nDimension == 5, 2, input,\n+                  \"4D or 5D (batch mode) tensor expected for input, but got: %s\");\n+\n+\n+  real *output_data;\n+  real *input_data;\n+\n+  int64_t sizeD, isizeT, isizeH, isizeW;\n+  int64_t istrideD, istrideT, istrideH, istrideW;\n+  int64_t totalZ;\n+\n+  if (input->nDimension == 4) {\n+    sizeD = input->size[0];\n+    isizeT = input->size[1];\n+    isizeH = input->size[2];\n+    isizeW = input->size[3];\n+\n+    istrideD = input->stride[0];\n+    istrideT = input->stride[1];\n+    istrideH = input->stride[2];\n+    istrideW = input->stride[3];\n+\n+    THCTensor_(resize4d)(state, output, sizeD, osizeT, osizeH, osizeW);\n+\n+    totalZ = sizeD * osizeT;\n+  } else {\n+    input = THCTensor_(newContiguous)(state, input);\n+\n+    int64_t sizeB = input->size[0];\n+    sizeD = input->size[1];\n+    isizeT = input->size[2];\n+    isizeH = input->size[3];\n+    isizeW = input->size[4];\n+\n+    istrideD = input->stride[1];\n+    istrideT = input->stride[2];\n+    istrideH = input->stride[3];\n+    istrideW = input->stride[4];\n+\n+    THCTensor_(resize5d)(state, output, sizeB, sizeD, osizeT, osizeH, osizeW);\n+\n+    totalZ = sizeB * sizeD * osizeT;\n+  }\n+\n+  input_data = THCTensor_(data)(state, input);\n+  output_data = THCTensor_(data)(state, output);\n+\n+  int64_t offsetZ = 0;\n+  dim3 threads(32, 8);\n+  // each H*W plane is processed by blocksH thread blocks\n+  int blocksH = max((int)(16L / totalZ), 1);\n+  while (totalZ > 0) {\n+    dim3 blocks(totalZ > 65535 ? 65535 : totalZ, blocksH);\n+    cunn_VolumetricAdaptiveAveragePooling_updateOutput_kernel\n+      <<<blocks, threads, 0, THCState_getCurrentStream(state)>>>(\n+        input_data, output_data, isizeT, isizeH, isizeW, osizeT, osizeH, osizeW,\n+        istrideD, istrideT, istrideH, istrideW, offsetZ\n+      );\n+\n+    totalZ -= 65535;\n+    offsetZ += 65535;\n+    THCudaCheck(cudaGetLastError());\n+  }\n+\n+  if (input->nDimension == 5) {\n+    // clean\n+    THCTensor_(free)(state, input);\n+  }\n+}\n+\n+void THNN_(VolumetricAdaptiveAveragePooling_updateGradInput)(\n+           THCState *state,\n+           THCTensor *input,\n+           THCTensor *gradOutput,\n+           THCTensor *gradInput)\n+{\n+  THCUNN_assertSameGPU(state, 3, input, gradOutput, gradInput);\n+\n+  gradOutput = THCTensor_(newContiguous)(state, gradOutput);\n+\n+  THCTensor_(resizeAs)(state, gradInput, input);\n+  THCTensor_(zero)(state, gradInput);\n+\n+  real *gradInput_data;\n+  real *gradOutput_data;\n+\n+  int64_t sizeD, isizeT, isizeH, isizeW;\n+  int64_t osizeT, osizeH, osizeW;\n+  int64_t totalZ;\n+\n+  if (input->nDimension == 4) {\n+    sizeD = input->size[0];\n+    isizeT = input->size[1];\n+    isizeH = input->size[2];\n+    isizeW = input->size[3];\n+\n+    osizeT = gradOutput->size[1];\n+    osizeH = gradOutput->size[2];\n+    osizeW = gradOutput->size[3];\n+  } else {\n+    sizeD = input->size[1];\n+    isizeT = input->size[2];\n+    isizeH = input->size[3];\n+    isizeW = input->size[4];\n+\n+    osizeT = gradOutput->size[2];\n+    osizeH = gradOutput->size[3];\n+    osizeW = gradOutput->size[4];\n+  }\n+\n+  // somehow nonatomic is passing all test for volumetric case.", "path": "torch/lib/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu", "position": 129, "original_position": 129, "commit_id": "24d5909882d7e19e01a6e002cff2a43e92b53724", "original_commit_id": "24d5909882d7e19e01a6e002cff2a43e92b53724", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Yeah, it should. Spatial version said its nonatomic kernel isn't passing the tests and thus disabled it. Either there is something weird going on with spatial kernel, or the author just copied from spatial adaptive max pooling, which also has that line (https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialAdaptiveMaxPooling.cu#L97). But my volumetric adaptive max pooling also works for the nonatomic case in tests #2782 , so I'm not really sure what's going on.", "created_at": "2017-09-25T19:12:55Z", "updated_at": "2018-11-23T15:34:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/2728#discussion_r140870962", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2728", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140870962"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2728#discussion_r140870962"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2728"}}, "body_html": "<p>Yeah, it should. Spatial version said its nonatomic kernel isn't passing the tests and thus disabled it. Either there is something weird going on with spatial kernel, or the author just copied from spatial adaptive max pooling, which also has that line (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialAdaptiveMaxPooling.cu#L97\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialAdaptiveMaxPooling.cu#L97</a>). But my volumetric adaptive max pooling also works for the nonatomic case in tests <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"258717816\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2782\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/2782/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/2782\">#2782</a> , so I'm not really sure what's going on.</p>", "body_text": "Yeah, it should. Spatial version said its nonatomic kernel isn't passing the tests and thus disabled it. Either there is something weird going on with spatial kernel, or the author just copied from spatial adaptive max pooling, which also has that line (https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/SpatialAdaptiveMaxPooling.cu#L97). But my volumetric adaptive max pooling also works for the nonatomic case in tests #2782 , so I'm not really sure what's going on.", "in_reply_to_id": 140867664}