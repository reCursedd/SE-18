{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12906", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12906/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12906/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12906/events", "html_url": "https://github.com/pytorch/pytorch/issues/12906", "id": 372281515, "node_id": "MDU6SXNzdWUzNzIyODE1MTU=", "number": 12906, "title": "[JIT] Traced pad_packed_sequence is specialized to the max sequence length seen during tracing", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-21T05:34:19Z", "updated_at": "2018-10-29T20:56:51Z", "closed_at": "2018-10-29T20:56:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Repro script:</p>\n<pre><code>import torch\n\nT, B, Cin, Chid, nlayers = 5, 3, 10, 20, 2\n\nclass RNNTest(torch.nn.Module):\n    def __init__(self):\n        super(RNNTest, self).__init__()\n        self.rnn = torch.nn.RNN(Cin, Chid, nlayers)\n\n    def forward(self, x, lengths, h0):\n        packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n        out, h = self.rnn(packed, h0)\n        padded_outs, _ = torch.nn.utils.rnn.pad_packed_sequence(out)\n        print(torch._C._get_tracing_state())\n        return padded_outs\n\nx, lengths, h0 = torch.rand(T, B, Cin), torch.LongTensor([5, 4, 3]), torch.randn(nlayers, B, Chid)\n\ntraced = torch.jit.trace(RNNTest(), (x, lengths, h0))\ntraced(x, lengths, h0)\n\nT = 7\nx, lengths, h0 = torch.rand(T, B, Cin), torch.LongTensor([7, 6, 5]), torch.randn(nlayers, B, Chid)\n\ntraced(x, lengths, h0)\n</code></pre>\n<p>Output:</p>\n<pre><code>graph(%0 : Float(5, 3, 10)\n      %1 : Long(3)\n      %2 : Float(2, 3, 20)\n      %3 : Float(20, 10)\n      %4 : Float(20, 20)\n      %5 : Float(20)\n      %6 : Float(20)\n      %7 : Float(20, 20)\n      %8 : Float(20, 20)\n      %9 : Float(20)\n      %10 : Float(20)) {\n  %11 : bool = prim::Constant[value=0](), scope: RNNTest\n  %12 : Long(3) = aten::_cast_Long(%1, %11), scope: RNNTest\n  %13 : bool = prim::Constant[value=0](), scope: RNNTest\n  %14 : Float(12, 10), %15 : Long(5) = aten::_pack_padded_sequence(%0, %12, %13), scope: RNNTest\n  %16 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %17 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %18 : Long() = aten::select(%15, %16, %17), scope: RNNTest/RNN[rnn]\n  %19 : int = prim::Constant[value=-1](), scope: RNNTest/RNN[rnn]\n  %20 : int = aten::size(%14, %19), scope: RNNTest/RNN[rnn]\n  %21 : Long() = prim::NumToTensor(%20), scope: RNNTest/RNN[rnn]\n  %22 : int = prim::Constant[value=10](), scope: RNNTest/RNN[rnn]\n  %23 : Byte() = aten::ne(%21, %22), scope: RNNTest/RNN[rnn]\n  %24 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %25 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %26 : Long() = aten::select(%15, %24, %25), scope: RNNTest/RNN[rnn]\n  %27 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %28 : int = aten::size(%2, %27), scope: RNNTest/RNN[rnn]\n  %29 : Long() = prim::NumToTensor(%28), scope: RNNTest/RNN[rnn]\n  %30 : int = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %31 : int = aten::size(%2, %30), scope: RNNTest/RNN[rnn]\n  %32 : Long() = prim::NumToTensor(%31), scope: RNNTest/RNN[rnn]\n  %33 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %34 : int = aten::size(%2, %33), scope: RNNTest/RNN[rnn]\n  %35 : Long() = prim::NumToTensor(%34), scope: RNNTest/RNN[rnn]\n  %36 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %37 : Byte() = aten::eq(%29, %36), scope: RNNTest/RNN[rnn]\n  %38 : int = prim::Constant[value=3](), scope: RNNTest/RNN[rnn]\n  %39 : Byte() = aten::eq(%32, %38), scope: RNNTest/RNN[rnn]\n  %40 : int = prim::Constant[value=20](), scope: RNNTest/RNN[rnn]\n  %41 : Byte() = aten::eq(%35, %40), scope: RNNTest/RNN[rnn]\n  %42 : Dynamic[] = prim::ListConstruct(%3, %4, %5, %6, %7, %8, %9, %10), scope: RNNTest/RNN[rnn]\n  %43 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %44 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %45 : float = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %46 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %47 : bool = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %48 : Float(12, 20), %49 : Float(2, 3, 20) = aten::rnn_tanh(%14, %15, %2, %42, %43, %44, %45, %46, %47), scope: RNNTest/RNN[rnn]\n  %50 : int = prim::Constant[value=0](), scope: RNNTest\n  %51 : int = aten::size(%15, %50), scope: RNNTest\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\n  %53 : bool = prim::Constant[value=0](), scope: RNNTest\n  %54 : float = prim::Constant[value=0](), scope: RNNTest\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\n  return ();\n}\n\ngraph(%0 : Float(5, 3, 10)\n      %1 : Long(3)\n      %2 : Float(2, 3, 20)\n      %3 : Float(20, 10)\n      %4 : Float(20, 20)\n      %5 : Float(20)\n      %6 : Float(20)\n      %7 : Float(20, 20)\n      %8 : Float(20, 20)\n      %9 : Float(20)\n      %10 : Float(20)) {\n  %11 : bool = prim::Constant[value=0](), scope: RNNTest\n  %12 : Long(3) = aten::_cast_Long(%1, %11), scope: RNNTest\n  %13 : bool = prim::Constant[value=0](), scope: RNNTest\n  %14 : Float(12, 10), %15 : Long(5) = aten::_pack_padded_sequence(%0, %12, %13), scope: RNNTest\n  %16 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %17 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %18 : Long() = aten::select(%15, %16, %17), scope: RNNTest/RNN[rnn]\n  %19 : int = prim::Constant[value=-1](), scope: RNNTest/RNN[rnn]\n  %20 : int = aten::size(%14, %19), scope: RNNTest/RNN[rnn]\n  %21 : Long() = prim::NumToTensor(%20), scope: RNNTest/RNN[rnn]\n  %22 : int = prim::Constant[value=10](), scope: RNNTest/RNN[rnn]\n  %23 : Byte() = aten::ne(%21, %22), scope: RNNTest/RNN[rnn]\n  %24 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %25 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %26 : Long() = aten::select(%15, %24, %25), scope: RNNTest/RNN[rnn]\n  %27 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %28 : int = aten::size(%2, %27), scope: RNNTest/RNN[rnn]\n  %29 : Long() = prim::NumToTensor(%28), scope: RNNTest/RNN[rnn]\n  %30 : int = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %31 : int = aten::size(%2, %30), scope: RNNTest/RNN[rnn]\n  %32 : Long() = prim::NumToTensor(%31), scope: RNNTest/RNN[rnn]\n  %33 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %34 : int = aten::size(%2, %33), scope: RNNTest/RNN[rnn]\n  %35 : Long() = prim::NumToTensor(%34), scope: RNNTest/RNN[rnn]\n  %36 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %37 : Byte() = aten::eq(%29, %36), scope: RNNTest/RNN[rnn]\n  %38 : int = prim::Constant[value=3](), scope: RNNTest/RNN[rnn]\n  %39 : Byte() = aten::eq(%32, %38), scope: RNNTest/RNN[rnn]\n  %40 : int = prim::Constant[value=20](), scope: RNNTest/RNN[rnn]\n  %41 : Byte() = aten::eq(%35, %40), scope: RNNTest/RNN[rnn]\n  %42 : Dynamic[] = prim::ListConstruct(%3, %4, %5, %6, %7, %8, %9, %10), scope: RNNTest/RNN[rnn]\n  %43 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %44 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %45 : float = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %46 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %47 : bool = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %48 : Float(12, 20), %49 : Float(2, 3, 20) = aten::rnn_tanh(%14, %15, %2, %42, %43, %44, %45, %46, %47), scope: RNNTest/RNN[rnn]\n  %50 : int = prim::Constant[value=0](), scope: RNNTest\n  %51 : int = aten::size(%15, %50), scope: RNNTest\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\n  %53 : bool = prim::Constant[value=0](), scope: RNNTest\n  %54 : float = prim::Constant[value=0](), scope: RNNTest\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\n  return ();\n}\n\nNone\nTraceback (most recent call last):\n  File \"rnn_test.py\", line 25, in &lt;module&gt;\n    traced(x, lengths, h0)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py\", line 1179, in forward\n    return self._get_method('forward')(*args, **kwargs)\nRuntimeError: \nExpected total_length to be at least the length of the longest sequence in input, but got total_length=5 and max sequence length being 7 (_pad_packed_sequence at ../aten/src/ATen/native/PackedSequence.cpp:115)\nframe #0: at::TypeDefault::_pad_packed_sequence(at::Tensor const&amp;, at::Tensor const&amp;, bool, at::Scalar, long long) const + 167 (0x118ae9397 in libcaffe2.dylib)\nframe #1: torch::autograd::VariableType::_pad_packed_sequence(at::Tensor const&amp;, at::Tensor const&amp;, bool, at::Scalar, long long) const + 1836 (0x11c148c6c in libtorch.dylib)\nframe #2: at::_pad_packed_sequence(at::Tensor const&amp;, at::Tensor const&amp;, bool, at::Scalar, long long) + 194 (0x11c4ea5b2 in libtorch.dylib)\nframe #3: torch::jit::(anonymous namespace)::$_25::operator()(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) const + 431 (0x11c4ea41f in libtorch.dylib)\nframe #4: int std::__1::__invoke_void_return_wrapper&lt;int&gt;::__call&lt;torch::jit::(anonymous namespace)::$_25&amp;, std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;&gt;(torch::jit::(anonymous namespace)::$_25&amp;&amp;&amp;, std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;&amp;&amp;) + 77 (0x11c4ea25d in libtorch.dylib)\nframe #5: std::__1::__function::__func&lt;torch::jit::(anonymous namespace)::$_25, std::__1::allocator&lt;torch::jit::(anonymous namespace)::$_25&gt;, int (std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;)&gt;::operator()(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) + 68 (0x11c4ea154 in libtorch.dylib)\nframe #6: std::__1::function&lt;int (std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;)&gt;::operator()(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) const + 121 (0x1175e6b39 in _C.cpython-36m-darwin.so)\nframe #7: torch::jit::InterpreterStateImpl::run(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) + 304 (0x11c9436c0 in libtorch.dylib)\nframe #8: torch::jit::InterpreterState::run(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) + 40 (0x11c943588 in libtorch.dylib)\nframe #9: torch::jit::(anonymous namespace)::ExecutionPlan::run(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) const + 46 (0x11c8c9f2e in libtorch.dylib)\nframe #10: torch::jit::GraphExecutorImpl::run(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) + 544 (0x11c8c0b60 in libtorch.dylib)\nframe #11: torch::jit::GraphExecutor::run(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) + 40 (0x11c8c0938 in libtorch.dylib)\nframe #12: torch::jit::script::Method::run(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;&amp;) + 1144 (0x117769838 in _C.cpython-36m-darwin.so)\nframe #13: torch::jit::invokeScriptMethodFromPython(torch::jit::script::Method&amp;, torch::jit::tuple_slice, pybind11::kwargs) + 207 (0x1177692ef in _C.cpython-36m-darwin.so)\nframe #14: torch::jit::script::initJitScriptBindings(_object*)::$_19::operator()(pybind11::args, pybind11::kwargs) const + 321 (0x11776ad91 in _C.cpython-36m-darwin.so)\nframe #15: pybind11::object pybind11::detail::argument_loader&lt;pybind11::args, pybind11::kwargs&gt;::call_impl&lt;pybind11::object, torch::jit::script::initJitScriptBindings(_object*)::$_19&amp;, 0ul, 1ul, pybind11::detail::void_type&gt;(torch::jit::script::initJitScriptBindings(_object*)::$_19&amp;&amp;&amp;, pybind11::detail::index_sequence&lt;0ul, 1ul&gt;, pybind11::detail::void_type&amp;&amp;) + 216 (0x11776abe8 in _C.cpython-36m-darwin.so)\nframe #16: std::__1::enable_if&lt;!(std::is_void&lt;pybind11::object&gt;::value), pybind11::object&gt;::type pybind11::detail::argument_loader&lt;pybind11::args, pybind11::kwargs&gt;::call&lt;pybind11::object, pybind11::detail::void_type, torch::jit::script::initJitScriptBindings(_object*)::$_19&amp;&gt;(torch::jit::script::initJitScriptBindings(_object*)::$_19&amp;&amp;&amp;) + 56 (0x11776aaf8 in _C.cpython-36m-darwin.so)\nframe #17: void pybind11::cpp_function::initialize&lt;torch::jit::script::initJitScriptBindings(_object*)::$_19, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(torch::jit::script::initJitScriptBindings(_object*)::$_19&amp;&amp;, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::'lambda'(pybind11::detail::function_call&amp;)::operator()(pybind11::detail::function_call&amp;) const + 223 (0x11776aa0f in _C.cpython-36m-darwin.so)\nframe #18: void pybind11::cpp_function::initialize&lt;torch::jit::script::initJitScriptBindings(_object*)::$_19, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(torch::jit::script::initJitScriptBindings(_object*)::$_19&amp;&amp;, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::'lambda'(pybind11::detail::function_call&amp;)::__invoke(pybind11::detail::function_call&amp;) + 24 (0x11776a918 in _C.cpython-36m-darwin.so)\nframe #19: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 6919 (0x116fa2dd7 in _C.cpython-36m-darwin.so)\n&lt;omitting python frames&gt;\nframe #48: start + 1 (0x7fff78774015 in libdyld.dylib)\n:\noperation failed in interpreter:\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/utils/rnn.py(193): pad_packed_sequence\nrnn_test.py(13): forward\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py(465): _slow_forward\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py(475): __call__\n/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py(572): trace\nrnn_test.py(19): &lt;module&gt;\n</code></pre>\n<p>In particular this portion of the graph is problematic:</p>\n<pre><code>  %55 : int = prim::Constant[value=5](), scope: RNNTest\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\n</code></pre>\n<p>The max sequence length seen during tracing (5) is hard-coded into the graph, so when we try to run the trace on a generalized input (e.g. with max sequence length 7) we fail. Notice, however, that we actually <em>do</em> record the proper size expression in the trace:</p>\n<pre><code>%50 : int = prim::Constant[value=0](), scope: RNNTest\n  %51 : int = aten::size(%15, %50), scope: RNNTest\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\n</code></pre>\n<p>However we do not record this as the input to the pad_packed_sequence instruction and rather burn in a constant.</p>", "body_text": "Repro script:\nimport torch\n\nT, B, Cin, Chid, nlayers = 5, 3, 10, 20, 2\n\nclass RNNTest(torch.nn.Module):\n    def __init__(self):\n        super(RNNTest, self).__init__()\n        self.rnn = torch.nn.RNN(Cin, Chid, nlayers)\n\n    def forward(self, x, lengths, h0):\n        packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n        out, h = self.rnn(packed, h0)\n        padded_outs, _ = torch.nn.utils.rnn.pad_packed_sequence(out)\n        print(torch._C._get_tracing_state())\n        return padded_outs\n\nx, lengths, h0 = torch.rand(T, B, Cin), torch.LongTensor([5, 4, 3]), torch.randn(nlayers, B, Chid)\n\ntraced = torch.jit.trace(RNNTest(), (x, lengths, h0))\ntraced(x, lengths, h0)\n\nT = 7\nx, lengths, h0 = torch.rand(T, B, Cin), torch.LongTensor([7, 6, 5]), torch.randn(nlayers, B, Chid)\n\ntraced(x, lengths, h0)\n\nOutput:\ngraph(%0 : Float(5, 3, 10)\n      %1 : Long(3)\n      %2 : Float(2, 3, 20)\n      %3 : Float(20, 10)\n      %4 : Float(20, 20)\n      %5 : Float(20)\n      %6 : Float(20)\n      %7 : Float(20, 20)\n      %8 : Float(20, 20)\n      %9 : Float(20)\n      %10 : Float(20)) {\n  %11 : bool = prim::Constant[value=0](), scope: RNNTest\n  %12 : Long(3) = aten::_cast_Long(%1, %11), scope: RNNTest\n  %13 : bool = prim::Constant[value=0](), scope: RNNTest\n  %14 : Float(12, 10), %15 : Long(5) = aten::_pack_padded_sequence(%0, %12, %13), scope: RNNTest\n  %16 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %17 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %18 : Long() = aten::select(%15, %16, %17), scope: RNNTest/RNN[rnn]\n  %19 : int = prim::Constant[value=-1](), scope: RNNTest/RNN[rnn]\n  %20 : int = aten::size(%14, %19), scope: RNNTest/RNN[rnn]\n  %21 : Long() = prim::NumToTensor(%20), scope: RNNTest/RNN[rnn]\n  %22 : int = prim::Constant[value=10](), scope: RNNTest/RNN[rnn]\n  %23 : Byte() = aten::ne(%21, %22), scope: RNNTest/RNN[rnn]\n  %24 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %25 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %26 : Long() = aten::select(%15, %24, %25), scope: RNNTest/RNN[rnn]\n  %27 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %28 : int = aten::size(%2, %27), scope: RNNTest/RNN[rnn]\n  %29 : Long() = prim::NumToTensor(%28), scope: RNNTest/RNN[rnn]\n  %30 : int = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %31 : int = aten::size(%2, %30), scope: RNNTest/RNN[rnn]\n  %32 : Long() = prim::NumToTensor(%31), scope: RNNTest/RNN[rnn]\n  %33 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %34 : int = aten::size(%2, %33), scope: RNNTest/RNN[rnn]\n  %35 : Long() = prim::NumToTensor(%34), scope: RNNTest/RNN[rnn]\n  %36 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %37 : Byte() = aten::eq(%29, %36), scope: RNNTest/RNN[rnn]\n  %38 : int = prim::Constant[value=3](), scope: RNNTest/RNN[rnn]\n  %39 : Byte() = aten::eq(%32, %38), scope: RNNTest/RNN[rnn]\n  %40 : int = prim::Constant[value=20](), scope: RNNTest/RNN[rnn]\n  %41 : Byte() = aten::eq(%35, %40), scope: RNNTest/RNN[rnn]\n  %42 : Dynamic[] = prim::ListConstruct(%3, %4, %5, %6, %7, %8, %9, %10), scope: RNNTest/RNN[rnn]\n  %43 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %44 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %45 : float = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %46 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %47 : bool = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %48 : Float(12, 20), %49 : Float(2, 3, 20) = aten::rnn_tanh(%14, %15, %2, %42, %43, %44, %45, %46, %47), scope: RNNTest/RNN[rnn]\n  %50 : int = prim::Constant[value=0](), scope: RNNTest\n  %51 : int = aten::size(%15, %50), scope: RNNTest\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\n  %53 : bool = prim::Constant[value=0](), scope: RNNTest\n  %54 : float = prim::Constant[value=0](), scope: RNNTest\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\n  return ();\n}\n\ngraph(%0 : Float(5, 3, 10)\n      %1 : Long(3)\n      %2 : Float(2, 3, 20)\n      %3 : Float(20, 10)\n      %4 : Float(20, 20)\n      %5 : Float(20)\n      %6 : Float(20)\n      %7 : Float(20, 20)\n      %8 : Float(20, 20)\n      %9 : Float(20)\n      %10 : Float(20)) {\n  %11 : bool = prim::Constant[value=0](), scope: RNNTest\n  %12 : Long(3) = aten::_cast_Long(%1, %11), scope: RNNTest\n  %13 : bool = prim::Constant[value=0](), scope: RNNTest\n  %14 : Float(12, 10), %15 : Long(5) = aten::_pack_padded_sequence(%0, %12, %13), scope: RNNTest\n  %16 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %17 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %18 : Long() = aten::select(%15, %16, %17), scope: RNNTest/RNN[rnn]\n  %19 : int = prim::Constant[value=-1](), scope: RNNTest/RNN[rnn]\n  %20 : int = aten::size(%14, %19), scope: RNNTest/RNN[rnn]\n  %21 : Long() = prim::NumToTensor(%20), scope: RNNTest/RNN[rnn]\n  %22 : int = prim::Constant[value=10](), scope: RNNTest/RNN[rnn]\n  %23 : Byte() = aten::ne(%21, %22), scope: RNNTest/RNN[rnn]\n  %24 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %25 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %26 : Long() = aten::select(%15, %24, %25), scope: RNNTest/RNN[rnn]\n  %27 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %28 : int = aten::size(%2, %27), scope: RNNTest/RNN[rnn]\n  %29 : Long() = prim::NumToTensor(%28), scope: RNNTest/RNN[rnn]\n  %30 : int = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %31 : int = aten::size(%2, %30), scope: RNNTest/RNN[rnn]\n  %32 : Long() = prim::NumToTensor(%31), scope: RNNTest/RNN[rnn]\n  %33 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %34 : int = aten::size(%2, %33), scope: RNNTest/RNN[rnn]\n  %35 : Long() = prim::NumToTensor(%34), scope: RNNTest/RNN[rnn]\n  %36 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %37 : Byte() = aten::eq(%29, %36), scope: RNNTest/RNN[rnn]\n  %38 : int = prim::Constant[value=3](), scope: RNNTest/RNN[rnn]\n  %39 : Byte() = aten::eq(%32, %38), scope: RNNTest/RNN[rnn]\n  %40 : int = prim::Constant[value=20](), scope: RNNTest/RNN[rnn]\n  %41 : Byte() = aten::eq(%35, %40), scope: RNNTest/RNN[rnn]\n  %42 : Dynamic[] = prim::ListConstruct(%3, %4, %5, %6, %7, %8, %9, %10), scope: RNNTest/RNN[rnn]\n  %43 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %44 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\n  %45 : float = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %46 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\n  %47 : bool = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\n  %48 : Float(12, 20), %49 : Float(2, 3, 20) = aten::rnn_tanh(%14, %15, %2, %42, %43, %44, %45, %46, %47), scope: RNNTest/RNN[rnn]\n  %50 : int = prim::Constant[value=0](), scope: RNNTest\n  %51 : int = aten::size(%15, %50), scope: RNNTest\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\n  %53 : bool = prim::Constant[value=0](), scope: RNNTest\n  %54 : float = prim::Constant[value=0](), scope: RNNTest\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\n  return ();\n}\n\nNone\nTraceback (most recent call last):\n  File \"rnn_test.py\", line 25, in <module>\n    traced(x, lengths, h0)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py\", line 1179, in forward\n    return self._get_method('forward')(*args, **kwargs)\nRuntimeError: \nExpected total_length to be at least the length of the longest sequence in input, but got total_length=5 and max sequence length being 7 (_pad_packed_sequence at ../aten/src/ATen/native/PackedSequence.cpp:115)\nframe #0: at::TypeDefault::_pad_packed_sequence(at::Tensor const&, at::Tensor const&, bool, at::Scalar, long long) const + 167 (0x118ae9397 in libcaffe2.dylib)\nframe #1: torch::autograd::VariableType::_pad_packed_sequence(at::Tensor const&, at::Tensor const&, bool, at::Scalar, long long) const + 1836 (0x11c148c6c in libtorch.dylib)\nframe #2: at::_pad_packed_sequence(at::Tensor const&, at::Tensor const&, bool, at::Scalar, long long) + 194 (0x11c4ea5b2 in libtorch.dylib)\nframe #3: torch::jit::(anonymous namespace)::$_25::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) const + 431 (0x11c4ea41f in libtorch.dylib)\nframe #4: int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_25&, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&>(torch::jit::(anonymous namespace)::$_25&&&, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&&&) + 77 (0x11c4ea25d in libtorch.dylib)\nframe #5: std::__1::__function::__func<torch::jit::(anonymous namespace)::$_25, std::__1::allocator<torch::jit::(anonymous namespace)::$_25>, int (std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 68 (0x11c4ea154 in libtorch.dylib)\nframe #6: std::__1::function<int (std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) const + 121 (0x1175e6b39 in _C.cpython-36m-darwin.so)\nframe #7: torch::jit::InterpreterStateImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 304 (0x11c9436c0 in libtorch.dylib)\nframe #8: torch::jit::InterpreterState::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 40 (0x11c943588 in libtorch.dylib)\nframe #9: torch::jit::(anonymous namespace)::ExecutionPlan::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) const + 46 (0x11c8c9f2e in libtorch.dylib)\nframe #10: torch::jit::GraphExecutorImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 544 (0x11c8c0b60 in libtorch.dylib)\nframe #11: torch::jit::GraphExecutor::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 40 (0x11c8c0938 in libtorch.dylib)\nframe #12: torch::jit::script::Method::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 1144 (0x117769838 in _C.cpython-36m-darwin.so)\nframe #13: torch::jit::invokeScriptMethodFromPython(torch::jit::script::Method&, torch::jit::tuple_slice, pybind11::kwargs) + 207 (0x1177692ef in _C.cpython-36m-darwin.so)\nframe #14: torch::jit::script::initJitScriptBindings(_object*)::$_19::operator()(pybind11::args, pybind11::kwargs) const + 321 (0x11776ad91 in _C.cpython-36m-darwin.so)\nframe #15: pybind11::object pybind11::detail::argument_loader<pybind11::args, pybind11::kwargs>::call_impl<pybind11::object, torch::jit::script::initJitScriptBindings(_object*)::$_19&, 0ul, 1ul, pybind11::detail::void_type>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&&, pybind11::detail::index_sequence<0ul, 1ul>, pybind11::detail::void_type&&) + 216 (0x11776abe8 in _C.cpython-36m-darwin.so)\nframe #16: std::__1::enable_if<!(std::is_void<pybind11::object>::value), pybind11::object>::type pybind11::detail::argument_loader<pybind11::args, pybind11::kwargs>::call<pybind11::object, pybind11::detail::void_type, torch::jit::script::initJitScriptBindings(_object*)::$_19&>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&&) + 56 (0x11776aaf8 in _C.cpython-36m-darwin.so)\nframe #17: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_19, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 223 (0x11776aa0f in _C.cpython-36m-darwin.so)\nframe #18: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_19, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 24 (0x11776a918 in _C.cpython-36m-darwin.so)\nframe #19: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 6919 (0x116fa2dd7 in _C.cpython-36m-darwin.so)\n<omitting python frames>\nframe #48: start + 1 (0x7fff78774015 in libdyld.dylib)\n:\noperation failed in interpreter:\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/utils/rnn.py(193): pad_packed_sequence\nrnn_test.py(13): forward\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py(465): _slow_forward\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py(475): __call__\n/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py(572): trace\nrnn_test.py(19): <module>\n\nIn particular this portion of the graph is problematic:\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\n\nThe max sequence length seen during tracing (5) is hard-coded into the graph, so when we try to run the trace on a generalized input (e.g. with max sequence length 7) we fail. Notice, however, that we actually do record the proper size expression in the trace:\n%50 : int = prim::Constant[value=0](), scope: RNNTest\n  %51 : int = aten::size(%15, %50), scope: RNNTest\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\n\nHowever we do not record this as the input to the pad_packed_sequence instruction and rather burn in a constant.", "body": "Repro script:\r\n\r\n```\r\nimport torch\r\n\r\nT, B, Cin, Chid, nlayers = 5, 3, 10, 20, 2\r\n\r\nclass RNNTest(torch.nn.Module):\r\n    def __init__(self):\r\n        super(RNNTest, self).__init__()\r\n        self.rnn = torch.nn.RNN(Cin, Chid, nlayers)\r\n\r\n    def forward(self, x, lengths, h0):\r\n        packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\r\n        out, h = self.rnn(packed, h0)\r\n        padded_outs, _ = torch.nn.utils.rnn.pad_packed_sequence(out)\r\n        print(torch._C._get_tracing_state())\r\n        return padded_outs\r\n\r\nx, lengths, h0 = torch.rand(T, B, Cin), torch.LongTensor([5, 4, 3]), torch.randn(nlayers, B, Chid)\r\n\r\ntraced = torch.jit.trace(RNNTest(), (x, lengths, h0))\r\ntraced(x, lengths, h0)\r\n\r\nT = 7\r\nx, lengths, h0 = torch.rand(T, B, Cin), torch.LongTensor([7, 6, 5]), torch.randn(nlayers, B, Chid)\r\n\r\ntraced(x, lengths, h0)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\ngraph(%0 : Float(5, 3, 10)\r\n      %1 : Long(3)\r\n      %2 : Float(2, 3, 20)\r\n      %3 : Float(20, 10)\r\n      %4 : Float(20, 20)\r\n      %5 : Float(20)\r\n      %6 : Float(20)\r\n      %7 : Float(20, 20)\r\n      %8 : Float(20, 20)\r\n      %9 : Float(20)\r\n      %10 : Float(20)) {\r\n  %11 : bool = prim::Constant[value=0](), scope: RNNTest\r\n  %12 : Long(3) = aten::_cast_Long(%1, %11), scope: RNNTest\r\n  %13 : bool = prim::Constant[value=0](), scope: RNNTest\r\n  %14 : Float(12, 10), %15 : Long(5) = aten::_pack_padded_sequence(%0, %12, %13), scope: RNNTest\r\n  %16 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %17 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %18 : Long() = aten::select(%15, %16, %17), scope: RNNTest/RNN[rnn]\r\n  %19 : int = prim::Constant[value=-1](), scope: RNNTest/RNN[rnn]\r\n  %20 : int = aten::size(%14, %19), scope: RNNTest/RNN[rnn]\r\n  %21 : Long() = prim::NumToTensor(%20), scope: RNNTest/RNN[rnn]\r\n  %22 : int = prim::Constant[value=10](), scope: RNNTest/RNN[rnn]\r\n  %23 : Byte() = aten::ne(%21, %22), scope: RNNTest/RNN[rnn]\r\n  %24 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %25 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %26 : Long() = aten::select(%15, %24, %25), scope: RNNTest/RNN[rnn]\r\n  %27 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %28 : int = aten::size(%2, %27), scope: RNNTest/RNN[rnn]\r\n  %29 : Long() = prim::NumToTensor(%28), scope: RNNTest/RNN[rnn]\r\n  %30 : int = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\r\n  %31 : int = aten::size(%2, %30), scope: RNNTest/RNN[rnn]\r\n  %32 : Long() = prim::NumToTensor(%31), scope: RNNTest/RNN[rnn]\r\n  %33 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\r\n  %34 : int = aten::size(%2, %33), scope: RNNTest/RNN[rnn]\r\n  %35 : Long() = prim::NumToTensor(%34), scope: RNNTest/RNN[rnn]\r\n  %36 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\r\n  %37 : Byte() = aten::eq(%29, %36), scope: RNNTest/RNN[rnn]\r\n  %38 : int = prim::Constant[value=3](), scope: RNNTest/RNN[rnn]\r\n  %39 : Byte() = aten::eq(%32, %38), scope: RNNTest/RNN[rnn]\r\n  %40 : int = prim::Constant[value=20](), scope: RNNTest/RNN[rnn]\r\n  %41 : Byte() = aten::eq(%35, %40), scope: RNNTest/RNN[rnn]\r\n  %42 : Dynamic[] = prim::ListConstruct(%3, %4, %5, %6, %7, %8, %9, %10), scope: RNNTest/RNN[rnn]\r\n  %43 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\r\n  %44 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\r\n  %45 : float = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %46 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\r\n  %47 : bool = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %48 : Float(12, 20), %49 : Float(2, 3, 20) = aten::rnn_tanh(%14, %15, %2, %42, %43, %44, %45, %46, %47), scope: RNNTest/RNN[rnn]\r\n  %50 : int = prim::Constant[value=0](), scope: RNNTest\r\n  %51 : int = aten::size(%15, %50), scope: RNNTest\r\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\r\n  %53 : bool = prim::Constant[value=0](), scope: RNNTest\r\n  %54 : float = prim::Constant[value=0](), scope: RNNTest\r\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\r\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\r\n  return ();\r\n}\r\n\r\ngraph(%0 : Float(5, 3, 10)\r\n      %1 : Long(3)\r\n      %2 : Float(2, 3, 20)\r\n      %3 : Float(20, 10)\r\n      %4 : Float(20, 20)\r\n      %5 : Float(20)\r\n      %6 : Float(20)\r\n      %7 : Float(20, 20)\r\n      %8 : Float(20, 20)\r\n      %9 : Float(20)\r\n      %10 : Float(20)) {\r\n  %11 : bool = prim::Constant[value=0](), scope: RNNTest\r\n  %12 : Long(3) = aten::_cast_Long(%1, %11), scope: RNNTest\r\n  %13 : bool = prim::Constant[value=0](), scope: RNNTest\r\n  %14 : Float(12, 10), %15 : Long(5) = aten::_pack_padded_sequence(%0, %12, %13), scope: RNNTest\r\n  %16 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %17 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %18 : Long() = aten::select(%15, %16, %17), scope: RNNTest/RNN[rnn]\r\n  %19 : int = prim::Constant[value=-1](), scope: RNNTest/RNN[rnn]\r\n  %20 : int = aten::size(%14, %19), scope: RNNTest/RNN[rnn]\r\n  %21 : Long() = prim::NumToTensor(%20), scope: RNNTest/RNN[rnn]\r\n  %22 : int = prim::Constant[value=10](), scope: RNNTest/RNN[rnn]\r\n  %23 : Byte() = aten::ne(%21, %22), scope: RNNTest/RNN[rnn]\r\n  %24 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %25 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %26 : Long() = aten::select(%15, %24, %25), scope: RNNTest/RNN[rnn]\r\n  %27 : int = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %28 : int = aten::size(%2, %27), scope: RNNTest/RNN[rnn]\r\n  %29 : Long() = prim::NumToTensor(%28), scope: RNNTest/RNN[rnn]\r\n  %30 : int = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\r\n  %31 : int = aten::size(%2, %30), scope: RNNTest/RNN[rnn]\r\n  %32 : Long() = prim::NumToTensor(%31), scope: RNNTest/RNN[rnn]\r\n  %33 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\r\n  %34 : int = aten::size(%2, %33), scope: RNNTest/RNN[rnn]\r\n  %35 : Long() = prim::NumToTensor(%34), scope: RNNTest/RNN[rnn]\r\n  %36 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\r\n  %37 : Byte() = aten::eq(%29, %36), scope: RNNTest/RNN[rnn]\r\n  %38 : int = prim::Constant[value=3](), scope: RNNTest/RNN[rnn]\r\n  %39 : Byte() = aten::eq(%32, %38), scope: RNNTest/RNN[rnn]\r\n  %40 : int = prim::Constant[value=20](), scope: RNNTest/RNN[rnn]\r\n  %41 : Byte() = aten::eq(%35, %40), scope: RNNTest/RNN[rnn]\r\n  %42 : Dynamic[] = prim::ListConstruct(%3, %4, %5, %6, %7, %8, %9, %10), scope: RNNTest/RNN[rnn]\r\n  %43 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\r\n  %44 : int = prim::Constant[value=2](), scope: RNNTest/RNN[rnn]\r\n  %45 : float = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %46 : bool = prim::Constant[value=1](), scope: RNNTest/RNN[rnn]\r\n  %47 : bool = prim::Constant[value=0](), scope: RNNTest/RNN[rnn]\r\n  %48 : Float(12, 20), %49 : Float(2, 3, 20) = aten::rnn_tanh(%14, %15, %2, %42, %43, %44, %45, %46, %47), scope: RNNTest/RNN[rnn]\r\n  %50 : int = prim::Constant[value=0](), scope: RNNTest\r\n  %51 : int = aten::size(%15, %50), scope: RNNTest\r\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\r\n  %53 : bool = prim::Constant[value=0](), scope: RNNTest\r\n  %54 : float = prim::Constant[value=0](), scope: RNNTest\r\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\r\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\r\n  return ();\r\n}\r\n\r\nNone\r\nTraceback (most recent call last):\r\n  File \"rnn_test.py\", line 25, in <module>\r\n    traced(x, lengths, h0)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py\", line 1179, in forward\r\n    return self._get_method('forward')(*args, **kwargs)\r\nRuntimeError: \r\nExpected total_length to be at least the length of the longest sequence in input, but got total_length=5 and max sequence length being 7 (_pad_packed_sequence at ../aten/src/ATen/native/PackedSequence.cpp:115)\r\nframe #0: at::TypeDefault::_pad_packed_sequence(at::Tensor const&, at::Tensor const&, bool, at::Scalar, long long) const + 167 (0x118ae9397 in libcaffe2.dylib)\r\nframe #1: torch::autograd::VariableType::_pad_packed_sequence(at::Tensor const&, at::Tensor const&, bool, at::Scalar, long long) const + 1836 (0x11c148c6c in libtorch.dylib)\r\nframe #2: at::_pad_packed_sequence(at::Tensor const&, at::Tensor const&, bool, at::Scalar, long long) + 194 (0x11c4ea5b2 in libtorch.dylib)\r\nframe #3: torch::jit::(anonymous namespace)::$_25::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) const + 431 (0x11c4ea41f in libtorch.dylib)\r\nframe #4: int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_25&, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&>(torch::jit::(anonymous namespace)::$_25&&&, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&&&) + 77 (0x11c4ea25d in libtorch.dylib)\r\nframe #5: std::__1::__function::__func<torch::jit::(anonymous namespace)::$_25, std::__1::allocator<torch::jit::(anonymous namespace)::$_25>, int (std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 68 (0x11c4ea154 in libtorch.dylib)\r\nframe #6: std::__1::function<int (std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) const + 121 (0x1175e6b39 in _C.cpython-36m-darwin.so)\r\nframe #7: torch::jit::InterpreterStateImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 304 (0x11c9436c0 in libtorch.dylib)\r\nframe #8: torch::jit::InterpreterState::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 40 (0x11c943588 in libtorch.dylib)\r\nframe #9: torch::jit::(anonymous namespace)::ExecutionPlan::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) const + 46 (0x11c8c9f2e in libtorch.dylib)\r\nframe #10: torch::jit::GraphExecutorImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 544 (0x11c8c0b60 in libtorch.dylib)\r\nframe #11: torch::jit::GraphExecutor::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 40 (0x11c8c0938 in libtorch.dylib)\r\nframe #12: torch::jit::script::Method::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 1144 (0x117769838 in _C.cpython-36m-darwin.so)\r\nframe #13: torch::jit::invokeScriptMethodFromPython(torch::jit::script::Method&, torch::jit::tuple_slice, pybind11::kwargs) + 207 (0x1177692ef in _C.cpython-36m-darwin.so)\r\nframe #14: torch::jit::script::initJitScriptBindings(_object*)::$_19::operator()(pybind11::args, pybind11::kwargs) const + 321 (0x11776ad91 in _C.cpython-36m-darwin.so)\r\nframe #15: pybind11::object pybind11::detail::argument_loader<pybind11::args, pybind11::kwargs>::call_impl<pybind11::object, torch::jit::script::initJitScriptBindings(_object*)::$_19&, 0ul, 1ul, pybind11::detail::void_type>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&&, pybind11::detail::index_sequence<0ul, 1ul>, pybind11::detail::void_type&&) + 216 (0x11776abe8 in _C.cpython-36m-darwin.so)\r\nframe #16: std::__1::enable_if<!(std::is_void<pybind11::object>::value), pybind11::object>::type pybind11::detail::argument_loader<pybind11::args, pybind11::kwargs>::call<pybind11::object, pybind11::detail::void_type, torch::jit::script::initJitScriptBindings(_object*)::$_19&>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&&) + 56 (0x11776aaf8 in _C.cpython-36m-darwin.so)\r\nframe #17: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_19, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 223 (0x11776aa0f in _C.cpython-36m-darwin.so)\r\nframe #18: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_19, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_19&&, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 24 (0x11776a918 in _C.cpython-36m-darwin.so)\r\nframe #19: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 6919 (0x116fa2dd7 in _C.cpython-36m-darwin.so)\r\n<omitting python frames>\r\nframe #48: start + 1 (0x7fff78774015 in libdyld.dylib)\r\n:\r\noperation failed in interpreter:\r\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/utils/rnn.py(193): pad_packed_sequence\r\nrnn_test.py(13): forward\r\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py(465): _slow_forward\r\n/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py(475): __call__\r\n/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py(572): trace\r\nrnn_test.py(19): <module>\r\n```\r\n\r\nIn particular this portion of the graph is problematic:\r\n\r\n```\r\n  %55 : int = prim::Constant[value=5](), scope: RNNTest\r\n  %56 : Float(5, 3, 20), %57 : Long(3) = aten::_pad_packed_sequence(%48, %15, %53, %54, %55), scope: RNNTest\r\n```\r\n\r\nThe max sequence length seen during tracing (5) is hard-coded into the graph, so when we try to run the trace on a generalized input (e.g. with max sequence length 7) we fail. Notice, however, that we actually *do* record the proper size expression in the trace:\r\n\r\n```\r\n%50 : int = prim::Constant[value=0](), scope: RNNTest\r\n  %51 : int = aten::size(%15, %50), scope: RNNTest\r\n  %52 : Long() = prim::NumToTensor(%51), scope: RNNTest\r\n```\r\n\r\nHowever we do not record this as the input to the pad_packed_sequence instruction and rather burn in a constant. "}