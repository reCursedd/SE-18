{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/329907197", "html_url": "https://github.com/pytorch/pytorch/issues/1433#issuecomment-329907197", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1433", "id": 329907197, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTkwNzE5Nw==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-15T21:21:38Z", "updated_at": "2017-09-15T21:21:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I had a discussion with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> about this.  Here are some notes from that discussion followed by my own proposal.  The below discussion is limited to \u201cScalars\u201d in autograd; we probably want to move the Tensor and Variable API together in some some sensible way (see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"255371480\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2633\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2633/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2633\">#2633</a>), but I don\u2019t want to muddy the discussion of autograd Scalars with the details of that process as much as is feasible.</p>\n<p><strong>Scalars vs 0-dimensional objects</strong></p>\n<p>NumPy has both scalars and 0-dimensional arrays.  Scalars are immutable numeric types that (at least for cases where the type is compatible) are instances of the relevant python numeric type, e.g.:</p>\n<pre><code>&gt;&gt;&gt; isinstance(np.double(5), float)\nTrue\n&gt;&gt;&gt; isinstance(np.float(5),float)\nTrue\n</code></pre>\n<p>They also \u201cquack\u201d like 0-dimensional arrays, i.e. they have <code>shape</code> and <code>ndim</code> attributes:</p>\n<pre><code>&gt;&gt;&gt; np.double(5).shape\n()\n&gt;&gt;&gt; np.double(5).ndim\n0\n</code></pre>\n<p>These properties allow you to pass scalar types to libraries outside of NumPy (even ones that do explicit type checking via isinstance or e.g. PyFloat_Check) and to mix NumPy scalars and nd-arrays in code (as long as you don\u2019t try to mutate a scalar).</p>\n<p>On the other handle, 0-dimensional arrays are mutable types that are not instances of python numeric types:</p>\n<pre><code>&gt;&gt;&gt; a=np.array(0.5)\n&gt;&gt;&gt; a[()]=2\n&gt;&gt;&gt; a\narray(2.0)\n&gt;&gt;&gt; isinstance(np.array(0.5), float)\nFalse\n&gt;&gt;&gt; isinstance(np.array(0.5), np.ndarray)\nTrue\n</code></pre>\n<p>Thus, at least for libraries that do explicit type checking, one would need to cast the 0-dimensional array to the correct python or NumPy scalar type before converting it (np.asscalar will do this)</p>\n<p><strong>Do we want Scalars or 0-dimensional Variables or both in PyTorch?</strong></p>\n<p>I\u2019m going to argue that we only want 0-dimensional Variables.</p>\n<p>First, we want at least 0-dimensional Variables in PyTorch.  Consider if we had only (immutable) Scalars and your loss is a .sum() over some variable.  Now, if you treat this as an immutable scalar, either it has a grad and can be optimized (which is weird for an immutable value), or it doesn\u2019t (and you have to hack around that in some way to still be able to do training).  You could also represent it as a 1-dimensional Variable (which is what we do today), but then the dimensions are inconsistent (i.e. reduction functions reduce the dimension by 1 except when the dimension is 1).  Representing this quantity as a 0-dimensional Variable is just much more straightforward and mathematically consistent.</p>\n<p>Now, if we have 0-dimensional Variables, do we need immutable scalars in autograd?  I\u2019d argue no:</p>\n<ol>\n<li>The ability to use Scalars in external libraries seem like not a strong reason for the extra complexity: many operations should just work on 0-dimensional arrays (because we define a number of numerical operators on Variables) and if we provide conversions, in the worst case users can just call the conversions.</li>\n<li>Immutability in-and-of-itself may be useful, but its usefulness is not just restricted to scalar types; you may want immutability on all tensor/Variable types.</li>\n<li>A related question is what to return when indexing the last dimension of a Variable.  The normal python, numpy, and current Tensor semantics are that it gives you an (immutable) scalar type, while the current Variable semantics are that it gives you a Variable that shares storage (of the wrong dimensionality).  Given that we already return Variables in this case, returning a 0-dimensional one seems acceptable.</li>\n</ol>\n<p>I\u2019ll also note here that TensorFlow only supports 0-dimensional tensors, although it\u2019s quite different because the execution doesn\u2019t define the graph, as in PyTorch.</p>\n<p>Many of the other arguments for scalars are not about immutability, but convenience, e.g. it\u2019s nice to get pretty prints (i.e. \u201c0\u201d or \u201ctorch.FloatScalar(0)\u201d instead of something like:</p>\n<pre><code>5\n[torch.FloatTensor of size ()]\n</code></pre>\n<p>Those kinds of convenience problems are solvable without adding scalar types.</p>\n<p><strong>A Proposal for adding 0-dimensional support to autograd</strong></p>\n<p>As mentioned above, the goal here is to get to a sensible Variable API; this doesn\u2019t purport to solve all issues with scalars (i.e. forced synchronization), but that we can work on those issues separately while maintaining the API as is.</p>\n<ol>\n<li>\n<p>Autograd forward functions can now return a python number to represent that a 0-dimensional Variable should be returned to the .apply caller.  Note that they can\u2019t return 0-dimensional objects directly because the forward functions are written in terms of tensors, which don\u2019t support 0-dimensions yet.  Note also that this will now match what the equivalent tensor functions do (i.e. <code>max()</code> returns a python number), and when they support 0-dimensional tensors, the code will just work as written.  This won\u2019t immediately solve all dimensionality problems, because for example <code>max(dim=0)</code> on a 1-dimensional tensor will still return a 1-dimensional tensor, but again, this will automatically be fixed when the tensor APIs are changed.</p>\n</li>\n<li>\n<p>Variable is now 0-dimensional aware (via its underlying ATen implementation).  For example:<br>\n<code>.dim()</code> will return 0<br>\n<code>.size()</code> will return ()<br>\n<code>.shape</code> will return ()<br>\nindexing with a dimension will throw an error (i.e. <code>[()]</code> and <code>[...]</code> will work, everything else will throw an error)<br>\nAll other functions will behave as before.  Note that accessing .data will return a 1-dimensional tensor, but again, that will automatically change when tensor changes.</p>\n</li>\n<li>\n<p>We will provide constructors for 0-dimensional Variables.  Unfortunately, a python number passed to a tensor constructor is interpreted as a size: i.e. <code>torch.FloatTensor(5)</code> creates a 1-dimensional tensor of size 5, so we eventually need a different name when we support 0-dimensions on tensors.  I propose torch.*Scalar (e.g. <code>torch.FloatScalar</code>); this has a more object-looking name than say, torch.float(...), which will make it more obvious it\u2019s a mutable type.  One question is how this should interact with the work in (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"255371480\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2633\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2633/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2633\">#2633</a>).  A full \u201cleap-of-faith\u201d approach would be to have this \u201cconstructor\u201d always return a Variable and have \u2018requires_grad\u2019 and \u2018volatile\u2019 parameters.  This name will probably be confusing in the short term because you would expect <code>torch.FloatScalar</code> to return a tensor type, but constructing these objects directly will probably be rare in the short term anyway.</p>\n</li>\n<li>\n<p><code>__str__</code> on a 0-dimensional tensor will return the underlying number, <code>__repr__</code> will return how it is constructed, i.e. <code>torch.FloatScalar(5)</code>.  This is consistent with numpy 0-dimensional arrrays:</p>\n</li>\n</ol>\n<pre><code>&gt;&gt;&gt; print(np.array(0))\n0\n&gt;&gt;&gt; np.array(0)\narray(0)\n</code></pre>\n<ol start=\"5\">\n<li>We will provide conversion functions (<code>__int__</code>, <code>__float__</code>, more?) for 0-dimensional Variables for ease of use with existing python libraries.  Calling these functions on non-0-dimensional Variables will throw an error.</li>\n</ol>\n<p>Thoughts?</p>", "body_text": "I had a discussion with @colesbury, @apaszke, @ezyang about this.  Here are some notes from that discussion followed by my own proposal.  The below discussion is limited to \u201cScalars\u201d in autograd; we probably want to move the Tensor and Variable API together in some some sensible way (see #2633), but I don\u2019t want to muddy the discussion of autograd Scalars with the details of that process as much as is feasible.\nScalars vs 0-dimensional objects\nNumPy has both scalars and 0-dimensional arrays.  Scalars are immutable numeric types that (at least for cases where the type is compatible) are instances of the relevant python numeric type, e.g.:\n>>> isinstance(np.double(5), float)\nTrue\n>>> isinstance(np.float(5),float)\nTrue\n\nThey also \u201cquack\u201d like 0-dimensional arrays, i.e. they have shape and ndim attributes:\n>>> np.double(5).shape\n()\n>>> np.double(5).ndim\n0\n\nThese properties allow you to pass scalar types to libraries outside of NumPy (even ones that do explicit type checking via isinstance or e.g. PyFloat_Check) and to mix NumPy scalars and nd-arrays in code (as long as you don\u2019t try to mutate a scalar).\nOn the other handle, 0-dimensional arrays are mutable types that are not instances of python numeric types:\n>>> a=np.array(0.5)\n>>> a[()]=2\n>>> a\narray(2.0)\n>>> isinstance(np.array(0.5), float)\nFalse\n>>> isinstance(np.array(0.5), np.ndarray)\nTrue\n\nThus, at least for libraries that do explicit type checking, one would need to cast the 0-dimensional array to the correct python or NumPy scalar type before converting it (np.asscalar will do this)\nDo we want Scalars or 0-dimensional Variables or both in PyTorch?\nI\u2019m going to argue that we only want 0-dimensional Variables.\nFirst, we want at least 0-dimensional Variables in PyTorch.  Consider if we had only (immutable) Scalars and your loss is a .sum() over some variable.  Now, if you treat this as an immutable scalar, either it has a grad and can be optimized (which is weird for an immutable value), or it doesn\u2019t (and you have to hack around that in some way to still be able to do training).  You could also represent it as a 1-dimensional Variable (which is what we do today), but then the dimensions are inconsistent (i.e. reduction functions reduce the dimension by 1 except when the dimension is 1).  Representing this quantity as a 0-dimensional Variable is just much more straightforward and mathematically consistent.\nNow, if we have 0-dimensional Variables, do we need immutable scalars in autograd?  I\u2019d argue no:\n\nThe ability to use Scalars in external libraries seem like not a strong reason for the extra complexity: many operations should just work on 0-dimensional arrays (because we define a number of numerical operators on Variables) and if we provide conversions, in the worst case users can just call the conversions.\nImmutability in-and-of-itself may be useful, but its usefulness is not just restricted to scalar types; you may want immutability on all tensor/Variable types.\nA related question is what to return when indexing the last dimension of a Variable.  The normal python, numpy, and current Tensor semantics are that it gives you an (immutable) scalar type, while the current Variable semantics are that it gives you a Variable that shares storage (of the wrong dimensionality).  Given that we already return Variables in this case, returning a 0-dimensional one seems acceptable.\n\nI\u2019ll also note here that TensorFlow only supports 0-dimensional tensors, although it\u2019s quite different because the execution doesn\u2019t define the graph, as in PyTorch.\nMany of the other arguments for scalars are not about immutability, but convenience, e.g. it\u2019s nice to get pretty prints (i.e. \u201c0\u201d or \u201ctorch.FloatScalar(0)\u201d instead of something like:\n5\n[torch.FloatTensor of size ()]\n\nThose kinds of convenience problems are solvable without adding scalar types.\nA Proposal for adding 0-dimensional support to autograd\nAs mentioned above, the goal here is to get to a sensible Variable API; this doesn\u2019t purport to solve all issues with scalars (i.e. forced synchronization), but that we can work on those issues separately while maintaining the API as is.\n\n\nAutograd forward functions can now return a python number to represent that a 0-dimensional Variable should be returned to the .apply caller.  Note that they can\u2019t return 0-dimensional objects directly because the forward functions are written in terms of tensors, which don\u2019t support 0-dimensions yet.  Note also that this will now match what the equivalent tensor functions do (i.e. max() returns a python number), and when they support 0-dimensional tensors, the code will just work as written.  This won\u2019t immediately solve all dimensionality problems, because for example max(dim=0) on a 1-dimensional tensor will still return a 1-dimensional tensor, but again, this will automatically be fixed when the tensor APIs are changed.\n\n\nVariable is now 0-dimensional aware (via its underlying ATen implementation).  For example:\n.dim() will return 0\n.size() will return ()\n.shape will return ()\nindexing with a dimension will throw an error (i.e. [()] and [...] will work, everything else will throw an error)\nAll other functions will behave as before.  Note that accessing .data will return a 1-dimensional tensor, but again, that will automatically change when tensor changes.\n\n\nWe will provide constructors for 0-dimensional Variables.  Unfortunately, a python number passed to a tensor constructor is interpreted as a size: i.e. torch.FloatTensor(5) creates a 1-dimensional tensor of size 5, so we eventually need a different name when we support 0-dimensions on tensors.  I propose torch.*Scalar (e.g. torch.FloatScalar); this has a more object-looking name than say, torch.float(...), which will make it more obvious it\u2019s a mutable type.  One question is how this should interact with the work in (#2633).  A full \u201cleap-of-faith\u201d approach would be to have this \u201cconstructor\u201d always return a Variable and have \u2018requires_grad\u2019 and \u2018volatile\u2019 parameters.  This name will probably be confusing in the short term because you would expect torch.FloatScalar to return a tensor type, but constructing these objects directly will probably be rare in the short term anyway.\n\n\n__str__ on a 0-dimensional tensor will return the underlying number, __repr__ will return how it is constructed, i.e. torch.FloatScalar(5).  This is consistent with numpy 0-dimensional arrrays:\n\n\n>>> print(np.array(0))\n0\n>>> np.array(0)\narray(0)\n\n\nWe will provide conversion functions (__int__, __float__, more?) for 0-dimensional Variables for ease of use with existing python libraries.  Calling these functions on non-0-dimensional Variables will throw an error.\n\nThoughts?", "body": "I had a discussion with @colesbury, @apaszke, @ezyang about this.  Here are some notes from that discussion followed by my own proposal.  The below discussion is limited to \u201cScalars\u201d in autograd; we probably want to move the Tensor and Variable API together in some some sensible way (see https://github.com/pytorch/pytorch/issues/2633), but I don\u2019t want to muddy the discussion of autograd Scalars with the details of that process as much as is feasible.\r\n\r\n**Scalars vs 0-dimensional objects**\r\n\r\nNumPy has both scalars and 0-dimensional arrays.  Scalars are immutable numeric types that (at least for cases where the type is compatible) are instances of the relevant python numeric type, e.g.:\r\n```\r\n>>> isinstance(np.double(5), float)\r\nTrue\r\n>>> isinstance(np.float(5),float)\r\nTrue\r\n```\r\nThey also \u201cquack\u201d like 0-dimensional arrays, i.e. they have `shape` and `ndim` attributes:\r\n```\r\n>>> np.double(5).shape\r\n()\r\n>>> np.double(5).ndim\r\n0\r\n```\r\nThese properties allow you to pass scalar types to libraries outside of NumPy (even ones that do explicit type checking via isinstance or e.g. PyFloat_Check) and to mix NumPy scalars and nd-arrays in code (as long as you don\u2019t try to mutate a scalar).\r\n\r\nOn the other handle, 0-dimensional arrays are mutable types that are not instances of python numeric types:\r\n```\r\n>>> a=np.array(0.5)\r\n>>> a[()]=2\r\n>>> a\r\narray(2.0)\r\n>>> isinstance(np.array(0.5), float)\r\nFalse\r\n>>> isinstance(np.array(0.5), np.ndarray)\r\nTrue\r\n```\r\nThus, at least for libraries that do explicit type checking, one would need to cast the 0-dimensional array to the correct python or NumPy scalar type before converting it (np.asscalar will do this)\r\n\r\n**Do we want Scalars or 0-dimensional Variables or both in PyTorch?**\r\n\r\nI\u2019m going to argue that we only want 0-dimensional Variables.\r\n\r\nFirst, we want at least 0-dimensional Variables in PyTorch.  Consider if we had only (immutable) Scalars and your loss is a .sum() over some variable.  Now, if you treat this as an immutable scalar, either it has a grad and can be optimized (which is weird for an immutable value), or it doesn\u2019t (and you have to hack around that in some way to still be able to do training).  You could also represent it as a 1-dimensional Variable (which is what we do today), but then the dimensions are inconsistent (i.e. reduction functions reduce the dimension by 1 except when the dimension is 1).  Representing this quantity as a 0-dimensional Variable is just much more straightforward and mathematically consistent.\r\n\r\nNow, if we have 0-dimensional Variables, do we need immutable scalars in autograd?  I\u2019d argue no:\r\n1) The ability to use Scalars in external libraries seem like not a strong reason for the extra complexity: many operations should just work on 0-dimensional arrays (because we define a number of numerical operators on Variables) and if we provide conversions, in the worst case users can just call the conversions.\r\n2) Immutability in-and-of-itself may be useful, but its usefulness is not just restricted to scalar types; you may want immutability on all tensor/Variable types.\r\n3) A related question is what to return when indexing the last dimension of a Variable.  The normal python, numpy, and current Tensor semantics are that it gives you an (immutable) scalar type, while the current Variable semantics are that it gives you a Variable that shares storage (of the wrong dimensionality).  Given that we already return Variables in this case, returning a 0-dimensional one seems acceptable.\r\n\r\nI\u2019ll also note here that TensorFlow only supports 0-dimensional tensors, although it\u2019s quite different because the execution doesn\u2019t define the graph, as in PyTorch.\r\n\r\nMany of the other arguments for scalars are not about immutability, but convenience, e.g. it\u2019s nice to get pretty prints (i.e. \u201c0\u201d or \u201ctorch.FloatScalar(0)\u201d instead of something like:\r\n ```\r\n 5\r\n[torch.FloatTensor of size ()]\r\n```\r\n\r\nThose kinds of convenience problems are solvable without adding scalar types.\r\n\r\n**A Proposal for adding 0-dimensional support to autograd**\r\n\r\n\r\nAs mentioned above, the goal here is to get to a sensible Variable API; this doesn\u2019t purport to solve all issues with scalars (i.e. forced synchronization), but that we can work on those issues separately while maintaining the API as is.\r\n\r\n1) Autograd forward functions can now return a python number to represent that a 0-dimensional Variable should be returned to the .apply caller.  Note that they can\u2019t return 0-dimensional objects directly because the forward functions are written in terms of tensors, which don\u2019t support 0-dimensions yet.  Note also that this will now match what the equivalent tensor functions do (i.e. `max()` returns a python number), and when they support 0-dimensional tensors, the code will just work as written.  This won\u2019t immediately solve all dimensionality problems, because for example `max(dim=0)` on a 1-dimensional tensor will still return a 1-dimensional tensor, but again, this will automatically be fixed when the tensor APIs are changed.\r\n\r\n2) Variable is now 0-dimensional aware (via its underlying ATen implementation).  For example:\r\n`.dim()` will return 0\r\n`.size()` will return ()\r\n`.shape` will return ()\r\nindexing with a dimension will throw an error (i.e. `[()]` and `[...]` will work, everything else will throw an error)\r\nAll other functions will behave as before.  Note that accessing .data will return a 1-dimensional tensor, but again, that will automatically change when tensor changes.\r\n\r\n3) We will provide constructors for 0-dimensional Variables.  Unfortunately, a python number passed to a tensor constructor is interpreted as a size: i.e. `torch.FloatTensor(5)` creates a 1-dimensional tensor of size 5, so we eventually need a different name when we support 0-dimensions on tensors.  I propose torch.*Scalar (e.g. `torch.FloatScalar`); this has a more object-looking name than say, torch.float(...), which will make it more obvious it\u2019s a mutable type.  One question is how this should interact with the work in (https://github.com/pytorch/pytorch/issues/2633).  A full \u201cleap-of-faith\u201d approach would be to have this \u201cconstructor\u201d always return a Variable and have \u2018requires_grad\u2019 and \u2018volatile\u2019 parameters.  This name will probably be confusing in the short term because you would expect `torch.FloatScalar` to return a tensor type, but constructing these objects directly will probably be rare in the short term anyway.\r\n\r\n4) `__str__` on a 0-dimensional tensor will return the underlying number, `__repr__` will return how it is constructed, i.e. `torch.FloatScalar(5)`.  This is consistent with numpy 0-dimensional arrrays:\r\n```\r\n>>> print(np.array(0))\r\n0\r\n>>> np.array(0)\r\narray(0)\r\n```\r\n\r\n5) We will provide conversion functions (`__int__`, `__float__`, more?) for 0-dimensional Variables for ease of use with existing python libraries.  Calling these functions on non-0-dimensional Variables will throw an error.\r\n\r\nThoughts?\r\n"}