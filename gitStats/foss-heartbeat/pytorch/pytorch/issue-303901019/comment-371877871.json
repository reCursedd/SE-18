{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/371877871", "html_url": "https://github.com/pytorch/pytorch/issues/5664#issuecomment-371877871", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5664", "id": 371877871, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTg3Nzg3MQ==", "user": {"login": "VladislavZavadskyy", "id": 16824835, "node_id": "MDQ6VXNlcjE2ODI0ODM1", "avatar_url": "https://avatars1.githubusercontent.com/u/16824835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VladislavZavadskyy", "html_url": "https://github.com/VladislavZavadskyy", "followers_url": "https://api.github.com/users/VladislavZavadskyy/followers", "following_url": "https://api.github.com/users/VladislavZavadskyy/following{/other_user}", "gists_url": "https://api.github.com/users/VladislavZavadskyy/gists{/gist_id}", "starred_url": "https://api.github.com/users/VladislavZavadskyy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VladislavZavadskyy/subscriptions", "organizations_url": "https://api.github.com/users/VladislavZavadskyy/orgs", "repos_url": "https://api.github.com/users/VladislavZavadskyy/repos", "events_url": "https://api.github.com/users/VladislavZavadskyy/events{/privacy}", "received_events_url": "https://api.github.com/users/VladislavZavadskyy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-09T17:10:59Z", "updated_at": "2018-03-09T17:10:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> Here you go:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">SomeModel</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.cell <span class=\"pl-k\">=</span> nn.GRUCell(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>)\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">batch_major</span>):\n        batch_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>batch_major\n        h <span class=\"pl-k\">=</span> Variable(torch.zeros(x.shape[batch_dim],<span class=\"pl-c1\">10</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>).cuda()\n        <span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(x.shape[<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>batch_dim]):\n            x_t <span class=\"pl-k\">=</span> x[:,t] <span class=\"pl-k\">if</span> batch_major <span class=\"pl-k\">else</span> x[t]\n            h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.cell(x_t, h)\n        <span class=\"pl-k\">return</span> h\n\nm <span class=\"pl-k\">=</span> SomeModel()\nm <span class=\"pl-k\">=</span> nn.DataParallel(m, [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>]).cuda()\ndata <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">11</span>,<span class=\"pl-c1\">11</span>,<span class=\"pl-c1\">10</span>)).cuda()\nm(data, <span class=\"pl-c1\">True</span>)</pre></div>\n<p>Works fine, but</p>\n<div class=\"highlight highlight-source-python\"><pre>m <span class=\"pl-k\">=</span> SomeModel()\nm <span class=\"pl-k\">=</span> nn.DataParallel(m, [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>).cuda()\ndata <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">11</span>,<span class=\"pl-c1\">11</span>,<span class=\"pl-c1\">10</span>)).cuda()\nm(data, <span class=\"pl-c1\">False</span>)</pre></div>\n<p>throws above mentioned error.</p>\n<p>I was going to go with:</p>\n<div class=\"highlight highlight-source-python\"><pre>m <span class=\"pl-k\">=</span> nn.GRU(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>)\nm <span class=\"pl-k\">=</span> torch.nn.DataParallel(m, [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>]).cuda()\ndata <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>)).cuda()\nm(data)</pre></div>\n<p>but it <a href=\"https://gist.github.com/VladislavZavadskyy/b223c1a704f067ea00f4397e308756a9\">deserves</a> an issue of it's own (my guess it's due to cuDNN backend, fails even without DataParallel).</p>", "body_text": "@fmassa Here you go:\nclass SomeModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cell = nn.GRUCell(10,10)\n    \n    def forward(self, x, batch_major):\n        batch_dim = 1-batch_major\n        h = Variable(torch.zeros(x.shape[batch_dim],10), requires_grad=False).cuda()\n        for t in range(x.shape[1-batch_dim]):\n            x_t = x[:,t] if batch_major else x[t]\n            h = self.cell(x_t, h)\n        return h\n\nm = SomeModel()\nm = nn.DataParallel(m, [0,1,2]).cuda()\ndata = Variable(torch.randn(11,11,10)).cuda()\nm(data, True)\nWorks fine, but\nm = SomeModel()\nm = nn.DataParallel(m, [0,1,2], dim=1).cuda()\ndata = Variable(torch.randn(11,11,10)).cuda()\nm(data, False)\nthrows above mentioned error.\nI was going to go with:\nm = nn.GRU(10,10)\nm = torch.nn.DataParallel(m, [0,1,2]).cuda()\ndata = Variable(torch.randn(10,10,10)).cuda()\nm(data)\nbut it deserves an issue of it's own (my guess it's due to cuDNN backend, fails even without DataParallel).", "body": "@fmassa Here you go:\r\n```python\r\nclass SomeModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.cell = nn.GRUCell(10,10)\r\n    \r\n    def forward(self, x, batch_major):\r\n        batch_dim = 1-batch_major\r\n        h = Variable(torch.zeros(x.shape[batch_dim],10), requires_grad=False).cuda()\r\n        for t in range(x.shape[1-batch_dim]):\r\n            x_t = x[:,t] if batch_major else x[t]\r\n            h = self.cell(x_t, h)\r\n        return h\r\n\r\nm = SomeModel()\r\nm = nn.DataParallel(m, [0,1,2]).cuda()\r\ndata = Variable(torch.randn(11,11,10)).cuda()\r\nm(data, True)\r\n```\r\nWorks fine, but\r\n```python\r\nm = SomeModel()\r\nm = nn.DataParallel(m, [0,1,2], dim=1).cuda()\r\ndata = Variable(torch.randn(11,11,10)).cuda()\r\nm(data, False)\r\n```\r\nthrows above mentioned error.\r\n\r\n\r\nI was going to go with:\r\n```python\r\nm = nn.GRU(10,10)\r\nm = torch.nn.DataParallel(m, [0,1,2]).cuda()\r\ndata = Variable(torch.randn(10,10,10)).cuda()\r\nm(data)\r\n```\r\nbut it [deserves](https://gist.github.com/VladislavZavadskyy/b223c1a704f067ea00f4397e308756a9) an issue of it's own (my guess it's due to cuDNN backend, fails even without DataParallel)."}