{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3241", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3241/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3241/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3241/events", "html_url": "https://github.com/pytorch/pytorch/issues/3241", "id": 267768079, "node_id": "MDU6SXNzdWUyNjc3NjgwNzk=", "number": 3241, "title": "ATen autograd functions don't handle None/undefined ATen grad tensors", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-10-23T18:22:11Z", "updated_at": "2017-11-08T05:15:51Z", "closed_at": "2017-11-06T23:43:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If a backward function returns None/ATen undefined tensor, it can be passed to a downstream ATen autograd function as a grad, but ATen autograd functions in general do not check that their grad arguments are defined.  This is in contrast to python autograd functions which will expand the undefined tensor to a tensor of 0s of the correct size/type/device.  I ran into this when implementing <code>chunk</code>s backward, but it's possible to hit this even for single-output functions.</p>\n<p>Reproduction steps (for single-output function):</p>\n<ol>\n<li>Define a \"myz\" function that returns a <code>None</code> in its backward as follows:</li>\n</ol>\n<pre><code>diff --git a/torch/autograd/_functions/basic_ops.py b/torch/autograd/_functions/basic_ops.py\nindex ef3b75b..5a705e7 100644\n--- a/torch/autograd/_functions/basic_ops.py\n+++ b/torch/autograd/_functions/basic_ops.py\n@@ -42,6 +42,16 @@ class PowConstant(Function):\n             var_result, = ctx.saved_variables\n             return None, grad_output.mul(var_result).mul_(math.log(ctx.constant))\n\n+class MyZ(Function):\n+\n+    @staticmethod\n+    def forward(ctx, a):\n+        return a * 0\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return None\n+\n\n @traceable\n class Negate(InplaceFunction):\ndiff --git a/torch/autograd/variable.py b/torch/autograd/variable.py\nindex 5298090..31277f8 100644\n--- a/torch/autograd/variable.py\n+++ b/torch/autograd/variable.py\n@@ -519,6 +519,9 @@ class Variable(_C._VariableBase):\n     def __hash__(self):\n         return id(self)\n\n+    def myz(self):\n+        return MyZ.apply(self)\n+\n     class _torch(object):\n\n         @staticmethod\n</code></pre>\n<p>Then run the following script:</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\na=Variable(torch.randn(1), requires_grad=True)\nb=Variable(torch.randn(1), requires_grad=True)\n(a.mul(b)).myz().backward()\n</code></pre>\n<p>This results in a segmentation fault, although there is an open issue in ATen to throw a proper exception.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>", "body_text": "If a backward function returns None/ATen undefined tensor, it can be passed to a downstream ATen autograd function as a grad, but ATen autograd functions in general do not check that their grad arguments are defined.  This is in contrast to python autograd functions which will expand the undefined tensor to a tensor of 0s of the correct size/type/device.  I ran into this when implementing chunks backward, but it's possible to hit this even for single-output functions.\nReproduction steps (for single-output function):\n\nDefine a \"myz\" function that returns a None in its backward as follows:\n\ndiff --git a/torch/autograd/_functions/basic_ops.py b/torch/autograd/_functions/basic_ops.py\nindex ef3b75b..5a705e7 100644\n--- a/torch/autograd/_functions/basic_ops.py\n+++ b/torch/autograd/_functions/basic_ops.py\n@@ -42,6 +42,16 @@ class PowConstant(Function):\n             var_result, = ctx.saved_variables\n             return None, grad_output.mul(var_result).mul_(math.log(ctx.constant))\n\n+class MyZ(Function):\n+\n+    @staticmethod\n+    def forward(ctx, a):\n+        return a * 0\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return None\n+\n\n @traceable\n class Negate(InplaceFunction):\ndiff --git a/torch/autograd/variable.py b/torch/autograd/variable.py\nindex 5298090..31277f8 100644\n--- a/torch/autograd/variable.py\n+++ b/torch/autograd/variable.py\n@@ -519,6 +519,9 @@ class Variable(_C._VariableBase):\n     def __hash__(self):\n         return id(self)\n\n+    def myz(self):\n+        return MyZ.apply(self)\n+\n     class _torch(object):\n\n         @staticmethod\n\nThen run the following script:\nimport torch\nfrom torch.autograd import Variable\na=Variable(torch.randn(1), requires_grad=True)\nb=Variable(torch.randn(1), requires_grad=True)\n(a.mul(b)).myz().backward()\n\nThis results in a segmentation fault, although there is an open issue in ATen to throw a proper exception.\nCC @colesbury", "body": "If a backward function returns None/ATen undefined tensor, it can be passed to a downstream ATen autograd function as a grad, but ATen autograd functions in general do not check that their grad arguments are defined.  This is in contrast to python autograd functions which will expand the undefined tensor to a tensor of 0s of the correct size/type/device.  I ran into this when implementing `chunk`s backward, but it's possible to hit this even for single-output functions.\r\n\r\nReproduction steps (for single-output function):\r\n1) Define a \"myz\" function that returns a `None` in its backward as follows:\r\n```\r\ndiff --git a/torch/autograd/_functions/basic_ops.py b/torch/autograd/_functions/basic_ops.py\r\nindex ef3b75b..5a705e7 100644\r\n--- a/torch/autograd/_functions/basic_ops.py\r\n+++ b/torch/autograd/_functions/basic_ops.py\r\n@@ -42,6 +42,16 @@ class PowConstant(Function):\r\n             var_result, = ctx.saved_variables\r\n             return None, grad_output.mul(var_result).mul_(math.log(ctx.constant))\r\n\r\n+class MyZ(Function):\r\n+\r\n+    @staticmethod\r\n+    def forward(ctx, a):\r\n+        return a * 0\r\n+\r\n+    @staticmethod\r\n+    def backward(ctx, grad_output):\r\n+        return None\r\n+\r\n\r\n @traceable\r\n class Negate(InplaceFunction):\r\ndiff --git a/torch/autograd/variable.py b/torch/autograd/variable.py\r\nindex 5298090..31277f8 100644\r\n--- a/torch/autograd/variable.py\r\n+++ b/torch/autograd/variable.py\r\n@@ -519,6 +519,9 @@ class Variable(_C._VariableBase):\r\n     def __hash__(self):\r\n         return id(self)\r\n\r\n+    def myz(self):\r\n+        return MyZ.apply(self)\r\n+\r\n     class _torch(object):\r\n\r\n         @staticmethod\r\n```\r\n\r\nThen run the following script:\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\na=Variable(torch.randn(1), requires_grad=True)\r\nb=Variable(torch.randn(1), requires_grad=True)\r\n(a.mul(b)).myz().backward()\r\n```\r\nThis results in a segmentation fault, although there is an open issue in ATen to throw a proper exception.\r\n\r\nCC @colesbury "}