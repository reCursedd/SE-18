{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12489", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12489/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12489/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12489/events", "html_url": "https://github.com/pytorch/pytorch/issues/12489", "id": 368320307, "node_id": "MDU6SXNzdWUzNjgzMjAzMDc=", "number": 12489, "title": "Providing a way to ignore \"size checking\" during backward for manually defined autograd.Function", "user": {"login": "phizaz", "id": 451667, "node_id": "MDQ6VXNlcjQ1MTY2Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/451667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phizaz", "html_url": "https://github.com/phizaz", "followers_url": "https://api.github.com/users/phizaz/followers", "following_url": "https://api.github.com/users/phizaz/following{/other_user}", "gists_url": "https://api.github.com/users/phizaz/gists{/gist_id}", "starred_url": "https://api.github.com/users/phizaz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phizaz/subscriptions", "organizations_url": "https://api.github.com/users/phizaz/orgs", "repos_url": "https://api.github.com/users/phizaz/repos", "events_url": "https://api.github.com/users/phizaz/events{/privacy}", "received_events_url": "https://api.github.com/users/phizaz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-09T17:37:41Z", "updated_at": "2018-10-17T00:17:07Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>Providing a way to ignore \"size checking\" during backward for manually defined <code>autograd.Function</code>.</p>\n<h2>Motivation</h2>\n<p>I'm trying to implement a straight-through gradient estimator for a categorical distribution sampling.</p>\n<p>Let's say that I have <code>P(x)</code> from which I sample <code>x</code>. I implement an <code>autograd.Function</code> for this process using straight-through estimator.</p>\n<p>In my scenario, I need to feed this <code>x</code> to an embedding function to get its vector representation.</p>\n<p>Note: the <code>F.embedding</code> doesn't support gradient through <code>x</code> (of course), I need to also implement an <code>autograd.Function</code> for this as well.</p>\n<p>Here is the problem:</p>\n<p>The input of the my embedding function is of shape <code>(batch,)</code> because <code>x</code> is just a list of ints. But its gradients aren't. Thanks to the straight through estimator, input of shape <code>(batch,)</code> could have gradients of other shape like <code>(batch, 10)</code> where 10 is the number of embeddings.</p>\n<p>When I tried to implement so, Pytorch cried at me saying:</p>\n<pre><code>~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     88     Variable._execution_engine.run_backward(\n     89         tensors, grad_tensors, retain_graph, create_graph,\n---&gt; 90         allow_unreachable=True)  # allow_unreachable flag\n     91 \n     92 \n\nRuntimeError: Function EmbeddingBackward returned an invalid gradient at index 0 - got [32, 10] but expected shape compatible with [32]\n</code></pre>\n<p>I interpret this message to be of some kind of size checking mechanism of the autograd. Is it a good idea to provide a way to switch this off?</p>\n<h2>Pitch</h2>\n<p>I know that size checking is a good feature. I don't know where to put this switch off button at.</p>\n<h2>Alternatives</h2>\n<p>I could think of an alternative:</p>\n<p>Use one-hot encoding rather than a list of ints. This way I ensure that the shape will be preserved. Using one-hot is not a deal breaker though, I will go with this for a moment.</p>\n<h2>Additional context</h2>\n<p>Pytorch version: 1.0.0.dev20181002</p>", "body_text": "\ud83d\ude80 Feature\nProviding a way to ignore \"size checking\" during backward for manually defined autograd.Function.\nMotivation\nI'm trying to implement a straight-through gradient estimator for a categorical distribution sampling.\nLet's say that I have P(x) from which I sample x. I implement an autograd.Function for this process using straight-through estimator.\nIn my scenario, I need to feed this x to an embedding function to get its vector representation.\nNote: the F.embedding doesn't support gradient through x (of course), I need to also implement an autograd.Function for this as well.\nHere is the problem:\nThe input of the my embedding function is of shape (batch,) because x is just a list of ints. But its gradients aren't. Thanks to the straight through estimator, input of shape (batch,) could have gradients of other shape like (batch, 10) where 10 is the number of embeddings.\nWhen I tried to implement so, Pytorch cried at me saying:\n~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     88     Variable._execution_engine.run_backward(\n     89         tensors, grad_tensors, retain_graph, create_graph,\n---> 90         allow_unreachable=True)  # allow_unreachable flag\n     91 \n     92 \n\nRuntimeError: Function EmbeddingBackward returned an invalid gradient at index 0 - got [32, 10] but expected shape compatible with [32]\n\nI interpret this message to be of some kind of size checking mechanism of the autograd. Is it a good idea to provide a way to switch this off?\nPitch\nI know that size checking is a good feature. I don't know where to put this switch off button at.\nAlternatives\nI could think of an alternative:\nUse one-hot encoding rather than a list of ints. This way I ensure that the shape will be preserved. Using one-hot is not a deal breaker though, I will go with this for a moment.\nAdditional context\nPytorch version: 1.0.0.dev20181002", "body": "## \ud83d\ude80 Feature\r\nProviding a way to ignore \"size checking\" during backward for manually defined `autograd.Function`.\r\n\r\n## Motivation\r\nI'm trying to implement a straight-through gradient estimator for a categorical distribution sampling.\r\n\r\nLet's say that I have `P(x)` from which I sample `x`. I implement an `autograd.Function` for this process using straight-through estimator. \r\n\r\nIn my scenario, I need to feed this `x` to an embedding function to get its vector representation.\r\n\r\nNote: the `F.embedding` doesn't support gradient through `x` (of course), I need to also implement an `autograd.Function` for this as well. \r\n\r\nHere is the problem:\r\n\r\nThe input of the my embedding function is of shape `(batch,)` because `x` is just a list of ints. But its gradients aren't. Thanks to the straight through estimator, input of shape `(batch,)` could have gradients of other shape like `(batch, 10)` where 10 is the number of embeddings.\r\n\r\nWhen I tried to implement so, Pytorch cried at me saying:\r\n\r\n```\r\n~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     88     Variable._execution_engine.run_backward(\r\n     89         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 90         allow_unreachable=True)  # allow_unreachable flag\r\n     91 \r\n     92 \r\n\r\nRuntimeError: Function EmbeddingBackward returned an invalid gradient at index 0 - got [32, 10] but expected shape compatible with [32]\r\n```\r\nI interpret this message to be of some kind of size checking mechanism of the autograd. Is it a good idea to provide a way to switch this off? \r\n\r\n## Pitch\r\nI know that size checking is a good feature. I don't know where to put this switch off button at.\r\n\r\n## Alternatives\r\nI could think of an alternative: \r\n\r\nUse one-hot encoding rather than a list of ints. This way I ensure that the shape will be preserved. Using one-hot is not a deal breaker though, I will go with this for a moment.\r\n\r\n## Additional context\r\nPytorch version: 1.0.0.dev20181002\r\n"}