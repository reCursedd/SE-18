{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/428442970", "html_url": "https://github.com/pytorch/pytorch/issues/12489#issuecomment-428442970", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12489", "id": 428442970, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODQ0Mjk3MA==", "user": {"login": "phizaz", "id": 451667, "node_id": "MDQ6VXNlcjQ1MTY2Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/451667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phizaz", "html_url": "https://github.com/phizaz", "followers_url": "https://api.github.com/users/phizaz/followers", "following_url": "https://api.github.com/users/phizaz/following{/other_user}", "gists_url": "https://api.github.com/users/phizaz/gists{/gist_id}", "starred_url": "https://api.github.com/users/phizaz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phizaz/subscriptions", "organizations_url": "https://api.github.com/users/phizaz/orgs", "repos_url": "https://api.github.com/users/phizaz/repos", "events_url": "https://api.github.com/users/phizaz/events{/privacy}", "received_events_url": "https://api.github.com/users/phizaz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T05:30:52Z", "updated_at": "2018-10-10T05:33:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> It sure breaks in most cases. But in my case, I have two of my own implementations of <code>autograd.Function</code> and I make sure that these two work together.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>P(x)</th>\n<th>sampling</th>\n<th>embedding(x)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>output shape</td>\n<td>(n_batch, n_class)</td>\n<td>(n_batch,)</td>\n<td>(n_batch, n_emb)</td>\n</tr>\n<tr>\n<td>grad shape</td>\n<td>(n_batch, n_class)</td>\n<td><strong>straight through</strong> (n_batch, n_class)</td>\n<td>(n_batch, n_emb)</td>\n</tr>\n</tbody>\n</table>\n<p>And the alternative is like:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>P(x)</th>\n<th>sampling</th>\n<th>embedding(x)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>output shape</td>\n<td>(n_batch, n_class)</td>\n<td><strong>one-hot</strong> (n_batch, n_class)</td>\n<td>(n_batch, n_emb)</td>\n</tr>\n<tr>\n<td>grad shape</td>\n<td>(n_batch, n_class)</td>\n<td>straight through (n_batch, n_class)</td>\n<td>(n_batch, n_emb)</td>\n</tr>\n</tbody>\n</table>\n<p>So no problem in this case.</p>", "body_text": "@apaszke It sure breaks in most cases. But in my case, I have two of my own implementations of autograd.Function and I make sure that these two work together.\n\n\n\n\nP(x)\nsampling\nembedding(x)\n\n\n\n\noutput shape\n(n_batch, n_class)\n(n_batch,)\n(n_batch, n_emb)\n\n\ngrad shape\n(n_batch, n_class)\nstraight through (n_batch, n_class)\n(n_batch, n_emb)\n\n\n\nAnd the alternative is like:\n\n\n\n\nP(x)\nsampling\nembedding(x)\n\n\n\n\noutput shape\n(n_batch, n_class)\none-hot (n_batch, n_class)\n(n_batch, n_emb)\n\n\ngrad shape\n(n_batch, n_class)\nstraight through (n_batch, n_class)\n(n_batch, n_emb)\n\n\n\nSo no problem in this case.", "body": "@apaszke It sure breaks in most cases. But in my case, I have two of my own implementations of `autograd.Function` and I make sure that these two work together. \r\n\r\n| | P(x) | sampling | embedding(x)\r\n| --- | --- | --- | --- |\r\n| output shape | (n_batch, n_class) | (n_batch,) | (n_batch, n_emb) \r\n| grad shape | (n_batch, n_class) | **straight through** (n_batch, n_class) | (n_batch, n_emb)\r\n\r\nAnd the alternative is like: \r\n\r\n| | P(x) | sampling | embedding(x)\r\n| --- | --- | --- | --- |\r\n| output shape | (n_batch, n_class) | **one-hot** (n_batch, n_class) | (n_batch, n_emb) \r\n| grad shape | (n_batch, n_class) | straight through (n_batch, n_class) | (n_batch, n_emb)\r\n\r\nSo no problem in this case."}