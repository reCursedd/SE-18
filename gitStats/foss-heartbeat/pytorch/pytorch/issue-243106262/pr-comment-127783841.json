{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/127783841", "pull_request_review_id": 50403161, "id": 127783841, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNzc4Mzg0MQ==", "diff_hunk": "@@ -7,54 +7,107 @@\n \n \n class PReLU(Function):\n-\n-    def forward(self, input, weight):\n-        self._backend = type2backend[type(input)]\n+    @staticmethod\n+    def forward(ctx, input, weight):\n+        ctx._backend = type2backend[type(input)]\n         output = input.new()\n-        self.num_parameters = weight.numel()\n-        if self.num_parameters == 1:\n-            self.num_parameters = 0\n-        self._backend.PReLU_updateOutput(\n-            self._backend.library_state,\n+        ctx.num_parameters = weight.numel()\n+        if ctx.num_parameters == 1:\n+            ctx.num_parameters = 0\n+        ctx._backend.PReLU_updateOutput(\n+            ctx._backend.library_state,\n             input,\n             output,\n             weight,\n-            self.num_parameters\n+            ctx.num_parameters\n         )\n-        self.save_for_backward(input, weight)\n+        ctx.save_for_backward(input, weight)\n         return output\n \n-    def backward(self, grad_output):\n-        input, weight = self.saved_tensors\n-        # TODO: check if requires grad\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        input, weight = ctx.saved_variables\n+        # alternatively, we could recalculate _backend, num_parameters\n+        return PReLUBackward.apply(input, weight, grad_output, ctx._backend, ctx.num_parameters)\n+\n+\n+class PReLUBackward(Function):\n+    @staticmethod\n+    def forward(ctx, input, weight, grad_output, backend, num_parameters):\n+        ctx.save_for_backward(input, weight, grad_output)\n+        ctx.num_parameters = num_parameters\n         grad_input = input.new()\n-        self._backend.PReLU_updateGradInput(\n-            self._backend.library_state,\n+        backend.PReLU_updateGradInput(\n+            backend.library_state,\n             input,\n             grad_output,\n             grad_input,\n             weight,\n-            self.num_parameters\n+            num_parameters\n         )\n \n         buf = weight.new()\n         buf2 = weight.new()\n         # TODO: this won't have to be zeroed in the future\n         grad_weight = weight.new().resize_as_(weight).zero_()\n-        self._backend.PReLU_accGradParameters(\n-            self._backend.library_state,\n+        backend.PReLU_accGradParameters(\n+            backend.library_state,\n             input,\n             grad_output,\n             grad_input,\n             weight,\n             grad_weight,\n             buf,\n             buf2,\n-            self.num_parameters,\n+            num_parameters,\n             1\n         )\n         return grad_input, grad_weight\n \n+    @staticmethod\n+    def backward(ctx, ggI, ggW):\n+        input, weight, gO = ctx.saved_variables\n+        positive_mask = (input > 0).type_as(ggI)\n+        nonpositive_mask = (input <= 0).type_as(ggW)\n+        # Explanation: Let input be i, weight be w, grad_output be gO.\n+        # f(i, w) = i  if i > 0\n+        #         = wi if i <= 0\n+        # df/dx * gO  = gO      if i > 0      df/dw * g0 = 0      if i > 0\n+        #             = g0 * w  if i <= 0                = g0 * i  if i <= 0\n+        # The rest is taking derivatives of these wrt i, w, gO and summing/expanding properly.\n+        if ctx.num_parameters == 0:\n+            mask = positive_mask + nonpositive_mask * weight.expand_as(input)\n+            ggO = ggI * mask + ggW.expand_as(gO) * (nonpositive_mask * input)\n+            return ggW.expand_as(gO) * gO * nonpositive_mask, (ggI * gO * nonpositive_mask).sum(), ggO, None, None\n+        else:\n+            # Expand ggW to match size of ggI; a simple expand doesn't work because\n+            # ggW is the size of the input channel (dim==1 unless there is only 1 dimension).", "path": "torch/nn/_functions/thnn/activation.py", "position": null, "original_position": 97, "commit_id": "62a071357fd6d39b8be2fa6472c27121dfe28e96", "original_commit_id": "2f20bcbc9f68564b0f6bedb8e91ca5427405e439", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "Am I interpreting this incorrectly?  I was going off the documentation, which says the following:\r\n\"Applies element-wise the function \\(PReLU(x) = max(0,x) + a * min(0,x)\\) Here \u201ca\u201d is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter \u201ca\u201d across all input channels. If called with nn.PReLU(nChannels), a separate \u201ca\u201d is used for each input channel\"\r\n\r\nThen, wouldn't gW and ggW be the size of the input channel?", "created_at": "2017-07-17T18:19:32Z", "updated_at": "2018-11-23T15:34:10Z", "html_url": "https://github.com/pytorch/pytorch/pull/2106#discussion_r127783841", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2106", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/127783841"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2106#discussion_r127783841"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2106"}}, "body_html": "<p>Am I interpreting this incorrectly?  I was going off the documentation, which says the following:<br>\n\"Applies element-wise the function (PReLU(x) = max(0,x) + a * min(0,x)) Here \u201ca\u201d is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter \u201ca\u201d across all input channels. If called with nn.PReLU(nChannels), a separate \u201ca\u201d is used for each input channel\"</p>\n<p>Then, wouldn't gW and ggW be the size of the input channel?</p>", "body_text": "Am I interpreting this incorrectly?  I was going off the documentation, which says the following:\n\"Applies element-wise the function (PReLU(x) = max(0,x) + a * min(0,x)) Here \u201ca\u201d is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter \u201ca\u201d across all input channels. If called with nn.PReLU(nChannels), a separate \u201ca\u201d is used for each input channel\"\nThen, wouldn't gW and ggW be the size of the input channel?", "in_reply_to_id": 127561869}