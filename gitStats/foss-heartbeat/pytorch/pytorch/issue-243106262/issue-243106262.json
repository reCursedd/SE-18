{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2106", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2106/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2106/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2106/events", "html_url": "https://github.com/pytorch/pytorch/pull/2106", "id": 243106262, "node_id": "MDExOlB1bGxSZXF1ZXN0MTMwNjcwMDU2", "number": 2106, "title": "Improve double backwards support for NN modules", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-07-14T20:39:50Z", "updated_at": "2018-11-23T15:34:14Z", "closed_at": "2017-07-25T02:07:26Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2106", "html_url": "https://github.com/pytorch/pytorch/pull/2106", "diff_url": "https://github.com/pytorch/pytorch/pull/2106.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2106.patch"}, "body_html": "<p>This PR makes a number of changes related to double backwards support for NN modules (this is not in the order of the commits):</p>\n<ol>\n<li>Adds gradgradchecks for NN modules by default.  These can be turned off by setting check_gradgrad=False in the test definition.  I went through the existing tests and set the failing ones to False; this was (almost) purely empirical, i.e. I didn't spend time figuring out why they were failing.</li>\n<li>Adds testing of double backwards with respect to the grad_output.  This caught an issue with advanced indexing that is now fixed.</li>\n<li>Implemented PReLU double backwards.  This was done in two ways:<br>\na) As a standard backwards function in terms of autograd ops  (i.e. it doesn't call the THCUNN backwards)<br>\nb) As two autograd functions, where backwards calls PReLUBackwards.apply, PReLUBackwards.forward calls the THCUNN backwards, and PReLUBackwards.backward implements double backwards in terms of autograd ops.  The idea here is to support double backwards without slowing down the first backwards.  Benchmarking showed this is useful:<br>\nI ran (32,8,512,512) PReLU 5 times through single and double backwards (anything more required GC to not OOM) on an M4:</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Version</th>\n<th>Single Backward</th>\n<th>Double Backwards</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Original</td>\n<td>0.07078</td>\n<td>Not supported</td>\n</tr>\n<tr>\n<td>Pure Autograd</td>\n<td>0.18315</td>\n<td>0.13091</td>\n</tr>\n<tr>\n<td>Fn/Fn Backwards</td>\n<td>0.07086</td>\n<td>0.16417</td>\n</tr>\n</tbody>\n</table>\n<p>So, there's not really any overhead on the first backwards to support double backwards.  I'm a bit surprised that the pure autograd is so fast on double backwards, but maybe I'm missing some optimization in my implementation.<br>\n4) auto-nn Functions and Criterions are now generated as \"new-style\" functions in the Fn/FnBackwards style.  By default double backwards just throws an error; HardTanh is implemented as a Function example and L1Loss as a Criterion example.<br>\n5) Implemented SoftSign double backwards</p>", "body_text": "This PR makes a number of changes related to double backwards support for NN modules (this is not in the order of the commits):\n\nAdds gradgradchecks for NN modules by default.  These can be turned off by setting check_gradgrad=False in the test definition.  I went through the existing tests and set the failing ones to False; this was (almost) purely empirical, i.e. I didn't spend time figuring out why they were failing.\nAdds testing of double backwards with respect to the grad_output.  This caught an issue with advanced indexing that is now fixed.\nImplemented PReLU double backwards.  This was done in two ways:\na) As a standard backwards function in terms of autograd ops  (i.e. it doesn't call the THCUNN backwards)\nb) As two autograd functions, where backwards calls PReLUBackwards.apply, PReLUBackwards.forward calls the THCUNN backwards, and PReLUBackwards.backward implements double backwards in terms of autograd ops.  The idea here is to support double backwards without slowing down the first backwards.  Benchmarking showed this is useful:\nI ran (32,8,512,512) PReLU 5 times through single and double backwards (anything more required GC to not OOM) on an M4:\n\n\n\n\nVersion\nSingle Backward\nDouble Backwards\n\n\n\n\nOriginal\n0.07078\nNot supported\n\n\nPure Autograd\n0.18315\n0.13091\n\n\nFn/Fn Backwards\n0.07086\n0.16417\n\n\n\nSo, there's not really any overhead on the first backwards to support double backwards.  I'm a bit surprised that the pure autograd is so fast on double backwards, but maybe I'm missing some optimization in my implementation.\n4) auto-nn Functions and Criterions are now generated as \"new-style\" functions in the Fn/FnBackwards style.  By default double backwards just throws an error; HardTanh is implemented as a Function example and L1Loss as a Criterion example.\n5) Implemented SoftSign double backwards", "body": "This PR makes a number of changes related to double backwards support for NN modules (this is not in the order of the commits):\r\n1) Adds gradgradchecks for NN modules by default.  These can be turned off by setting check_gradgrad=False in the test definition.  I went through the existing tests and set the failing ones to False; this was (almost) purely empirical, i.e. I didn't spend time figuring out why they were failing.\r\n2) Adds testing of double backwards with respect to the grad_output.  This caught an issue with advanced indexing that is now fixed.\r\n3) Implemented PReLU double backwards.  This was done in two ways:\r\n  a) As a standard backwards function in terms of autograd ops  (i.e. it doesn't call the THCUNN backwards)\r\n  b) As two autograd functions, where backwards calls PReLUBackwards.apply, PReLUBackwards.forward calls the THCUNN backwards, and PReLUBackwards.backward implements double backwards in terms of autograd ops.  The idea here is to support double backwards without slowing down the first backwards.  Benchmarking showed this is useful:\r\nI ran (32,8,512,512) PReLU 5 times through single and double backwards (anything more required GC to not OOM) on an M4:\r\n\r\n| Version | Single Backward | Double Backwards |\r\n| ------- | ----------------- | ------------------- |\r\n| Original                 | 0.07078 | Not supported |\r\n| Pure Autograd      | 0.18315  | 0.13091           |\r\n| Fn/Fn Backwards | 0.07086 | 0.16417           |\r\n\r\nSo, there's not really any overhead on the first backwards to support double backwards.  I'm a bit surprised that the pure autograd is so fast on double backwards, but maybe I'm missing some optimization in my implementation.\r\n4) auto-nn Functions and Criterions are now generated as \"new-style\" functions in the Fn/FnBackwards style.  By default double backwards just throws an error; HardTanh is implemented as a Function example and L1Loss as a Criterion example.\r\n5) Implemented SoftSign double backwards"}