{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/127561386", "pull_request_review_id": 50164066, "id": 127561386, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNzU2MTM4Ng==", "diff_hunk": "@@ -7,54 +7,107 @@\n \n \n class PReLU(Function):\n-\n-    def forward(self, input, weight):\n-        self._backend = type2backend[type(input)]\n+    @staticmethod\n+    def forward(ctx, input, weight):\n+        ctx._backend = type2backend[type(input)]\n         output = input.new()\n-        self.num_parameters = weight.numel()\n-        if self.num_parameters == 1:\n-            self.num_parameters = 0\n-        self._backend.PReLU_updateOutput(\n-            self._backend.library_state,\n+        ctx.num_parameters = weight.numel()\n+        if ctx.num_parameters == 1:\n+            ctx.num_parameters = 0\n+        ctx._backend.PReLU_updateOutput(\n+            ctx._backend.library_state,\n             input,\n             output,\n             weight,\n-            self.num_parameters\n+            ctx.num_parameters\n         )\n-        self.save_for_backward(input, weight)\n+        ctx.save_for_backward(input, weight)\n         return output\n \n-    def backward(self, grad_output):\n-        input, weight = self.saved_tensors\n-        # TODO: check if requires grad\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        input, weight = ctx.saved_variables\n+        # alternatively, we could recalculate _backend, num_parameters\n+        return PReLUBackward.apply(input, weight, grad_output, ctx._backend, ctx.num_parameters)\n+\n+\n+class PReLUBackward(Function):\n+    @staticmethod\n+    def forward(ctx, input, weight, grad_output, backend, num_parameters):\n+        ctx.save_for_backward(input, weight, grad_output)\n+        ctx.num_parameters = num_parameters\n         grad_input = input.new()\n-        self._backend.PReLU_updateGradInput(\n-            self._backend.library_state,\n+        backend.PReLU_updateGradInput(\n+            backend.library_state,\n             input,\n             grad_output,\n             grad_input,\n             weight,\n-            self.num_parameters\n+            num_parameters\n         )\n \n         buf = weight.new()\n         buf2 = weight.new()\n         # TODO: this won't have to be zeroed in the future\n         grad_weight = weight.new().resize_as_(weight).zero_()\n-        self._backend.PReLU_accGradParameters(\n-            self._backend.library_state,\n+        backend.PReLU_accGradParameters(\n+            backend.library_state,\n             input,\n             grad_output,\n             grad_input,\n             weight,\n             grad_weight,\n             buf,\n             buf2,\n-            self.num_parameters,\n+            num_parameters,\n             1\n         )\n         return grad_input, grad_weight\n \n+    @staticmethod\n+    def backward(ctx, ggI, ggW):\n+        input, weight, gO = ctx.saved_variables\n+        positive_mask = (input > 0).type_as(ggI)\n+        nonpositive_mask = (input <= 0).type_as(ggW)\n+        # Explanation: Let input be i, weight be w, grad_output be gO.\n+        # f(i, w) = i  if i > 0\n+        #         = wi if i <= 0\n+        # df/dx * gO  = gO      if i > 0      df/dw * g0 = 0      if i > 0\n+        #             = g0 * w  if i <= 0                = g0 * i  if i <= 0\n+        # The rest is taking derivatives of these wrt i, w, gO and summing/expanding properly.\n+        if ctx.num_parameters == 0:", "path": "torch/nn/_functions/thnn/activation.py", "position": null, "original_position": 91, "commit_id": "62a071357fd6d39b8be2fa6472c27121dfe28e96", "original_commit_id": "2f20bcbc9f68564b0f6bedb8e91ca5427405e439", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can you add a comment saying that it doesn't really mean that there are no parameters, but that there's a single weight shared among all features?", "created_at": "2017-07-14T22:36:37Z", "updated_at": "2018-11-23T15:34:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/2106#discussion_r127561386", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2106", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/127561386"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2106#discussion_r127561386"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2106"}}, "body_html": "<p>Can you add a comment saying that it doesn't really mean that there are no parameters, but that there's a single weight shared among all features?</p>", "body_text": "Can you add a comment saying that it doesn't really mean that there are no parameters, but that there's a single weight shared among all features?"}