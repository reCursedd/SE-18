{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413838113", "html_url": "https://github.com/pytorch/pytorch/issues/10582#issuecomment-413838113", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10582", "id": 413838113, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzgzODExMw==", "user": {"login": "gyani91", "id": 4917261, "node_id": "MDQ6VXNlcjQ5MTcyNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/4917261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gyani91", "html_url": "https://github.com/gyani91", "followers_url": "https://api.github.com/users/gyani91/followers", "following_url": "https://api.github.com/users/gyani91/following{/other_user}", "gists_url": "https://api.github.com/users/gyani91/gists{/gist_id}", "starred_url": "https://api.github.com/users/gyani91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gyani91/subscriptions", "organizations_url": "https://api.github.com/users/gyani91/orgs", "repos_url": "https://api.github.com/users/gyani91/repos", "events_url": "https://api.github.com/users/gyani91/events{/privacy}", "received_events_url": "https://api.github.com/users/gyani91/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T11:34:23Z", "updated_at": "2018-08-17T11:34:23Z", "author_association": "NONE", "body_html": "<p>Another two points that I think are worth mentioning are that I have followed all the changes mentioned in a <a href=\"https://github.com/caffe2/caffe2/issues/1050\" data-hovercard-type=\"issue\" data-hovercard-url=\"/caffe2/caffe2/issues/1050/hovercard\">diff</a> by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9845\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pietern\">@pietern</a>, even the changes for the Gloo header files. But it doesn't help with the situation.</p>\n<p>And I have changed the code for resnet_trainer.py from:</p>\n<pre><code>num_shards = args.num_shards\nshard_id = args.shard_id\n\ninterfaces = args.distributed_interfaces.split(\",\")\n\n# Rendezvous using MPI when run with mpirun\nif os.getenv(\"OMPI_COMM_WORLD_SIZE\") is not None:\n    num_shards = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\", 1))\n    shard_id = int(os.getenv(\"OMPI_COMM_WORLD_RANK\", 0))\n</code></pre>\n<p>to:</p>\n<pre><code>num_shards = args.num_shards\nshard_id = args.shard_id\n\ninterfaces = args.distributed_interfaces.split(\",\")\n\n# Rendezvous using MPI when run with mpirun\n#if os.getenv(\"OMPI_COMM_WORLD_SIZE\") is not None:\nif True:\n    #num_shards = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\", 1))\n    #shard_id = int(os.getenv(\"OMPI_COMM_WORLD_RANK\", 0))\n    shard_id = int(os.getenv(\"SLURM_PROCID\", 0))\n</code></pre>\n<p>before the above change the error was:</p>\n<pre><code>E0817 11:20:18.277760 29415 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.277838 29415 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.277843 29415 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nE0817 11:20:18.278333 15794 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.278412 15794 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.278416 15794 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nINFO:resnet50_trainer:Using epoch size: 1000\nINFO:resnet50_trainer:Using epoch size: 1000\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Adding gradient operators\nE0817 11:20:18.365449 16188 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.365527 16188 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.365532 16188 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nINFO:resnet50_trainer:Using epoch size: 1000\nE0817 11:20:18.377269 14509 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.377346 14509 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.377351 14509 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nINFO:resnet50_trainer:Using epoch size: 1000\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Creating barrier net\nINFO:data_parallel_model:Creating barrier net\nE0817 11:20:18.563855 15794 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3\nE0817 11:20:18.564237 15794 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.564251 15794 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nE0817 11:20:18.567664 29415 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3\nE0817 11:20:18.568056 29415 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.568071 29415 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0135490894318 secs\nINFO:memonger:Memonger memory optimization took 0.0135049819946 secs\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Creating barrier net\nINFO:data_parallel_model:Creating barrier net\nE0817 11:20:18.654834 16188 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3\nE0817 11:20:18.655194 16188 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.655208 16188 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nE0817 11:20:18.663950 14509 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3\nE0817 11:20:18.664294 14509 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.664320 14509 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0132520198822 secs\nINFO:memonger:Memonger memory optimization took 0.0132851600647 secs\nE0817 11:20:51.136189 15794 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nE0817 11:20:51.183814 29415 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nE0817 11:20:51.454591 14509 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\nE0817 11:20:51.455612 16188 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in &lt;module&gt;\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nsrun: error: nid02966: task 3: Exited with exit code 1\nsrun: Terminating job step 9072007.0\nsrun: error: nid02963: task 0: Exited with exit code 1\nsrun: error: nid02965: task 2: Exited with exit code 1\nsrun: error: nid02964: task 1: Exited with exit code 1\n</code></pre>", "body_text": "Another two points that I think are worth mentioning are that I have followed all the changes mentioned in a diff by @pietern, even the changes for the Gloo header files. But it doesn't help with the situation.\nAnd I have changed the code for resnet_trainer.py from:\nnum_shards = args.num_shards\nshard_id = args.shard_id\n\ninterfaces = args.distributed_interfaces.split(\",\")\n\n# Rendezvous using MPI when run with mpirun\nif os.getenv(\"OMPI_COMM_WORLD_SIZE\") is not None:\n    num_shards = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\", 1))\n    shard_id = int(os.getenv(\"OMPI_COMM_WORLD_RANK\", 0))\n\nto:\nnum_shards = args.num_shards\nshard_id = args.shard_id\n\ninterfaces = args.distributed_interfaces.split(\",\")\n\n# Rendezvous using MPI when run with mpirun\n#if os.getenv(\"OMPI_COMM_WORLD_SIZE\") is not None:\nif True:\n    #num_shards = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\", 1))\n    #shard_id = int(os.getenv(\"OMPI_COMM_WORLD_RANK\", 0))\n    shard_id = int(os.getenv(\"SLURM_PROCID\", 0))\n\nbefore the above change the error was:\nE0817 11:20:18.277760 29415 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.277838 29415 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.277843 29415 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nE0817 11:20:18.278333 15794 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.278412 15794 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.278416 15794 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nINFO:resnet50_trainer:Using epoch size: 1000\nINFO:resnet50_trainer:Using epoch size: 1000\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Adding gradient operators\nE0817 11:20:18.365449 16188 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.365527 16188 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.365532 16188 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nINFO:resnet50_trainer:Using epoch size: 1000\nE0817 11:20:18.377269 14509 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.377346 14509 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0817 11:20:18.377351 14509 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0]\nINFO:resnet50_trainer:Using epoch size: 1000\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Parallelizing model for devices: [0]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Creating barrier net\nINFO:data_parallel_model:Creating barrier net\nE0817 11:20:18.563855 15794 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0817 11:20:18.564237 15794 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.564251 15794 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nE0817 11:20:18.567664 29415 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0817 11:20:18.568056 29415 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.568071 29415 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0135490894318 secs\nINFO:memonger:Memonger memory optimization took 0.0135049819946 secs\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Creating barrier net\nINFO:data_parallel_model:Creating barrier net\nE0817 11:20:18.654834 16188 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0817 11:20:18.655194 16188 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.655208 16188 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nE0817 11:20:18.663950 14509 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0817 11:20:18.664294 14509 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0817 11:20:18.664320 14509 operator.cc:498] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0132520198822 secs\nINFO:memonger:Memonger memory optimization took 0.0132851600647 secs\nE0817 11:20:51.136189 15794 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in <module>\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nE0817 11:20:51.183814 29415 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in <module>\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nE0817 11:20:51.454591 14509 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in <module>\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\nE0817 11:20:51.455612 16188 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nTraceback (most recent call last):\n  File \"resnet50_trainer.py\", line 608, in <module>\n    main()\n  File \"resnet50_trainer.py\", line 604, in main\n    Train(args)\n  File \"resnet50_trainer.py\", line 444, in Train\n    workspace.RunNetOnce(train_model.param_init_net)\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\n    StringifyProto(net),\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\n    return func(*args, **kwargs)\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\nsrun: error: nid02966: task 3: Exited with exit code 1\nsrun: Terminating job step 9072007.0\nsrun: error: nid02963: task 0: Exited with exit code 1\nsrun: error: nid02965: task 2: Exited with exit code 1\nsrun: error: nid02964: task 1: Exited with exit code 1", "body": "Another two points that I think are worth mentioning are that I have followed all the changes mentioned in a [diff](https://github.com/caffe2/caffe2/issues/1050) by @pietern, even the changes for the Gloo header files. But it doesn't help with the situation.\r\n\r\nAnd I have changed the code for resnet_trainer.py from:\r\n\r\n```\r\nnum_shards = args.num_shards\r\nshard_id = args.shard_id\r\n\r\ninterfaces = args.distributed_interfaces.split(\",\")\r\n\r\n# Rendezvous using MPI when run with mpirun\r\nif os.getenv(\"OMPI_COMM_WORLD_SIZE\") is not None:\r\n    num_shards = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\", 1))\r\n    shard_id = int(os.getenv(\"OMPI_COMM_WORLD_RANK\", 0))\r\n```\r\n\r\nto:\r\n\r\n```\r\nnum_shards = args.num_shards\r\nshard_id = args.shard_id\r\n\r\ninterfaces = args.distributed_interfaces.split(\",\")\r\n\r\n# Rendezvous using MPI when run with mpirun\r\n#if os.getenv(\"OMPI_COMM_WORLD_SIZE\") is not None:\r\nif True:\r\n    #num_shards = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\", 1))\r\n    #shard_id = int(os.getenv(\"OMPI_COMM_WORLD_RANK\", 0))\r\n    shard_id = int(os.getenv(\"SLURM_PROCID\", 0))\r\n```\r\n\r\nbefore the above change the error was:\r\n\r\n```\r\nE0817 11:20:18.277760 29415 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.277838 29415 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.277843 29415 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0]\r\nE0817 11:20:18.278333 15794 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.278412 15794 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.278416 15794 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0]\r\nINFO:resnet50_trainer:Using epoch size: 1000\r\nINFO:resnet50_trainer:Using epoch size: 1000\r\nINFO:data_parallel_model:Parallelizing model for devices: [0]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Parallelizing model for devices: [0]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Adding gradient operators\r\nE0817 11:20:18.365449 16188 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.365527 16188 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.365532 16188 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0]\r\nINFO:resnet50_trainer:Using epoch size: 1000\r\nE0817 11:20:18.377269 14509 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.377346 14509 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0817 11:20:18.377351 14509 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0]\r\nINFO:resnet50_trainer:Using epoch size: 1000\r\nINFO:data_parallel_model:Parallelizing model for devices: [0]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Parallelizing model for devices: [0]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Creating barrier net\r\nINFO:data_parallel_model:Creating barrier net\r\nE0817 11:20:18.563855 15794 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\r\nE0817 11:20:18.564237 15794 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0817 11:20:18.564251 15794 operator.cc:498] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nE0817 11:20:18.567664 29415 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\r\nE0817 11:20:18.568056 29415 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0817 11:20:18.568071 29415 operator.cc:498] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0135490894318 secs\r\nINFO:memonger:Memonger memory optimization took 0.0135049819946 secs\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Creating barrier net\r\nINFO:data_parallel_model:Creating barrier net\r\nE0817 11:20:18.654834 16188 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\r\nE0817 11:20:18.655194 16188 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0817 11:20:18.655208 16188 operator.cc:498] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nE0817 11:20:18.663950 14509 operator.cc:496] Shape inference error: [enforce fail at conv_pool_op_base.h:626] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\r\nE0817 11:20:18.664294 14509 operator.cc:497] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0817 11:20:18.664320 14509 operator.cc:498] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0132520198822 secs\r\nINFO:memonger:Memonger memory optimization took 0.0132851600647 secs\r\nE0817 11:20:51.136189 15794 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 608, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 604, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 444, in Train\r\n    workspace.RunNetOnce(train_model.param_init_net)\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\r\n    StringifyProto(net),\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\nE0817 11:20:51.183814 29415 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 608, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 604, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 444, in Train\r\n    workspace.RunNetOnce(train_model.param_init_net)\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\r\n    StringifyProto(net),\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\nE0817 11:20:51.454591 14509 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 608, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 604, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 444, in Train\r\nE0817 11:20:51.455612 16188 common_world_ops.h:110] Caught store handler timeout exception: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\n    workspace.RunNetOnce(train_model.param_init_net)\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\r\n    StringifyProto(net),\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\nWARNING:caffe2.python.workspace:Original python traceback for operator `268` in network `resnet50_init` in exception above (most recent call last):\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 608, in <module>\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 604, in main\r\nWARNING:caffe2.python.workspace:  File \"resnet50_trainer.py\", line 439, in Train\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 296, in Parallelize\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1215, in _AllReduceBlobs\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1362, in _AllReduceBlobsDistributed\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1346, in allreduce\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1296, in get_control_and_context\r\nWARNING:caffe2.python.workspace:  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1814, in _CreateOrCloneCommonWorld\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 608, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 604, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 444, in Train\r\n    workspace.RunNetOnce(train_model.param_init_net)\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 201, in RunNetOnce\r\n    StringifyProto(net),\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 180, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [/pytorch/caffe2/distributed/file_store_handler.cc:154] Wait timeout for name(s): allreduce_0_cw_op/1\r\nsrun: error: nid02966: task 3: Exited with exit code 1\r\nsrun: Terminating job step 9072007.0\r\nsrun: error: nid02963: task 0: Exited with exit code 1\r\nsrun: error: nid02965: task 2: Exited with exit code 1\r\nsrun: error: nid02964: task 1: Exited with exit code 1\r\n```"}