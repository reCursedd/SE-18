{"url": "https://api.github.com/repos/pytorch/pytorch/issues/847", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/847/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/847/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/847/events", "html_url": "https://github.com/pytorch/pytorch/issues/847", "id": 210208283, "node_id": "MDU6SXNzdWUyMTAyMDgyODM=", "number": 847, "title": "the loss is not decreasing", "user": {"login": "elaheh61", "id": 9373076, "node_id": "MDQ6VXNlcjkzNzMwNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9373076?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elaheh61", "html_url": "https://github.com/elaheh61", "followers_url": "https://api.github.com/users/elaheh61/followers", "following_url": "https://api.github.com/users/elaheh61/following{/other_user}", "gists_url": "https://api.github.com/users/elaheh61/gists{/gist_id}", "starred_url": "https://api.github.com/users/elaheh61/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elaheh61/subscriptions", "organizations_url": "https://api.github.com/users/elaheh61/orgs", "repos_url": "https://api.github.com/users/elaheh61/repos", "events_url": "https://api.github.com/users/elaheh61/events{/privacy}", "received_events_url": "https://api.github.com/users/elaheh61/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-02-25T03:56:14Z", "updated_at": "2017-02-25T10:00:12Z", "closed_at": "2017-02-25T10:00:12Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nI have created a simple model consisting of two 1-layer nn competing each other. So, I have my own loss function based on those nn outputs.  It is very similar to GAN. The problem is that for a very simple test sample case, the loss function is not decreasing. For now I am using non-stochastic optimizer to eliminate randomness.  Here is the pseudo code with explanation</p>\n<p>n1_model = Net1(Dimension_in_n1, Dimension_out) # 1-layer nn with sigmoid<br>\nn2_model =Net2(Dimension_in_n2, Dimension_out) # 1-layer nn with sigmoid</p>\n<p>n1_optimizer = torch.optim.LBFGS(n1_model.parameters(), lr=0.01,max_iter = 50)<br>\nn2_optimizer = torch.optim.LBFGS(n2_model.parameters(), lr=0.01, max_iter = 50)</p>\n<p>for t in range(iter):<br>\nx_n1 = Variable(torch.from_numpy(...)) #load input of nn1 in batch size<br>\nx_n2 = Variable(torch.from_numpy(...)) #load input of nn2 in batch size</p>\n<pre><code>def closure():\n   reset_grad(n1_model.parameters())\n   reset_grad(n2.parameters())\n    y_n1 = n1_model(x_n1)\n    y_n2 = n2_model(x_n2)\n    n1_params =getParams(n1_model)\n    n2_param =getParams(n2_model)\n    loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer\n    loss.backward(retain_variables=True)\n    return loss\n\nn1_optimizer.step(closure)\n\n\ndef clos():\n   reset_grad(n1_model.parameters())\n   reset_grad(n2_model.parameters())\n    y_n1 = n1_model(x_n1)\n    y_n2 = n2_model(x_n2)\n    n1_params =getParams(n1_model)\n    n2_param =getParams(n2_model)\n    loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer\n    loss.backward()\n    return loss\n\nn2_optimizer.step(clos)\n</code></pre>\n<p>and here is the definition of my loss function:</p>\n<p>def my_loss_function(n1_output, n2_output, n1_parm, n2_param):<br>\nsm = torch.pow(n1_output - n2_output, 2)<br>\nreg = torch.norm(n1_parm,2) + torch.norm(n2_param,2)<br>\ny = torch.sum(sm) + 1 * reg<br>\nreturn y</p>\n<p>when I plot loss function, it has oscillation; I expect it to decrease during training.</p>", "body_text": "Hi,\nI have created a simple model consisting of two 1-layer nn competing each other. So, I have my own loss function based on those nn outputs.  It is very similar to GAN. The problem is that for a very simple test sample case, the loss function is not decreasing. For now I am using non-stochastic optimizer to eliminate randomness.  Here is the pseudo code with explanation\nn1_model = Net1(Dimension_in_n1, Dimension_out) # 1-layer nn with sigmoid\nn2_model =Net2(Dimension_in_n2, Dimension_out) # 1-layer nn with sigmoid\nn1_optimizer = torch.optim.LBFGS(n1_model.parameters(), lr=0.01,max_iter = 50)\nn2_optimizer = torch.optim.LBFGS(n2_model.parameters(), lr=0.01, max_iter = 50)\nfor t in range(iter):\nx_n1 = Variable(torch.from_numpy(...)) #load input of nn1 in batch size\nx_n2 = Variable(torch.from_numpy(...)) #load input of nn2 in batch size\ndef closure():\n   reset_grad(n1_model.parameters())\n   reset_grad(n2.parameters())\n    y_n1 = n1_model(x_n1)\n    y_n2 = n2_model(x_n2)\n    n1_params =getParams(n1_model)\n    n2_param =getParams(n2_model)\n    loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer\n    loss.backward(retain_variables=True)\n    return loss\n\nn1_optimizer.step(closure)\n\n\ndef clos():\n   reset_grad(n1_model.parameters())\n   reset_grad(n2_model.parameters())\n    y_n1 = n1_model(x_n1)\n    y_n2 = n2_model(x_n2)\n    n1_params =getParams(n1_model)\n    n2_param =getParams(n2_model)\n    loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer\n    loss.backward()\n    return loss\n\nn2_optimizer.step(clos)\n\nand here is the definition of my loss function:\ndef my_loss_function(n1_output, n2_output, n1_parm, n2_param):\nsm = torch.pow(n1_output - n2_output, 2)\nreg = torch.norm(n1_parm,2) + torch.norm(n2_param,2)\ny = torch.sum(sm) + 1 * reg\nreturn y\nwhen I plot loss function, it has oscillation; I expect it to decrease during training.", "body": "Hi,\r\nI have created a simple model consisting of two 1-layer nn competing each other. So, I have my own loss function based on those nn outputs.  It is very similar to GAN. The problem is that for a very simple test sample case, the loss function is not decreasing. For now I am using non-stochastic optimizer to eliminate randomness.  Here is the pseudo code with explanation\r\n\r\nn1_model = Net1(Dimension_in_n1, Dimension_out) # 1-layer nn with sigmoid\r\nn2_model =Net2(Dimension_in_n2, Dimension_out) # 1-layer nn with sigmoid\r\n\r\nn1_optimizer = torch.optim.LBFGS(n1_model.parameters(), lr=0.01,max_iter = 50)\r\nn2_optimizer = torch.optim.LBFGS(n2_model.parameters(), lr=0.01, max_iter = 50)\r\n\r\nfor t in range(iter):\r\n    x_n1 = Variable(torch.from_numpy(...)) #load input of nn1 in batch size\r\n    x_n2 = Variable(torch.from_numpy(...)) #load input of nn2 in batch size\r\n\r\n    def closure():\r\n       reset_grad(n1_model.parameters())\r\n       reset_grad(n2.parameters())\r\n        y_n1 = n1_model(x_n1)\r\n        y_n2 = n2_model(x_n2)\r\n        n1_params =getParams(n1_model)\r\n        n2_param =getParams(n2_model)\r\n        loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer\r\n        loss.backward(retain_variables=True)\r\n        return loss\r\n\r\n    n1_optimizer.step(closure)\r\n\r\n\r\n    def clos():\r\n       reset_grad(n1_model.parameters())\r\n       reset_grad(n2_model.parameters())\r\n        y_n1 = n1_model(x_n1)\r\n        y_n2 = n2_model(x_n2)\r\n        n1_params =getParams(n1_model)\r\n        n2_param =getParams(n2_model)\r\n        loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer\r\n        loss.backward()\r\n        return loss\r\n\r\n    n2_optimizer.step(clos)\r\n\r\n\r\nand here is the definition of my loss function:\r\n\r\ndef my_loss_function(n1_output, n2_output, n1_parm, n2_param):\r\n    sm = torch.pow(n1_output - n2_output, 2)\r\n    reg = torch.norm(n1_parm,2) + torch.norm(n2_param,2)\r\n    y = torch.sum(sm) + 1 * reg\r\n    return y\r\n\r\nwhen I plot loss function, it has oscillation; I expect it to decrease during training.\r\n"}