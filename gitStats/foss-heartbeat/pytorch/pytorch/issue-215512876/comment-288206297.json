{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/288206297", "html_url": "https://github.com/pytorch/pytorch/issues/1051#issuecomment-288206297", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1051", "id": 288206297, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODIwNjI5Nw==", "user": {"login": "thatguymike", "id": 3503919, "node_id": "MDQ6VXNlcjM1MDM5MTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3503919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thatguymike", "html_url": "https://github.com/thatguymike", "followers_url": "https://api.github.com/users/thatguymike/followers", "following_url": "https://api.github.com/users/thatguymike/following{/other_user}", "gists_url": "https://api.github.com/users/thatguymike/gists{/gist_id}", "starred_url": "https://api.github.com/users/thatguymike/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thatguymike/subscriptions", "organizations_url": "https://api.github.com/users/thatguymike/orgs", "repos_url": "https://api.github.com/users/thatguymike/repos", "events_url": "https://api.github.com/users/thatguymike/events{/privacy}", "received_events_url": "https://api.github.com/users/thatguymike/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-21T20:22:23Z", "updated_at": "2017-03-21T20:22:23Z", "author_association": "NONE", "body_html": "<p>This does seem bad.  For large networks like resnet50 where you have small batches, each GPU has a poor estimate of the norm, especially at the beginning of training.  We have found that allowing each GPU to run it's own batch norm can be more stable and better, calculating a total batch norm across all GPUs.  Final convergence is likely not impacted much since the final norm will work itself out after a few 10s of epochs.</p>\n<p>But I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> that this looks like a bug in general for anything with state even if we may argue about batch norm specifically.</p>", "body_text": "This does seem bad.  For large networks like resnet50 where you have small batches, each GPU has a poor estimate of the norm, especially at the beginning of training.  We have found that allowing each GPU to run it's own batch norm can be more stable and better, calculating a total batch norm across all GPUs.  Final convergence is likely not impacted much since the final norm will work itself out after a few 10s of epochs.\nBut I agree with @ngimel that this looks like a bug in general for anything with state even if we may argue about batch norm specifically.", "body": "This does seem bad.  For large networks like resnet50 where you have small batches, each GPU has a poor estimate of the norm, especially at the beginning of training.  We have found that allowing each GPU to run it's own batch norm can be more stable and better, calculating a total batch norm across all GPUs.  Final convergence is likely not impacted much since the final norm will work itself out after a few 10s of epochs.  \r\n\r\nBut I agree with @ngimel that this looks like a bug in general for anything with state even if we may argue about batch norm specifically."}