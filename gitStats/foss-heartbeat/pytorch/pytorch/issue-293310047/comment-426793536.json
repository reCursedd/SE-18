{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/426793536", "html_url": "https://github.com/pytorch/pytorch/issues/4969#issuecomment-426793536", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4969", "id": 426793536, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjc5MzUzNg==", "user": {"login": "brtknr", "id": 2181426, "node_id": "MDQ6VXNlcjIxODE0MjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/2181426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brtknr", "html_url": "https://github.com/brtknr", "followers_url": "https://api.github.com/users/brtknr/followers", "following_url": "https://api.github.com/users/brtknr/following{/other_user}", "gists_url": "https://api.github.com/users/brtknr/gists{/gist_id}", "starred_url": "https://api.github.com/users/brtknr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brtknr/subscriptions", "organizations_url": "https://api.github.com/users/brtknr/orgs", "repos_url": "https://api.github.com/users/brtknr/repos", "events_url": "https://api.github.com/users/brtknr/events{/privacy}", "received_events_url": "https://api.github.com/users/brtknr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-03T20:39:16Z", "updated_at": "2018-10-03T20:39:16Z", "author_association": "NONE", "body_html": "<p>Sorry for grave digging but I am having somewhat similar issue, trying to run ./examples/dogs_cats.ipynb from fastai/fastai repo (version 1) with pytorch built from source on macOS. I have tried reducing the image size from 228 to 60 pixels and the problem seems goes away.... Also goes away when I set num_workers=0.</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-4-1f26e5fd1026&gt; in &lt;module&gt;()\n      1 learn = ConvLearner(data, tvm.resnet34, metrics=accuracy)\n----&gt; 2 learn.fit_one_cycle(1)\n\n~/Envs/ML/lib/python3.6/site-packages/fastai/train.py in fit_one_cycle(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, **kwargs)\n     16     cbs = [OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n     17                              pct_start=pct_start, **kwargs)]\n---&gt; 18     learn.fit(cyc_len, max_lr, wd=wd, callbacks=cbs)\n     19 \n     20 def lr_find(learn:Learner, start_lr:float=1e-5, end_lr:float=10, num_it:int=100, **kwargs:Any):\n\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(self, epochs, lr, wd, callbacks)\n    131         callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)\n    132         fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n--&gt; 133             callbacks=self.callbacks+callbacks)\n    134 \n    135     def create_opt(self, lr:Floats, wd:Floats=0.)-&gt;None:\n\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(epochs, model, loss_fn, opt, data, callbacks, metrics)\n     84     except Exception as e:\n     85         exception = e\n---&gt; 86         raise e\n     87     finally: cb_handler.on_train_end(exception)\n     88 \n\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(epochs, model, loss_fn, opt, data, callbacks, metrics)\n     68             cb_handler.on_epoch_begin()\n     69 \n---&gt; 70             for xb,yb in progress_bar(data.train_dl, parent=pbar):\n     71                 xb, yb = cb_handler.on_batch_begin(xb, yb)\n     72                 loss,_ = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n\n~/Envs/ML/lib/python3.6/site-packages/fastprogress/fastprogress.py in __iter__(self)\n     59         self.update(0)\n     60         try:\n---&gt; 61             for i,o in enumerate(self._gen):\n     62                 yield o\n     63                 if self.auto_update: self.update(i+1)\n\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    345         while True:\n    346             assert (not self.shutdown and self.batches_outstanding &gt; 0)\n--&gt; 347             idx, batch = self._get_batch()\n    348             self.batches_outstanding -= 1\n    349             if idx != self.rcvd_idx:\n\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\n    324                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n    325         else:\n--&gt; 326             return self.data_queue.get()\n    327 \n    328     def __next__(self):\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py in get(self, block, timeout)\n     92         if block and timeout is None:\n     93             with self._rlock:\n---&gt; 94                 res = self._recv_bytes()\n     95             self._sem.release()\n     96         else:\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in recv_bytes(self, maxlength)\n    214         if maxlength is not None and maxlength &lt; 0:\n    215             raise ValueError(\"negative maxlength\")\n--&gt; 216         buf = self._recv_bytes(maxlength)\n    217         if buf is None:\n    218             self._bad_message_length()\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in _recv_bytes(self, maxsize)\n    405 \n    406     def _recv_bytes(self, maxsize=None):\n--&gt; 407         buf = self._recv(4)\n    408         size, = struct.unpack(\"!i\", buf.getvalue())\n    409         if maxsize is not None and size &gt; maxsize:\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in _recv(self, size, read)\n    377         remaining = size\n    378         while remaining &gt; 0:\n--&gt; 379             chunk = read(handle, remaining)\n    380             n = len(chunk)\n    381             if n == 0:\n\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in handler(signum, frame)\n    236         # This following call uses `waitid` with WNOHANG from C side. Therefore,\n    237         # Python can still get and update the process status successfully.\n--&gt; 238         _error_if_any_worker_fails()\n    239         if previous_handler is not None:\n    240             previous_handler(signum, frame)\n\nRuntimeError: DataLoader worker (pid 62155) is killed by signal: Unknown signal: 0. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n</code></pre>", "body_text": "Sorry for grave digging but I am having somewhat similar issue, trying to run ./examples/dogs_cats.ipynb from fastai/fastai repo (version 1) with pytorch built from source on macOS. I have tried reducing the image size from 228 to 60 pixels and the problem seems goes away.... Also goes away when I set num_workers=0.\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-4-1f26e5fd1026> in <module>()\n      1 learn = ConvLearner(data, tvm.resnet34, metrics=accuracy)\n----> 2 learn.fit_one_cycle(1)\n\n~/Envs/ML/lib/python3.6/site-packages/fastai/train.py in fit_one_cycle(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, **kwargs)\n     16     cbs = [OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n     17                              pct_start=pct_start, **kwargs)]\n---> 18     learn.fit(cyc_len, max_lr, wd=wd, callbacks=cbs)\n     19 \n     20 def lr_find(learn:Learner, start_lr:float=1e-5, end_lr:float=10, num_it:int=100, **kwargs:Any):\n\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(self, epochs, lr, wd, callbacks)\n    131         callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)\n    132         fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n--> 133             callbacks=self.callbacks+callbacks)\n    134 \n    135     def create_opt(self, lr:Floats, wd:Floats=0.)->None:\n\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(epochs, model, loss_fn, opt, data, callbacks, metrics)\n     84     except Exception as e:\n     85         exception = e\n---> 86         raise e\n     87     finally: cb_handler.on_train_end(exception)\n     88 \n\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(epochs, model, loss_fn, opt, data, callbacks, metrics)\n     68             cb_handler.on_epoch_begin()\n     69 \n---> 70             for xb,yb in progress_bar(data.train_dl, parent=pbar):\n     71                 xb, yb = cb_handler.on_batch_begin(xb, yb)\n     72                 loss,_ = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n\n~/Envs/ML/lib/python3.6/site-packages/fastprogress/fastprogress.py in __iter__(self)\n     59         self.update(0)\n     60         try:\n---> 61             for i,o in enumerate(self._gen):\n     62                 yield o\n     63                 if self.auto_update: self.update(i+1)\n\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    345         while True:\n    346             assert (not self.shutdown and self.batches_outstanding > 0)\n--> 347             idx, batch = self._get_batch()\n    348             self.batches_outstanding -= 1\n    349             if idx != self.rcvd_idx:\n\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\n    324                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n    325         else:\n--> 326             return self.data_queue.get()\n    327 \n    328     def __next__(self):\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py in get(self, block, timeout)\n     92         if block and timeout is None:\n     93             with self._rlock:\n---> 94                 res = self._recv_bytes()\n     95             self._sem.release()\n     96         else:\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in recv_bytes(self, maxlength)\n    214         if maxlength is not None and maxlength < 0:\n    215             raise ValueError(\"negative maxlength\")\n--> 216         buf = self._recv_bytes(maxlength)\n    217         if buf is None:\n    218             self._bad_message_length()\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in _recv_bytes(self, maxsize)\n    405 \n    406     def _recv_bytes(self, maxsize=None):\n--> 407         buf = self._recv(4)\n    408         size, = struct.unpack(\"!i\", buf.getvalue())\n    409         if maxsize is not None and size > maxsize:\n\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in _recv(self, size, read)\n    377         remaining = size\n    378         while remaining > 0:\n--> 379             chunk = read(handle, remaining)\n    380             n = len(chunk)\n    381             if n == 0:\n\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in handler(signum, frame)\n    236         # This following call uses `waitid` with WNOHANG from C side. Therefore,\n    237         # Python can still get and update the process status successfully.\n--> 238         _error_if_any_worker_fails()\n    239         if previous_handler is not None:\n    240             previous_handler(signum, frame)\n\nRuntimeError: DataLoader worker (pid 62155) is killed by signal: Unknown signal: 0. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.", "body": "Sorry for grave digging but I am having somewhat similar issue, trying to run ./examples/dogs_cats.ipynb from fastai/fastai repo (version 1) with pytorch built from source on macOS. I have tried reducing the image size from 228 to 60 pixels and the problem seems goes away.... Also goes away when I set num_workers=0.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-1f26e5fd1026> in <module>()\r\n      1 learn = ConvLearner(data, tvm.resnet34, metrics=accuracy)\r\n----> 2 learn.fit_one_cycle(1)\r\n\r\n~/Envs/ML/lib/python3.6/site-packages/fastai/train.py in fit_one_cycle(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, **kwargs)\r\n     16     cbs = [OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\r\n     17                              pct_start=pct_start, **kwargs)]\r\n---> 18     learn.fit(cyc_len, max_lr, wd=wd, callbacks=cbs)\r\n     19 \r\n     20 def lr_find(learn:Learner, start_lr:float=1e-5, end_lr:float=10, num_it:int=100, **kwargs:Any):\r\n\r\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(self, epochs, lr, wd, callbacks)\r\n    131         callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)\r\n    132         fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\r\n--> 133             callbacks=self.callbacks+callbacks)\r\n    134 \r\n    135     def create_opt(self, lr:Floats, wd:Floats=0.)->None:\r\n\r\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(epochs, model, loss_fn, opt, data, callbacks, metrics)\r\n     84     except Exception as e:\r\n     85         exception = e\r\n---> 86         raise e\r\n     87     finally: cb_handler.on_train_end(exception)\r\n     88 \r\n\r\n~/Envs/ML/lib/python3.6/site-packages/fastai/basic_train.py in fit(epochs, model, loss_fn, opt, data, callbacks, metrics)\r\n     68             cb_handler.on_epoch_begin()\r\n     69 \r\n---> 70             for xb,yb in progress_bar(data.train_dl, parent=pbar):\r\n     71                 xb, yb = cb_handler.on_batch_begin(xb, yb)\r\n     72                 loss,_ = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\r\n\r\n~/Envs/ML/lib/python3.6/site-packages/fastprogress/fastprogress.py in __iter__(self)\r\n     59         self.update(0)\r\n     60         try:\r\n---> 61             for i,o in enumerate(self._gen):\r\n     62                 yield o\r\n     63                 if self.auto_update: self.update(i+1)\r\n\r\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    345         while True:\r\n    346             assert (not self.shutdown and self.batches_outstanding > 0)\r\n--> 347             idx, batch = self._get_batch()\r\n    348             self.batches_outstanding -= 1\r\n    349             if idx != self.rcvd_idx:\r\n\r\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\r\n    324                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\r\n    325         else:\r\n--> 326             return self.data_queue.get()\r\n    327 \r\n    328     def __next__(self):\r\n\r\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py in get(self, block, timeout)\r\n     92         if block and timeout is None:\r\n     93             with self._rlock:\r\n---> 94                 res = self._recv_bytes()\r\n     95             self._sem.release()\r\n     96         else:\r\n\r\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in recv_bytes(self, maxlength)\r\n    214         if maxlength is not None and maxlength < 0:\r\n    215             raise ValueError(\"negative maxlength\")\r\n--> 216         buf = self._recv_bytes(maxlength)\r\n    217         if buf is None:\r\n    218             self._bad_message_length()\r\n\r\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in _recv_bytes(self, maxsize)\r\n    405 \r\n    406     def _recv_bytes(self, maxsize=None):\r\n--> 407         buf = self._recv(4)\r\n    408         size, = struct.unpack(\"!i\", buf.getvalue())\r\n    409         if maxsize is not None and size > maxsize:\r\n\r\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py in _recv(self, size, read)\r\n    377         remaining = size\r\n    378         while remaining > 0:\r\n--> 379             chunk = read(handle, remaining)\r\n    380             n = len(chunk)\r\n    381             if n == 0:\r\n\r\n~/Envs/ML/lib/python3.6/site-packages/torch/utils/data/dataloader.py in handler(signum, frame)\r\n    236         # This following call uses `waitid` with WNOHANG from C side. Therefore,\r\n    237         # Python can still get and update the process status successfully.\r\n--> 238         _error_if_any_worker_fails()\r\n    239         if previous_handler is not None:\r\n    240             previous_handler(signum, frame)\r\n\r\nRuntimeError: DataLoader worker (pid 62155) is killed by signal: Unknown signal: 0. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\r\n```"}