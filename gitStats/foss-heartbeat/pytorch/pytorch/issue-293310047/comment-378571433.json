{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/378571433", "html_url": "https://github.com/pytorch/pytorch/issues/4969#issuecomment-378571433", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4969", "id": 378571433, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODU3MTQzMw==", "user": {"login": "naifrec", "id": 14828647, "node_id": "MDQ6VXNlcjE0ODI4NjQ3", "avatar_url": "https://avatars2.githubusercontent.com/u/14828647?v=4", "gravatar_id": "", "url": "https://api.github.com/users/naifrec", "html_url": "https://github.com/naifrec", "followers_url": "https://api.github.com/users/naifrec/followers", "following_url": "https://api.github.com/users/naifrec/following{/other_user}", "gists_url": "https://api.github.com/users/naifrec/gists{/gist_id}", "starred_url": "https://api.github.com/users/naifrec/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/naifrec/subscriptions", "organizations_url": "https://api.github.com/users/naifrec/orgs", "repos_url": "https://api.github.com/users/naifrec/repos", "events_url": "https://api.github.com/users/naifrec/events{/privacy}", "received_events_url": "https://api.github.com/users/naifrec/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-04T11:43:20Z", "updated_at": "2018-04-05T12:09:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hello all,</p>\n<h2><code>segfault</code> issue</h2>\n<p>I seem to be facing the same issue where my data loading fails with a segmentation fault when <code>num_workers &gt; 0</code>, otherwise the script runs fine (see traceback below). The underlying dataset is a HDF5 dataset (using <code>h5py</code>), and there are some transforms on top which mainly use <code>numpy</code>.</p>\n<div class=\"highlight highlight-text-python-traceback\"><pre><span class=\"pl-en\">ERROR</span>: <span class=\"pl-s\">Unexpected segmentation fault encountered in worker.</span>\nTraceback (most recent call last):\n  File <span class=\"pl-s\">\"experiments/script.py\"</span>, line <span class=\"pl-c1\">197</span>, in <span class=\"pl-en\">&lt;module&gt;</span>\n    exp.configure(clargs)\n  File <span class=\"pl-s\">\"experiments/script.py\"</span>, line <span class=\"pl-c1\">110</span>, in <span class=\"pl-en\">configure</span>\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> train_stream:\n  File <span class=\"pl-s\">\"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\"</span>, line <span class=\"pl-c1\">275</span>, in <span class=\"pl-en\">__next__</span>\n    idx, batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._get_batch()\n  File <span class=\"pl-s\">\"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\"</span>, line <span class=\"pl-c1\">254</span>, in <span class=\"pl-en\">_get_batch</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.data_queue.get()\n  File <span class=\"pl-s\">\"/usr/lib/python3.6/multiprocessing/queues.py\"</span>, line <span class=\"pl-c1\">335</span>, in <span class=\"pl-en\">get</span>\n    res <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._reader.recv_bytes()\n  File <span class=\"pl-s\">\"/usr/lib/python3.6/multiprocessing/connection.py\"</span>, line <span class=\"pl-c1\">216</span>, in <span class=\"pl-en\">recv_bytes</span>\n    buf <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._recv_bytes(maxlength)\n  File <span class=\"pl-s\">\"/usr/lib/python3.6/multiprocessing/connection.py\"</span>, line <span class=\"pl-c1\">407</span>, in <span class=\"pl-en\">_recv_bytes</span>\n    buf <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._recv(<span class=\"pl-c1\">4</span>)\n  File <span class=\"pl-s\">\"/usr/lib/python3.6/multiprocessing/connection.py\"</span>, line <span class=\"pl-c1\">379</span>, in <span class=\"pl-en\">_recv</span>\n    chunk <span class=\"pl-k\">=</span> read(handle, remaining)\n  File <span class=\"pl-s\">\"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\"</span>, line <span class=\"pl-c1\">175</span>, in <span class=\"pl-en\">handler</span>\n    _error_if_any_worker_fails()\n<span class=\"pl-en\">RuntimeError</span>: <span class=\"pl-s\">DataLoader worker (pid 33818) is killed by signal: Segmentation fault</span></pre></div>\n<h2>Configuration</h2>\n<ul>\n<li>ubuntu 14.04</li>\n<li>python 3.6.5 (in a virtualenv)</li>\n<li>CUDA 8.0</li>\n<li>GPU nVidia GTX 1080Ti</li>\n<li>how you installed PyTorch: <code>pip install torch==0.3.1</code></li>\n<li>torch 0.3.1</li>\n<li>torchvision 0.2.0</li>\n<li>h5py 2.7.1</li>\n<li>numpy 1.14.0</li>\n</ul>\n<p>Interestingly, with the same setup on another machine, where only difference is ubuntu 1<strong>6</strong>.04 and python 3.6.<strong>3</strong>, the script runs with <code>num_workers &gt; 0</code>. So <strong>it clearly is setup related</strong>.</p>\n<h2>Related</h2>\n<p>This <a href=\"https://discuss.pytorch.org/t/data-loader-crashes-during-training-something-to-do-with-multiprocessing-in-docker/4379\" rel=\"nofollow\">thread</a> seems to report a related issue, though not segmentation fault. A <a href=\"https://discuss.pytorch.org/t/data-loader-crashes-during-training-something-to-do-with-multiprocessing-in-docker/4379/16\" rel=\"nofollow\">reported solution</a> is to increase shared memory size.</p>\n<h2>Edit on 18/04/05</h2>\n<p>I tried multiple things since yesterday, one of them was to use <code>pyenv</code> and use different minor versions of <code>python3.6</code>. When using <code>python3.6.3</code> (and keeping all of the above config the same) I got a new error which has already been reported in <a href=\"https://github.com/pytorch/pytorch/issues/2314\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2314/hovercard\">this post</a>.</p>\n<div class=\"highlight highlight-text-python-traceback\"><pre>*** Error in `python': munmap_chunk(): invalid pointer: 0x00007ffa654e95e4 ***\nTraceback (most recent call last):\n  File <span class=\"pl-s\">\"experiments/script.py\"</span>, line <span class=\"pl-c1\">197</span>, in <span class=\"pl-en\">&lt;module&gt;</span>\n    exp.configure(clargs)\n  File <span class=\"pl-s\">\"experiments/script.py\"</span>, line <span class=\"pl-c1\">110</span>, in <span class=\"pl-en\">configure</span>\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> train_stream:\n  File <span class=\"pl-s\">\"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\"</span>, line <span class=\"pl-c1\">275</span>, in <span class=\"pl-en\">__next__</span>\n    idx, batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._get_batch()\n  File <span class=\"pl-s\">\"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\"</span>, line <span class=\"pl-c1\">254</span>, in <span class=\"pl-en\">_get_batch</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.data_queue.get()\n  File <span class=\"pl-s\">\"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/queues.py\"</span>, line <span class=\"pl-c1\">335</span>, in <span class=\"pl-en\">get</span>\n    res <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._reader.recv_bytes()\n  File <span class=\"pl-s\">\"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\"</span>, line <span class=\"pl-c1\">216</span>, in <span class=\"pl-en\">recv_bytes</span>\n    buf <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._recv_bytes(maxlength)\n  File <span class=\"pl-s\">\"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\"</span>, line <span class=\"pl-c1\">407</span>, in <span class=\"pl-en\">_recv_bytes</span>\n    buf <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._recv(<span class=\"pl-c1\">4</span>)\n  File <span class=\"pl-s\">\"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\"</span>, line <span class=\"pl-c1\">379</span>, in <span class=\"pl-en\">_recv</span>\n    chunk <span class=\"pl-k\">=</span> read(handle, remaining)\n  File <span class=\"pl-s\">\"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\"</span>, line <span class=\"pl-c1\">175</span>, in <span class=\"pl-en\">handler</span>\n    _error_if_any_worker_fails()\n<span class=\"pl-en\">RuntimeError</span>: <span class=\"pl-s\">DataLoader worker (pid 915) is killed by signal: Aborted.</span></pre></div>\n<p>The proposed fix is:</p>\n<div class=\"highlight highlight-source-shell\"><pre>sudo apt-get install libtcmalloc-minimal4\n<span class=\"pl-k\">export</span> LD_PRELOAD=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/libtcmalloc_minimal.so.4<span class=\"pl-pds\">\"</span></span></pre></div>\n<p>In my case, it does allow me to progress further in my script, but I still get a new error:</p>\n<div class=\"highlight highlight-source-shell\"><pre>src/tcmalloc.cc:277] Attempt to free invalid pointer 0x12 \nAborted (core dumped)</pre></div>\n<p>This is also a known issue as reported (very recently) <a href=\"https://github.com/pytorch/pytorch/issues/5961\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5961/hovercard\">here</a>, but no solution was provided and I cannot find any.</p>", "body_text": "Hello all,\nsegfault issue\nI seem to be facing the same issue where my data loading fails with a segmentation fault when num_workers > 0, otherwise the script runs fine (see traceback below). The underlying dataset is a HDF5 dataset (using h5py), and there are some transforms on top which mainly use numpy.\nERROR: Unexpected segmentation fault encountered in worker.\nTraceback (most recent call last):\n  File \"experiments/script.py\", line 197, in <module>\n    exp.configure(clargs)\n  File \"experiments/script.py\", line 110, in configure\n    for batch in train_stream:\n  File \"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\n    idx, batch = self._get_batch()\n  File \"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\n    return self.data_queue.get()\n  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n    res = self._reader.recv_bytes()\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n    _error_if_any_worker_fails()\nRuntimeError: DataLoader worker (pid 33818) is killed by signal: Segmentation fault\nConfiguration\n\nubuntu 14.04\npython 3.6.5 (in a virtualenv)\nCUDA 8.0\nGPU nVidia GTX 1080Ti\nhow you installed PyTorch: pip install torch==0.3.1\ntorch 0.3.1\ntorchvision 0.2.0\nh5py 2.7.1\nnumpy 1.14.0\n\nInterestingly, with the same setup on another machine, where only difference is ubuntu 16.04 and python 3.6.3, the script runs with num_workers > 0. So it clearly is setup related.\nRelated\nThis thread seems to report a related issue, though not segmentation fault. A reported solution is to increase shared memory size.\nEdit on 18/04/05\nI tried multiple things since yesterday, one of them was to use pyenv and use different minor versions of python3.6. When using python3.6.3 (and keeping all of the above config the same) I got a new error which has already been reported in this post.\n*** Error in `python': munmap_chunk(): invalid pointer: 0x00007ffa654e95e4 ***\nTraceback (most recent call last):\n  File \"experiments/script.py\", line 197, in <module>\n    exp.configure(clargs)\n  File \"experiments/script.py\", line 110, in configure\n    for batch in train_stream:\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\n    idx, batch = self._get_batch()\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\n    return self.data_queue.get()\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n    res = self._reader.recv_bytes()\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n    _error_if_any_worker_fails()\nRuntimeError: DataLoader worker (pid 915) is killed by signal: Aborted.\nThe proposed fix is:\nsudo apt-get install libtcmalloc-minimal4\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\nIn my case, it does allow me to progress further in my script, but I still get a new error:\nsrc/tcmalloc.cc:277] Attempt to free invalid pointer 0x12 \nAborted (core dumped)\nThis is also a known issue as reported (very recently) here, but no solution was provided and I cannot find any.", "body": "Hello all,\r\n\r\n## `segfault` issue\r\n\r\nI seem to be facing the same issue where my data loading fails with a segmentation fault when `num_workers > 0`, otherwise the script runs fine (see traceback below). The underlying dataset is a HDF5 dataset (using `h5py`), and there are some transforms on top which mainly use `numpy`.\r\n\r\n```python-traceback\r\nERROR: Unexpected segmentation fault encountered in worker.\r\nTraceback (most recent call last):\r\n  File \"experiments/script.py\", line 197, in <module>\r\n    exp.configure(clargs)\r\n  File \"experiments/script.py\", line 110, in configure\r\n    for batch in train_stream:\r\n  File \"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/usr2/guillaume/code/hermes/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 33818) is killed by signal: Segmentation fault\r\n```\r\n\r\n## Configuration\r\n\r\n- ubuntu 14.04\r\n- python 3.6.5 (in a virtualenv)\r\n- CUDA 8.0\r\n- GPU nVidia GTX 1080Ti\r\n- how you installed PyTorch: `pip install torch==0.3.1`\r\n- torch 0.3.1\r\n- torchvision 0.2.0\r\n- h5py 2.7.1\r\n- numpy 1.14.0\r\n\r\nInterestingly, with the same setup on another machine, where only difference is ubuntu 1**6**.04 and python 3.6.**3**, the script runs with `num_workers > 0`. So **it clearly is setup related**.\r\n\r\n## Related\r\n\r\nThis [thread](https://discuss.pytorch.org/t/data-loader-crashes-during-training-something-to-do-with-multiprocessing-in-docker/4379) seems to report a related issue, though not segmentation fault. A [reported solution](https://discuss.pytorch.org/t/data-loader-crashes-during-training-something-to-do-with-multiprocessing-in-docker/4379/16) is to increase shared memory size.\r\n\r\n## Edit on 18/04/05\r\n\r\nI tried multiple things since yesterday, one of them was to use `pyenv` and use different minor versions of `python3.6`. When using `python3.6.3` (and keeping all of the above config the same) I got a new error which has already been reported in [this post](https://github.com/pytorch/pytorch/issues/2314).\r\n\r\n```python-traceback\r\n*** Error in `python': munmap_chunk(): invalid pointer: 0x00007ffa654e95e4 ***\r\nTraceback (most recent call last):\r\n  File \"experiments/script.py\", line 197, in <module>\r\n    exp.configure(clargs)\r\n  File \"experiments/script.py\", line 110, in configure\r\n    for batch in train_stream:\r\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\r\n    idx, batch = self._get_batch()\r\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\r\n    return self.data_queue.get()\r\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\n  File \"/usr2/guillaume/.pyenv/versions/3.6.3/envs/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 915) is killed by signal: Aborted.\r\n```\r\n\r\nThe proposed fix is:\r\n\r\n```bash\r\nsudo apt-get install libtcmalloc-minimal4\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\r\n```\r\n\r\nIn my case, it does allow me to progress further in my script, but I still get a new error:\r\n\r\n```bash\r\nsrc/tcmalloc.cc:277] Attempt to free invalid pointer 0x12 \r\nAborted (core dumped)\r\n```\r\n\r\nThis is also a known issue as reported (very recently) [here](https://github.com/pytorch/pytorch/issues/5961), but no solution was provided and I cannot find any."}