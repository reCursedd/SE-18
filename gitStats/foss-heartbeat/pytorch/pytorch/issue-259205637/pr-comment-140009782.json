{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140009782", "pull_request_review_id": 64022158, "id": 140009782, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDAwOTc4Mg==", "diff_hunk": "@@ -193,5 +193,39 @@ struct multiply_s\n   }\n };\n \n+template <typename Dtype, typename Acctype>\n+struct renorm_functor\n+{\n+  thrust::device_ptr<Dtype> weight_ptr;\n+  int64_t stride;\n+  Dtype maxNorm;\n+  Dtype normType;\n+  thrust::plus<Acctype> binary_plus;\n+  pow_v<Dtype, Acctype> unary_pow;\n+\n+  renorm_functor(\n+      thrust::device_ptr<Dtype> w, int64_t s, Dtype m, Dtype n, \n+      thrust::plus<Acctype> bp, pow_v<Dtype, Acctype> up\n+  ) : weight_ptr(w), stride(s), maxNorm(m), normType(n), \n+      binary_plus(bp), unary_pow(up) {}\n+\n+  #pragma hd_warning_disable\n+  __host__ __device__\n+  void operator()(const THCIndex_t idx)\n+  {\n+    THCIndex_t k  = idx - TH_INDEX_BASE;\n+    thrust::device_ptr<Dtype> row_ptr = weight_ptr + k * stride;\n+    Acctype norm = thrust::transform_reduce(thrust::device, row_ptr, row_ptr + stride,", "path": "torch/lib/THCUNN/LookupTable.cu", "position": null, "original_position": 38, "commit_id": "c13c9ffa8c1229a94e9a8824ef3e0750bc428d45", "original_commit_id": "fecec2cd1eaaadcdd534097979d80d3e9fdebb5b", "user": {"login": "wickedfoo", "id": 1911637, "node_id": "MDQ6VXNlcjE5MTE2Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1911637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wickedfoo", "html_url": "https://github.com/wickedfoo", "followers_url": "https://api.github.com/users/wickedfoo/followers", "following_url": "https://api.github.com/users/wickedfoo/following{/other_user}", "gists_url": "https://api.github.com/users/wickedfoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/wickedfoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wickedfoo/subscriptions", "organizations_url": "https://api.github.com/users/wickedfoo/orgs", "repos_url": "https://api.github.com/users/wickedfoo/repos", "events_url": "https://api.github.com/users/wickedfoo/events{/privacy}", "received_events_url": "https://api.github.com/users/wickedfoo/received_events", "type": "User", "site_admin": false}, "body": "Unfortunately, I don't think this is the right solution.\r\n\r\nIn this case, each individual CUDA thread would be performing a single reduction on an entire vector, which means that all of the memory loads by that thread will be non-coalesced since the reads are strided across the warp by a rather large stride, so the kernel would be performing up to 1/32x slower than it needs to perform. \r\n\r\nThe proper solution to parallelize the outer loop below unfortunately would be to write a custom kernel here that calculates the norm on a per-block basis, then goes back and transforms the data. The outer dimension (numel) is handled over the block index. Norm calculation would be a block-wise reduction. All threads in the block would load from the vector in a coalesced manner, and would then go back and update the data.\r\n\r\nThe current code has a rather large kernel launch overhead, but does have the coalesced accesses. You have eliminated the kernel launch overhead but have made the memory accesses worse. Just as the old version would perform rather well for a small batch size but massive vectors, your version would perform poorly if the vectors were really huge.\r\n\r\nTo \"correctly\" solve a problem like this might require several different specializations (large batch / small dim, small dim / large batch, both large), but giving each vector to a single block is a good middle ground, and should be ok for pretty much anything.\r\n\r\n", "created_at": "2017-09-20T15:43:48Z", "updated_at": "2018-11-23T15:34:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/2803#discussion_r140009782", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2803", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140009782"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2803#discussion_r140009782"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2803"}}, "body_html": "<p>Unfortunately, I don't think this is the right solution.</p>\n<p>In this case, each individual CUDA thread would be performing a single reduction on an entire vector, which means that all of the memory loads by that thread will be non-coalesced since the reads are strided across the warp by a rather large stride, so the kernel would be performing up to 1/32x slower than it needs to perform.</p>\n<p>The proper solution to parallelize the outer loop below unfortunately would be to write a custom kernel here that calculates the norm on a per-block basis, then goes back and transforms the data. The outer dimension (numel) is handled over the block index. Norm calculation would be a block-wise reduction. All threads in the block would load from the vector in a coalesced manner, and would then go back and update the data.</p>\n<p>The current code has a rather large kernel launch overhead, but does have the coalesced accesses. You have eliminated the kernel launch overhead but have made the memory accesses worse. Just as the old version would perform rather well for a small batch size but massive vectors, your version would perform poorly if the vectors were really huge.</p>\n<p>To \"correctly\" solve a problem like this might require several different specializations (large batch / small dim, small dim / large batch, both large), but giving each vector to a single block is a good middle ground, and should be ok for pretty much anything.</p>", "body_text": "Unfortunately, I don't think this is the right solution.\nIn this case, each individual CUDA thread would be performing a single reduction on an entire vector, which means that all of the memory loads by that thread will be non-coalesced since the reads are strided across the warp by a rather large stride, so the kernel would be performing up to 1/32x slower than it needs to perform.\nThe proper solution to parallelize the outer loop below unfortunately would be to write a custom kernel here that calculates the norm on a per-block basis, then goes back and transforms the data. The outer dimension (numel) is handled over the block index. Norm calculation would be a block-wise reduction. All threads in the block would load from the vector in a coalesced manner, and would then go back and update the data.\nThe current code has a rather large kernel launch overhead, but does have the coalesced accesses. You have eliminated the kernel launch overhead but have made the memory accesses worse. Just as the old version would perform rather well for a small batch size but massive vectors, your version would perform poorly if the vectors were really huge.\nTo \"correctly\" solve a problem like this might require several different specializations (large batch / small dim, small dim / large batch, both large), but giving each vector to a single block is a good middle ground, and should be ok for pretty much anything."}