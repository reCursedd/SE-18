{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140124162", "pull_request_review_id": 64151271, "id": 140124162, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDEyNDE2Mg==", "diff_hunk": "@@ -162,36 +162,76 @@ __global__ void cunn_LookupTable_accGradParametersKernel(\n   }\n }\n \n-/*\n- * Keep the norm of weight smaller than maxNorm\n- */\n template <typename Dtype, typename Acctype>\n-struct pow_v\n-{\n-  Acctype normType;\n-  pow_v(Dtype v) : normType(ScalarConvert<Dtype, Acctype>::to(v)) {}\n-  __host__ __device__\n-  Acctype operator()(const Dtype& x) const {\n+__device__\n+Acctype fast_pow(Dtype x, Acctype normType) {\n     Acctype xA = ScalarConvert<Dtype, Acctype>::to(x);\n     if (normType == 1)", "path": "torch/lib/THCUNN/LookupTable.cu", "position": null, "original_position": 29, "commit_id": "c13c9ffa8c1229a94e9a8824ef3e0750bc428d45", "original_commit_id": "6490ce94a82690b1e4642f082ddc270a705fcba8", "user": {"login": "wickedfoo", "id": 1911637, "node_id": "MDQ6VXNlcjE5MTE2Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1911637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wickedfoo", "html_url": "https://github.com/wickedfoo", "followers_url": "https://api.github.com/users/wickedfoo/followers", "following_url": "https://api.github.com/users/wickedfoo/following{/other_user}", "gists_url": "https://api.github.com/users/wickedfoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/wickedfoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wickedfoo/subscriptions", "organizations_url": "https://api.github.com/users/wickedfoo/orgs", "repos_url": "https://api.github.com/users/wickedfoo/repos", "events_url": "https://api.github.com/users/wickedfoo/events{/privacy}", "received_events_url": "https://api.github.com/users/wickedfoo/received_events", "type": "User", "site_admin": false}, "body": "It's likely going to do all three branches anyways with per-lane predication, unless the compiler can figure out that the parameter is uniform across the warp (which it is, because it is an argument to the kernel). The way to do this sort of thing is put this in a struct with `int Norm` as one of the template parameters, and provide different partial specialization of the struct based on `Norm` being 1, 2, or -1 (for all other powers).", "created_at": "2017-09-21T00:20:04Z", "updated_at": "2018-11-23T15:34:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/2803#discussion_r140124162", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2803", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140124162"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2803#discussion_r140124162"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2803"}}, "body_html": "<p>It's likely going to do all three branches anyways with per-lane predication, unless the compiler can figure out that the parameter is uniform across the warp (which it is, because it is an argument to the kernel). The way to do this sort of thing is put this in a struct with <code>int Norm</code> as one of the template parameters, and provide different partial specialization of the struct based on <code>Norm</code> being 1, 2, or -1 (for all other powers).</p>", "body_text": "It's likely going to do all three branches anyways with per-lane predication, unless the compiler can figure out that the parameter is uniform across the warp (which it is, because it is an argument to the kernel). The way to do this sort of thing is put this in a struct with int Norm as one of the template parameters, and provide different partial specialization of the struct based on Norm being 1, 2, or -1 (for all other powers)."}