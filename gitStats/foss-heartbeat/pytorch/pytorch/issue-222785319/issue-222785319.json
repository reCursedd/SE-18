{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1299", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1299/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1299/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1299/events", "html_url": "https://github.com/pytorch/pytorch/issues/1299", "id": 222785319, "node_id": "MDU6SXNzdWUyMjI3ODUzMTk=", "number": 1299, "title": "Constructors of CUDA tensors are slow when given iterables", "user": {"login": "John-Jumper", "id": 4306211, "node_id": "MDQ6VXNlcjQzMDYyMTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/4306211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/John-Jumper", "html_url": "https://github.com/John-Jumper", "followers_url": "https://api.github.com/users/John-Jumper/followers", "following_url": "https://api.github.com/users/John-Jumper/following{/other_user}", "gists_url": "https://api.github.com/users/John-Jumper/gists{/gist_id}", "starred_url": "https://api.github.com/users/John-Jumper/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/John-Jumper/subscriptions", "organizations_url": "https://api.github.com/users/John-Jumper/orgs", "repos_url": "https://api.github.com/users/John-Jumper/repos", "events_url": "https://api.github.com/users/John-Jumper/events{/privacy}", "received_events_url": "https://api.github.com/users/John-Jumper/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 586698444, "node_id": "MDU6TGFiZWw1ODY2OTg0NDQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/1hr", "name": "1hr", "color": "d4c5f9", "default": false}, {"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-04-19T15:53:23Z", "updated_at": "2018-04-16T17:46:05Z", "closed_at": "2018-04-16T17:46:05Z", "author_association": "NONE", "body_html": "<p>The creation time for creating a GPU FloatTensor with a numpy array is much slower than creating a CPU FloatTensor then transferring with <code>.cuda()</code>; it also depends very strongly on the number of dimensions.  I know now that I should be using <code>torch.from_numpy(x).cuda()</code>, but the performance cliff going from CPU to GPU FloatTensor's was very surprising.</p>\n<p>The following program demonstrates the issue:</p>\n<pre><code>import numpy as np\nimport torch\nimport torch.cuda as cu\nimport contextlib\nimport time\n\n# Timing helper with CUDA synchonization\n@contextlib.contextmanager\ndef timing(name):\n    cu.synchronize()\n    start_time = time.time()\n    yield\n    cu.synchronize()\n    end_time = time.time()\n    print '{} {:6.3f} seconds'.format(name, end_time-start_time)\n\nfor shape in [(128**3,), (128,128**2), (128,128,128), (32,32,32,64)]:\n    print 'shape {}, {:.1f} MB'.format(shape, np.zeros(shape).nbytes/1024.**2)\n\n    with timing('from_numpy sent to GPU     '): torch.from_numpy (np.zeros(shape)).cuda()\n    with timing('CPU constructor            '): torch.FloatTensor(np.zeros(shape))\n    with timing('CPU constructor sent to GPU'): torch.FloatTensor(np.zeros(shape)).cuda()\n    with timing('GPU constructor            '): cu.   FloatTensor(np.zeros(shape))\n    print\n</code></pre>\n<p>I get the following output:</p>\n<pre><code>shape (2097152,), 16.0 MB\nfrom_numpy sent to GPU       0.003 seconds\nCPU constructor              0.065 seconds\nCPU constructor sent to GPU  0.066 seconds\nGPU constructor              0.065 seconds\n\nshape (128, 16384), 16.0 MB\nfrom_numpy sent to GPU       0.004 seconds\nCPU constructor              0.066 seconds\nCPU constructor sent to GPU  0.068 seconds\nGPU constructor              0.183 seconds\n\nshape (128, 128, 128), 16.0 MB\nfrom_numpy sent to GPU       0.004 seconds\nCPU constructor              0.069 seconds\nCPU constructor sent to GPU  0.070 seconds\nGPU constructor             16.038 seconds\n\nshape (32, 32, 32, 64), 16.0 MB\nfrom_numpy sent to GPU       0.004 seconds\nCPU constructor              0.072 seconds\nCPU constructor sent to GPU  0.073 seconds\nGPU constructor             32.051 seconds\n</code></pre>\n<p>Notice the large time required to call the GPU tensor <code>__init__</code> (fourth line) when using 3 or 4 dimensions.  All of the arrays have to same number of elements.  Independently, I was unable to initialize a GPU FloatTensor using an numpy array of dtype <code>float32</code> due to a type error.  This seems like an odd limitation, and the CPU FloatTensor has no such limitation.</p>\n<p>I am using PyTorch 0.1.11+9150e33 compiled from source on Ubuntu 16.04.2 with CUDA 8.0 and Python 2.7.12.  I have a Intel Core i5-4670 and NVIDIA GTX 1080 Ti.</p>", "body_text": "The creation time for creating a GPU FloatTensor with a numpy array is much slower than creating a CPU FloatTensor then transferring with .cuda(); it also depends very strongly on the number of dimensions.  I know now that I should be using torch.from_numpy(x).cuda(), but the performance cliff going from CPU to GPU FloatTensor's was very surprising.\nThe following program demonstrates the issue:\nimport numpy as np\nimport torch\nimport torch.cuda as cu\nimport contextlib\nimport time\n\n# Timing helper with CUDA synchonization\n@contextlib.contextmanager\ndef timing(name):\n    cu.synchronize()\n    start_time = time.time()\n    yield\n    cu.synchronize()\n    end_time = time.time()\n    print '{} {:6.3f} seconds'.format(name, end_time-start_time)\n\nfor shape in [(128**3,), (128,128**2), (128,128,128), (32,32,32,64)]:\n    print 'shape {}, {:.1f} MB'.format(shape, np.zeros(shape).nbytes/1024.**2)\n\n    with timing('from_numpy sent to GPU     '): torch.from_numpy (np.zeros(shape)).cuda()\n    with timing('CPU constructor            '): torch.FloatTensor(np.zeros(shape))\n    with timing('CPU constructor sent to GPU'): torch.FloatTensor(np.zeros(shape)).cuda()\n    with timing('GPU constructor            '): cu.   FloatTensor(np.zeros(shape))\n    print\n\nI get the following output:\nshape (2097152,), 16.0 MB\nfrom_numpy sent to GPU       0.003 seconds\nCPU constructor              0.065 seconds\nCPU constructor sent to GPU  0.066 seconds\nGPU constructor              0.065 seconds\n\nshape (128, 16384), 16.0 MB\nfrom_numpy sent to GPU       0.004 seconds\nCPU constructor              0.066 seconds\nCPU constructor sent to GPU  0.068 seconds\nGPU constructor              0.183 seconds\n\nshape (128, 128, 128), 16.0 MB\nfrom_numpy sent to GPU       0.004 seconds\nCPU constructor              0.069 seconds\nCPU constructor sent to GPU  0.070 seconds\nGPU constructor             16.038 seconds\n\nshape (32, 32, 32, 64), 16.0 MB\nfrom_numpy sent to GPU       0.004 seconds\nCPU constructor              0.072 seconds\nCPU constructor sent to GPU  0.073 seconds\nGPU constructor             32.051 seconds\n\nNotice the large time required to call the GPU tensor __init__ (fourth line) when using 3 or 4 dimensions.  All of the arrays have to same number of elements.  Independently, I was unable to initialize a GPU FloatTensor using an numpy array of dtype float32 due to a type error.  This seems like an odd limitation, and the CPU FloatTensor has no such limitation.\nI am using PyTorch 0.1.11+9150e33 compiled from source on Ubuntu 16.04.2 with CUDA 8.0 and Python 2.7.12.  I have a Intel Core i5-4670 and NVIDIA GTX 1080 Ti.", "body": "The creation time for creating a GPU FloatTensor with a numpy array is much slower than creating a CPU FloatTensor then transferring with `.cuda()`; it also depends very strongly on the number of dimensions.  I know now that I should be using `torch.from_numpy(x).cuda()`, but the performance cliff going from CPU to GPU FloatTensor's was very surprising.\r\n\r\nThe following program demonstrates the issue:\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport torch.cuda as cu\r\nimport contextlib\r\nimport time\r\n\r\n# Timing helper with CUDA synchonization\r\n@contextlib.contextmanager\r\ndef timing(name):\r\n    cu.synchronize()\r\n    start_time = time.time()\r\n    yield\r\n    cu.synchronize()\r\n    end_time = time.time()\r\n    print '{} {:6.3f} seconds'.format(name, end_time-start_time)\r\n\r\nfor shape in [(128**3,), (128,128**2), (128,128,128), (32,32,32,64)]:\r\n    print 'shape {}, {:.1f} MB'.format(shape, np.zeros(shape).nbytes/1024.**2)\r\n\r\n    with timing('from_numpy sent to GPU     '): torch.from_numpy (np.zeros(shape)).cuda()\r\n    with timing('CPU constructor            '): torch.FloatTensor(np.zeros(shape))\r\n    with timing('CPU constructor sent to GPU'): torch.FloatTensor(np.zeros(shape)).cuda()\r\n    with timing('GPU constructor            '): cu.   FloatTensor(np.zeros(shape))\r\n    print\r\n```\r\n\r\nI get the following output:\r\n```\r\nshape (2097152,), 16.0 MB\r\nfrom_numpy sent to GPU       0.003 seconds\r\nCPU constructor              0.065 seconds\r\nCPU constructor sent to GPU  0.066 seconds\r\nGPU constructor              0.065 seconds\r\n\r\nshape (128, 16384), 16.0 MB\r\nfrom_numpy sent to GPU       0.004 seconds\r\nCPU constructor              0.066 seconds\r\nCPU constructor sent to GPU  0.068 seconds\r\nGPU constructor              0.183 seconds\r\n\r\nshape (128, 128, 128), 16.0 MB\r\nfrom_numpy sent to GPU       0.004 seconds\r\nCPU constructor              0.069 seconds\r\nCPU constructor sent to GPU  0.070 seconds\r\nGPU constructor             16.038 seconds\r\n\r\nshape (32, 32, 32, 64), 16.0 MB\r\nfrom_numpy sent to GPU       0.004 seconds\r\nCPU constructor              0.072 seconds\r\nCPU constructor sent to GPU  0.073 seconds\r\nGPU constructor             32.051 seconds\r\n```\r\n\r\nNotice the large time required to call the GPU tensor `__init__` (fourth line) when using 3 or 4 dimensions.  All of the arrays have to same number of elements.  Independently, I was unable to initialize a GPU FloatTensor using an numpy array of dtype `float32` due to a type error.  This seems like an odd limitation, and the CPU FloatTensor has no such limitation.\r\n\r\nI am using PyTorch 0.1.11+9150e33 compiled from source on Ubuntu 16.04.2 with CUDA 8.0 and Python 2.7.12.  I have a Intel Core i5-4670 and NVIDIA GTX 1080 Ti."}