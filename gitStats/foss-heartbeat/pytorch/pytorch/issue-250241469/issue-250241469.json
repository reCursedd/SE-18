{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2419", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2419/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2419/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2419/events", "html_url": "https://github.com/pytorch/pytorch/issues/2419", "id": 250241469, "node_id": "MDU6SXNzdWUyNTAyNDE0Njk=", "number": 2419, "title": "RuntimeError: CUDNN_STATUS_NOT_SUPPORTED during backward()", "user": {"login": "rinuboney", "id": 1678100, "node_id": "MDQ6VXNlcjE2NzgxMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1678100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rinuboney", "html_url": "https://github.com/rinuboney", "followers_url": "https://api.github.com/users/rinuboney/followers", "following_url": "https://api.github.com/users/rinuboney/following{/other_user}", "gists_url": "https://api.github.com/users/rinuboney/gists{/gist_id}", "starred_url": "https://api.github.com/users/rinuboney/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rinuboney/subscriptions", "organizations_url": "https://api.github.com/users/rinuboney/orgs", "repos_url": "https://api.github.com/users/rinuboney/repos", "events_url": "https://api.github.com/users/rinuboney/events{/privacy}", "received_events_url": "https://api.github.com/users/rinuboney/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-15T07:33:01Z", "updated_at": "2017-10-30T16:25:21Z", "closed_at": "2017-10-30T16:25:21Z", "author_association": "NONE", "body_html": "<p>I ran into a <a href=\"https://discuss.pytorch.org/t/unclear-cudnn-runtimeerror-during-backward/6202\" rel=\"nofollow\">problem</a> when trying to implement some meta-learning algorithms. To summarize what I'm trying to do:</p>\n<ol>\n<li>Compute the gradients of a model parametrized by <code>Theta</code> based on the loss from some training samples.</li>\n<li>Compute <code>Theta_ = Theta - lr*Theta_grad</code></li>\n<li>Compute loss of the model when parametrized by <code>Theta_</code> on some test samples.</li>\n<li>Compute the gradients all the way back to <code>Theta</code></li>\n</ol>\n<p>Here is a simple code to reproduce the problem:</p>\n<pre><code>import torch as th\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nlr = 0.01\ncuda = False\n\n# Parameters\nW1 = Variable(th.Tensor(3, 1, 2, 2), requires_grad=True)\nW2 = Variable(th.Tensor(2, 3), requires_grad=True)\n\n# Train data\nx_1 = Variable(th.FloatTensor([[0.1, 0.2], [0.3, 0.4]])).view(1, 1, 2, 2)\nx_2 = Variable(th.FloatTensor([[0.3, 0.6], [0.1, 0.4]])).view(1, 1, 2, 2)\n\n# Test data\ny_1 = Variable(th.LongTensor([1]))\ny_2 = Variable(th.LongTensor([0]))\n\nif cuda:\n    W1, W2 = W1.cuda(), W2.cuda()\n    x_1, x_2 = x_1.cuda(), x_2.cuda()\n    y_1, y_2 = y_1.cuda(), y_2.cuda()\n\n# Compute loss on train data using W1 and W2\nout = F.conv2d(x_1, W1).view(1, -1)\ntrain_output = F.linear(out, W2)\ntrain_loss = F.cross_entropy(train_output, y_1)\n\n# Compute gradients of W1 and W2\nW1_grad, W2_grad = th.autograd.grad(train_loss, [W1, W2], create_graph=True)\n\n# Compute W*_ based on W* and W*_grad\nW1_ = W1 - lr*W1_grad\nW2_ = W2 - lr*W2_grad\n\n# Compute loss on test data using W1 and W2\nout = F.conv2d(x_2, W1_).view(1, -1)\ntest_output = F.linear(out, W2_)\ntest_loss = F.cross_entropy(test_output, y_2)\n\n# Compute gradients of W1 and W2 based on the test loss\ntest_loss.backward()\n</code></pre>\n<p>This code works when I run it on CPU ie., <code>cuda=False</code>, but fails with the following error when I run it on my GPU ie., <code>cuda=True</code>:</p>\n<pre><code>Traceback (most recent call last):\n  File \"main_f.py\", line 260, in &lt;module&gt;\n    test_loss.backward()\n  File \"/home/lib/python3.5/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/home/lib/python3.5/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n</code></pre>", "body_text": "I ran into a problem when trying to implement some meta-learning algorithms. To summarize what I'm trying to do:\n\nCompute the gradients of a model parametrized by Theta based on the loss from some training samples.\nCompute Theta_ = Theta - lr*Theta_grad\nCompute loss of the model when parametrized by Theta_ on some test samples.\nCompute the gradients all the way back to Theta\n\nHere is a simple code to reproduce the problem:\nimport torch as th\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nlr = 0.01\ncuda = False\n\n# Parameters\nW1 = Variable(th.Tensor(3, 1, 2, 2), requires_grad=True)\nW2 = Variable(th.Tensor(2, 3), requires_grad=True)\n\n# Train data\nx_1 = Variable(th.FloatTensor([[0.1, 0.2], [0.3, 0.4]])).view(1, 1, 2, 2)\nx_2 = Variable(th.FloatTensor([[0.3, 0.6], [0.1, 0.4]])).view(1, 1, 2, 2)\n\n# Test data\ny_1 = Variable(th.LongTensor([1]))\ny_2 = Variable(th.LongTensor([0]))\n\nif cuda:\n    W1, W2 = W1.cuda(), W2.cuda()\n    x_1, x_2 = x_1.cuda(), x_2.cuda()\n    y_1, y_2 = y_1.cuda(), y_2.cuda()\n\n# Compute loss on train data using W1 and W2\nout = F.conv2d(x_1, W1).view(1, -1)\ntrain_output = F.linear(out, W2)\ntrain_loss = F.cross_entropy(train_output, y_1)\n\n# Compute gradients of W1 and W2\nW1_grad, W2_grad = th.autograd.grad(train_loss, [W1, W2], create_graph=True)\n\n# Compute W*_ based on W* and W*_grad\nW1_ = W1 - lr*W1_grad\nW2_ = W2 - lr*W2_grad\n\n# Compute loss on test data using W1 and W2\nout = F.conv2d(x_2, W1_).view(1, -1)\ntest_output = F.linear(out, W2_)\ntest_loss = F.cross_entropy(test_output, y_2)\n\n# Compute gradients of W1 and W2 based on the test loss\ntest_loss.backward()\n\nThis code works when I run it on CPU ie., cuda=False, but fails with the following error when I run it on my GPU ie., cuda=True:\nTraceback (most recent call last):\n  File \"main_f.py\", line 260, in <module>\n    test_loss.backward()\n  File \"/home/lib/python3.5/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/home/lib/python3.5/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.", "body": "I ran into a [problem](https://discuss.pytorch.org/t/unclear-cudnn-runtimeerror-during-backward/6202) when trying to implement some meta-learning algorithms. To summarize what I'm trying to do:\r\n\r\n1. Compute the gradients of a model parametrized by `Theta` based on the loss from some training samples.\r\n2.  Compute `Theta_ = Theta - lr*Theta_grad`\r\n3. Compute loss of the model when parametrized by `Theta_` on some test samples.\r\n4. Compute the gradients all the way back to `Theta`\r\n\r\nHere is a simple code to reproduce the problem:\r\n```\r\nimport torch as th\r\nfrom torch.autograd import Variable\r\nimport torch.nn.functional as F\r\n\r\nlr = 0.01\r\ncuda = False\r\n\r\n# Parameters\r\nW1 = Variable(th.Tensor(3, 1, 2, 2), requires_grad=True)\r\nW2 = Variable(th.Tensor(2, 3), requires_grad=True)\r\n\r\n# Train data\r\nx_1 = Variable(th.FloatTensor([[0.1, 0.2], [0.3, 0.4]])).view(1, 1, 2, 2)\r\nx_2 = Variable(th.FloatTensor([[0.3, 0.6], [0.1, 0.4]])).view(1, 1, 2, 2)\r\n\r\n# Test data\r\ny_1 = Variable(th.LongTensor([1]))\r\ny_2 = Variable(th.LongTensor([0]))\r\n\r\nif cuda:\r\n    W1, W2 = W1.cuda(), W2.cuda()\r\n    x_1, x_2 = x_1.cuda(), x_2.cuda()\r\n    y_1, y_2 = y_1.cuda(), y_2.cuda()\r\n\r\n# Compute loss on train data using W1 and W2\r\nout = F.conv2d(x_1, W1).view(1, -1)\r\ntrain_output = F.linear(out, W2)\r\ntrain_loss = F.cross_entropy(train_output, y_1)\r\n\r\n# Compute gradients of W1 and W2\r\nW1_grad, W2_grad = th.autograd.grad(train_loss, [W1, W2], create_graph=True)\r\n\r\n# Compute W*_ based on W* and W*_grad\r\nW1_ = W1 - lr*W1_grad\r\nW2_ = W2 - lr*W2_grad\r\n\r\n# Compute loss on test data using W1 and W2\r\nout = F.conv2d(x_2, W1_).view(1, -1)\r\ntest_output = F.linear(out, W2_)\r\ntest_loss = F.cross_entropy(test_output, y_2)\r\n\r\n# Compute gradients of W1 and W2 based on the test loss\r\ntest_loss.backward()\r\n```\r\n\r\nThis code works when I run it on CPU ie., `cuda=False`, but fails with the following error when I run it on my GPU ie., `cuda=True`:\r\n```\r\nTraceback (most recent call last):\r\n  File \"main_f.py\", line 260, in <module>\r\n    test_loss.backward()\r\n  File \"/home/lib/python3.5/site-packages/torch/autograd/variable.py\", line 156, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/home/lib/python3.5/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\r\n```"}