{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/248340728", "html_url": "https://github.com/pytorch/pytorch/pull/36#issuecomment-248340728", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/36", "id": 248340728, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODM0MDcyOA==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-20T15:40:31Z", "updated_at": "2016-09-20T15:40:31Z", "author_association": "MEMBER", "body_html": "<p>The function-backend abstraction adds an extra layer of indirection and code bloat without substantial benefits. The problem is that it <strong>forces the choice of implementation all the way up to the module</strong>. This is the wrong place: the function, not the module, has the right information to determine the choice of kernel: it knows the shape of the input and the all the parameters, and it can make decisions on the fly.</p>\n<p>The backend-indirection instead requires you to split all the functions along implementation lines. For example, if there's a single backend doesn't support all types of convolutions (e.g. atrous, striding, etc.), you have to split all convolution functions into atrous and non-atorus functions. Otherwise, you risk choosing a backend at the module level that doesn't support the current parameters. This leads to more code bloat.</p>\n<p>There's a similar problem with backend choice and serialization. Presumably, the choice of backend is serialized (otherwise you would lose the choice during checkpointing.) But, the default backend on one machine might not be the best (or even supported) on another. Choosing the implementation at the function-level avoids this problem because it's done at every forward call.</p>\n<hr>\n<p>We should focus on supporting two use cases:</p>\n<ol>\n<li>Core support for the fastest CPU and GPU kernels</li>\n<li>Support for experimenting with different function implementations</li>\n</ol>\n<p>For the core, we should choose whatever kernels are fastest and available on the current architecture (cuDNN, neon, nnpack, etc.). This means tight-coupling and possibly redistributing binaries. In general, there should be a <strong>one-to-one mapping</strong> between modules and functions with the functions determining the implementation. The choice should be made on the fly, in the <code>forward</code>, based on the input, current architecture, and user \"hints\". (The \"hints\" can be strings instead of booleans like <code>use_cudnn</code>)</p>\n<p>For experimenting with different kernels, users should override <code>forward</code> to substitute their own Function.</p>", "body_text": "The function-backend abstraction adds an extra layer of indirection and code bloat without substantial benefits. The problem is that it forces the choice of implementation all the way up to the module. This is the wrong place: the function, not the module, has the right information to determine the choice of kernel: it knows the shape of the input and the all the parameters, and it can make decisions on the fly.\nThe backend-indirection instead requires you to split all the functions along implementation lines. For example, if there's a single backend doesn't support all types of convolutions (e.g. atrous, striding, etc.), you have to split all convolution functions into atrous and non-atorus functions. Otherwise, you risk choosing a backend at the module level that doesn't support the current parameters. This leads to more code bloat.\nThere's a similar problem with backend choice and serialization. Presumably, the choice of backend is serialized (otherwise you would lose the choice during checkpointing.) But, the default backend on one machine might not be the best (or even supported) on another. Choosing the implementation at the function-level avoids this problem because it's done at every forward call.\n\nWe should focus on supporting two use cases:\n\nCore support for the fastest CPU and GPU kernels\nSupport for experimenting with different function implementations\n\nFor the core, we should choose whatever kernels are fastest and available on the current architecture (cuDNN, neon, nnpack, etc.). This means tight-coupling and possibly redistributing binaries. In general, there should be a one-to-one mapping between modules and functions with the functions determining the implementation. The choice should be made on the fly, in the forward, based on the input, current architecture, and user \"hints\". (The \"hints\" can be strings instead of booleans like use_cudnn)\nFor experimenting with different kernels, users should override forward to substitute their own Function.", "body": "The function-backend abstraction adds an extra layer of indirection and code bloat without substantial benefits. The problem is that it **forces the choice of implementation all the way up to the module**. This is the wrong place: the function, not the module, has the right information to determine the choice of kernel: it knows the shape of the input and the all the parameters, and it can make decisions on the fly.\n\nThe backend-indirection instead requires you to split all the functions along implementation lines. For example, if there's a single backend doesn't support all types of convolutions (e.g. atrous, striding, etc.), you have to split all convolution functions into atrous and non-atorus functions. Otherwise, you risk choosing a backend at the module level that doesn't support the current parameters. This leads to more code bloat.\n\nThere's a similar problem with backend choice and serialization. Presumably, the choice of backend is serialized (otherwise you would lose the choice during checkpointing.) But, the default backend on one machine might not be the best (or even supported) on another. Choosing the implementation at the function-level avoids this problem because it's done at every forward call.\n\n---\n\nWe should focus on supporting two use cases:\n1. Core support for the fastest CPU and GPU kernels\n2. Support for experimenting with different function implementations\n\nFor the core, we should choose whatever kernels are fastest and available on the current architecture (cuDNN, neon, nnpack, etc.). This means tight-coupling and possibly redistributing binaries. In general, there should be a **one-to-one mapping** between modules and functions with the functions determining the implementation. The choice should be made on the fly, in the `forward`, based on the input, current architecture, and user \"hints\". (The \"hints\" can be strings instead of booleans like `use_cudnn`)\n\nFor experimenting with different kernels, users should override `forward` to substitute their own Function. \n"}