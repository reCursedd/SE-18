{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3322", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3322/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3322/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3322/events", "html_url": "https://github.com/pytorch/pytorch/issues/3322", "id": 269105041, "node_id": "MDU6SXNzdWUyNjkxMDUwNDE=", "number": 3322, "title": "[docfix] Parameter size_average=False behaves differently for mse_loss and cross_entropy", "user": {"login": "tarvaina", "id": 2739, "node_id": "MDQ6VXNlcjI3Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2739?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tarvaina", "html_url": "https://github.com/tarvaina", "followers_url": "https://api.github.com/users/tarvaina/followers", "following_url": "https://api.github.com/users/tarvaina/following{/other_user}", "gists_url": "https://api.github.com/users/tarvaina/gists{/gist_id}", "starred_url": "https://api.github.com/users/tarvaina/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tarvaina/subscriptions", "organizations_url": "https://api.github.com/users/tarvaina/orgs", "repos_url": "https://api.github.com/users/tarvaina/repos", "events_url": "https://api.github.com/users/tarvaina/events{/privacy}", "received_events_url": "https://api.github.com/users/tarvaina/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-27T13:21:33Z", "updated_at": "2017-12-02T00:08:38Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Setting <code>size_average=False</code> behaves differently for <code>mse_loss</code> than for <code>cross_entropy</code>. For <code>cross_entropy</code>, it multiplies the result by the batch size. For <code>mse_loss</code> it multiplies the result by the overall size of the matrix.</p>\n<p>I believe the <code>cross_entropy</code> behavior is correct and <code>mse_loss</code> should be changed to behave similarly. Alternatively, the documentation should be changed to clarify the difference.</p>\n<p>Actually, <code>mse_loss</code> behaves similarly to <code>kl_div</code>, but at least the behavior is documented in <code>kl_div</code>. (But before reading the documentation, I expected also <code>kl_div</code> to behave similarly to <code>cross_entropy</code>.) I haven't checked how other loss functions behave.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.version\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> FloatTensor\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">v</span>(<span class=\"pl-smi\">array</span>):\n    <span class=\"pl-k\">return</span> Variable(FloatTensor(array))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">kl</span>(<span class=\"pl-smi\">logit</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-k\">return</span> F.kl_div(F.log_softmax(logit), target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span>size_average)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">xe</span>(<span class=\"pl-smi\">logit</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-k\">return</span> F.cross_entropy(logit, target.max(<span class=\"pl-c1\">1</span>)[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span>size_average)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">mse</span>(<span class=\"pl-smi\">logit</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-k\">return</span> F.mse_loss(F.softmax(logit), target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span>size_average)\n\ntarget <span class=\"pl-k\">=</span> v(np.eye(<span class=\"pl-c1\">5</span>)[:<span class=\"pl-c1\">3</span>])\nlogit <span class=\"pl-k\">=</span> v(np.zeros((<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>)))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Torch version:<span class=\"pl-pds\">\"</span></span>, torch.<span class=\"pl-c1\">__version__</span>)\n<span class=\"pl-c1\">print</span>()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>How each loss function scales the result with size_average:<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">for</span> loss <span class=\"pl-k\">in</span> [kl, mse, xe]:\n    with_size_average <span class=\"pl-k\">=</span> loss(logit, target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    without_size_average <span class=\"pl-k\">=</span> loss(logit, target, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    factor <span class=\"pl-k\">=</span> (without_size_average <span class=\"pl-k\">/</span> with_size_average).data[<span class=\"pl-c1\">0</span>]\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>- <span class=\"pl-c1\">{loss.__name__}</span>: <span class=\"pl-c1\">{factor}</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-k\">**</span><span class=\"pl-c1\">locals</span>()))<span class=\"pl-bu\">```</span></pre></div>\n<pre><code>Torch version: 0.2.0+40ca356\n\nHow each loss function scales the result with size_average:\n- kl: 15.0\n- mse: 15.0\n- xe: 3.0\n</code></pre>", "body_text": "Setting size_average=False behaves differently for mse_loss than for cross_entropy. For cross_entropy, it multiplies the result by the batch size. For mse_loss it multiplies the result by the overall size of the matrix.\nI believe the cross_entropy behavior is correct and mse_loss should be changed to behave similarly. Alternatively, the documentation should be changed to clarify the difference.\nActually, mse_loss behaves similarly to kl_div, but at least the behavior is documented in kl_div. (But before reading the documentation, I expected also kl_div to behave similarly to cross_entropy.) I haven't checked how other loss functions behave.\nimport torch.version\nfrom torch.nn import functional as F\nfrom torch import FloatTensor\nfrom torch.autograd import Variable\n\ndef v(array):\n    return Variable(FloatTensor(array))\n\ndef kl(logit, target, size_average=True):\n    return F.kl_div(F.log_softmax(logit), target, size_average=size_average)\n\ndef xe(logit, target, size_average=True):\n    return F.cross_entropy(logit, target.max(1)[1], size_average=size_average)\n\ndef mse(logit, target, size_average=True):\n    return F.mse_loss(F.softmax(logit), target, size_average=size_average)\n\ntarget = v(np.eye(5)[:3])\nlogit = v(np.zeros((3, 5)))\n\nprint(\"Torch version:\", torch.__version__)\nprint()\nprint(\"How each loss function scales the result with size_average:\")\nfor loss in [kl, mse, xe]:\n    with_size_average = loss(logit, target, size_average=True)\n    without_size_average = loss(logit, target, size_average=False)\n    factor = (without_size_average / with_size_average).data[0]\n    print(\"- {loss.__name__}: {factor}\".format(**locals()))```\nTorch version: 0.2.0+40ca356\n\nHow each loss function scales the result with size_average:\n- kl: 15.0\n- mse: 15.0\n- xe: 3.0", "body": "Setting `size_average=False` behaves differently for `mse_loss` than for `cross_entropy`. For `cross_entropy`, it multiplies the result by the batch size. For `mse_loss` it multiplies the result by the overall size of the matrix.\r\n\r\nI believe the `cross_entropy` behavior is correct and `mse_loss` should be changed to behave similarly. Alternatively, the documentation should be changed to clarify the difference.\r\n\r\nActually, `mse_loss` behaves similarly to `kl_div`, but at least the behavior is documented in `kl_div`. (But before reading the documentation, I expected also `kl_div` to behave similarly to `cross_entropy`.) I haven't checked how other loss functions behave. \r\n\r\n```python\r\nimport torch.version\r\nfrom torch.nn import functional as F\r\nfrom torch import FloatTensor\r\nfrom torch.autograd import Variable\r\n\r\ndef v(array):\r\n    return Variable(FloatTensor(array))\r\n\r\ndef kl(logit, target, size_average=True):\r\n    return F.kl_div(F.log_softmax(logit), target, size_average=size_average)\r\n\r\ndef xe(logit, target, size_average=True):\r\n    return F.cross_entropy(logit, target.max(1)[1], size_average=size_average)\r\n\r\ndef mse(logit, target, size_average=True):\r\n    return F.mse_loss(F.softmax(logit), target, size_average=size_average)\r\n\r\ntarget = v(np.eye(5)[:3])\r\nlogit = v(np.zeros((3, 5)))\r\n\r\nprint(\"Torch version:\", torch.__version__)\r\nprint()\r\nprint(\"How each loss function scales the result with size_average:\")\r\nfor loss in [kl, mse, xe]:\r\n    with_size_average = loss(logit, target, size_average=True)\r\n    without_size_average = loss(logit, target, size_average=False)\r\n    factor = (without_size_average / with_size_average).data[0]\r\n    print(\"- {loss.__name__}: {factor}\".format(**locals()))```\r\n```\r\n\r\n```\r\nTorch version: 0.2.0+40ca356\r\n\r\nHow each loss function scales the result with size_average:\r\n- kl: 15.0\r\n- mse: 15.0\r\n- xe: 3.0\r\n```"}