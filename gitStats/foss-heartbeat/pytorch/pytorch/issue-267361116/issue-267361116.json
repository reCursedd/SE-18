{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3219", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3219/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3219/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3219/events", "html_url": "https://github.com/pytorch/pytorch/issues/3219", "id": 267361116, "node_id": "MDU6SXNzdWUyNjczNjExMTY=", "number": 3219, "title": "segmentation fault when using Tensor.numpy() and Tensor.from_numpy() in backward()", "user": {"login": "jeasinema", "id": 10633528, "node_id": "MDQ6VXNlcjEwNjMzNTI4", "avatar_url": "https://avatars3.githubusercontent.com/u/10633528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeasinema", "html_url": "https://github.com/jeasinema", "followers_url": "https://api.github.com/users/jeasinema/followers", "following_url": "https://api.github.com/users/jeasinema/following{/other_user}", "gists_url": "https://api.github.com/users/jeasinema/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeasinema/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeasinema/subscriptions", "organizations_url": "https://api.github.com/users/jeasinema/orgs", "repos_url": "https://api.github.com/users/jeasinema/repos", "events_url": "https://api.github.com/users/jeasinema/events{/privacy}", "received_events_url": "https://api.github.com/users/jeasinema/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679955625, "node_id": "MDU6TGFiZWw2Nzk5NTU2MjU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/crash", "name": "crash", "color": "d93f0b", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-21T08:12:27Z", "updated_at": "2017-11-17T22:29:46Z", "closed_at": "2017-11-17T22:29:46Z", "author_association": "NONE", "body_html": "<p>The code below will cause a segmentation fault on some test platforms:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Foo</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> torch.from_numpy(x.numpy())\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):\n        a <span class=\"pl-k\">=</span> grad_output.numpy()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> a = a + 0.</span>\n        <span class=\"pl-k\">return</span> torch.from_numpy(a)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">16</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">16</span>)\n        <span class=\"pl-c1\">self</span>.foo <span class=\"pl-k\">=</span> Foo()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc1(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.foo(x)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> x = self.fc2(x)</span>\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>)\n        <span class=\"pl-k\">return</span> x<span class=\"pl-bu\">;</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">epoch</span>):\n    model <span class=\"pl-k\">=</span> Net()\n    model.train()\n    optimizer <span class=\"pl-k\">=</span> optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.99</span>)\n    data <span class=\"pl-k\">=</span> Variable(torch.from_numpy(np.ones((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32))) <span class=\"pl-c\"><span class=\"pl-c\">#</span><span class=\"pl-k\">TODO</span></span>\n    target <span class=\"pl-k\">=</span> Variable(torch.from_numpy(np.zeros((<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int64))) <span class=\"pl-c\"><span class=\"pl-c\">#</span><span class=\"pl-k\">TODO</span></span>\n\n    test_data <span class=\"pl-k\">=</span> torch.from_numpy(np.ones((<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)) <span class=\"pl-c\"><span class=\"pl-c\">#</span><span class=\"pl-k\">TODO</span></span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">func</span>(<span class=\"pl-smi\">a</span>):\n        b <span class=\"pl-k\">=</span> a.numpy()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> b = b + 0.</span>\n        b[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">return</span> torch.from_numpy(b)\n    <span class=\"pl-c1\">print</span>(test_data)\n    d <span class=\"pl-k\">=</span> func(test_data)\n    <span class=\"pl-c1\">print</span>(d)\n    <span class=\"pl-c1\">print</span>(test_data)\n    d[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n    <span class=\"pl-c1\">print</span>(d)\n    <span class=\"pl-c1\">print</span>(test_data)\n\n    optimizer.zero_grad()\n    output <span class=\"pl-k\">=</span> model(data)\n    loss <span class=\"pl-k\">=</span> F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>():\n    model <span class=\"pl-k\">=</span> torch.load(args.model_path)\n    model.eval()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    train(<span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    main()</pre></div>\n<p>Some useful information:</p>\n<ol>\n<li>Test platform 1:</li>\n</ol>\n<ul>\n<li><code>Ubuntu 16.04.3</code></li>\n<li><code>gcc 5.4.0</code></li>\n<li><code>Python 3.5.2</code></li>\n<li><code>PyTorch 0.2.0_3</code></li>\n</ul>\n<ol start=\"2\">\n<li>Test platform 2:</li>\n</ol>\n<ul>\n<li><code>macOS 10.12.6</code></li>\n<li><code>Apple LLVM version 9.0.0 (clang-900.0.38)</code></li>\n<li><code>Python 3.6.3</code></li>\n<li><code>PyTorch 0.2.0_2</code></li>\n</ul>\n<ol start=\"3\">\n<li>If the customized layer is placed the first layer in the model, this error will not happen since there is no gradient for it to back prop to the layer before.</li>\n<li>Compared to the operation in <code>def func(a)</code>, the code here will be executed in \"pure python\", however, the back prop code are connected by PyTorch's C backend. But I'm not familiar with this part.</li>\n<li>If I uncomment the <code>a = a + 0</code>, this error will disappear.</li>\n</ol>\n<p>Any idea on this problem?</p>", "body_text": "The code below will cause a segmentation fault on some test platforms:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nclass Foo(torch.autograd.Function):\n    def forward(ctx, x):\n        return torch.from_numpy(x.numpy())\n    def backward(ctx, grad_output):\n        a = grad_output.numpy()\n        # a = a + 0.\n        return torch.from_numpy(a)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(16, 16)\n        self.fc2 = nn.Linear(16, 16)\n        self.foo = Foo()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.foo(x)\n        # x = self.fc2(x)\n        x = x.view(1, 16)\n        return x;\n\ndef train(epoch):\n    model = Net()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.99)\n    data = Variable(torch.from_numpy(np.ones((1, 16), dtype=np.float32))) #TODO\n    target = Variable(torch.from_numpy(np.zeros((1), dtype=np.int64))) #TODO\n\n    test_data = torch.from_numpy(np.ones((1), dtype=np.float32)) #TODO\n    def func(a):\n        b = a.numpy()\n        # b = b + 0.\n        b[0] = 0\n        return torch.from_numpy(b)\n    print(test_data)\n    d = func(test_data)\n    print(d)\n    print(test_data)\n    d[0] = 2\n    print(d)\n    print(test_data)\n\n    optimizer.zero_grad()\n    output = model(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n\ndef test():\n    model = torch.load(args.model_path)\n    model.eval()\n\ndef main():\n    train(0)\n\nif __name__ == \"__main__\":\n    main()\nSome useful information:\n\nTest platform 1:\n\n\nUbuntu 16.04.3\ngcc 5.4.0\nPython 3.5.2\nPyTorch 0.2.0_3\n\n\nTest platform 2:\n\n\nmacOS 10.12.6\nApple LLVM version 9.0.0 (clang-900.0.38)\nPython 3.6.3\nPyTorch 0.2.0_2\n\n\nIf the customized layer is placed the first layer in the model, this error will not happen since there is no gradient for it to back prop to the layer before.\nCompared to the operation in def func(a), the code here will be executed in \"pure python\", however, the back prop code are connected by PyTorch's C backend. But I'm not familiar with this part.\nIf I uncomment the a = a + 0, this error will disappear.\n\nAny idea on this problem?", "body": "The code below will cause a segmentation fault on some test platforms:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\n\r\n\r\nclass Foo(torch.autograd.Function):\r\n    def forward(ctx, x):\r\n        return torch.from_numpy(x.numpy())\r\n    def backward(ctx, grad_output):\r\n        a = grad_output.numpy()\r\n        # a = a + 0.\r\n        return torch.from_numpy(a)\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc1 = nn.Linear(16, 16)\r\n        self.fc2 = nn.Linear(16, 16)\r\n        self.foo = Foo()\r\n\r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        x = self.foo(x)\r\n        # x = self.fc2(x)\r\n        x = x.view(1, 16)\r\n        return x;\r\n\r\ndef train(epoch):\r\n    model = Net()\r\n    model.train()\r\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.99)\r\n    data = Variable(torch.from_numpy(np.ones((1, 16), dtype=np.float32))) #TODO\r\n    target = Variable(torch.from_numpy(np.zeros((1), dtype=np.int64))) #TODO\r\n\r\n    test_data = torch.from_numpy(np.ones((1), dtype=np.float32)) #TODO\r\n    def func(a):\r\n        b = a.numpy()\r\n        # b = b + 0.\r\n        b[0] = 0\r\n        return torch.from_numpy(b)\r\n    print(test_data)\r\n    d = func(test_data)\r\n    print(d)\r\n    print(test_data)\r\n    d[0] = 2\r\n    print(d)\r\n    print(test_data)\r\n\r\n    optimizer.zero_grad()\r\n    output = model(data)\r\n    loss = F.nll_loss(output, target)\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\ndef test():\r\n    model = torch.load(args.model_path)\r\n    model.eval()\r\n\r\ndef main():\r\n    train(0)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nSome useful information:\r\n1. Test platform 1:\r\n- `Ubuntu 16.04.3`\r\n- `gcc 5.4.0`\r\n- `Python 3.5.2`\r\n- `PyTorch 0.2.0_3`\r\n2. Test platform 2:\r\n- `macOS 10.12.6`\r\n- `Apple LLVM version 9.0.0 (clang-900.0.38)`\r\n- `Python 3.6.3`\r\n- `PyTorch 0.2.0_2`\r\n3. If the customized layer is placed the first layer in the model, this error will not happen since there is no gradient for it to back prop to the layer before. \r\n4. Compared to the operation in `def func(a)`, the code here will be executed in \"pure python\", however, the back prop code are connected by PyTorch's C backend. But I'm not familiar with this part.\r\n5. If I uncomment the `a = a + 0`, this error will disappear.\r\n\r\nAny idea on this problem?\r\n"}