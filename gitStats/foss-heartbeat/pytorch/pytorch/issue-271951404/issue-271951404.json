{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3540", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3540/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3540/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3540/events", "html_url": "https://github.com/pytorch/pytorch/issues/3540", "id": 271951404, "node_id": "MDU6SXNzdWUyNzE5NTE0MDQ=", "number": 3540, "title": "Suggestion: Allow loss functions to accept per-label weights rather than per-sample", "user": {"login": "meder411", "id": 6818607, "node_id": "MDQ6VXNlcjY4MTg2MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6818607?v=4", "gravatar_id": "", "url": "https://api.github.com/users/meder411", "html_url": "https://github.com/meder411", "followers_url": "https://api.github.com/users/meder411/followers", "following_url": "https://api.github.com/users/meder411/following{/other_user}", "gists_url": "https://api.github.com/users/meder411/gists{/gist_id}", "starred_url": "https://api.github.com/users/meder411/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/meder411/subscriptions", "organizations_url": "https://api.github.com/users/meder411/orgs", "repos_url": "https://api.github.com/users/meder411/repos", "events_url": "https://api.github.com/users/meder411/events{/privacy}", "received_events_url": "https://api.github.com/users/meder411/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-11-07T19:20:21Z", "updated_at": "2017-11-17T22:08:18Z", "closed_at": "2017-11-17T22:08:18Z", "author_association": "NONE", "body_html": "<p>I'd like to suggest allowing classification loss functions to accept a weight per-label as a parameter rather than a weight per-sample. Perhaps a dictionary of class labels as keys and weights as values would suffice. The big draw to PyTorch is how high-level it is, yet for networks that predict matrices (e.g. for segmentation), it is not trivial to use a weighted loss function. Maybe there is a better way (nothing in the docs or forums suggests it), but as far as I know the following example is the only way to do this.</p>\n<p>If my output is a 8x1x500x500 labeling of 8 images with C labels, I have to create a 8x1x500x500 tensor where instead of a label I have that label's weight for every batch. Then, I must change the weights of the loss function at each batch, as</p>\n<pre><code>for i in xrange(batches):\n    ...\n    my_new_weight_tensor = # Create the new weight tensor (possibly on the GPU)\n    loss.weight = my_new_weight_tensor\n    ...\n</code></pre>\n<p>In addition to being slightly cumbersome to code, this seems to use unnecessary memory and (possibly) communication overhead.</p>\n<p>Instead I suggest the following user flow:</p>\n<pre><code>w_dict = {0:0.25, 1:0.5, 2:1.} # Here C=3\nmy_loss = BCELoss(weight=w_dict).cuda()\n</code></pre>\n<p>Now the weights are already on the GPU, and the calculation of the loss simply requires a lookup from the weights map.</p>\n<p>I'm honestly surprised this isn't already how it's done. Although perhaps it is and I am just ignorant of it...</p>\n<p>Also, even if there is no way to circumvent the memory usage (I suspect weighting with a tensor allows us to take advantage of fast matrix operations), the user flow would be substantially improved if the function constructs these tensors on the back-end.</p>", "body_text": "I'd like to suggest allowing classification loss functions to accept a weight per-label as a parameter rather than a weight per-sample. Perhaps a dictionary of class labels as keys and weights as values would suffice. The big draw to PyTorch is how high-level it is, yet for networks that predict matrices (e.g. for segmentation), it is not trivial to use a weighted loss function. Maybe there is a better way (nothing in the docs or forums suggests it), but as far as I know the following example is the only way to do this.\nIf my output is a 8x1x500x500 labeling of 8 images with C labels, I have to create a 8x1x500x500 tensor where instead of a label I have that label's weight for every batch. Then, I must change the weights of the loss function at each batch, as\nfor i in xrange(batches):\n    ...\n    my_new_weight_tensor = # Create the new weight tensor (possibly on the GPU)\n    loss.weight = my_new_weight_tensor\n    ...\n\nIn addition to being slightly cumbersome to code, this seems to use unnecessary memory and (possibly) communication overhead.\nInstead I suggest the following user flow:\nw_dict = {0:0.25, 1:0.5, 2:1.} # Here C=3\nmy_loss = BCELoss(weight=w_dict).cuda()\n\nNow the weights are already on the GPU, and the calculation of the loss simply requires a lookup from the weights map.\nI'm honestly surprised this isn't already how it's done. Although perhaps it is and I am just ignorant of it...\nAlso, even if there is no way to circumvent the memory usage (I suspect weighting with a tensor allows us to take advantage of fast matrix operations), the user flow would be substantially improved if the function constructs these tensors on the back-end.", "body": "I'd like to suggest allowing classification loss functions to accept a weight per-label as a parameter rather than a weight per-sample. Perhaps a dictionary of class labels as keys and weights as values would suffice. The big draw to PyTorch is how high-level it is, yet for networks that predict matrices (e.g. for segmentation), it is not trivial to use a weighted loss function. Maybe there is a better way (nothing in the docs or forums suggests it), but as far as I know the following example is the only way to do this.\r\n\r\nIf my output is a 8x1x500x500 labeling of 8 images with C labels, I have to create a 8x1x500x500 tensor where instead of a label I have that label's weight for every batch. Then, I must change the weights of the loss function at each batch, as\r\n    \r\n    for i in xrange(batches):\r\n        ...\r\n        my_new_weight_tensor = # Create the new weight tensor (possibly on the GPU)\r\n        loss.weight = my_new_weight_tensor\r\n        ...\r\n\r\nIn addition to being slightly cumbersome to code, this seems to use unnecessary memory and (possibly) communication overhead.\r\n\r\nInstead I suggest the following user flow:\r\n\r\n    w_dict = {0:0.25, 1:0.5, 2:1.} # Here C=3\r\n    my_loss = BCELoss(weight=w_dict).cuda()\r\n\r\nNow the weights are already on the GPU, and the calculation of the loss simply requires a lookup from the weights map.\r\n\r\nI'm honestly surprised this isn't already how it's done. Although perhaps it is and I am just ignorant of it...\r\n\r\nAlso, even if there is no way to circumvent the memory usage (I suspect weighting with a tensor allows us to take advantage of fast matrix operations), the user flow would be substantially improved if the function constructs these tensors on the back-end."}