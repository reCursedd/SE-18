{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326355203", "html_url": "https://github.com/pytorch/pytorch/issues/2584#issuecomment-326355203", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2584", "id": 326355203, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjM1NTIwMw==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T16:46:32Z", "updated_at": "2017-08-31T16:46:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In many applications, having per-GPU batchnorm is both acceptable and desirable, (see, e.g. <a href=\"https://arxiv.org/pdf/1706.02677.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1706.02677.pdf</a>), so multi-GPU batch norm should be optional. Parameters and gradients are already synchronized in DataParallelTable, mean and variance, however, are not. Torch batchnorm implementation would have to be rewritten to use stable single pass variance computation, but still, this will introduce a sync point across the GPUs, so it will have performance (as in, speed of execution, not model accuracy) impact.</p>", "body_text": "In many applications, having per-GPU batchnorm is both acceptable and desirable, (see, e.g. https://arxiv.org/pdf/1706.02677.pdf), so multi-GPU batch norm should be optional. Parameters and gradients are already synchronized in DataParallelTable, mean and variance, however, are not. Torch batchnorm implementation would have to be rewritten to use stable single pass variance computation, but still, this will introduce a sync point across the GPUs, so it will have performance (as in, speed of execution, not model accuracy) impact.", "body": "In many applications, having per-GPU batchnorm is both acceptable and desirable, (see, e.g. https://arxiv.org/pdf/1706.02677.pdf), so multi-GPU batch norm should be optional. Parameters and gradients are already synchronized in DataParallelTable, mean and variance, however, are not. Torch batchnorm implementation would have to be rewritten to use stable single pass variance computation, but still, this will introduce a sync point across the GPUs, so it will have performance (as in, speed of execution, not model accuracy) impact. "}