{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388426232", "html_url": "https://github.com/pytorch/pytorch/issues/2584#issuecomment-388426232", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2584", "id": 388426232, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODQyNjIzMg==", "user": {"login": "mario98", "id": 23034262, "node_id": "MDQ6VXNlcjIzMDM0MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/23034262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mario98", "html_url": "https://github.com/mario98", "followers_url": "https://api.github.com/users/mario98/followers", "following_url": "https://api.github.com/users/mario98/following{/other_user}", "gists_url": "https://api.github.com/users/mario98/gists{/gist_id}", "starred_url": "https://api.github.com/users/mario98/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mario98/subscriptions", "organizations_url": "https://api.github.com/users/mario98/orgs", "repos_url": "https://api.github.com/users/mario98/repos", "events_url": "https://api.github.com/users/mario98/events{/privacy}", "received_events_url": "https://api.github.com/users/mario98/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-11T17:10:29Z", "updated_at": "2018-05-11T19:14:24Z", "author_association": "NONE", "body_html": "<p>I really think if there was a synchronized batch norm as an official part of the framework maintained by the pytorch developers, this would be of great value and attract a lot of people to use pytorch. As already said, in tasks like semantic segmentation or object detection, model size is too big for a reasonable batch size per GPU, but to compete against state of the art who all use synced batch norm or some other tricks (like in-place batch norm, but that's another story) you have to do a lot of implementations yourself since they usually keep their implementation secret (like PSPNet) because they know how key this is to high accuracies.<br>\nWhile I appreciate all attempts made by others to provide sync batch norm as an unofficial extension, my experience is that none of these implementations are stable enough for people to use it and they often suffer from compatibility issues since pytorch is developed with the gloves off and things get broken from version to version (sometimes even undocumented). It requires a lot of tinkering to get those to run and even then unexpected behavior happens again and again.</p>\n<p>On the other side, if pytorch developers fail to provide such a functionality, and another framework steps in and provides synced batch norm out-of-the-box, pytorch will be over the hill. The other framework will win. Legend says that approximately each year, a new disrupting framework emerges and the old ones are unable to compete any more. Remember torch, caffe, theano? (and maybe soon tensorflow? Although tensorflow recognized the sign of the times and implemented dynamic graphs, so chances are that tensorflow will survive, at least for now...)<br>\nSync batch norm is for many developers the killer feature. And pytorch already had a great year in 2017... I'm just saying...</p>", "body_text": "I really think if there was a synchronized batch norm as an official part of the framework maintained by the pytorch developers, this would be of great value and attract a lot of people to use pytorch. As already said, in tasks like semantic segmentation or object detection, model size is too big for a reasonable batch size per GPU, but to compete against state of the art who all use synced batch norm or some other tricks (like in-place batch norm, but that's another story) you have to do a lot of implementations yourself since they usually keep their implementation secret (like PSPNet) because they know how key this is to high accuracies.\nWhile I appreciate all attempts made by others to provide sync batch norm as an unofficial extension, my experience is that none of these implementations are stable enough for people to use it and they often suffer from compatibility issues since pytorch is developed with the gloves off and things get broken from version to version (sometimes even undocumented). It requires a lot of tinkering to get those to run and even then unexpected behavior happens again and again.\nOn the other side, if pytorch developers fail to provide such a functionality, and another framework steps in and provides synced batch norm out-of-the-box, pytorch will be over the hill. The other framework will win. Legend says that approximately each year, a new disrupting framework emerges and the old ones are unable to compete any more. Remember torch, caffe, theano? (and maybe soon tensorflow? Although tensorflow recognized the sign of the times and implemented dynamic graphs, so chances are that tensorflow will survive, at least for now...)\nSync batch norm is for many developers the killer feature. And pytorch already had a great year in 2017... I'm just saying...", "body": "I really think if there was a synchronized batch norm as an official part of the framework maintained by the pytorch developers, this would be of great value and attract a lot of people to use pytorch. As already said, in tasks like semantic segmentation or object detection, model size is too big for a reasonable batch size per GPU, but to compete against state of the art who all use synced batch norm or some other tricks (like in-place batch norm, but that's another story) you have to do a lot of implementations yourself since they usually keep their implementation secret (like PSPNet) because they know how key this is to high accuracies.\r\nWhile I appreciate all attempts made by others to provide sync batch norm as an unofficial extension, my experience is that none of these implementations are stable enough for people to use it and they often suffer from compatibility issues since pytorch is developed with the gloves off and things get broken from version to version (sometimes even undocumented). It requires a lot of tinkering to get those to run and even then unexpected behavior happens again and again.\r\n\r\nOn the other side, if pytorch developers fail to provide such a functionality, and another framework steps in and provides synced batch norm out-of-the-box, pytorch will be over the hill. The other framework will win. Legend says that approximately each year, a new disrupting framework emerges and the old ones are unable to compete any more. Remember torch, caffe, theano? (and maybe soon tensorflow? Although tensorflow recognized the sign of the times and implemented dynamic graphs, so chances are that tensorflow will survive, at least for now...)\r\nSync batch norm is for many developers the killer feature. And pytorch already had a great year in 2017... I'm just saying... "}