{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413709953", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-413709953", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 413709953, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzcwOTk1Mw==", "user": {"login": "c0nn3r", "id": 6255953, "node_id": "MDQ6VXNlcjYyNTU5NTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6255953?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c0nn3r", "html_url": "https://github.com/c0nn3r", "followers_url": "https://api.github.com/users/c0nn3r/followers", "following_url": "https://api.github.com/users/c0nn3r/following{/other_user}", "gists_url": "https://api.github.com/users/c0nn3r/gists{/gist_id}", "starred_url": "https://api.github.com/users/c0nn3r/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c0nn3r/subscriptions", "organizations_url": "https://api.github.com/users/c0nn3r/orgs", "repos_url": "https://api.github.com/users/c0nn3r/repos", "events_url": "https://api.github.com/users/c0nn3r/events{/privacy}", "received_events_url": "https://api.github.com/users/c0nn3r/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-16T23:03:29Z", "updated_at": "2018-08-16T23:09:59Z", "author_association": "NONE", "body_html": "<p>Sure, but I think this more speaks to the need for better ways to <a href=\"https://github.com/salesforce/matchbox\">handle padding</a> (something that comes up a lot when implementing Transformer models). Integrating padding in the JIT is a great step for this. <a href=\"https://github.com/pytorch/pytorch/issues/4164\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4164/hovercard\">Named dimensions</a> are another way we can allow Transformer code immensely more readable, here is an example of this (<a href=\"https://github.com/mttk/rnn-classifier/blob/master/model.py\">taken from @mttk's implementation of an attention mechanism for an RNN</a>):</p>\n<p>Before:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Attention</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">query_dim</span>, <span class=\"pl-smi\">key_dim</span>, <span class=\"pl-smi\">value_dim</span>):\n    <span class=\"pl-c1\">super</span>(Attention, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>. <span class=\"pl-k\">/</span> math.sqrt(query_dim)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">query</span>, <span class=\"pl-smi\">keys</span>, <span class=\"pl-smi\">values</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Query = [BxQ]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Keys = [TxBxK]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Values = [TxBxV]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Outputs = a:[TxB], lin_comb:[BxV]</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Here we assume q_dim == k_dim (dot product attention)</span>\n\n    query <span class=\"pl-k\">=</span> query.unsqueeze(<span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> [BxQ] -&gt; [Bx1xQ]</span>\n    keys <span class=\"pl-k\">=</span> keys.transpose(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>).transpose(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> [TxBxK] -&gt; [BxKxT]</span>\n    energy <span class=\"pl-k\">=</span> query <span class=\"pl-k\">@</span> keys <span class=\"pl-c\"><span class=\"pl-c\">#</span> [Bx1xQ]x[BxKxT] -&gt; [Bx1xT]</span>\n    energy <span class=\"pl-k\">=</span> F.softmax(energy.mul_(<span class=\"pl-c1\">self</span>.scale), <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> scale, normalize</span>\n\n    values <span class=\"pl-k\">=</span> values.transpose(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> [TxBxV] -&gt; [BxTxV]</span>\n    linear_combination <span class=\"pl-k\">=</span> (energy <span class=\"pl-k\">@</span> values).squeeze(<span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span>[Bx1xT]x[BxTxV] -&gt; [BxV]</span>\n    <span class=\"pl-k\">return</span> energy, linear_combination</pre></div>\n<p>These types of comments are very common on Transformer implementations to keep track of the current dimensions.</p>\n<p>After:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Attention</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">query_dim</span>, <span class=\"pl-smi\">key_dim</span>, <span class=\"pl-smi\">value_dim</span>):\n    <span class=\"pl-c1\">super</span>(Attention, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>. <span class=\"pl-k\">/</span> math.sqrt(query_dim)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">query</span>, <span class=\"pl-smi\">keys</span>, <span class=\"pl-smi\">values</span>):\n    query <span class=\"pl-k\">=</span> query.unsqueeze(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch<span class=\"pl-pds\">'</span></span>)\n    keys <span class=\"pl-k\">=</span> keys.transpose(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timesteps<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch<span class=\"pl-pds\">'</span></span>).transpose(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timesteps<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>keys<span class=\"pl-pds\">'</span></span>)\n    energy <span class=\"pl-k\">=</span> query <span class=\"pl-k\">@</span> keys\n    energy <span class=\"pl-k\">=</span> F.softmax(energy.mul_(<span class=\"pl-c1\">self</span>.scale), <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>timesteps<span class=\"pl-pds\">'</span></span>)\n\n    values <span class=\"pl-k\">=</span> values.transpose(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timesteps<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch<span class=\"pl-pds\">'</span></span>)\n    linear_combination <span class=\"pl-k\">=</span> (energy <span class=\"pl-k\">@</span> values).squeeze(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timesteps<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> energy, linear_combination</pre></div>\n<p>(Sorry if I made any mistakes converting this) Why not better enable people to write better Transformer and attention code?</p>\n<p>Yes, Transformers have shown impressive performance in a variate of tasks, but if anything this speaks to my point that there are potentially even larger gains to be had.</p>\n<p>Completely agree with your point about <code>tf.contrib</code> - definitely about balance.</p>", "body_text": "Sure, but I think this more speaks to the need for better ways to handle padding (something that comes up a lot when implementing Transformer models). Integrating padding in the JIT is a great step for this. Named dimensions are another way we can allow Transformer code immensely more readable, here is an example of this (taken from @mttk's implementation of an attention mechanism for an RNN):\nBefore:\nclass Attention(nn.Module):\n  def __init__(self, query_dim, key_dim, value_dim):\n    super(Attention, self).__init__()\n    self.scale = 1. / math.sqrt(query_dim)\n\n  def forward(self, query, keys, values):\n    # Query = [BxQ]\n    # Keys = [TxBxK]\n    # Values = [TxBxV]\n    # Outputs = a:[TxB], lin_comb:[BxV]\n\n    # Here we assume q_dim == k_dim (dot product attention)\n\n    query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\n    keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n    energy = query @ keys # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n    energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\n\n    values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n    linear_combination = (energy @ values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n    return energy, linear_combination\nThese types of comments are very common on Transformer implementations to keep track of the current dimensions.\nAfter:\nclass Attention(nn.Module):\n  def __init__(self, query_dim, key_dim, value_dim):\n    super(Attention, self).__init__()\n    self.scale = 1. / math.sqrt(query_dim)\n\n  def forward(self, query, keys, values):\n    query = query.unsqueeze('batch')\n    keys = keys.transpose('timesteps', 'batch').transpose('timesteps', 'keys')\n    energy = query @ keys\n    energy = F.softmax(energy.mul_(self.scale), dim='timesteps')\n\n    values = values.transpose('timesteps', 'batch')\n    linear_combination = (energy @ values).squeeze('timesteps')\n    return energy, linear_combination\n(Sorry if I made any mistakes converting this) Why not better enable people to write better Transformer and attention code?\nYes, Transformers have shown impressive performance in a variate of tasks, but if anything this speaks to my point that there are potentially even larger gains to be had.\nCompletely agree with your point about tf.contrib - definitely about balance.", "body": "Sure, but I think this more speaks to the need for better ways to [handle padding](https://github.com/salesforce/matchbox) (something that comes up a lot when implementing Transformer models). Integrating padding in the JIT is a great step for this. [Named dimensions](https://github.com/pytorch/pytorch/issues/4164) are another way we can allow Transformer code immensely more readable, here is an example of this ([taken from @mttk's implementation of an attention mechanism for an RNN](https://github.com/mttk/rnn-classifier/blob/master/model.py)):\r\n\r\nBefore:\r\n```python\r\nclass Attention(nn.Module):\r\n  def __init__(self, query_dim, key_dim, value_dim):\r\n    super(Attention, self).__init__()\r\n    self.scale = 1. / math.sqrt(query_dim)\r\n\r\n  def forward(self, query, keys, values):\r\n    # Query = [BxQ]\r\n    # Keys = [TxBxK]\r\n    # Values = [TxBxV]\r\n    # Outputs = a:[TxB], lin_comb:[BxV]\r\n\r\n    # Here we assume q_dim == k_dim (dot product attention)\r\n\r\n    query = query.unsqueeze(1) # [BxQ] -> [Bx1xQ]\r\n    keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\r\n    energy = query @ keys # [Bx1xQ]x[BxKxT] -> [Bx1xT]\r\n    energy = F.softmax(energy.mul_(self.scale), dim=2) # scale, normalize\r\n\r\n    values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\r\n    linear_combination = (energy @ values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\r\n    return energy, linear_combination\r\n```\r\nThese types of comments are very common on Transformer implementations to keep track of the current dimensions. \r\n\r\nAfter:\r\n```python\r\nclass Attention(nn.Module):\r\n  def __init__(self, query_dim, key_dim, value_dim):\r\n    super(Attention, self).__init__()\r\n    self.scale = 1. / math.sqrt(query_dim)\r\n\r\n  def forward(self, query, keys, values):\r\n    query = query.unsqueeze('batch')\r\n    keys = keys.transpose('timesteps', 'batch').transpose('timesteps', 'keys')\r\n    energy = query @ keys\r\n    energy = F.softmax(energy.mul_(self.scale), dim='timesteps')\r\n\r\n    values = values.transpose('timesteps', 'batch')\r\n    linear_combination = (energy @ values).squeeze('timesteps')\r\n    return energy, linear_combination\r\n```\r\n(Sorry if I made any mistakes converting this) Why not better enable people to write better Transformer and attention code?\r\n\r\nYes, Transformers have shown impressive performance in a variate of tasks, but if anything this speaks to my point that there are potentially even larger gains to be had.\r\n\r\nCompletely agree with your point about `tf.contrib` - definitely about balance."}