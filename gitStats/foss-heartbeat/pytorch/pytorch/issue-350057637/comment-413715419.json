{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413715419", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-413715419", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 413715419, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzcxNTQxOQ==", "user": {"login": "ArmenAg", "id": 4429794, "node_id": "MDQ6VXNlcjQ0Mjk3OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/4429794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArmenAg", "html_url": "https://github.com/ArmenAg", "followers_url": "https://api.github.com/users/ArmenAg/followers", "following_url": "https://api.github.com/users/ArmenAg/following{/other_user}", "gists_url": "https://api.github.com/users/ArmenAg/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArmenAg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArmenAg/subscriptions", "organizations_url": "https://api.github.com/users/ArmenAg/orgs", "repos_url": "https://api.github.com/users/ArmenAg/repos", "events_url": "https://api.github.com/users/ArmenAg/events{/privacy}", "received_events_url": "https://api.github.com/users/ArmenAg/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-16T23:36:44Z", "updated_at": "2018-08-16T23:36:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The discussion on named dimensions should be moved to a different issue. If we are to agree that some common attention models need to be implemented we should also discuss masking\\padding.</p>\n<p>The current way I deal with masking is by passing a secondary byte tensor marking with 1 all the padding locations. From there you can do something like so:</p>\n<pre><code>class ScaledDotProductAttention(nn.Module):\n    \"\"\"Scaled Dot Product Attentin from the Attention is All You Need paper.\n        Refs: https://arxiv.org/abs/1706.03762\n    \"\"\"\n\n    def __init__(self, dropout: float=0.1):\n        \"\"\"\n            Args:\n                dropout: float ; s.e.\n        \"\"\"\n        super(ScaledDotProductAttention, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, attn_mask: Optional[torch.ByteTensor]=None) -&gt; torch.Tensor:\n        \"\"\"\n            Args:\n                query: torch.Tensor(batch_size, time_step_query, d_model) ; query tensor\n                keys: torch.Tensor(batch_size, time_step_keys, d_model) ; keys tensor\n                values: torch.Tensor(batch_size, time_step_keys, d_value) ; values tensor\n                attn_mask: Optional[torch.ByteTensor(batch_size, time_step_query)] ; attn mask tensor\n            Returns:\n                output: torch.Tensor(batch_size, time_step_query, d_value) ; output tensor\n        \"\"\"\n        attn = torch.bmm(query, keys.transpose(1, 2))\n        attn = attn.div(query.size(2)**0.5)\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(2).expand_as(attn)\n            # instability with -inf\n            attn.data.masked_fill_(attn_mask, -100)\n\n        attn = F.softmax(attn, dim=2)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, values)\n\n        return output\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Do you have any comments on this thread?</p>", "body_text": "The discussion on named dimensions should be moved to a different issue. If we are to agree that some common attention models need to be implemented we should also discuss masking\\padding.\nThe current way I deal with masking is by passing a secondary byte tensor marking with 1 all the padding locations. From there you can do something like so:\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"Scaled Dot Product Attentin from the Attention is All You Need paper.\n        Refs: https://arxiv.org/abs/1706.03762\n    \"\"\"\n\n    def __init__(self, dropout: float=0.1):\n        \"\"\"\n            Args:\n                dropout: float ; s.e.\n        \"\"\"\n        super(ScaledDotProductAttention, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, attn_mask: Optional[torch.ByteTensor]=None) -> torch.Tensor:\n        \"\"\"\n            Args:\n                query: torch.Tensor(batch_size, time_step_query, d_model) ; query tensor\n                keys: torch.Tensor(batch_size, time_step_keys, d_model) ; keys tensor\n                values: torch.Tensor(batch_size, time_step_keys, d_value) ; values tensor\n                attn_mask: Optional[torch.ByteTensor(batch_size, time_step_query)] ; attn mask tensor\n            Returns:\n                output: torch.Tensor(batch_size, time_step_query, d_value) ; output tensor\n        \"\"\"\n        attn = torch.bmm(query, keys.transpose(1, 2))\n        attn = attn.div(query.size(2)**0.5)\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(2).expand_as(attn)\n            # instability with -inf\n            attn.data.masked_fill_(attn_mask, -100)\n\n        attn = F.softmax(attn, dim=2)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, values)\n\n        return output\n\n@soumith @apaszke Do you have any comments on this thread?", "body": "The discussion on named dimensions should be moved to a different issue. If we are to agree that some common attention models need to be implemented we should also discuss masking\\padding.\r\n\r\nThe current way I deal with masking is by passing a secondary byte tensor marking with 1 all the padding locations. From there you can do something like so:\r\n\r\n```\r\nclass ScaledDotProductAttention(nn.Module):\r\n    \"\"\"Scaled Dot Product Attentin from the Attention is All You Need paper.\r\n        Refs: https://arxiv.org/abs/1706.03762\r\n    \"\"\"\r\n\r\n    def __init__(self, dropout: float=0.1):\r\n        \"\"\"\r\n            Args:\r\n                dropout: float ; s.e.\r\n        \"\"\"\r\n        super(ScaledDotProductAttention, self).__init__()\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, query: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, attn_mask: Optional[torch.ByteTensor]=None) -> torch.Tensor:\r\n        \"\"\"\r\n            Args:\r\n                query: torch.Tensor(batch_size, time_step_query, d_model) ; query tensor\r\n                keys: torch.Tensor(batch_size, time_step_keys, d_model) ; keys tensor\r\n                values: torch.Tensor(batch_size, time_step_keys, d_value) ; values tensor\r\n                attn_mask: Optional[torch.ByteTensor(batch_size, time_step_query)] ; attn mask tensor\r\n            Returns:\r\n                output: torch.Tensor(batch_size, time_step_query, d_value) ; output tensor\r\n        \"\"\"\r\n        attn = torch.bmm(query, keys.transpose(1, 2))\r\n        attn = attn.div(query.size(2)**0.5)\r\n\r\n        if attn_mask is not None:\r\n            attn_mask = attn_mask.unsqueeze(2).expand_as(attn)\r\n            # instability with -inf\r\n            attn.data.masked_fill_(attn_mask, -100)\r\n\r\n        attn = F.softmax(attn, dim=2)\r\n        attn = self.dropout(attn)\r\n        output = torch.bmm(attn, values)\r\n\r\n        return output\r\n```\r\n\r\n@soumith @apaszke Do you have any comments on this thread?"}