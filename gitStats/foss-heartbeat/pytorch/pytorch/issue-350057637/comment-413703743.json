{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413703743", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-413703743", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 413703743, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzcwMzc0Mw==", "user": {"login": "c0nn3r", "id": 6255953, "node_id": "MDQ6VXNlcjYyNTU5NTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6255953?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c0nn3r", "html_url": "https://github.com/c0nn3r", "followers_url": "https://api.github.com/users/c0nn3r/followers", "following_url": "https://api.github.com/users/c0nn3r/following{/other_user}", "gists_url": "https://api.github.com/users/c0nn3r/gists{/gist_id}", "starred_url": "https://api.github.com/users/c0nn3r/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c0nn3r/subscriptions", "organizations_url": "https://api.github.com/users/c0nn3r/orgs", "repos_url": "https://api.github.com/users/c0nn3r/repos", "events_url": "https://api.github.com/users/c0nn3r/events{/privacy}", "received_events_url": "https://api.github.com/users/c0nn3r/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-16T22:29:11Z", "updated_at": "2018-08-16T22:32:34Z", "author_association": "NONE", "body_html": "<p>I don't think Transformers should be added for a reason demonstrated in this thread.</p>\n<p>LSTMs are a <a href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\" rel=\"nofollow\">very mature model</a> (introduced in 1997) and it has been demonstrated that it is <a href=\"https://arxiv.org/abs/1707.05589\" rel=\"nofollow\">very hard to improve with over a properly turning LSTM</a>. However, a major improvement to the LSTM: the forget gate, <a href=\"https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf\" rel=\"nofollow\">was introduced in 1999</a>, two years later from the original paper. At this point after many papers evaluating LSTM variants it's clear what type of API is needed for most LSTM applications and the same cannot be said for Transformers.</p>\n<p>The problem with adding Transformers to core is because there will always be \"a new Transformer town\" (at least it seems for the next few months ;) ) and I think there needs to be evaluation after the dust has settled.</p>\n<p>Another question is tradeoffs. If the implementation in core is focused around speed then it will be hard to tweak (granted the JIT will help with this issue somewhat) and if it focused on usability then there will always be <a href=\"https://github.com/marian-nmt/marian\">implementations that outperform it</a>.</p>\n<p>I'm not saying a implementation shouldn't exist in the PyTorch ecosystem, I'm just not convinced it should be part of core.</p>", "body_text": "I don't think Transformers should be added for a reason demonstrated in this thread.\nLSTMs are a very mature model (introduced in 1997) and it has been demonstrated that it is very hard to improve with over a properly turning LSTM. However, a major improvement to the LSTM: the forget gate, was introduced in 1999, two years later from the original paper. At this point after many papers evaluating LSTM variants it's clear what type of API is needed for most LSTM applications and the same cannot be said for Transformers.\nThe problem with adding Transformers to core is because there will always be \"a new Transformer town\" (at least it seems for the next few months ;) ) and I think there needs to be evaluation after the dust has settled.\nAnother question is tradeoffs. If the implementation in core is focused around speed then it will be hard to tweak (granted the JIT will help with this issue somewhat) and if it focused on usability then there will always be implementations that outperform it.\nI'm not saying a implementation shouldn't exist in the PyTorch ecosystem, I'm just not convinced it should be part of core.", "body": "I don't think Transformers should be added for a reason demonstrated in this thread.\r\n\r\nLSTMs are a [very mature model](http://www.bioinf.jku.at/publications/older/2604.pdf) (introduced in 1997) and it has been demonstrated that it is [very hard to improve with over a properly turning LSTM](https://arxiv.org/abs/1707.05589). However, a major improvement to the LSTM: the forget gate, [was introduced in 1999](https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf), two years later from the original paper. At this point after many papers evaluating LSTM variants it's clear what type of API is needed for most LSTM applications and the same cannot be said for Transformers.\r\n\r\nThe problem with adding Transformers to core is because there will always be \"a new Transformer town\" (at least it seems for the next few months ;) ) and I think there needs to be evaluation after the dust has settled.\r\n\r\nAnother question is tradeoffs. If the implementation in core is focused around speed then it will be hard to tweak (granted the JIT will help with this issue somewhat) and if it focused on usability then there will always be [implementations that outperform it](https://github.com/marian-nmt/marian).\r\n\r\nI'm not saying a implementation shouldn't exist in the PyTorch ecosystem, I'm just not convinced it should be part of core."}