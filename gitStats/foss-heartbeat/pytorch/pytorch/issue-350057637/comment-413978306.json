{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413978306", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-413978306", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 413978306, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzk3ODMwNg==", "user": {"login": "myleott", "id": 231798, "node_id": "MDQ6VXNlcjIzMTc5OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/231798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myleott", "html_url": "https://github.com/myleott", "followers_url": "https://api.github.com/users/myleott/followers", "following_url": "https://api.github.com/users/myleott/following{/other_user}", "gists_url": "https://api.github.com/users/myleott/gists{/gist_id}", "starred_url": "https://api.github.com/users/myleott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myleott/subscriptions", "organizations_url": "https://api.github.com/users/myleott/orgs", "repos_url": "https://api.github.com/users/myleott/repos", "events_url": "https://api.github.com/users/myleott/events{/privacy}", "received_events_url": "https://api.github.com/users/myleott/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T20:21:58Z", "updated_at": "2018-08-17T20:34:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'll echo the sentiment that the \"Transformer\" architecture is a moving target and doesn't seem like it belongs in core. On the other hand, a clean reference implementation of the \"Attention is all you need\" paper in <code>torch/contrib</code> seems very useful.</p>\n<p>I also think there's value in provided some of the building blocks in core, particularly multi-headed attention. The interface for multi-headed attention is pretty clean [1], there are a few performance improvements that would be nice to have in core [2], and it's arguably the most complicated part of implementing a Transformer model anyway.</p>\n<p>[1] Here's an example interface for multi-head attention, assuming we can use packed sequences to handle padding:</p>\n<pre><code>MultiHeadAttention(Query, Key, Value)\n\nArgs:\n    in_features: The number of expected features in the query.\n    out_features: The number of expected features in the value.\n    num_heads: Number of attention heads. `in_features` and `out_features`\n        must be divisible by `num_heads`.\n    dropout: If non-zero, introduces a Dropout layer on the attention\n        weights, with dropout probability equal to `dropout`. Default: 0.\n\nInputs: query, key, value\n    - **query** of shape `(num_queries, batch, in_features)`\n    - **key** of shape `(num_keys, batch, in_features)`\n    - **value** of shape `(num_keys, batch, out_features)`\n\n    The `query` and `key` can also be a packed variable length sequence, in\n    which case padding elements will not receive any attention weight. See\n    `torch.nn.utils.rnn.pack_padded_sequence()` for details.\n\nOutputs: output\n    - **output** of shape `(num_queries, batch, out_features)`\n</code></pre>\n<p>[2] There are a number of performance enhancements in the fairseq implementation that are useful, e.g., identifying when query==key==value (self-attention) and using a single large linear projection. I'm sure much more could be done here too, e.g., fused kernels.</p>", "body_text": "I'll echo the sentiment that the \"Transformer\" architecture is a moving target and doesn't seem like it belongs in core. On the other hand, a clean reference implementation of the \"Attention is all you need\" paper in torch/contrib seems very useful.\nI also think there's value in provided some of the building blocks in core, particularly multi-headed attention. The interface for multi-headed attention is pretty clean [1], there are a few performance improvements that would be nice to have in core [2], and it's arguably the most complicated part of implementing a Transformer model anyway.\n[1] Here's an example interface for multi-head attention, assuming we can use packed sequences to handle padding:\nMultiHeadAttention(Query, Key, Value)\n\nArgs:\n    in_features: The number of expected features in the query.\n    out_features: The number of expected features in the value.\n    num_heads: Number of attention heads. `in_features` and `out_features`\n        must be divisible by `num_heads`.\n    dropout: If non-zero, introduces a Dropout layer on the attention\n        weights, with dropout probability equal to `dropout`. Default: 0.\n\nInputs: query, key, value\n    - **query** of shape `(num_queries, batch, in_features)`\n    - **key** of shape `(num_keys, batch, in_features)`\n    - **value** of shape `(num_keys, batch, out_features)`\n\n    The `query` and `key` can also be a packed variable length sequence, in\n    which case padding elements will not receive any attention weight. See\n    `torch.nn.utils.rnn.pack_padded_sequence()` for details.\n\nOutputs: output\n    - **output** of shape `(num_queries, batch, out_features)`\n\n[2] There are a number of performance enhancements in the fairseq implementation that are useful, e.g., identifying when query==key==value (self-attention) and using a single large linear projection. I'm sure much more could be done here too, e.g., fused kernels.", "body": "I'll echo the sentiment that the \"Transformer\" architecture is a moving target and doesn't seem like it belongs in core. On the other hand, a clean reference implementation of the \"Attention is all you need\" paper in `torch/contrib` seems very useful.\r\n\r\nI also think there's value in provided some of the building blocks in core, particularly multi-headed attention. The interface for multi-headed attention is pretty clean [1], there are a few performance improvements that would be nice to have in core [2], and it's arguably the most complicated part of implementing a Transformer model anyway.\r\n\r\n[1] Here's an example interface for multi-head attention, assuming we can use packed sequences to handle padding:\r\n```\r\nMultiHeadAttention(Query, Key, Value)\r\n\r\nArgs:\r\n    in_features: The number of expected features in the query.\r\n    out_features: The number of expected features in the value.\r\n    num_heads: Number of attention heads. `in_features` and `out_features`\r\n        must be divisible by `num_heads`.\r\n    dropout: If non-zero, introduces a Dropout layer on the attention\r\n        weights, with dropout probability equal to `dropout`. Default: 0.\r\n\r\nInputs: query, key, value\r\n    - **query** of shape `(num_queries, batch, in_features)`\r\n    - **key** of shape `(num_keys, batch, in_features)`\r\n    - **value** of shape `(num_keys, batch, out_features)`\r\n\r\n    The `query` and `key` can also be a packed variable length sequence, in\r\n    which case padding elements will not receive any attention weight. See\r\n    `torch.nn.utils.rnn.pack_padded_sequence()` for details.\r\n\r\nOutputs: output\r\n    - **output** of shape `(num_queries, batch, out_features)`\r\n```\r\n\r\n[2] There are a number of performance enhancements in the fairseq implementation that are useful, e.g., identifying when query==key==value (self-attention) and using a single large linear projection. I'm sure much more could be done here too, e.g., fused kernels."}