{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413914519", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-413914519", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 413914519, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzkxNDUxOQ==", "user": {"login": "mttk", "id": 3007947, "node_id": "MDQ6VXNlcjMwMDc5NDc=", "avatar_url": "https://avatars2.githubusercontent.com/u/3007947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mttk", "html_url": "https://github.com/mttk", "followers_url": "https://api.github.com/users/mttk/followers", "following_url": "https://api.github.com/users/mttk/following{/other_user}", "gists_url": "https://api.github.com/users/mttk/gists{/gist_id}", "starred_url": "https://api.github.com/users/mttk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mttk/subscriptions", "organizations_url": "https://api.github.com/users/mttk/orgs", "repos_url": "https://api.github.com/users/mttk/repos", "events_url": "https://api.github.com/users/mttk/events{/privacy}", "received_events_url": "https://api.github.com/users/mttk/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T16:11:44Z", "updated_at": "2018-08-17T16:31:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It would be really important to start populating <code>torch/contrib</code> a bit more with building blocks and complete model implementations, similar to that other large DL library. The transformer would fit under the complete model umbrella, while various attention implementations &amp; positional encoding are building blocks.</p>\n<p>I have a file with a bunch of implemented models which I keep copy/pasting to every new project, but I'm sure that there are a lot of inefficiencies in the implementations. For this reason it would be awesome to have a lot of people stress testing and improving an \"official\" implementation, and the decisions whether to add some component and where to add it to can be handled via PR's.</p>\n<p>There's a lot of great libraries such as fairseq and AllenNLP, but I dislike importing a whole library and all of its dependencies / overhead if I just need one module. Having clean, optimized and minimal implementations would be awesome.</p>\n<p>Edit: just to clarify, the point here is that models which are actively developed and improved on in current research can be part of <code>contrib</code>, while when they have stabilized, they can be moved to core. The upshot is having the implementation <em>somewhere</em>.</p>", "body_text": "It would be really important to start populating torch/contrib a bit more with building blocks and complete model implementations, similar to that other large DL library. The transformer would fit under the complete model umbrella, while various attention implementations & positional encoding are building blocks.\nI have a file with a bunch of implemented models which I keep copy/pasting to every new project, but I'm sure that there are a lot of inefficiencies in the implementations. For this reason it would be awesome to have a lot of people stress testing and improving an \"official\" implementation, and the decisions whether to add some component and where to add it to can be handled via PR's.\nThere's a lot of great libraries such as fairseq and AllenNLP, but I dislike importing a whole library and all of its dependencies / overhead if I just need one module. Having clean, optimized and minimal implementations would be awesome.\nEdit: just to clarify, the point here is that models which are actively developed and improved on in current research can be part of contrib, while when they have stabilized, they can be moved to core. The upshot is having the implementation somewhere.", "body": "It would be really important to start populating `torch/contrib` a bit more with building blocks and complete model implementations, similar to that other large DL library. The transformer would fit under the complete model umbrella, while various attention implementations & positional encoding are building blocks.\r\n\r\nI have a file with a bunch of implemented models which I keep copy/pasting to every new project, but I'm sure that there are a lot of inefficiencies in the implementations. For this reason it would be awesome to have a lot of people stress testing and improving an \"official\" implementation, and the decisions whether to add some component and where to add it to can be handled via PR's.\r\n\r\nThere's a lot of great libraries such as fairseq and AllenNLP, but I dislike importing a whole library and all of its dependencies / overhead if I just need one module. Having clean, optimized and minimal implementations would be awesome. \r\n\r\nEdit: just to clarify, the point here is that models which are actively developed and improved on in current research can be part of `contrib`, while when they have stabilized, they can be moved to core. The upshot is having the implementation _somewhere_."}