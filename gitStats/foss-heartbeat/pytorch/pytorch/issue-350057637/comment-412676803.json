{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/412676803", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-412676803", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 412676803, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjY3NjgwMw==", "user": {"login": "shawnjhenry", "id": 9464836, "node_id": "MDQ6VXNlcjk0NjQ4MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9464836?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnjhenry", "html_url": "https://github.com/shawnjhenry", "followers_url": "https://api.github.com/users/shawnjhenry/followers", "following_url": "https://api.github.com/users/shawnjhenry/following{/other_user}", "gists_url": "https://api.github.com/users/shawnjhenry/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnjhenry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnjhenry/subscriptions", "organizations_url": "https://api.github.com/users/shawnjhenry/orgs", "repos_url": "https://api.github.com/users/shawnjhenry/repos", "events_url": "https://api.github.com/users/shawnjhenry/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnjhenry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-13T21:48:59Z", "updated_at": "2018-08-13T21:48:59Z", "author_association": "NONE", "body_html": "<p>I've seen a few variants of the transformer architecture floating around, including one for monolingual text generation used here: <a href=\"https://arxiv.org/pdf/1801.10198.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1801.10198.pdf</a> to generate Wikipedia articles from source documents and here: <a href=\"https://arxiv.org/pdf/1808.02622.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1808.02622.pdf</a> to generate physician notes from electronic health records.  The main difference is that, rather than vanilla self attention, this model uses what the authors call \"local attention\" (basically just regular self attention sharded into more manageable sizes) and \"memory compressed attention\" (the keys and values are length compressed using a strided 1d convolution before applying the self attention).  This model would be impossible to implement with a monolithic nn.Transformer module unless it includes the sharding and memory compression options.  There may be benefits to implementing the layers that comprise the transformer architecture individually as suggested above.</p>", "body_text": "I've seen a few variants of the transformer architecture floating around, including one for monolingual text generation used here: https://arxiv.org/pdf/1801.10198.pdf to generate Wikipedia articles from source documents and here: https://arxiv.org/pdf/1808.02622.pdf to generate physician notes from electronic health records.  The main difference is that, rather than vanilla self attention, this model uses what the authors call \"local attention\" (basically just regular self attention sharded into more manageable sizes) and \"memory compressed attention\" (the keys and values are length compressed using a strided 1d convolution before applying the self attention).  This model would be impossible to implement with a monolithic nn.Transformer module unless it includes the sharding and memory compression options.  There may be benefits to implementing the layers that comprise the transformer architecture individually as suggested above.", "body": "I've seen a few variants of the transformer architecture floating around, including one for monolingual text generation used here: https://arxiv.org/pdf/1801.10198.pdf to generate Wikipedia articles from source documents and here: https://arxiv.org/pdf/1808.02622.pdf to generate physician notes from electronic health records.  The main difference is that, rather than vanilla self attention, this model uses what the authors call \"local attention\" (basically just regular self attention sharded into more manageable sizes) and \"memory compressed attention\" (the keys and values are length compressed using a strided 1d convolution before applying the self attention).  This model would be impossible to implement with a monolithic nn.Transformer module unless it includes the sharding and memory compression options.  There may be benefits to implementing the layers that comprise the transformer architecture individually as suggested above."}