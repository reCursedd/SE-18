{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/416305983", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-416305983", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 416305983, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjMwNTk4Mw==", "user": {"login": "ArmenAg", "id": 4429794, "node_id": "MDQ6VXNlcjQ0Mjk3OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/4429794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArmenAg", "html_url": "https://github.com/ArmenAg", "followers_url": "https://api.github.com/users/ArmenAg/followers", "following_url": "https://api.github.com/users/ArmenAg/following{/other_user}", "gists_url": "https://api.github.com/users/ArmenAg/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArmenAg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArmenAg/subscriptions", "organizations_url": "https://api.github.com/users/ArmenAg/orgs", "repos_url": "https://api.github.com/users/ArmenAg/repos", "events_url": "https://api.github.com/users/ArmenAg/events{/privacy}", "received_events_url": "https://api.github.com/users/ArmenAg/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-27T17:38:15Z", "updated_at": "2018-08-27T17:38:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It seems like the conclusion reached is that we should implement common attention modules in core, while leaving end-to-end architectures to be cleanly implemented in contrib. As long as we're vigilant about what code ends up in contrib/ this approach makes sense.</p>\n<p>In terms of padding do we want to use <code>torch.nn.utils.rnn.pack_padded_sequence()</code>? In my opinion it's easier to pass through a boolean tensor with 1's representing pad values in order to mask. It seems like this is how fairseq does it and how our team does it as well. I can pick up this work item once the masking api is somewhat clear.</p>", "body_text": "It seems like the conclusion reached is that we should implement common attention modules in core, while leaving end-to-end architectures to be cleanly implemented in contrib. As long as we're vigilant about what code ends up in contrib/ this approach makes sense.\nIn terms of padding do we want to use torch.nn.utils.rnn.pack_padded_sequence()? In my opinion it's easier to pass through a boolean tensor with 1's representing pad values in order to mask. It seems like this is how fairseq does it and how our team does it as well. I can pick up this work item once the masking api is somewhat clear.", "body": "It seems like the conclusion reached is that we should implement common attention modules in core, while leaving end-to-end architectures to be cleanly implemented in contrib. As long as we're vigilant about what code ends up in contrib/ this approach makes sense.\r\n\r\nIn terms of padding do we want to use `torch.nn.utils.rnn.pack_padded_sequence()`? In my opinion it's easier to pass through a boolean tensor with 1's representing pad values in order to mask. It seems like this is how fairseq does it and how our team does it as well. I can pick up this work item once the masking api is somewhat clear.\r\n"}