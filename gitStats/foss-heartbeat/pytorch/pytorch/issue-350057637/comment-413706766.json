{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413706766", "html_url": "https://github.com/pytorch/pytorch/issues/10459#issuecomment-413706766", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10459", "id": 413706766, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzcwNjc2Ng==", "user": {"login": "ArmenAg", "id": 4429794, "node_id": "MDQ6VXNlcjQ0Mjk3OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/4429794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArmenAg", "html_url": "https://github.com/ArmenAg", "followers_url": "https://api.github.com/users/ArmenAg/followers", "following_url": "https://api.github.com/users/ArmenAg/following{/other_user}", "gists_url": "https://api.github.com/users/ArmenAg/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArmenAg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArmenAg/subscriptions", "organizations_url": "https://api.github.com/users/ArmenAg/orgs", "repos_url": "https://api.github.com/users/ArmenAg/repos", "events_url": "https://api.github.com/users/ArmenAg/events{/privacy}", "received_events_url": "https://api.github.com/users/ArmenAg/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-16T22:45:57Z", "updated_at": "2018-08-16T22:46:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is a good point. Transformer as an end to end model might not be mature but in NLP we are seeing a general trend toward utilization of attention. Even if we're not to implement the Transfomer I still believe it is necessary to implement the common attention modules.</p>\n<p>In general this is a question of how agile PyTorch as a framework will be. If we're too agile we might have a mess on our hands along the lines of tf.contrib, but on the other hand only implementing the very core modules in deep learning will lead to a lot of code duplication or bugs across various projects/codebases. The debate should be where this sweet spot is.</p>\n<p>And to play devils advocate to your point about Transformer, in the short time that they have been out they're showing <a href=\"https://arxiv.org/abs/1808.04444\" rel=\"nofollow\">better performance than LSTMs</a>. Maybe we should't go all out in writing a highly performant C++ implementation of Transformers, but a well tested standardized python version would be of great help to the NLP community. At least common attention modules should be implemented.</p>", "body_text": "This is a good point. Transformer as an end to end model might not be mature but in NLP we are seeing a general trend toward utilization of attention. Even if we're not to implement the Transfomer I still believe it is necessary to implement the common attention modules.\nIn general this is a question of how agile PyTorch as a framework will be. If we're too agile we might have a mess on our hands along the lines of tf.contrib, but on the other hand only implementing the very core modules in deep learning will lead to a lot of code duplication or bugs across various projects/codebases. The debate should be where this sweet spot is.\nAnd to play devils advocate to your point about Transformer, in the short time that they have been out they're showing better performance than LSTMs. Maybe we should't go all out in writing a highly performant C++ implementation of Transformers, but a well tested standardized python version would be of great help to the NLP community. At least common attention modules should be implemented.", "body": "This is a good point. Transformer as an end to end model might not be mature but in NLP we are seeing a general trend toward utilization of attention. Even if we're not to implement the Transfomer I still believe it is necessary to implement the common attention modules.\r\n\r\nIn general this is a question of how agile PyTorch as a framework will be. If we're too agile we might have a mess on our hands along the lines of tf.contrib, but on the other hand only implementing the very core modules in deep learning will lead to a lot of code duplication or bugs across various projects/codebases. The debate should be where this sweet spot is.\r\n\r\nAnd to play devils advocate to your point about Transformer, in the short time that they have been out they're showing [better performance than LSTMs](https://arxiv.org/abs/1808.04444). Maybe we should't go all out in writing a highly performant C++ implementation of Transformers, but a well tested standardized python version would be of great help to the NLP community. At least common attention modules should be implemented."}