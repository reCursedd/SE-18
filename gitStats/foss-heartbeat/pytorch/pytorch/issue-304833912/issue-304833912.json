{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5742", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5742/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5742/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5742/events", "html_url": "https://github.com/pytorch/pytorch/issues/5742", "id": 304833912, "node_id": "MDU6SXNzdWUzMDQ4MzM5MTI=", "number": 5742, "title": "Why BatchNorm2D has inconsistent gradients and sizes for running_mean and running_variance", "user": {"login": "aSingh071954", "id": 37342925, "node_id": "MDQ6VXNlcjM3MzQyOTI1", "avatar_url": "https://avatars1.githubusercontent.com/u/37342925?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aSingh071954", "html_url": "https://github.com/aSingh071954", "followers_url": "https://api.github.com/users/aSingh071954/followers", "following_url": "https://api.github.com/users/aSingh071954/following{/other_user}", "gists_url": "https://api.github.com/users/aSingh071954/gists{/gist_id}", "starred_url": "https://api.github.com/users/aSingh071954/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aSingh071954/subscriptions", "organizations_url": "https://api.github.com/users/aSingh071954/orgs", "repos_url": "https://api.github.com/users/aSingh071954/repos", "events_url": "https://api.github.com/users/aSingh071954/events{/privacy}", "received_events_url": "https://api.github.com/users/aSingh071954/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-13T16:15:23Z", "updated_at": "2018-03-13T16:16:08Z", "closed_at": "2018-03-13T16:16:07Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I have a question regarding BatchNorm2d, below is the output when i call optimizer.step() to update parameters. I can see that the conv1.weight tensor size is 64 but it should have been 64x64x3x3 (Number of Input Channels = 64)?</p>\n<p>Similarly, why bn1.running_mean size is [64, 64, 3, 3] ?<br>\nAnd why module.bn1.running_var size is [64] ?<br>\nWhy don't we have Gradients computed for running_mean as we have computed for running_var OR why gradients are computed for running_var at all (as it is not learnable parameter) ?</p>\n<p>Below are the outputs I printed for the conv1 and batchnorm layer --&gt;</p>\n<p>module.conv1.weight: 0.5900921821594238<br>\nmodule.conv1.weight.size(): torch.Size([64])<br>\nmodule.conv1.weight.Gradients: 0.8657947182655334</p>\n<p>module.bn1.running_mean: 1.9200005531311035<br>\nmodule.bn1.running_mean.size(): torch.Size([64, 64, 3, 3])</p>\n<p>module.bn1.running_var: 0.6700683832168579<br>\nmodule.bn1.running_var.size(): torch.Size([64])<br>\nmodule.bn1.running_var.Gradients: 2.270807981491089</p>\n<p>I am using the following resent model from --&gt;<br>\n<a href=\"https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\">https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py</a></p>", "body_text": "Hi,\nI have a question regarding BatchNorm2d, below is the output when i call optimizer.step() to update parameters. I can see that the conv1.weight tensor size is 64 but it should have been 64x64x3x3 (Number of Input Channels = 64)?\nSimilarly, why bn1.running_mean size is [64, 64, 3, 3] ?\nAnd why module.bn1.running_var size is [64] ?\nWhy don't we have Gradients computed for running_mean as we have computed for running_var OR why gradients are computed for running_var at all (as it is not learnable parameter) ?\nBelow are the outputs I printed for the conv1 and batchnorm layer -->\nmodule.conv1.weight: 0.5900921821594238\nmodule.conv1.weight.size(): torch.Size([64])\nmodule.conv1.weight.Gradients: 0.8657947182655334\nmodule.bn1.running_mean: 1.9200005531311035\nmodule.bn1.running_mean.size(): torch.Size([64, 64, 3, 3])\nmodule.bn1.running_var: 0.6700683832168579\nmodule.bn1.running_var.size(): torch.Size([64])\nmodule.bn1.running_var.Gradients: 2.270807981491089\nI am using the following resent model from -->\nhttps://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py", "body": "Hi, \r\n\r\nI have a question regarding BatchNorm2d, below is the output when i call optimizer.step() to update parameters. I can see that the conv1.weight tensor size is 64 but it should have been 64x64x3x3 (Number of Input Channels = 64)?\r\n\r\nSimilarly, why bn1.running_mean size is [64, 64, 3, 3] ? \r\nAnd why module.bn1.running_var size is [64] ?\r\nWhy don't we have Gradients computed for running_mean as we have computed for running_var OR why gradients are computed for running_var at all (as it is not learnable parameter) ?\r\n\r\nBelow are the outputs I printed for the conv1 and batchnorm layer -->\r\n\r\nmodule.conv1.weight: 0.5900921821594238\r\nmodule.conv1.weight.size(): torch.Size([64])\r\nmodule.conv1.weight.Gradients: 0.8657947182655334\r\n\r\nmodule.bn1.running_mean: 1.9200005531311035\r\nmodule.bn1.running_mean.size(): torch.Size([64, 64, 3, 3])\r\n\r\nmodule.bn1.running_var: 0.6700683832168579\r\nmodule.bn1.running_var.size(): torch.Size([64])\r\nmodule.bn1.running_var.Gradients: 2.270807981491089\r\n\r\nI am using the following resent model from -->\r\nhttps://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py"}