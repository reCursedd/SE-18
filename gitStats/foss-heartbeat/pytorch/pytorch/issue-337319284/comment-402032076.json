{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/402032076", "html_url": "https://github.com/pytorch/pytorch/issues/9084#issuecomment-402032076", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9084", "id": 402032076, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjAzMjA3Ng==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-03T06:52:57Z", "updated_at": "2018-07-03T06:52:57Z", "author_association": "MEMBER", "body_html": "<p>We are indeed moving more and more of our layers to ATen, and having batch norm entirely in ATen is definitely something that will happen at some point in time.<br>\nBut there are still a few things missing in ATen that would make converting BN to ATen easy. The first thing that comes to my mind is a DeviceTensor, that allows us to index nd tensors in cuda kernels, and is used in BN on the GPU.</p>", "body_text": "We are indeed moving more and more of our layers to ATen, and having batch norm entirely in ATen is definitely something that will happen at some point in time.\nBut there are still a few things missing in ATen that would make converting BN to ATen easy. The first thing that comes to my mind is a DeviceTensor, that allows us to index nd tensors in cuda kernels, and is used in BN on the GPU.", "body": "We are indeed moving more and more of our layers to ATen, and having batch norm entirely in ATen is definitely something that will happen at some point in time.\r\nBut there are still a few things missing in ATen that would make converting BN to ATen easy. The first thing that comes to my mind is a DeviceTensor, that allows us to index nd tensors in cuda kernels, and is used in BN on the GPU. "}