{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11867", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11867/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11867/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11867/events", "html_url": "https://github.com/pytorch/pytorch/issues/11867", "id": 361863052, "node_id": "MDU6SXNzdWUzNjE4NjMwNTI=", "number": 11867, "title": "aten::layer_norm perf is extremely slow compared to Caffe2", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/performance", "name": "performance", "color": "f9d0c4", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-19T18:04:07Z", "updated_at": "2018-09-24T17:53:54Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Running on CPU, it appears that <code>aten::layer_norm</code>, which is implemented in terms of <code>thnn_batch_normalization</code>, is 6.5x slower than the equivalent operator implemented in Caffe2 using eigen. Example benchmark:</p>\n<pre><code>import torch\nimport time\n\ninput = torch.randn(20, 5, 10, 10, dtype=torch.float)\nm = torch.nn.LayerNorm(input.size()[1:])\nm.eval()\n\nNITER = 10000\n\ns = time.time()\nfor i in range(NITER):\n   o = m(input)\nt_pytorch = time.time() - s\nprint('pytorch time per iter', t_pytorch / NITER)\n\nfrom caffe2.python import core, workspace\n\ntest_net = core.Net(\"layer_norm_test\")\ntest_net.LayerNorm([\"input\"], [\"output\", \"mean\", \"stddev\"], epsilon=1e-5)\n\nimport numpy as np\nworkspace.FeedBlob('input', np.random.rand(20, 5, 10, 10).astype('f'))\nworkspace.CreateNet(test_net)\ns = time.time()\nworkspace.RunNet(test_net.Name(), NITER)\nt_caffe2 = time.time() - s\nprint('caffe2 time per iter', t_caffe2 / NITER)\n\nprint('pytorch / caffe2', t_pytorch / t_caffe2)\n</code></pre>\n<pre><code>pytorch time per iter 0.00012711503505706786\ncaffe2 time per iter 1.960470676422119e-05\npytorch / caffe2 6.48390392092241\n</code></pre>\n<p>Machine info:</p>\n<pre><code>!lscpu\n</code></pre>\n<pre><code>Architecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              56\nOn-line CPU(s) list: 0-55\nThread(s) per core:  2\nCore(s) per socket:  14\nSocket(s):           2\nNUMA node(s):        2\nVendor ID:           GenuineIntel\nCPU family:          6\nModel:               79\nModel name:          Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz\nStepping:            1\nCPU MHz:             2401.000\nCPU max MHz:         2401.0000\nCPU min MHz:         1200.0000\nBogoMIPS:            4788.85\nVirtualization:      VT-x\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            256K\nL3 cache:            35840K\nNUMA node0 CPU(s):   0-13,28-41\nNUMA node1 CPU(s):   14-27,42-55\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\n</code></pre>\n<p>The hot path in the THNN op seems to be this <code>set1d</code> call right here: <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L56\">https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L56</a></p>\n<p>If we dig into the caffe2 implementation, we find that for GPU there are nontrivial states persisted across invocations of the op: <a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/operators/layer_norm_op.h#L56\">https://github.com/pytorch/pytorch/blob/master/caffe2/operators/layer_norm_op.h#L56</a>. The motivation behind this is that instantiating a caffe2 tensor grabs a global lock, so if you do that in the <code>Run</code> method of an operator, you end up serializing your network's execution.</p>\n<p>We should investigate what the proper path for op unification is, and see if we can get away with using the caching allocator for these GPU states</p>", "body_text": "Running on CPU, it appears that aten::layer_norm, which is implemented in terms of thnn_batch_normalization, is 6.5x slower than the equivalent operator implemented in Caffe2 using eigen. Example benchmark:\nimport torch\nimport time\n\ninput = torch.randn(20, 5, 10, 10, dtype=torch.float)\nm = torch.nn.LayerNorm(input.size()[1:])\nm.eval()\n\nNITER = 10000\n\ns = time.time()\nfor i in range(NITER):\n   o = m(input)\nt_pytorch = time.time() - s\nprint('pytorch time per iter', t_pytorch / NITER)\n\nfrom caffe2.python import core, workspace\n\ntest_net = core.Net(\"layer_norm_test\")\ntest_net.LayerNorm([\"input\"], [\"output\", \"mean\", \"stddev\"], epsilon=1e-5)\n\nimport numpy as np\nworkspace.FeedBlob('input', np.random.rand(20, 5, 10, 10).astype('f'))\nworkspace.CreateNet(test_net)\ns = time.time()\nworkspace.RunNet(test_net.Name(), NITER)\nt_caffe2 = time.time() - s\nprint('caffe2 time per iter', t_caffe2 / NITER)\n\nprint('pytorch / caffe2', t_pytorch / t_caffe2)\n\npytorch time per iter 0.00012711503505706786\ncaffe2 time per iter 1.960470676422119e-05\npytorch / caffe2 6.48390392092241\n\nMachine info:\n!lscpu\n\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              56\nOn-line CPU(s) list: 0-55\nThread(s) per core:  2\nCore(s) per socket:  14\nSocket(s):           2\nNUMA node(s):        2\nVendor ID:           GenuineIntel\nCPU family:          6\nModel:               79\nModel name:          Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz\nStepping:            1\nCPU MHz:             2401.000\nCPU max MHz:         2401.0000\nCPU min MHz:         1200.0000\nBogoMIPS:            4788.85\nVirtualization:      VT-x\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            256K\nL3 cache:            35840K\nNUMA node0 CPU(s):   0-13,28-41\nNUMA node1 CPU(s):   14-27,42-55\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\n\nThe hot path in the THNN op seems to be this set1d call right here: https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L56\nIf we dig into the caffe2 implementation, we find that for GPU there are nontrivial states persisted across invocations of the op: https://github.com/pytorch/pytorch/blob/master/caffe2/operators/layer_norm_op.h#L56. The motivation behind this is that instantiating a caffe2 tensor grabs a global lock, so if you do that in the Run method of an operator, you end up serializing your network's execution.\nWe should investigate what the proper path for op unification is, and see if we can get away with using the caching allocator for these GPU states", "body": "Running on CPU, it appears that `aten::layer_norm`, which is implemented in terms of `thnn_batch_normalization`, is 6.5x slower than the equivalent operator implemented in Caffe2 using eigen. Example benchmark:\r\n\r\n```\r\nimport torch\r\nimport time\r\n\r\ninput = torch.randn(20, 5, 10, 10, dtype=torch.float)\r\nm = torch.nn.LayerNorm(input.size()[1:])\r\nm.eval()\r\n\r\nNITER = 10000\r\n\r\ns = time.time()\r\nfor i in range(NITER):\r\n   o = m(input)\r\nt_pytorch = time.time() - s\r\nprint('pytorch time per iter', t_pytorch / NITER)\r\n\r\nfrom caffe2.python import core, workspace\r\n\r\ntest_net = core.Net(\"layer_norm_test\")\r\ntest_net.LayerNorm([\"input\"], [\"output\", \"mean\", \"stddev\"], epsilon=1e-5)\r\n\r\nimport numpy as np\r\nworkspace.FeedBlob('input', np.random.rand(20, 5, 10, 10).astype('f'))\r\nworkspace.CreateNet(test_net)\r\ns = time.time()\r\nworkspace.RunNet(test_net.Name(), NITER)\r\nt_caffe2 = time.time() - s\r\nprint('caffe2 time per iter', t_caffe2 / NITER)\r\n\r\nprint('pytorch / caffe2', t_pytorch / t_caffe2)\r\n```\r\n\r\n```\r\npytorch time per iter 0.00012711503505706786\r\ncaffe2 time per iter 1.960470676422119e-05\r\npytorch / caffe2 6.48390392092241\r\n```\r\n\r\nMachine info:\r\n```\r\n!lscpu\r\n```\r\n\r\n```\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              56\r\nOn-line CPU(s) list: 0-55\r\nThread(s) per core:  2\r\nCore(s) per socket:  14\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               79\r\nModel name:          Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz\r\nStepping:            1\r\nCPU MHz:             2401.000\r\nCPU max MHz:         2401.0000\r\nCPU min MHz:         1200.0000\r\nBogoMIPS:            4788.85\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            256K\r\nL3 cache:            35840K\r\nNUMA node0 CPU(s):   0-13,28-41\r\nNUMA node1 CPU(s):   14-27,42-55\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\r\n```\r\n\r\nThe hot path in the THNN op seems to be this `set1d` call right here: https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L56\r\n\r\nIf we dig into the caffe2 implementation, we find that for GPU there are nontrivial states persisted across invocations of the op: https://github.com/pytorch/pytorch/blob/master/caffe2/operators/layer_norm_op.h#L56. The motivation behind this is that instantiating a caffe2 tensor grabs a global lock, so if you do that in the `Run` method of an operator, you end up serializing your network's execution.\r\n\r\nWe should investigate what the proper path for op unification is, and see if we can get away with using the caching allocator for these GPU states"}