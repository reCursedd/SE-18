{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390341812", "html_url": "https://github.com/pytorch/pytorch/pull/7672#issuecomment-390341812", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7672", "id": 390341812, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDM0MTgxMg==", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-18T21:49:10Z", "updated_at": "2018-05-18T21:49:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Can't get this working on devgpu (I think because the gcc version there doesn't support std::regex well) but I played around with the trivial example from that one blog post where we had issues with overheads from loop conditions:</p>\n<pre><code>    def test_loop_addone(self):\n        @torch.jit.script\n        def fn(x):\n            one = 1.0\n            for i in range(100):\n                x += one\n            return x\n\n        print(fn.graph)\n\n        import time\n        s = time.time()\n        for _ in range(1000):\n            fn(torch.zeros(4, 4, dtype=torch.float))\n        loop = time.time() - s\n        print('loop', loop, 'seconds')\n\n        self.run_pass('loop_unrolling', fn.graph)\n\n        print(fn.graph)\n\n        s = time.time()\n        for _ in range(1000):\n            fn(torch.zeros(4, 4, dtype=torch.float))\n        unrolled = time.time() - s\n        print('unrolled', unrolled, 'seconds')\n\n        print('unrolled/loop', unrolled/loop * 100, '%')\n</code></pre>\n<pre><code>graph(%x.1 : Dynamic) {\n  %1 : Long() = prim::Constant[value={100}]()\n  %2 : Byte() = prim::Constant[value={1}]()\n  %x : Dynamic = prim::Loop(%1, %2, %x.1)\n    block0(%i : Dynamic, %4 : Dynamic) {\n      %5 : Float() = prim::Constant[value={1}]()\n      %x.2 : Dynamic = aten::add[alpha={1}](%4, %5)\n      %7 : Byte() = prim::Constant[value={1}]()\n      -&gt; (%7, %x.2)\n    }\n  return (%x);\n}\n\nloop 1.6148180961608887 seconds\ngraph(%x.1 : Dynamic) {\n  %1 : Long() = prim::Constant[value={100}]()\n  %2 : Byte() = prim::Constant[value={1}]()\n  %36 : Long() = aten::div[other={8}](%1)\n  %37 : Dynamic = aten::mul[other={8}](%36)\n  %38 : Long() = aten::sub[alpha={1}](%1, %37)\n  %x.3 : Dynamic = prim::Loop(%36, %2, %x.1)\n    block0(%i.1 : Dynamic, %4 : Dynamic) {\n      %5 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%4, %5)\n      %15 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1.1.1.1, %15)\n      %18 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1, %18)\n      %21 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1, %21)\n      %24 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1, %24)\n      %27 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1, %27)\n      %30 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1, %30)\n      %33 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1.1, %33)\n      %35 : Byte() = prim::Constant[value={1}]()\n      -&gt; (%35, %x.2.1.1.1.1.1.1.1.1)\n    }\n  %x : Dynamic = prim::Loop(%38, %2, %x.3)\n    block0(%i : Dynamic, %11 : Dynamic) {\n      %12 : Float() = prim::Constant[value={1}]()\n      %x.2.1 : Dynamic = aten::add[alpha={1}](%11, %12)\n      %14 : Byte() = prim::Constant[value={1}]()\n      -&gt; (%14, %x.2.1)\n    }\n  return (%x);\n}\n\nunrolled 1.5762600898742676 seconds\nunrolled/loop 97.61223840763923 %\n</code></pre>\n<p>Basically no difference between unrolled and not. But then I tried reordering fusion to be after unrolling in the GraphExecutor:</p>\n<pre><code>loop 1.6108458042144775 seconds\nunrolled 1.2125589847564697 seconds\nunrolled/loop 75.27467753797634 %\n</code></pre>\n<p>wowowow! Dumb benchmark but it's fun to look at :)</p>", "body_text": "Can't get this working on devgpu (I think because the gcc version there doesn't support std::regex well) but I played around with the trivial example from that one blog post where we had issues with overheads from loop conditions:\n    def test_loop_addone(self):\n        @torch.jit.script\n        def fn(x):\n            one = 1.0\n            for i in range(100):\n                x += one\n            return x\n\n        print(fn.graph)\n\n        import time\n        s = time.time()\n        for _ in range(1000):\n            fn(torch.zeros(4, 4, dtype=torch.float))\n        loop = time.time() - s\n        print('loop', loop, 'seconds')\n\n        self.run_pass('loop_unrolling', fn.graph)\n\n        print(fn.graph)\n\n        s = time.time()\n        for _ in range(1000):\n            fn(torch.zeros(4, 4, dtype=torch.float))\n        unrolled = time.time() - s\n        print('unrolled', unrolled, 'seconds')\n\n        print('unrolled/loop', unrolled/loop * 100, '%')\n\ngraph(%x.1 : Dynamic) {\n  %1 : Long() = prim::Constant[value={100}]()\n  %2 : Byte() = prim::Constant[value={1}]()\n  %x : Dynamic = prim::Loop(%1, %2, %x.1)\n    block0(%i : Dynamic, %4 : Dynamic) {\n      %5 : Float() = prim::Constant[value={1}]()\n      %x.2 : Dynamic = aten::add[alpha={1}](%4, %5)\n      %7 : Byte() = prim::Constant[value={1}]()\n      -> (%7, %x.2)\n    }\n  return (%x);\n}\n\nloop 1.6148180961608887 seconds\ngraph(%x.1 : Dynamic) {\n  %1 : Long() = prim::Constant[value={100}]()\n  %2 : Byte() = prim::Constant[value={1}]()\n  %36 : Long() = aten::div[other={8}](%1)\n  %37 : Dynamic = aten::mul[other={8}](%36)\n  %38 : Long() = aten::sub[alpha={1}](%1, %37)\n  %x.3 : Dynamic = prim::Loop(%36, %2, %x.1)\n    block0(%i.1 : Dynamic, %4 : Dynamic) {\n      %5 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%4, %5)\n      %15 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1.1.1.1, %15)\n      %18 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1, %18)\n      %21 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1, %21)\n      %24 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1, %24)\n      %27 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1, %27)\n      %30 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1, %30)\n      %33 : Float() = prim::Constant[value={1}]()\n      %x.2.1.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1.1, %33)\n      %35 : Byte() = prim::Constant[value={1}]()\n      -> (%35, %x.2.1.1.1.1.1.1.1.1)\n    }\n  %x : Dynamic = prim::Loop(%38, %2, %x.3)\n    block0(%i : Dynamic, %11 : Dynamic) {\n      %12 : Float() = prim::Constant[value={1}]()\n      %x.2.1 : Dynamic = aten::add[alpha={1}](%11, %12)\n      %14 : Byte() = prim::Constant[value={1}]()\n      -> (%14, %x.2.1)\n    }\n  return (%x);\n}\n\nunrolled 1.5762600898742676 seconds\nunrolled/loop 97.61223840763923 %\n\nBasically no difference between unrolled and not. But then I tried reordering fusion to be after unrolling in the GraphExecutor:\nloop 1.6108458042144775 seconds\nunrolled 1.2125589847564697 seconds\nunrolled/loop 75.27467753797634 %\n\nwowowow! Dumb benchmark but it's fun to look at :)", "body": "Can't get this working on devgpu (I think because the gcc version there doesn't support std::regex well) but I played around with the trivial example from that one blog post where we had issues with overheads from loop conditions:\r\n\r\n```\r\n    def test_loop_addone(self):\r\n        @torch.jit.script\r\n        def fn(x):\r\n            one = 1.0\r\n            for i in range(100):\r\n                x += one\r\n            return x\r\n\r\n        print(fn.graph)\r\n\r\n        import time\r\n        s = time.time()\r\n        for _ in range(1000):\r\n            fn(torch.zeros(4, 4, dtype=torch.float))\r\n        loop = time.time() - s\r\n        print('loop', loop, 'seconds')\r\n\r\n        self.run_pass('loop_unrolling', fn.graph)\r\n\r\n        print(fn.graph)\r\n\r\n        s = time.time()\r\n        for _ in range(1000):\r\n            fn(torch.zeros(4, 4, dtype=torch.float))\r\n        unrolled = time.time() - s\r\n        print('unrolled', unrolled, 'seconds')\r\n\r\n        print('unrolled/loop', unrolled/loop * 100, '%')\r\n```\r\n\r\n```\r\ngraph(%x.1 : Dynamic) {\r\n  %1 : Long() = prim::Constant[value={100}]()\r\n  %2 : Byte() = prim::Constant[value={1}]()\r\n  %x : Dynamic = prim::Loop(%1, %2, %x.1)\r\n    block0(%i : Dynamic, %4 : Dynamic) {\r\n      %5 : Float() = prim::Constant[value={1}]()\r\n      %x.2 : Dynamic = aten::add[alpha={1}](%4, %5)\r\n      %7 : Byte() = prim::Constant[value={1}]()\r\n      -> (%7, %x.2)\r\n    }\r\n  return (%x);\r\n}\r\n\r\nloop 1.6148180961608887 seconds\r\ngraph(%x.1 : Dynamic) {\r\n  %1 : Long() = prim::Constant[value={100}]()\r\n  %2 : Byte() = prim::Constant[value={1}]()\r\n  %36 : Long() = aten::div[other={8}](%1)\r\n  %37 : Dynamic = aten::mul[other={8}](%36)\r\n  %38 : Long() = aten::sub[alpha={1}](%1, %37)\r\n  %x.3 : Dynamic = prim::Loop(%36, %2, %x.1)\r\n    block0(%i.1 : Dynamic, %4 : Dynamic) {\r\n      %5 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%4, %5)\r\n      %15 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1.1.1.1, %15)\r\n      %18 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1, %18)\r\n      %21 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1, %21)\r\n      %24 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1, %24)\r\n      %27 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1, %27)\r\n      %30 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1, %30)\r\n      %33 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1.1.1.1.1.1.1.1 : Dynamic = aten::add[alpha={1}](%x.2.1.1.1.1.1.1.1, %33)\r\n      %35 : Byte() = prim::Constant[value={1}]()\r\n      -> (%35, %x.2.1.1.1.1.1.1.1.1)\r\n    }\r\n  %x : Dynamic = prim::Loop(%38, %2, %x.3)\r\n    block0(%i : Dynamic, %11 : Dynamic) {\r\n      %12 : Float() = prim::Constant[value={1}]()\r\n      %x.2.1 : Dynamic = aten::add[alpha={1}](%11, %12)\r\n      %14 : Byte() = prim::Constant[value={1}]()\r\n      -> (%14, %x.2.1)\r\n    }\r\n  return (%x);\r\n}\r\n\r\nunrolled 1.5762600898742676 seconds\r\nunrolled/loop 97.61223840763923 %\r\n```\r\n\r\nBasically no difference between unrolled and not. But then I tried reordering fusion to be after unrolling in the GraphExecutor:\r\n\r\n```\r\nloop 1.6108458042144775 seconds\r\nunrolled 1.2125589847564697 seconds\r\nunrolled/loop 75.27467753797634 %\r\n```\r\n\r\nwowowow! Dumb benchmark but it's fun to look at :)"}