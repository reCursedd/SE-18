{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5985", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5985/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5985/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5985/events", "html_url": "https://github.com/pytorch/pytorch/issues/5985", "id": 308301518, "node_id": "MDU6SXNzdWUzMDgzMDE1MTg=", "number": 5985, "title": "RNN `dropout` arg needs better documentation", "user": {"login": "flauted", "id": 29395172, "node_id": "MDQ6VXNlcjI5Mzk1MTcy", "avatar_url": "https://avatars2.githubusercontent.com/u/29395172?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flauted", "html_url": "https://github.com/flauted", "followers_url": "https://api.github.com/users/flauted/followers", "following_url": "https://api.github.com/users/flauted/following{/other_user}", "gists_url": "https://api.github.com/users/flauted/gists{/gist_id}", "starred_url": "https://api.github.com/users/flauted/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flauted/subscriptions", "organizations_url": "https://api.github.com/users/flauted/orgs", "repos_url": "https://api.github.com/users/flauted/repos", "events_url": "https://api.github.com/users/flauted/events{/privacy}", "received_events_url": "https://api.github.com/users/flauted/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-03-24T21:46:03Z", "updated_at": "2018-03-29T20:44:28Z", "closed_at": "2018-03-29T20:44:28Z", "author_association": "NONE", "body_html": "<p>The documentation for the argument <code>dropout</code> of the LSTM module <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.LSTM\" rel=\"nofollow\">here</a> confused me. Here's the quote:</p>\n<blockquote>\n<p>If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer</p>\n</blockquote>\n<p>GRU uses the same documentation. Initially I assumed that meant supplying a value other than <code>False</code> will implement a dropout layer with a default dropout probability.</p>\n<p>Today I dug through the source code. I went through LSTM, RNN base, Module, THNN backends, Function backends, functions RNN, functions Autograd RNN, and finally <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L68\">functions Stacked RNN</a> before I figured out that the argument is a float probability and not a boolean.</p>\n<p>I did some searches on the forums. I'm not the first person to become confused (Exhibit <a href=\"https://discuss.pytorch.org/t/lstm-dropout-clarification-of-last-layer/5588\" rel=\"nofollow\">A</a>, <a href=\"https://discuss.pytorch.org/t/dropout-in-lstm/7784\" rel=\"nofollow\">B</a>, <a href=\"https://discuss.pytorch.org/t/dropout-for-rnns/633\" rel=\"nofollow\">C</a>). It wasn't until I read through A that I figured out by \"layer\" it didn't mean the last unwrapped layer but rather the last stacked layer. That is to say, using the arg when <code>num_layers</code> is 1 actually did nothing. Possibly worthy of a warning...</p>\n<p>I opened a topic on the forums <a href=\"https://discuss.pytorch.org/t/does-rnn-dropout-arg-need-better-documentation/15407\" rel=\"nofollow\">here</a>. Didn't gain traction, but the one reply at this time suggested an issue or PR.</p>\n<p><strong>Tl;dr: The docs for <code>nn.LSTM</code> and <code>nn.GRU</code> should say the arg is a float in [0, 1], should clarify what's meant by layer, and maybe the source should issue a warning when <code>dropout</code> is not 0 and <code>num_layers</code> is 1.</strong></p>", "body_text": "The documentation for the argument dropout of the LSTM module here confused me. Here's the quote:\n\nIf non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n\nGRU uses the same documentation. Initially I assumed that meant supplying a value other than False will implement a dropout layer with a default dropout probability.\nToday I dug through the source code. I went through LSTM, RNN base, Module, THNN backends, Function backends, functions RNN, functions Autograd RNN, and finally functions Stacked RNN before I figured out that the argument is a float probability and not a boolean.\nI did some searches on the forums. I'm not the first person to become confused (Exhibit A, B, C). It wasn't until I read through A that I figured out by \"layer\" it didn't mean the last unwrapped layer but rather the last stacked layer. That is to say, using the arg when num_layers is 1 actually did nothing. Possibly worthy of a warning...\nI opened a topic on the forums here. Didn't gain traction, but the one reply at this time suggested an issue or PR.\nTl;dr: The docs for nn.LSTM and nn.GRU should say the arg is a float in [0, 1], should clarify what's meant by layer, and maybe the source should issue a warning when dropout is not 0 and num_layers is 1.", "body": "The documentation for the argument ``dropout`` of the LSTM module [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM) confused me. Here's the quote:\r\n\r\n>If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\r\n\r\nGRU uses the same documentation. Initially I assumed that meant supplying a value other than `False` will implement a dropout layer with a default dropout probability. \r\n\r\nToday I dug through the source code. I went through LSTM, RNN base, Module, THNN backends, Function backends, functions RNN, functions Autograd RNN, and finally [functions Stacked RNN](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L68) before I figured out that the argument is a float probability and not a boolean.\r\n\r\nI did some searches on the forums. I'm not the first person to become confused (Exhibit [A](https://discuss.pytorch.org/t/lstm-dropout-clarification-of-last-layer/5588), [B](https://discuss.pytorch.org/t/dropout-in-lstm/7784), [C](https://discuss.pytorch.org/t/dropout-for-rnns/633)). It wasn't until I read through A that I figured out by \"layer\" it didn't mean the last unwrapped layer but rather the last stacked layer. That is to say, using the arg when `num_layers` is 1 actually did nothing. Possibly worthy of a warning...\r\n\r\nI opened a topic on the forums [here](https://discuss.pytorch.org/t/does-rnn-dropout-arg-need-better-documentation/15407). Didn't gain traction, but the one reply at this time suggested an issue or PR.\r\n\r\n**Tl;dr: The docs for `nn.LSTM` and `nn.GRU` should say the arg is a float in [0, 1], should clarify what's meant by layer, and maybe the source should issue a warning when `dropout` is not 0 and `num_layers` is 1.**"}