{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12701", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12701/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12701/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12701/events", "html_url": "https://github.com/pytorch/pytorch/issues/12701", "id": 370478095, "node_id": "MDU6SXNzdWUzNzA0NzgwOTU=", "number": 12701, "title": "Fail to mix nn.BatchNorm with user defined BatchNorm", "user": {"login": "XingangPan", "id": 13579537, "node_id": "MDQ6VXNlcjEzNTc5NTM3", "avatar_url": "https://avatars1.githubusercontent.com/u/13579537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XingangPan", "html_url": "https://github.com/XingangPan", "followers_url": "https://api.github.com/users/XingangPan/followers", "following_url": "https://api.github.com/users/XingangPan/following{/other_user}", "gists_url": "https://api.github.com/users/XingangPan/gists{/gist_id}", "starred_url": "https://api.github.com/users/XingangPan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XingangPan/subscriptions", "organizations_url": "https://api.github.com/users/XingangPan/orgs", "repos_url": "https://api.github.com/users/XingangPan/repos", "events_url": "https://api.github.com/users/XingangPan/events{/privacy}", "received_events_url": "https://api.github.com/users/XingangPan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-10-16T07:39:11Z", "updated_at": "2018-10-18T03:27:40Z", "closed_at": "2018-10-16T16:31:59Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji>Fail to mix nn.BatchNorm with user defined BatchNorm</h2>\n<p>When using nn.BatchNorm2d and a user defined BatchNorm2d in a module, the following error occurs during backpropagation:<br>\n\"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"<br>\nThis might be caused by the inplace update of \"running_mean\" and \"running_var\" in the user defined BatchNorm. However, these two Tensors has to be updated via inplace operation.<br>\nStrangely, the above error does not occur if there is no nn.BatchNorm2d or if nn.DataParallel is not used.</p>\n<h2>To Reproduce</h2>\n<p>This is a simple code to reproduce the error:</p>\n<div class=\"highlight highlight-source-shell\"><pre>import torch\nimport torch.nn as nn\n\nclass BatchNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n        super(BatchNorm2d, <span class=\"pl-en\">self).__init__</span>()\n        self.eps = eps\n        self.momentum = momentum\n        self.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>running_mean<span class=\"pl-pds\">'</span></span>, torch.zeros(num_features, 1))\n        self.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>running_var<span class=\"pl-pds\">'</span></span>, torch.ones(num_features, 1))\n\n    def forward(self, x):\n        N, C, H, W = <span class=\"pl-en\">x.size</span>()\n        x = torch.transpose(x, 0, <span class=\"pl-en\">1).contiguous</span>()\n        x = x.view(C, N <span class=\"pl-k\">*</span> H <span class=\"pl-k\">*</span> W)\n        <span class=\"pl-k\">if</span> self.training:\n            mean_b = x.mean(1, keepdim=True)\n            var_b = x.var(1, keepdim=True)\n            self.running_mean.mul_(self.momentum)\n            self.running_mean.add_<span class=\"pl-s\"><span class=\"pl-pds\">((</span><span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> self.momentum) <span class=\"pl-k\">*</span> mean_b.detach(<span class=\"pl-pds\">))</span></span>\n            self.running_var.mul_(self.momentum)\n            self.running_var.add_<span class=\"pl-s\"><span class=\"pl-pds\">((</span><span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> self.momentum) <span class=\"pl-k\">*</span> var_b.detach(<span class=\"pl-pds\">))</span></span>\n        else:\n            mean_b = self.running_mean\n            var_b = self.running_var\n        x_bn = (x - mean_b) / (var_b + <span class=\"pl-en\">self.eps).sqrt</span>()\n        out = x_bn.view(C, N, H, W)\n        <span class=\"pl-k\">return</span> torch.transpose(out, 0, <span class=\"pl-en\">1).contiguous</span>()\n\nmodel = nn.Sequential(\n    nn.BatchNorm2d(4),\n    BatchNorm2d(4))\nmodel = <span class=\"pl-en\">torch.nn.DataParallel(model).cuda</span>()\n\ninput = <span class=\"pl-en\">torch.rand(16,4,4,4).cuda</span>()\nlabel = <span class=\"pl-en\">torch.rand(16,4,4,4).cuda</span>()\noutput = model(input)\ncriterion = <span class=\"pl-en\">nn.L1Loss</span>()\nloss = criterion(output, label)\n<span class=\"pl-en\">loss.backward</span>()</pre></div>\n<h2>Expected behavior</h2>\n<p>Hope that this error could be fixed.</p>\n<h2>Environment</h2>\n<p>PyTorch version: 0.4.0a0+1ab248d<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: CentOS Linux 7 (Core)<br>\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)<br>\nCMake version: version 2.8.12.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.61<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1080 Ti<br>\nGPU 1: GeForce GTX 1080 Ti<br>\nGPU 2: GeForce GTX 1080 Ti<br>\nGPU 3: GeForce GTX 1080 Ti</p>\n<p>Nvidia driver version: 384.98<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.3)<br>\n[pip] torch (0.4.0a0+1ab248d)<br>\n[pip] torchvision (0.2.0)<br>\n[conda] pytorch                   0.4.0a0+1ab248d py36_cuda8.0.61_cudnn7.0.5_0    ostrokach-forge<br>\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch</p>", "body_text": "\ud83d\udc1bFail to mix nn.BatchNorm with user defined BatchNorm\nWhen using nn.BatchNorm2d and a user defined BatchNorm2d in a module, the following error occurs during backpropagation:\n\"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"\nThis might be caused by the inplace update of \"running_mean\" and \"running_var\" in the user defined BatchNorm. However, these two Tensors has to be updated via inplace operation.\nStrangely, the above error does not occur if there is no nn.BatchNorm2d or if nn.DataParallel is not used.\nTo Reproduce\nThis is a simple code to reproduce the error:\nimport torch\nimport torch.nn as nn\n\nclass BatchNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n        super(BatchNorm2d, self).__init__()\n        self.eps = eps\n        self.momentum = momentum\n        self.register_buffer('running_mean', torch.zeros(num_features, 1))\n        self.register_buffer('running_var', torch.ones(num_features, 1))\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = torch.transpose(x, 0, 1).contiguous()\n        x = x.view(C, N * H * W)\n        if self.training:\n            mean_b = x.mean(1, keepdim=True)\n            var_b = x.var(1, keepdim=True)\n            self.running_mean.mul_(self.momentum)\n            self.running_mean.add_((1 - self.momentum) * mean_b.detach())\n            self.running_var.mul_(self.momentum)\n            self.running_var.add_((1 - self.momentum) * var_b.detach())\n        else:\n            mean_b = self.running_mean\n            var_b = self.running_var\n        x_bn = (x - mean_b) / (var_b + self.eps).sqrt()\n        out = x_bn.view(C, N, H, W)\n        return torch.transpose(out, 0, 1).contiguous()\n\nmodel = nn.Sequential(\n    nn.BatchNorm2d(4),\n    BatchNorm2d(4))\nmodel = torch.nn.DataParallel(model).cuda()\n\ninput = torch.rand(16,4,4,4).cuda()\nlabel = torch.rand(16,4,4,4).cuda()\noutput = model(input)\ncriterion = nn.L1Loss()\nloss = criterion(output, label)\nloss.backward()\nExpected behavior\nHope that this error could be fixed.\nEnvironment\nPyTorch version: 0.4.0a0+1ab248d\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\nCMake version: version 2.8.12.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\nNvidia driver version: 384.98\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] torch (0.4.0a0+1ab248d)\n[pip] torchvision (0.2.0)\n[conda] pytorch                   0.4.0a0+1ab248d py36_cuda8.0.61_cudnn7.0.5_0    ostrokach-forge\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch", "body": "## \ud83d\udc1bFail to mix nn.BatchNorm with user defined BatchNorm\r\n\r\nWhen using nn.BatchNorm2d and a user defined BatchNorm2d in a module, the following error occurs during backpropagation: \r\n\"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"\r\nThis might be caused by the inplace update of \"running_mean\" and \"running_var\" in the user defined BatchNorm. However, these two Tensors has to be updated via inplace operation.\r\nStrangely, the above error does not occur if there is no nn.BatchNorm2d or if nn.DataParallel is not used.\r\n\r\n## To Reproduce\r\n\r\nThis is a simple code to reproduce the error:\r\n\r\n```Shell\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass BatchNorm2d(nn.Module):\r\n    def __init__(self, num_features, eps=1e-5, momentum=0.9):\r\n        super(BatchNorm2d, self).__init__()\r\n        self.eps = eps\r\n        self.momentum = momentum\r\n        self.register_buffer('running_mean', torch.zeros(num_features, 1))\r\n        self.register_buffer('running_var', torch.ones(num_features, 1))\r\n\r\n    def forward(self, x):\r\n        N, C, H, W = x.size()\r\n        x = torch.transpose(x, 0, 1).contiguous()\r\n        x = x.view(C, N * H * W)\r\n        if self.training:\r\n            mean_b = x.mean(1, keepdim=True)\r\n            var_b = x.var(1, keepdim=True)\r\n            self.running_mean.mul_(self.momentum)\r\n            self.running_mean.add_((1 - self.momentum) * mean_b.detach())\r\n            self.running_var.mul_(self.momentum)\r\n            self.running_var.add_((1 - self.momentum) * var_b.detach())\r\n        else:\r\n            mean_b = self.running_mean\r\n            var_b = self.running_var\r\n        x_bn = (x - mean_b) / (var_b + self.eps).sqrt()\r\n        out = x_bn.view(C, N, H, W)\r\n        return torch.transpose(out, 0, 1).contiguous()\r\n\r\nmodel = nn.Sequential(\r\n    nn.BatchNorm2d(4),\r\n    BatchNorm2d(4))\r\nmodel = torch.nn.DataParallel(model).cuda()\r\n\r\ninput = torch.rand(16,4,4,4).cuda()\r\nlabel = torch.rand(16,4,4,4).cuda()\r\noutput = model(input)\r\ncriterion = nn.L1Loss()\r\nloss = criterion(output, label)\r\nloss.backward()\r\n```\r\n\r\n## Expected behavior\r\n\r\nHope that this error could be fixed.\r\n\r\n## Environment\r\n\r\nPyTorch version: 0.4.0a0+1ab248d\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.98\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] torch (0.4.0a0+1ab248d)\r\n[pip] torchvision (0.2.0)\r\n[conda] pytorch                   0.4.0a0+1ab248d py36_cuda8.0.61_cudnn7.0.5_0    ostrokach-forge\r\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch\r\n"}