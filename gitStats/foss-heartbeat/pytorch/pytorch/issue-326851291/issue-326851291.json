{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7886", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7886/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7886/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7886/events", "html_url": "https://github.com/pytorch/pytorch/pull/7886", "id": 326851291, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkwODA3MTYw", "number": 7886, "title": "Fix seeding random module in DataLoader", "user": {"login": "thuyen", "id": 4015328, "node_id": "MDQ6VXNlcjQwMTUzMjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4015328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thuyen", "html_url": "https://github.com/thuyen", "followers_url": "https://api.github.com/users/thuyen/followers", "following_url": "https://api.github.com/users/thuyen/following{/other_user}", "gists_url": "https://api.github.com/users/thuyen/gists{/gist_id}", "starred_url": "https://api.github.com/users/thuyen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thuyen/subscriptions", "organizations_url": "https://api.github.com/users/thuyen/orgs", "repos_url": "https://api.github.com/users/thuyen/repos", "events_url": "https://api.github.com/users/thuyen/events{/privacy}", "received_events_url": "https://api.github.com/users/thuyen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-05-27T21:14:52Z", "updated_at": "2018-11-23T15:44:40Z", "closed_at": "2018-05-29T19:55:05Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7886", "html_url": "https://github.com/pytorch/pytorch/pull/7886", "diff_url": "https://github.com/pytorch/pytorch/pull/7886.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7886.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #7882.\">Fix</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"326796108\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7882\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7882/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7882\">#7882</a></p>\n<p>The  <code>random</code> module is seeded <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L86\">here</a>. The problem with that line is that <code>seed</code> is not an <code>int</code> but a <code>Tensor</code>. Python then will use <code>hash(seed)</code>  to seed the random module (<a href=\"https://docs.python.org/2/library/random.html#random.seed\" rel=\"nofollow\">docs</a>). Without a hash function for <code>Tensor</code>, the actually seed will be the address of the tensor. And it changes every time!</p>\n<p>The line should be changed to: <code>random.seed(int(seed))</code></p>\n<p>Test on fllowing script from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"326796108\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7882\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7882/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7882\">#7882</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> random\n\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> Dataset, DataLoader\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Data</span>(<span class=\"pl-e\">Dataset</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">10000</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">index</span>):\n        <span class=\"pl-c1\">print</span>(index, torch.rand(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>).sum().item(), random.uniform(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>))\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">1</span>\n\nseed <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2018</span>\ntorch.manual_seed(seed)\nloader <span class=\"pl-k\">=</span> DataLoader(Data(), <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> loader:\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">*</span><span class=\"pl-c1\">10</span>)\n    <span class=\"pl-k\">break</span></pre></div>\n<h2>Before</h2>\n<p>First run</p>\n<pre><code>4717 2.202341079711914 0.9952153654478976\n4607 2.3166141510009766 0.6813692345925851\n4194 1.9806793928146362 0.6281118075687344\n2595 2.95841383934021 0.8414756141240453\n4691 0.9809015393257141 0.7622458327788627\n9868 2.521920680999756 0.5253262288522356\n7367 2.333574056625366 0.35079311205192487\n9490 3.02830171585083 0.16235006783937567\n----------\n6759 3.1252167224884033 0.4424384676992986\n</code></pre>\n<p>Next run</p>\n<pre><code>4607 2.3166141510009766 0.15198273935290807\n4194 1.9806793928146362 0.36414129463658884\n4691 0.9809015393257141 0.027569260048619926\n4717 2.202341079711914 0.5512619092026773\n7367 2.333574056625366 0.7932627754589792\n9490 3.02830171585083 0.19395324967791994\n9868 2.521920680999756 0.5497794735158222\n2595 2.95841383934021 0.782779934368899\n----------\n6759 3.1252167224884033 0.7098308465010348\n</code></pre>\n<h2>After</h2>\n<p>First run</p>\n<pre><code>4194 1.9806793928146362 0.28176797222610817\n4717 2.202341079711914 0.9839100412778289\n4607 2.3166141510009766 0.6780782905745018\n4691 0.9809015393257141 0.3976280065444453\n9868 2.521920680999756 0.7470951277406424\n2595 2.95841383934021 0.3961659556772974\n7367 2.333574056625366 0.021153138172570696\n9490 3.02830171585083 0.07198172828873439\n----------\n6759 3.1252167224884033 0.5803779056735119\n</code></pre>\n<p>Next run</p>\n<pre><code>4717 2.202341079711914 0.9839100412778289\n4194 1.9806793928146362 0.28176797222610817\n4607 2.3166141510009766 0.6780782905745018\n2595 2.95841383934021 0.3961659556772974\n4691 0.9809015393257141 0.3976280065444453\n9868 2.521920680999756 0.7470951277406424\n7367 2.333574056625366 0.021153138172570696\n9490 3.02830171585083 0.07198172828873439\n----------\n6759 3.1252167224884033 0.5803779056735119\n\n</code></pre>", "body_text": "Fix #7882\nThe  random module is seeded here. The problem with that line is that seed is not an int but a Tensor. Python then will use hash(seed)  to seed the random module (docs). Without a hash function for Tensor, the actually seed will be the address of the tensor. And it changes every time!\nThe line should be changed to: random.seed(int(seed))\nTest on fllowing script from #7882\nimport torch\nimport random\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data(Dataset):\n    def __len__(self):\n        return 10000\n    def __getitem__(self, index):\n        print(index, torch.rand(2, 2).sum().item(), random.uniform(0, 1))\n        return 1\n\nseed = 2018\ntorch.manual_seed(seed)\nloader = DataLoader(Data(), num_workers=4, shuffle=True)\n\nfor x in loader:\n    print('-'*10)\n    break\nBefore\nFirst run\n4717 2.202341079711914 0.9952153654478976\n4607 2.3166141510009766 0.6813692345925851\n4194 1.9806793928146362 0.6281118075687344\n2595 2.95841383934021 0.8414756141240453\n4691 0.9809015393257141 0.7622458327788627\n9868 2.521920680999756 0.5253262288522356\n7367 2.333574056625366 0.35079311205192487\n9490 3.02830171585083 0.16235006783937567\n----------\n6759 3.1252167224884033 0.4424384676992986\n\nNext run\n4607 2.3166141510009766 0.15198273935290807\n4194 1.9806793928146362 0.36414129463658884\n4691 0.9809015393257141 0.027569260048619926\n4717 2.202341079711914 0.5512619092026773\n7367 2.333574056625366 0.7932627754589792\n9490 3.02830171585083 0.19395324967791994\n9868 2.521920680999756 0.5497794735158222\n2595 2.95841383934021 0.782779934368899\n----------\n6759 3.1252167224884033 0.7098308465010348\n\nAfter\nFirst run\n4194 1.9806793928146362 0.28176797222610817\n4717 2.202341079711914 0.9839100412778289\n4607 2.3166141510009766 0.6780782905745018\n4691 0.9809015393257141 0.3976280065444453\n9868 2.521920680999756 0.7470951277406424\n2595 2.95841383934021 0.3961659556772974\n7367 2.333574056625366 0.021153138172570696\n9490 3.02830171585083 0.07198172828873439\n----------\n6759 3.1252167224884033 0.5803779056735119\n\nNext run\n4717 2.202341079711914 0.9839100412778289\n4194 1.9806793928146362 0.28176797222610817\n4607 2.3166141510009766 0.6780782905745018\n2595 2.95841383934021 0.3961659556772974\n4691 0.9809015393257141 0.3976280065444453\n9868 2.521920680999756 0.7470951277406424\n7367 2.333574056625366 0.021153138172570696\n9490 3.02830171585083 0.07198172828873439\n----------\n6759 3.1252167224884033 0.5803779056735119", "body": "Fix #7882 \r\n\r\nThe  `random` module is seeded [here](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L86). The problem with that line is that `seed` is not an `int` but a `Tensor`. Python then will use `hash(seed)`  to seed the random module ([docs](https://docs.python.org/2/library/random.html#random.seed)). Without a hash function for `Tensor`, the actually seed will be the address of the tensor. And it changes every time!\r\n\r\nThe line should be changed to: `random.seed(int(seed))`\r\n\r\nTest on fllowing script from #7882 \r\n```python\r\nimport torch\r\nimport random\r\n\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nclass Data(Dataset):\r\n    def __len__(self):\r\n        return 10000\r\n    def __getitem__(self, index):\r\n        print(index, torch.rand(2, 2).sum().item(), random.uniform(0, 1))\r\n        return 1\r\n\r\nseed = 2018\r\ntorch.manual_seed(seed)\r\nloader = DataLoader(Data(), num_workers=4, shuffle=True)\r\n\r\nfor x in loader:\r\n    print('-'*10)\r\n    break\r\n```\r\n## Before\r\nFirst run\r\n```\r\n4717 2.202341079711914 0.9952153654478976\r\n4607 2.3166141510009766 0.6813692345925851\r\n4194 1.9806793928146362 0.6281118075687344\r\n2595 2.95841383934021 0.8414756141240453\r\n4691 0.9809015393257141 0.7622458327788627\r\n9868 2.521920680999756 0.5253262288522356\r\n7367 2.333574056625366 0.35079311205192487\r\n9490 3.02830171585083 0.16235006783937567\r\n----------\r\n6759 3.1252167224884033 0.4424384676992986\r\n```\r\nNext run\r\n```\r\n4607 2.3166141510009766 0.15198273935290807\r\n4194 1.9806793928146362 0.36414129463658884\r\n4691 0.9809015393257141 0.027569260048619926\r\n4717 2.202341079711914 0.5512619092026773\r\n7367 2.333574056625366 0.7932627754589792\r\n9490 3.02830171585083 0.19395324967791994\r\n9868 2.521920680999756 0.5497794735158222\r\n2595 2.95841383934021 0.782779934368899\r\n----------\r\n6759 3.1252167224884033 0.7098308465010348\r\n```\r\n\r\n## After\r\nFirst run\r\n```\r\n4194 1.9806793928146362 0.28176797222610817\r\n4717 2.202341079711914 0.9839100412778289\r\n4607 2.3166141510009766 0.6780782905745018\r\n4691 0.9809015393257141 0.3976280065444453\r\n9868 2.521920680999756 0.7470951277406424\r\n2595 2.95841383934021 0.3961659556772974\r\n7367 2.333574056625366 0.021153138172570696\r\n9490 3.02830171585083 0.07198172828873439\r\n----------\r\n6759 3.1252167224884033 0.5803779056735119\r\n```\r\nNext run\r\n```\r\n4717 2.202341079711914 0.9839100412778289\r\n4194 1.9806793928146362 0.28176797222610817\r\n4607 2.3166141510009766 0.6780782905745018\r\n2595 2.95841383934021 0.3961659556772974\r\n4691 0.9809015393257141 0.3976280065444453\r\n9868 2.521920680999756 0.7470951277406424\r\n7367 2.333574056625366 0.021153138172570696\r\n9490 3.02830171585083 0.07198172828873439\r\n----------\r\n6759 3.1252167224884033 0.5803779056735119\r\n\r\n```"}