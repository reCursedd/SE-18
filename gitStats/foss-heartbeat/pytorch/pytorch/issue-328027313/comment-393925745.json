{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393925745", "html_url": "https://github.com/pytorch/pytorch/pull/7984#issuecomment-393925745", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7984", "id": 393925745, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzkyNTc0NQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-01T15:59:05Z", "updated_at": "2018-06-01T15:59:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>OK, let's talk about how we are going to measure this. Given the numbers I posted in the previous post, we expect to see on order of 70ns of overhead added on account of ska_hash_map when the hash map has 100000 elements,  and maybe something more like 50ns when the hash map has very few elements. The current overhead of a full round trip from from Python to C++ and allocation of a new tensor is on order of a microsecond, which  means this should be something like a 5% performance regression. In particular, the 0% measurement from my previous ipython timeit cannot be correct.</p>\n<p>The problem is in the measurement. The numbers I posted above look like they are able to capture precision in the tens of nanoseconds, but this is actually a lie; there is 100s of nanoseconds of variance between different <strong>runs of the process</strong>, due various randomness and jitter. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> gave me a very good suggestion, which was to arrange for both the old and new codepaths to live in the same address space, so I can perform both measurements without having to start a new codepath. The code here is <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/c0225715bf67caf0903c3aa24dccf0796ad25463/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/c0225715bf67caf0903c3aa24dccf0796ad25463\"><tt>c022571</tt></a> <strong>Note that this is SLIGHTLY unfair to the hash map, as I unconditionally also set the pyobj field in Variable to make sure I can safely toggle between the two schemes without segfaulting. If Variable's fields are in L1 cache, this should only be a 1ns difference.</strong> There is also the additional overhead from testing which codepath to go down, but this overhead is applied uniformly to both schemes and should only reduce the relative difference.</p>\n<p>For additional safety, I ran the following commands to reduce system jitter:</p>\n<pre><code>echo 0 | sudo tee /sys/devices/system/cpu/cpu26/online\necho 0 | sudo tee /sys/devices/system/cpu/cpu27/online\nsudo /etc/init.d/irqbalance stop\nsudo cpupower -c 2-3 frequency-set -f 2400MHz # underclock\nsudo wrmsr -p2 0x1a0 0x4000850089 # turn off turboboost\n</code></pre>\n<p>and ran with the following kernel command line:</p>\n<pre><code>Command line: BOOT_IMAGE=/boot/vmlinuz-4.4.0-92-generic root=UUID=d5ea393a-b06f-48f6-86a9-59d64ea7a02d ro quiet splash isolcpus=2,3 nohz_full=2,3 rcu_nocbs=2,3 intel_pstate=disable vt.handoff=7\n</code></pre>\n<p>Here is my IPython session (NB: we start with the hash map disabled). Note that these numbers are INFLATED because the processor has been underclocked and turboboost disabled.</p>\n<pre><code>(/private/home/ezyang/pytorch-pymap-env) ezyang@devfair024:~/pytorch-pymap$ numactl --cpunodebind=0 --membind=0 taskset -c 2 ipython\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \nType 'copyright', 'credits' or 'license' for more information\nIPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import torch\n\nIn [2]: x = torch.zeros(0)\n\nIn [3]: %timeit x.clone()\n1.29 \u00b5s \u00b1 2.76 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [4]: %timeit x.clone()\n1.29 \u00b5s \u00b1 3.74 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [5]: %timeit x.clone()\n1.31 \u00b5s \u00b1 6.03 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [6]: %timeit x.clone()\n1.29 \u00b5s \u00b1 4.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [7]: %timeit x.clone()\n1.29 \u00b5s \u00b1 5.46 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [8]: x._toggle()\ngUseHashMap now is 1\n\nIn [9]: %timeit x.clone()\n1.34 \u00b5s \u00b1 15.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [10]: %timeit x.clone()\n1.35 \u00b5s \u00b1 6.47 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [11]: %timeit x.clone()\n1.34 \u00b5s \u00b1 8.65 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [12]: %timeit x.clone()\n1.35 \u00b5s \u00b1 12.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [13]: %timeit x.clone()\n1.33 \u00b5s \u00b1 13.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\n# The below is a sanity check to demonstrate that there is no hysteresis from toggling\n\nIn [14]: x._toggle()\ngUseHashMap now is 0\n\nIn [15]: %timeit x.clone()\n1.28 \u00b5s \u00b1 3.35 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [16]: %timeit x.clone()\n1.28 \u00b5s \u00b1 2.12 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [17]: x._toggle()\ngUseHashMap now is 1\n\nIn [18]: %timeit x.clone()\n1.33 \u00b5s \u00b1 2.02 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [19]: %timeit x.clone()\n1.34 \u00b5s \u00b1 2.83 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n</code></pre>\n<p>Taking the minimum measurement in each regime, we have 1.28 us versus 1.33 us, an absolute difference of 50ns (as expected), and a relative difference of 4%.</p>\n<p>(more coming)</p>", "body_text": "OK, let's talk about how we are going to measure this. Given the numbers I posted in the previous post, we expect to see on order of 70ns of overhead added on account of ska_hash_map when the hash map has 100000 elements,  and maybe something more like 50ns when the hash map has very few elements. The current overhead of a full round trip from from Python to C++ and allocation of a new tensor is on order of a microsecond, which  means this should be something like a 5% performance regression. In particular, the 0% measurement from my previous ipython timeit cannot be correct.\nThe problem is in the measurement. The numbers I posted above look like they are able to capture precision in the tens of nanoseconds, but this is actually a lie; there is 100s of nanoseconds of variance between different runs of the process, due various randomness and jitter. @colesbury gave me a very good suggestion, which was to arrange for both the old and new codepaths to live in the same address space, so I can perform both measurements without having to start a new codepath. The code here is c022571 Note that this is SLIGHTLY unfair to the hash map, as I unconditionally also set the pyobj field in Variable to make sure I can safely toggle between the two schemes without segfaulting. If Variable's fields are in L1 cache, this should only be a 1ns difference. There is also the additional overhead from testing which codepath to go down, but this overhead is applied uniformly to both schemes and should only reduce the relative difference.\nFor additional safety, I ran the following commands to reduce system jitter:\necho 0 | sudo tee /sys/devices/system/cpu/cpu26/online\necho 0 | sudo tee /sys/devices/system/cpu/cpu27/online\nsudo /etc/init.d/irqbalance stop\nsudo cpupower -c 2-3 frequency-set -f 2400MHz # underclock\nsudo wrmsr -p2 0x1a0 0x4000850089 # turn off turboboost\n\nand ran with the following kernel command line:\nCommand line: BOOT_IMAGE=/boot/vmlinuz-4.4.0-92-generic root=UUID=d5ea393a-b06f-48f6-86a9-59d64ea7a02d ro quiet splash isolcpus=2,3 nohz_full=2,3 rcu_nocbs=2,3 intel_pstate=disable vt.handoff=7\n\nHere is my IPython session (NB: we start with the hash map disabled). Note that these numbers are INFLATED because the processor has been underclocked and turboboost disabled.\n(/private/home/ezyang/pytorch-pymap-env) ezyang@devfair024:~/pytorch-pymap$ numactl --cpunodebind=0 --membind=0 taskset -c 2 ipython\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \nType 'copyright', 'credits' or 'license' for more information\nIPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import torch\n\nIn [2]: x = torch.zeros(0)\n\nIn [3]: %timeit x.clone()\n1.29 \u00b5s \u00b1 2.76 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [4]: %timeit x.clone()\n1.29 \u00b5s \u00b1 3.74 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [5]: %timeit x.clone()\n1.31 \u00b5s \u00b1 6.03 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [6]: %timeit x.clone()\n1.29 \u00b5s \u00b1 4.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [7]: %timeit x.clone()\n1.29 \u00b5s \u00b1 5.46 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [8]: x._toggle()\ngUseHashMap now is 1\n\nIn [9]: %timeit x.clone()\n1.34 \u00b5s \u00b1 15.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [10]: %timeit x.clone()\n1.35 \u00b5s \u00b1 6.47 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [11]: %timeit x.clone()\n1.34 \u00b5s \u00b1 8.65 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [12]: %timeit x.clone()\n1.35 \u00b5s \u00b1 12.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [13]: %timeit x.clone()\n1.33 \u00b5s \u00b1 13.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\n# The below is a sanity check to demonstrate that there is no hysteresis from toggling\n\nIn [14]: x._toggle()\ngUseHashMap now is 0\n\nIn [15]: %timeit x.clone()\n1.28 \u00b5s \u00b1 3.35 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [16]: %timeit x.clone()\n1.28 \u00b5s \u00b1 2.12 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [17]: x._toggle()\ngUseHashMap now is 1\n\nIn [18]: %timeit x.clone()\n1.33 \u00b5s \u00b1 2.02 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [19]: %timeit x.clone()\n1.34 \u00b5s \u00b1 2.83 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nTaking the minimum measurement in each regime, we have 1.28 us versus 1.33 us, an absolute difference of 50ns (as expected), and a relative difference of 4%.\n(more coming)", "body": "OK, let's talk about how we are going to measure this. Given the numbers I posted in the previous post, we expect to see on order of 70ns of overhead added on account of ska_hash_map when the hash map has 100000 elements,  and maybe something more like 50ns when the hash map has very few elements. The current overhead of a full round trip from from Python to C++ and allocation of a new tensor is on order of a microsecond, which  means this should be something like a 5% performance regression. In particular, the 0% measurement from my previous ipython timeit cannot be correct.\r\n\r\nThe problem is in the measurement. The numbers I posted above look like they are able to capture precision in the tens of nanoseconds, but this is actually a lie; there is 100s of nanoseconds of variance between different **runs of the process**, due various randomness and jitter. @colesbury gave me a very good suggestion, which was to arrange for both the old and new codepaths to live in the same address space, so I can perform both measurements without having to start a new codepath. The code here is https://github.com/pytorch/pytorch/commit/c0225715bf67caf0903c3aa24dccf0796ad25463 **Note that this is SLIGHTLY unfair to the hash map, as I unconditionally also set the pyobj field in Variable to make sure I can safely toggle between the two schemes without segfaulting. If Variable's fields are in L1 cache, this should only be a 1ns difference.** There is also the additional overhead from testing which codepath to go down, but this overhead is applied uniformly to both schemes and should only reduce the relative difference.\r\n\r\nFor additional safety, I ran the following commands to reduce system jitter:\r\n\r\n```\r\necho 0 | sudo tee /sys/devices/system/cpu/cpu26/online\r\necho 0 | sudo tee /sys/devices/system/cpu/cpu27/online\r\nsudo /etc/init.d/irqbalance stop\r\nsudo cpupower -c 2-3 frequency-set -f 2400MHz # underclock\r\nsudo wrmsr -p2 0x1a0 0x4000850089 # turn off turboboost\r\n```\r\n\r\nand ran with the following kernel command line:\r\n\r\n```\r\nCommand line: BOOT_IMAGE=/boot/vmlinuz-4.4.0-92-generic root=UUID=d5ea393a-b06f-48f6-86a9-59d64ea7a02d ro quiet splash isolcpus=2,3 nohz_full=2,3 rcu_nocbs=2,3 intel_pstate=disable vt.handoff=7\r\n```\r\n\r\nHere is my IPython session (NB: we start with the hash map disabled). Note that these numbers are INFLATED because the processor has been underclocked and turboboost disabled.\r\n\r\n```\r\n(/private/home/ezyang/pytorch-pymap-env) ezyang@devfair024:~/pytorch-pymap$ numactl --cpunodebind=0 --membind=0 taskset -c 2 ipython\r\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import torch\r\n\r\nIn [2]: x = torch.zeros(0)\r\n\r\nIn [3]: %timeit x.clone()\r\n1.29 \u00b5s \u00b1 2.76 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [4]: %timeit x.clone()\r\n1.29 \u00b5s \u00b1 3.74 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [5]: %timeit x.clone()\r\n1.31 \u00b5s \u00b1 6.03 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [6]: %timeit x.clone()\r\n1.29 \u00b5s \u00b1 4.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [7]: %timeit x.clone()\r\n1.29 \u00b5s \u00b1 5.46 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [8]: x._toggle()\r\ngUseHashMap now is 1\r\n\r\nIn [9]: %timeit x.clone()\r\n1.34 \u00b5s \u00b1 15.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [10]: %timeit x.clone()\r\n1.35 \u00b5s \u00b1 6.47 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [11]: %timeit x.clone()\r\n1.34 \u00b5s \u00b1 8.65 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [12]: %timeit x.clone()\r\n1.35 \u00b5s \u00b1 12.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [13]: %timeit x.clone()\r\n1.33 \u00b5s \u00b1 13.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\n# The below is a sanity check to demonstrate that there is no hysteresis from toggling\r\n\r\nIn [14]: x._toggle()\r\ngUseHashMap now is 0\r\n\r\nIn [15]: %timeit x.clone()\r\n1.28 \u00b5s \u00b1 3.35 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [16]: %timeit x.clone()\r\n1.28 \u00b5s \u00b1 2.12 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [17]: x._toggle()\r\ngUseHashMap now is 1\r\n\r\nIn [18]: %timeit x.clone()\r\n1.33 \u00b5s \u00b1 2.02 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [19]: %timeit x.clone()\r\n1.34 \u00b5s \u00b1 2.83 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n```\r\n\r\nTaking the minimum measurement in each regime, we have 1.28 us versus 1.33 us, an absolute difference of 50ns (as expected), and a relative difference of 4%.\r\n\r\n(more coming)"}