{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393951181", "html_url": "https://github.com/pytorch/pytorch/pull/7984#issuecomment-393951181", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7984", "id": 393951181, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Mzk1MTE4MQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-01T17:28:12Z", "updated_at": "2018-06-01T17:54:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Let's talk about these numbers. A lot of the text here is based on discussions with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> about what PyTorch's unwritten motivations are.</p>\n<p>How fast should an allocation of the tensor metadata be from Python? (We see from our numbers that it is about a microsecond.  But is this our target number?) How much of a regression is \"acceptable\" for this path? If the change results in no easily measurable difference when running ImageNet training, does that make it OK?</p>\n<p>Based on the WLM example (138839 hits to existing variables, 545558 fresh variables), we see that C++-Python tensor crossings happen a lot: easily in the millions. (And this is just for a single epoch: typical training will run over many epochs; furthermore, it's worth noting that WLM is a fairly \"well-written\" model, which attempts to minimize the number of function calls that are made.)  For something like this, tens (to say nothing of hundreds) of nanoseconds matter. We can see from existing benchmarks that the current overhead of a tensor allocation into Python is 1000 nanoseconds. Compare this to some other costs (h/t <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a>):</p>\n<ul>\n<li>It takes ~70 ns to do a PyObject allocation</li>\n<li>It takes ~35 ns to index a Python list</li>\n<li>It takes ~10 ns to do an FFI crossing</li>\n<li>It takes ~80 ns to do Python argument parsing</li>\n</ul>\n<p>In the light of these costs, 1000 nanoseconds is ridiculously expensive. If we were to shave off 70% of it, that translates into savings of <strong>minutes</strong> per epoch. We should be targeting <strong>300 ns</strong> for an indexing operation like <code>x[0]</code>, and as the perf regression in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"299855779\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5388\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5388/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5388\">#5388</a>, we can pin this cost on the fact that we are returning a Tensor where were previously returning a Variable. In this regime, the 50 ns of a hash table is a big deal.</p>\n<p>There are a few ancillary considerations worth thinking about too:</p>\n<ul>\n<li>\n<p>Conversion into PyObject must occur under the GIL, and it's worth noting that overhead inside the GIL is much worse than overhead outside of the GIL, because it is overhead in the critical path. Overhead in malloc can be covered up by having multiple threads; but GIL overhead always adds to the absolute wallclock time of your program.</p>\n</li>\n<li>\n<p>Word language model is a \"good\" PyTorch model; it is fairly economical in the operations that it does. But many machine learning researchers are not good engineers, and they are going to do dumb things like use tensors which are too small, because that's the problem they want to solve, not because they are thinking about how to make their model run as quickly as possible. PyTorch wants to serve these users well, and not slap them with invisible costs, and we also want to support future users, who might not want to use PyTorch today because the overheads are too high for the way they want to use the library.</p>\n</li>\n</ul>", "body_text": "Let's talk about these numbers. A lot of the text here is based on discussions with @colesbury about what PyTorch's unwritten motivations are.\nHow fast should an allocation of the tensor metadata be from Python? (We see from our numbers that it is about a microsecond.  But is this our target number?) How much of a regression is \"acceptable\" for this path? If the change results in no easily measurable difference when running ImageNet training, does that make it OK?\nBased on the WLM example (138839 hits to existing variables, 545558 fresh variables), we see that C++-Python tensor crossings happen a lot: easily in the millions. (And this is just for a single epoch: typical training will run over many epochs; furthermore, it's worth noting that WLM is a fairly \"well-written\" model, which attempts to minimize the number of function calls that are made.)  For something like this, tens (to say nothing of hundreds) of nanoseconds matter. We can see from existing benchmarks that the current overhead of a tensor allocation into Python is 1000 nanoseconds. Compare this to some other costs (h/t @colesbury):\n\nIt takes ~70 ns to do a PyObject allocation\nIt takes ~35 ns to index a Python list\nIt takes ~10 ns to do an FFI crossing\nIt takes ~80 ns to do Python argument parsing\n\nIn the light of these costs, 1000 nanoseconds is ridiculously expensive. If we were to shave off 70% of it, that translates into savings of minutes per epoch. We should be targeting 300 ns for an indexing operation like x[0], and as the perf regression in #5388, we can pin this cost on the fact that we are returning a Tensor where were previously returning a Variable. In this regime, the 50 ns of a hash table is a big deal.\nThere are a few ancillary considerations worth thinking about too:\n\n\nConversion into PyObject must occur under the GIL, and it's worth noting that overhead inside the GIL is much worse than overhead outside of the GIL, because it is overhead in the critical path. Overhead in malloc can be covered up by having multiple threads; but GIL overhead always adds to the absolute wallclock time of your program.\n\n\nWord language model is a \"good\" PyTorch model; it is fairly economical in the operations that it does. But many machine learning researchers are not good engineers, and they are going to do dumb things like use tensors which are too small, because that's the problem they want to solve, not because they are thinking about how to make their model run as quickly as possible. PyTorch wants to serve these users well, and not slap them with invisible costs, and we also want to support future users, who might not want to use PyTorch today because the overheads are too high for the way they want to use the library.", "body": "Let's talk about these numbers. A lot of the text here is based on discussions with @colesbury about what PyTorch's unwritten motivations are.\r\n\r\nHow fast should an allocation of the tensor metadata be from Python? (We see from our numbers that it is about a microsecond.  But is this our target number?) How much of a regression is \"acceptable\" for this path? If the change results in no easily measurable difference when running ImageNet training, does that make it OK?\r\n\r\nBased on the WLM example (138839 hits to existing variables, 545558 fresh variables), we see that C++-Python tensor crossings happen a lot: easily in the millions. (And this is just for a single epoch: typical training will run over many epochs; furthermore, it's worth noting that WLM is a fairly \"well-written\" model, which attempts to minimize the number of function calls that are made.)  For something like this, tens (to say nothing of hundreds) of nanoseconds matter. We can see from existing benchmarks that the current overhead of a tensor allocation into Python is 1000 nanoseconds. Compare this to some other costs (h/t @colesbury):\r\n\r\n* It takes ~70 ns to do a PyObject allocation\r\n* It takes ~35 ns to index a Python list\r\n* It takes ~10 ns to do an FFI crossing\r\n* It takes ~80 ns to do Python argument parsing\r\n\r\nIn the light of these costs, 1000 nanoseconds is ridiculously expensive. If we were to shave off 70% of it, that translates into savings of **minutes** per epoch. We should be targeting **300 ns** for an indexing operation like `x[0]`, and as the perf regression in #5388, we can pin this cost on the fact that we are returning a Tensor where were previously returning a Variable. In this regime, the 50 ns of a hash table is a big deal.\r\n\r\nThere are a few ancillary considerations worth thinking about too:\r\n\r\n* Conversion into PyObject must occur under the GIL, and it's worth noting that overhead inside the GIL is much worse than overhead outside of the GIL, because it is overhead in the critical path. Overhead in malloc can be covered up by having multiple threads; but GIL overhead always adds to the absolute wallclock time of your program.\r\n\r\n* Word language model is a \"good\" PyTorch model; it is fairly economical in the operations that it does. But many machine learning researchers are not good engineers, and they are going to do dumb things like use tensors which are too small, because that's the problem they want to solve, not because they are thinking about how to make their model run as quickly as possible. PyTorch wants to serve these users well, and not slap them with invisible costs, and we also want to support future users, who might not want to use PyTorch today because the overheads are too high for the way they want to use the library."}