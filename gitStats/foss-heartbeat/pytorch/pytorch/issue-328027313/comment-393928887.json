{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393928887", "html_url": "https://github.com/pytorch/pytorch/pull/7984#issuecomment-393928887", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7984", "id": 393928887, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzkyODg4Nw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-01T16:10:11Z", "updated_at": "2018-06-01T16:12:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We can now perform <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>'s requested experiment, to test the overhead when the hash table is loaded with 100k tensors. I did this experiment in another instance of the IPython interpreter, so I reprise all of the previous measurements.</p>\n<pre><code>In [1]: import torch\n\nIn [2]: x = torch.zeros(0)\n\nIn [3]: %timeit x.clone()\n1.38 \u00b5s \u00b1 4.46 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [4]: %timeit x.clone()\n1.38 \u00b5s \u00b1 1.93 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [5]: %timeit x.clone()\n1.38 \u00b5s \u00b1 3.36 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [6]: x._toggle()\ngUseHashMap now is 1\n\nIn [7]: %timeit x.clone()\n1.42 \u00b5s \u00b1 0.885 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [8]: %timeit x.clone()\n1.42 \u00b5s \u00b1 1.83 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [9]: lst = [x.clone() for i in range(1)]\n\nIn [10]: %timeit x.clone()\n1.44 \u00b5s \u00b1 1.82 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [11]: lst = [x.clone() for i in range(10)]\n\nIn [12]: %timeit x.clone()\n1.43 \u00b5s \u00b1 2.03 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [13]: lst = [x.clone() for i in range(100)]\n\nIn [14]: %timeit x.clone()\n1.41 \u00b5s \u00b1 2.72 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [15]: lst = [x.clone() for i in range(1000)]\n\nIn [16]: %timeit x.clone()\n1.46 \u00b5s \u00b1 7.59 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [17]: lst = [x.clone() for i in range(10000)]\n\nIn [18]: %timeit x.clone()\n1.43 \u00b5s \u00b1 1.95 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [19]: lst = [x.clone() for i in range(100000)]\n\nIn [20]: %timeit x.clone()\n1.42 \u00b5s \u00b1 3.68 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [21]: %timeit x.clone()\n1.42 \u00b5s \u00b1 1.67 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [22]: lst = [x.clone() for i in range(1000000)]\n\nIn [23]: %timeit x.clone()\n1.41 \u00b5s \u00b1 0.427 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [24]: lst = [x.clone() for i in range(10000000)] # This took a while to run!\n\nIn [25]: %timeit x.clone()\n1.4 \u00b5s \u00b1 1.01 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\n</code></pre>\n<p>From these measurements we infer that size of the hash table does not materially impact performance of lookups.</p>\n<p>(more coming)</p>", "body_text": "We can now perform @soumith's requested experiment, to test the overhead when the hash table is loaded with 100k tensors. I did this experiment in another instance of the IPython interpreter, so I reprise all of the previous measurements.\nIn [1]: import torch\n\nIn [2]: x = torch.zeros(0)\n\nIn [3]: %timeit x.clone()\n1.38 \u00b5s \u00b1 4.46 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [4]: %timeit x.clone()\n1.38 \u00b5s \u00b1 1.93 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [5]: %timeit x.clone()\n1.38 \u00b5s \u00b1 3.36 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [6]: x._toggle()\ngUseHashMap now is 1\n\nIn [7]: %timeit x.clone()\n1.42 \u00b5s \u00b1 0.885 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [8]: %timeit x.clone()\n1.42 \u00b5s \u00b1 1.83 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [9]: lst = [x.clone() for i in range(1)]\n\nIn [10]: %timeit x.clone()\n1.44 \u00b5s \u00b1 1.82 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [11]: lst = [x.clone() for i in range(10)]\n\nIn [12]: %timeit x.clone()\n1.43 \u00b5s \u00b1 2.03 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [13]: lst = [x.clone() for i in range(100)]\n\nIn [14]: %timeit x.clone()\n1.41 \u00b5s \u00b1 2.72 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [15]: lst = [x.clone() for i in range(1000)]\n\nIn [16]: %timeit x.clone()\n1.46 \u00b5s \u00b1 7.59 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [17]: lst = [x.clone() for i in range(10000)]\n\nIn [18]: %timeit x.clone()\n1.43 \u00b5s \u00b1 1.95 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [19]: lst = [x.clone() for i in range(100000)]\n\nIn [20]: %timeit x.clone()\n1.42 \u00b5s \u00b1 3.68 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [21]: %timeit x.clone()\n1.42 \u00b5s \u00b1 1.67 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [22]: lst = [x.clone() for i in range(1000000)]\n\nIn [23]: %timeit x.clone()\n1.41 \u00b5s \u00b1 0.427 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [24]: lst = [x.clone() for i in range(10000000)] # This took a while to run!\n\nIn [25]: %timeit x.clone()\n1.4 \u00b5s \u00b1 1.01 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\n\nFrom these measurements we infer that size of the hash table does not materially impact performance of lookups.\n(more coming)", "body": "We can now perform @soumith's requested experiment, to test the overhead when the hash table is loaded with 100k tensors. I did this experiment in another instance of the IPython interpreter, so I reprise all of the previous measurements.\r\n\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: x = torch.zeros(0)\r\n\r\nIn [3]: %timeit x.clone()\r\n1.38 \u00b5s \u00b1 4.46 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [4]: %timeit x.clone()\r\n1.38 \u00b5s \u00b1 1.93 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [5]: %timeit x.clone()\r\n1.38 \u00b5s \u00b1 3.36 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [6]: x._toggle()\r\ngUseHashMap now is 1\r\n\r\nIn [7]: %timeit x.clone()\r\n1.42 \u00b5s \u00b1 0.885 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [8]: %timeit x.clone()\r\n1.42 \u00b5s \u00b1 1.83 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [9]: lst = [x.clone() for i in range(1)]\r\n\r\nIn [10]: %timeit x.clone()\r\n1.44 \u00b5s \u00b1 1.82 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [11]: lst = [x.clone() for i in range(10)]\r\n\r\nIn [12]: %timeit x.clone()\r\n1.43 \u00b5s \u00b1 2.03 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [13]: lst = [x.clone() for i in range(100)]\r\n\r\nIn [14]: %timeit x.clone()\r\n1.41 \u00b5s \u00b1 2.72 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [15]: lst = [x.clone() for i in range(1000)]\r\n\r\nIn [16]: %timeit x.clone()\r\n1.46 \u00b5s \u00b1 7.59 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [17]: lst = [x.clone() for i in range(10000)]\r\n\r\nIn [18]: %timeit x.clone()\r\n1.43 \u00b5s \u00b1 1.95 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [19]: lst = [x.clone() for i in range(100000)]\r\n\r\nIn [20]: %timeit x.clone()\r\n1.42 \u00b5s \u00b1 3.68 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [21]: %timeit x.clone()\r\n1.42 \u00b5s \u00b1 1.67 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [22]: lst = [x.clone() for i in range(1000000)]\r\n\r\nIn [23]: %timeit x.clone()\r\n1.41 \u00b5s \u00b1 0.427 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\nIn [24]: lst = [x.clone() for i in range(10000000)] # This took a while to run!\r\n\r\nIn [25]: %timeit x.clone()\r\n1.4 \u00b5s \u00b1 1.01 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n\r\n```\r\n\r\nFrom these measurements we infer that size of the hash table does not materially impact performance of lookups.\r\n\r\n(more coming)"}