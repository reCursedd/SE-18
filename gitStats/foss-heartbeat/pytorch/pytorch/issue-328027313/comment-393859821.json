{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393859821", "html_url": "https://github.com/pytorch/pytorch/pull/7984#issuecomment-393859821", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7984", "id": 393859821, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Mzg1OTgyMQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-01T12:02:12Z", "updated_at": "2018-06-01T17:32:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Before I get into more numbers, I want to preface this with more discussion about implicit PyTorch needs, which explain the intuitive aversion of many developers to using a hash table here.</p>\n<ul>\n<li>Performance of wrapping tensors in PyTorch is critical for CPU NLP models in PyTorch, c.f. <a href=\"https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet\">https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet</a> (h/t <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17890620\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dzhulgakov\">@dzhulgakov</a> for this text)</li>\n<li>We want simple, predictable behavior in all cases. If something is simple and predictable, there is no chance that it will blow up in our face in some weird, esoteric condition. A single field in the C++ object is simple and predictable. A hash table is not simple and it is only mostly predictable: there are many footguns like poor choice of hash function/distribution of hashes, hash table load factor, cache behavior, probing/collision strategy, which all need to be handled. This is not to say a well written hash table cannot be made to work well in all cases, but if something goes wrong in some esoteric environment, we are far more likely to be able to figure it out if you have a simple implementation strategy instead of a complex one.Case in point, we would not have to do the analysis below if we were using a simple strategy; with a hash table, we have to think and benchmark. There is definite merit to avoiding thinking/benchmarking if it is not necessary.</li>\n</ul>\n<hr>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> requested information on the absolute overheads of the hash table lookup. <code>ska::flat_hash_map</code> is described in more detail at <a href=\"https://probablydance.com/2017/02/26/i-wrote-the-fastest-hashtable/\" rel=\"nofollow\">https://probablydance.com/2017/02/26/i-wrote-the-fastest-hashtable/</a> (though, unfortunately, he does not say what hardware he did his tests on.) <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> mentioned that 100k live tensors is a workload we see in practice. Cross-referencing the blog post, and simply \"using my eyeball\" to compute what the extra numbers are, we see that we are looking at <strong>successful lookup in the range of 8-10ns</strong> (depending on if we are using prime numbers of power of two; I am not sure if we are allowed to use the power of two resizing policy with integer pointer hashes--an example of complexity) and you are just barely in the range where the hash table fits in cache; <strong>successful lookup with cache miss will be more like 25-30ns</strong> (this is about the same cost of a mutex lock/unlock (citing <a href=\"https://gist.github.com/hellerbarde/2843375#gistcomment-1896153\">https://gist.github.com/hellerbarde/2843375#gistcomment-1896153</a>), and Python function call overhead is something like 150ns (citing <a href=\"https://ilovesymposia.com/2015/12/10/the-cost-of-a-python-function-call/\" rel=\"nofollow\">https://ilovesymposia.com/2015/12/10/the-cost-of-a-python-function-call/</a>)). <strong>Unsuccessful lookup with cache miss will be 30-40ns.</strong> And <strong>insertion is 50-70ns</strong> and <strong>deletion 20-30ns</strong>.</p>\n<p>Let's discuss the average lifecycle of entries in this table. It will look something like this:</p>\n<ol>\n<li>A tensor gets created in C++</li>\n<li>It moves to Python, at which point an unsuccessful lookup and subsequent insert occurs. (The cost here is rolled up with the cost of allocating the initial PyObject to represent the tensor.)</li>\n<li><em>Possibly, some number of times</em>, another movement to Python happens, at which point a successful lookup occurs.</li>\n<li>At some point, the Python object goes dead, and the destructor deletes the object from the hash table. We possibly go back to (2)</li>\n</ol>\n<p>Given this lifecycle, <code>ska::flat_hash_map</code> is described as being optimized for fast lookup, may not be optimal for PyTorch, where most objects crossing the C-Python FFI boundary are new objects. To assess this, I added counters for the \"Wrapping var, existing/new one\" debug prints from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17890620\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dzhulgakov\">@dzhulgakov</a>'s implementation and ran a single epoch of training with <code>word_language_model</code> on CUDA from <a href=\"https://github.com/pytorch/examples\">https://github.com/pytorch/examples</a>. I found that there were 138839 hits to existing variables versus 545558 fresh variables. <strong>That's 684397 Tensor crossings across Python-C++ in total, for a single epoch of training in a mid-size model.</strong></p>\n<p>(more to come)</p>", "body_text": "Before I get into more numbers, I want to preface this with more discussion about implicit PyTorch needs, which explain the intuitive aversion of many developers to using a hash table here.\n\nPerformance of wrapping tensors in PyTorch is critical for CPU NLP models in PyTorch, c.f. https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet (h/t @dzhulgakov for this text)\nWe want simple, predictable behavior in all cases. If something is simple and predictable, there is no chance that it will blow up in our face in some weird, esoteric condition. A single field in the C++ object is simple and predictable. A hash table is not simple and it is only mostly predictable: there are many footguns like poor choice of hash function/distribution of hashes, hash table load factor, cache behavior, probing/collision strategy, which all need to be handled. This is not to say a well written hash table cannot be made to work well in all cases, but if something goes wrong in some esoteric environment, we are far more likely to be able to figure it out if you have a simple implementation strategy instead of a complex one.Case in point, we would not have to do the analysis below if we were using a simple strategy; with a hash table, we have to think and benchmark. There is definite merit to avoiding thinking/benchmarking if it is not necessary.\n\n\n@colesbury requested information on the absolute overheads of the hash table lookup. ska::flat_hash_map is described in more detail at https://probablydance.com/2017/02/26/i-wrote-the-fastest-hashtable/ (though, unfortunately, he does not say what hardware he did his tests on.) @soumith mentioned that 100k live tensors is a workload we see in practice. Cross-referencing the blog post, and simply \"using my eyeball\" to compute what the extra numbers are, we see that we are looking at successful lookup in the range of 8-10ns (depending on if we are using prime numbers of power of two; I am not sure if we are allowed to use the power of two resizing policy with integer pointer hashes--an example of complexity) and you are just barely in the range where the hash table fits in cache; successful lookup with cache miss will be more like 25-30ns (this is about the same cost of a mutex lock/unlock (citing https://gist.github.com/hellerbarde/2843375#gistcomment-1896153), and Python function call overhead is something like 150ns (citing https://ilovesymposia.com/2015/12/10/the-cost-of-a-python-function-call/)). Unsuccessful lookup with cache miss will be 30-40ns. And insertion is 50-70ns and deletion 20-30ns.\nLet's discuss the average lifecycle of entries in this table. It will look something like this:\n\nA tensor gets created in C++\nIt moves to Python, at which point an unsuccessful lookup and subsequent insert occurs. (The cost here is rolled up with the cost of allocating the initial PyObject to represent the tensor.)\nPossibly, some number of times, another movement to Python happens, at which point a successful lookup occurs.\nAt some point, the Python object goes dead, and the destructor deletes the object from the hash table. We possibly go back to (2)\n\nGiven this lifecycle, ska::flat_hash_map is described as being optimized for fast lookup, may not be optimal for PyTorch, where most objects crossing the C-Python FFI boundary are new objects. To assess this, I added counters for the \"Wrapping var, existing/new one\" debug prints from @dzhulgakov's implementation and ran a single epoch of training with word_language_model on CUDA from https://github.com/pytorch/examples. I found that there were 138839 hits to existing variables versus 545558 fresh variables. That's 684397 Tensor crossings across Python-C++ in total, for a single epoch of training in a mid-size model.\n(more to come)", "body": "Before I get into more numbers, I want to preface this with more discussion about implicit PyTorch needs, which explain the intuitive aversion of many developers to using a hash table here.\r\n\r\n* Performance of wrapping tensors in PyTorch is critical for CPU NLP models in PyTorch, c.f. https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet (h/t @dzhulgakov for this text)\r\n* We want simple, predictable behavior in all cases. If something is simple and predictable, there is no chance that it will blow up in our face in some weird, esoteric condition. A single field in the C++ object is simple and predictable. A hash table is not simple and it is only mostly predictable: there are many footguns like poor choice of hash function/distribution of hashes, hash table load factor, cache behavior, probing/collision strategy, which all need to be handled. This is not to say a well written hash table cannot be made to work well in all cases, but if something goes wrong in some esoteric environment, we are far more likely to be able to figure it out if you have a simple implementation strategy instead of a complex one.Case in point, we would not have to do the analysis below if we were using a simple strategy; with a hash table, we have to think and benchmark. There is definite merit to avoiding thinking/benchmarking if it is not necessary.\r\n\r\n----\r\n\r\n@colesbury requested information on the absolute overheads of the hash table lookup. `ska::flat_hash_map` is described in more detail at https://probablydance.com/2017/02/26/i-wrote-the-fastest-hashtable/ (though, unfortunately, he does not say what hardware he did his tests on.) @soumith mentioned that 100k live tensors is a workload we see in practice. Cross-referencing the blog post, and simply \"using my eyeball\" to compute what the extra numbers are, we see that we are looking at **successful lookup in the range of 8-10ns** (depending on if we are using prime numbers of power of two; I am not sure if we are allowed to use the power of two resizing policy with integer pointer hashes--an example of complexity) and you are just barely in the range where the hash table fits in cache; **successful lookup with cache miss will be more like 25-30ns** (this is about the same cost of a mutex lock/unlock (citing https://gist.github.com/hellerbarde/2843375#gistcomment-1896153), and Python function call overhead is something like 150ns (citing https://ilovesymposia.com/2015/12/10/the-cost-of-a-python-function-call/)). **Unsuccessful lookup with cache miss will be 30-40ns.** And **insertion is 50-70ns** and **deletion 20-30ns**.\r\n\r\nLet's discuss the average lifecycle of entries in this table. It will look something like this:\r\n\r\n1. A tensor gets created in C++\r\n2. It moves to Python, at which point an unsuccessful lookup and subsequent insert occurs. (The cost here is rolled up with the cost of allocating the initial PyObject to represent the tensor.)\r\n3. *Possibly, some number of times*, another movement to Python happens, at which point a successful lookup occurs.\r\n4. At some point, the Python object goes dead, and the destructor deletes the object from the hash table. We possibly go back to (2)\r\n\r\nGiven this lifecycle, `ska::flat_hash_map` is described as being optimized for fast lookup, may not be optimal for PyTorch, where most objects crossing the C-Python FFI boundary are new objects. To assess this, I added counters for the \"Wrapping var, existing/new one\" debug prints from @dzhulgakov's implementation and ran a single epoch of training with `word_language_model` on CUDA from https://github.com/pytorch/examples. I found that there were 138839 hits to existing variables versus 545558 fresh variables. **That's 684397 Tensor crossings across Python-C++ in total, for a single epoch of training in a mid-size model.** \r\n\r\n(more to come)\r\n"}