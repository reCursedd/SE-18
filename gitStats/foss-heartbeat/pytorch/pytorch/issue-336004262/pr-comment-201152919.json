{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201152919", "pull_request_review_id": 135166498, "id": 201152919, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTE1MjkxOQ==", "diff_hunk": "@@ -0,0 +1,514 @@\n+#include \"TensorIterator.h\"\n+\n+#include <ATen/ExpandUtils.h>\n+#include <ATen/Parallel.h>\n+#include <time.h>\n+\n+namespace at {\n+\n+void TensorIterator::reorder_dimensions() {\n+  // Sort the dimensions based on the sum-of-strides in ascending order. NOTE:\n+  // that this inverts the order of C-contiguous tensors. strides[0] is the\n+  // fastest moving dimension instead of strides[ndim - 1].\n+\n+  auto sum_of_strides = SmallVector<double, 6>(ndim(), 0.0);\n+  for (int dim = 0; dim < ndim(); dim++) {\n+    double sum = 0.0;\n+    for (const auto& op : operands_) {\n+      if (op.stride_.size() == 0) continue;\n+      sum += op.stride_[dim];\n+    }\n+\n+    // Weight each dimension by its index. Given two dimensions with equal\n+    // some of strides, this preserves the given relative ordering.\n+    sum += (ndim() - dim - 1) / (double)ndim();\n+\n+    sum_of_strides[dim] = sum;\n+  }\n+\n+  // initialize perm with 0, 1, 2, ...\n+  perm_.resize(ndim());\n+  std::iota(std::begin(perm_), std::end(perm_), 0);\n+\n+  std::sort(std::begin(perm_), std::end(perm_), [&](size_t i1, size_t i2) {\n+    return sum_of_strides[i1] < sum_of_strides[i2];\n+  });\n+\n+  auto reorder = [](IntList data, IntList perm_) {\n+    auto res = DimVector(data.size(), 0);\n+    for (size_t i = 0; i < perm_.size(); i++) {\n+      res[i] = data[perm_[i]];\n+    }\n+    return res;\n+  };\n+\n+  // Update shape and strides\n+  shape_ = reorder(shape_, perm_);\n+  for (auto& op : operands_) {\n+    if (op.stride_.size() > 0) {\n+      op.stride_ = reorder(op.stride_, perm_);\n+    }\n+  }\n+}\n+\n+template <typename F>\n+static std::tuple<ScalarType, Backend>\n+compute_result_type(at::ArrayRef<OperandInfo> operands, const F& predicate) {\n+  auto result_type = ScalarType::Undefined;\n+  auto backend = Backend::Undefined;\n+  for (auto& op : operands) {\n+    if (!op.tensor_->defined()) continue;\n+    if (!predicate(*op.tensor_)) continue;\n+    auto dtype = op.tensor_->type().scalarType();;\n+    result_type = (result_type == ScalarType::Undefined\n+        ? dtype\n+        : promoteTypes(result_type, dtype));\n+    backend = (backend == Backend::Undefined\n+        ? op.tensor_->type().backend()\n+        : backend);\n+  }\n+  return std::make_tuple(result_type, backend);\n+}\n+\n+void TensorIterator::compute_common_type() {\n+  // The result dtype is computed with the precedence:\n+  // 1) Tensors of rank one or higher\n+  // 2) Tensors of any rank that aren't wrapped numbers\n+  // 3) Any tensor\n+  //\n+  // For example:\n+  //  torch.randn(3, 3) + torch.tensor([1.0], dtype=torch.double) -> torch.double\n+  //  torch.randn(3, 3) + torch.tensor(1.0, dtype=torch.double) -> torch.float\n+  //  torch.tensor(3, dtype=torch.uint8) + 5 -> torch.uint8\n+  //\n+  auto result_type = ScalarType::Undefined;\n+  auto backend = Backend::Undefined;\n+  std::tie(result_type, backend) = compute_result_type(operands_, [](const Tensor& t) {\n+    return t.dim() > 0;\n+  });\n+  if (result_type == ScalarType::Undefined) {\n+    std::tie(result_type, backend) = compute_result_type(operands_, [](const Tensor& t) {\n+      return !t.get()->is_wrapped_number();\n+    });\n+  }\n+  if (result_type == ScalarType::Undefined) {\n+    std::tie(result_type, backend) = compute_result_type(operands_, [](const Tensor& t) {\n+      return true;\n+    });\n+  }\n+\n+  AT_ASSERT(result_type != ScalarType::Undefined);\n+  AT_ASSERT(backend != Backend::Undefined);\n+\n+  auto& type = at::globalContext().getType(backend, result_type);\n+\n+  for (auto& op : operands_) {\n+    if (!op.type_) {\n+      op.type_ = &type;\n+      if (op.tensor_->defined() && type != op.tensor_->type()) {\n+        if (op.tensor_->dim() == 0) {\n+          if (type.backend() != at::kCUDA) {\n+            *op.tensor_ = op.tensor_->toType(type);\n+          }\n+        } else {\n+          op.needs_cast_ = true;\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+DimVector TensorIterator::compatible_stride(int element_size) const {\n+  auto stride = DimVector();\n+  stride.push_back(element_size);\n+  for (int i = 0; i < ndim() - 1; i++) {\n+    stride.push_back(shape_[i] * stride[i]);\n+  }\n+  return stride;\n+}\n+\n+DimVector TensorIterator::invert_perm(IntList input) const {\n+  // Invert the permutation caused by reorder_dimensions. This is not valid\n+  // after coalesce_dimensions is called.\n+  auto res = DimVector(input.size(), 0);\n+  for (int dim = 0; dim < ndim(); dim++) {\n+    res[perm_[dim]] = input[dim];\n+  }\n+  return res;\n+}\n+\n+void TensorIterator::allocate_outputs() {\n+  for (int i = 0; i < num_outputs_; i++) {\n+    auto& op = operands_[i];\n+    if (!op.tensor_->defined()) {\n+      int element_size = op.type_->elementSizeInBytes();\n+      op.stride_ = compatible_stride(element_size);\n+\n+      auto tensor_shape = invert_perm(shape_);\n+      auto tensor_stride = invert_perm(op.stride_);\n+      for (int dim = 0; dim < ndim(); dim++) {\n+        tensor_stride[dim] /= element_size;\n+      }\n+      *op.tensor_ = op.type_->tensor(tensor_shape, tensor_stride);\n+    }\n+  }\n+}\n+\n+void TensorIterator::coalesce_dimensions() {\n+  if (ndim() == 0) {\n+    return;\n+  }\n+\n+  // We can coalesce two adjacent dimensions if either dim has size 1 or if:\n+  // shape[n] * stride[n] == shape[n + 1].\n+  auto can_coalesce = [&](int dim0, int dim1) {\n+    auto shape0 = shape_[dim0];\n+    auto shape1 = shape_[dim1];\n+    if (shape0 == 1 || shape1 == 1) {\n+      return true;\n+    }\n+    for (int i = 0; i < ntensors(); i++) {\n+      auto& stride = operands_[i].stride_;\n+      if (shape0 * stride[dim0] != stride[dim1]) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  };\n+\n+  auto copy_strides = [&](int dim0, int dim1) {\n+    for (int i = 0; i < ntensors(); i++) {\n+      auto& stride = operands_[i].stride_;\n+      stride[dim0] = stride[dim1];\n+    }\n+  };\n+\n+  int prev_dim = 0;\n+  for (int dim = 1; dim < ndim(); dim++) {\n+    if (can_coalesce(prev_dim, dim)) {\n+      if (shape_[prev_dim] == 1) {\n+        copy_strides(prev_dim, dim);\n+      }\n+      shape_[prev_dim] *= shape_[dim];\n+    } else {\n+      prev_dim++;\n+      copy_strides(prev_dim, dim);\n+      shape_[prev_dim] = shape_[dim];\n+    }\n+  }\n+\n+  shape_.resize(prev_dim + 1);\n+  for (int i = 0; i < ntensors(); i++) {\n+    operands_[i].stride_.resize(ndim());\n+  }\n+}\n+\n+int64_t TensorIterator::numel() const {\n+  int64_t numel = 1;\n+  for (int64_t size : shape_) {\n+    numel *= size;\n+  }\n+  return numel;\n+}\n+\n+DimVector TensorIterator::get_inner_strides() const {\n+  auto dims = ndim();\n+  auto inner_strides = DimVector();\n+  for (auto& op : operands_) {\n+    inner_strides.push_back(dims == 0 ? 0 : op.stride_[0]);\n+  }\n+  return inner_strides;\n+}\n+\n+SmallVector<char*, 4> TensorIterator::get_data_ptrs(ArrayRef<char*> base, IntList counter) const {\n+  auto ptrs = SmallVector<char*, 4>(base);\n+  for (int i = 0; i < ntensors(); i++) {\n+    auto& stride = operands_[i].stride_;\n+    for (int dim = 0; dim < ndim(); dim++) {\n+      ptrs[i] += counter[dim] * stride[dim];\n+    }\n+  }\n+  return ptrs;\n+}\n+\n+SmallVector<char*, 4> TensorIterator::get_base_ptrs() const {\n+  auto ptrs = SmallVector<char*, 4>();\n+  for (int i = 0; i < ntensors(); i++) {\n+    ptrs.push_back((char*)data_ptr(i));\n+  }\n+  return ptrs;\n+}\n+\n+DimVector TensorIterator::make_counter(int64_t linear_offset) const {\n+  auto counter = DimVector();\n+  int64_t x = linear_offset;\n+  for (auto size : shape_) {\n+    counter.push_back(x % size);\n+    x /= size;\n+  }\n+  AT_ASSERT(x == 0);\n+  return counter;\n+}\n+\n+void TensorIterator::increment_counter(DimVector& counter, int64_t n) const {\n+  int64_t overflow = n;\n+  for (int i = 0; i < ndim(); i++) {\n+    auto size = shape_[i];\n+    auto value = counter[i];\n+    value += overflow;\n+    overflow = value / size;\n+    counter[i] = value % size;\n+  }\n+}\n+\n+void TensorIterator::for_each(loop_t loop) {\n+  auto inner_strides = get_inner_strides();\n+  auto base_ptrs = get_base_ptrs();\n+\n+  at::parallel_for(0, numel(), internal::GRAIN_SIZE, [&](int64_t begin, int64_t end) {\n+    serial_for_each(loop, base_ptrs, inner_strides, begin, end - begin);\n+  });\n+}\n+\n+void TensorIterator::serial_for_each(loop_t loop, ArrayRef<char*> base_ptrs, IntList inner_strides, int64_t start, int64_t n) {\n+  if (ndim() <= 1) {\n+    auto ptrs = get_data_ptrs(base_ptrs, { start });\n+    loop(ntensors(), ptrs.data(), inner_strides.data(), n);\n+  } else {\n+    auto counter = make_counter(start);\n+    while (n > 0) {\n+      auto ptrs = get_data_ptrs(base_ptrs, counter);\n+      int64_t loop_size = std::min(n, shape_[0] - counter[0]);\n+      loop(ntensors(), ptrs.data(), inner_strides.data(), loop_size);\n+      n -= loop_size;\n+      if (n == 0) break;\n+      increment_counter(counter, loop_size);\n+    }\n+  }\n+}\n+\n+bool TensorIterator::is_trivial_1d() const {\n+  // TODO: check for casting once it's supported\n+  return ndim() == 1;\n+}\n+\n+bool TensorIterator::is_scalar(int arg) const {\n+  const auto& stride = operands_[arg].stride_;\n+  for (int i = 0; i < ndim(); i++) {\n+    if (stride[i] != 0 && shape_[i] != 1) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+bool TensorIterator::is_cpu_scalar(int arg) const {\n+  return is_scalar(arg) && operands_[arg].tensor_->type().backend() == at::kCPU;\n+}\n+\n+void* TensorIterator::data_ptr(int arg) const {\n+  return operands_[arg].data_;\n+}\n+\n+void TensorIterator::remove_operand(int arg) {\n+  operands_.erase(operands_.begin() + arg);\n+}\n+\n+void TensorIterator::narrow(int dim, int64_t start, int64_t size) {\n+  AT_ASSERT(dim < ndim() && size >= 1);\n+  shape_[dim] = size;\n+  for (auto& op : operands_) {\n+    op.data_ = ((char*)op.data_) + op.stride_[dim] * start;\n+  }\n+  if (size == 1) {\n+    coalesce_dimensions();\n+  }\n+}\n+\n+std::unique_ptr<TensorIterator> TensorIterator::binary_op(const Tensor& a, const Tensor& b, Tensor& out) {\n+  auto builder = TensorIterator::Builder();\n+  builder.add_output(out);\n+  builder.add_input(a);\n+  builder.add_input(b);\n+  return builder.build();\n+}\n+\n+TensorIterator TensorIterator::reduce_op(const Tensor& a, IntList dims) {\n+  return TensorIterator();\n+}\n+\n+void TensorIterator::mark_outputs() {\n+  for (int i = 0; i < num_outputs_; i++) {\n+    operands_[i].is_output_ = true;\n+    auto output = *operands_[i].tensor_;\n+    if (!output.defined()) continue;\n+\n+    // check if output is also an input\n+    for (int arg = num_outputs_; arg < ntensors(); arg++) {\n+      auto input = *operands_[arg].tensor_;\n+      if (output.get() == input.get()) {\n+        operands_[i].is_read_write_ = true;\n+      }\n+    }\n+  }\n+}\n+\n+void TensorIterator::compute_shape() {\n+  for (auto& op : operands_) {\n+    if (!op.tensor_->defined()) continue;\n+\n+    // For now, don't include output tensors that are not also input tensors.\n+    // This preserves the legacy behavior where torch.add(..., out=dst) resizes\n+    // the destination tensor.\n+    if (op.is_output_ && !op.is_read_write_) continue;\n+\n+    auto shape = op.tensor_->sizes();\n+    if (shape_.empty()) {\n+      shape_ = shape;\n+    } else if (!shape.equals(shape_)) {\n+      shape_ = DimVector(infer_size(shape_, shape));\n+    }\n+  }\n+\n+  // Outputs cannot be broadcasted. Check that the shape of the outputs matches\n+  // the inferred shape. There's", "path": "aten/src/ATen/native/TensorIterator.cpp", "position": null, "original_position": 374, "commit_id": "b2efd9cc7a5dcff977b0be9aff2ca3bc0cfa159d", "original_commit_id": "38d31d836a8c373b2f34acc35953c2ad910bca3e", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "missing rest of comment?", "created_at": "2018-07-09T21:28:16Z", "updated_at": "2018-11-23T15:47:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201152919", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8919", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201152919"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201152919"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8919"}}, "body_html": "<p>missing rest of comment?</p>", "body_text": "missing rest of comment?"}