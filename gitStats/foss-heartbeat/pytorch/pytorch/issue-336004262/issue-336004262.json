{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8919", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8919/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8919/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8919/events", "html_url": "https://github.com/pytorch/pytorch/pull/8919", "id": 336004262, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk3NTYxODU1", "number": 8919, "title": "Implement add, sub, mul, div using TensorIterator", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-06-26T21:52:02Z", "updated_at": "2018-11-23T15:48:19Z", "closed_at": "2018-07-27T21:44:28Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8919", "html_url": "https://github.com/pytorch/pytorch/pull/8919", "diff_url": "https://github.com/pytorch/pytorch/pull/8919.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8919.patch"}, "body_html": "<pre><code>This adds TensorIterator, a helper class for computing element-wise\noperations that's intended to replace the CPU and CUDA apply utils\nfunctions.\n\nCPU kernels are implemented as functions that operate on strided 1-d\ntensors compared to CPUApplyUtils which operated individual elements. This\nallows the kernels to handle vectorization, while TensorIterator handles\nparallelization and non-coalesced dimensions.\n\nGPU kernels continue to operate on elements, but the number of\nspecializations is reduced. The contiguous case remains the same. The\nnon-contiguous case uses a single (reduced) shape for all operands and\nthe fast integer division from THCIntegerDivider. To avoid extra\nspecializations for indexing with 64-bits, large operations are split\ninto smaller operations that can be indexed with 32-bits.\n\nMajor semantic changes:\n\n - No more s_add, s_mul, s_div, or s_sub. Broadcasting is handled by\n   TensorIterator. The autograd engine performs the reduction assuming\n   standard broadcasting if the gradient shape does not match the\n   expected shape. Functions that do not use standard broadcasting rules\n   should either continue to trace the expand calls or handle the\n   reduction in their derivative formula.\n\n - Use ONNX v7, which supports broadcasting ops.\n\nPerformance impact:\n\n - Small increased fixed overhead (~0.5 us)\n - Larger overhead for wrapped numbers (~2.5 us)\n - No significant change for ops on contiguous tensors\n - Much faster worst-case performance for non-contiguous GPU tensors\n - Faster CPU bias addition (~2x)\n - Faster GPU bias addition (~30% faster)\n\nFuture work:\n\n - Decrease overhead, especially for wrapping numbers in Tensors\n - Handle general inter-type operations\n - Extend to unary ops and reductions\n - Use buffering for compute-bound operations on non-contiguous tensors\n   (pull in from CPUApplyUtils)\n</code></pre>", "body_text": "This adds TensorIterator, a helper class for computing element-wise\noperations that's intended to replace the CPU and CUDA apply utils\nfunctions.\n\nCPU kernels are implemented as functions that operate on strided 1-d\ntensors compared to CPUApplyUtils which operated individual elements. This\nallows the kernels to handle vectorization, while TensorIterator handles\nparallelization and non-coalesced dimensions.\n\nGPU kernels continue to operate on elements, but the number of\nspecializations is reduced. The contiguous case remains the same. The\nnon-contiguous case uses a single (reduced) shape for all operands and\nthe fast integer division from THCIntegerDivider. To avoid extra\nspecializations for indexing with 64-bits, large operations are split\ninto smaller operations that can be indexed with 32-bits.\n\nMajor semantic changes:\n\n - No more s_add, s_mul, s_div, or s_sub. Broadcasting is handled by\n   TensorIterator. The autograd engine performs the reduction assuming\n   standard broadcasting if the gradient shape does not match the\n   expected shape. Functions that do not use standard broadcasting rules\n   should either continue to trace the expand calls or handle the\n   reduction in their derivative formula.\n\n - Use ONNX v7, which supports broadcasting ops.\n\nPerformance impact:\n\n - Small increased fixed overhead (~0.5 us)\n - Larger overhead for wrapped numbers (~2.5 us)\n - No significant change for ops on contiguous tensors\n - Much faster worst-case performance for non-contiguous GPU tensors\n - Faster CPU bias addition (~2x)\n - Faster GPU bias addition (~30% faster)\n\nFuture work:\n\n - Decrease overhead, especially for wrapping numbers in Tensors\n - Handle general inter-type operations\n - Extend to unary ops and reductions\n - Use buffering for compute-bound operations on non-contiguous tensors\n   (pull in from CPUApplyUtils)", "body": "```\r\nThis adds TensorIterator, a helper class for computing element-wise\r\noperations that's intended to replace the CPU and CUDA apply utils\r\nfunctions.\r\n\r\nCPU kernels are implemented as functions that operate on strided 1-d\r\ntensors compared to CPUApplyUtils which operated individual elements. This\r\nallows the kernels to handle vectorization, while TensorIterator handles\r\nparallelization and non-coalesced dimensions.\r\n\r\nGPU kernels continue to operate on elements, but the number of\r\nspecializations is reduced. The contiguous case remains the same. The\r\nnon-contiguous case uses a single (reduced) shape for all operands and\r\nthe fast integer division from THCIntegerDivider. To avoid extra\r\nspecializations for indexing with 64-bits, large operations are split\r\ninto smaller operations that can be indexed with 32-bits.\r\n\r\nMajor semantic changes:\r\n\r\n - No more s_add, s_mul, s_div, or s_sub. Broadcasting is handled by\r\n   TensorIterator. The autograd engine performs the reduction assuming\r\n   standard broadcasting if the gradient shape does not match the\r\n   expected shape. Functions that do not use standard broadcasting rules\r\n   should either continue to trace the expand calls or handle the\r\n   reduction in their derivative formula.\r\n\r\n - Use ONNX v7, which supports broadcasting ops.\r\n\r\nPerformance impact:\r\n\r\n - Small increased fixed overhead (~0.5 us)\r\n - Larger overhead for wrapped numbers (~2.5 us)\r\n - No significant change for ops on contiguous tensors\r\n - Much faster worst-case performance for non-contiguous GPU tensors\r\n - Faster CPU bias addition (~2x)\r\n - Faster GPU bias addition (~30% faster)\r\n\r\nFuture work:\r\n\r\n - Decrease overhead, especially for wrapping numbers in Tensors\r\n - Handle general inter-type operations\r\n - Extend to unary ops and reductions\r\n - Use buffering for compute-bound operations on non-contiguous tensors\r\n   (pull in from CPUApplyUtils)\r\n```"}