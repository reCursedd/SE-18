{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201137502", "pull_request_review_id": 135166498, "id": 201137502, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTEzNzUwMg==", "diff_hunk": "@@ -0,0 +1,213 @@\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+#include <ATen/SmallVector.h>\n+#include <ATen/optional.h>\n+#include <ATen/detail/ScalarTypeConversions.h>\n+\n+// TensorIterator is a helper class for element-wise operations, such as\n+// arithmetic, comparisions, and trigonometric functions. It handles\n+// broadcasting and type conversions of operands.\n+//\n+// The files Loops.h and Loops.cuh provide functions to build kernels that\n+// use TensorIterator.\n+//\n+// Example:\n+//\n+//   auto iter = TensorIterator::Builder()\n+//      .add_output(output)\n+//      .add_input(input)\n+//      .build()\n+//\n+// [MyKernel.cpp / MyKernel.cu]\n+//   cpu_binary_kernel(iter, [](float a, float b) {\n+//     return a + b;\n+//   });\n+//\n+//   gpu_binary_kernel(iter, []GPU_LAMBDA(float a, float b) -> float {\n+//     return a + b;\n+//   });\n+//\n+// This inspired by NumPy's Array Iterator API (NpyIter).\n+\n+namespace at {\n+\n+struct OperandInfo {\n+  OperandInfo() {}\n+  OperandInfo(const Tensor& t) : tensor_(const_cast<Tensor*>(&t)) {}\n+\n+  /// Stride after broadcasting. The stride is in bytes, not number of elements.\n+  DimVector stride_;\n+\n+  /// The original tensor operand. Note that the strides, data pointer, and\n+  /// other attributes may differ from due to dimension reordering and\n+  /// coalescing.\n+  Tensor* tensor_;\n+\n+  /// The desired type for the operand. This may be different from the actual\n+  /// tensor type, in which case casting is necessary.\n+  Type* type_ = nullptr;\n+\n+  /// The data pointer. This may be different from tensor.data_ptr() if the\n+  /// iterator is split.\n+  void* data_ = nullptr;\n+\n+  /// True if the kernel needs to handle a cast operation for this operand.\n+  bool needs_cast_ = false;\n+\n+  bool is_output_ = false;\n+\n+  bool is_read_write_ = false;\n+};\n+\n+enum class IteratorFlags {\n+  COMMON_DTYPE = 1,\n+  ALLOW_CPU_SCALARS = 2,\n+};\n+\n+struct SplitUntil32Bit;\n+\n+struct TensorIterator {\n+  struct Builder;\n+  friend struct Builder;\n+\n+  TensorIterator() {}\n+\n+  using loop_t = const std::function<void(int, char**, const int64_t*, int64_t)>&;\n+\n+  static std::unique_ptr<TensorIterator> binary_op(const Tensor& a, const Tensor& b, Tensor& out);\n+  static TensorIterator reduce_op(const Tensor& a, IntList dims);\n+\n+  int ndim() const { return shape_.size(); }", "path": "aten/src/ATen/native/TensorIterator.h", "position": null, "original_position": 81, "commit_id": "b2efd9cc7a5dcff977b0be9aff2ca3bc0cfa159d", "original_commit_id": "38d31d836a8c373b2f34acc35953c2ad910bca3e", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "can you use dim?  We have dim, ndimension, and now ndim?", "created_at": "2018-07-09T20:32:50Z", "updated_at": "2018-11-23T15:47:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201137502", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8919", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201137502"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201137502"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8919"}}, "body_html": "<p>can you use dim?  We have dim, ndimension, and now ndim?</p>", "body_text": "can you use dim?  We have dim, ndimension, and now ndim?"}