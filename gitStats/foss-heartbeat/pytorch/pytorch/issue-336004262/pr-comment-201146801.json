{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201146801", "pull_request_review_id": 135166498, "id": 201146801, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTE0NjgwMQ==", "diff_hunk": "@@ -0,0 +1,514 @@\n+#include \"TensorIterator.h\"\n+\n+#include <ATen/ExpandUtils.h>\n+#include <ATen/Parallel.h>\n+#include <time.h>\n+\n+namespace at {\n+\n+void TensorIterator::reorder_dimensions() {\n+  // Sort the dimensions based on the sum-of-strides in ascending order. NOTE:\n+  // that this inverts the order of C-contiguous tensors. strides[0] is the\n+  // fastest moving dimension instead of strides[ndim - 1].\n+\n+  auto sum_of_strides = SmallVector<double, 6>(ndim(), 0.0);\n+  for (int dim = 0; dim < ndim(); dim++) {\n+    double sum = 0.0;\n+    for (const auto& op : operands_) {\n+      if (op.stride_.size() == 0) continue;\n+      sum += op.stride_[dim];\n+    }\n+\n+    // Weight each dimension by its index. Given two dimensions with equal\n+    // some of strides, this preserves the given relative ordering.\n+    sum += (ndim() - dim - 1) / (double)ndim();\n+\n+    sum_of_strides[dim] = sum;\n+  }\n+\n+  // initialize perm with 0, 1, 2, ...\n+  perm_.resize(ndim());\n+  std::iota(std::begin(perm_), std::end(perm_), 0);\n+\n+  std::sort(std::begin(perm_), std::end(perm_), [&](size_t i1, size_t i2) {\n+    return sum_of_strides[i1] < sum_of_strides[i2];\n+  });\n+\n+  auto reorder = [](IntList data, IntList perm_) {\n+    auto res = DimVector(data.size(), 0);\n+    for (size_t i = 0; i < perm_.size(); i++) {\n+      res[i] = data[perm_[i]];\n+    }\n+    return res;\n+  };\n+\n+  // Update shape and strides\n+  shape_ = reorder(shape_, perm_);\n+  for (auto& op : operands_) {\n+    if (op.stride_.size() > 0) {\n+      op.stride_ = reorder(op.stride_, perm_);\n+    }\n+  }\n+}\n+\n+template <typename F>\n+static std::tuple<ScalarType, Backend>\n+compute_result_type(at::ArrayRef<OperandInfo> operands, const F& predicate) {\n+  auto result_type = ScalarType::Undefined;\n+  auto backend = Backend::Undefined;\n+  for (auto& op : operands) {\n+    if (!op.tensor_->defined()) continue;\n+    if (!predicate(*op.tensor_)) continue;\n+    auto dtype = op.tensor_->type().scalarType();;\n+    result_type = (result_type == ScalarType::Undefined\n+        ? dtype\n+        : promoteTypes(result_type, dtype));\n+    backend = (backend == Backend::Undefined\n+        ? op.tensor_->type().backend()\n+        : backend);\n+  }\n+  return std::make_tuple(result_type, backend);\n+}\n+\n+void TensorIterator::compute_common_type() {\n+  // The result dtype is computed with the precedence:\n+  // 1) Tensors of rank one or higher\n+  // 2) Tensors of any rank that aren't wrapped numbers\n+  // 3) Any tensor\n+  //\n+  // For example:\n+  //  torch.randn(3, 3) + torch.tensor([1.0], dtype=torch.double) -> torch.double", "path": "aten/src/ATen/native/TensorIterator.cpp", "position": null, "original_position": 80, "commit_id": "b2efd9cc7a5dcff977b0be9aff2ca3bc0cfa159d", "original_commit_id": "38d31d836a8c373b2f34acc35953c2ad910bca3e", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "the LHS here and below don't specify a dtype (it depends on what the default is set to).", "created_at": "2018-07-09T21:05:35Z", "updated_at": "2018-11-23T15:47:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201146801", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8919", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201146801"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201146801"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8919"}}, "body_html": "<p>the LHS here and below don't specify a dtype (it depends on what the default is set to).</p>", "body_text": "the LHS here and below don't specify a dtype (it depends on what the default is set to)."}