{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201466635", "pull_request_review_id": 135943123, "id": 201466635, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTQ2NjYzNQ==", "diff_hunk": "@@ -0,0 +1,514 @@\n+#include \"TensorIterator.h\"\n+\n+#include <ATen/ExpandUtils.h>\n+#include <ATen/Parallel.h>\n+#include <time.h>\n+\n+namespace at {\n+\n+void TensorIterator::reorder_dimensions() {\n+  // Sort the dimensions based on the sum-of-strides in ascending order. NOTE:\n+  // that this inverts the order of C-contiguous tensors. strides[0] is the\n+  // fastest moving dimension instead of strides[ndim - 1].\n+\n+  auto sum_of_strides = SmallVector<double, 6>(ndim(), 0.0);\n+  for (int dim = 0; dim < ndim(); dim++) {\n+    double sum = 0.0;\n+    for (const auto& op : operands_) {\n+      if (op.stride_.size() == 0) continue;\n+      sum += op.stride_[dim];\n+    }\n+\n+    // Weight each dimension by its index. Given two dimensions with equal\n+    // some of strides, this preserves the given relative ordering.\n+    sum += (ndim() - dim - 1) / (double)ndim();\n+\n+    sum_of_strides[dim] = sum;\n+  }\n+\n+  // initialize perm with 0, 1, 2, ...\n+  perm_.resize(ndim());\n+  std::iota(std::begin(perm_), std::end(perm_), 0);\n+\n+  std::sort(std::begin(perm_), std::end(perm_), [&](size_t i1, size_t i2) {\n+    return sum_of_strides[i1] < sum_of_strides[i2];\n+  });\n+\n+  auto reorder = [](IntList data, IntList perm_) {\n+    auto res = DimVector(data.size(), 0);\n+    for (size_t i = 0; i < perm_.size(); i++) {\n+      res[i] = data[perm_[i]];\n+    }\n+    return res;\n+  };\n+\n+  // Update shape and strides\n+  shape_ = reorder(shape_, perm_);\n+  for (auto& op : operands_) {\n+    if (op.stride_.size() > 0) {\n+      op.stride_ = reorder(op.stride_, perm_);\n+    }\n+  }\n+}\n+\n+template <typename F>\n+static std::tuple<ScalarType, Backend>\n+compute_result_type(at::ArrayRef<OperandInfo> operands, const F& predicate) {\n+  auto result_type = ScalarType::Undefined;\n+  auto backend = Backend::Undefined;\n+  for (auto& op : operands) {\n+    if (!op.tensor_->defined()) continue;\n+    if (!predicate(*op.tensor_)) continue;\n+    auto dtype = op.tensor_->type().scalarType();;\n+    result_type = (result_type == ScalarType::Undefined\n+        ? dtype\n+        : promoteTypes(result_type, dtype));\n+    backend = (backend == Backend::Undefined\n+        ? op.tensor_->type().backend()\n+        : backend);\n+  }\n+  return std::make_tuple(result_type, backend);\n+}\n+\n+void TensorIterator::compute_common_type() {\n+  // The result dtype is computed with the precedence:\n+  // 1) Tensors of rank one or higher\n+  // 2) Tensors of any rank that aren't wrapped numbers\n+  // 3) Any tensor\n+  //\n+  // For example:\n+  //  torch.randn(3, 3) + torch.tensor([1.0], dtype=torch.double) -> torch.double\n+  //  torch.randn(3, 3) + torch.tensor(1.0, dtype=torch.double) -> torch.float\n+  //  torch.tensor(3, dtype=torch.uint8) + 5 -> torch.uint8\n+  //\n+  auto result_type = ScalarType::Undefined;\n+  auto backend = Backend::Undefined;\n+  std::tie(result_type, backend) = compute_result_type(operands_, [](const Tensor& t) {\n+    return t.dim() > 0;\n+  });\n+  if (result_type == ScalarType::Undefined) {\n+    std::tie(result_type, backend) = compute_result_type(operands_, [](const Tensor& t) {\n+      return !t.get()->is_wrapped_number();\n+    });\n+  }\n+  if (result_type == ScalarType::Undefined) {\n+    std::tie(result_type, backend) = compute_result_type(operands_, [](const Tensor& t) {\n+      return true;\n+    });\n+  }\n+\n+  AT_ASSERT(result_type != ScalarType::Undefined);\n+  AT_ASSERT(backend != Backend::Undefined);\n+\n+  auto& type = at::globalContext().getType(backend, result_type);\n+\n+  for (auto& op : operands_) {\n+    if (!op.type_) {\n+      op.type_ = &type;\n+      if (op.tensor_->defined() && type != op.tensor_->type()) {\n+        if (op.tensor_->dim() == 0) {\n+          if (type.backend() != at::kCUDA) {\n+            *op.tensor_ = op.tensor_->toType(type);\n+          }\n+        } else {\n+          op.needs_cast_ = true;\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+DimVector TensorIterator::compatible_stride(int element_size) const {\n+  auto stride = DimVector();\n+  stride.push_back(element_size);\n+  for (int i = 0; i < ndim() - 1; i++) {\n+    stride.push_back(shape_[i] * stride[i]);\n+  }\n+  return stride;\n+}\n+\n+DimVector TensorIterator::invert_perm(IntList input) const {\n+  // Invert the permutation caused by reorder_dimensions. This is not valid\n+  // after coalesce_dimensions is called.\n+  auto res = DimVector(input.size(), 0);\n+  for (int dim = 0; dim < ndim(); dim++) {\n+    res[perm_[dim]] = input[dim];\n+  }\n+  return res;\n+}\n+\n+void TensorIterator::allocate_outputs() {\n+  for (int i = 0; i < num_outputs_; i++) {\n+    auto& op = operands_[i];\n+    if (!op.tensor_->defined()) {\n+      int element_size = op.type_->elementSizeInBytes();\n+      op.stride_ = compatible_stride(element_size);\n+\n+      auto tensor_shape = invert_perm(shape_);\n+      auto tensor_stride = invert_perm(op.stride_);\n+      for (int dim = 0; dim < ndim(); dim++) {\n+        tensor_stride[dim] /= element_size;\n+      }\n+      *op.tensor_ = op.type_->tensor(tensor_shape, tensor_stride);\n+    }\n+  }\n+}\n+\n+void TensorIterator::coalesce_dimensions() {\n+  if (ndim() == 0) {\n+    return;\n+  }\n+\n+  // We can coalesce two adjacent dimensions if either dim has size 1 or if:\n+  // shape[n] * stride[n] == shape[n + 1].\n+  auto can_coalesce = [&](int dim0, int dim1) {\n+    auto shape0 = shape_[dim0];\n+    auto shape1 = shape_[dim1];\n+    if (shape0 == 1 || shape1 == 1) {\n+      return true;\n+    }\n+    for (int i = 0; i < ntensors(); i++) {\n+      auto& stride = operands_[i].stride_;\n+      if (shape0 * stride[dim0] != stride[dim1]) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  };\n+\n+  auto copy_strides = [&](int dim0, int dim1) {", "path": "aten/src/ATen/native/TensorIterator.cpp", "position": null, "original_position": 179, "commit_id": "b2efd9cc7a5dcff977b0be9aff2ca3bc0cfa159d", "original_commit_id": "38d31d836a8c373b2f34acc35953c2ad910bca3e", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I renamed it `replace_stride` since it replaces the stride at dim0 with the stride at dim1. I didn't like \"coalesce\" because it doesn't combine the strides, it just overwrites one of them.", "created_at": "2018-07-10T19:31:05Z", "updated_at": "2018-11-23T15:47:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201466635", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8919", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201466635"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201466635"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8919"}}, "body_html": "<p>I renamed it <code>replace_stride</code> since it replaces the stride at dim0 with the stride at dim1. I didn't like \"coalesce\" because it doesn't combine the strides, it just overwrites one of them.</p>", "body_text": "I renamed it replace_stride since it replaces the stride at dim0 with the stride at dim1. I didn't like \"coalesce\" because it doesn't combine the strides, it just overwrites one of them.", "in_reply_to_id": 201151625}