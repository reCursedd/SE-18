{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201439937", "pull_request_review_id": 135943123, "id": 201439937, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTQzOTkzNw==", "diff_hunk": "@@ -65,6 +65,18 @@ struct TensorImpl : public Retainable {\n     is_scalar = s;\n   }\n \n+  // True if a tensor was auto-wrapped from a C++ or Python number.\n+  // Wrapped numbers are considered \"lower priority\" when computing the result", "path": "aten/src/ATen/TensorImpl.h", "position": null, "original_position": 5, "commit_id": "b2efd9cc7a5dcff977b0be9aff2ca3bc0cfa159d", "original_commit_id": "38d31d836a8c373b2f34acc35953c2ad910bca3e", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I'll describe the type promotion logic in  TensorIterator.h. Currently, `byte_tensor + 2.5` gives you a `byte_tensor` (2.5 is truncated to 2). This PR preserves that behavior. Eventually, I think we should promote `byte_tensor` to a floating-point tensor so that `byte_tensor + 2.5` gives you a `FloatTensor` or `DoubleTensor` depending your default tensor type.", "created_at": "2018-07-10T18:02:16Z", "updated_at": "2018-11-23T15:47:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201439937", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8919", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201439937"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8919#discussion_r201439937"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8919"}}, "body_html": "<p>I'll describe the type promotion logic in  TensorIterator.h. Currently, <code>byte_tensor + 2.5</code> gives you a <code>byte_tensor</code> (2.5 is truncated to 2). This PR preserves that behavior. Eventually, I think we should promote <code>byte_tensor</code> to a floating-point tensor so that <code>byte_tensor + 2.5</code> gives you a <code>FloatTensor</code> or <code>DoubleTensor</code> depending your default tensor type.</p>", "body_text": "I'll describe the type promotion logic in  TensorIterator.h. Currently, byte_tensor + 2.5 gives you a byte_tensor (2.5 is truncated to 2). This PR preserves that behavior. Eventually, I think we should promote byte_tensor to a floating-point tensor so that byte_tensor + 2.5 gives you a FloatTensor or DoubleTensor depending your default tensor type.", "in_reply_to_id": 200679738}