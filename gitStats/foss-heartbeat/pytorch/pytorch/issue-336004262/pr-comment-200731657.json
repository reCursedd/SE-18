{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200731657", "pull_request_review_id": 135105270, "id": 200731657, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDczMTY1Nw==", "diff_hunk": "@@ -65,6 +65,18 @@ struct TensorImpl : public Retainable {\n     is_scalar = s;\n   }\n \n+  // True if a tensor was auto-wrapped from a C++ or Python number.\n+  // Wrapped numbers are considered \"lower priority\" when computing the result", "path": "aten/src/ATen/TensorImpl.h", "position": null, "original_position": 5, "commit_id": "b2efd9cc7a5dcff977b0be9aff2ca3bc0cfa159d", "original_commit_id": "38d31d836a8c373b2f34acc35953c2ad910bca3e", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Wrapped numbers do not participate in the result type calculation if there are any Tensors that are not wrapped numbers. (I'll improve the comment).\r\n\r\nThis is intended to keep backwards compatibility. We wrap all integers as 0-d LongTensors and Python floats as DoubleTensors, but we don't want to promote `byte_tensor + 5` to long or `float_tensor + 3.5` to double.\r\n\r\nNumPy result_type calculation is a bit different. I'm not sure we want to use it because it promotes float arrays to double arrays in more cases than we would like.", "created_at": "2018-07-06T18:12:08Z", "updated_at": "2018-11-23T15:46:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/8919#discussion_r200731657", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8919", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200731657"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8919#discussion_r200731657"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8919"}}, "body_html": "<p>Wrapped numbers do not participate in the result type calculation if there are any Tensors that are not wrapped numbers. (I'll improve the comment).</p>\n<p>This is intended to keep backwards compatibility. We wrap all integers as 0-d LongTensors and Python floats as DoubleTensors, but we don't want to promote <code>byte_tensor + 5</code> to long or <code>float_tensor + 3.5</code> to double.</p>\n<p>NumPy result_type calculation is a bit different. I'm not sure we want to use it because it promotes float arrays to double arrays in more cases than we would like.</p>", "body_text": "Wrapped numbers do not participate in the result type calculation if there are any Tensors that are not wrapped numbers. (I'll improve the comment).\nThis is intended to keep backwards compatibility. We wrap all integers as 0-d LongTensors and Python floats as DoubleTensors, but we don't want to promote byte_tensor + 5 to long or float_tensor + 3.5 to double.\nNumPy result_type calculation is a bit different. I'm not sure we want to use it because it promotes float arrays to double arrays in more cases than we would like.", "in_reply_to_id": 200679738}