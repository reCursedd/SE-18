{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/353650956", "html_url": "https://github.com/pytorch/pytorch/issues/4260#issuecomment-353650956", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4260", "id": 353650956, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzY1MDk1Ng==", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-22T18:22:07Z", "updated_at": "2017-12-22T18:22:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>One of the <code>test_distributions.py</code> tests is now failing due to refactoring in the last few days. It would have been caught by CI.</p>\n<details>\n<pre><code>=================================================== FAILURES ====================================================\n___________________________________ TestDistributions.test_gamma_sample_grad ____________________________________\n\nself = &lt;test_distributions.TestDistributions testMethod=test_gamma_sample_grad&gt;\n\n    @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n    def test_gamma_sample_grad(self):\n        self._set_rng_seed(1)\n        num_samples = 100\n        for alpha in [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]:\n            alphas = Variable(torch.Tensor([alpha] * num_samples), requires_grad=True)\n            betas = Variable(torch.ones(num_samples))\n            x = Gamma(alphas, betas).rsample()\n&gt;           x.sum().backward()\n\ntest/test_distributions.py:415:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntorch/autograd/variable.py:103: in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nvariables = (Variable containing:\n1.00000e-07 *\n  3.8609\n[torch.DoubleTensor of size 1]\n,)\ngrad_variables = (Variable containing:\n 1\n[torch.DoubleTensor of size 1]\n,), retain_graph = False\ncreate_graph = False\n\n    def backward(variables, grad_variables=None, retain_graph=None, create_graph=False):\n        \"\"\"Computes the sum of gradients of given variables w.r.t. graph leaves.\n\n        The graph is differentiated using the chain rule. If any of ``variables``\n        are non-scalar (i.e. their data has more than one element) and require\n        gradient, the function additionally requires specifying ``grad_variables``.\n        It should be a sequence of matching length, that contains gradient of\n        the differentiated function w.r.t. corresponding variables (``None`` is an\n        acceptable value for all variables that don't need gradient tensors).\n\n        This function accumulates gradients in the leaves - you might need to zero\n        them before calling it.\n\n        Arguments:\n            variables (sequence of Variable): Variables of which the derivative will be\n                computed.\n            grad_variables (sequence of (Tensor, Variable or None)): Gradients w.r.t.\n                each element of corresponding variables.  Any tensors will be\n                automatically converted to Variables that are volatile unless\n                ``create_graph`` is ``True``.  None values can be specified for scalar\n                Variables or ones that don't require grad. If a None value would\n                be acceptable for all grad_variables, then this argument is optional.\n            retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n                will be freed. Note that in nearly all cases setting this option to ``True``\n                is not needed and often can be worked around in a much more efficient\n                way. Defaults to the value of ``create_graph``.\n            create_graph (bool, optional): If ``True``, graph of the derivative will\n                be constructed, allowing to compute higher order derivative products.\n                Defaults to ``False``.\n        \"\"\"\n        variables = (variables,) if isinstance(variables, Variable) else tuple(variables)\n\n        if grad_variables is None:\n            grad_variables = [None] * len(variables)\n        elif isinstance(grad_variables, Variable) or torch.is_tensor(grad_variables):\n            grad_variables = [grad_variables]\n        else:\n            grad_variables = list(grad_variables)\n\n        grad_variables = _make_grads(variables, grad_variables)\n        if retain_graph is None:\n            retain_graph = create_graph\n\n        Variable._execution_engine.run_backward(\n&gt;           variables, grad_variables, retain_graph, create_graph)\nE       RuntimeError: VariableType::_standard_gamma_grad NYI\n\ntorch/autograd/__init__.py:83: RuntimeError\n</code></pre>\n</details>", "body_text": "One of the test_distributions.py tests is now failing due to refactoring in the last few days. It would have been caught by CI.\n\n=================================================== FAILURES ====================================================\n___________________________________ TestDistributions.test_gamma_sample_grad ____________________________________\n\nself = <test_distributions.TestDistributions testMethod=test_gamma_sample_grad>\n\n    @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\n    def test_gamma_sample_grad(self):\n        self._set_rng_seed(1)\n        num_samples = 100\n        for alpha in [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]:\n            alphas = Variable(torch.Tensor([alpha] * num_samples), requires_grad=True)\n            betas = Variable(torch.ones(num_samples))\n            x = Gamma(alphas, betas).rsample()\n>           x.sum().backward()\n\ntest/test_distributions.py:415:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntorch/autograd/variable.py:103: in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nvariables = (Variable containing:\n1.00000e-07 *\n  3.8609\n[torch.DoubleTensor of size 1]\n,)\ngrad_variables = (Variable containing:\n 1\n[torch.DoubleTensor of size 1]\n,), retain_graph = False\ncreate_graph = False\n\n    def backward(variables, grad_variables=None, retain_graph=None, create_graph=False):\n        \"\"\"Computes the sum of gradients of given variables w.r.t. graph leaves.\n\n        The graph is differentiated using the chain rule. If any of ``variables``\n        are non-scalar (i.e. their data has more than one element) and require\n        gradient, the function additionally requires specifying ``grad_variables``.\n        It should be a sequence of matching length, that contains gradient of\n        the differentiated function w.r.t. corresponding variables (``None`` is an\n        acceptable value for all variables that don't need gradient tensors).\n\n        This function accumulates gradients in the leaves - you might need to zero\n        them before calling it.\n\n        Arguments:\n            variables (sequence of Variable): Variables of which the derivative will be\n                computed.\n            grad_variables (sequence of (Tensor, Variable or None)): Gradients w.r.t.\n                each element of corresponding variables.  Any tensors will be\n                automatically converted to Variables that are volatile unless\n                ``create_graph`` is ``True``.  None values can be specified for scalar\n                Variables or ones that don't require grad. If a None value would\n                be acceptable for all grad_variables, then this argument is optional.\n            retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n                will be freed. Note that in nearly all cases setting this option to ``True``\n                is not needed and often can be worked around in a much more efficient\n                way. Defaults to the value of ``create_graph``.\n            create_graph (bool, optional): If ``True``, graph of the derivative will\n                be constructed, allowing to compute higher order derivative products.\n                Defaults to ``False``.\n        \"\"\"\n        variables = (variables,) if isinstance(variables, Variable) else tuple(variables)\n\n        if grad_variables is None:\n            grad_variables = [None] * len(variables)\n        elif isinstance(grad_variables, Variable) or torch.is_tensor(grad_variables):\n            grad_variables = [grad_variables]\n        else:\n            grad_variables = list(grad_variables)\n\n        grad_variables = _make_grads(variables, grad_variables)\n        if retain_graph is None:\n            retain_graph = create_graph\n\n        Variable._execution_engine.run_backward(\n>           variables, grad_variables, retain_graph, create_graph)\nE       RuntimeError: VariableType::_standard_gamma_grad NYI\n\ntorch/autograd/__init__.py:83: RuntimeError", "body": "One of the `test_distributions.py` tests is now failing due to refactoring in the last few days. It would have been caught by CI.\r\n<details>\r\n\r\n```\r\n=================================================== FAILURES ====================================================\r\n___________________________________ TestDistributions.test_gamma_sample_grad ____________________________________\r\n\r\nself = <test_distributions.TestDistributions testMethod=test_gamma_sample_grad>\r\n\r\n    @unittest.skipIf(not TEST_NUMPY, \"Numpy not found\")\r\n    def test_gamma_sample_grad(self):\r\n        self._set_rng_seed(1)\r\n        num_samples = 100\r\n        for alpha in [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]:\r\n            alphas = Variable(torch.Tensor([alpha] * num_samples), requires_grad=True)\r\n            betas = Variable(torch.ones(num_samples))\r\n            x = Gamma(alphas, betas).rsample()\r\n>           x.sum().backward()\r\n\r\ntest/test_distributions.py:415:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\ntorch/autograd/variable.py:103: in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nvariables = (Variable containing:\r\n1.00000e-07 *\r\n  3.8609\r\n[torch.DoubleTensor of size 1]\r\n,)\r\ngrad_variables = (Variable containing:\r\n 1\r\n[torch.DoubleTensor of size 1]\r\n,), retain_graph = False\r\ncreate_graph = False\r\n\r\n    def backward(variables, grad_variables=None, retain_graph=None, create_graph=False):\r\n        \"\"\"Computes the sum of gradients of given variables w.r.t. graph leaves.\r\n\r\n        The graph is differentiated using the chain rule. If any of ``variables``\r\n        are non-scalar (i.e. their data has more than one element) and require\r\n        gradient, the function additionally requires specifying ``grad_variables``.\r\n        It should be a sequence of matching length, that contains gradient of\r\n        the differentiated function w.r.t. corresponding variables (``None`` is an\r\n        acceptable value for all variables that don't need gradient tensors).\r\n\r\n        This function accumulates gradients in the leaves - you might need to zero\r\n        them before calling it.\r\n\r\n        Arguments:\r\n            variables (sequence of Variable): Variables of which the derivative will be\r\n                computed.\r\n            grad_variables (sequence of (Tensor, Variable or None)): Gradients w.r.t.\r\n                each element of corresponding variables.  Any tensors will be\r\n                automatically converted to Variables that are volatile unless\r\n                ``create_graph`` is ``True``.  None values can be specified for scalar\r\n                Variables or ones that don't require grad. If a None value would\r\n                be acceptable for all grad_variables, then this argument is optional.\r\n            retain_graph (bool, optional): If ``False``, the graph used to compute the grad\r\n                will be freed. Note that in nearly all cases setting this option to ``True``\r\n                is not needed and often can be worked around in a much more efficient\r\n                way. Defaults to the value of ``create_graph``.\r\n            create_graph (bool, optional): If ``True``, graph of the derivative will\r\n                be constructed, allowing to compute higher order derivative products.\r\n                Defaults to ``False``.\r\n        \"\"\"\r\n        variables = (variables,) if isinstance(variables, Variable) else tuple(variables)\r\n\r\n        if grad_variables is None:\r\n            grad_variables = [None] * len(variables)\r\n        elif isinstance(grad_variables, Variable) or torch.is_tensor(grad_variables):\r\n            grad_variables = [grad_variables]\r\n        else:\r\n            grad_variables = list(grad_variables)\r\n\r\n        grad_variables = _make_grads(variables, grad_variables)\r\n        if retain_graph is None:\r\n            retain_graph = create_graph\r\n\r\n        Variable._execution_engine.run_backward(\r\n>           variables, grad_variables, retain_graph, create_graph)\r\nE       RuntimeError: VariableType::_standard_gamma_grad NYI\r\n\r\ntorch/autograd/__init__.py:83: RuntimeError\r\n```\r\n</details>"}