{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/296576813", "html_url": "https://github.com/pytorch/pytorch/issues/1338#issuecomment-296576813", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1338", "id": 296576813, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjU3NjgxMw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-24T08:34:18Z", "updated_at": "2017-04-24T08:34:18Z", "author_association": "MEMBER", "body_html": "<p>The most important thing are the requirements of the CUDA API. I'd discourage its use unless you know some lower level detail of CUDA IPC. The most important limitation is that if you allocate a CUDA tensor X in process A and send it to process B, then X should <strong>never go out of scope and be freed in A</strong> until it's used in B (NOTE: it's automatically freed upon A's exit). So it might be ok for sharing CUDA parameters across processes, but not e.g. for data loading.</p>", "body_text": "The most important thing are the requirements of the CUDA API. I'd discourage its use unless you know some lower level detail of CUDA IPC. The most important limitation is that if you allocate a CUDA tensor X in process A and send it to process B, then X should never go out of scope and be freed in A until it's used in B (NOTE: it's automatically freed upon A's exit). So it might be ok for sharing CUDA parameters across processes, but not e.g. for data loading.", "body": "The most important thing are the requirements of the CUDA API. I'd discourage its use unless you know some lower level detail of CUDA IPC. The most important limitation is that if you allocate a CUDA tensor X in process A and send it to process B, then X should **never go out of scope and be freed in A** until it's used in B (NOTE: it's automatically freed upon A's exit). So it might be ok for sharing CUDA parameters across processes, but not e.g. for data loading."}