{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/401404140", "html_url": "https://github.com/pytorch/pytorch/issues/7420#issuecomment-401404140", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7420", "id": 401404140, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTQwNDE0MA==", "user": {"login": "hartb", "id": 18429659, "node_id": "MDQ6VXNlcjE4NDI5NjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/18429659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hartb", "html_url": "https://github.com/hartb", "followers_url": "https://api.github.com/users/hartb/followers", "following_url": "https://api.github.com/users/hartb/following{/other_user}", "gists_url": "https://api.github.com/users/hartb/gists{/gist_id}", "starred_url": "https://api.github.com/users/hartb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hartb/subscriptions", "organizations_url": "https://api.github.com/users/hartb/orgs", "repos_url": "https://api.github.com/users/hartb/repos", "events_url": "https://api.github.com/users/hartb/events{/privacy}", "received_events_url": "https://api.github.com/users/hartb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-29T16:21:09Z", "updated_at": "2018-06-29T16:21:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Looking at the failure of <code>test_Conv2d_groups_nobias</code>...</p>\n<p>This is also a \"value out of tolerance\" failure that we see only when using CUDA and torch.half/float16 on a system with V100 GPUs.</p>\n<p>The failure is data specific. I cut down the test script to include just the failing case with fixed parameters and input values. (See attached <code>Conv2d.py</code>.)</p>\n<p>With that setup, the values that differ out of tolerance are weights. The failing assert is:</p>\n<pre><code>        self.assertEqual(m.weight.grad.data[2:], m2.weight.grad.data, 1e-2)\n</code></pre>\n<p>And the differing values:</p>\n<pre><code>$ python Conv2d.py\ntensor([[[[-0.23413086, 16.67187500, -12.15625000],\n...                     ^^^^^^^^^^^\ntensor([[[[-0.23486328, 16.68750000, -12.15625000],\n...                     ^^^^^^^^^^^\nAssertionError: tensor(1.00000e-02 *\n       1.56250000, dtype=torch.float16, device='cuda:0') not less than or equal to 0.01\n</code></pre>\n<p>Note the mis-matched values are beween 16 and 32. In that range the limit of precision for float16 should be <code>2^-6 == 1.5625e-02</code> (at least per: <a href=\"https://en.wikipedia.org/wiki/Half-precision_floating-point_format\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Half-precision_floating-point_format</a>), which is exactly the mismatch difference. So the values mismatch by a single bit at the limit of resolution for float16 in this range, and the tolerance of the assert is finer than the resolution of float16 in this range.</p>\n<p>So if the test intends to allow slight imprecision at the resolution of the type, then the assert for the weights may be too strict. But if calculation should be deterministic and values exactly match, then there's something deeper gone wrong.</p>", "body_text": "Looking at the failure of test_Conv2d_groups_nobias...\nThis is also a \"value out of tolerance\" failure that we see only when using CUDA and torch.half/float16 on a system with V100 GPUs.\nThe failure is data specific. I cut down the test script to include just the failing case with fixed parameters and input values. (See attached Conv2d.py.)\nWith that setup, the values that differ out of tolerance are weights. The failing assert is:\n        self.assertEqual(m.weight.grad.data[2:], m2.weight.grad.data, 1e-2)\n\nAnd the differing values:\n$ python Conv2d.py\ntensor([[[[-0.23413086, 16.67187500, -12.15625000],\n...                     ^^^^^^^^^^^\ntensor([[[[-0.23486328, 16.68750000, -12.15625000],\n...                     ^^^^^^^^^^^\nAssertionError: tensor(1.00000e-02 *\n       1.56250000, dtype=torch.float16, device='cuda:0') not less than or equal to 0.01\n\nNote the mis-matched values are beween 16 and 32. In that range the limit of precision for float16 should be 2^-6 == 1.5625e-02 (at least per: https://en.wikipedia.org/wiki/Half-precision_floating-point_format), which is exactly the mismatch difference. So the values mismatch by a single bit at the limit of resolution for float16 in this range, and the tolerance of the assert is finer than the resolution of float16 in this range.\nSo if the test intends to allow slight imprecision at the resolution of the type, then the assert for the weights may be too strict. But if calculation should be deterministic and values exactly match, then there's something deeper gone wrong.", "body": "Looking at the failure of `test_Conv2d_groups_nobias`...\r\n\r\nThis is also a \"value out of tolerance\" failure that we see only when using CUDA and torch.half/float16 on a system with V100 GPUs.\r\n\r\nThe failure is data specific. I cut down the test script to include just the failing case with fixed parameters and input values. (See attached `Conv2d.py`.)\r\n\r\nWith that setup, the values that differ out of tolerance are weights. The failing assert is:\r\n\r\n```\r\n        self.assertEqual(m.weight.grad.data[2:], m2.weight.grad.data, 1e-2)\r\n```\r\n\r\nAnd the differing values:\r\n\r\n```\r\n$ python Conv2d.py\r\ntensor([[[[-0.23413086, 16.67187500, -12.15625000],\r\n...                     ^^^^^^^^^^^\r\ntensor([[[[-0.23486328, 16.68750000, -12.15625000],\r\n...                     ^^^^^^^^^^^\r\nAssertionError: tensor(1.00000e-02 *\r\n       1.56250000, dtype=torch.float16, device='cuda:0') not less than or equal to 0.01\r\n```\r\n\r\nNote the mis-matched values are beween 16 and 32. In that range the limit of precision for float16 should be `2^-6 == 1.5625e-02` (at least per: https://en.wikipedia.org/wiki/Half-precision_floating-point_format), which is exactly the mismatch difference. So the values mismatch by a single bit at the limit of resolution for float16 in this range, and the tolerance of the assert is finer than the resolution of float16 in this range.\r\n\r\nSo if the test intends to allow slight imprecision at the resolution of the type, then the assert for the weights may be too strict. But if calculation should be deterministic and values exactly match, then there's something deeper gone wrong."}