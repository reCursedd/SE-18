{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422393146", "html_url": "https://github.com/pytorch/pytorch/pull/11579#issuecomment-422393146", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11579", "id": 422393146, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjM5MzE0Ng==", "user": {"login": "avmgithub", "id": 9083746, "node_id": "MDQ6VXNlcjkwODM3NDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9083746?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avmgithub", "html_url": "https://github.com/avmgithub", "followers_url": "https://api.github.com/users/avmgithub/followers", "following_url": "https://api.github.com/users/avmgithub/following{/other_user}", "gists_url": "https://api.github.com/users/avmgithub/gists{/gist_id}", "starred_url": "https://api.github.com/users/avmgithub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avmgithub/subscriptions", "organizations_url": "https://api.github.com/users/avmgithub/orgs", "repos_url": "https://api.github.com/users/avmgithub/repos", "events_url": "https://api.github.com/users/avmgithub/events{/privacy}", "received_events_url": "https://api.github.com/users/avmgithub/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T13:28:24Z", "updated_at": "2018-09-18T13:28:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> are you referring to the Canonicalize function here <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/00df09b65d9f07bec7ee693658f017fda30a27e7/torch/csrc/jit/passes/canonicalize.cpp#L7\">pytorch/torch/csrc/jit/passes/canonicalize.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 7\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/00df09b65d9f07bec7ee693658f017fda30a27e7\">00df09b</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L7\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"7\"></td>\n          <td id=\"LC7\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> std::shared_ptr&lt;Graph&gt; <span class=\"pl-en\">Canonicalize</span>(<span class=\"pl-k\">const</span> std::shared_ptr&lt;Graph&gt;&amp; graph) { </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n    ?  And when you refer to \"not canonicalizing subgraphs\"  are you referring to :  <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/00df09b65d9f07bec7ee693658f017fda30a27e7/torch/csrc/jit/passes/canonicalize.cpp#L28\">pytorch/torch/csrc/jit/passes/canonicalize.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 28\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/00df09b65d9f07bec7ee693658f017fda30a27e7\">00df09b</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L28\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"28\"></td>\n          <td id=\"LC28\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> r_node-&gt;<span class=\"pl-c1\">g_</span>(attr::Subgraph, <span class=\"pl-c1\">Canonicalize</span>(node-&gt;<span class=\"pl-c1\">g</span>(attr::Subgraph))); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n   ?</p>\n<p>It looks to me they are fine.  If they are not would I not see the problem also happen in the forward graph?</p>\n<p>Just to add to the problem description:  The problem only happens for the backward graph .  The forward graph is fine.</p>\n<p>So for example part of the output of the backward graph of  the test_milstm_fusion_cuda is below:</p>\n<p>Output of ppc64le<br>\n%22 : Float(*, <em>) = aten::mul(%8, %3)<br>\n%23 : Float(</em>, <em>) = aten::neg(%3)<br>\n%24 : int = prim::Constant<a href=\"\">value=1</a><br>\n%25 : Float(</em>, <em>) = aten::add(%23, %24, %24)<br>\n%26 : Float(</em>, *) = aten::mul(%22, %25)</p>\n<p>Output of x86<br>\n%22 : Float(*, <em>) = aten::neg(%3)<br>\n%23 : int = prim::Constant<a href=\"\">value=1</a><br>\n%24 : Float(</em>, <em>) = aten::add(%22, %23, %23)<br>\n%25 : Float(</em>, <em>) = aten::mul(%8, %3)<br>\n%26 : Float(</em>, *) = aten::mul(%25, %24)</p>\n<p>So it seems the instructions are kind of re-arranged.    Do you think this is due to canonicalization ?<br>\nI'm thinking something to do with the backward functionality.  But I may be totally wrong.  Need your expert opinion and help as to which source file I need to be looking at.</p>", "body_text": "@ezyang are you referring to the Canonicalize function here \n  \n    \n      pytorch/torch/csrc/jit/passes/canonicalize.cpp\n    \n    \n         Line 7\n      in\n      00df09b\n    \n    \n    \n    \n\n        \n          \n           std::shared_ptr<Graph> Canonicalize(const std::shared_ptr<Graph>& graph) { \n        \n    \n  \n\n    ?  And when you refer to \"not canonicalizing subgraphs\"  are you referring to :  \n  \n    \n      pytorch/torch/csrc/jit/passes/canonicalize.cpp\n    \n    \n         Line 28\n      in\n      00df09b\n    \n    \n    \n    \n\n        \n          \n           r_node->g_(attr::Subgraph, Canonicalize(node->g(attr::Subgraph))); \n        \n    \n  \n\n   ?\nIt looks to me they are fine.  If they are not would I not see the problem also happen in the forward graph?\nJust to add to the problem description:  The problem only happens for the backward graph .  The forward graph is fine.\nSo for example part of the output of the backward graph of  the test_milstm_fusion_cuda is below:\nOutput of ppc64le\n%22 : Float(*, ) = aten::mul(%8, %3)\n%23 : Float(, ) = aten::neg(%3)\n%24 : int = prim::Constantvalue=1\n%25 : Float(, ) = aten::add(%23, %24, %24)\n%26 : Float(, *) = aten::mul(%22, %25)\nOutput of x86\n%22 : Float(*, ) = aten::neg(%3)\n%23 : int = prim::Constantvalue=1\n%24 : Float(, ) = aten::add(%22, %23, %23)\n%25 : Float(, ) = aten::mul(%8, %3)\n%26 : Float(, *) = aten::mul(%25, %24)\nSo it seems the instructions are kind of re-arranged.    Do you think this is due to canonicalization ?\nI'm thinking something to do with the backward functionality.  But I may be totally wrong.  Need your expert opinion and help as to which source file I need to be looking at.", "body": "@ezyang are you referring to the Canonicalize function here https://github.com/pytorch/pytorch/blob/00df09b65d9f07bec7ee693658f017fda30a27e7/torch/csrc/jit/passes/canonicalize.cpp#L7    ?  And when you refer to \"not canonicalizing subgraphs\"  are you referring to :  https://github.com/pytorch/pytorch/blob/00df09b65d9f07bec7ee693658f017fda30a27e7/torch/csrc/jit/passes/canonicalize.cpp#L28   ?\r\n\r\nIt looks to me they are fine.  If they are not would I not see the problem also happen in the forward graph?\r\n\r\nJust to add to the problem description:  The problem only happens for the backward graph .  The forward graph is fine.  \r\n\r\nSo for example part of the output of the backward graph of  the test_milstm_fusion_cuda is below: \r\n\r\nOutput of ppc64le\r\n%22 : Float(*, *) = aten::mul(%8, %3)\r\n%23 : Float(*, *) = aten::neg(%3)\r\n%24 : int = prim::Constant[value=1]()\r\n%25 : Float(*, *) = aten::add(%23, %24, %24)\r\n%26 : Float(*, *) = aten::mul(%22, %25)\r\n\r\nOutput of x86\r\n%22 : Float(*, *) = aten::neg(%3)\r\n%23 : int = prim::Constant[value=1]()\r\n%24 : Float(*, *) = aten::add(%22, %23, %23)\r\n%25 : Float(*, *) = aten::mul(%8, %3)\r\n%26 : Float(*, *) = aten::mul(%25, %24)\r\n\r\nSo it seems the instructions are kind of re-arranged.    Do you think this is due to canonicalization ?\r\nI'm thinking something to do with the backward functionality.  But I may be totally wrong.  Need your expert opinion and help as to which source file I need to be looking at.\r\n"}