{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/320128863", "html_url": "https://github.com/pytorch/pytorch/pull/2053#issuecomment-320128863", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2053", "id": 320128863, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDEyODg2Mw==", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-04T01:02:05Z", "updated_at": "2017-08-04T01:02:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, if I get it right, without the <code>.cuda()</code> statements, we can only compare performance on CPU mode.<br>\nFortunately I did not terminate the IPython session, so I ran a few quick benchmarks:</p>\n<pre><code>In [209]: def reverse_padded_sequence_fast(inputs, lengths, batch_first=False):\n     ...:     \"\"\"Reverses sequences according to their lengths.\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\n     ...:     Arguments:\n     ...:         inputs (Variable): padded batch of variable length sequences.\n     ...:         lengths (list[int]): list of sequence lengths\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\n     ...:     Returns:\n     ...:         A Variable with the same size as inputs, but with each sequence\n     ...:         reversed according to its length.\n     ...:     \"\"\"\n     ...:     if not batch_first:\n     ...:         inputs = inputs.transpose(0, 1)\n     ...:     if inputs.size(0) != len(lengths):\n     ...:         raise ValueError('inputs incompatible with lengths.')\n     ...:     reversed_indices = [list(range(inputs.size(1))) for _ in range(inputs.size(0))]\n     ...:     for i, length in enumerate(lengths):\n     ...:         if length &gt; 0:\n     ...:             reversed_indices[i][:length] = reversed_indices[i][length-1::-1]\n     ...:     reversed_indices = torch.LongTensor(reversed_indices).unsqueeze(2).expand_as(inputs)\n     ...:     reversed_indices = Variable(reversed_indices)\n     ...:     reversed_inputs = torch.gather(inputs, 1, reversed_indices)\n     ...:     if not batch_first:\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\n     ...:     return reversed_inputs\n     ...:\n\nIn [210]: def reverse_padded_sequence(inputs, lengths, batch_first=False):\n     ...:     \"\"\"Reverses sequences according to their lengths.\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\n     ...:     Arguments:\n     ...:         inputs (Variable): padded batch of variable length sequences.\n     ...:         lengths (list[int]): list of sequence lengths\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\n     ...:     Returns:\n     ...:         A Variable with the same size as inputs, but with each sequence\n     ...:         reversed according to its length.\n     ...:     \"\"\"\n     ...:     if batch_first:\n     ...:         inputs = inputs.transpose(0, 1)\n     ...:     if inputs.size(1) != len(lengths):\n     ...:         raise ValueError('inputs incompatible with lengths.')\n     ...:     reversed_inputs = Variable(inputs.data.clone())\n     ...:     for i, length in enumerate(lengths):\n     ...:         time_ind = torch.LongTensor(list(reversed(range(length))))\n     ...:         if length &gt; 0:\n     ...:             reversed_inputs[:length, i] = inputs[:, i][time_ind]\n     ...:     if batch_first:\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\n     ...:     return reversed_inputs\n     ...:\n\nIn [211]: inputs = inputs.cpu()\n\nIn [212]: inputs.size()\nOut[212]: torch.Size([128, 100, 300])\n\nIn [213]: rnn = torch.nn.LSTM(300, 300, batch_first=True)\n\nIn [214]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\n539 ms \u00b1 2.03 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [215]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\n571 ms \u00b1 1.56 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [216]: inputs = inputs.transpose(0, 1)\n\nIn [217]: inputs.size()\nOut[217]: torch.Size([100, 128, 300])\n\nIn [218]: rnn = torch.nn.LSTM(300, 300, batch_first=False)\n\nIn [219]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=False))\n526 ms \u00b1 2.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [220]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=False))\n565 ms \u00b1 825 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n</code></pre>\n<p>It seems that your version is faster than mine when run on CPU...<br>\n(Of course the best scenario is to implement this feature in C/C++ as in TensorFlow, however that would make the work much more complicated!)</p>\n<p>If you are fine, for this PR to be processed quickly, I want to delegate some modifications (and decisions) to you.<br>\nI found one point that need revision:<br>\nFor the code run on both CPU and GPU, <code>time_ind</code> (your version) and <code>reversed_indices</code> (my version) should be transferred to GPU if <code>inputs</code> is in GPU.<br>\nI usually use the below snippets to do this, but there might be more elegant ways:</p>\n<pre><code>if inputs.is_cuda:\n    device = inputs.get_device()  # To operate on a multi-GPU machine\n    time_ind = time_ind.cuda(device)\n</code></pre>", "body_text": "Hi, if I get it right, without the .cuda() statements, we can only compare performance on CPU mode.\nFortunately I did not terminate the IPython session, so I ran a few quick benchmarks:\nIn [209]: def reverse_padded_sequence_fast(inputs, lengths, batch_first=False):\n     ...:     \"\"\"Reverses sequences according to their lengths.\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\n     ...:     Arguments:\n     ...:         inputs (Variable): padded batch of variable length sequences.\n     ...:         lengths (list[int]): list of sequence lengths\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\n     ...:     Returns:\n     ...:         A Variable with the same size as inputs, but with each sequence\n     ...:         reversed according to its length.\n     ...:     \"\"\"\n     ...:     if not batch_first:\n     ...:         inputs = inputs.transpose(0, 1)\n     ...:     if inputs.size(0) != len(lengths):\n     ...:         raise ValueError('inputs incompatible with lengths.')\n     ...:     reversed_indices = [list(range(inputs.size(1))) for _ in range(inputs.size(0))]\n     ...:     for i, length in enumerate(lengths):\n     ...:         if length > 0:\n     ...:             reversed_indices[i][:length] = reversed_indices[i][length-1::-1]\n     ...:     reversed_indices = torch.LongTensor(reversed_indices).unsqueeze(2).expand_as(inputs)\n     ...:     reversed_indices = Variable(reversed_indices)\n     ...:     reversed_inputs = torch.gather(inputs, 1, reversed_indices)\n     ...:     if not batch_first:\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\n     ...:     return reversed_inputs\n     ...:\n\nIn [210]: def reverse_padded_sequence(inputs, lengths, batch_first=False):\n     ...:     \"\"\"Reverses sequences according to their lengths.\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\n     ...:     Arguments:\n     ...:         inputs (Variable): padded batch of variable length sequences.\n     ...:         lengths (list[int]): list of sequence lengths\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\n     ...:     Returns:\n     ...:         A Variable with the same size as inputs, but with each sequence\n     ...:         reversed according to its length.\n     ...:     \"\"\"\n     ...:     if batch_first:\n     ...:         inputs = inputs.transpose(0, 1)\n     ...:     if inputs.size(1) != len(lengths):\n     ...:         raise ValueError('inputs incompatible with lengths.')\n     ...:     reversed_inputs = Variable(inputs.data.clone())\n     ...:     for i, length in enumerate(lengths):\n     ...:         time_ind = torch.LongTensor(list(reversed(range(length))))\n     ...:         if length > 0:\n     ...:             reversed_inputs[:length, i] = inputs[:, i][time_ind]\n     ...:     if batch_first:\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\n     ...:     return reversed_inputs\n     ...:\n\nIn [211]: inputs = inputs.cpu()\n\nIn [212]: inputs.size()\nOut[212]: torch.Size([128, 100, 300])\n\nIn [213]: rnn = torch.nn.LSTM(300, 300, batch_first=True)\n\nIn [214]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\n539 ms \u00b1 2.03 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [215]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\n571 ms \u00b1 1.56 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [216]: inputs = inputs.transpose(0, 1)\n\nIn [217]: inputs.size()\nOut[217]: torch.Size([100, 128, 300])\n\nIn [218]: rnn = torch.nn.LSTM(300, 300, batch_first=False)\n\nIn [219]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=False))\n526 ms \u00b1 2.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [220]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=False))\n565 ms \u00b1 825 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\nIt seems that your version is faster than mine when run on CPU...\n(Of course the best scenario is to implement this feature in C/C++ as in TensorFlow, however that would make the work much more complicated!)\nIf you are fine, for this PR to be processed quickly, I want to delegate some modifications (and decisions) to you.\nI found one point that need revision:\nFor the code run on both CPU and GPU, time_ind (your version) and reversed_indices (my version) should be transferred to GPU if inputs is in GPU.\nI usually use the below snippets to do this, but there might be more elegant ways:\nif inputs.is_cuda:\n    device = inputs.get_device()  # To operate on a multi-GPU machine\n    time_ind = time_ind.cuda(device)", "body": "Hi, if I get it right, without the `.cuda()` statements, we can only compare performance on CPU mode.\r\nFortunately I did not terminate the IPython session, so I ran a few quick benchmarks:\r\n\r\n```\r\nIn [209]: def reverse_padded_sequence_fast(inputs, lengths, batch_first=False):\r\n     ...:     \"\"\"Reverses sequences according to their lengths.\r\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\r\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\r\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\r\n     ...:     Arguments:\r\n     ...:         inputs (Variable): padded batch of variable length sequences.\r\n     ...:         lengths (list[int]): list of sequence lengths\r\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\r\n     ...:     Returns:\r\n     ...:         A Variable with the same size as inputs, but with each sequence\r\n     ...:         reversed according to its length.\r\n     ...:     \"\"\"\r\n     ...:     if not batch_first:\r\n     ...:         inputs = inputs.transpose(0, 1)\r\n     ...:     if inputs.size(0) != len(lengths):\r\n     ...:         raise ValueError('inputs incompatible with lengths.')\r\n     ...:     reversed_indices = [list(range(inputs.size(1))) for _ in range(inputs.size(0))]\r\n     ...:     for i, length in enumerate(lengths):\r\n     ...:         if length > 0:\r\n     ...:             reversed_indices[i][:length] = reversed_indices[i][length-1::-1]\r\n     ...:     reversed_indices = torch.LongTensor(reversed_indices).unsqueeze(2).expand_as(inputs)\r\n     ...:     reversed_indices = Variable(reversed_indices)\r\n     ...:     reversed_inputs = torch.gather(inputs, 1, reversed_indices)\r\n     ...:     if not batch_first:\r\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\r\n     ...:     return reversed_inputs\r\n     ...:\r\n\r\nIn [210]: def reverse_padded_sequence(inputs, lengths, batch_first=False):\r\n     ...:     \"\"\"Reverses sequences according to their lengths.\r\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\r\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\r\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\r\n     ...:     Arguments:\r\n     ...:         inputs (Variable): padded batch of variable length sequences.\r\n     ...:         lengths (list[int]): list of sequence lengths\r\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\r\n     ...:     Returns:\r\n     ...:         A Variable with the same size as inputs, but with each sequence\r\n     ...:         reversed according to its length.\r\n     ...:     \"\"\"\r\n     ...:     if batch_first:\r\n     ...:         inputs = inputs.transpose(0, 1)\r\n     ...:     if inputs.size(1) != len(lengths):\r\n     ...:         raise ValueError('inputs incompatible with lengths.')\r\n     ...:     reversed_inputs = Variable(inputs.data.clone())\r\n     ...:     for i, length in enumerate(lengths):\r\n     ...:         time_ind = torch.LongTensor(list(reversed(range(length))))\r\n     ...:         if length > 0:\r\n     ...:             reversed_inputs[:length, i] = inputs[:, i][time_ind]\r\n     ...:     if batch_first:\r\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\r\n     ...:     return reversed_inputs\r\n     ...:\r\n\r\nIn [211]: inputs = inputs.cpu()\r\n\r\nIn [212]: inputs.size()\r\nOut[212]: torch.Size([128, 100, 300])\r\n\r\nIn [213]: rnn = torch.nn.LSTM(300, 300, batch_first=True)\r\n\r\nIn [214]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\r\n539 ms \u00b1 2.03 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [215]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\r\n571 ms \u00b1 1.56 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [216]: inputs = inputs.transpose(0, 1)\r\n\r\nIn [217]: inputs.size()\r\nOut[217]: torch.Size([100, 128, 300])\r\n\r\nIn [218]: rnn = torch.nn.LSTM(300, 300, batch_first=False)\r\n\r\nIn [219]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=False))\r\n526 ms \u00b1 2.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [220]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=False))\r\n565 ms \u00b1 825 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n```\r\n\r\nIt seems that your version is faster than mine when run on CPU...\r\n(Of course the best scenario is to implement this feature in C/C++ as in TensorFlow, however that would make the work much more complicated!)\r\n\r\nIf you are fine, for this PR to be processed quickly, I want to delegate some modifications (and decisions) to you.\r\nI found one point that need revision:\r\nFor the code run on both CPU and GPU, `time_ind` (your version) and `reversed_indices` (my version) should be transferred to GPU if `inputs` is in GPU.\r\nI usually use the below snippets to do this, but there might be more elegant ways:\r\n```\r\nif inputs.is_cuda:\r\n    device = inputs.get_device()  # To operate on a multi-GPU machine\r\n    time_ind = time_ind.cuda(device)\r\n```"}