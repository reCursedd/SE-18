{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/319922541", "html_url": "https://github.com/pytorch/pytorch/pull/2053#issuecomment-319922541", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2053", "id": 319922541, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTkyMjU0MQ==", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-03T09:48:01Z", "updated_at": "2017-08-03T09:48:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, it seems that the code is quite slow when run on GPU, due to frequent CPU-&gt;GPU transfers.<br>\nI think making a variable of (reversed) indices and then using gather function would make the run faster on GPU:</p>\n<pre><code>def reverse_padded_sequence_fast(inputs, lengths, batch_first=False):\n    \"\"\"Reverses sequences according to their lengths.\n    Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n    ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n    B is the batch size, and * is any number of dimensions (including 0).\n    Arguments:\n        inputs (Variable): padded batch of variable length sequences.\n        lengths (list[int]): list of sequence lengths\n        batch_first (bool, optional): if True, inputs should be B x T x *.\n    Returns:\n        A Variable with the same size as inputs, but with each sequence\n        reversed according to its length.\n    \"\"\"\n    if not batch_first:\n        inputs = inputs.transpose(0, 1)\n    if inputs.size(0) != len(lengths):\n        raise ValueError('inputs incompatible with lengths.')\n    reversed_indices = [list(range(inputs.size(1))) for _ in range(inputs.size(0))]\n    for i, length in enumerate(lengths):\n        if length &gt; 0:\n            reversed_indices[i][:length] = reversed_indices[i][length-1::-1]\n    reversed_indices = torch.LongTensor(reversed_indices).unsqueeze(2).expand_as(inputs)\n    reversed_indices = Variable(reversed_indices)\n    if inputs.is_cuda:\n        reversed_indices = reversed_indices.cuda()\n    reversed_inputs = torch.gather(inputs, 1, reversed_indices)\n    if not batch_first:\n        reversed_inputs = reversed_inputs.transpose(0, 1)\n    return reversed_inputs\n</code></pre>\n<p>However the faster version might be slower than the original one when run on CPU mode... :$</p>\n<p>Some benchmarks (modified the original code a bit to make it able to be run on GPU):</p>\n<pre><code># CPU mode, not batch_first\nIn [182]: def reverse_padded_sequence(inputs, lengths, batch_first=False):\n     ...:     \"\"\"Reverses sequences according to their lengths.\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\n     ...:     Arguments:\n     ...:         inputs (Variable): padded batch of variable length sequences.\n     ...:         lengths (list[int]): list of sequence lengths\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\n     ...:     Returns:\n     ...:         A Variable with the same size as inputs, but with each sequence\n     ...:         reversed according to its length.\n     ...:     \"\"\"\n     ...:     if batch_first:\n     ...:         inputs = inputs.transpose(0, 1)\n     ...:     if inputs.size(1) != len(lengths):\n     ...:         raise ValueError('inputs incompatible with lengths.')\n     ...:     reversed_inputs = Variable(inputs.data.clone())\n     ...:     for i, length in enumerate(lengths):\n     ...:         time_ind = torch.LongTensor(list(reversed(range(length))))\n     ...:         if inputs.is_cuda:\n     ...:             time_ind = time_ind.cuda()\n     ...:         if length &gt; 0:\n     ...:             reversed_inputs[:length, i] = inputs[:, i][time_ind]\n     ...:     if batch_first:\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\n     ...:     return reversed_inputs\n\nIn [186]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths))\n570 ms \u00b1 1.41 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [187]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths))\n556 ms \u00b1 2.61 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# CPU mode, batch_first\nIn [188]: inputs = Variable(torch.randn(128,100,300))\n\nIn [189]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\n584 ms \u00b1 2.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [190]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\n596 ms \u00b1 1.84 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# GPU mode, not batch_first\nIn [197]: rnn.cuda()\nOut[197]: LSTM(300, 300)\n\nIn [198]: inputs = Variable(torch.randn(100,128,300)).cuda()\n\nIn [199]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths))\n14.7 ms \u00b1 282 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [200]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths))\n10.3 ms \u00b1 107 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# GPU mode, batch_first\nIn [201]: inputs = Variable(torch.randn(128,100,300)).cuda()\n\nIn [202]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\n16.5 ms \u00b1 178 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [203]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\n11.7 ms \u00b1 116 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# FYI: when inputs are not reversed\nIn [204]: %timeit -n10 rnn(inputs)\n9.35 ms \u00b1 841 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre>\n<p>Hope it helps!</p>", "body_text": "Hi, it seems that the code is quite slow when run on GPU, due to frequent CPU->GPU transfers.\nI think making a variable of (reversed) indices and then using gather function would make the run faster on GPU:\ndef reverse_padded_sequence_fast(inputs, lengths, batch_first=False):\n    \"\"\"Reverses sequences according to their lengths.\n    Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n    ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n    B is the batch size, and * is any number of dimensions (including 0).\n    Arguments:\n        inputs (Variable): padded batch of variable length sequences.\n        lengths (list[int]): list of sequence lengths\n        batch_first (bool, optional): if True, inputs should be B x T x *.\n    Returns:\n        A Variable with the same size as inputs, but with each sequence\n        reversed according to its length.\n    \"\"\"\n    if not batch_first:\n        inputs = inputs.transpose(0, 1)\n    if inputs.size(0) != len(lengths):\n        raise ValueError('inputs incompatible with lengths.')\n    reversed_indices = [list(range(inputs.size(1))) for _ in range(inputs.size(0))]\n    for i, length in enumerate(lengths):\n        if length > 0:\n            reversed_indices[i][:length] = reversed_indices[i][length-1::-1]\n    reversed_indices = torch.LongTensor(reversed_indices).unsqueeze(2).expand_as(inputs)\n    reversed_indices = Variable(reversed_indices)\n    if inputs.is_cuda:\n        reversed_indices = reversed_indices.cuda()\n    reversed_inputs = torch.gather(inputs, 1, reversed_indices)\n    if not batch_first:\n        reversed_inputs = reversed_inputs.transpose(0, 1)\n    return reversed_inputs\n\nHowever the faster version might be slower than the original one when run on CPU mode... :$\nSome benchmarks (modified the original code a bit to make it able to be run on GPU):\n# CPU mode, not batch_first\nIn [182]: def reverse_padded_sequence(inputs, lengths, batch_first=False):\n     ...:     \"\"\"Reverses sequences according to their lengths.\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\n     ...:     Arguments:\n     ...:         inputs (Variable): padded batch of variable length sequences.\n     ...:         lengths (list[int]): list of sequence lengths\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\n     ...:     Returns:\n     ...:         A Variable with the same size as inputs, but with each sequence\n     ...:         reversed according to its length.\n     ...:     \"\"\"\n     ...:     if batch_first:\n     ...:         inputs = inputs.transpose(0, 1)\n     ...:     if inputs.size(1) != len(lengths):\n     ...:         raise ValueError('inputs incompatible with lengths.')\n     ...:     reversed_inputs = Variable(inputs.data.clone())\n     ...:     for i, length in enumerate(lengths):\n     ...:         time_ind = torch.LongTensor(list(reversed(range(length))))\n     ...:         if inputs.is_cuda:\n     ...:             time_ind = time_ind.cuda()\n     ...:         if length > 0:\n     ...:             reversed_inputs[:length, i] = inputs[:, i][time_ind]\n     ...:     if batch_first:\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\n     ...:     return reversed_inputs\n\nIn [186]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths))\n570 ms \u00b1 1.41 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [187]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths))\n556 ms \u00b1 2.61 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# CPU mode, batch_first\nIn [188]: inputs = Variable(torch.randn(128,100,300))\n\nIn [189]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\n584 ms \u00b1 2.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [190]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\n596 ms \u00b1 1.84 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# GPU mode, not batch_first\nIn [197]: rnn.cuda()\nOut[197]: LSTM(300, 300)\n\nIn [198]: inputs = Variable(torch.randn(100,128,300)).cuda()\n\nIn [199]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths))\n14.7 ms \u00b1 282 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [200]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths))\n10.3 ms \u00b1 107 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# GPU mode, batch_first\nIn [201]: inputs = Variable(torch.randn(128,100,300)).cuda()\n\nIn [202]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\n16.5 ms \u00b1 178 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [203]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\n11.7 ms \u00b1 116 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# FYI: when inputs are not reversed\nIn [204]: %timeit -n10 rnn(inputs)\n9.35 ms \u00b1 841 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nHope it helps!", "body": "Hi, it seems that the code is quite slow when run on GPU, due to frequent CPU->GPU transfers.\r\nI think making a variable of (reversed) indices and then using gather function would make the run faster on GPU:\r\n\r\n```\r\ndef reverse_padded_sequence_fast(inputs, lengths, batch_first=False):\r\n    \"\"\"Reverses sequences according to their lengths.\r\n    Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\r\n    ``B x T x *`` if True. T is the length of the longest sequence (or larger),\r\n    B is the batch size, and * is any number of dimensions (including 0).\r\n    Arguments:\r\n        inputs (Variable): padded batch of variable length sequences.\r\n        lengths (list[int]): list of sequence lengths\r\n        batch_first (bool, optional): if True, inputs should be B x T x *.\r\n    Returns:\r\n        A Variable with the same size as inputs, but with each sequence\r\n        reversed according to its length.\r\n    \"\"\"\r\n    if not batch_first:\r\n        inputs = inputs.transpose(0, 1)\r\n    if inputs.size(0) != len(lengths):\r\n        raise ValueError('inputs incompatible with lengths.')\r\n    reversed_indices = [list(range(inputs.size(1))) for _ in range(inputs.size(0))]\r\n    for i, length in enumerate(lengths):\r\n        if length > 0:\r\n            reversed_indices[i][:length] = reversed_indices[i][length-1::-1]\r\n    reversed_indices = torch.LongTensor(reversed_indices).unsqueeze(2).expand_as(inputs)\r\n    reversed_indices = Variable(reversed_indices)\r\n    if inputs.is_cuda:\r\n        reversed_indices = reversed_indices.cuda()\r\n    reversed_inputs = torch.gather(inputs, 1, reversed_indices)\r\n    if not batch_first:\r\n        reversed_inputs = reversed_inputs.transpose(0, 1)\r\n    return reversed_inputs\r\n```\r\n\r\nHowever the faster version might be slower than the original one when run on CPU mode... :$\r\n\r\nSome benchmarks (modified the original code a bit to make it able to be run on GPU):\r\n```\r\n# CPU mode, not batch_first\r\nIn [182]: def reverse_padded_sequence(inputs, lengths, batch_first=False):\r\n     ...:     \"\"\"Reverses sequences according to their lengths.\r\n     ...:     Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\r\n     ...:     ``B x T x *`` if True. T is the length of the longest sequence (or larger),\r\n     ...:     B is the batch size, and * is any number of dimensions (including 0).\r\n     ...:     Arguments:\r\n     ...:         inputs (Variable): padded batch of variable length sequences.\r\n     ...:         lengths (list[int]): list of sequence lengths\r\n     ...:         batch_first (bool, optional): if True, inputs should be B x T x *.\r\n     ...:     Returns:\r\n     ...:         A Variable with the same size as inputs, but with each sequence\r\n     ...:         reversed according to its length.\r\n     ...:     \"\"\"\r\n     ...:     if batch_first:\r\n     ...:         inputs = inputs.transpose(0, 1)\r\n     ...:     if inputs.size(1) != len(lengths):\r\n     ...:         raise ValueError('inputs incompatible with lengths.')\r\n     ...:     reversed_inputs = Variable(inputs.data.clone())\r\n     ...:     for i, length in enumerate(lengths):\r\n     ...:         time_ind = torch.LongTensor(list(reversed(range(length))))\r\n     ...:         if inputs.is_cuda:\r\n     ...:             time_ind = time_ind.cuda()\r\n     ...:         if length > 0:\r\n     ...:             reversed_inputs[:length, i] = inputs[:, i][time_ind]\r\n     ...:     if batch_first:\r\n     ...:         reversed_inputs = reversed_inputs.transpose(0, 1)\r\n     ...:     return reversed_inputs\r\n\r\nIn [186]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths))\r\n570 ms \u00b1 1.41 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [187]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths))\r\n556 ms \u00b1 2.61 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n# CPU mode, batch_first\r\nIn [188]: inputs = Variable(torch.randn(128,100,300))\r\n\r\nIn [189]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\r\n584 ms \u00b1 2.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [190]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\r\n596 ms \u00b1 1.84 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n# GPU mode, not batch_first\r\nIn [197]: rnn.cuda()\r\nOut[197]: LSTM(300, 300)\r\n\r\nIn [198]: inputs = Variable(torch.randn(100,128,300)).cuda()\r\n\r\nIn [199]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths))\r\n14.7 ms \u00b1 282 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [200]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths))\r\n10.3 ms \u00b1 107 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n# GPU mode, batch_first\r\nIn [201]: inputs = Variable(torch.randn(128,100,300)).cuda()\r\n\r\nIn [202]: %timeit -n10 rnn(reverse_padded_sequence(inputs, lengths, batch_first=True))\r\n16.5 ms \u00b1 178 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [203]: %timeit -n10 rnn(reverse_padded_sequence_fast(inputs, lengths, batch_first=True))\r\n11.7 ms \u00b1 116 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n# FYI: when inputs are not reversed\r\nIn [204]: %timeit -n10 rnn(inputs)\r\n9.35 ms \u00b1 841 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\nHope it helps!"}