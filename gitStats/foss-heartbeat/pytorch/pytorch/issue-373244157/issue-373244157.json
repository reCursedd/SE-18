{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13022", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13022/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13022/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13022/events", "html_url": "https://github.com/pytorch/pytorch/issues/13022", "id": 373244157, "node_id": "MDU6SXNzdWUzNzMyNDQxNTc=", "number": 13022, "title": "Support multiple simultaneous LR schedulers", "user": {"login": "jeanm", "id": 107696, "node_id": "MDQ6VXNlcjEwNzY5Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/107696?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeanm", "html_url": "https://github.com/jeanm", "followers_url": "https://api.github.com/users/jeanm/followers", "following_url": "https://api.github.com/users/jeanm/following{/other_user}", "gists_url": "https://api.github.com/users/jeanm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeanm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeanm/subscriptions", "organizations_url": "https://api.github.com/users/jeanm/orgs", "repos_url": "https://api.github.com/users/jeanm/repos", "events_url": "https://api.github.com/users/jeanm/events{/privacy}", "received_events_url": "https://api.github.com/users/jeanm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-23T23:31:41Z", "updated_at": "2018-11-13T23:24:47Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>Currently only one <code>_LRScheduler</code> can be used with the same optimizer. It would be great to support using multiple ones simultaneously.</p>\n<h2>Motivation</h2>\n<p>It's not uncommon in NLP (see e.g. <a href=\"https://github.com/facebookresearch/InferSent\">here</a>) to want to use both <code>StepLR</code> and <code>ReduceLROnPlateau</code>. This isn't possible currently, because <code>StepLR</code> calculates learning rate updates based on what was the initial learning rate, and how many epochs have gone by (see <code>get_lr()</code>). This is all based on the assumption the learning rate hasn't been touched by anything else.</p>\n<h2>Pitch</h2>\n<p>This can be fixed by having <code>StepLR</code>,  <code>ExponentialLR</code> (and potentially others) query the optimizer to find out the current learning rate, instead of attempting to work it out what it should be. <code>ReduceLROnPlateau</code> already does this in <code>_reduce_lr()</code>.</p>", "body_text": "\ud83d\ude80 Feature\nCurrently only one _LRScheduler can be used with the same optimizer. It would be great to support using multiple ones simultaneously.\nMotivation\nIt's not uncommon in NLP (see e.g. here) to want to use both StepLR and ReduceLROnPlateau. This isn't possible currently, because StepLR calculates learning rate updates based on what was the initial learning rate, and how many epochs have gone by (see get_lr()). This is all based on the assumption the learning rate hasn't been touched by anything else.\nPitch\nThis can be fixed by having StepLR,  ExponentialLR (and potentially others) query the optimizer to find out the current learning rate, instead of attempting to work it out what it should be. ReduceLROnPlateau already does this in _reduce_lr().", "body": "## \ud83d\ude80 Feature\r\n\r\nCurrently only one `_LRScheduler` can be used with the same optimizer. It would be great to support using multiple ones simultaneously.\r\n\r\n## Motivation\r\n\r\nIt's not uncommon in NLP (see e.g. [here](https://github.com/facebookresearch/InferSent)) to want to use both `StepLR` and `ReduceLROnPlateau`. This isn't possible currently, because `StepLR` calculates learning rate updates based on what was the initial learning rate, and how many epochs have gone by (see `get_lr()`). This is all based on the assumption the learning rate hasn't been touched by anything else.\r\n\r\n## Pitch\r\n\r\nThis can be fixed by having `StepLR`,  `ExponentialLR` (and potentially others) query the optimizer to find out the current learning rate, instead of attempting to work it out what it should be. `ReduceLROnPlateau` already does this in `_reduce_lr()`."}