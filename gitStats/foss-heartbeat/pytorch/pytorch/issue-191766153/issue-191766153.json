{"url": "https://api.github.com/repos/pytorch/pytorch/issues/253", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/253/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/253/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/253/events", "html_url": "https://github.com/pytorch/pytorch/issues/253", "id": 191766153, "node_id": "MDU6SXNzdWUxOTE3NjYxNTM=", "number": 253, "title": "batch_first broken in AutogradRNN", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-11-25T21:20:06Z", "updated_at": "2016-11-26T06:10:38Z", "closed_at": "2016-11-26T04:55:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The last line here fails on CPU or when CUDNN is otherwise unavailable:</p>\n<div class=\"highlight highlight-source-python\"><pre>l, b, t, x, h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">20</span>\n\nrnn <span class=\"pl-k\">=</span> nn.LSTM(x, h, l, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ninpt <span class=\"pl-k\">=</span> Variable(torch.randn(b, t, x))\nh0 <span class=\"pl-k\">=</span> Variable(torch.randn(l, b, h))\nc0 <span class=\"pl-k\">=</span> Variable(torch.randn(l, b, h))\noutput, hn <span class=\"pl-k\">=</span> rnn(inpt, (h0, c0))</pre></div>\n<p>This is because <code>AutogradRNN.forward</code> accidentally assumes <code>Tensor</code>'s in-place <code>transpose</code> semantics rather than the functional semantics of <code>Variable</code> (<code>cudnn.rnn.forward</code> gets it right):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">hidden</span>):\n    <span class=\"pl-k\">if</span> batch_first:\n        <span class=\"pl-c1\">input</span>.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n    nexth, output <span class=\"pl-k\">=</span> func(<span class=\"pl-c1\">input</span>, hidden, weight)\n    <span class=\"pl-k\">if</span> batch_first:\n        output.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)</pre></div>\n<p>I can push a PR that fixes this, or one of the devs can put it in the next bugfix PR:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">hidden</span>):\n    <span class=\"pl-k\">if</span> batch_first:\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n    nexth, output <span class=\"pl-k\">=</span> func(<span class=\"pl-c1\">input</span>, hidden, weight)\n    <span class=\"pl-k\">if</span> batch_first:\n        output <span class=\"pl-k\">=</span> output.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)</pre></div>", "body_text": "The last line here fails on CPU or when CUDNN is otherwise unavailable:\nl, b, t, x, h = 2, 3, 5, 10, 20\n\nrnn = nn.LSTM(x, h, l, batch_first=True)\ninpt = Variable(torch.randn(b, t, x))\nh0 = Variable(torch.randn(l, b, h))\nc0 = Variable(torch.randn(l, b, h))\noutput, hn = rnn(inpt, (h0, c0))\nThis is because AutogradRNN.forward accidentally assumes Tensor's in-place transpose semantics rather than the functional semantics of Variable (cudnn.rnn.forward gets it right):\ndef forward(input, weight, hidden):\n    if batch_first:\n        input.transpose(0, 1)\n    nexth, output = func(input, hidden, weight)\n    if batch_first:\n        output.transpose(0, 1)\nI can push a PR that fixes this, or one of the devs can put it in the next bugfix PR:\ndef forward(input, weight, hidden):\n    if batch_first:\n        input = input.transpose(0, 1)\n    nexth, output = func(input, hidden, weight)\n    if batch_first:\n        output = output.transpose(0, 1)", "body": "The last line here fails on CPU or when CUDNN is otherwise unavailable:\r\n\r\n```python\r\nl, b, t, x, h = 2, 3, 5, 10, 20\r\n\r\nrnn = nn.LSTM(x, h, l, batch_first=True)\r\ninpt = Variable(torch.randn(b, t, x))\r\nh0 = Variable(torch.randn(l, b, h))\r\nc0 = Variable(torch.randn(l, b, h))\r\noutput, hn = rnn(inpt, (h0, c0))\r\n```\r\n\r\nThis is because `AutogradRNN.forward` accidentally assumes `Tensor`'s in-place `transpose` semantics rather than the functional semantics of `Variable` (`cudnn.rnn.forward` gets it right):\r\n\r\n```python\r\ndef forward(input, weight, hidden):\r\n    if batch_first:\r\n        input.transpose(0, 1)\r\n    nexth, output = func(input, hidden, weight)\r\n    if batch_first:\r\n        output.transpose(0, 1)\r\n```\r\n\r\nI can push a PR that fixes this, or one of the devs can put it in the next bugfix PR:\r\n```python\r\ndef forward(input, weight, hidden):\r\n    if batch_first:\r\n        input = input.transpose(0, 1)\r\n    nexth, output = func(input, hidden, weight)\r\n    if batch_first:\r\n        output = output.transpose(0, 1)\r\n```"}