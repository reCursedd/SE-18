{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391050728", "html_url": "https://github.com/pytorch/pytorch/pull/7596#issuecomment-391050728", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7596", "id": 391050728, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTA1MDcyOA==", "user": {"login": "lematt1991", "id": 13142923, "node_id": "MDQ6VXNlcjEzMTQyOTIz", "avatar_url": "https://avatars1.githubusercontent.com/u/13142923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lematt1991", "html_url": "https://github.com/lematt1991", "followers_url": "https://api.github.com/users/lematt1991/followers", "following_url": "https://api.github.com/users/lematt1991/following{/other_user}", "gists_url": "https://api.github.com/users/lematt1991/gists{/gist_id}", "starred_url": "https://api.github.com/users/lematt1991/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lematt1991/subscriptions", "organizations_url": "https://api.github.com/users/lematt1991/orgs", "repos_url": "https://api.github.com/users/lematt1991/repos", "events_url": "https://api.github.com/users/lematt1991/events{/privacy}", "received_events_url": "https://api.github.com/users/lematt1991/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-22T16:14:53Z", "updated_at": "2018-05-22T16:14:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hmm, that's strange.  I've pulled the <code>linux-trusty-py3.6-gcc7.2</code> docker container and tried to reproduce this issue.  If I print out the <code>running_loss</code> in each iteration and dump it to a file, multiple runs are giving me the exact same results:</p>\n<pre><code>jenkins@c8c8df3924a6:/workspace$ ./tools/cpp_build/build/libtorch/bin/test_api optim --section lbfgs &gt; out1\nCUDA not available. Disabling CUDA tests\njenkins@c8c8df3924a6:/workspace$ ./tools/cpp_build/build/libtorch/bin/test_api optim --section lbfgs &gt; out2\nCUDA not available. Disabling CUDA tests\njenkins@c8c8df3924a6:/workspace$ head -n 5 out1\nloss = 0.996719\nloss = 0.994648\nloss = 0.991368\nloss = 0.988453\nloss = 0.985467\njenkins@c8c8df3924a6:/workspace$ diff out1 out2\njenkins@c8c8df3924a6:/workspace$\n</code></pre>\n<p>I've seen that LBFGS is very sensitive as even very minor changes to any of the hyper parameters seem to cause it to diverge.  Maybe following <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>'s suggestion of testing it by comparing it to the Python implementation would be a better approach, but I'm not sure I see what could be causing the nondeterminism with all of the RNGs seeded.  What are your guys' thoughts?</p>", "body_text": "Hmm, that's strange.  I've pulled the linux-trusty-py3.6-gcc7.2 docker container and tried to reproduce this issue.  If I print out the running_loss in each iteration and dump it to a file, multiple runs are giving me the exact same results:\njenkins@c8c8df3924a6:/workspace$ ./tools/cpp_build/build/libtorch/bin/test_api optim --section lbfgs > out1\nCUDA not available. Disabling CUDA tests\njenkins@c8c8df3924a6:/workspace$ ./tools/cpp_build/build/libtorch/bin/test_api optim --section lbfgs > out2\nCUDA not available. Disabling CUDA tests\njenkins@c8c8df3924a6:/workspace$ head -n 5 out1\nloss = 0.996719\nloss = 0.994648\nloss = 0.991368\nloss = 0.988453\nloss = 0.985467\njenkins@c8c8df3924a6:/workspace$ diff out1 out2\njenkins@c8c8df3924a6:/workspace$\n\nI've seen that LBFGS is very sensitive as even very minor changes to any of the hyper parameters seem to cause it to diverge.  Maybe following @ezyang's suggestion of testing it by comparing it to the Python implementation would be a better approach, but I'm not sure I see what could be causing the nondeterminism with all of the RNGs seeded.  What are your guys' thoughts?", "body": "Hmm, that's strange.  I've pulled the `linux-trusty-py3.6-gcc7.2` docker container and tried to reproduce this issue.  If I print out the `running_loss` in each iteration and dump it to a file, multiple runs are giving me the exact same results:\r\n\r\n```\r\njenkins@c8c8df3924a6:/workspace$ ./tools/cpp_build/build/libtorch/bin/test_api optim --section lbfgs > out1\r\nCUDA not available. Disabling CUDA tests\r\njenkins@c8c8df3924a6:/workspace$ ./tools/cpp_build/build/libtorch/bin/test_api optim --section lbfgs > out2\r\nCUDA not available. Disabling CUDA tests\r\njenkins@c8c8df3924a6:/workspace$ head -n 5 out1\r\nloss = 0.996719\r\nloss = 0.994648\r\nloss = 0.991368\r\nloss = 0.988453\r\nloss = 0.985467\r\njenkins@c8c8df3924a6:/workspace$ diff out1 out2\r\njenkins@c8c8df3924a6:/workspace$\r\n```\r\n\r\nI've seen that LBFGS is very sensitive as even very minor changes to any of the hyper parameters seem to cause it to diverge.  Maybe following @ezyang's suggestion of testing it by comparing it to the Python implementation would be a better approach, but I'm not sure I see what could be causing the nondeterminism with all of the RNGs seeded.  What are your guys' thoughts?\r\n\r\n"}