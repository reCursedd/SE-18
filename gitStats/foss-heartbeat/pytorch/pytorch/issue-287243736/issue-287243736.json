{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4567", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4567/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4567/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4567/events", "html_url": "https://github.com/pytorch/pytorch/issues/4567", "id": 287243736, "node_id": "MDU6SXNzdWUyODcyNDM3MzY=", "number": 4567, "title": "Generalize 'grad' buffer hack in derivatives.yaml", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-01-09T21:54:44Z", "updated_at": "2018-01-17T18:00:29Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Can you spot the difference between these two definitions in derivatives.yaml?</p>\n<div class=\"highlight highlight-source-diff\"><pre>index bb24685..47cb958 100644\n<span class=\"pl-md\">--- a/tools/autograd/derivatives.yaml</span>\n<span class=\"pl-mi1\">+++ b/tools/autograd/derivatives.yaml</span>\n<span class=\"pl-mdr\">@@ -1035,7 +1035,7 @@</span>\n # work.)\n # NB2: The quotes around the gradient are needed to appease YAML parsing rules.\n - name: cudnn_batch_norm(Tensor input, Tensor weight, Tensor bias, Tensor running_mean, Tensor running_var, bool training, double exponential_average_factor, double epsilon)\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  input, weight, bias: \"training ? cudnn_batch_norm_backward(input, grads[0].contiguous(), weight, running_mean, running_var, result1, result2, epsilon) : thnn_batch_norm_backward(grads[0].contiguous(), input, weight, running_mean, running_var, training, epsilon, result1, result2, grad_input_mask)\"</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  input, weight, bias: \"training ? cudnn_batch_norm_backward(input, grad.contiguous(), weight, running_mean, running_var, result1, result2, epsilon) : thnn_batch_norm_backward(grad.contiguous(), input, weight, running_mean, running_var, training, epsilon, result1, result2, grad_input_mask)\"</span></pre></div>\n<p>One of these definitions uses <code>grads[0]</code>, and the other uses <code>grad</code>. They are different: if you say <code>grads[0]</code>, you are implying that ALL outputs of the function are differentiable; if you say <code>grad</code>, you are saying that only the FIRST output is differentiable. This matters because it affects how we set <code>requires_grad</code> flag, and was the root cause of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"285282516\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4422\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4422/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4422\">#4422</a>.</p>\n<p>One way to soften this sharp edge is to special case <code>grads[0]</code>, if it is the only used index of <code>grads</code>. But the proper way to fix this is to observe that we should ONLY treat an output as differentiable if its corresponding <code>grads[i]</code> is referenced in <em>some way</em> from derivative computation. Unfortunately, this is a little complicated to implement because code generation needs to be adjusted to handle this case as well; and it is not a priority until we find a function that (1) returns multiple differentiable outputs, but (2) also returns nondifferentiable outputs.</p>", "body_text": "Can you spot the difference between these two definitions in derivatives.yaml?\nindex bb24685..47cb958 100644\n--- a/tools/autograd/derivatives.yaml\n+++ b/tools/autograd/derivatives.yaml\n@@ -1035,7 +1035,7 @@\n # work.)\n # NB2: The quotes around the gradient are needed to appease YAML parsing rules.\n - name: cudnn_batch_norm(Tensor input, Tensor weight, Tensor bias, Tensor running_mean, Tensor running_var, bool training, double exponential_average_factor, double epsilon)\n-  input, weight, bias: \"training ? cudnn_batch_norm_backward(input, grads[0].contiguous(), weight, running_mean, running_var, result1, result2, epsilon) : thnn_batch_norm_backward(grads[0].contiguous(), input, weight, running_mean, running_var, training, epsilon, result1, result2, grad_input_mask)\"\n+  input, weight, bias: \"training ? cudnn_batch_norm_backward(input, grad.contiguous(), weight, running_mean, running_var, result1, result2, epsilon) : thnn_batch_norm_backward(grad.contiguous(), input, weight, running_mean, running_var, training, epsilon, result1, result2, grad_input_mask)\"\nOne of these definitions uses grads[0], and the other uses grad. They are different: if you say grads[0], you are implying that ALL outputs of the function are differentiable; if you say grad, you are saying that only the FIRST output is differentiable. This matters because it affects how we set requires_grad flag, and was the root cause of #4422.\nOne way to soften this sharp edge is to special case grads[0], if it is the only used index of grads. But the proper way to fix this is to observe that we should ONLY treat an output as differentiable if its corresponding grads[i] is referenced in some way from derivative computation. Unfortunately, this is a little complicated to implement because code generation needs to be adjusted to handle this case as well; and it is not a priority until we find a function that (1) returns multiple differentiable outputs, but (2) also returns nondifferentiable outputs.", "body": "Can you spot the difference between these two definitions in derivatives.yaml?\r\n\r\n```diff --git a/tools/autograd/derivatives.yaml b/tools/autograd/derivatives.yaml\r\nindex bb24685..47cb958 100644\r\n--- a/tools/autograd/derivatives.yaml\r\n+++ b/tools/autograd/derivatives.yaml\r\n@@ -1035,7 +1035,7 @@\r\n # work.)\r\n # NB2: The quotes around the gradient are needed to appease YAML parsing rules.\r\n - name: cudnn_batch_norm(Tensor input, Tensor weight, Tensor bias, Tensor running_mean, Tensor running_var, bool training, double exponential_average_factor, double epsilon)\r\n-  input, weight, bias: \"training ? cudnn_batch_norm_backward(input, grads[0].contiguous(), weight, running_mean, running_var, result1, result2, epsilon) : thnn_batch_norm_backward(grads[0].contiguous(), input, weight, running_mean, running_var, training, epsilon, result1, result2, grad_input_mask)\"\r\n+  input, weight, bias: \"training ? cudnn_batch_norm_backward(input, grad.contiguous(), weight, running_mean, running_var, result1, result2, epsilon) : thnn_batch_norm_backward(grad.contiguous(), input, weight, running_mean, running_var, training, epsilon, result1, result2, grad_input_mask)\"\r\n```\r\n\r\nOne of these definitions uses `grads[0]`, and the other uses `grad`. They are different: if you say `grads[0]`, you are implying that ALL outputs of the function are differentiable; if you say `grad`, you are saying that only the FIRST output is differentiable. This matters because it affects how we set `requires_grad` flag, and was the root cause of #4422.\r\n\r\nOne way to soften this sharp edge is to special case `grads[0]`, if it is the only used index of `grads`. But the proper way to fix this is to observe that we should ONLY treat an output as differentiable if its corresponding `grads[i]` is referenced in *some way* from derivative computation. Unfortunately, this is a little complicated to implement because code generation needs to be adjusted to handle this case as well; and it is not a priority until we find a function that (1) returns multiple differentiable outputs, but (2) also returns nondifferentiable outputs."}