{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5655", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5655/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5655/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5655/events", "html_url": "https://github.com/pytorch/pytorch/pull/5655", "id": 303696034, "node_id": "MDExOlB1bGxSZXF1ZXN0MTczODk5MjU2", "number": 5655, "title": "Add gpu guard for broadcast_coalesce", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-03-09T01:50:05Z", "updated_at": "2018-03-09T02:59:20Z", "closed_at": "2018-03-09T02:59:20Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5655", "html_url": "https://github.com/pytorch/pytorch/pull/5655", "diff_url": "https://github.com/pytorch/pytorch/pull/5655.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5655.patch"}, "body_html": "<p>This patch fixes a bug triggered by <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"296292388\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5182\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/5182/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/5182\">#5182</a> when we have multiple layers in the model, and the DDP is run on a single node, with a subset of GPUs each.<br>\nFor example, as in the test we run 2 processes on a 8 GPU node, both processes are visible to all GPUs. We create the DDP model by <code>nn.parallel.DistributedDataParallel(model_DDP, device_ids=gpu_subset)</code> where gpu_subset is 0,1,2,3 for process 1, and 4,5,6,7 for process 2.<br>\nutils::flatten_dense_tensors(chunk.tensors) will actually create a new Tensor which a flatten version of layer weights. Without this patch, this tensor goes to default GPU 0 despite all layers weights for process 2 are on GPU4, this will further error out when broadcast requires the tensor to be on the GPU 4 for process 2.<br>\nThe gpu guard inside the for loop has nothing to do with the current bug, I thought it's good to add it as a safety guard.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>", "body_text": "This patch fixes a bug triggered by #5182 when we have multiple layers in the model, and the DDP is run on a single node, with a subset of GPUs each.\nFor example, as in the test we run 2 processes on a 8 GPU node, both processes are visible to all GPUs. We create the DDP model by nn.parallel.DistributedDataParallel(model_DDP, device_ids=gpu_subset) where gpu_subset is 0,1,2,3 for process 1, and 4,5,6,7 for process 2.\nutils::flatten_dense_tensors(chunk.tensors) will actually create a new Tensor which a flatten version of layer weights. Without this patch, this tensor goes to default GPU 0 despite all layers weights for process 2 are on GPU4, this will further error out when broadcast requires the tensor to be on the GPU 4 for process 2.\nThe gpu guard inside the for loop has nothing to do with the current bug, I thought it's good to add it as a safety guard.\n@apaszke", "body": "This patch fixes a bug triggered by #5182 when we have multiple layers in the model, and the DDP is run on a single node, with a subset of GPUs each. \r\nFor example, as in the test we run 2 processes on a 8 GPU node, both processes are visible to all GPUs. We create the DDP model by `nn.parallel.DistributedDataParallel(model_DDP, device_ids=gpu_subset)` where gpu_subset is 0,1,2,3 for process 1, and 4,5,6,7 for process 2. \r\nutils::flatten_dense_tensors(chunk.tensors) will actually create a new Tensor which a flatten version of layer weights. Without this patch, this tensor goes to default GPU 0 despite all layers weights for process 2 are on GPU4, this will further error out when broadcast requires the tensor to be on the GPU 4 for process 2. \r\nThe gpu guard inside the for loop has nothing to do with the current bug, I thought it's good to add it as a safety guard. \r\n@apaszke "}