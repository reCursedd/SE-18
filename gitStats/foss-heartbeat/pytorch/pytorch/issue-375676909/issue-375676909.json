{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13328", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13328/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13328/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13328/events", "html_url": "https://github.com/pytorch/pytorch/issues/13328", "id": 375676909, "node_id": "MDU6SXNzdWUzNzU2NzY5MDk=", "number": 13328, "title": "l1loss different results based on arguments position", "user": {"login": "Vozf", "id": 22998537, "node_id": "MDQ6VXNlcjIyOTk4NTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/22998537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Vozf", "html_url": "https://github.com/Vozf", "followers_url": "https://api.github.com/users/Vozf/followers", "following_url": "https://api.github.com/users/Vozf/following{/other_user}", "gists_url": "https://api.github.com/users/Vozf/gists{/gist_id}", "starred_url": "https://api.github.com/users/Vozf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Vozf/subscriptions", "organizations_url": "https://api.github.com/users/Vozf/orgs", "repos_url": "https://api.github.com/users/Vozf/repos", "events_url": "https://api.github.com/users/Vozf/events{/privacy}", "received_events_url": "https://api.github.com/users/Vozf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-30T20:22:52Z", "updated_at": "2018-11-08T09:14:28Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>l1loss changes its value if arguments are swapped</p>\n\n<h2>To Reproduce</h2>\n<pre><code>with torch.set_grad_enabled(False):\n    pred1 = model(x)\n    \nwith torch.set_grad_enabled(True):\n    pred2 = model(x)\n\n\nprint(\n        (nn.functional.l1_loss(y, pred1),\n         nn.functional.l1_loss(pred1, y),\n         nn.functional.l1_loss(y, pred2), \n         nn.functional.l1_loss(pred2, y))\n    )\n\n</code></pre>\n<p>output</p>\n<pre><code>(tensor(0.6039, device='cuda:0'), tensor(0.6039, device='cuda:0'), tensor(150279.9062, device='cuda:0', grad_fn=&lt;SumBackward0&gt;), tensor(0.6039, device='cuda:0', grad_fn=&lt;L1LossBackward&gt;))\n</code></pre>\n<h2>Expected behavior</h2>\n<p>All printed losses are equal</p>\n\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version (e.g., 1.0): '0.4.1'</li>\n<li>OS (e.g., Linux): ubuntu</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): pip</li>\n<li>Build command you used (if compiling from source):</li>\n<li>Python version:3.6</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration:</li>\n<li>Any other relevant information:</li>\n</ul>\n<h2>Additional context</h2>\n<p>the model itself i think is irrelevant although here it is</p>\n<pre><code>def get_conv(in_channels, out_channels, kernel_size=3, actn=True):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)]\n    if actn: layers.append(nn.ReLU())\n    \n    return nn.Sequential(*layers)\n    \n\nclass ResSequential(nn.Module):\n    def __init__(self, layers, mult):\n        super().__init__()\n        self.layers = nn.Sequential(*layers)\n        self.mult = mult\n    \n    def forward(self, input):\n        return input + self.layers(input) * self.mult\n\ndef res_block(num_features):\n    layers = [get_conv(num_features, num_features),\n              get_conv(num_features, num_features, actn=False)]\n    return ResSequential(layers, 0.1)\n\ndef upsample(in_channels, out_channels, scale):\n    layers = []\n    for i in range(int(log(scale, 2))):\n        layers += [get_conv(in_channels, out_channels * 4), nn.PixelShuffle(2)]\n        \n    return nn.Sequential(*layers)\n\nclass SuperResNet(nn.Module):\n    def __init__(self, scale, nf=64):\n        super().__init__()\n        \n        layers = [\n            get_conv(3, nf),\n            *[res_block(nf) for i in range(8)],\n            upsample(nf, nf, scale),\n            nn.BatchNorm2d(nf),\n            get_conv(nf, 3, actn=False),\n        ]\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)\n\nmodel = SuperResNet(scale)\n\ndevice = torch.device(\"cuda:0\")\nmodel = model.to(device)\n\nmodel\n</code></pre>", "body_text": "\ud83d\udc1b Bug\nl1loss changes its value if arguments are swapped\n\nTo Reproduce\nwith torch.set_grad_enabled(False):\n    pred1 = model(x)\n    \nwith torch.set_grad_enabled(True):\n    pred2 = model(x)\n\n\nprint(\n        (nn.functional.l1_loss(y, pred1),\n         nn.functional.l1_loss(pred1, y),\n         nn.functional.l1_loss(y, pred2), \n         nn.functional.l1_loss(pred2, y))\n    )\n\n\noutput\n(tensor(0.6039, device='cuda:0'), tensor(0.6039, device='cuda:0'), tensor(150279.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(0.6039, device='cuda:0', grad_fn=<L1LossBackward>))\n\nExpected behavior\nAll printed losses are equal\n\nEnvironment\n\nPyTorch Version (e.g., 1.0): '0.4.1'\nOS (e.g., Linux): ubuntu\nHow you installed PyTorch (conda, pip, source): pip\nBuild command you used (if compiling from source):\nPython version:3.6\nCUDA/cuDNN version:\nGPU models and configuration:\nAny other relevant information:\n\nAdditional context\nthe model itself i think is irrelevant although here it is\ndef get_conv(in_channels, out_channels, kernel_size=3, actn=True):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)]\n    if actn: layers.append(nn.ReLU())\n    \n    return nn.Sequential(*layers)\n    \n\nclass ResSequential(nn.Module):\n    def __init__(self, layers, mult):\n        super().__init__()\n        self.layers = nn.Sequential(*layers)\n        self.mult = mult\n    \n    def forward(self, input):\n        return input + self.layers(input) * self.mult\n\ndef res_block(num_features):\n    layers = [get_conv(num_features, num_features),\n              get_conv(num_features, num_features, actn=False)]\n    return ResSequential(layers, 0.1)\n\ndef upsample(in_channels, out_channels, scale):\n    layers = []\n    for i in range(int(log(scale, 2))):\n        layers += [get_conv(in_channels, out_channels * 4), nn.PixelShuffle(2)]\n        \n    return nn.Sequential(*layers)\n\nclass SuperResNet(nn.Module):\n    def __init__(self, scale, nf=64):\n        super().__init__()\n        \n        layers = [\n            get_conv(3, nf),\n            *[res_block(nf) for i in range(8)],\n            upsample(nf, nf, scale),\n            nn.BatchNorm2d(nf),\n            get_conv(nf, 3, actn=False),\n        ]\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)\n\nmodel = SuperResNet(scale)\n\ndevice = torch.device(\"cuda:0\")\nmodel = model.to(device)\n\nmodel", "body": "## \ud83d\udc1b Bug\r\nl1loss changes its value if arguments are swapped\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\n```\r\nwith torch.set_grad_enabled(False):\r\n    pred1 = model(x)\r\n    \r\nwith torch.set_grad_enabled(True):\r\n    pred2 = model(x)\r\n\r\n\r\nprint(\r\n        (nn.functional.l1_loss(y, pred1),\r\n         nn.functional.l1_loss(pred1, y),\r\n         nn.functional.l1_loss(y, pred2), \r\n         nn.functional.l1_loss(pred2, y))\r\n    )\r\n\r\n```\r\noutput \r\n```\r\n(tensor(0.6039, device='cuda:0'), tensor(0.6039, device='cuda:0'), tensor(150279.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(0.6039, device='cuda:0', grad_fn=<L1LossBackward>))\r\n```\r\n\r\n\r\n## Expected behavior\r\nAll printed losses are equal\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): '0.4.1'\r\n - OS (e.g., Linux): ubuntu\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version:3.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nthe model itself i think is irrelevant although here it is\r\n\r\n```\r\ndef get_conv(in_channels, out_channels, kernel_size=3, actn=True):\r\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)]\r\n    if actn: layers.append(nn.ReLU())\r\n    \r\n    return nn.Sequential(*layers)\r\n    \r\n\r\nclass ResSequential(nn.Module):\r\n    def __init__(self, layers, mult):\r\n        super().__init__()\r\n        self.layers = nn.Sequential(*layers)\r\n        self.mult = mult\r\n    \r\n    def forward(self, input):\r\n        return input + self.layers(input) * self.mult\r\n\r\ndef res_block(num_features):\r\n    layers = [get_conv(num_features, num_features),\r\n              get_conv(num_features, num_features, actn=False)]\r\n    return ResSequential(layers, 0.1)\r\n\r\ndef upsample(in_channels, out_channels, scale):\r\n    layers = []\r\n    for i in range(int(log(scale, 2))):\r\n        layers += [get_conv(in_channels, out_channels * 4), nn.PixelShuffle(2)]\r\n        \r\n    return nn.Sequential(*layers)\r\n\r\nclass SuperResNet(nn.Module):\r\n    def __init__(self, scale, nf=64):\r\n        super().__init__()\r\n        \r\n        layers = [\r\n            get_conv(3, nf),\r\n            *[res_block(nf) for i in range(8)],\r\n            upsample(nf, nf, scale),\r\n            nn.BatchNorm2d(nf),\r\n            get_conv(nf, 3, actn=False),\r\n        ]\r\n        self.model = nn.Sequential(*layers)\r\n    \r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\nmodel = SuperResNet(scale)\r\n\r\ndevice = torch.device(\"cuda:0\")\r\nmodel = model.to(device)\r\n\r\nmodel\r\n```\r\n"}