{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434603060", "html_url": "https://github.com/pytorch/pytorch/issues/13328#issuecomment-434603060", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13328", "id": 434603060, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDYwMzA2MA==", "user": {"login": "arsenyinfo", "id": 4929993, "node_id": "MDQ6VXNlcjQ5Mjk5OTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/4929993?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arsenyinfo", "html_url": "https://github.com/arsenyinfo", "followers_url": "https://api.github.com/users/arsenyinfo/followers", "following_url": "https://api.github.com/users/arsenyinfo/following{/other_user}", "gists_url": "https://api.github.com/users/arsenyinfo/gists{/gist_id}", "starred_url": "https://api.github.com/users/arsenyinfo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arsenyinfo/subscriptions", "organizations_url": "https://api.github.com/users/arsenyinfo/orgs", "repos_url": "https://api.github.com/users/arsenyinfo/repos", "events_url": "https://api.github.com/users/arsenyinfo/events{/privacy}", "received_events_url": "https://api.github.com/users/arsenyinfo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T08:35:32Z", "updated_at": "2018-10-31T08:35:32Z", "author_association": "NONE", "body_html": "<p>I believe it's somehow related to autograd:</p>\n<pre><code>ipdb&gt; mse = lambda x, y: np.mean((x-y)**2)\n\n# MSE is expected to be symmetric\nipdb&gt; mse(gt.cpu().detach().numpy(), pred.cpu().detach().numpy())\n0.35414878\n\nipdb&gt; mse(pred.cpu().detach().numpy(), gt.cpu().detach().numpy())\n0.35414878\n# OK, it is really symmetric\n\nipdb&gt; F.mse_loss(gt.cpu(), pred.cpu())\ntensor(417771., grad_fn=&lt;SumBackward0&gt;)\n# surprise!\n\nipdb&gt; F.mse_loss(pred.cpu(), gt.cpu())\ntensor(0.3541, grad_fn=&lt;MseLossBackward&gt;)\n# works fine when the arguments are not messed\n\nipdb&gt; F.mse_loss(pred.cpu().detach(), gt.cpu().detach())\ntensor(0.3541)\nipdb&gt; F.mse_loss(gt.cpu().detach(), pred.cpu().detach())\ntensor(0.3541)\n# also works fine when detached\n</code></pre>\n<p>I'm not sure if it's a bug as there are no guarantees that a function works fine when arguments are slightly misused, however it's really counterintuitive.</p>\n<p>IMHO it makes sense to add a warning. E.g. I use this decorator for my own losses:</p>\n<pre><code>def safe_loss(f):\n    \"\"\"\n    When loss is decorated with a `safe_loss`, it helps you avoid a bug with incorrect arguments order.\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(y_pred, y_true, **kwargs):\n        if y_true.grad_fn is not None:\n            warn('Usually y_true should have no gradients attached. Please make sure you\\'re calling the loss properly')\n        return f(y_pred, y_true, **kwargs)\n\n    return wrapper\n</code></pre>", "body_text": "I believe it's somehow related to autograd:\nipdb> mse = lambda x, y: np.mean((x-y)**2)\n\n# MSE is expected to be symmetric\nipdb> mse(gt.cpu().detach().numpy(), pred.cpu().detach().numpy())\n0.35414878\n\nipdb> mse(pred.cpu().detach().numpy(), gt.cpu().detach().numpy())\n0.35414878\n# OK, it is really symmetric\n\nipdb> F.mse_loss(gt.cpu(), pred.cpu())\ntensor(417771., grad_fn=<SumBackward0>)\n# surprise!\n\nipdb> F.mse_loss(pred.cpu(), gt.cpu())\ntensor(0.3541, grad_fn=<MseLossBackward>)\n# works fine when the arguments are not messed\n\nipdb> F.mse_loss(pred.cpu().detach(), gt.cpu().detach())\ntensor(0.3541)\nipdb> F.mse_loss(gt.cpu().detach(), pred.cpu().detach())\ntensor(0.3541)\n# also works fine when detached\n\nI'm not sure if it's a bug as there are no guarantees that a function works fine when arguments are slightly misused, however it's really counterintuitive.\nIMHO it makes sense to add a warning. E.g. I use this decorator for my own losses:\ndef safe_loss(f):\n    \"\"\"\n    When loss is decorated with a `safe_loss`, it helps you avoid a bug with incorrect arguments order.\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(y_pred, y_true, **kwargs):\n        if y_true.grad_fn is not None:\n            warn('Usually y_true should have no gradients attached. Please make sure you\\'re calling the loss properly')\n        return f(y_pred, y_true, **kwargs)\n\n    return wrapper", "body": "I believe it's somehow related to autograd: \r\n\r\n```\r\nipdb> mse = lambda x, y: np.mean((x-y)**2)\r\n\r\n# MSE is expected to be symmetric\r\nipdb> mse(gt.cpu().detach().numpy(), pred.cpu().detach().numpy())\r\n0.35414878\r\n\r\nipdb> mse(pred.cpu().detach().numpy(), gt.cpu().detach().numpy())\r\n0.35414878\r\n# OK, it is really symmetric\r\n\r\nipdb> F.mse_loss(gt.cpu(), pred.cpu())\r\ntensor(417771., grad_fn=<SumBackward0>)\r\n# surprise!\r\n\r\nipdb> F.mse_loss(pred.cpu(), gt.cpu())\r\ntensor(0.3541, grad_fn=<MseLossBackward>)\r\n# works fine when the arguments are not messed\r\n\r\nipdb> F.mse_loss(pred.cpu().detach(), gt.cpu().detach())\r\ntensor(0.3541)\r\nipdb> F.mse_loss(gt.cpu().detach(), pred.cpu().detach())\r\ntensor(0.3541)\r\n# also works fine when detached\r\n```\r\n\r\nI'm not sure if it's a bug as there are no guarantees that a function works fine when arguments are slightly misused, however it's really counterintuitive. \r\n\r\nIMHO it makes sense to add a warning. E.g. I use this decorator for my own losses: \r\n\r\n```\r\ndef safe_loss(f):\r\n    \"\"\"\r\n    When loss is decorated with a `safe_loss`, it helps you avoid a bug with incorrect arguments order.\r\n    \"\"\"\r\n\r\n    @wraps(f)\r\n    def wrapper(y_pred, y_true, **kwargs):\r\n        if y_true.grad_fn is not None:\r\n            warn('Usually y_true should have no gradients attached. Please make sure you\\'re calling the loss properly')\r\n        return f(y_pred, y_true, **kwargs)\r\n\r\n    return wrapper\r\n```"}