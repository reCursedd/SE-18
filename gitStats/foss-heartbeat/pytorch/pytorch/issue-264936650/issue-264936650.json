{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3089", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3089/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3089/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3089/events", "html_url": "https://github.com/pytorch/pytorch/issues/3089", "id": 264936650, "node_id": "MDU6SXNzdWUyNjQ5MzY2NTA=", "number": 3089, "title": "Segfault when using data parallel on PPC64", "user": {"login": "M-Eng", "id": 8820331, "node_id": "MDQ6VXNlcjg4MjAzMzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/8820331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/M-Eng", "html_url": "https://github.com/M-Eng", "followers_url": "https://api.github.com/users/M-Eng/followers", "following_url": "https://api.github.com/users/M-Eng/following{/other_user}", "gists_url": "https://api.github.com/users/M-Eng/gists{/gist_id}", "starred_url": "https://api.github.com/users/M-Eng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/M-Eng/subscriptions", "organizations_url": "https://api.github.com/users/M-Eng/orgs", "repos_url": "https://api.github.com/users/M-Eng/repos", "events_url": "https://api.github.com/users/M-Eng/events{/privacy}", "received_events_url": "https://api.github.com/users/M-Eng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-12T13:12:07Z", "updated_at": "2017-10-12T15:44:46Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>When training a resnet152 wrap into a data_parallel it creates a segfault.<br>\nIt works on a single GPU with the same parameters, but when switching to two or more it produces a segfault after a few iterations.</p>\n<p>Here is the gdb trace:</p>\n<blockquote>\n<p>Thread 10 \"python\" received signal SIGSEGV, Segmentation fault.<br>\n[Switching to Thread 0x120000d1f1a0 (LWP 101627)]<br>\ntorch::autograd::InputBuffer::add (this=0x120000d1e2a0, pos=0, var=...) at torch/csrc/autograd/input_buffer.cpp:17<br>\n17        if (!item.first.defined()) {<br>\n(gdb) bt<br>\n#0  torch::autograd::InputBuffer::add (this=0x120000d1e2a0, pos=0, var=...) at torch/csrc/autograd/input_buffer.cpp:17<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"171281708\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1\">#1</a>  0x00001000a6e6774c in torch::autograd::Engine::evaluate_function (this=, task=...)<br>\nat torch/csrc/autograd/engine.cpp:268<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"171402941\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2\">#2</a>  0x00001000a6e6920c in torch::autograd::Engine::thread_main (this=0x1000a80e1700 , graph_task=0x0)<br>\nat torch/csrc/autograd/engine.cpp:144<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"171485123\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3\">#3</a>  0x00001000a6e6522c in torch::autograd::Engine::thread_init (this=0x1000a80e1700 , device=)<br>\nat torch/csrc/autograd/engine.cpp:121<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"171522963\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4\">#4</a>  0x00001000a6e96590 in torch::autograd::python::PythonEngine::thread_init (this=0x1000a80e1700 ,<br>\ndevice=) at torch/csrc/autograd/python_engine.cpp:28<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"173498149\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5\">#5</a>  0x00001000a6e6c014 in std::_Mem_fn_base&lt;void (torch::autograd::Engine::<em>)(int), true&gt;::operator()&lt;int, void&gt;(torch::autograd::Engine</em>, int&amp;&amp;) const (__object=, this=)<br>\nat /usr/include/c++/5/functional:600<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"174289461\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6\">#6</a>  std::_Bind_simple&lt;std::_Mem_fn&lt;void (torch::autograd::Engine::<em>)(int)&gt; (torch::autograd::Engine</em>, int)&gt;::_M_invoke&lt;0ul, 1ul&gt;(std::_Index_tuple&lt;0ul, 1ul&gt;) (this=)<br>\n---Type  to continue, or q  to quit---<br>\n5/functional:1531<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"174818921\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7\">#7</a>  std::_Bind_simple&lt;std::_Mem_fn&lt;void (torch::autograd::Engine::<em>)(int)&gt; (torch::autograd::Engine</em>, int)&gt;::operator()() (this=) at /usr/include/c++/5/functional:1520<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"174871471\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8\">#8</a>  std::thread::_Impl&lt;std::_Bind_simple&lt;std::_Mem_fn&lt;void (torch::autograd::Engine::<em>)(int)&gt; (torch::autograd::Engine</em>, int)&gt; &gt;::_M_run() (this=) at /usr/include/c++/5/thread:115<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"175537036\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9\">#9</a>  0x00001000a30f49a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"175552272\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/10\">#10</a> 0x0000100000338070 in start_thread (arg=0x120000d1f1a0) at pthread_create.c:335<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"175559413\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11\">#11</a> 0x00001000005e3a30 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96</p>\n</blockquote>\n<p>I'm working a ppc64 machine, cuda-8.0, where I installed pytorch from source, no error at compilation. Tests fails but might come from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"222733393\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1297\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1297/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1297\">#1297</a>. The installation seems to work fine on a single GPU.</p>", "body_text": "When training a resnet152 wrap into a data_parallel it creates a segfault.\nIt works on a single GPU with the same parameters, but when switching to two or more it produces a segfault after a few iterations.\nHere is the gdb trace:\n\nThread 10 \"python\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x120000d1f1a0 (LWP 101627)]\ntorch::autograd::InputBuffer::add (this=0x120000d1e2a0, pos=0, var=...) at torch/csrc/autograd/input_buffer.cpp:17\n17        if (!item.first.defined()) {\n(gdb) bt\n#0  torch::autograd::InputBuffer::add (this=0x120000d1e2a0, pos=0, var=...) at torch/csrc/autograd/input_buffer.cpp:17\n#1  0x00001000a6e6774c in torch::autograd::Engine::evaluate_function (this=, task=...)\nat torch/csrc/autograd/engine.cpp:268\n#2  0x00001000a6e6920c in torch::autograd::Engine::thread_main (this=0x1000a80e1700 , graph_task=0x0)\nat torch/csrc/autograd/engine.cpp:144\n#3  0x00001000a6e6522c in torch::autograd::Engine::thread_init (this=0x1000a80e1700 , device=)\nat torch/csrc/autograd/engine.cpp:121\n#4  0x00001000a6e96590 in torch::autograd::python::PythonEngine::thread_init (this=0x1000a80e1700 ,\ndevice=) at torch/csrc/autograd/python_engine.cpp:28\n#5  0x00001000a6e6c014 in std::_Mem_fn_base<void (torch::autograd::Engine::)(int), true>::operator()<int, void>(torch::autograd::Engine, int&&) const (__object=, this=)\nat /usr/include/c++/5/functional:600\n#6  std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::)(int)> (torch::autograd::Engine, int)>::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (this=)\n---Type  to continue, or q  to quit---\n5/functional:1531\n#7  std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::)(int)> (torch::autograd::Engine, int)>::operator()() (this=) at /usr/include/c++/5/functional:1520\n#8  std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::)(int)> (torch::autograd::Engine, int)> >::_M_run() (this=) at /usr/include/c++/5/thread:115\n#9  0x00001000a30f49a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6\n#10 0x0000100000338070 in start_thread (arg=0x120000d1f1a0) at pthread_create.c:335\n#11 0x00001000005e3a30 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96\n\nI'm working a ppc64 machine, cuda-8.0, where I installed pytorch from source, no error at compilation. Tests fails but might come from #1297. The installation seems to work fine on a single GPU.", "body": "When training a resnet152 wrap into a data_parallel it creates a segfault. \r\nIt works on a single GPU with the same parameters, but when switching to two or more it produces a segfault after a few iterations.\r\n\r\nHere is the gdb trace:\r\n\r\n> Thread 10 \"python\" received signal SIGSEGV, Segmentation fault.\r\n> [Switching to Thread 0x120000d1f1a0 (LWP 101627)]\r\n> torch::autograd::InputBuffer::add (this=0x120000d1e2a0, pos=0, var=...) at torch/csrc/autograd/input_buffer.cpp:17\r\n> 17        if (!item.first.defined()) {\r\n> (gdb) bt\r\n> #0  torch::autograd::InputBuffer::add (this=0x120000d1e2a0, pos=0, var=...) at torch/csrc/autograd/input_buffer.cpp:17\r\n> #1  0x00001000a6e6774c in torch::autograd::Engine::evaluate_function (this=<optimized out>, task=...)\r\n>     at torch/csrc/autograd/engine.cpp:268\r\n> #2  0x00001000a6e6920c in torch::autograd::Engine::thread_main (this=0x1000a80e1700 <engine>, graph_task=0x0)\r\n>     at torch/csrc/autograd/engine.cpp:144\r\n> #3  0x00001000a6e6522c in torch::autograd::Engine::thread_init (this=0x1000a80e1700 <engine>, device=<optimized out>)\r\n>     at torch/csrc/autograd/engine.cpp:121\r\n> #4  0x00001000a6e96590 in torch::autograd::python::PythonEngine::thread_init (this=0x1000a80e1700 <engine>,\r\n>     device=<optimized out>) at torch/csrc/autograd/python_engine.cpp:28\r\n> #5  0x00001000a6e6c014 in std::_Mem_fn_base<void (torch::autograd::Engine::*)(int), true>::operator()<int, void>(torch::autograd::Engine*, int&&) const (__object=<optimized out>, this=<error reading variable: value has been optimized out>)\r\n>     at /usr/include/c++/5/functional:600\r\n> #6  std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (this=<error reading variable: value has been optimized out>)\r\n> ---Type <return> to continue, or q <return> to quit---\r\n>    5/functional:1531\r\n> #7  std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::operator()() (this=<error reading variable: value has been optimized out>) at /usr/include/c++/5/functional:1520\r\n> #8  std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)> >::_M_run() (this=<optimized out>) at /usr/include/c++/5/thread:115\r\n> #9  0x00001000a30f49a4 in ?? () from /usr/lib/powerpc64le-linux-gnu/libstdc++.so.6\r\n> #10 0x0000100000338070 in start_thread (arg=0x120000d1f1a0) at pthread_create.c:335\r\n> #11 0x00001000005e3a30 in clone () at ../sysdeps/unix/sysv/linux/powerpc/powerpc64/clone.S:96\r\n\r\nI'm working a ppc64 machine, cuda-8.0, where I installed pytorch from source, no error at compilation. Tests fails but might come from #1297. The installation seems to work fine on a single GPU."}