{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/302942510", "html_url": "https://github.com/pytorch/pytorch/issues/1595#issuecomment-302942510", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1595", "id": 302942510, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjk0MjUxMA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-21T15:08:05Z", "updated_at": "2017-05-21T15:08:05Z", "author_association": "MEMBER", "body_html": "<p>I think I have a reproduction:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.utils.data\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DS</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n        <span class=\"pl-k\">return</span> torch.rand(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">640</span>, <span class=\"pl-c1\">640</span>)\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">2000</span>\n\nds <span class=\"pl-k\">=</span> DS()\nit <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(ds, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">500</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">for</span> i, data <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(it):\n    <span class=\"pl-c1\">print</span>(i)</pre></div>\n<p>In my machine, it got stuck at <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/multiprocessing/queue.py#L21\"><code>recv_bytes</code></a>.<br>\nNote that the total size of a batch is greater than 2GB in this case, and there might be some limitation of <code>queue</code> to be &lt; 2^31, or that pickle <a href=\"https://bugs.python.org/msg134369\" rel=\"nofollow\">can't handle objects larger than 2^31</a>.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> do you see a solution for this problem?</p>", "body_text": "I think I have a reproduction:\nimport torch\nimport torch.utils.data\n\nclass DS(object):\n    def __getitem__(self, idx):\n        return torch.rand(3, 640, 640)\n    def __len__(self):\n        return 2000\n\nds = DS()\nit = torch.utils.data.DataLoader(ds, batch_size=500, num_workers=1)\n\nfor i, data in enumerate(it):\n    print(i)\nIn my machine, it got stuck at recv_bytes.\nNote that the total size of a batch is greater than 2GB in this case, and there might be some limitation of queue to be < 2^31, or that pickle can't handle objects larger than 2^31.\n@apaszke do you see a solution for this problem?", "body": "I think I have a reproduction:\r\n\r\n\r\n```python\r\nimport torch\r\nimport torch.utils.data\r\n\r\nclass DS(object):\r\n    def __getitem__(self, idx):\r\n        return torch.rand(3, 640, 640)\r\n    def __len__(self):\r\n        return 2000\r\n\r\nds = DS()\r\nit = torch.utils.data.DataLoader(ds, batch_size=500, num_workers=1)\r\n\r\nfor i, data in enumerate(it):\r\n    print(i)\r\n```\r\nIn my machine, it got stuck at [`recv_bytes`](https://github.com/pytorch/pytorch/blob/master/torch/multiprocessing/queue.py#L21).\r\nNote that the total size of a batch is greater than 2GB in this case, and there might be some limitation of `queue` to be < 2^31, or that pickle [can't handle objects larger than 2^31](https://bugs.python.org/msg134369).\r\n\r\n@apaszke do you see a solution for this problem?"}