{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303074987", "html_url": "https://github.com/pytorch/pytorch/issues/1595#issuecomment-303074987", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1595", "id": 303074987, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzA3NDk4Nw==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-22T11:34:49Z", "updated_at": "2017-05-22T11:34:49Z", "author_association": "MEMBER", "body_html": "<p>Ok, I now have a repro with the issue I was facing in the cluster.<br>\nThe problem I was seeing was that I was indeed passing large blobs via numpy arrays (by circumventing the <code>collate_fn</code> to return a <code>np.array</code>). So a MWE that was causing my error message was the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.utils.data\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DS</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n        <span class=\"pl-k\">return</span> np.zeros((<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">640</span>, <span class=\"pl-c1\">480</span>))\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">8000</span>\n\nds <span class=\"pl-k\">=</span> DS()\nit <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(ds, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">500</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, \n                                 <span class=\"pl-v\">collate_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x)\n\n<span class=\"pl-k\">for</span> i, data <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(it):\n    <span class=\"pl-c1\">print</span>(i)</pre></div>\n<p>This is expected, as numpy arrays are pickled entirely, while torch tensors use shared memory .</p>\n<p>And the issue I was having in my local machine with the script I sent earlier was probably insufficient shared memory (I had 4GB, one batch was &gt; 2GB).</p>\n<p>Maybe we should write a summary with gotchas of the dataloader? I'll start</p>\n<ul>\n<li><strong>always</strong> return a tensor from your <code>collate_fn</code>, and <em>not</em> an <code>np.array</code>.</li>\n<li>If you are facing deadlocks, try increasing the amount of shared memory</li>\n</ul>\n<p>Any others? :)</p>", "body_text": "Ok, I now have a repro with the issue I was facing in the cluster.\nThe problem I was seeing was that I was indeed passing large blobs via numpy arrays (by circumventing the collate_fn to return a np.array). So a MWE that was causing my error message was the following:\nimport torch.utils.data\nimport numpy as np\n\nclass DS(object):\n    def __getitem__(self, idx):\n        return np.zeros((3, 640, 480))\n    def __len__(self):\n        return 8000\n\nds = DS()\nit = torch.utils.data.DataLoader(ds, batch_size=500, num_workers=1, \n                                 collate_fn=lambda x: x)\n\nfor i, data in enumerate(it):\n    print(i)\nThis is expected, as numpy arrays are pickled entirely, while torch tensors use shared memory .\nAnd the issue I was having in my local machine with the script I sent earlier was probably insufficient shared memory (I had 4GB, one batch was > 2GB).\nMaybe we should write a summary with gotchas of the dataloader? I'll start\n\nalways return a tensor from your collate_fn, and not an np.array.\nIf you are facing deadlocks, try increasing the amount of shared memory\n\nAny others? :)", "body": "Ok, I now have a repro with the issue I was facing in the cluster.\r\nThe problem I was seeing was that I was indeed passing large blobs via numpy arrays (by circumventing the `collate_fn` to return a `np.array`). So a MWE that was causing my error message was the following:\r\n```python\r\nimport torch.utils.data\r\nimport numpy as np\r\n\r\nclass DS(object):\r\n    def __getitem__(self, idx):\r\n        return np.zeros((3, 640, 480))\r\n    def __len__(self):\r\n        return 8000\r\n\r\nds = DS()\r\nit = torch.utils.data.DataLoader(ds, batch_size=500, num_workers=1, \r\n                                 collate_fn=lambda x: x)\r\n\r\nfor i, data in enumerate(it):\r\n    print(i)\r\n```\r\n\r\nThis is expected, as numpy arrays are pickled entirely, while torch tensors use shared memory .\r\n\r\nAnd the issue I was having in my local machine with the script I sent earlier was probably insufficient shared memory (I had 4GB, one batch was > 2GB).\r\n\r\nMaybe we should write a summary with gotchas of the dataloader? I'll start\r\n* **always** return a tensor from your `collate_fn`, and *not* an `np.array`.\r\n* If you are facing deadlocks, try increasing the amount of shared memory\r\n\r\nAny others? :)"}