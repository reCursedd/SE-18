{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9701", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9701/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9701/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9701/events", "html_url": "https://github.com/pytorch/pytorch/issues/9701", "id": 343482514, "node_id": "MDU6SXNzdWUzNDM0ODI1MTQ=", "number": 9701, "title": "[request] speed-up multidim slicing backward", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-23T04:20:40Z", "updated_at": "2018-07-24T14:12:21Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>The current way of apply slicing (indexing) isn't very efficient (<a href=\"https://github.com/pytorch/pytorch/blob/7160846c81d9ebd298a4c1608d67b8f4bd76fee7/torch/csrc/autograd/python_variable_indexing.cpp#L151-L199\">here</a>). Each slicing at each dim is applied as a separate autograd op, making the backward extremely inefficient:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y <span class=\"pl-k\">=</span> x[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>]\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y.grad_fn\n<span class=\"pl-k\">&lt;</span>SelectBackward <span class=\"pl-c1\">object</span> at <span class=\"pl-c1\"><span class=\"pl-k\">0x</span>7fbca03e0668</span><span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y.grad_fn.next_functions  <span class=\"pl-c\"><span class=\"pl-c\">#</span> two &lt;SelectBackward&gt;s</span>\n((<span class=\"pl-k\">&lt;</span>SelectBackward <span class=\"pl-c1\">object</span> at <span class=\"pl-c1\"><span class=\"pl-k\">0x</span>7fbca03e0550</span><span class=\"pl-k\">&gt;</span>, <span class=\"pl-c1\">0</span>),)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y.grad_fn.next_functions[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">0</span>].next_functions\n((<span class=\"pl-k\">&lt;</span>AccumulateGrad <span class=\"pl-c1\">object</span> at <span class=\"pl-c1\"><span class=\"pl-k\">0x</span>7fbca03e0668</span><span class=\"pl-k\">&gt;</span>, <span class=\"pl-c1\">0</span>),)</pre></div>\n<p>The backward of this would be:</p>\n<pre><code>   gy      = [1]\n=&gt; g(x[0]) = [0, 1, 0]\n=&gt; g(x)    = [[0, 1, 0], [0, 0, 0], [0, 0, 0]]\n</code></pre>\n<p>, incurring extra copying and malloc'ing. This will be especially slow with large tensors.</p>\n<p>Potential solutions:</p>\n<ol>\n<li>Use <code>as_strided</code> in <code>applySlicing</code>.<br>\npros: fits the current structure easily.<br>\ncons: the backward isn't well optimized for slicing (but it's not terrible either if input is not expanded...). I'll think about ways to speed this up when updating <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336394542\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8965\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8965/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8965\">#8965</a> .</li>\n<li>Move multidim slicing to ATen.<br>\npros: benefit cpp API<br>\ncons: need ways to represent <code>3..5</code> and <code>...</code>.</li>\n</ol>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>", "body_text": "The current way of apply slicing (indexing) isn't very efficient (here). Each slicing at each dim is applied as a separate autograd op, making the backward extremely inefficient:\n>>> x = torch.randn(3,3,requires_grad=True)\n>>> y = x[0, 1]\n>>> y.grad_fn\n<SelectBackward object at 0x7fbca03e0668>\n>>> y.grad_fn.next_functions  # two <SelectBackward>s\n((<SelectBackward object at 0x7fbca03e0550>, 0),)\n>>> y.grad_fn.next_functions[0][0].next_functions\n((<AccumulateGrad object at 0x7fbca03e0668>, 0),)\nThe backward of this would be:\n   gy      = [1]\n=> g(x[0]) = [0, 1, 0]\n=> g(x)    = [[0, 1, 0], [0, 0, 0], [0, 0, 0]]\n\n, incurring extra copying and malloc'ing. This will be especially slow with large tensors.\nPotential solutions:\n\nUse as_strided in applySlicing.\npros: fits the current structure easily.\ncons: the backward isn't well optimized for slicing (but it's not terrible either if input is not expanded...). I'll think about ways to speed this up when updating #8965 .\nMove multidim slicing to ATen.\npros: benefit cpp API\ncons: need ways to represent 3..5 and ....\n\ncc @colesbury", "body": "The current way of apply slicing (indexing) isn't very efficient ([here](https://github.com/pytorch/pytorch/blob/7160846c81d9ebd298a4c1608d67b8f4bd76fee7/torch/csrc/autograd/python_variable_indexing.cpp#L151-L199)). Each slicing at each dim is applied as a separate autograd op, making the backward extremely inefficient:\r\n```py\r\n>>> x = torch.randn(3,3,requires_grad=True)\r\n>>> y = x[0, 1]\r\n>>> y.grad_fn\r\n<SelectBackward object at 0x7fbca03e0668>\r\n>>> y.grad_fn.next_functions  # two <SelectBackward>s\r\n((<SelectBackward object at 0x7fbca03e0550>, 0),)\r\n>>> y.grad_fn.next_functions[0][0].next_functions\r\n((<AccumulateGrad object at 0x7fbca03e0668>, 0),)\r\n```\r\nThe backward of this would be:\r\n```\r\n   gy      = [1]\r\n=> g(x[0]) = [0, 1, 0]\r\n=> g(x)    = [[0, 1, 0], [0, 0, 0], [0, 0, 0]]\r\n```\r\n, incurring extra copying and malloc'ing. This will be especially slow with large tensors.\r\n\r\nPotential solutions: \r\n1. Use `as_strided` in `applySlicing`.\r\n  pros: fits the current structure easily.\r\n  cons: the backward isn't well optimized for slicing (but it's not terrible either if input is not expanded...). I'll think about ways to speed this up when updating https://github.com/pytorch/pytorch/pull/8965 .\r\n2. Move multidim slicing to ATen.\r\n  pros: benefit cpp API\r\n  cons: need ways to represent `3..5` and `...`.\r\n\r\ncc @colesbury "}