{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162756378", "pull_request_review_id": 90271330, "id": 162756378, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Mjc1NjM3OA==", "diff_hunk": "@@ -496,7 +435,122 @@ void interpStageTest() {\n     JIT_ASSERT(exactlyEqual(outputs[1],cx));\n }\n \n+using var_meta_type = std::vector<int64_t>;\n+using var_meta_list = std::vector<var_meta_type>;\n+using test_fn_type = std::function<variable_list(const variable_list&)>;\n+\n+struct ADTestSpec {\n+  ADTestSpec(const char *name, var_meta_list input_meta, test_fn_type test_fn)\n+    : name(name)\n+    , input_meta(input_meta)\n+    , test_fn(test_fn) {}\n+\n+  variable_list operator()(const variable_list& inputs) const {\n+    return test_fn(inputs);\n+  };\n+\n+  std::vector<Variable> make_vars() const {\n+    std::vector<Variable> out;\n+    for (const auto & m : input_meta) {\n+      out.emplace_back(make_variable(at::CPU(at::kFloat).tensor(m).normal_(), true));\n+    }\n+    return out;\n+  }\n+\n+  const char *name;\n+  var_meta_list input_meta;\n+  test_fn_type test_fn;\n+};\n+\n+variable_list get_grad_outputs(const variable_list& vars) {\n+  return fmap(vars, [](const Variable& v) -> Variable {\n+                      return v.type().tensor(v.sizes()).normal_();\n+                    });\n+}\n+\n+std::shared_ptr<Graph> trace(const ADTestSpec& test, const variable_list& vars_in) {\n+  std::shared_ptr<tracer::TracingState> state;\n+  variable_list trace_vars_in;\n+  std::tie(state, trace_vars_in) = tracer::enter(fmap<tracer::TraceInput>(vars_in), 1);\n+  auto trace_vars_out = test(trace_vars_in);\n+  tracer::exit(trace_vars_out);\n+  return state->graph;\n+}\n+\n+variable_list grad(const variable_list& outputs, const variable_list& inputs, const variable_list& grad_outputs) {\n+  static const auto get_edge = [](const Variable& v) -> edge_type {\n+    return std::make_pair(v.grad_fn() ? v.grad_fn() : v.grad_accumulator(), v.output_nr());\n+  };\n+  auto & engine = torch::autograd::python::PythonEngine::getDefaultEngine();\n+  return engine.execute(fmap(outputs, get_edge), grad_outputs, true, false, fmap(inputs, get_edge));\n+}\n+\n+\n+void differentiate(std::shared_ptr<Graph>& graph) {\n+  graph->advanceStage();\n+  auto in_grads = fmap(graph->outputs(), [&](Value*) { return graph->addInput(); });\n+\n+  auto nodes = std::vector<Node*>(graph->nodes().begin(), graph->nodes().end());\n+  JIT_ASSERT(nodes.size() == 1);\n+\n+  auto out_grads = gradientForNode(nodes[0], in_grads);\n+  for (auto grad : out_grads)\n+    graph->registerOutput(grad);\n+}\n+\n+void assertAllClose(const tensor_list& a, const tensor_list& b) {\n+  JIT_ASSERT(a.size() == b.size());\n+  for (std::size_t i = 0; i < a.size(); ++i) {\n+    JIT_ASSERT(a[i].is_same_size(b[i]));\n+    JIT_ASSERT(a[i].allclose(b[i]));\n+  }\n+}\n+\n+void testAD() {", "path": "torch/csrc/jit/test_jit.cpp", "position": null, "original_position": 177, "commit_id": "6166c723a72fea116fa2c515dbba327099e706d2", "original_commit_id": "ab6bea0d82767c85cc62be6128b5592ea98b593d", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "This seems good! Simple but will keep this synced with autograd.", "created_at": "2018-01-19T23:25:53Z", "updated_at": "2018-11-23T15:38:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/4743#discussion_r162756378", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4743", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162756378"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4743#discussion_r162756378"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4743"}}, "body_html": "<p>This seems good! Simple but will keep this synced with autograd.</p>", "body_text": "This seems good! Simple but will keep this synced with autograd."}