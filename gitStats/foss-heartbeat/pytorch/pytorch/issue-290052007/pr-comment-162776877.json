{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162776877", "pull_request_review_id": 90296963, "id": 162776877, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Mjc3Njg3Nw==", "diff_hunk": "@@ -496,7 +435,168 @@ void interpStageTest() {\n     JIT_ASSERT(exactlyEqual(outputs[1],cx));\n }\n \n+using var_meta_type = std::vector<int64_t>;\n+using var_meta_list = std::vector<var_meta_type>;\n+using test_fn_type = std::function<variable_list(const variable_list&)>;\n+\n+struct ADTestSpec {\n+  ADTestSpec(const char *name, var_meta_list input_meta, test_fn_type test_fn)\n+    : name(name)\n+    , input_meta(input_meta)\n+    , test_fn(test_fn) {}\n+\n+  variable_list operator()(const variable_list& inputs) const {\n+    return test_fn(inputs);\n+  };\n+\n+  std::vector<Variable> make_vars() const {\n+    std::vector<Variable> out;\n+    for (const auto & m : input_meta) {\n+      out.emplace_back(make_variable(at::CPU(at::kFloat).tensor(m).normal_(), true));\n+    }\n+    return out;\n+  }\n+\n+  const char *name;\n+  var_meta_list input_meta;\n+  test_fn_type test_fn;\n+};\n+\n+variable_list get_grad_outputs(const variable_list& vars) {\n+  return fmap(vars, [](const Variable& v) -> Variable {\n+                      return v.type().tensor(v.sizes()).normal_();\n+                    });\n+}\n+\n+std::shared_ptr<Graph> trace(const ADTestSpec& test, const variable_list& vars_in) {\n+  std::shared_ptr<tracer::TracingState> state;\n+  variable_list trace_vars_in;\n+  std::tie(state, trace_vars_in) = tracer::enter(fmap<tracer::TraceInput>(vars_in), 1);\n+  auto trace_vars_out = test(trace_vars_in);\n+  tracer::exit(trace_vars_out);\n+  return state->graph;\n+}\n+\n+variable_list grad(const variable_list& outputs, const variable_list& inputs, const variable_list& grad_outputs) {\n+  static const auto get_edge = [](const Variable& v) -> edge_type {\n+    return std::make_pair(v.grad_fn() ? v.grad_fn() : v.grad_accumulator(), v.output_nr());\n+  };\n+  auto & engine = torch::autograd::python::PythonEngine::getDefaultEngine();\n+  return engine.execute(fmap(outputs, get_edge), grad_outputs, true, false, fmap(inputs, get_edge));\n+}\n+\n+void assertAllClose(const tensor_list& a, const tensor_list& b) {\n+  JIT_ASSERT(a.size() == b.size());\n+  for (std::size_t i = 0; i < a.size(); ++i) {\n+    JIT_ASSERT(a[i].is_same_size(b[i]));\n+    JIT_ASSERT(a[i].allclose(b[i]));\n+  }\n+}\n+\n+void testAD() {\n+  static const auto unwrap = [](const Variable& v) { return v.data(); };\n+\n+  using VL = variable_list;\n+  static const var_meta_list binary_pointwise = {{2, 3, 4, 5}, {2, 3, 4, 5}};\n+  static const std::vector<ADTestSpec> ad_tests = {\n+    {\"add\", binary_pointwise, [](const VL& v) -> VL { return {v[0] + v[1]}; }},\n+    {\"sub\", binary_pointwise, [](const VL& v) -> VL { return {v[0] - v[1]}; }},\n+    {\"mul\", binary_pointwise, [](const VL& v) -> VL { return {v[0] * v[1]}; }},\n+  };\n+\n+  // We have to release the GIL inside this method, because if we happen to\n+  // initialize the autograd engine here, the newly spawned worker threads will\n+  // try to initialize their PyThreadState*, and they need the GIL for this.\n+  AutoNoGIL _no_gil;\n+  for (const auto & test : ad_tests) {\n+    // Get reference values form autograd\n+    auto vars_in        = test.make_vars();\n+    auto vars_out       = test(vars_in);\n+    auto var_grads_in   = get_grad_outputs(vars_out);\n+    auto var_grads_out  = grad(vars_out, vars_in, var_grads_in);\n+\n+    // Trace and differentiate the op\n+    auto graph = trace(test, vars_in);\n+    differentiate(graph);\n+    Code bytecode {graph};\n+    InterpreterState interpreter {bytecode};\n+\n+    // Get outputs from the interpreter\n+    auto tensors_in                = fmap(vars_in, unwrap);\n+    auto tensor_grads_in           = fmap(var_grads_in, unwrap);\n+    tensor_list tensors_out, tensor_grads_out;\n+    interpreter.runOneStage(tensors_in, tensors_out);\n+    interpreter.runOneStage(tensor_grads_in, tensor_grads_out);\n+\n+    // Compare results\n+    auto expected_tensors_out      = fmap(vars_out, unwrap);\n+    auto expected_tensor_grads_out = fmap(var_grads_out, unwrap);\n+    assertAllClose(tensors_out,      expected_tensors_out);\n+    assertAllClose(tensor_grads_out, expected_tensor_grads_out);\n+  }\n+}\n+\n+std::string toString(std::shared_ptr<Graph>& graph) {\n+  std::ostringstream s;\n+  s << *graph;\n+  return s.str();\n+}\n+\n+void testGraphDiff() {", "path": "torch/csrc/jit/test_jit.cpp", "position": null, "original_position": 252, "commit_id": "6166c723a72fea116fa2c515dbba327099e706d2", "original_commit_id": "8154b3c87f9abd2e4de2c281b3fe0a2901b85a18", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "We will need more rigorous testing for boundary cases, but that can wait until we have the meta-data for gradient, so that we can actually test by running the code.", "created_at": "2018-01-20T07:39:20Z", "updated_at": "2018-11-23T15:38:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/4743#discussion_r162776877", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4743", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162776877"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4743#discussion_r162776877"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4743"}}, "body_html": "<p>We will need more rigorous testing for boundary cases, but that can wait until we have the meta-data for gradient, so that we can actually test by running the code.</p>", "body_text": "We will need more rigorous testing for boundary cases, but that can wait until we have the meta-data for gradient, so that we can actually test by running the code."}