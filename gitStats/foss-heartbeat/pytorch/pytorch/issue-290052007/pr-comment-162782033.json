{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162782033", "pull_request_review_id": 90303020, "id": 162782033, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Mjc4MjAzMw==", "diff_hunk": "@@ -0,0 +1,100 @@\n+#include \"torch/csrc/jit/autodiff.h\"\n+\n+#include \"torch/csrc/jit/symbolic_variable.h\"\n+#include \"torch/csrc/utils/functional.h\"\n+\n+namespace torch { namespace jit {\n+\n+using value_list = std::vector<Value*>;\n+\n+std::vector<Value*> gradientForNode(Node* node, ArrayRef<Value*> grad_values) {\n+  const auto build_sym_grad = [node](const std::vector<SymbolicVariable>& grads) -> std::vector<SymbolicVariable> {\n+    auto inputs = node->inputs();\n+    switch(node->kind()) {\n+      case kadd:\n+        return {grads[0], grads[0]};\n+      case ksub:\n+        return {grads[0], -grads[0]};\n+      case kmul:\n+        return {grads[0] * inputs[1], grads[0] * inputs[0]};\n+    }\n+    throw std::runtime_error(std::string(\"don't support differentiation of `\") +\n+                            node->kind().toString() + \"`\");\n+  };\n+  auto sym_grads = build_sym_grad(fmap<SymbolicVariable>(grad_values));\n+  return fmap(sym_grads, [](const SymbolicVariable &v) { return v.value(); });\n+}\n+\n+void differentiate(std::shared_ptr<Graph>& graph) {\n+  JIT_ASSERT(graph->stage() == 0);\n+  graph->advanceStage();\n+\n+  std::unordered_map<Value*, Value*> grad_map; // x -> dx mapping\n+  const auto get_grad = [&](Value* v) { return grad_map[v]; };\n+  for (auto output : graph->outputs())\n+    grad_map[output] = graph->addInput()->setType(output->typeOption());\n+\n+  for (auto it = graph->rbegin(), end = graph->rend(); it != end; ++it) {\n+    Node *node = *it;\n+    auto inputs = node->inputs();\n+    value_list grad_inputs = gradientForNode(node, fmap(node->outputs(), get_grad));\n+    JIT_ASSERT(grad_inputs.size() == node->inputs().size());\n+    for (std::size_t i = 0, num_inputs = grad_inputs.size(); i < num_inputs; ++i) {\n+      if (Value * prev_grad = grad_map[inputs[i]]) {\n+        Node *new_grad_node = graph->create(kadd, {prev_grad, grad_inputs[i]})\n+                                   ->t_(kalpha, at::Scalar(1).toTensor());\n+        new_grad_node->insertAfter(grad_inputs[i]->node());\n+        Value *new_grad = new_grad_node->output();\n+        new_grad->setType(prev_grad->typeOption());\n+        grad_map[inputs[i]] = new_grad;\n+      } else {\n+        grad_map[inputs[i]] = grad_inputs[i];\n+      }\n+    }\n+  }\n+\n+  for (auto input : graph->inputs()) {\n+    if (input->stage() > 0) break;\n+    graph->registerOutput(grad_map.at(input));\n+  }\n+}\n+\n+// TODO: return metadata that allows to associate inputs with values form graph\n+std::pair<std::shared_ptr<Graph>, value_list> backwardLambdaLift(std::shared_ptr<Graph>& graph) {\n+  JIT_ASSERT(graph->stage() == 1);\n+\n+  std::unordered_set<Value*> backward_inputs_set;\n+  for (Node *node : *graph) {\n+    if (node->stage() == 0) continue;\n+    for (Value *input : node->inputs()) {\n+      // XXX: this assumes that the single Param node in the graph (that holds inputs) has stage 0\n+      if (input->node()->stage() == 0)\n+        backward_inputs_set.insert(input);\n+    }\n+  }\n+  value_list backward_inputs(backward_inputs_set.begin(), backward_inputs_set.end());\n+  // We want them nicely sorted with inputs being first, and then following\n+  // the topological ordering (at least approximately, we could do a better job here).\n+  std::sort(backward_inputs.begin(), backward_inputs.end(),\n+            [](Value *a, Value*b) { return a->unique() < b->unique(); });\n+\n+  auto dgraph = std::make_shared<Graph>();\n+  std::unordered_map<Value*, Value*> val_map; // values in graph -> values in dgraph\n+  const auto lookup_val = [&](Value *v) { return val_map.at(v); };\n+  for (auto input : backward_inputs)\n+    val_map[input] = dgraph->addInput()->setType(input->typeOption());\n+  for (Node *node : *graph) {\n+    if (node->stage() == 0) continue;\n+    Node *clone = dgraph->createClone(node, lookup_val);\n+    for (std::size_t i = 0, num_outputs = clone->outputs().size(); i < num_outputs; ++i)\n+      val_map[node->outputs()[i]] = clone->outputs()[i];\n+    dgraph->appendNode(clone);\n+  }\n+  for (auto output : graph->outputs()) {\n+    if (output->stage() == 0) continue;\n+    dgraph->registerOutput(val_map.at(output));\n+  }\n+  return std::make_pair(dgraph, backward_inputs);", "path": "torch/csrc/jit/autodiff.cpp", "position": null, "original_position": 97, "commit_id": "6166c723a72fea116fa2c515dbba327099e706d2", "original_commit_id": "8154b3c87f9abd2e4de2c281b3fe0a2901b85a18", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Hmm good point. This lifting has to give us two graphs, not one.", "created_at": "2018-01-20T12:56:01Z", "updated_at": "2018-11-23T15:38:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/4743#discussion_r162782033", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4743", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162782033"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4743#discussion_r162782033"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4743"}}, "body_html": "<p>Hmm good point. This lifting has to give us two graphs, not one.</p>", "body_text": "Hmm good point. This lifting has to give us two graphs, not one.", "in_reply_to_id": 162776840}