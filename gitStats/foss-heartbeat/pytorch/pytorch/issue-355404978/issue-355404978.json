{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11062", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11062/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11062/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11062/events", "html_url": "https://github.com/pytorch/pytorch/issues/11062", "id": 355404978, "node_id": "MDU6SXNzdWUzNTU0MDQ5Nzg=", "number": 11062, "title": "non-shuffling data loaders can affect random states, thus the results of shuffling data loaders.", "user": {"login": "zym1010", "id": 4273204, "node_id": "MDQ6VXNlcjQyNzMyMDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4273204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zym1010", "html_url": "https://github.com/zym1010", "followers_url": "https://api.github.com/users/zym1010/followers", "following_url": "https://api.github.com/users/zym1010/following{/other_user}", "gists_url": "https://api.github.com/users/zym1010/gists{/gist_id}", "starred_url": "https://api.github.com/users/zym1010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zym1010/subscriptions", "organizations_url": "https://api.github.com/users/zym1010/orgs", "repos_url": "https://api.github.com/users/zym1010/repos", "events_url": "https://api.github.com/users/zym1010/events{/privacy}", "received_events_url": "https://api.github.com/users/zym1010/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-30T02:56:33Z", "updated_at": "2018-09-12T06:02:45Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>See the title. While it's arguable this behavior is bug or not, I feel it's not very intuitive. Maybe it's good to doc this.</p>\n<p>For an actual example, see below.</p>\n<p>In the example, I have two data loaders, one shuffling for training, one non-shuffling for validation. Turns out, the order of data sample points for the shuffling training loader depends on how many times I call the non-shuffling validation loader first.</p>\n<p>Overall, I think making <strong>behaviorally deterministic</strong> operations affect random states is certainly unintuitive; for example, you would not expect that running a few matrix multiplications in <code>numpy</code> can affect a random number generation operation that follows these matrix multiplications.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> In[1]:</span>\n\n\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n<span class=\"pl-k\">from</span> torch.utils.data.dataset <span class=\"pl-k\">import</span> TensorDataset\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-c1\">print</span>(torch.<span class=\"pl-c1\">__version__</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> In[2]:</span>\n\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\nnp.random.seed(<span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> In[3]:</span>\n\n\ndataset_train <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">100</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>,)).astype(np.float32)\ndataset_val <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">100</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>,)).astype(np.float32)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>,dataset_train)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>val<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>,dataset_val)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [44. 47. 64. 67. 67.  9. 83. 21. 36. 87.]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> val</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [70. 88. 88. 12. 58. 65. 39. 87. 46. 88.]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> In[4]:</span>\n\n\ndataloader_train <span class=\"pl-k\">=</span> DataLoader(TensorDataset(torch.tensor(dataset_train)),\n                                            <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ndataloader_val <span class=\"pl-k\">=</span> DataLoader(TensorDataset(torch.tensor(dataset_val)),\n                         <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loop_one</span>(<span class=\"pl-smi\">dataloader_this</span>):\n    <span class=\"pl-k\">for</span> (x,) <span class=\"pl-k\">in</span> dataloader_this:\n        <span class=\"pl-c1\">print</span>(x)\n\n\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> In[5]:</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> only train</span>\nnp.random.seed(<span class=\"pl-c1\">0</span>)\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\nloop_one(dataloader_train)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> no val, we get the following for dataloader_train</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([67., 21.,  9., 64., 44., 36., 47., 83., 87., 67.])</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> In[6]:</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> val + train</span>\nnp.random.seed(<span class=\"pl-c1\">0</span>)\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\nloop_one(dataloader_val)\nloop_one(dataloader_train)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> one val</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([67., 21.,  9., 83., 64., 36., 47., 87., 67., 44.])</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> In[7]:</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> val + val + train</span>\nnp.random.seed(<span class=\"pl-c1\">0</span>)\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\nloop_one(dataloader_val)\nloop_one(dataloader_val)\nloop_one(dataloader_train)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> two val</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([21., 64., 67., 36., 87., 83.,  9., 44., 47., 67.])</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> In[8]:</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> val + val + reseed +  train</span>\nnp.random.seed(<span class=\"pl-c1\">0</span>)\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\nloop_one(dataloader_val)\nloop_one(dataloader_val)\nnp.random.seed(<span class=\"pl-c1\">0</span>)\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\nloop_one(dataloader_train)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> reseed</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([67., 21.,  9., 64., 44., 36., 47., 83., 87., 67.])</span></pre></div>\n<h2>System Info</h2>\n<p>I ran that <code>collect_env.py</code> script  under a Singularity container with Ubuntu 14.04 host.</p>\n<pre><code>Collecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: Could not collect\nCMake version: Could not collect\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration:\nGPU 0: GeForce GTX TITAN Black\nGPU 1: Tesla K40c\n\nNvidia driver version: 384.111\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n</code></pre>", "body_text": "Issue description\nSee the title. While it's arguable this behavior is bug or not, I feel it's not very intuitive. Maybe it's good to doc this.\nFor an actual example, see below.\nIn the example, I have two data loaders, one shuffling for training, one non-shuffling for validation. Turns out, the order of data sample points for the shuffling training loader depends on how many times I call the non-shuffling validation loader first.\nOverall, I think making behaviorally deterministic operations affect random states is certainly unintuitive; for example, you would not expect that running a few matrix multiplications in numpy can affect a random number generation operation that follows these matrix multiplications.\nCode example\n# In[1]:\n\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import TensorDataset\nimport torch\nprint(torch.__version__)\n\n\n# In[2]:\n\n\nimport numpy as np\nnp.random.seed(0)\n\n# In[3]:\n\n\ndataset_train = np.random.randint(100, size=(10,)).astype(np.float32)\ndataset_val = np.random.randint(100, size=(10,)).astype(np.float32)\nprint('train\\n',dataset_train)\nprint('val\\n',dataset_val)\n\n# train\n# [44. 47. 64. 67. 67.  9. 83. 21. 36. 87.]\n# val\n# [70. 88. 88. 12. 58. 65. 39. 87. 46. 88.]\n\n# In[4]:\n\n\ndataloader_train = DataLoader(TensorDataset(torch.tensor(dataset_train)),\n                                            batch_size=10, shuffle=True)\ndataloader_val = DataLoader(TensorDataset(torch.tensor(dataset_val)),\n                         batch_size=10, shuffle=False)\ndef loop_one(dataloader_this):\n    for (x,) in dataloader_this:\n        print(x)\n\n\n\n\n# In[5]:\n\n\n# only train\nnp.random.seed(0)\ntorch.manual_seed(0)\nloop_one(dataloader_train)\n\n# no val, we get the following for dataloader_train\n# tensor([67., 21.,  9., 64., 44., 36., 47., 83., 87., 67.])\n\n# In[6]:\n\n\n# val + train\nnp.random.seed(0)\ntorch.manual_seed(0)\nloop_one(dataloader_val)\nloop_one(dataloader_train)\n\n# one val\n# tensor([67., 21.,  9., 83., 64., 36., 47., 87., 67., 44.])\n\n# In[7]:\n\n\n# val + val + train\nnp.random.seed(0)\ntorch.manual_seed(0)\nloop_one(dataloader_val)\nloop_one(dataloader_val)\nloop_one(dataloader_train)\n\n# two val\n# tensor([21., 64., 67., 36., 87., 83.,  9., 44., 47., 67.])\n\n# In[8]:\n\n\n# val + val + reseed +  train\nnp.random.seed(0)\ntorch.manual_seed(0)\nloop_one(dataloader_val)\nloop_one(dataloader_val)\nnp.random.seed(0)\ntorch.manual_seed(0)\nloop_one(dataloader_train)\n\n# reseed\n# tensor([67., 21.,  9., 64., 44., 36., 47., 83., 87., 67.])\nSystem Info\nI ran that collect_env.py script  under a Singularity container with Ubuntu 14.04 host.\nCollecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: Could not collect\nCMake version: Could not collect\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration:\nGPU 0: GeForce GTX TITAN Black\nGPU 1: Tesla K40c\n\nNvidia driver version: 384.111\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect", "body": "## Issue description\r\n\r\nSee the title. While it's arguable this behavior is bug or not, I feel it's not very intuitive. Maybe it's good to doc this.\r\n\r\nFor an actual example, see below.\r\n\r\nIn the example, I have two data loaders, one shuffling for training, one non-shuffling for validation. Turns out, the order of data sample points for the shuffling training loader depends on how many times I call the non-shuffling validation loader first.\r\n\r\nOverall, I think making **behaviorally deterministic** operations affect random states is certainly unintuitive; for example, you would not expect that running a few matrix multiplications in `numpy` can affect a random number generation operation that follows these matrix multiplications.\r\n\r\n## Code example\r\n\r\n```python\r\n# In[1]:\r\n\r\n\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data.dataset import TensorDataset\r\nimport torch\r\nprint(torch.__version__)\r\n\r\n\r\n# In[2]:\r\n\r\n\r\nimport numpy as np\r\nnp.random.seed(0)\r\n\r\n# In[3]:\r\n\r\n\r\ndataset_train = np.random.randint(100, size=(10,)).astype(np.float32)\r\ndataset_val = np.random.randint(100, size=(10,)).astype(np.float32)\r\nprint('train\\n',dataset_train)\r\nprint('val\\n',dataset_val)\r\n\r\n# train\r\n# [44. 47. 64. 67. 67.  9. 83. 21. 36. 87.]\r\n# val\r\n# [70. 88. 88. 12. 58. 65. 39. 87. 46. 88.]\r\n\r\n# In[4]:\r\n\r\n\r\ndataloader_train = DataLoader(TensorDataset(torch.tensor(dataset_train)),\r\n                                            batch_size=10, shuffle=True)\r\ndataloader_val = DataLoader(TensorDataset(torch.tensor(dataset_val)),\r\n                         batch_size=10, shuffle=False)\r\ndef loop_one(dataloader_this):\r\n    for (x,) in dataloader_this:\r\n        print(x)\r\n\r\n\r\n\r\n\r\n# In[5]:\r\n\r\n\r\n# only train\r\nnp.random.seed(0)\r\ntorch.manual_seed(0)\r\nloop_one(dataloader_train)\r\n\r\n# no val, we get the following for dataloader_train\r\n# tensor([67., 21.,  9., 64., 44., 36., 47., 83., 87., 67.])\r\n\r\n# In[6]:\r\n\r\n\r\n# val + train\r\nnp.random.seed(0)\r\ntorch.manual_seed(0)\r\nloop_one(dataloader_val)\r\nloop_one(dataloader_train)\r\n\r\n# one val\r\n# tensor([67., 21.,  9., 83., 64., 36., 47., 87., 67., 44.])\r\n\r\n# In[7]:\r\n\r\n\r\n# val + val + train\r\nnp.random.seed(0)\r\ntorch.manual_seed(0)\r\nloop_one(dataloader_val)\r\nloop_one(dataloader_val)\r\nloop_one(dataloader_train)\r\n\r\n# two val\r\n# tensor([21., 64., 67., 36., 87., 83.,  9., 44., 47., 67.])\r\n\r\n# In[8]:\r\n\r\n\r\n# val + val + reseed +  train\r\nnp.random.seed(0)\r\ntorch.manual_seed(0)\r\nloop_one(dataloader_val)\r\nloop_one(dataloader_val)\r\nnp.random.seed(0)\r\ntorch.manual_seed(0)\r\nloop_one(dataloader_train)\r\n\r\n# reseed\r\n# tensor([67., 21.,  9., 64., 44., 36., 47., 83., 87., 67.])\r\n```\r\n\r\n\r\n## System Info\r\n\r\nI ran that `collect_env.py` script  under a Singularity container with Ubuntu 14.04 host.\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN Black\r\nGPU 1: Tesla K40c\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n"}