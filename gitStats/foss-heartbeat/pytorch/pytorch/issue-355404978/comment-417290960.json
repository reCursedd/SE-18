{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/417290960", "html_url": "https://github.com/pytorch/pytorch/issues/11062#issuecomment-417290960", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11062", "id": 417290960, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzI5MDk2MA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-30T11:47:49Z", "updated_at": "2018-08-30T11:47:49Z", "author_association": "MEMBER", "body_html": "<p>This happens because we unconditionally <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L255\">generate a random number in the <code>DataLoaderIter</code></a>, which changes the state of the rng.<br>\nThis is kind of documented in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L433-L441\">this part</a>.</p>\n<p>I think that we could reset the state of the rng to the state it was before the generation of this number so that this doesn't happen.</p>", "body_text": "This happens because we unconditionally generate a random number in the DataLoaderIter, which changes the state of the rng.\nThis is kind of documented in this part.\nI think that we could reset the state of the rng to the state it was before the generation of this number so that this doesn't happen.", "body": "This happens because we unconditionally [generate a random number in the `DataLoaderIter`](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L255), which changes the state of the rng.\r\nThis is kind of documented in [this part](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L433-L441).\r\n\r\nI think that we could reset the state of the rng to the state it was before the generation of this number so that this doesn't happen."}