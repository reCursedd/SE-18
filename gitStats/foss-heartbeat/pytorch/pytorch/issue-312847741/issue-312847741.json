{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6462", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6462/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6462/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6462/events", "html_url": "https://github.com/pytorch/pytorch/issues/6462", "id": 312847741, "node_id": "MDU6SXNzdWUzMTI4NDc3NDE=", "number": 6462, "title": "How to set batch_size when using DataParallel mode in pytorch?", "user": {"login": "melspectrum007", "id": 34162176, "node_id": "MDQ6VXNlcjM0MTYyMTc2", "avatar_url": "https://avatars3.githubusercontent.com/u/34162176?v=4", "gravatar_id": "", "url": "https://api.github.com/users/melspectrum007", "html_url": "https://github.com/melspectrum007", "followers_url": "https://api.github.com/users/melspectrum007/followers", "following_url": "https://api.github.com/users/melspectrum007/following{/other_user}", "gists_url": "https://api.github.com/users/melspectrum007/gists{/gist_id}", "starred_url": "https://api.github.com/users/melspectrum007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/melspectrum007/subscriptions", "organizations_url": "https://api.github.com/users/melspectrum007/orgs", "repos_url": "https://api.github.com/users/melspectrum007/repos", "events_url": "https://api.github.com/users/melspectrum007/events{/privacy}", "received_events_url": "https://api.github.com/users/melspectrum007/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-10T09:43:36Z", "updated_at": "2018-04-10T10:16:14Z", "closed_at": "2018-04-10T10:16:14Z", "author_association": "NONE", "body_html": "<p>I can't find document about the batch_size setting and DataParallel mode.<br>\nso How to set batch_size when using DataParallel mode in pytorch? Is it similiar with Keras?<br>\nThanks</p>\n<p>In Keras, the parallel_model( like DataParallel mode in pytorch), which document is very clear I think:<br>\n-------------------------------------------Keras document beginging------------------------------------------<br>\nE.g. if your batch_size is 64 and you use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples</p>\n<p>This <code>fit</code> call will be distributed on 8 GPUs.<br>\nSince the batch size is 256, each GPU will process 32 samples.</p>\n<p>parallel_model.fit(x, y, epochs=20, batch_size=256)<br>\n-------------------------------------------Keras document ended------------------------------------------</p>", "body_text": "I can't find document about the batch_size setting and DataParallel mode.\nso How to set batch_size when using DataParallel mode in pytorch? Is it similiar with Keras?\nThanks\nIn Keras, the parallel_model( like DataParallel mode in pytorch), which document is very clear I think:\n-------------------------------------------Keras document beginging------------------------------------------\nE.g. if your batch_size is 64 and you use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples\nThis fit call will be distributed on 8 GPUs.\nSince the batch size is 256, each GPU will process 32 samples.\nparallel_model.fit(x, y, epochs=20, batch_size=256)\n-------------------------------------------Keras document ended------------------------------------------", "body": "I can't find document about the batch_size setting and DataParallel mode.\r\nso How to set batch_size when using DataParallel mode in pytorch? Is it similiar with Keras?\r\nThanks\r\n\r\nIn Keras, the parallel_model( like DataParallel mode in pytorch), which document is very clear I think:\r\n-------------------------------------------Keras document beginging------------------------------------------\r\nE.g. if your batch_size is 64 and you use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples\r\n\r\nThis `fit` call will be distributed on 8 GPUs.\r\n Since the batch size is 256, each GPU will process 32 samples.\r\n\r\nparallel_model.fit(x, y, epochs=20, batch_size=256)\r\n-------------------------------------------Keras document ended------------------------------------------"}