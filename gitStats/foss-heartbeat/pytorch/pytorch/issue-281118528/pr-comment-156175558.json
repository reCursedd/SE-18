{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156175558", "pull_request_review_id": 82609987, "id": 156175558, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NjE3NTU1OA==", "diff_hunk": "@@ -262,3 +262,107 @@ def log_prob(self, value):\n         return (self.alpha * torch.log(self.beta) +\n                 (self.alpha - 1) * torch.log(value) -\n                 self.beta * value - torch.lgamma(self.alpha))\n+\n+\n+def _dirichlet_sample_nograd(alpha):\n+    gammas = torch._C._standard_gamma(alpha)\n+    return gammas / gammas.sum(-1, True)\n+\n+\n+class _Dirichlet(Function):\n+    @staticmethod\n+    def forward(ctx, alpha):\n+        x = _dirichlet_sample_nograd(alpha)\n+        ctx.save_for_backward(x, alpha)\n+        return x\n+\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n+        x, alpha = ctx.saved_tensors\n+        total = alpha.sum(-1, True).expand_as(alpha)\n+        grad = torch._C._dirichlet_grad(x, alpha, total)\n+        return grad_output * grad\n+\n+\n+class Dirichlet(Distribution):\n+    r\"\"\"\n+    Creates a Dirichlet distribution parameterized by concentration `alpha`.\n+\n+    Example::\n+\n+        >>> m = Dirichlet(torch.Tensor([0.5, 0.5]))\n+        >>> m.sample()  # Dirichlet distributed with concentrarion alpha\n+         0.1046\n+         0.8954\n+        [torch.FloatTensor of size 2]\n+\n+    Args:\n+        alpha (Tensor or Variable): concentration parameter of the distribution\n+    \"\"\"\n+    has_rsample = True\n+\n+    def __init__(self, alpha):\n+        self.alpha = alpha\n+\n+    def _sample(self, alpha):\n+        if not isinstance(alpha, Variable):\n+            return _dirichlet_sample_nograd(alpha)\n+        return _Dirichlet.apply(alpha)\n+\n+    def sample(self):\n+        return self._sample(self.alpha)\n+\n+    def sample_n(self, n):\n+        return self._sample(_expand_n(self.alpha, n))\n+\n+    def log_prob(self, value):\n+        return ((torch.log(value) * (self.alpha - 1.0)).sum(-1) +\n+                torch.lgamma(self.alpha.sum(-1)) -\n+                torch.lgamma(self.alpha).sum(-1))\n+\n+\n+class Beta(Distribution):\n+    r\"\"\"\n+    Creates a Beta distribution parameterized by concentration `alpha` and `beta`.\n+\n+    Example::\n+\n+        >>> m = Beta(torch.Tensor([0.5]), torch.Tensor([0.5]))\n+        >>> m.sample()  # Beta distributed with concentrarion alpha\n+         0.1046\n+        [torch.FloatTensor of size 2]\n+\n+    Args:\n+        alpha (Tensor or Variable): concentration parameter of the distribution\n+    \"\"\"\n+    has_rsample = True\n+\n+    def __init__(self, alpha, beta):\n+        alpha_num = isinstance(alpha, Number)\n+        beta_num = isinstance(beta, Number)\n+        if alpha_num and beta_num:\n+            alpha_beta = torch.Tensor([alpha, beta])\n+        else:\n+            if alpha_num and not beta_num:\n+                alpha = beta.new(beta.size()).fill_(alpha)\n+            elif not alpha_num and beta_num:\n+                beta = alpha.new(alpha.size()).fill_(beta)\n+            elif alpha.size() != beta.size():\n+                raise ValueError('Expected alpha.size() == beta.size(), actual {} vs {}'.format(\n+                    alpha.size(), beta.size()))", "path": "torch/distributions.py", "position": null, "original_position": 92, "commit_id": "8d1c86ec401d659d545f165fadff23a9d8f69795", "original_commit_id": "e7abfca99ec8f1a97961ac6a8508bb4b664c24d4", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This seems to be a recurring pattern so it would be nice to have a helper for this. Neeraj had a neat way of dealing with it (upcasting everything to tensors, and then a tiny helper for broadcasting them together)", "created_at": "2017-12-11T19:31:16Z", "updated_at": "2018-11-23T15:37:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/4117#discussion_r156175558", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4117", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156175558"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4117#discussion_r156175558"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4117"}}, "body_html": "<p>This seems to be a recurring pattern so it would be nice to have a helper for this. Neeraj had a neat way of dealing with it (upcasting everything to tensors, and then a tiny helper for broadcasting them together)</p>", "body_text": "This seems to be a recurring pattern so it would be nice to have a helper for this. Neeraj had a neat way of dealing with it (upcasting everything to tensors, and then a tiny helper for broadcasting them together)"}