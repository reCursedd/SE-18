{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5801", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5801/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5801/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5801/events", "html_url": "https://github.com/pytorch/pytorch/issues/5801", "id": 305428143, "node_id": "MDU6SXNzdWUzMDU0MjgxNDM=", "number": 5801, "title": "KLDivLoss behaves differently on CPU/GPU", "user": {"login": "yuandong-tian", "id": 2973937, "node_id": "MDQ6VXNlcjI5NzM5Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2973937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuandong-tian", "html_url": "https://github.com/yuandong-tian", "followers_url": "https://api.github.com/users/yuandong-tian/followers", "following_url": "https://api.github.com/users/yuandong-tian/following{/other_user}", "gists_url": "https://api.github.com/users/yuandong-tian/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuandong-tian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuandong-tian/subscriptions", "organizations_url": "https://api.github.com/users/yuandong-tian/orgs", "repos_url": "https://api.github.com/users/yuandong-tian/repos", "events_url": "https://api.github.com/users/yuandong-tian/events{/privacy}", "received_events_url": "https://api.github.com/users/yuandong-tian/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-15T06:17:41Z", "updated_at": "2018-03-17T15:17:08Z", "closed_at": "2018-03-17T15:17:08Z", "author_association": "NONE", "body_html": "<p>Pytorch Version: '0.3.0'</p>\n<p>Code:</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport sys\n\nbatchsize = 8\nnclass = 5\n\nif sys.argv[1] == \"cuda\":\n    cuda = True\nelse:\n    cuda = False\n\nprint(\"Cuda: \" + str(cuda))\n\ndef make_prob():\n    if cuda:\n        pr = torch.cuda.FloatTensor(batchsize, nclass)\n    else:\n        pr = torch.FloatTensor(batchsize, nclass)\n    pr.uniform_()\n    pr /= pr.sum(dim=1).view(batchsize, 1)\n    return pr\n\ndef loss_kl(pi, targets):\n    pi_var = Variable(pi, requires_grad=True)\n    logsoftmax = nn.LogSoftmax(dim=1)\n    kl = nn.KLDivLoss()\n\n    if cuda:\n        logsoftmax.cuda()\n        kl.cuda()\n\n    logpi = logsoftmax(pi_var)\n    return kl(logpi, Variable(targets)) * logpi.size(1), pi_var\n\ndef loss_plain(pi, targets):\n    pi_var = Variable(pi, requires_grad=True)\n    targets_var = Variable(targets)\n    logsoftmax = nn.LogSoftmax(dim=1)\n\n    if cuda:\n        logsoftmax.cuda()\n\n    logpi = logsoftmax(pi_var)\n    loss = - (logpi * targets_var).sum(dim=1).mean() + (targets_var * targets_var.log()).sum(dim=1).mean()\n    return loss, pi_var\n\ntargets = make_prob()\npi = make_prob()\n\nloss1, pi_var1 = loss_kl(pi, targets)\nprint(loss1)\nloss1.backward()\nprint(pi_var1.grad)\n\nloss2, pi_var2 = loss_plain(pi, targets)\nprint(loss2)\nloss2.backward()\nprint(pi_var2.grad)\n</code></pre>\n<p>Without cuda, both the gradients and the loss are the same:</p>\n<pre><code>$ python3 ~/test_kl.py nocuda\nCuda: False\nVariable containing:\n 0.1930\n[torch.FloatTensor of size 1]\n\nVariable containing:\n1.00000e-02 *\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\n -2.1244  2.5148  0.9348  0.5213 -1.8464\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\n[torch.FloatTensor of size 8x5]\n\nVariable containing:\n 0.1930\n[torch.FloatTensor of size 1]\n\nVariable containing:\n1.00000e-02 *\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\n -2.1244  2.5148  0.9348  0.5213 -1.8464\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\n[torch.FloatTensor of size 8x5]\n</code></pre>\n<p>With cuda the gradients returned by KLDivLoss are scaled down by the number of class, while the loss is the same.</p>\n<pre><code>$ python3 ~/test_kl.py cuda\nCuda: True\nVariable containing:\n 0.1756\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n\nVariable containing:\n1.00000e-03 *\n -1.2566 -3.5917 -0.8016  2.9373  2.7126\n -0.0264  2.3321 -1.3892  1.0676 -1.9841\n  0.1204  4.5507 -2.2642 -3.7239  1.3170\n  0.5667 -2.7765  4.3564 -4.8917  2.7452\n  2.3703 -2.4578  2.7945  1.8861 -4.5931\n -0.2237 -0.2954  1.7677  1.7757 -3.0243\n -3.9952 -7.2011  2.1794  3.3324  5.6845\n -0.2814  0.0368 -2.2764 -0.2955  2.8164\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]\n\nVariable containing:\n 0.1756\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n\nVariable containing:\n1.00000e-02 *\n -0.6283 -1.7959 -0.4008  1.4686  1.3563\n -0.0132  1.1661 -0.6946  0.5338 -0.9920\n  0.0602  2.2753 -1.1321 -1.8619  0.6585\n  0.2833 -1.3883  2.1782 -2.4459  1.3726\n  1.1852 -1.2289  1.3972  0.9430 -2.2966\n -0.1119 -0.1477  0.8838  0.8879 -1.5121\n -1.9976 -3.6006  1.0897  1.6662  2.8423\n -0.1407  0.0184 -1.1382 -0.1477  1.4082\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]\n</code></pre>", "body_text": "Pytorch Version: '0.3.0'\nCode:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport sys\n\nbatchsize = 8\nnclass = 5\n\nif sys.argv[1] == \"cuda\":\n    cuda = True\nelse:\n    cuda = False\n\nprint(\"Cuda: \" + str(cuda))\n\ndef make_prob():\n    if cuda:\n        pr = torch.cuda.FloatTensor(batchsize, nclass)\n    else:\n        pr = torch.FloatTensor(batchsize, nclass)\n    pr.uniform_()\n    pr /= pr.sum(dim=1).view(batchsize, 1)\n    return pr\n\ndef loss_kl(pi, targets):\n    pi_var = Variable(pi, requires_grad=True)\n    logsoftmax = nn.LogSoftmax(dim=1)\n    kl = nn.KLDivLoss()\n\n    if cuda:\n        logsoftmax.cuda()\n        kl.cuda()\n\n    logpi = logsoftmax(pi_var)\n    return kl(logpi, Variable(targets)) * logpi.size(1), pi_var\n\ndef loss_plain(pi, targets):\n    pi_var = Variable(pi, requires_grad=True)\n    targets_var = Variable(targets)\n    logsoftmax = nn.LogSoftmax(dim=1)\n\n    if cuda:\n        logsoftmax.cuda()\n\n    logpi = logsoftmax(pi_var)\n    loss = - (logpi * targets_var).sum(dim=1).mean() + (targets_var * targets_var.log()).sum(dim=1).mean()\n    return loss, pi_var\n\ntargets = make_prob()\npi = make_prob()\n\nloss1, pi_var1 = loss_kl(pi, targets)\nprint(loss1)\nloss1.backward()\nprint(pi_var1.grad)\n\nloss2, pi_var2 = loss_plain(pi, targets)\nprint(loss2)\nloss2.backward()\nprint(pi_var2.grad)\n\nWithout cuda, both the gradients and the loss are the same:\n$ python3 ~/test_kl.py nocuda\nCuda: False\nVariable containing:\n 0.1930\n[torch.FloatTensor of size 1]\n\nVariable containing:\n1.00000e-02 *\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\n -2.1244  2.5148  0.9348  0.5213 -1.8464\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\n[torch.FloatTensor of size 8x5]\n\nVariable containing:\n 0.1930\n[torch.FloatTensor of size 1]\n\nVariable containing:\n1.00000e-02 *\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\n -2.1244  2.5148  0.9348  0.5213 -1.8464\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\n[torch.FloatTensor of size 8x5]\n\nWith cuda the gradients returned by KLDivLoss are scaled down by the number of class, while the loss is the same.\n$ python3 ~/test_kl.py cuda\nCuda: True\nVariable containing:\n 0.1756\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n\nVariable containing:\n1.00000e-03 *\n -1.2566 -3.5917 -0.8016  2.9373  2.7126\n -0.0264  2.3321 -1.3892  1.0676 -1.9841\n  0.1204  4.5507 -2.2642 -3.7239  1.3170\n  0.5667 -2.7765  4.3564 -4.8917  2.7452\n  2.3703 -2.4578  2.7945  1.8861 -4.5931\n -0.2237 -0.2954  1.7677  1.7757 -3.0243\n -3.9952 -7.2011  2.1794  3.3324  5.6845\n -0.2814  0.0368 -2.2764 -0.2955  2.8164\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]\n\nVariable containing:\n 0.1756\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n\nVariable containing:\n1.00000e-02 *\n -0.6283 -1.7959 -0.4008  1.4686  1.3563\n -0.0132  1.1661 -0.6946  0.5338 -0.9920\n  0.0602  2.2753 -1.1321 -1.8619  0.6585\n  0.2833 -1.3883  2.1782 -2.4459  1.3726\n  1.1852 -1.2289  1.3972  0.9430 -2.2966\n -0.1119 -0.1477  0.8838  0.8879 -1.5121\n -1.9976 -3.6006  1.0897  1.6662  2.8423\n -0.1407  0.0184 -1.1382 -0.1477  1.4082\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]", "body": "Pytorch Version: '0.3.0'\r\n\r\nCode:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport sys\r\n\r\nbatchsize = 8\r\nnclass = 5\r\n\r\nif sys.argv[1] == \"cuda\":\r\n    cuda = True\r\nelse:\r\n    cuda = False\r\n\r\nprint(\"Cuda: \" + str(cuda))\r\n\r\ndef make_prob():\r\n    if cuda:\r\n        pr = torch.cuda.FloatTensor(batchsize, nclass)\r\n    else:\r\n        pr = torch.FloatTensor(batchsize, nclass)\r\n    pr.uniform_()\r\n    pr /= pr.sum(dim=1).view(batchsize, 1)\r\n    return pr\r\n\r\ndef loss_kl(pi, targets):\r\n    pi_var = Variable(pi, requires_grad=True)\r\n    logsoftmax = nn.LogSoftmax(dim=1)\r\n    kl = nn.KLDivLoss()\r\n\r\n    if cuda:\r\n        logsoftmax.cuda()\r\n        kl.cuda()\r\n\r\n    logpi = logsoftmax(pi_var)\r\n    return kl(logpi, Variable(targets)) * logpi.size(1), pi_var\r\n\r\ndef loss_plain(pi, targets):\r\n    pi_var = Variable(pi, requires_grad=True)\r\n    targets_var = Variable(targets)\r\n    logsoftmax = nn.LogSoftmax(dim=1)\r\n\r\n    if cuda:\r\n        logsoftmax.cuda()\r\n\r\n    logpi = logsoftmax(pi_var)\r\n    loss = - (logpi * targets_var).sum(dim=1).mean() + (targets_var * targets_var.log()).sum(dim=1).mean()\r\n    return loss, pi_var\r\n\r\ntargets = make_prob()\r\npi = make_prob()\r\n\r\nloss1, pi_var1 = loss_kl(pi, targets)\r\nprint(loss1)\r\nloss1.backward()\r\nprint(pi_var1.grad)\r\n\r\nloss2, pi_var2 = loss_plain(pi, targets)\r\nprint(loss2)\r\nloss2.backward()\r\nprint(pi_var2.grad)\r\n```\r\n\r\nWithout cuda, both the gradients and the loss are the same:\r\n```\r\n$ python3 ~/test_kl.py nocuda\r\nCuda: False\r\nVariable containing:\r\n 0.1930\r\n[torch.FloatTensor of size 1]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\r\n -2.1244  2.5148  0.9348  0.5213 -1.8464\r\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\r\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\r\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\r\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\r\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\r\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\r\n[torch.FloatTensor of size 8x5]\r\n\r\nVariable containing:\r\n 0.1930\r\n[torch.FloatTensor of size 1]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\r\n -2.1244  2.5148  0.9348  0.5213 -1.8464\r\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\r\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\r\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\r\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\r\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\r\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\r\n[torch.FloatTensor of size 8x5]\r\n```\r\n\r\nWith cuda the gradients returned by KLDivLoss are scaled down by the number of class, while the loss is the same. \r\n```\r\n$ python3 ~/test_kl.py cuda\r\nCuda: True\r\nVariable containing:\r\n 0.1756\r\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\r\n\r\nVariable containing:\r\n1.00000e-03 *\r\n -1.2566 -3.5917 -0.8016  2.9373  2.7126\r\n -0.0264  2.3321 -1.3892  1.0676 -1.9841\r\n  0.1204  4.5507 -2.2642 -3.7239  1.3170\r\n  0.5667 -2.7765  4.3564 -4.8917  2.7452\r\n  2.3703 -2.4578  2.7945  1.8861 -4.5931\r\n -0.2237 -0.2954  1.7677  1.7757 -3.0243\r\n -3.9952 -7.2011  2.1794  3.3324  5.6845\r\n -0.2814  0.0368 -2.2764 -0.2955  2.8164\r\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]\r\n\r\nVariable containing:\r\n 0.1756\r\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n -0.6283 -1.7959 -0.4008  1.4686  1.3563\r\n -0.0132  1.1661 -0.6946  0.5338 -0.9920\r\n  0.0602  2.2753 -1.1321 -1.8619  0.6585\r\n  0.2833 -1.3883  2.1782 -2.4459  1.3726\r\n  1.1852 -1.2289  1.3972  0.9430 -2.2966\r\n -0.1119 -0.1477  0.8838  0.8879 -1.5121\r\n -1.9976 -3.6006  1.0897  1.6662  2.8423\r\n -0.1407  0.0184 -1.1382 -0.1477  1.4082\r\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]\r\n```"}