{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397759877", "html_url": "https://github.com/pytorch/pytorch/issues/8508#issuecomment-397759877", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8508", "id": 397759877, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Nzc1OTg3Nw==", "user": {"login": "praveen-palanisamy", "id": 4770482, "node_id": "MDQ6VXNlcjQ3NzA0ODI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4770482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/praveen-palanisamy", "html_url": "https://github.com/praveen-palanisamy", "followers_url": "https://api.github.com/users/praveen-palanisamy/followers", "following_url": "https://api.github.com/users/praveen-palanisamy/following{/other_user}", "gists_url": "https://api.github.com/users/praveen-palanisamy/gists{/gist_id}", "starred_url": "https://api.github.com/users/praveen-palanisamy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/praveen-palanisamy/subscriptions", "organizations_url": "https://api.github.com/users/praveen-palanisamy/orgs", "repos_url": "https://api.github.com/users/praveen-palanisamy/repos", "events_url": "https://api.github.com/users/praveen-palanisamy/events{/privacy}", "received_events_url": "https://api.github.com/users/praveen-palanisamy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-15T22:40:29Z", "updated_at": "2018-06-15T22:40:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23639302\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vishwakftw\">@vishwakftw</a> : Did you test it with your suggestion? It will cause a <code>RuntimeError dimension specified as -1 but tensor has no dimensions</code> at this line:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32\">pytorch/torch/distributions/multivariate_normal.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 32\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4\">302408e</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L32\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"32\"></td>\n          <td id=\"LC32\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> n <span class=\"pl-k\">=</span> bvec.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>I think the <code>SIGFPE</code> arises when <code>MultivariateNormal</code>'s <code>self.scale_tril</code> is expanded and reshaped to have <code>.dim() == 3</code> ( so as to conform with <code>torch.bmm</code>'s interface) when <code>eps</code> is  a scalar. Specifically, in this line: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L36\">pytorch/torch/distributions/multivariate_normal.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 36\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4\">302408e</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L36\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"36\"></td>\n          <td id=\"LC36\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> bmat <span class=\"pl-k\">=</span> bmat.expand(batch_shape <span class=\"pl-k\">+</span> (n, n)).reshape((<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, n, n)) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>The  root cause in my opinion is that  the <code>self._extended_shape(...)</code> method  in <code>distribution.py</code> fails to upcast the returned shape to (1,) when the <code>batch_shape</code> and the <code>event_shape</code> is empty which causes <code>eps</code> to be a 0 dimensional tensor leading to the <code>SIGFPE</code>.</p>\n<p>In the PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"332618541\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8543\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8543/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8543\">#8543</a> , I upcasted the <code>loc</code> in <code>MultivariateNormal.__init__(...)</code> if <code>loc</code> is a scalar. To me, that looked transparent and easy to follow than if it was done inside the <code>_extended_shape(...)</code> method by upcasting the <code>event_shape</code> when the <code>loc</code>'s shape was the root cause. Let me know if someone thinks, it is better to do it in <code>_extended_shape(...)</code> due to some reason (like, taking care of scalars in other distributions as well).</p>", "body_text": "@vishwakftw : Did you test it with your suggestion? It will cause a RuntimeError dimension specified as -1 but tensor has no dimensions at this line:\n\n  \n    \n      pytorch/torch/distributions/multivariate_normal.py\n    \n    \n         Line 32\n      in\n      302408e\n    \n    \n    \n    \n\n        \n          \n           n = bvec.size(-1) \n        \n    \n  \n\n\nI think the SIGFPE arises when MultivariateNormal's self.scale_tril is expanded and reshaped to have .dim() == 3 ( so as to conform with torch.bmm's interface) when eps is  a scalar. Specifically, in this line: \n  \n    \n      pytorch/torch/distributions/multivariate_normal.py\n    \n    \n         Line 36\n      in\n      302408e\n    \n    \n    \n    \n\n        \n          \n           bmat = bmat.expand(batch_shape + (n, n)).reshape((-1, n, n)) \n        \n    \n  \n\n\nThe  root cause in my opinion is that  the self._extended_shape(...) method  in distribution.py fails to upcast the returned shape to (1,) when the batch_shape and the event_shape is empty which causes eps to be a 0 dimensional tensor leading to the SIGFPE.\nIn the PR #8543 , I upcasted the loc in MultivariateNormal.__init__(...) if loc is a scalar. To me, that looked transparent and easy to follow than if it was done inside the _extended_shape(...) method by upcasting the event_shape when the loc's shape was the root cause. Let me know if someone thinks, it is better to do it in _extended_shape(...) due to some reason (like, taking care of scalars in other distributions as well).", "body": "@vishwakftw : Did you test it with your suggestion? It will cause a `RuntimeError dimension specified as -1 but tensor has no dimensions` at this line:\r\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32\r\n\r\nI think the `SIGFPE` arises when `MultivariateNormal`'s `self.scale_tril` is expanded and reshaped to have `.dim() == 3` ( so as to conform with `torch.bmm`'s interface) when `eps` is  a scalar. Specifically, in this line: https://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L36\r\n\r\nThe  root cause in my opinion is that  the `self._extended_shape(...)` method  in `distribution.py` fails to upcast the returned shape to (1,) when the `batch_shape` and the `event_shape` is empty which causes `eps` to be a 0 dimensional tensor leading to the `SIGFPE`. \r\n\r\n In the PR #8543 , I upcasted the `loc` in `MultivariateNormal.__init__(...)` if `loc` is a scalar. To me, that looked transparent and easy to follow than if it was done inside the `_extended_shape(...)` method by upcasting the `event_shape` when the `loc`'s shape was the root cause. Let me know if someone thinks, it is better to do it in `_extended_shape(...)` due to some reason (like, taking care of scalars in other distributions as well).\r\n\r\n"}