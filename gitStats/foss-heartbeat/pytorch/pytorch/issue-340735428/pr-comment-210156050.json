{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210156050", "pull_request_review_id": 146313219, "id": 210156050, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDE1NjA1MA==", "diff_hunk": "@@ -5885,3 +5885,58 @@ def parse_kwargs(desc):\n             [4, 5, 6],\n             [4, 5, 6]])\n \"\"\")\n+\n+\n+add_docstr(torch.cartesian_prod,\n+           r\"\"\"\n+cartesian_prod(tensors) -> seq\n+\n+Do cartesian product of the given sequence of tensors. The behavior is similar to\n+python's `itertools.prod`. The difference is, if the arguments is a sequence of\n+size :math:`k`, `itertools.prod` generate :math:`k`-tuples, while `torch.cartesian_prod`\n+create :math:`k` output tensors.\n+\n+Arguments:\n+    tensors (sequence of Tensors): sequence of scalars or 1 dimensional tensors.\n+        Scalars will be treated as tensors of size :math:`(1,)` automatically.\n+\n+Returns:\n+    seq (sequence of Tensors): If the input has :math:`k` tensors of size\n+        :math:`(N_1,), (N_2,), \\ldots , (N_k,)`, then the output would also has :math:`k` tensors,\n+        where all tensors are of size :math:`N_1 \\times N_2 \\times \\ldots \\times N_k`.\n+\n+Example::\n+\n+    >>> a = torch.tensor([1, 2, 3])\n+    >>> b = torch.tensor([4, 5])\n+    >>> torch.cartesian_prod([a, b])\n+    (tensor([1, 1, 2, 2, 3, 3]), tensor([4, 5, 4, 5, 4, 5]))\n+\"\"\")\n+\n+\n+add_docstr(torch.combinations,\n+           r\"\"\"\n+combinations(tensor, r=2, with_replacement=False) -> seq\n+\n+Compute combinations of length :math:`r` of the given tensor. The behavior is similar to\n+python's `itertools.combinations` when `with_replacement` is set to `False`, and\n+`itertools.combinations_with_replacement` when `with_replacement` is set to `True`.\n+\n+Arguments:\n+    tensor (Tensor): the tensor.\n+    r (int, optional): number of elements to combine\n+    with_replacement (boolean, optional): whether to allow duplication in combination\n+\n+Returns:\n+    seq (sequence of Tensors): :math:`r` tensors.\n+\n+Example::\n+\n+    >>> a = torch.tensor([1, 2, 3])\n+    >>> torch.combinations(a)\n+    (tensor([1, 1, 2]), tensor([2, 3, 3]))", "path": "torch/_torch_docs.py", "position": null, "original_position": 53, "commit_id": "c201cbe8327481d310f1300cb5b9a866a3fdcc5d", "original_commit_id": "b520abc520d6cec86bb924cb26aa2f979828be0c", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "I prefer one of `tensor([[1, 2], [1, 3], [2, 3]])` or `(tensor([1, 2]), tensor(1, 3), tensor(2, 3))` to `(tensor([1, 1, 2]), tensor([2, 3, 3]))` because it makes more sense to me; I can do\r\n```\r\nfor combo in torch.combinations(tensor):\r\n    # do something.\r\n```\r\n\r\nThat brings up a larger question though -- what do you see are the use cases of cartesian_product and combinations? I think those are useful in `for` loop constructs like the example I wrote above, but please let me know your thoughts\r\n", "created_at": "2018-08-15T02:18:44Z", "updated_at": "2018-11-23T15:49:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/9393#discussion_r210156050", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9393", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210156050"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9393#discussion_r210156050"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9393"}}, "body_html": "<p>I prefer one of <code>tensor([[1, 2], [1, 3], [2, 3]])</code> or <code>(tensor([1, 2]), tensor(1, 3), tensor(2, 3))</code> to <code>(tensor([1, 1, 2]), tensor([2, 3, 3]))</code> because it makes more sense to me; I can do</p>\n<pre><code>for combo in torch.combinations(tensor):\n    # do something.\n</code></pre>\n<p>That brings up a larger question though -- what do you see are the use cases of cartesian_product and combinations? I think those are useful in <code>for</code> loop constructs like the example I wrote above, but please let me know your thoughts</p>", "body_text": "I prefer one of tensor([[1, 2], [1, 3], [2, 3]]) or (tensor([1, 2]), tensor(1, 3), tensor(2, 3)) to (tensor([1, 1, 2]), tensor([2, 3, 3])) because it makes more sense to me; I can do\nfor combo in torch.combinations(tensor):\n    # do something.\n\nThat brings up a larger question though -- what do you see are the use cases of cartesian_product and combinations? I think those are useful in for loop constructs like the example I wrote above, but please let me know your thoughts", "in_reply_to_id": 210027879}