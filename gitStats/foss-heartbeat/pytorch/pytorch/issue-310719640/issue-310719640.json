{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6222", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6222/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6222/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6222/events", "html_url": "https://github.com/pytorch/pytorch/issues/6222", "id": 310719640, "node_id": "MDU6SXNzdWUzMTA3MTk2NDA=", "number": 6222, "title": "[Bug] MaxPool3d causes GPU memory leaking", "user": {"login": "xichangzun", "id": 19312309, "node_id": "MDQ6VXNlcjE5MzEyMzA5", "avatar_url": "https://avatars3.githubusercontent.com/u/19312309?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xichangzun", "html_url": "https://github.com/xichangzun", "followers_url": "https://api.github.com/users/xichangzun/followers", "following_url": "https://api.github.com/users/xichangzun/following{/other_user}", "gists_url": "https://api.github.com/users/xichangzun/gists{/gist_id}", "starred_url": "https://api.github.com/users/xichangzun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xichangzun/subscriptions", "organizations_url": "https://api.github.com/users/xichangzun/orgs", "repos_url": "https://api.github.com/users/xichangzun/repos", "events_url": "https://api.github.com/users/xichangzun/events{/privacy}", "received_events_url": "https://api.github.com/users/xichangzun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-03T07:12:10Z", "updated_at": "2018-04-03T19:47:30Z", "closed_at": "2018-04-03T19:47:30Z", "author_association": "NONE", "body_html": "<p>When I try to train my model which contains MaxPool3d , It always end up with 'out of memory' error.<br>\nmy environment info is here:</p>\n<ul>\n<li>16.04</li>\n<li>PyTorch version: 0.4.0a0+da6c3c9</li>\n<li>installed PyTorch from source</li>\n<li>python version is 3.6.2</li>\n<li>CUDA/cuDNN version: 9.1/7.1.2</li>\n<li>GCC version (if compiling from source):GCC 5.4</li>\n<li>Build command you used (if compiling from source): as default</li>\n</ul>\n<p>I can reproduce this bug by the following script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> optim\n<span class=\"pl-k\">from</span> torch.nn.functional <span class=\"pl-k\">import</span> smooth_l1_loss\nmodel <span class=\"pl-k\">=</span> nn.Sequential(\n    nn.Conv3d(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,<span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n    nn.ReLU(),\n    nn.MaxPool3d(<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>optimizer = optim.SGD((x for x in model.parameters() if x.requires_grad is True), lr=1e-3, momentum=0.9,nesterov=True)</span>\ncrit <span class=\"pl-k\">=</span> smooth_l1_loss\nmodel.cuda()\ncount <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>    optimizer.zero_grad()</span>\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">30</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">200</span>,<span class=\"pl-c1\">200</span>,<span class=\"pl-c1\">200</span>).cuda()\n    loc_output<span class=\"pl-k\">=</span> model(<span class=\"pl-c1\">input</span>)\n    loc_outpus <span class=\"pl-k\">=</span> loc_output\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">type</span>(loc_output) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">tuple</span>:\n        loc_output <span class=\"pl-k\">=</span> loc_output[<span class=\"pl-c1\">0</span>]\n    targets <span class=\"pl-k\">=</span> torch.rand(loc_output.size()).cuda()\n    loss <span class=\"pl-k\">=</span> crit(loc_output,targets)\n    loss.backward()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>    optimizer.step()</span>\n    <span class=\"pl-k\">del</span> loss,loc_output,targets,<span class=\"pl-c1\">input</span>\n    torch.cuda.empty_cache()\n    count <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-c1\">print</span>(count)</pre></div>", "body_text": "When I try to train my model which contains MaxPool3d , It always end up with 'out of memory' error.\nmy environment info is here:\n\n16.04\nPyTorch version: 0.4.0a0+da6c3c9\ninstalled PyTorch from source\npython version is 3.6.2\nCUDA/cuDNN version: 9.1/7.1.2\nGCC version (if compiling from source):GCC 5.4\nBuild command you used (if compiling from source): as default\n\nI can reproduce this bug by the following script:\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.nn.functional import smooth_l1_loss\nmodel = nn.Sequential(\n    nn.Conv3d(1,1,kernel_size=3,padding=1),\n    nn.ReLU(),\n    nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n)\n#optimizer = optim.SGD((x for x in model.parameters() if x.requires_grad is True), lr=1e-3, momentum=0.9,nesterov=True)\ncrit = smooth_l1_loss\nmodel.cuda()\ncount = 0\nwhile True:\n#    optimizer.zero_grad()\n    input = torch.rand(30,1,200,200,200).cuda()\n    loc_output= model(input)\n    loc_outpus = loc_output\n    if type(loc_output) == tuple:\n        loc_output = loc_output[0]\n    targets = torch.rand(loc_output.size()).cuda()\n    loss = crit(loc_output,targets)\n    loss.backward()\n#    optimizer.step()\n    del loss,loc_output,targets,input\n    torch.cuda.empty_cache()\n    count += 1\n    print(count)", "body": "When I try to train my model which contains MaxPool3d , It always end up with 'out of memory' error. \r\nmy environment info is here:\r\n* 16.04\r\n* PyTorch version: 0.4.0a0+da6c3c9\r\n* installed PyTorch from source\r\n* python version is 3.6.2\r\n* CUDA/cuDNN version: 9.1/7.1.2\r\n* GCC version (if compiling from source):GCC 5.4\r\n* Build command you used (if compiling from source): as default\r\n\r\nI can reproduce this bug by the following script:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch import optim\r\nfrom torch.nn.functional import smooth_l1_loss\r\nmodel = nn.Sequential(\r\n    nn.Conv3d(1,1,kernel_size=3,padding=1),\r\n    nn.ReLU(),\r\n    nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\r\n)\r\n#optimizer = optim.SGD((x for x in model.parameters() if x.requires_grad is True), lr=1e-3, momentum=0.9,nesterov=True)\r\ncrit = smooth_l1_loss\r\nmodel.cuda()\r\ncount = 0\r\nwhile True:\r\n#    optimizer.zero_grad()\r\n    input = torch.rand(30,1,200,200,200).cuda()\r\n    loc_output= model(input)\r\n    loc_outpus = loc_output\r\n    if type(loc_output) == tuple:\r\n        loc_output = loc_output[0]\r\n    targets = torch.rand(loc_output.size()).cuda()\r\n    loss = crit(loc_output,targets)\r\n    loss.backward()\r\n#    optimizer.step()\r\n    del loss,loc_output,targets,input\r\n    torch.cuda.empty_cache()\r\n    count += 1\r\n    print(count)\r\n```"}