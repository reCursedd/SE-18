{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230907615", "pull_request_review_id": 171748419, "id": 230907615, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMDkwNzYxNQ==", "diff_hunk": "@@ -0,0 +1,138 @@\n+from __future__ import absolute_import, division, print_function, unicode_literals\n+\n+import multiprocessing\n+import multiprocessing.connection\n+import signal\n+import sys\n+\n+\n+def _wrap(fn, i, args, error_queue):\n+    try:\n+        fn(i, *args)\n+    except KeyboardInterrupt:\n+        pass  # SIGINT; Killed by parent, do nothing\n+    except Exception:\n+        # Propagate exception to parent process, keeping original traceback\n+        import traceback\n+        error_queue.put(traceback.format_exc())\n+        sys.exit(1)\n+\n+\n+class SpawnContext:\n+    def __init__(self, processes, error_queues):\n+        self.error_queues = error_queues\n+        self.processes = processes\n+        self.sentinels = {\n+            process.sentinel: index\n+            for index, process in enumerate(processes)\n+        }\n+\n+    def join(self, timeout=None):\n+        r\"\"\"\n+        Tries to join one or more processes in this spawn context.\n+        If one of them exited with a non-zero exit status, this function\n+        kills the remaining processes and raises an exception with the cause\n+        of the first process exiting.\n+\n+        Returns ``True`` if all processes have been joined successfully,\n+        ``False`` if there are more processes that need to be joined.\n+\n+        Arguments:\n+            timeout (float): Wait this long before giving up on waiting.\n+        \"\"\"\n+        # Ensure this function can be called even when we're done.\n+        if len(self.sentinels) == 0:\n+            return True\n+\n+        # Wait for any process to fail or all of them to succeed.\n+        ready = multiprocessing.connection.wait(\n+            self.sentinels.keys(),\n+            timeout=timeout,\n+        )\n+\n+        error_index = None\n+        for sentinel in ready:\n+            index = self.sentinels.pop(sentinel)\n+            process = self.processes[index]\n+            process.join()\n+            if process.exitcode != 0:\n+                error_index = index\n+                break\n+\n+        # Return if there was no error.\n+        if error_index is None:\n+            # Return whether or not all processes have been joined.\n+            return len(self.sentinels) == 0\n+\n+        # Assume failure. Terminate processes that are still alive.\n+        for process in self.processes:\n+            if process.is_alive():\n+                process.terminate()\n+            process.join()\n+\n+        # There won't be an error on the queue if the process crashed.\n+        if self.error_queues[error_index].empty():\n+            exitcode = self.processes[error_index].exitcode\n+            if exitcode < 0:\n+                name = signal.Signals(-exitcode).name\n+                raise Exception(\n+                    \"process %d terminated with signal %s\" %\n+                    (error_index, name)\n+                )\n+            else:\n+                raise Exception(\n+                    \"process %d terminated with exit code %d\" %\n+                    (error_index, exitcode)\n+                )\n+\n+        original_trace = self.error_queues[error_index].get()\n+        msg = \"\\n\\n-- Process %d terminated with the following error:\\n\" % error_index\n+        msg += original_trace\n+        raise Exception(msg)\n+\n+\n+def spawn(fn, args=(), nprocs=1, join=True):\n+    r\"\"\"Spawns ``nprocs`` processes that run ``fn`` with ``args``.", "path": "torch/multiprocessing/spawn.py", "position": 95, "original_position": 95, "commit_id": "33e98fca64140f99ec01978852c2235998904035", "original_commit_id": "021d13ef79a639864a4b20e745df8cf1a9a293a8", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "You may want to add this to `multiprocessing.rst`.\r\n\r\nYou should probably also mention the Python 3.4+ limitation here in the function docs, since I think that's what users will see.", "created_at": "2018-11-05T20:53:32Z", "updated_at": "2018-11-23T15:54:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/13518#discussion_r230907615", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13518", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230907615"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13518#discussion_r230907615"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13518"}}, "body_html": "<p>You may want to add this to <code>multiprocessing.rst</code>.</p>\n<p>You should probably also mention the Python 3.4+ limitation here in the function docs, since I think that's what users will see.</p>", "body_text": "You may want to add this to multiprocessing.rst.\nYou should probably also mention the Python 3.4+ limitation here in the function docs, since I think that's what users will see."}