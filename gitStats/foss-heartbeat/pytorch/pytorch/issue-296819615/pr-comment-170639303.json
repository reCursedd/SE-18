{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170639303", "pull_request_review_id": 99355322, "id": 170639303, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDYzOTMwMw==", "diff_hunk": "@@ -63,86 +72,103 @@ def compiled_with_cuda():\n     return 'not compiled w/ CUDA'\n \n \n+env_summary = \"\"\"\n+--------------------------------------------------------------------------------\n+  Environment Summary\n+--------------------------------------------------------------------------------\n+PyTorch {pytorch_version}{debug_str} {cuda_compiled}\n+Running with Python {py_version} and {cuda_runtime}\n+\n+`{pip_version} list` truncated output:\n+{pip_list_output}\n+\"\"\".strip()\n+\n+\n def run_env_analysis():\n     print('Running environment analysis...')\n     result = []\n \n     debug_str = ''\n     if torch.version.debug:\n         debug_str = ' DEBUG'\n-    result.append('PyTorch {}{} {}'.format(\n-        torch.__version__, debug_str,\n-        compiled_with_cuda()))\n \n-    avail = 'Running with python {}.{}, '.format(sys.version_info[0], sys.version_info[1])\n+    cuda_avail = ''\n     if torch.cuda.is_available():\n         cuda = check_running_cuda_version()\n-        if cuda is None:\n-            cuda = ''\n-        avail += 'CUDA {}'.format(cuda)\n+        if cuda is not None:\n+            cuda_avail = 'CUDA ' + cuda\n     else:\n-        avail += 'CUDA unavailable'\n-    result.append(avail)\n-\n-    result.append('')\n-\n-    pip = check_pip_packages()\n-    if pip is not None:\n-        result.append(check_pip_packages())\n-\n-    return '\\n'.join(result)\n-\n-\n-def set_env_variables(gpu_device):\n-    # Override CUDA_LAUNCH_BLOCKING by default for more accurate profiling.\n-    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n-\n-    # Only profile on one GPU for simplicity\n-    os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(gpu_device)\n-\n+        cuda = 'CUDA unavailable'\n+\n+    pip_version, pip_list_output = check_pip_packages()\n+    if pip_list_output is None:\n+        pip_list_output = 'Unable to fetch'\n+\n+    result = {\n+        'debug_str': debug_str,\n+        'pytorch_version': torch.__version__,\n+        'cuda_compiled': compiled_with_cuda(),\n+        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'cuda_runtime': cuda_avail,\n+        'pip_version': pip_version,\n+        'pip_list_output': pip_list_output,\n+    }\n \n-def get_env_variables_description(gpu_device, prefix=' '):\n-    if torch.cuda.is_available():\n-        return '{}with environment variables CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES={}'.format(\n-            prefix, gpu_device)\n-    else:\n-        return ''\n+    return env_summary.format(**result)\n \n \n-def run_cprofile(code, globs, gpu_device=0):\n-    set_env_variables(gpu_device)\n-    env_str = get_env_variables_description(gpu_device)\n-    print('Running your script with cProfile{}...'.format(env_str))\n+def run_cprofile(code, globs, launch_blocking=False):\n+    print('Running your script with cProfile')\n     prof = cProfile.Profile()\n     prof.enable()\n     exec(code, globs, None)\n     prof.disable()\n     return prof\n \n \n-def print_line(width=80):\n-    print('-' * width)\n+cprof_summary = \"\"\"\n+--------------------------------------------------------------------------------\n+  cProfile output\n+--------------------------------------------------------------------------------\n+\"\"\".strip()\n+\n+\n+def print_cprofile_summary(prof, sortby='tottime', topk=15):\n+    result = {}\n \n+    print(cprof_summary.format(**result))\n \n-def print_cprofile_summary(prof, gpu_device=0, sortby='tottime', topk=15):\n-    print_line()\n-    env_str = get_env_variables_description(gpu_device, '\\n')\n-    print('cProfile output{}'.format(env_str))\n-    print_line()\n     cprofile_stats = pstats.Stats(prof).sort_stats(sortby)\n     cprofile_stats.print_stats(topk)\n \n \n-def run_autograd_prof(code, globs, gpu_device=0):\n-    set_env_variables(gpu_device)\n-    env_str = get_env_variables_description(gpu_device)\n-    print('Running your script with the autograd profiler{}...'.format(env_str))\n-    with profiler.profile() as prof:\n-        exec(code, globs, None)\n-    return prof\n+def run_autograd_prof(code, globs):\n+    def run_prof(use_cuda=False):\n+        with profiler.profile(use_cuda=use_cuda) as prof:\n+            exec(code, globs, None)\n+        return prof\n+\n+    print('Running your script with the autograd profiler...')\n+    result = [run_prof(use_cuda=False)]\n+    if torch.cuda.is_available():\n+        result.append(run_prof(use_cuda=True))", "path": "torch/utils/bottleneck/__main__.py", "position": 155, "original_position": 182, "commit_id": "1528ed4f2ff57111dc5224eb86864738bd56957b", "original_commit_id": "f3866470e3314dffbfdfe47cb0c18e240ae04262", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Oh I must have missed that. That's great!", "created_at": "2018-02-26T15:58:02Z", "updated_at": "2018-11-23T15:39:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/5216#discussion_r170639303", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5216", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170639303"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5216#discussion_r170639303"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5216"}}, "body_html": "<p>Oh I must have missed that. That's great!</p>", "body_text": "Oh I must have missed that. That's great!", "in_reply_to_id": 170208343}