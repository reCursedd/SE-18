{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168010135", "pull_request_review_id": 96310336, "id": 168010135, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODAxMDEzNQ==", "diff_hunk": "@@ -0,0 +1,231 @@\n+import argparse\n+import cProfile\n+import pstats\n+import subprocess\n+import sys\n+import os\n+import re\n+\n+try:\n+    from StringIO import StringIO\n+except ImportError:\n+    from io import StringIO\n+\n+import torch\n+from torch.autograd import profiler\n+\n+PY3 = sys.version_info >= (3, 0)\n+\n+\n+def run(command):\n+    \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n+    p = subprocess.Popen(command, stdout=subprocess.PIPE,\n+                         stderr=subprocess.PIPE, shell=True)\n+    output, err = p.communicate()\n+    rc = p.returncode\n+    if PY3:\n+        output = output.decode(\"ascii\")\n+        err = err.decode(\"ascii\")\n+    return (rc, output, err)\n+\n+\n+def check_running_cuda_version():\n+    (rc, out, err) = run('nvcc --version')\n+    if rc is not 0:\n+        return None\n+    m = re.search(r'V(.*)$', out)\n+    assert m is not None\n+    return m.group(1)\n+\n+\n+def check_pip_packages():\n+    # People generally have `pip` as `pip` or `pip3`\n+    def run_with_pip(pip):\n+        (rc, out, err) = run(pip + ' list --format=legacy | grep torch')\n+        if rc is 0:\n+            return '`{}` list truncated output:\\n{}'.format(pip, out)\n+        return None\n+\n+    result = []\n+    out = run_with_pip('pip')\n+    if out is not None:\n+        result.append(out)\n+    out_pip3 = run_with_pip('pip3')\n+    if out_pip3 is not None:\n+        result.append(out_pip3)\n+\n+    return '\\n'.join(result)\n+\n+\n+def compiled_with_cuda():\n+    if torch.version.cuda:\n+        return 'compiled w/ CUDA {}'.format(torch.version.cuda)\n+    return 'not compiled w/ CUDA'\n+\n+\n+def run_env_analysis():\n+    print('Running environment analysis...')\n+    result = []\n+\n+    debug_str = ''\n+    if torch.version.debug:\n+        debug_str = ' DEBUG'\n+    result.append('PyTorch {}{} {}'.format(\n+        torch.__version__, debug_str,\n+        compiled_with_cuda()))\n+\n+    avail = 'Running with python {}.{}, '.format(sys.version_info[0], sys.version_info[1])\n+    if torch.cuda.is_available():\n+        cuda = check_running_cuda_version()\n+        if cuda is None:\n+            cuda = ''\n+        avail += 'CUDA {}'.format(cuda)\n+    else:\n+        avail += 'CUDA unavailable'\n+    result.append(avail)\n+\n+    result.append('')\n+\n+    pip = check_pip_packages()\n+    if pip is not None:\n+        result.append(check_pip_packages())\n+\n+    return '\\n'.join(result)\n+\n+\n+def set_env_variables(gpu_device):\n+    # Override CUDA_LAUNCH_BLOCKING by default for more accurate profiling.\n+    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'", "path": "torch/utils/bottleneck/__main__.py", "position": null, "original_position": 98, "commit_id": "1528ed4f2ff57111dc5224eb86864738bd56957b", "original_commit_id": "b56fa144bf379cb0fb25f89bf80281bff7d2d04e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "We really should use the CUDA mode of the profiler instead of CUDA_LAUNCH_BLOCKING. This will give a very skewed picture. It would be nice if we could run the script twice: first without CUDA-mode profiling enabled, then with CUDA-mode profiling enabled. If the execution times are similar for both (which means that CUDA-mode didn't add a lot of overhead), present only the CUDA-level result. Otherwise present both.", "created_at": "2018-02-13T21:41:00Z", "updated_at": "2018-11-23T15:39:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/5216#discussion_r168010135", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5216", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168010135"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5216#discussion_r168010135"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5216"}}, "body_html": "<p>We really should use the CUDA mode of the profiler instead of CUDA_LAUNCH_BLOCKING. This will give a very skewed picture. It would be nice if we could run the script twice: first without CUDA-mode profiling enabled, then with CUDA-mode profiling enabled. If the execution times are similar for both (which means that CUDA-mode didn't add a lot of overhead), present only the CUDA-level result. Otherwise present both.</p>", "body_text": "We really should use the CUDA mode of the profiler instead of CUDA_LAUNCH_BLOCKING. This will give a very skewed picture. It would be nice if we could run the script twice: first without CUDA-mode profiling enabled, then with CUDA-mode profiling enabled. If the execution times are similar for both (which means that CUDA-mode didn't add a lot of overhead), present only the CUDA-level result. Otherwise present both."}