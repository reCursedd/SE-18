{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/173950045", "pull_request_review_id": 103223540, "id": 173950045, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3Mzk1MDA0NQ==", "diff_hunk": "@@ -0,0 +1,31 @@\n+torch.utils.bottleneck\n+===============\n+\n+.. currentmodule:: torch.utils.bottleneck\n+\n+`torch.utils.bottleneck` is a tool that can be used as an initial step for\n+debugging bottlenecks in your program. It summarizes runs of your script with \n+the Python profiler and PyTorch's autograd profiler. \n+\n+Run it on the command line with \n+\n+::\n+\n+    python -m torch.utils.bottleneck -- /path/to/source/script.py [args]\n+\n+where [args] are any number of arguments to `script.py`, or run\n+``python -m torch.utils.bottleneck -h`` for more usage instructions.\n+\n+.. warning::\n+    Because your script will be profiled, please ensure that it exits in a \n+    finite amount of time.\n+\n+.. warning::\n+    Due to the asynchronous nature of CUDA kernels, when running against\n+    CUDA code, the cProfile output and CPU-mode autograd profilers may\n+    not show correct timings. In this case, the CUDA-mode autograd\n+    profiler is better at assigning blame to the relevant operator(s).", "path": "docs/source/bottleneck.rst", "position": 27, "original_position": 27, "commit_id": "1528ed4f2ff57111dc5224eb86864738bd56957b", "original_commit_id": "1528ed4f2ff57111dc5224eb86864738bd56957b", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "The picture for CUDA is not this easy, because what you should really do would be to check if your script is CPU-bound (\"CPU time total is much smaller than CUDA time total\" is a fairly good heuristic - profiling overhead is invisible if CPU is far ahead of the GPU anyway). Only if this is not the case should you start looking for responsible CUDA operators.\r\n\r\nOf course the reality is much more complicated and your script can be in any of those two regimes depending on the part of the model you're evaluating. If the previous steps don't help you could try `nvprof` with `emit_nvtx`.\r\n\r\n---\r\n\r\nIt would be nice to expand on this like I did above", "created_at": "2018-03-12T21:18:49Z", "updated_at": "2018-11-23T15:40:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/5216#discussion_r173950045", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5216", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/173950045"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5216#discussion_r173950045"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5216"}}, "body_html": "<p>The picture for CUDA is not this easy, because what you should really do would be to check if your script is CPU-bound (\"CPU time total is much smaller than CUDA time total\" is a fairly good heuristic - profiling overhead is invisible if CPU is far ahead of the GPU anyway). Only if this is not the case should you start looking for responsible CUDA operators.</p>\n<p>Of course the reality is much more complicated and your script can be in any of those two regimes depending on the part of the model you're evaluating. If the previous steps don't help you could try <code>nvprof</code> with <code>emit_nvtx</code>.</p>\n<hr>\n<p>It would be nice to expand on this like I did above</p>", "body_text": "The picture for CUDA is not this easy, because what you should really do would be to check if your script is CPU-bound (\"CPU time total is much smaller than CUDA time total\" is a fairly good heuristic - profiling overhead is invisible if CPU is far ahead of the GPU anyway). Only if this is not the case should you start looking for responsible CUDA operators.\nOf course the reality is much more complicated and your script can be in any of those two regimes depending on the part of the model you're evaluating. If the previous steps don't help you could try nvprof with emit_nvtx.\n\nIt would be nice to expand on this like I did above"}