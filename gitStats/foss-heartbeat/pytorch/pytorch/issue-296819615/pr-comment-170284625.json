{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170284625", "pull_request_review_id": 98953193, "id": 170284625, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDI4NDYyNQ==", "diff_hunk": "@@ -63,86 +72,103 @@ def compiled_with_cuda():\n     return 'not compiled w/ CUDA'\n \n \n+env_summary = \"\"\"\n+--------------------------------------------------------------------------------\n+  Environment Summary\n+--------------------------------------------------------------------------------\n+PyTorch {pytorch_version}{debug_str} {cuda_compiled}\n+Running with Python {py_version} and {cuda_runtime}\n+\n+`{pip_version} list` truncated output:\n+{pip_list_output}\n+\"\"\".strip()\n+\n+\n def run_env_analysis():\n     print('Running environment analysis...')\n     result = []\n \n     debug_str = ''\n     if torch.version.debug:\n         debug_str = ' DEBUG'\n-    result.append('PyTorch {}{} {}'.format(\n-        torch.__version__, debug_str,\n-        compiled_with_cuda()))\n \n-    avail = 'Running with python {}.{}, '.format(sys.version_info[0], sys.version_info[1])\n+    cuda_avail = ''\n     if torch.cuda.is_available():\n         cuda = check_running_cuda_version()\n-        if cuda is None:\n-            cuda = ''\n-        avail += 'CUDA {}'.format(cuda)\n+        if cuda is not None:\n+            cuda_avail = 'CUDA ' + cuda\n     else:\n-        avail += 'CUDA unavailable'\n-    result.append(avail)\n-\n-    result.append('')\n-\n-    pip = check_pip_packages()\n-    if pip is not None:\n-        result.append(check_pip_packages())\n-\n-    return '\\n'.join(result)\n-\n-\n-def set_env_variables(gpu_device):\n-    # Override CUDA_LAUNCH_BLOCKING by default for more accurate profiling.\n-    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n-\n-    # Only profile on one GPU for simplicity\n-    os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(gpu_device)\n-\n+        cuda = 'CUDA unavailable'\n+\n+    pip_version, pip_list_output = check_pip_packages()\n+    if pip_list_output is None:\n+        pip_list_output = 'Unable to fetch'\n+\n+    result = {\n+        'debug_str': debug_str,\n+        'pytorch_version': torch.__version__,\n+        'cuda_compiled': compiled_with_cuda(),\n+        'py_version': '{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n+        'cuda_runtime': cuda_avail,\n+        'pip_version': pip_version,\n+        'pip_list_output': pip_list_output,\n+    }\n \n-def get_env_variables_description(gpu_device, prefix=' '):\n-    if torch.cuda.is_available():\n-        return '{}with environment variables CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES={}'.format(\n-            prefix, gpu_device)\n-    else:\n-        return ''\n+    return env_summary.format(**result)\n \n \n-def run_cprofile(code, globs, gpu_device=0):\n-    set_env_variables(gpu_device)\n-    env_str = get_env_variables_description(gpu_device)\n-    print('Running your script with cProfile{}...'.format(env_str))\n+def run_cprofile(code, globs, launch_blocking=False):\n+    print('Running your script with cProfile')\n     prof = cProfile.Profile()\n     prof.enable()\n     exec(code, globs, None)\n     prof.disable()\n     return prof\n \n \n-def print_line(width=80):\n-    print('-' * width)\n+cprof_summary = \"\"\"\n+--------------------------------------------------------------------------------\n+  cProfile output\n+--------------------------------------------------------------------------------\n+\"\"\".strip()\n+\n+\n+def print_cprofile_summary(prof, sortby='tottime', topk=15):\n+    result = {}\n \n+    print(cprof_summary.format(**result))\n \n-def print_cprofile_summary(prof, gpu_device=0, sortby='tottime', topk=15):\n-    print_line()\n-    env_str = get_env_variables_description(gpu_device, '\\n')\n-    print('cProfile output{}'.format(env_str))\n-    print_line()\n     cprofile_stats = pstats.Stats(prof).sort_stats(sortby)\n     cprofile_stats.print_stats(topk)\n \n \n-def run_autograd_prof(code, globs, gpu_device=0):\n-    set_env_variables(gpu_device)\n-    env_str = get_env_variables_description(gpu_device)\n-    print('Running your script with the autograd profiler{}...'.format(env_str))\n-    with profiler.profile() as prof:\n-        exec(code, globs, None)\n-    return prof\n+def run_autograd_prof(code, globs):\n+    def run_prof(use_cuda=False):\n+        with profiler.profile(use_cuda=use_cuda) as prof:\n+            exec(code, globs, None)\n+        return prof\n+\n+    print('Running your script with the autograd profiler...')\n+    result = [run_prof(use_cuda=False)]\n+    if torch.cuda.is_available():\n+        result.append(run_prof(use_cuda=True))", "path": "torch/utils/bottleneck/__main__.py", "position": 155, "original_position": 182, "commit_id": "1528ed4f2ff57111dc5224eb86864738bd56957b", "original_commit_id": "f3866470e3314dffbfdfe47cb0c18e240ae04262", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "Do you mean with the `profiler.emit_nvtx`?", "created_at": "2018-02-23T15:41:57Z", "updated_at": "2018-11-23T15:39:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/5216#discussion_r170284625", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5216", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170284625"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5216#discussion_r170284625"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5216"}}, "body_html": "<p>Do you mean with the <code>profiler.emit_nvtx</code>?</p>", "body_text": "Do you mean with the profiler.emit_nvtx?", "in_reply_to_id": 170208343}