{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168213516", "pull_request_review_id": 96543873, "id": 168213516, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODIxMzUxNg==", "diff_hunk": "@@ -0,0 +1,231 @@\n+import argparse\n+import cProfile\n+import pstats\n+import subprocess\n+import sys\n+import os\n+import re\n+\n+try:\n+    from StringIO import StringIO\n+except ImportError:\n+    from io import StringIO\n+\n+import torch\n+from torch.autograd import profiler\n+\n+PY3 = sys.version_info >= (3, 0)\n+\n+\n+def run(command):\n+    \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n+    p = subprocess.Popen(command, stdout=subprocess.PIPE,\n+                         stderr=subprocess.PIPE, shell=True)\n+    output, err = p.communicate()\n+    rc = p.returncode\n+    if PY3:\n+        output = output.decode(\"ascii\")\n+        err = err.decode(\"ascii\")\n+    return (rc, output, err)\n+\n+\n+def check_running_cuda_version():\n+    (rc, out, err) = run('nvcc --version')\n+    if rc is not 0:\n+        return None\n+    m = re.search(r'V(.*)$', out)\n+    assert m is not None\n+    return m.group(1)\n+\n+\n+def check_pip_packages():\n+    # People generally have `pip` as `pip` or `pip3`\n+    def run_with_pip(pip):\n+        (rc, out, err) = run(pip + ' list --format=legacy | grep torch')\n+        if rc is 0:\n+            return '`{}` list truncated output:\\n{}'.format(pip, out)\n+        return None\n+\n+    result = []\n+    out = run_with_pip('pip')\n+    if out is not None:\n+        result.append(out)\n+    out_pip3 = run_with_pip('pip3')\n+    if out_pip3 is not None:\n+        result.append(out_pip3)\n+\n+    return '\\n'.join(result)\n+\n+\n+def compiled_with_cuda():\n+    if torch.version.cuda:\n+        return 'compiled w/ CUDA {}'.format(torch.version.cuda)\n+    return 'not compiled w/ CUDA'\n+\n+\n+def run_env_analysis():\n+    print('Running environment analysis...')\n+    result = []\n+\n+    debug_str = ''\n+    if torch.version.debug:\n+        debug_str = ' DEBUG'\n+    result.append('PyTorch {}{} {}'.format(\n+        torch.__version__, debug_str,\n+        compiled_with_cuda()))\n+\n+    avail = 'Running with python {}.{}, '.format(sys.version_info[0], sys.version_info[1])\n+    if torch.cuda.is_available():\n+        cuda = check_running_cuda_version()\n+        if cuda is None:\n+            cuda = ''\n+        avail += 'CUDA {}'.format(cuda)\n+    else:\n+        avail += 'CUDA unavailable'\n+    result.append(avail)\n+\n+    result.append('')\n+\n+    pip = check_pip_packages()\n+    if pip is not None:\n+        result.append(check_pip_packages())\n+\n+    return '\\n'.join(result)\n+\n+\n+def set_env_variables(gpu_device):\n+    # Override CUDA_LAUNCH_BLOCKING by default for more accurate profiling.\n+    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n+\n+    # Only profile on one GPU for simplicity\n+    os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(gpu_device)\n+\n+\n+def get_env_variables_description(gpu_device, prefix=' '):\n+    if torch.cuda.is_available():\n+        return '{}with environment variables CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES={}'.format(\n+            prefix, gpu_device)\n+    else:\n+        return ''\n+\n+\n+def run_cprofile(code, globs, gpu_device=0):\n+    set_env_variables(gpu_device)\n+    env_str = get_env_variables_description(gpu_device)\n+    print('Running your script with cProfile{}...'.format(env_str))\n+    prof = cProfile.Profile()\n+    prof.enable()\n+    exec(code, globs, None)\n+    prof.disable()\n+    return prof\n+\n+\n+def print_line(width=80):\n+    print('-' * width)\n+\n+\n+def print_cprofile_summary(prof, gpu_device=0, sortby='tottime', topk=15):\n+    print_line()\n+    env_str = get_env_variables_description(gpu_device, '\\n')\n+    print('cProfile output{}'.format(env_str))\n+    print_line()\n+    cprofile_stats = pstats.Stats(prof).sort_stats(sortby)\n+    cprofile_stats.print_stats(topk)\n+\n+\n+def run_autograd_prof(code, globs, gpu_device=0):\n+    set_env_variables(gpu_device)\n+    env_str = get_env_variables_description(gpu_device)\n+    print('Running your script with the autograd profiler{}...'.format(env_str))\n+    with profiler.profile() as prof:\n+        exec(code, globs, None)\n+    return prof\n+\n+\n+def print_autograd_prof_summary(prof, gpu_device=0, sortby='cpu_time', topk=15):\n+    valid_sortby = ['cpu_time', 'cuda_time', 'cpu_time_total', 'cuda_time_total', 'count']\n+    if sortby not in valid_sortby:\n+        warn = ('WARNING: invalid sorting option for autograd profiler results: {}\\n'\n+                'Expected `cpu_time`, `cpu_time_total`, or `count`. '\n+                'Defaulting to `cpu_time`.')\n+        print(warn.format(autograd_prof_sortby))\n+        sortby = 'cpu_time'\n+\n+    print_line()\n+    env_str = get_env_variables_description(gpu_device, '\\n')\n+    print('autograd profiler output{}'.format(env_str))\n+    print_line()\n+    print('\\ttop {} events sorted by {}'.format(topk, sortby))\n+    ex = ('    Note that because CUDA_LAUNCH_BLOCKING=1 is set, the reported CPU time\\n'\n+          '    includes the CUDA time. Ignore the empty CUDA time column here.\\n')\n+    if torch.cuda.is_available():\n+        print('')\n+        print(ex)\n+\n+    sorted_events = sorted(prof.function_events,\n+                           key=lambda x: getattr(x, sortby), reverse=True)\n+    topk_events = sorted_events[:topk]\n+    print(torch.autograd.profiler.build_table(topk_events))\n+\n+\n+descript = ('`bottleneck` is a tool that can be used as an initial step for debugging '\n+            'bottlenecks in your program.\\n\\n'\n+            'It summarizes runs of your script with the Python profiler '\n+            'and PyTorch\\'s autograd profiler. For ease of use and intepretability of '\n+            'results, `bottleneck` runs multi-GPU code on only one GPU device. '\n+            'Because your script will be profiled, please ensure that it exits '\n+            'in a finite amount of time. \\n\\n'\n+            'For more complicated uses of the profilers (like in a multi-GPU case), '\n+            'please see https://docs.python.org/3/library/profile.html '\n+            'and http://pytorch.org/docs/master/autograd.html#profiler '\n+            'for more information. \\n')\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(description=descript)\n+    parser.add_argument('scriptfile', type=str,\n+                        help='Path to the script to be run. '\n+                        'Usually run with `python path/to/script`.')\n+    parser.add_argument('--gpu', dest='gpu_device', type=int, default=0,\n+                        help='If applicable, which GPU device to run '\n+                        'on. Default: 0.')\n+    return parser.parse_args()\n+    return parser.parse_args()", "path": "torch/utils/bottleneck/__main__.py", "position": null, "original_position": 193, "commit_id": "1528ed4f2ff57111dc5224eb86864738bd56957b", "original_commit_id": "b56fa144bf379cb0fb25f89bf80281bff7d2d04e", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "Thanks for catching that!", "created_at": "2018-02-14T15:41:04Z", "updated_at": "2018-11-23T15:39:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/5216#discussion_r168213516", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5216", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168213516"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5216#discussion_r168213516"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5216"}}, "body_html": "<p>Thanks for catching that!</p>", "body_text": "Thanks for catching that!", "in_reply_to_id": 167966186}