{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7462", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7462/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7462/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7462/events", "html_url": "https://github.com/pytorch/pytorch/issues/7462", "id": 321914384, "node_id": "MDU6SXNzdWUzMjE5MTQzODQ=", "number": 7462, "title": "use detach_ to cut gradient propagation \uff0c but get a lower accuracy", "user": {"login": "Ken-Con", "id": 14369757, "node_id": "MDQ6VXNlcjE0MzY5NzU3", "avatar_url": "https://avatars2.githubusercontent.com/u/14369757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ken-Con", "html_url": "https://github.com/Ken-Con", "followers_url": "https://api.github.com/users/Ken-Con/followers", "following_url": "https://api.github.com/users/Ken-Con/following{/other_user}", "gists_url": "https://api.github.com/users/Ken-Con/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ken-Con/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ken-Con/subscriptions", "organizations_url": "https://api.github.com/users/Ken-Con/orgs", "repos_url": "https://api.github.com/users/Ken-Con/repos", "events_url": "https://api.github.com/users/Ken-Con/events{/privacy}", "received_events_url": "https://api.github.com/users/Ken-Con/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-05-10T12:29:51Z", "updated_at": "2018-05-10T12:29:51Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>this is my code\uff1a</p>\n<pre><code>probs = (P*class_mask) \npt = torch.pow(1-probs, 0)\npt.detach_()\nlogmean  = (probs.log()).sum(1).view(-1,1)\nlogmean  = (pt*probs.log()).sum(1).view(-1,1)\n</code></pre>\n<p>the two loss function above with pt or not have different test accuracy, the first function always have a higher acc than the second. I have tried many times .But in my opinion, due to the pt is identically equal to 1 and has been detached\uff0cpt will not have an impact on gradients. is there anything that I understand wrongly?thanks</p>", "body_text": "this is my code\uff1a\nprobs = (P*class_mask) \npt = torch.pow(1-probs, 0)\npt.detach_()\nlogmean  = (probs.log()).sum(1).view(-1,1)\nlogmean  = (pt*probs.log()).sum(1).view(-1,1)\n\nthe two loss function above with pt or not have different test accuracy, the first function always have a higher acc than the second. I have tried many times .But in my opinion, due to the pt is identically equal to 1 and has been detached\uff0cpt will not have an impact on gradients. is there anything that I understand wrongly?thanks", "body": "this is my code\uff1a\r\n```\r\nprobs = (P*class_mask) \r\npt = torch.pow(1-probs, 0)\r\npt.detach_()\r\nlogmean  = (probs.log()).sum(1).view(-1,1)\r\nlogmean  = (pt*probs.log()).sum(1).view(-1,1)\r\n```\r\nthe two loss function above with pt or not have different test accuracy, the first function always have a higher acc than the second. I have tried many times .But in my opinion, due to the pt is identically equal to 1 and has been detached\uff0cpt will not have an impact on gradients. is there anything that I understand wrongly?thanks"}