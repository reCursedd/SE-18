{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197497630", "pull_request_review_id": 131243833, "id": 197497630, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NzQ5NzYzMA==", "diff_hunk": "@@ -0,0 +1,506 @@\n+#include <ATen/ATen.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/native/sparse/SparseUtils.h>\n+#include <ATen/native/sparse/cuda/SparseCUDAApplyUtils.cuh>\n+#include <ATen/native/sparse/cuda/SparseCUDABlas.cuh>\n+#include <ATen/cuda/CUDAApplyUtils.cuh>\n+#include <ATen/cuda/detail/IndexUtils.cuh>\n+\n+#include <THC/THCTensorMathPointwise.cuh>\n+#include <THC/THCThrustAllocator.cuh>\n+#include <THC/THCNumerics.cuh>\n+#include <thrust/device_ptr.h>\n+#include <thrust/sequence.h>\n+#include <thrust/system/cuda/execution_policy.h>\n+\n+#define I_INFO(tensor) cuda::detail::getTensorInfo<int64_t, uint64_t>(tensor)\n+#define V_INFO(tensor) cuda::detail::getTensorInfo<scalar_t, uint64_t>(tensor)\n+\n+namespace at { namespace native {\n+\n+// --------------------------------------------------------------------\n+// Utility functions\n+// --------------------------------------------------------------------\n+\n+#ifndef __HIP_PLATFORM_HCC__\n+namespace {\n+  IntTensor _to_csr_int(const LongTensor& rowIndices, int64_t dim, int64_t nnz) {\n+    IntTensor csr = at::empty({dim+1}, CUDA(kInt));\n+    IntTensor rowIndicesInt = at::empty({rowIndices.size(0)}, CUDA(kInt));\n+    rowIndicesInt.copy_(rowIndices);\n+    sparse::cuda::Xcoo2csr(rowIndicesInt.data<int32_t>(), nnz, dim, csr.data<int32_t>());\n+    return csr;\n+  }\n+}\n+#endif\n+\n+// NB: Deleted spaddcmul (aka addcmul_, but not actually wired up), spaddcdiv (not\n+// wired at all)\n+\n+// --------------------------------------------------------------------\n+// addmm(Tensor, SparseTensorRef, Tensor, Scalar, Scalar)  [broadcasts]\n+// --------------------------------------------------------------------\n+\n+Tensor& s_addmm_out_sparse_dense_cuda(Tensor& r_, const Tensor& t, const SparseTensor& sparse_, const Tensor& dense, Scalar beta, Scalar alpha) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  AT_CHECK(_check_device({sparse_, r_, t, dense}));\n+  // THCudaIntTensor *csr;\n+  // THCIndexTensor *indices;\n+  // THCTensor *values, *r__, *dense_;\n+\n+  // TODO: This error message seems awfully opaque\n+  AT_CHECK(sparse_._sparseDims() == 2, \"matrices expected, got \", sparse_._sparseDims(), \"D tensor\");\n+  AT_CHECK(sparse_._denseDims() == 0, \"scalar values expected, got \", sparse_._denseDims(), \"D values\");\n+  AT_CHECK(dense.dim() == 2, \"matrices expected, got \", dense.dim(), \"D tensor\");\n+\n+  // mxk * kxn = mxn\n+  int64_t m = sparse_.size(0);\n+  int64_t k = sparse_.size(1);\n+  int64_t n = dense.size(1);\n+\n+  r_.resize_({m, n});\n+\n+  AT_CHECK(t.size(0) == m,\n+      \"Argument #1 (t): Expected dim 0 size \", m, \", got \", t.size(0));\n+  AT_CHECK(t.size(1) == n,\n+      \"Argument #1 (t): Expected dim 1 size \", n, \", got \", t.size(1));\n+  AT_CHECK(dense.size(0) == k,\n+      \"Argument #3 (dense): Expected dim 0 size \", k, \", got \", dense.size(0));\n+\n+  SparseTensor sparse = sparse_.coalesce();\n+\n+  int64_t nnz = sparse._nnz();\n+  LongTensor indices = sparse._indices();\n+  Tensor values = sparse._values();\n+\n+  LongTensor rowIndices = indices.select(0, 0);\n+  LongTensor colIndices = indices.select(0, 1);\n+  IntTensor csr = _to_csr_int(rowIndices, m, nnz);\n+  IntTensor colIndicesInt = at::empty({colIndices.size(0)}, indices.type().toScalarType(kInt));\n+  colIndicesInt.copy_(colIndices);\n+\n+  // No half support, so we don't have to use CUDATypeConversion\n+  Tensor r__;\n+  AT_DISPATCH_FLOATING_TYPES(\n+      values.type(), \"addmm_sparse_cuda\", [&] {\n+        scalar_t cast_beta = beta.to<scalar_t>();\n+        scalar_t cast_alpha = alpha.to<scalar_t>();\n+        if (cast_beta == 0) {\n+          r_.zero_();\n+        } else if (cast_beta == ScalarConvert<int, scalar_t>::to(1)) {\n+          if (!isSameTensor(t, r_)) {\n+            r_.copy_(t);\n+          }\n+        } else {\n+          at::mul_out(r_, t, beta);\n+        }\n+\n+        /* r_ */\n+        if(r_.stride(0) == 1 && r_.stride(1) == r_.size(0)) {\n+          r__ = r_;\n+        } else {\n+          // TODO: how... strange\n+          r__ = r_.transpose(0, 1).clone();\n+          r__.transpose_(0, 1);\n+        }\n+\n+        /* dense */\n+        Tensor dense_;\n+        char transpose_dense;\n+        if(dense.stride(0) == 1 && dense.stride(1) == dense.size(0)) {\n+          transpose_dense = 'n';\n+          dense_ = dense;\n+        } else if(dense.stride(1) == 1 && dense.stride(0) != dense.size(1)) {\n+          transpose_dense = 't';\n+          dense_ = dense;\n+        } else {\n+          transpose_dense = 't';\n+          dense_ = dense.contiguous();\n+        }\n+\n+        sparse::cuda::csrmm2(\n+          'n',\n+          transpose_dense,\n+          m,\n+          n,\n+          k,\n+          nnz,\n+          cast_alpha,\n+          values.data<scalar_t>(),\n+          csr.data<int32_t>(),\n+          colIndicesInt.data<int32_t>(),\n+          dense_.data<scalar_t>(),\n+          (transpose_dense == 'n' ? dense_.stride(1) : dense_.stride(0)),\n+          cast_beta,\n+          r__.data<scalar_t>(),\n+          r__.stride(1));\n+\n+      });\n+\n+  r_.copy_(r__);\n+  return r_;\n+#else\n+  AT_ERROR(\"s_addmm_out_sparse_dense_cuda: HIP not supported\");\n+#endif\n+}\n+\n+Tensor s_addmm_sparse_dense_cuda(\n+    const Tensor& t,\n+    const SparseTensor& sparse,\n+    const Tensor& dense,\n+    Scalar beta,\n+    Scalar alpha\n+) {\n+  Tensor r = t.type().tensor();\n+  s_addmm_out_sparse_dense_cuda(r, t, sparse, dense, beta, alpha);\n+  return r;\n+}\n+\n+Tensor& s_addmm_sparse_dense_cuda_(\n+    Tensor& t,\n+    const SparseTensor& sparse,\n+    const Tensor& dense,\n+    Scalar beta,\n+    Scalar alpha\n+) {\n+  return s_addmm_out_sparse_dense_cuda(t, t, sparse, dense, beta, alpha);\n+}\n+\n+// Deleted sspaddmm (sparse, dense) -> sparse\n+\n+// --------------------------------------------------------------------\n+// hspmm(SparseTensor mat1, Tensor mat2)\n+// --------------------------------------------------------------------\n+\n+SparseTensor& hspmm_out_sparse_cuda(SparseTensor& r_, const SparseTensor& sparse_, const Tensor& dense/* , Scalar alpha */) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  cudaStream_t stream = globalContext().getCurrentCUDAStream();\n+  auto allocator = THCThrustAllocator(globalContext().lazyInitCUDA());\n+  auto policy = thrust::cuda::par(allocator).on(stream);\n+\n+  AT_CHECK(_check_device({r_, sparse_, dense}));\n+\n+  AT_CHECK(sparse_._sparseDims() == 2,\n+      \"Argument #2: matrices expected, got \", sparse_._sparseDims(), \"D tensor\");\n+  AT_CHECK(sparse_._denseDims() == 0,\n+      \"Argument #2: scalar values expected, got \", sparse_._denseDims(), \"D values\");\n+  AT_CHECK(dense.dim() == 2,\n+      \"Argument #3: matrices expected, got \", dense.dim(), \"D tensor\");\n+\n+  int64_t m = sparse_.size(0);\n+  int64_t k = sparse_.size(1);\n+  int64_t n = dense.size(1);\n+\n+  AT_CHECK(dense.size(0) == k,\n+      \"Argument #3: Expected dim 0 size \", k, \", got \", dense.size(0));\n+  _get_sparse_impl(r_)->raw_resize_(1, 1, {m, n});\n+\n+  SparseTensor sparse = sparse_.coalesce();\n+\n+  int64_t nnz = sparse._nnz();\n+\n+  LongTensor indices = at::empty({1, nnz}, CUDA(kLong));\n+  // create values in column-major format to avoid copying in spaddmm\n+  Tensor values = at::empty({n, nnz}, dense.type());\n+  values.transpose_(0, 1);\n+\n+  // why does sparse need to be cloned? If this is really necessary maybe we\n+  // need to fuse this with newCoalesce\n+  SparseTensor newSparse = sparse.clone();\n+  LongTensor spIndices = newSparse._indices();\n+  LongTensor dstIndices = spIndices.select(0, 0);\n+  // Save destination indices to output hybrid tensor\n+  indices.copy_(dstIndices);\n+  // Replace destination indices with 0, 1, 2, 3, ... and compute output values\n+  // tensor with sparse * dense multiplication\n+  thrust::device_ptr<int64_t> indicesIter(dstIndices.data<int64_t>());\n+  thrust::sequence(policy, indicesIter, indicesIter + nnz);\n+  _get_sparse_impl(newSparse)->_sizes_mut()[0] = nnz; // TODO: use something safer)\n+  s_addmm_out_sparse_dense_cuda(values, values, newSparse, dense, 0, /*alpha*/ 1);\n+  _get_sparse_impl(r_)->set_indices_and_values(indices, values);\n+\n+  return r_;\n+#else\n+  AT_ERROR(\"hspmm_out_sparse_cuda: HIP not supported\");\n+#endif\n+}\n+\n+SparseTensor hspmm_sparse_cuda(const SparseTensor& sparse, const Tensor& dense) {\n+  SparseTensor r = sparse.type().tensor();\n+  hspmm_out_sparse_cuda(r, sparse, dense);\n+  return r;\n+}\n+\n+// --------------------------------------------------------------------\n+// add(Tensor, SparseTensorRef, Scalar)\n+//    formerly known as spcadd\n+// --------------------------------------------------------------------\n+\n+Tensor& add_out_dense_sparse_cuda(Tensor& r_, const Tensor& dense, SparseTensorRef sparse_, at::Scalar value) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  const SparseTensor& sparse = sparse_.tref;\n+\n+  AT_CHECK(_check_device({sparse, r_, dense}));\n+\n+  const int64_t nnz = sparse._nnz();\n+  if (nnz == 0) {\n+    r_.resize_as_(dense);\n+    r_.copy_(dense);\n+    return r_;\n+  }\n+\n+  Tensor r = r_;\n+  if (!isSameTensor(r, dense)) {\n+    r_.resize_as_(dense);\n+    r_.copy_(dense);\n+  } else {\n+    AT_CHECK(r_.is_contiguous(), \"CUDA dense-sparse addition known bug\");\n+    r = r_.contiguous();\n+  }\n+\n+  LongTensor indices = sparse._indices();\n+  Tensor values = sparse._values();\n+  int64_t nDim = dense.dim();\n+  int64_t nDimI = sparse._sparseDims();\n+\n+  if (sparse.is_coalesced()) {\n+    // TODO benchmark to decide whether to remove this special case\n+    const dim3 block = cuda::getApplyBlock();\n+    dim3 grid;\n+    int curDevice = -1;\n+    cudaGetDevice(&curDevice);\n+    cudaStream_t stream = globalContext().getCurrentCUDAStreamOnDevice(curDevice);\n+    if (sparse._denseDims() == 0) {\n+      AT_CHECK(cuda::getApplyGrid(nnz, grid, curDevice), \"Argument #0: tensor too large or too many dimensions\");\n+\n+      AT_DISPATCH_ALL_TYPES_AND_HALF(\n+          values.type(), \"add_out_dense_sparse_cuda\", [&] {\n+            apply::sparseElementwiseKernelScalar<TensorCAddOp<scalar_t>, uint64_t, scalar_t>\n+              <<<grid, block, 0, stream>>>(\n+                TensorCAddOp<scalar_t>(value.to<scalar_t>()),\n+                V_INFO(r_), I_INFO(indices), V_INFO(values),\n+                static_cast<uint64_t>(nnz));\n+          });\n+    } else {\n+      AT_CHECK(cuda::getApplyGrid(nnz * block.x, grid, curDevice), \"Argument #0: tensor too large or too many dimensions\");\n+\n+      AT_DISPATCH_ALL_TYPES_AND_HALF(\n+          values.type(), \"add_out_dense_sparse_cuda\", [&] {\n+            apply::sparseElementwiseKernel<TensorCAddOp<scalar_t>, uint64_t, scalar_t>\n+              <<<grid, block, 0, stream>>>(\n+                TensorCAddOp<scalar_t>(value.to<scalar_t>()),\n+                V_INFO(r_), I_INFO(indices), V_INFO(values),\n+                static_cast<uint64_t>(nnz));\n+          });\n+    }\n+  } else {\n+    LongTensor indices1D = _newFlattenedIndices(sparse, 0);\n+    indices1D.resize_({nnz});\n+\n+    // FIXME: at some point we can wrap the scale into indexAdd\n+    // NB: Purposely not inplace!\n+    AT_DISPATCH_ALL_TYPES_AND_HALF(\n+        values.type(), \"add_out_dense_sparse_cuda\", [&] {\n+          if (value.to<scalar_t>() != ScalarConvert<int, scalar_t>::to(1)) {\n+            values = values.mul(value);\n+          }\n+        });\n+\n+    int64_t view_rows = 1;\n+    int64_t view_columns = 1;\n+    for (int i = 0; i < nDimI; i++) {\n+      view_rows *= r.size(i);\n+    }\n+    for (int i = nDimI; i < nDim; i++) {\n+      view_columns *= r.size(i);\n+    }\n+\n+    Tensor r_view = r.view({view_rows, view_columns});\n+    values.resize_({nnz, view_columns});\n+    r_view.index_add_(0, indices1D, values);\n+  }\n+  THCudaCheck(cudaGetLastError());\n+\n+  return r_;\n+#else\n+  AT_ERROR(\"add_out_dense_sparse_cuda: HIP not supported\");\n+#endif\n+}\n+\n+Tensor add_dense_sparse_cuda(const Tensor& t, SparseTensorRef src, Scalar alpha) {\n+  Tensor r = t.type().tensor();\n+  add_out_dense_sparse_cuda(r, t, src, alpha);\n+  return r;\n+}\n+\n+Tensor& add_dense_sparse_cuda_(Tensor& t, SparseTensorRef src, Scalar alpha) {\n+  return add_out_dense_sparse_cuda(t, t, src, alpha);\n+}\n+\n+// --------------------------------------------------------------------\n+// add(SparseTensor, SparseTensor, Scalar)  [broadcasts]\n+// --------------------------------------------------------------------\n+\n+SparseTensor& s_add_out_sparse_cuda(SparseTensor& r_, const SparseTensor& t, const SparseTensor& src, Scalar value) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  AT_CHECK(_check_device({r_, t, src}));\n+  AT_CHECK(t.sizes().equals(src.sizes()), \"cadd operands have incompatible sizes\");\n+\n+  if (src._nnz() == 0) {\n+    return raw_copy_sparse_(r_, t);\n+  }\n+  if (t._nnz() == 0) {\n+    return mul_out_sparse_scalar(r_, src, value);\n+  }\n+\n+  AT_CHECK(_is_same_density(t, src), \"cadd operands have incompatible desnitities\");\n+\n+  // We deliberately choose to simply concat the indices and values tensors\n+  // rather than merging them. This removes the need to synchronously fetch nnz\n+  // at the end of the operation, at the cost of having a non-coalesced result.\n+  // This trade-off is preferable for the common use-case of gradient accumulation.\n+  LongTensor t_indices_ = t._indices();\n+  Tensor t_values_ = t._values();\n+  LongTensor s_indices_ = src._indices();\n+  Tensor s_values_ = src._values();\n+\n+  AT_DISPATCH_ALL_TYPES_AND_HALF(\n+      s_values_.type(), \"s_add_out_sparse_cuda\", [&] {\n+        if (value.to<scalar_t>() != ScalarConvert<int, scalar_t>::to(1)) {\n+          s_values_ = s_values_.mul(value);\n+        }\n+      });\n+\n+  LongTensor r_indices_ = at::cat({t_indices_, s_indices_}, 1);\n+  Tensor r_values_ = at::cat({t_values_, s_values_}, 0);\n+  r_.resize_as_(src);\n+  _alias_into_sparse(r_, r_indices_, r_values_);\n+\n+  // FIXME: add some heuristic about when to call coalesce() here, so that\n+  // tensors don't totally blow up in size by concatenation; e.g.\n+  //   r->minUnique = max(a->minUnique + b->minUnique);\n+  //   if (r->nnz / r->minUnique > COMPACTION_THRESHOLD) {\n+  //     THCSTensor_(contiguous)(r);\n+  //     r->minUnique = r->nnz;\n+  //   }\n+\n+  return r_;\n+#else\n+  AT_ERROR(\"s_add_out_sparse_cuda: HIP not supported\");\n+#endif\n+}\n+\n+SparseTensor s_add_sparse_cuda(const SparseTensor& t, const SparseTensor& src, Scalar alpha) {\n+  SparseTensor r = t.type().tensor();\n+  s_add_out_sparse_cuda(r, t, src, alpha);\n+  return r;\n+}\n+\n+SparseTensor& s_add_sparse_cuda_(SparseTensor& t, const SparseTensor& src, Scalar alpha) {\n+  return s_add_out_sparse_cuda(t, t, src, alpha);\n+}\n+\n+// --------------------------------------------------------------------\n+// sub(SparseTensor, SparseTensor, Scalar)  [broadcasts]\n+// --------------------------------------------------------------------\n+\n+SparseTensor& s_sub_out_sparse_cuda(SparseTensor& r, const SparseTensor& t, const SparseTensor& src, Scalar value) {\n+  AT_DISPATCH_ALL_TYPES(\n+      t.type(), \"sub_sparse\", [&] {\n+        scalar_t cast_value = value.to<scalar_t>();\n+        s_add_out_sparse_cuda(r, t, src, ScalarNegate<scalar_t>::to(cast_value));\n+      }\n+  );\n+  return r;\n+}\n+\n+SparseTensor s_sub_sparse_cuda(const SparseTensor& t, const SparseTensor& src, Scalar alpha) {\n+  SparseTensor r = t.type().tensor();\n+  s_sub_out_sparse_cuda(r, t, src, alpha);\n+  return r;\n+}\n+\n+SparseTensor& s_sub_sparse_cuda_(SparseTensor& t, const SparseTensor& src, Scalar alpha) {\n+  return s_sub_out_sparse_cuda(t, t, src, alpha);\n+}\n+\n+// --------------------------------------------------------------------\n+// mul(SparseTensor, SparseTensor, Scalar)  [broadcasts]\n+// --------------------------------------------------------------------\n+\n+SparseTensor& s_mul_out_sparse_cuda(SparseTensor& r_, const SparseTensor& t_, const SparseTensor& src_) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  AT_CHECK(_check_device({r_, t_, src_}));\n+  AT_CHECK(t_.sizes().equals(src_.sizes()), \"mul operands have incompatible sizes\");", "path": "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "position": 434, "original_position": 434, "commit_id": "1a3c38e577f84307b7b6f7c8e49818fde11f343f", "original_commit_id": "c7d4a43fe1a6c2cf74d918f277b1fc341a196cf3", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "this isn't equivalent -- previous code checked density as well.", "created_at": "2018-06-22T16:20:34Z", "updated_at": "2018-11-23T15:46:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/8689#discussion_r197497630", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8689", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197497630"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8689#discussion_r197497630"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8689"}}, "body_html": "<p>this isn't equivalent -- previous code checked density as well.</p>", "body_text": "this isn't equivalent -- previous code checked density as well."}