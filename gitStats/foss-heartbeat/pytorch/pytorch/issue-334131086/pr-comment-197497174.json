{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197497174", "pull_request_review_id": 131243833, "id": 197497174, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NzQ5NzE3NA==", "diff_hunk": "@@ -0,0 +1,506 @@\n+#include <ATen/ATen.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/native/sparse/SparseUtils.h>\n+#include <ATen/native/sparse/cuda/SparseCUDAApplyUtils.cuh>\n+#include <ATen/native/sparse/cuda/SparseCUDABlas.cuh>\n+#include <ATen/cuda/CUDAApplyUtils.cuh>\n+#include <ATen/cuda/detail/IndexUtils.cuh>\n+\n+#include <THC/THCTensorMathPointwise.cuh>\n+#include <THC/THCThrustAllocator.cuh>\n+#include <THC/THCNumerics.cuh>\n+#include <thrust/device_ptr.h>\n+#include <thrust/sequence.h>\n+#include <thrust/system/cuda/execution_policy.h>\n+\n+#define I_INFO(tensor) cuda::detail::getTensorInfo<int64_t, uint64_t>(tensor)\n+#define V_INFO(tensor) cuda::detail::getTensorInfo<scalar_t, uint64_t>(tensor)\n+\n+namespace at { namespace native {\n+\n+// --------------------------------------------------------------------\n+// Utility functions\n+// --------------------------------------------------------------------\n+\n+#ifndef __HIP_PLATFORM_HCC__\n+namespace {\n+  IntTensor _to_csr_int(const LongTensor& rowIndices, int64_t dim, int64_t nnz) {\n+    IntTensor csr = at::empty({dim+1}, CUDA(kInt));\n+    IntTensor rowIndicesInt = at::empty({rowIndices.size(0)}, CUDA(kInt));\n+    rowIndicesInt.copy_(rowIndices);\n+    sparse::cuda::Xcoo2csr(rowIndicesInt.data<int32_t>(), nnz, dim, csr.data<int32_t>());\n+    return csr;\n+  }\n+}\n+#endif\n+\n+// NB: Deleted spaddcmul (aka addcmul_, but not actually wired up), spaddcdiv (not\n+// wired at all)\n+\n+// --------------------------------------------------------------------\n+// addmm(Tensor, SparseTensorRef, Tensor, Scalar, Scalar)  [broadcasts]\n+// --------------------------------------------------------------------\n+\n+Tensor& s_addmm_out_sparse_dense_cuda(Tensor& r_, const Tensor& t, const SparseTensor& sparse_, const Tensor& dense, Scalar beta, Scalar alpha) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  AT_CHECK(_check_device({sparse_, r_, t, dense}));\n+  // THCudaIntTensor *csr;\n+  // THCIndexTensor *indices;\n+  // THCTensor *values, *r__, *dense_;\n+\n+  // TODO: This error message seems awfully opaque\n+  AT_CHECK(sparse_._sparseDims() == 2, \"matrices expected, got \", sparse_._sparseDims(), \"D tensor\");\n+  AT_CHECK(sparse_._denseDims() == 0, \"scalar values expected, got \", sparse_._denseDims(), \"D values\");\n+  AT_CHECK(dense.dim() == 2, \"matrices expected, got \", dense.dim(), \"D tensor\");\n+\n+  // mxk * kxn = mxn\n+  int64_t m = sparse_.size(0);\n+  int64_t k = sparse_.size(1);\n+  int64_t n = dense.size(1);\n+\n+  r_.resize_({m, n});\n+\n+  AT_CHECK(t.size(0) == m,\n+      \"Argument #1 (t): Expected dim 0 size \", m, \", got \", t.size(0));\n+  AT_CHECK(t.size(1) == n,\n+      \"Argument #1 (t): Expected dim 1 size \", n, \", got \", t.size(1));\n+  AT_CHECK(dense.size(0) == k,\n+      \"Argument #3 (dense): Expected dim 0 size \", k, \", got \", dense.size(0));\n+\n+  SparseTensor sparse = sparse_.coalesce();\n+\n+  int64_t nnz = sparse._nnz();\n+  LongTensor indices = sparse._indices();\n+  Tensor values = sparse._values();\n+\n+  LongTensor rowIndices = indices.select(0, 0);\n+  LongTensor colIndices = indices.select(0, 1);\n+  IntTensor csr = _to_csr_int(rowIndices, m, nnz);\n+  IntTensor colIndicesInt = at::empty({colIndices.size(0)}, indices.type().toScalarType(kInt));\n+  colIndicesInt.copy_(colIndices);\n+\n+  // No half support, so we don't have to use CUDATypeConversion\n+  Tensor r__;\n+  AT_DISPATCH_FLOATING_TYPES(\n+      values.type(), \"addmm_sparse_cuda\", [&] {\n+        scalar_t cast_beta = beta.to<scalar_t>();\n+        scalar_t cast_alpha = alpha.to<scalar_t>();\n+        if (cast_beta == 0) {\n+          r_.zero_();\n+        } else if (cast_beta == ScalarConvert<int, scalar_t>::to(1)) {\n+          if (!isSameTensor(t, r_)) {\n+            r_.copy_(t);\n+          }\n+        } else {\n+          at::mul_out(r_, t, beta);\n+        }\n+\n+        /* r_ */\n+        if(r_.stride(0) == 1 && r_.stride(1) == r_.size(0)) {\n+          r__ = r_;\n+        } else {\n+          // TODO: how... strange\n+          r__ = r_.transpose(0, 1).clone();\n+          r__.transpose_(0, 1);\n+        }\n+\n+        /* dense */\n+        Tensor dense_;\n+        char transpose_dense;\n+        if(dense.stride(0) == 1 && dense.stride(1) == dense.size(0)) {\n+          transpose_dense = 'n';\n+          dense_ = dense;\n+        } else if(dense.stride(1) == 1 && dense.stride(0) != dense.size(1)) {\n+          transpose_dense = 't';\n+          dense_ = dense;\n+        } else {\n+          transpose_dense = 't';\n+          dense_ = dense.contiguous();\n+        }\n+\n+        sparse::cuda::csrmm2(\n+          'n',\n+          transpose_dense,\n+          m,\n+          n,\n+          k,\n+          nnz,\n+          cast_alpha,\n+          values.data<scalar_t>(),\n+          csr.data<int32_t>(),\n+          colIndicesInt.data<int32_t>(),\n+          dense_.data<scalar_t>(),\n+          (transpose_dense == 'n' ? dense_.stride(1) : dense_.stride(0)),\n+          cast_beta,\n+          r__.data<scalar_t>(),\n+          r__.stride(1));\n+\n+      });\n+\n+  r_.copy_(r__);\n+  return r_;\n+#else\n+  AT_ERROR(\"s_addmm_out_sparse_dense_cuda: HIP not supported\");\n+#endif\n+}\n+\n+Tensor s_addmm_sparse_dense_cuda(\n+    const Tensor& t,\n+    const SparseTensor& sparse,\n+    const Tensor& dense,\n+    Scalar beta,\n+    Scalar alpha\n+) {\n+  Tensor r = t.type().tensor();\n+  s_addmm_out_sparse_dense_cuda(r, t, sparse, dense, beta, alpha);\n+  return r;\n+}\n+\n+Tensor& s_addmm_sparse_dense_cuda_(\n+    Tensor& t,\n+    const SparseTensor& sparse,\n+    const Tensor& dense,\n+    Scalar beta,\n+    Scalar alpha\n+) {\n+  return s_addmm_out_sparse_dense_cuda(t, t, sparse, dense, beta, alpha);\n+}\n+\n+// Deleted sspaddmm (sparse, dense) -> sparse\n+\n+// --------------------------------------------------------------------\n+// hspmm(SparseTensor mat1, Tensor mat2)\n+// --------------------------------------------------------------------\n+\n+SparseTensor& hspmm_out_sparse_cuda(SparseTensor& r_, const SparseTensor& sparse_, const Tensor& dense/* , Scalar alpha */) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  cudaStream_t stream = globalContext().getCurrentCUDAStream();\n+  auto allocator = THCThrustAllocator(globalContext().lazyInitCUDA());\n+  auto policy = thrust::cuda::par(allocator).on(stream);\n+\n+  AT_CHECK(_check_device({r_, sparse_, dense}));\n+\n+  AT_CHECK(sparse_._sparseDims() == 2,\n+      \"Argument #2: matrices expected, got \", sparse_._sparseDims(), \"D tensor\");\n+  AT_CHECK(sparse_._denseDims() == 0,\n+      \"Argument #2: scalar values expected, got \", sparse_._denseDims(), \"D values\");\n+  AT_CHECK(dense.dim() == 2,", "path": "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "position": 187, "original_position": 187, "commit_id": "1a3c38e577f84307b7b6f7c8e49818fde11f343f", "original_commit_id": "c7d4a43fe1a6c2cf74d918f277b1fc341a196cf3", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "also here.", "created_at": "2018-06-22T16:18:50Z", "updated_at": "2018-11-23T15:46:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/8689#discussion_r197497174", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8689", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197497174"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8689#discussion_r197497174"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8689"}}, "body_html": "<p>also here.</p>", "body_text": "also here."}