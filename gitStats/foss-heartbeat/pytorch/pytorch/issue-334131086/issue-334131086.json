{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8689", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8689/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8689/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8689/events", "html_url": "https://github.com/pytorch/pytorch/pull/8689", "id": 334131086, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk2MTgwNzAz", "number": 8689, "title": "Port THCS to ATen.", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-20T15:25:57Z", "updated_at": "2018-11-23T15:46:09Z", "closed_at": "2018-06-24T19:14:10Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8689", "html_url": "https://github.com/pytorch/pytorch/pull/8689", "diff_url": "https://github.com/pytorch/pytorch/pull/8689.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8689.patch"}, "body_html": "<p>General structure of the sparse implementation:</p>\n<ul>\n<li>SparseCUDATensor.{cpp, cu} and SparseCUDATensorMath.cu contain<br>\nthe same functions as their CPU analogues</li>\n<li>SparseCUDAApplyUtils.cuh contains what used to be in<br>\nTHCSTensor.cu</li>\n<li>SparseCUDABlas.cu contains what used to be THCSparse.cu</li>\n</ul>\n<p>Unrelated improvements:</p>\n<ul>\n<li>Forward declared CUDA types in Context.h are now moved<br>\nexclusively to CUDAHooks</li>\n<li>New getCurrentCUDASparseHandle in Context</li>\n<li>Support for printing CUSPARSE_STATUS_ZERO_PIVOT error message<br>\ndirectly</li>\n</ul>\n<p>Some unusual pieces:</p>\n<ul>\n<li>get_device got the LegacyBridge makeover, as it needs special<br>\nlogic on sparse tensors (defer to the inner tensors).</li>\n<li>I noticed that I need to turn off device_guard codegen<br>\nfor many functions in sparse, noticed because get_device<br>\nbecame a native function, and resulted in an infinite recursion.  This was<br>\ndone by adding device_guard: False to the native definitions.  An alternative<br>\nstrategy might be to make the heuristic for deciding when to put in a device<br>\nguard more clever.</li>\n</ul>\n<p>Scaffolding removal:</p>\n<ul>\n<li>LegacyBridge now special-cases only on sparse versus dense;<br>\nno more CUDA test (hooray!)</li>\n<li>Native bindings get CUDA/SparseCUDA dispatch entries.</li>\n</ul>\n<p>CPU sparse refactoring:</p>\n<ul>\n<li>New SparseUtils.h header, with all of the utility functions that<br>\nused to live in SparseTensor.cpp</li>\n<li>new_with_tensor_sparse now correctly handles both CPU and CUDA</li>\n<li>transpose functions in sparse/ turned out to be dead, so I killed them</li>\n</ul>\n<p>Bugs I noticed while working on this:</p>\n<ul>\n<li>I used accessor&lt;...&gt;() on a CUDA tensor, because I thought it does<br>\nthe CUDA-CPU sync.  It does not.</li>\n</ul>\n<p>TODO:</p>\n<ul>\n<li>For sparse only methods, we can now remove the TH binding<br>\nentirely</li>\n</ul>\n<p>Signed-off-by: Edward Z. Yang <a href=\"mailto:ezyang@fb.com\">ezyang@fb.com</a></p>", "body_text": "General structure of the sparse implementation:\n\nSparseCUDATensor.{cpp, cu} and SparseCUDATensorMath.cu contain\nthe same functions as their CPU analogues\nSparseCUDAApplyUtils.cuh contains what used to be in\nTHCSTensor.cu\nSparseCUDABlas.cu contains what used to be THCSparse.cu\n\nUnrelated improvements:\n\nForward declared CUDA types in Context.h are now moved\nexclusively to CUDAHooks\nNew getCurrentCUDASparseHandle in Context\nSupport for printing CUSPARSE_STATUS_ZERO_PIVOT error message\ndirectly\n\nSome unusual pieces:\n\nget_device got the LegacyBridge makeover, as it needs special\nlogic on sparse tensors (defer to the inner tensors).\nI noticed that I need to turn off device_guard codegen\nfor many functions in sparse, noticed because get_device\nbecame a native function, and resulted in an infinite recursion.  This was\ndone by adding device_guard: False to the native definitions.  An alternative\nstrategy might be to make the heuristic for deciding when to put in a device\nguard more clever.\n\nScaffolding removal:\n\nLegacyBridge now special-cases only on sparse versus dense;\nno more CUDA test (hooray!)\nNative bindings get CUDA/SparseCUDA dispatch entries.\n\nCPU sparse refactoring:\n\nNew SparseUtils.h header, with all of the utility functions that\nused to live in SparseTensor.cpp\nnew_with_tensor_sparse now correctly handles both CPU and CUDA\ntranspose functions in sparse/ turned out to be dead, so I killed them\n\nBugs I noticed while working on this:\n\nI used accessor<...>() on a CUDA tensor, because I thought it does\nthe CUDA-CPU sync.  It does not.\n\nTODO:\n\nFor sparse only methods, we can now remove the TH binding\nentirely\n\nSigned-off-by: Edward Z. Yang ezyang@fb.com", "body": "General structure of the sparse implementation:\r\n- SparseCUDATensor.{cpp, cu} and SparseCUDATensorMath.cu contain\r\n  the same functions as their CPU analogues\r\n- SparseCUDAApplyUtils.cuh contains what used to be in\r\n  THCSTensor.cu\r\n- SparseCUDABlas.cu contains what used to be THCSparse.cu\r\n\r\nUnrelated improvements:\r\n- Forward declared CUDA types in Context.h are now moved\r\n  exclusively to CUDAHooks\r\n- New getCurrentCUDASparseHandle in Context\r\n- Support for printing CUSPARSE_STATUS_ZERO_PIVOT error message\r\n  directly\r\n\r\nSome unusual pieces:\r\n- get_device got the LegacyBridge makeover, as it needs special\r\n  logic on sparse tensors (defer to the inner tensors).\r\n- I noticed that I need to turn off device_guard codegen\r\n  for many functions in sparse, noticed because get_device\r\n  became a native function, and resulted in an infinite recursion.  This was\r\n  done by adding device_guard: False to the native definitions.  An alternative\r\n  strategy might be to make the heuristic for deciding when to put in a device\r\n  guard more clever.\r\n\r\nScaffolding removal:\r\n- LegacyBridge now special-cases only on sparse versus dense;\r\n  no more CUDA test (hooray!)\r\n- Native bindings get CUDA/SparseCUDA dispatch entries.\r\n\r\nCPU sparse refactoring:\r\n- New SparseUtils.h header, with all of the utility functions that\r\n  used to live in SparseTensor.cpp\r\n- new_with_tensor_sparse now correctly handles both CPU and CUDA\r\n- transpose functions in sparse/ turned out to be dead, so I killed them\r\n\r\nBugs I noticed while working on this:\r\n- I used accessor<...>() on a CUDA tensor, because I thought it does\r\n  the CUDA-CPU sync.  It does not.\r\n\r\nTODO:\r\n- For sparse only methods, we can now remove the TH binding\r\n  entirely\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\n"}