{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197561004", "pull_request_review_id": 131337395, "id": 197561004, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NzU2MTAwNA==", "diff_hunk": "@@ -0,0 +1,506 @@\n+#include <ATen/ATen.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/native/sparse/SparseUtils.h>\n+#include <ATen/native/sparse/cuda/SparseCUDAApplyUtils.cuh>\n+#include <ATen/native/sparse/cuda/SparseCUDABlas.cuh>\n+#include <ATen/cuda/CUDAApplyUtils.cuh>\n+#include <ATen/cuda/detail/IndexUtils.cuh>\n+\n+#include <THC/THCTensorMathPointwise.cuh>\n+#include <THC/THCThrustAllocator.cuh>\n+#include <THC/THCNumerics.cuh>\n+#include <thrust/device_ptr.h>\n+#include <thrust/sequence.h>\n+#include <thrust/system/cuda/execution_policy.h>\n+\n+#define I_INFO(tensor) cuda::detail::getTensorInfo<int64_t, uint64_t>(tensor)\n+#define V_INFO(tensor) cuda::detail::getTensorInfo<scalar_t, uint64_t>(tensor)\n+\n+namespace at { namespace native {\n+\n+// --------------------------------------------------------------------\n+// Utility functions\n+// --------------------------------------------------------------------\n+\n+#ifndef __HIP_PLATFORM_HCC__\n+namespace {\n+  IntTensor _to_csr_int(const LongTensor& rowIndices, int64_t dim, int64_t nnz) {\n+    IntTensor csr = at::empty({dim+1}, CUDA(kInt));\n+    IntTensor rowIndicesInt = at::empty({rowIndices.size(0)}, CUDA(kInt));\n+    rowIndicesInt.copy_(rowIndices);\n+    sparse::cuda::Xcoo2csr(rowIndicesInt.data<int32_t>(), nnz, dim, csr.data<int32_t>());\n+    return csr;\n+  }\n+}\n+#endif\n+\n+// NB: Deleted spaddcmul (aka addcmul_, but not actually wired up), spaddcdiv (not\n+// wired at all)\n+\n+// --------------------------------------------------------------------\n+// addmm(Tensor, SparseTensorRef, Tensor, Scalar, Scalar)  [broadcasts]\n+// --------------------------------------------------------------------\n+\n+Tensor& s_addmm_out_sparse_dense_cuda(Tensor& r_, const Tensor& t, const SparseTensor& sparse_, const Tensor& dense, Scalar beta, Scalar alpha) {\n+#ifndef __HIP_PLATFORM_HCC__\n+  AT_CHECK(_check_device({sparse_, r_, t, dense}));\n+  // THCudaIntTensor *csr;\n+  // THCIndexTensor *indices;\n+  // THCTensor *values, *r__, *dense_;\n+\n+  // TODO: This error message seems awfully opaque\n+  AT_CHECK(sparse_._sparseDims() == 2, \"matrices expected, got \", sparse_._sparseDims(), \"D tensor\");\n+  AT_CHECK(sparse_._denseDims() == 0, \"scalar values expected, got \", sparse_._denseDims(), \"D values\");\n+  AT_CHECK(dense.dim() == 2, \"matrices expected, got \", dense.dim(), \"D tensor\");", "path": "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "position": 54, "original_position": 54, "commit_id": "1a3c38e577f84307b7b6f7c8e49818fde11f343f", "original_commit_id": "c7d4a43fe1a6c2cf74d918f277b1fc341a196cf3", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "deferring, since empty matrices should actually multiply.", "created_at": "2018-06-22T20:29:30Z", "updated_at": "2018-11-23T15:46:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/8689#discussion_r197561004", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8689", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197561004"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8689#discussion_r197561004"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8689"}}, "body_html": "<p>deferring, since empty matrices should actually multiply.</p>", "body_text": "deferring, since empty matrices should actually multiply.", "in_reply_to_id": 197496810}