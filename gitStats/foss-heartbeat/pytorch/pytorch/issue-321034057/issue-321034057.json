{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7366", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7366/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7366/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7366/events", "html_url": "https://github.com/pytorch/pytorch/pull/7366", "id": 321034057, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg2NTIwNDgw", "number": 7366, "title": "Removes -2 special case and specialization from pointwise apply", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-08T03:55:31Z", "updated_at": "2018-11-23T15:43:42Z", "closed_at": "2018-05-08T18:58:47Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7366", "html_url": "https://github.com/pytorch/pytorch/pull/7366", "diff_url": "https://github.com/pytorch/pytorch/pull/7366.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7366.patch"}, "body_html": "<p>Currently in pointwise apply there are four special cases:</p>\n<p>-2, a 1D tensor with stride == 1<br>\n-1, a nD tensor<br>\n1, a 1D tensor with stride != 1<br>\n2, a 2D (noncontiguous) tensor</p>\n<p>There are also template specializations for the -2 special case in THC's OffsetInfo and and ATen's IndexToOffset.</p>\n<p>The practical difference, however, between the -2 and 1 code paths is only a multiplication. The -2 code path simply returns its input (implicitly * 1) while the 1 code path returns input * stride. Since the pointwise apply kernels are expected to be bandwidth bound, performing this additional multiplication likely has no performance impact. To demonstrate this empirically I reviewed the THC pointwise apply 1 and 2 kernels on 1D tensors of 3 billion, 1 billion, half a billion, and 1 million floats. Operations were performed on these tensors at strides of 1, 2, and 4. There was no significant performance difference between Master and this PR, with maximum variation of only .3% between the two, well within the observed noise of the experiments I was running (avg of 10 reps for the tensors of 1 billion or greater size, and 100 reps for the smaller tensors).</p>\n<p>While there is (almost certainly) no performance change from this fix, it does reduce build time and simplify the existing code. Pointwise apply makes note of its sensitivity to excess template instantiations, after all.</p>\n<p>Note that, for readability, this PR currently leaves the dims = 1 instantiations of OffsetInfo and IndexToOffset. Without this the code path would be the same, although the reader would have to review the code somewhat carefully to determine this. Given the importance of 1D tensors it seemed appropriate to leave these specializations as a guide to readers.</p>", "body_text": "Currently in pointwise apply there are four special cases:\n-2, a 1D tensor with stride == 1\n-1, a nD tensor\n1, a 1D tensor with stride != 1\n2, a 2D (noncontiguous) tensor\nThere are also template specializations for the -2 special case in THC's OffsetInfo and and ATen's IndexToOffset.\nThe practical difference, however, between the -2 and 1 code paths is only a multiplication. The -2 code path simply returns its input (implicitly * 1) while the 1 code path returns input * stride. Since the pointwise apply kernels are expected to be bandwidth bound, performing this additional multiplication likely has no performance impact. To demonstrate this empirically I reviewed the THC pointwise apply 1 and 2 kernels on 1D tensors of 3 billion, 1 billion, half a billion, and 1 million floats. Operations were performed on these tensors at strides of 1, 2, and 4. There was no significant performance difference between Master and this PR, with maximum variation of only .3% between the two, well within the observed noise of the experiments I was running (avg of 10 reps for the tensors of 1 billion or greater size, and 100 reps for the smaller tensors).\nWhile there is (almost certainly) no performance change from this fix, it does reduce build time and simplify the existing code. Pointwise apply makes note of its sensitivity to excess template instantiations, after all.\nNote that, for readability, this PR currently leaves the dims = 1 instantiations of OffsetInfo and IndexToOffset. Without this the code path would be the same, although the reader would have to review the code somewhat carefully to determine this. Given the importance of 1D tensors it seemed appropriate to leave these specializations as a guide to readers.", "body": "Currently in pointwise apply there are four special cases:\r\n\r\n-2, a 1D tensor with stride == 1\r\n-1, a nD tensor\r\n1, a 1D tensor with stride != 1\r\n2, a 2D (noncontiguous) tensor\r\n\r\nThere are also template specializations for the -2 special case in THC's OffsetInfo and and ATen's IndexToOffset. \r\n\r\nThe practical difference, however, between the -2 and 1 code paths is only a multiplication. The -2 code path simply returns its input (implicitly * 1) while the 1 code path returns input * stride. Since the pointwise apply kernels are expected to be bandwidth bound, performing this additional multiplication likely has no performance impact. To demonstrate this empirically I reviewed the THC pointwise apply 1 and 2 kernels on 1D tensors of 3 billion, 1 billion, half a billion, and 1 million floats. Operations were performed on these tensors at strides of 1, 2, and 4. There was no significant performance difference between Master and this PR, with maximum variation of only .3% between the two, well within the observed noise of the experiments I was running (avg of 10 reps for the tensors of 1 billion or greater size, and 100 reps for the smaller tensors).\r\n\r\nWhile there is (almost certainly) no performance change from this fix, it does reduce build time and simplify the existing code. Pointwise apply makes note of its sensitivity to excess template instantiations, after all.  \r\n\r\nNote that, for readability, this PR currently leaves the dims = 1 instantiations of OffsetInfo and IndexToOffset. Without this the code path would be the same, although the reader would have to review the code somewhat carefully to determine this. Given the importance of 1D tensors it seemed appropriate to leave these specializations as a guide to readers.\r\n"}