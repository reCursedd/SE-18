{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380962894", "html_url": "https://github.com/pytorch/pytorch/issues/6570#issuecomment-380962894", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6570", "id": 380962894, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDk2Mjg5NA==", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T22:25:11Z", "updated_at": "2018-04-12T22:25:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I would not say that this is possible at the moment. Your best bet is to use <a href=\"https://github.com/ebetica/autogradpp/\">autogradpp</a>, our unofficial C++ API, to rewrite your model in C++, and then use JNI to wrap it.</p>\n<p>We are actively working on an official C++ API <a href=\"https://github.com/pytorch/pytorch/pull/6345\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6345/hovercard\">here</a> and we'll make sure to allow deserialization of models written in Python. But this is still a little far away at the moment. You'll have to use JNI either way.</p>\n<p>You may also be able to export your model to Caffe2 with ONNX, and then wrap the inference code (maybe 30 lines of C++).</p>\n<p>Of course, you could also put your Python model behind a REST server somewhere and make calls to it, if feasible.</p>", "body_text": "I would not say that this is possible at the moment. Your best bet is to use autogradpp, our unofficial C++ API, to rewrite your model in C++, and then use JNI to wrap it.\nWe are actively working on an official C++ API here and we'll make sure to allow deserialization of models written in Python. But this is still a little far away at the moment. You'll have to use JNI either way.\nYou may also be able to export your model to Caffe2 with ONNX, and then wrap the inference code (maybe 30 lines of C++).\nOf course, you could also put your Python model behind a REST server somewhere and make calls to it, if feasible.", "body": "I would not say that this is possible at the moment. Your best bet is to use [autogradpp](https://github.com/ebetica/autogradpp/), our unofficial C++ API, to rewrite your model in C++, and then use JNI to wrap it.\r\n\r\nWe are actively working on an official C++ API [here](https://github.com/pytorch/pytorch/pull/6345) and we'll make sure to allow deserialization of models written in Python. But this is still a little far away at the moment. You'll have to use JNI either way.\r\n\r\nYou may also be able to export your model to Caffe2 with ONNX, and then wrap the inference code (maybe 30 lines of C++).\r\n\r\nOf course, you could also put your Python model behind a REST server somewhere and make calls to it, if feasible."}