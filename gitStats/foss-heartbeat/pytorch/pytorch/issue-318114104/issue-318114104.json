{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7001", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7001/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7001/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7001/events", "html_url": "https://github.com/pytorch/pytorch/issues/7001", "id": 318114104, "node_id": "MDU6SXNzdWUzMTgxMTQxMDQ=", "number": 7001, "title": "Destructing objects holding CUDA resources", "user": {"login": "singam-sanjay", "id": 6310523, "node_id": "MDQ6VXNlcjYzMTA1MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6310523?v=4", "gravatar_id": "", "url": "https://api.github.com/users/singam-sanjay", "html_url": "https://github.com/singam-sanjay", "followers_url": "https://api.github.com/users/singam-sanjay/followers", "following_url": "https://api.github.com/users/singam-sanjay/following{/other_user}", "gists_url": "https://api.github.com/users/singam-sanjay/gists{/gist_id}", "starred_url": "https://api.github.com/users/singam-sanjay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/singam-sanjay/subscriptions", "organizations_url": "https://api.github.com/users/singam-sanjay/orgs", "repos_url": "https://api.github.com/users/singam-sanjay/repos", "events_url": "https://api.github.com/users/singam-sanjay/events{/privacy}", "received_events_url": "https://api.github.com/users/singam-sanjay/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-26T16:50:51Z", "updated_at": "2018-04-27T16:20:38Z", "closed_at": "2018-04-27T16:20:37Z", "author_association": "NONE", "body_html": "<p>I need to maintain multiple instances of the FusionCompiler class and have created an <code>unordered_map&lt;size_t,shared_ptr&lt;FusionCompiler&gt;&gt;</code> to accomplish the same.</p>\n<p>On calling <code>exit()</code> from Python, the destructor of the <code>FusionCompiler</code> object is invoked and that in turn invokes the destructor of the <code>CompiledFunction</code>s in the compile cache.<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/2b44c420c8e61501ef04a5e4af2347ecc3c86202/torch/csrc/jit/fusion_compiler.h#L151\">pytorch/torch/csrc/jit/fusion_compiler.h</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 151\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/2b44c420c8e61501ef04a5e4af2347ecc3c86202\">2b44c42</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L151\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"151\"></td>\n          <td id=\"LC151\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> std::unordered_map&lt;std::string, std::shared_ptr&lt;CompiledFusionFunction&gt;&gt; cache; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nThe destructor attempts to unload the compiled module.<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/2b44c420c8e61501ef04a5e4af2347ecc3c86202/torch/csrc/jit/fusion_compiler.cpp#L535\">pytorch/torch/csrc/jit/fusion_compiler.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 535\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/2b44c420c8e61501ef04a5e4af2347ecc3c86202\">2b44c42</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L535\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"535\"></td>\n          <td id=\"LC535\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">TORCH_CU_CHECK</span>(<span class=\"pl-c1\">cuModuleUnload</span>(module)); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nIn my case, the destructor fails due to the absence of the CUDA runtime,</p>\n<pre><code>#8  0x00007fffeca4b451 in torch::cuCheck (result=CUDA_ERROR_DEINITIALIZED, ...\n#9  0x00007fffeca4ee7f in torch::jit::compiler::CompiledFusionFunction::~CompiledFusionFunction (this=0x55559ac5b720, ...\n</code></pre>\n<p>I recreated this behaviour by having an object hold on to <code>cudaEvent</code>s and <code>cudaEventDestroy</code>ing them in the destructor. Other people have faced <a href=\"https://devtalk.nvidia.com/default/topic/1019780/cuda-run-time-library-unload/\" rel=\"nofollow\">similar issues and NVIDIA seems to discourage the practice</a> of deallocating resources in the destructors.</p>\n<p>How did PyTorch get around this issue ?</p>", "body_text": "I need to maintain multiple instances of the FusionCompiler class and have created an unordered_map<size_t,shared_ptr<FusionCompiler>> to accomplish the same.\nOn calling exit() from Python, the destructor of the FusionCompiler object is invoked and that in turn invokes the destructor of the CompiledFunctions in the compile cache.\n\n  \n    \n      pytorch/torch/csrc/jit/fusion_compiler.h\n    \n    \n         Line 151\n      in\n      2b44c42\n    \n    \n    \n    \n\n        \n          \n           std::unordered_map<std::string, std::shared_ptr<CompiledFusionFunction>> cache; \n        \n    \n  \n\n\nThe destructor attempts to unload the compiled module.\n\n  \n    \n      pytorch/torch/csrc/jit/fusion_compiler.cpp\n    \n    \n         Line 535\n      in\n      2b44c42\n    \n    \n    \n    \n\n        \n          \n           TORCH_CU_CHECK(cuModuleUnload(module)); \n        \n    \n  \n\n\nIn my case, the destructor fails due to the absence of the CUDA runtime,\n#8  0x00007fffeca4b451 in torch::cuCheck (result=CUDA_ERROR_DEINITIALIZED, ...\n#9  0x00007fffeca4ee7f in torch::jit::compiler::CompiledFusionFunction::~CompiledFusionFunction (this=0x55559ac5b720, ...\n\nI recreated this behaviour by having an object hold on to cudaEvents and cudaEventDestroying them in the destructor. Other people have faced similar issues and NVIDIA seems to discourage the practice of deallocating resources in the destructors.\nHow did PyTorch get around this issue ?", "body": "I need to maintain multiple instances of the FusionCompiler class and have created an `unordered_map<size_t,shared_ptr<FusionCompiler>>` to accomplish the same.\r\n\r\nOn calling `exit()` from Python, the destructor of the `FusionCompiler` object is invoked and that in turn invokes the destructor of the `CompiledFunction`s in the compile cache.\r\nhttps://github.com/pytorch/pytorch/blob/2b44c420c8e61501ef04a5e4af2347ecc3c86202/torch/csrc/jit/fusion_compiler.h#L151\r\nThe destructor attempts to unload the compiled module.\r\nhttps://github.com/pytorch/pytorch/blob/2b44c420c8e61501ef04a5e4af2347ecc3c86202/torch/csrc/jit/fusion_compiler.cpp#L535\r\nIn my case, the destructor fails due to the absence of the CUDA runtime,\r\n```\r\n#8  0x00007fffeca4b451 in torch::cuCheck (result=CUDA_ERROR_DEINITIALIZED, ...\r\n#9  0x00007fffeca4ee7f in torch::jit::compiler::CompiledFusionFunction::~CompiledFusionFunction (this=0x55559ac5b720, ...\r\n```\r\n\r\nI recreated this behaviour by having an object hold on to `cudaEvent`s and `cudaEventDestroy`ing them in the destructor. Other people have faced [similar issues and NVIDIA seems to discourage the practice](https://devtalk.nvidia.com/default/topic/1019780/cuda-run-time-library-unload/) of deallocating resources in the destructors.\r\n\r\nHow did PyTorch get around this issue ?"}