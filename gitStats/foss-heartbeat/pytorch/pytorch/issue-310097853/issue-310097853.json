{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6135", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6135/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6135/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6135/events", "html_url": "https://github.com/pytorch/pytorch/issues/6135", "id": 310097853, "node_id": "MDU6SXNzdWUzMTAwOTc4NTM=", "number": 6135, "title": "Test suite should test implementations", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-03-30T15:43:29Z", "updated_at": "2018-03-30T15:43:29Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>This issue was prompted by <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"310083839\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6132\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6132/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/6132\">#6132</a>. To explain the situation, let us consider the situation of convolution.</p>\n<p>Today, we have a number of tests for convolution. The majority of these tests go through the \"official\" convolution interface, which is what all of our users use. However, inside the implementation of convolution, there are a large number of if-statements which specify which particular implementation we use, depending on some settings. This too, is also a good idea, because we want our users to get the best kernels without thinking about it.</p>\n<p>However, when it comes time to test these kernels, we now have some very ugly code:</p>\n<pre><code>                # test that forwards of module runs correctly without cuDNN\n                if self.cudnn:\n                    with torch.backends.cudnn.flags(enabled=False):\n                        module(input)\n                        for p in module.parameters():\n                            test_case.assertIsInstance(p, torch.cuda.FloatTensor)\n                            test_case.assertEqual(p.get_device(), 0)\n</code></pre>\n<p>Essentially, we need to <em>indirectly</em> force the dispatch code one way or another, to make sure we test all of the actual kernel implementations. This makes it very easy to miss test configurations where you (a) forget to toggle between implementations or (b) the toggling code is wrong and you're not exercising a codepath that you actually wanted to test.</p>\n<p>The basic situation is that there are two orthogonal things we want to test:</p>\n<ol>\n<li>An integration test: we want to test that the actual function all of our users use for convolution actually works.</li>\n<li>An implementation test: we want to test that each of our individual kernels is correct.</li>\n</ol>\n<p>The current test suite conflates the two. It would be good if it did not.</p>", "body_text": "This issue was prompted by #6132. To explain the situation, let us consider the situation of convolution.\nToday, we have a number of tests for convolution. The majority of these tests go through the \"official\" convolution interface, which is what all of our users use. However, inside the implementation of convolution, there are a large number of if-statements which specify which particular implementation we use, depending on some settings. This too, is also a good idea, because we want our users to get the best kernels without thinking about it.\nHowever, when it comes time to test these kernels, we now have some very ugly code:\n                # test that forwards of module runs correctly without cuDNN\n                if self.cudnn:\n                    with torch.backends.cudnn.flags(enabled=False):\n                        module(input)\n                        for p in module.parameters():\n                            test_case.assertIsInstance(p, torch.cuda.FloatTensor)\n                            test_case.assertEqual(p.get_device(), 0)\n\nEssentially, we need to indirectly force the dispatch code one way or another, to make sure we test all of the actual kernel implementations. This makes it very easy to miss test configurations where you (a) forget to toggle between implementations or (b) the toggling code is wrong and you're not exercising a codepath that you actually wanted to test.\nThe basic situation is that there are two orthogonal things we want to test:\n\nAn integration test: we want to test that the actual function all of our users use for convolution actually works.\nAn implementation test: we want to test that each of our individual kernels is correct.\n\nThe current test suite conflates the two. It would be good if it did not.", "body": "This issue was prompted by #6132. To explain the situation, let us consider the situation of convolution.\r\n\r\nToday, we have a number of tests for convolution. The majority of these tests go through the \"official\" convolution interface, which is what all of our users use. However, inside the implementation of convolution, there are a large number of if-statements which specify which particular implementation we use, depending on some settings. This too, is also a good idea, because we want our users to get the best kernels without thinking about it.\r\n\r\nHowever, when it comes time to test these kernels, we now have some very ugly code:\r\n\r\n```\r\n                # test that forwards of module runs correctly without cuDNN\r\n                if self.cudnn:\r\n                    with torch.backends.cudnn.flags(enabled=False):\r\n                        module(input)\r\n                        for p in module.parameters():\r\n                            test_case.assertIsInstance(p, torch.cuda.FloatTensor)\r\n                            test_case.assertEqual(p.get_device(), 0)\r\n```\r\n\r\nEssentially, we need to *indirectly* force the dispatch code one way or another, to make sure we test all of the actual kernel implementations. This makes it very easy to miss test configurations where you (a) forget to toggle between implementations or (b) the toggling code is wrong and you're not exercising a codepath that you actually wanted to test.\r\n\r\nThe basic situation is that there are two orthogonal things we want to test:\r\n\r\n1. An integration test: we want to test that the actual function all of our users use for convolution actually works.\r\n2. An implementation test: we want to test that each of our individual kernels is correct.\r\n\r\nThe current test suite conflates the two. It would be good if it did not."}