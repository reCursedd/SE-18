{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5870", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5870/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5870/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5870/events", "html_url": "https://github.com/pytorch/pytorch/issues/5870", "id": 306400692, "node_id": "MDU6SXNzdWUzMDY0MDA2OTI=", "number": 5870, "title": "[BUG?]The same code behaves differently on pytorch based on py2 and py3 whose version are both 0.3.0.post_4", "user": {"login": "sonack", "id": 12935189, "node_id": "MDQ6VXNlcjEyOTM1MTg5", "avatar_url": "https://avatars1.githubusercontent.com/u/12935189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sonack", "html_url": "https://github.com/sonack", "followers_url": "https://api.github.com/users/sonack/followers", "following_url": "https://api.github.com/users/sonack/following{/other_user}", "gists_url": "https://api.github.com/users/sonack/gists{/gist_id}", "starred_url": "https://api.github.com/users/sonack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sonack/subscriptions", "organizations_url": "https://api.github.com/users/sonack/orgs", "repos_url": "https://api.github.com/users/sonack/repos", "events_url": "https://api.github.com/users/sonack/events{/privacy}", "received_events_url": "https://api.github.com/users/sonack/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-03-19T10:12:48Z", "updated_at": "2018-03-20T04:14:57Z", "closed_at": "2018-03-19T13:11:43Z", "author_association": "NONE", "body_html": "<p>I have tried to train an autoencoder-like network with pytorch based on py3, its loss decreased normally on my local machine. However, when I moved the code to my training server whose pytorch is based on py2 , I found the loss was never converge. I debugged it for a long time, and suspected it's because of the framework itself finally, because when I changed the server's environment to pytorch based on py3, the bug disappeared and behaved the same as my local machine, the loss could be declined quickly.</p>\n<p>All the training data and code are the same, following is my network's definition and training code.</p>\n<p>In summary, I found the bug appear's version is <strong>0.3.0.post4 of python2</strong>.<br>\nI found the bug disappear's version is <strong>0.3.0.post4 of python3(my local machine)</strong>, and <strong>'0.3.1.post2 of python3(my server's new environment)</strong>.</p>\n<p><strong>trainning code</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>coding:utf8</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">from</span> config <span class=\"pl-k\">import</span> opt\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> math\n<span class=\"pl-k\">import</span> torch <span class=\"pl-k\">as</span> t\n<span class=\"pl-k\">import</span> models\n\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torchnet <span class=\"pl-k\">import</span> meter\n<span class=\"pl-k\">from</span> utils <span class=\"pl-k\">import</span> AverageValueMeter\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> transforms, datasets\n<span class=\"pl-k\">import</span> pdb\n\n<span class=\"pl-k\">from</span> utils.visualize <span class=\"pl-k\">import</span> Visualizer, PlotSaver\n<span class=\"pl-k\">from</span> extend <span class=\"pl-k\">import</span> RateLoss, LimuRateLoss\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\n<span class=\"pl-k\">import</span> time\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n    opt.parse(kwargs)\n    ps <span class=\"pl-k\">=</span> PlotSaver(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TrainImageNet12WithoutImpMap_<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">+</span>time.strftime(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>%m_<span class=\"pl-c1\">%d</span>_%H:%M:%S<span class=\"pl-pds\">\"</span></span>)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.log<span class=\"pl-pds\">\"</span></span>)\n\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> step1: Model</span>\n    model <span class=\"pl-k\">=</span> <span class=\"pl-c1\">getattr</span>(models, opt.model)(<span class=\"pl-v\">use_imp</span> <span class=\"pl-k\">=</span> opt.use_imp, <span class=\"pl-v\">model_name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CWCNN_limu_ImageNet_imp_r=<span class=\"pl-c1\">{r}</span>_\u03b3=<span class=\"pl-c1\">{w}</span><span class=\"pl-pds\">\"</span></span>.format(\n                                                                <span class=\"pl-v\">r</span><span class=\"pl-k\">=</span>opt.rate_loss_threshold,\n                                                                <span class=\"pl-v\">w</span><span class=\"pl-k\">=</span>opt.rate_loss_weight)\n                                                                <span class=\"pl-k\">if</span> opt.use_imp <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>)\n\n    <span class=\"pl-k\">if</span> opt.use_gpu:\n        model.cuda()\n\n   \n    normalize <span class=\"pl-k\">=</span> transforms.Normalize(\n                <span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>],\n                <span class=\"pl-v\">std</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>]\n                )\n\n    train_data_transforms <span class=\"pl-k\">=</span> transforms.Compose(\n        [\n            transforms.ToTensor(),\n            normalize\n        ]\n    )\n    val_data_transforms <span class=\"pl-k\">=</span> transforms.Compose(\n        [\n            transforms.ToTensor(),\n            normalize\n        ]\n    )\n\n    train_data <span class=\"pl-k\">=</span> datasets.ImageFolder(\n        opt.train_data_root,\n        train_data_transforms\n    )\n    val_data <span class=\"pl-k\">=</span> datasets.ImageFolder(\n        opt.val_data_root,\n        val_data_transforms\n    )\n \n    train_dataloader <span class=\"pl-k\">=</span> DataLoader(train_data, opt.batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span>opt.num_workers)\n    val_dataloader <span class=\"pl-k\">=</span> DataLoader(val_data, opt.batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span>opt.num_workers)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> step3: criterion and optimizer</span>\n\n    mse_loss <span class=\"pl-k\">=</span> t.nn.MSELoss(<span class=\"pl-v\">size_average</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\n\n    lr <span class=\"pl-k\">=</span> opt.lr\n\n    optimizer <span class=\"pl-k\">=</span> t.optim.Adam(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>lr, <span class=\"pl-v\">betas</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">0.9</span>, <span class=\"pl-c1\">0.999</span>))\n\n    start_epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> step4: meters</span>\n    mse_loss_meter <span class=\"pl-k\">=</span> AverageValueMeter()\n  \n    previous_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e100</span>\n    tolerant_now <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    same_lr_epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(start_epoch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, opt.max_epoch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>):\n        same_lr_epoch <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n  \n        mse_loss_meter.reset()\n        total_loss_meter <span class=\"pl-k\">=</span> mse_loss_meter\n        model.train()\n\n        <span class=\"pl-k\">for</span> idx, (data, _) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_dataloader):\n            ipt <span class=\"pl-k\">=</span> Variable(data)\n\n            <span class=\"pl-k\">if</span> opt.use_gpu:\n                ipt <span class=\"pl-k\">=</span> ipt.cuda()\n\n            optimizer.zero_grad()  \n            reconstructed <span class=\"pl-k\">=</span> model(ipt)\n\n            loss <span class=\"pl-k\">=</span> mse_loss(reconstructed, ipt)\n            caffe_loss <span class=\"pl-k\">=</span> loss <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> opt.batch_size)\n\n            total_loss <span class=\"pl-k\">=</span> caffe_loss\n\n            total_loss.backward()\n            optimizer.step()\n\n            mse_loss_meter.add(caffe_loss.data[<span class=\"pl-c1\">0</span>])\n\n            <span class=\"pl-k\">if</span> idx <span class=\"pl-k\">%</span> opt.print_freq <span class=\"pl-k\">==</span> opt.print_freq <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>:\n                <span class=\"pl-k\">pass</span>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> print sth. I omitted it for clarity</span>\n\n       \n        model.save(optimizer, epoch)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> valdation </span>\n        val()\n\n        <span class=\"pl-k\">if</span> opt.use_early_adjust:\n            <span class=\"pl-k\">if</span> total_loss_meter.value()[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">&gt;</span> previous_loss:\n                tolerant_now <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n                <span class=\"pl-k\">if</span> tolerant_now <span class=\"pl-k\">==</span> opt.tolerant_max:\n                    tolerant_now <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n                    same_lr_epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n                    lr <span class=\"pl-k\">=</span> lr <span class=\"pl-k\">*</span> opt.lr_decay\n                    <span class=\"pl-k\">for</span> param_group <span class=\"pl-k\">in</span> optimizer.param_groups:\n                        param_group[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>lr<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> lr\n                    <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Due to early stop anneal lr to<span class=\"pl-pds\">'</span></span>,lr,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>at epoch<span class=\"pl-pds\">'</span></span>,epoch)\n            <span class=\"pl-k\">else</span>:\n                tolerant_now <span class=\"pl-k\">-=</span> <span class=\"pl-c1\">1</span>\n\n        <span class=\"pl-k\">if</span> same_lr_epoch <span class=\"pl-k\">and</span> same_lr_epoch <span class=\"pl-k\">%</span> opt.lr_anneal_epochs <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            same_lr_epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            tolerant_now <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            lr <span class=\"pl-k\">=</span> lr <span class=\"pl-k\">*</span> opt.lr_decay\n            <span class=\"pl-k\">for</span> param_group <span class=\"pl-k\">in</span> optimizer.param_groups:\n                param_group[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>lr<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> lr\n            <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Due to full epochs anneal lr to<span class=\"pl-pds\">'</span></span>,lr,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>at epoch<span class=\"pl-pds\">'</span></span>,epoch)\n\n        previous_loss <span class=\"pl-k\">=</span> total_loss_meter.value()[<span class=\"pl-c1\">0</span>]\n\n</pre></div>\n<p><strong>Model definition</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>coding:utf-8</span>\n<span class=\"pl-k\">from</span> .BasicModule <span class=\"pl-k\">import</span> BasicModule\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> math\n\n\n<span class=\"pl-k\">from</span> extend <span class=\"pl-k\">import</span> Round\n<span class=\"pl-k\">from</span> torch.nn.init <span class=\"pl-k\">import</span> xavier_uniform\n\n<span class=\"pl-k\">import</span> pdb\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">ResidualBlock</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    Residual Module</span>\n<span class=\"pl-s\">        input(nchw) -&gt; Conv(3x3x128, pad 1) -&gt; ReLu -&gt; Conv(3x3xc, pad 1) -&gt; ReLU</span>\n<span class=\"pl-s\">        input(nchw)                                                                +  = output(nchw)</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">ch_in</span>, <span class=\"pl-smi\">ch_out</span>, <span class=\"pl-smi\">shortcut</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">super</span>(ResidualBlock, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.left <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Conv2d(ch_in, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ch_in, ch_out, kernel_size, stride, pad</span>\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n            nn.Conv2d(<span class=\"pl-c1\">128</span>, ch_out, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n        )\n        <span class=\"pl-c1\">self</span>.right <span class=\"pl-k\">=</span> shortcut\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.left(x)\n        residual <span class=\"pl-k\">=</span> x <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.right <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>.right(x)\n        out <span class=\"pl-k\">+=</span> residual\n        <span class=\"pl-k\">return</span> F.relu(out, <span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">xavier_init</span>(<span class=\"pl-smi\">data</span>):\n    fan_in <span class=\"pl-k\">=</span> data.numel() <span class=\"pl-k\">/</span> data.size(<span class=\"pl-c1\">0</span>)\n    scale <span class=\"pl-k\">=</span> math.sqrt(<span class=\"pl-c1\">3</span> <span class=\"pl-k\">/</span> fan_in)\n    data.uniform_(<span class=\"pl-k\">-</span>scale, scale)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">weights_initialization</span>(<span class=\"pl-smi\">m</span>):\n    classname <span class=\"pl-k\">=</span> m.<span class=\"pl-c1\">__class__</span>.<span class=\"pl-c1\">__name__</span>\n    <span class=\"pl-k\">if</span> classname.find(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Conv<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">!=</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>:\n        xavier_init(m.weight.data)\n        m.bias.data.fill_(<span class=\"pl-c1\">0</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> </span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">ContentWeightedCNN</span>(<span class=\"pl-e\">BasicModule</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    BasicModule is just a Module with save() and load() convenient method, I don't think it matters, so I ommited the relevant files.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">use_imp</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">model_name</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">super</span>(ContentWeightedCNN, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.model_name <span class=\"pl-k\">=</span> model_name <span class=\"pl-k\">if</span> model_name <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ContentWeightedCNN<span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-c1\">self</span>.encoder <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.make_encoder()\n        <span class=\"pl-c1\">self</span>.decoder <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.make_decoder()\n        <span class=\"pl-c1\">self</span>.reset_parameters()\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">reset_parameters</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>.apply(weights_initialization)\n    \n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        mgdata <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.encoder(x)\n        enc_data <span class=\"pl-k\">=</span> mgdata\n        dec_data <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.decoder(enc_data)\n        <span class=\"pl-k\">return</span> dec_data\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">make_encoder</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        layers <span class=\"pl-k\">=</span> [\n            nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>),\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-c\"><span class=\"pl-c\">#</span> 54</span>\n\n            ResidualBlock(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>),\n\n            nn.Conv2d(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-c\"><span class=\"pl-c\">#</span> 115</span>\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n\n            ResidualBlock(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>),\n\n            nn.Conv2d(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-c\"><span class=\"pl-c\">#</span>192</span>\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n\n            ResidualBlock(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>),\n\n            nn.Conv2d(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>),   \n            nn.Sigmoid(),                    \n        ]\n        <span class=\"pl-k\">return</span> nn.Sequential(<span class=\"pl-k\">*</span>layers)\n\n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">make_decoder</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        layers <span class=\"pl-k\">=</span> [\n            nn.Conv2d(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n\n            ResidualBlock(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">512</span>),\n\n            nn.Conv2d(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n\n            ResidualBlock(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">512</span>),\n\n            nn.PixelShuffle(<span class=\"pl-c1\">2</span>),\n\n            nn.Conv2d(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n\n            ResidualBlock(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>),\n\n            nn.PixelShuffle(<span class=\"pl-c1\">4</span>),\n\n            nn.Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n            \n            nn.Conv2d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>)   \n        ]\n        <span class=\"pl-k\">return</span> nn.Sequential(<span class=\"pl-k\">*</span>layers)    </pre></div>\n<p>For clarity, I omitted some code.<br>\nIf you need more relevant code to understand or reproduce, comment to tell me.</p>\n<p>I know it is a little bit long, but I don't know where is the matter. Thanks for your attention.</p>", "body_text": "I have tried to train an autoencoder-like network with pytorch based on py3, its loss decreased normally on my local machine. However, when I moved the code to my training server whose pytorch is based on py2 , I found the loss was never converge. I debugged it for a long time, and suspected it's because of the framework itself finally, because when I changed the server's environment to pytorch based on py3, the bug disappeared and behaved the same as my local machine, the loss could be declined quickly.\nAll the training data and code are the same, following is my network's definition and training code.\nIn summary, I found the bug appear's version is 0.3.0.post4 of python2.\nI found the bug disappear's version is 0.3.0.post4 of python3(my local machine), and '0.3.1.post2 of python3(my server's new environment).\ntrainning code\n#coding:utf8\nfrom __future__ import print_function\nfrom config import opt\nimport os\nimport numpy as np\nimport math\nimport torch as t\nimport models\n\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torchnet import meter\nfrom utils import AverageValueMeter\nfrom torchvision import transforms, datasets\nimport pdb\n\nfrom utils.visualize import Visualizer, PlotSaver\nfrom extend import RateLoss, LimuRateLoss\nimport torch.backends.cudnn as cudnn\nimport time\n\n\ndef train(**kwargs):\n    opt.parse(kwargs)\n    ps = PlotSaver(\"TrainImageNet12WithoutImpMap_\"+time.strftime(\"%m_%d_%H:%M:%S\")+\".log\")\n\n\n    # step1: Model\n    model = getattr(models, opt.model)(use_imp = opt.use_imp, model_name=\"CWCNN_limu_ImageNet_imp_r={r}_\u03b3={w}\".format(\n                                                                r=opt.rate_loss_threshold,\n                                                                w=opt.rate_loss_weight)\n                                                                if opt.use_imp else None)\n\n    if opt.use_gpu:\n        model.cuda()\n\n   \n    normalize = transforms.Normalize(\n                mean=[0.5, 0.5, 0.5],\n                std=[0.5, 0.5, 0.5]\n                )\n\n    train_data_transforms = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            normalize\n        ]\n    )\n    val_data_transforms = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            normalize\n        ]\n    )\n\n    train_data = datasets.ImageFolder(\n        opt.train_data_root,\n        train_data_transforms\n    )\n    val_data = datasets.ImageFolder(\n        opt.val_data_root,\n        val_data_transforms\n    )\n \n    train_dataloader = DataLoader(train_data, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n    val_dataloader = DataLoader(val_data, opt.batch_size, shuffle=False, num_workers=opt.num_workers)\n\n    # step3: criterion and optimizer\n\n    mse_loss = t.nn.MSELoss(size_average = False)\n\n    lr = opt.lr\n\n    optimizer = t.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n\n    start_epoch = 0\n\n    # step4: meters\n    mse_loss_meter = AverageValueMeter()\n  \n    previous_loss = 1e100\n    tolerant_now = 0\n    same_lr_epoch = 0\n\n\n    for epoch in range(start_epoch+1, opt.max_epoch+1):\n        same_lr_epoch += 1\n  \n        mse_loss_meter.reset()\n        total_loss_meter = mse_loss_meter\n        model.train()\n\n        for idx, (data, _) in enumerate(train_dataloader):\n            ipt = Variable(data)\n\n            if opt.use_gpu:\n                ipt = ipt.cuda()\n\n            optimizer.zero_grad()  \n            reconstructed = model(ipt)\n\n            loss = mse_loss(reconstructed, ipt)\n            caffe_loss = loss / (2 * opt.batch_size)\n\n            total_loss = caffe_loss\n\n            total_loss.backward()\n            optimizer.step()\n\n            mse_loss_meter.add(caffe_loss.data[0])\n\n            if idx % opt.print_freq == opt.print_freq - 1:\n                pass    # print sth. I omitted it for clarity\n\n       \n        model.save(optimizer, epoch)\n\n        # valdation \n        val()\n\n        if opt.use_early_adjust:\n            if total_loss_meter.value()[0] > previous_loss:\n                tolerant_now += 1\n                if tolerant_now == opt.tolerant_max:\n                    tolerant_now = 0\n                    same_lr_epoch = 0\n                    lr = lr * opt.lr_decay\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = lr\n                    print ('Due to early stop anneal lr to',lr,'at epoch',epoch)\n            else:\n                tolerant_now -= 1\n\n        if same_lr_epoch and same_lr_epoch % opt.lr_anneal_epochs == 0:\n            same_lr_epoch = 0\n            tolerant_now = 0\n            lr = lr * opt.lr_decay\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n            print ('Due to full epochs anneal lr to',lr,'at epoch',epoch)\n\n        previous_loss = total_loss_meter.value()[0]\n\n\nModel definition\n#coding:utf-8\nfrom .BasicModule import BasicModule\nfrom torch import nn\nfrom torch.nn import functional as F\nimport sys\nimport math\n\n\nfrom extend import Round\nfrom torch.nn.init import xavier_uniform\n\nimport pdb\n\nclass ResidualBlock(nn.Module):\n    '''\n    Residual Module\n        input(nchw) -> Conv(3x3x128, pad 1) -> ReLu -> Conv(3x3xc, pad 1) -> ReLU\n        input(nchw)                                                                +  = output(nchw)\n    '''\n    def __init__(self, ch_in, ch_out, shortcut = None):\n        super(ResidualBlock, self).__init__()\n        self.left = nn.Sequential(\n            nn.Conv2d(ch_in, 128, 3, 1, 1),  # ch_in, ch_out, kernel_size, stride, pad\n            nn.ReLU(inplace=False),\n            nn.Conv2d(128, ch_out, 3, 1, 1),\n            nn.ReLU(inplace=False),\n        )\n        self.right = shortcut\n    \n    def forward(self, x):\n        out = self.left(x)\n        residual = x if self.right is None else self.right(x)\n        out += residual\n        return F.relu(out, inplace=False)\n\n\ndef xavier_init(data):\n    fan_in = data.numel() / data.size(0)\n    scale = math.sqrt(3 / fan_in)\n    data.uniform_(-scale, scale)\n\n\ndef weights_initialization(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        xavier_init(m.weight.data)\n        m.bias.data.fill_(0)\n\n\n# \nclass ContentWeightedCNN(BasicModule):\n    '''\n    BasicModule is just a Module with save() and load() convenient method, I don't think it matters, so I ommited the relevant files.\n    '''\n    def __init__(self, use_imp = True, model_name = None):\n        super(ContentWeightedCNN, self).__init__()\n        self.model_name = model_name if model_name else 'ContentWeightedCNN'\n        self.encoder = self.make_encoder()\n        self.decoder = self.make_decoder()\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        self.apply(weights_initialization)\n    \n\n\n    def forward(self, x):\n        mgdata = self.encoder(x)\n        enc_data = mgdata\n        dec_data = self.decoder(enc_data)\n        return dec_data\n\n    def make_encoder(self):\n        layers = [\n            nn.Conv2d(3, 128, 8, 4, 2),\n            nn.ReLU(inplace=False), # 54\n\n            ResidualBlock(128, 128),\n\n            nn.Conv2d(128, 256, 4, 2, 1), # 115\n            nn.ReLU(inplace=False),\n\n            ResidualBlock(256, 256),\n\n            nn.Conv2d(256, 256, 3, 1, 1), #192\n            nn.ReLU(inplace=False),\n\n            ResidualBlock(256, 256),\n\n            nn.Conv2d(256, 64, 1, 1, 0),   \n            nn.Sigmoid(),                    \n        ]\n        return nn.Sequential(*layers)\n\n\n\n    def make_decoder(self):\n        layers = [\n            nn.Conv2d(64, 512, 3, 1, 1),\n            nn.ReLU(inplace=False),\n\n            ResidualBlock(512, 512),\n\n            nn.Conv2d(512, 512, 3, 1, 1),\n            nn.ReLU(inplace=False),\n\n            ResidualBlock(512, 512),\n\n            nn.PixelShuffle(2),\n\n            nn.Conv2d(128, 256, 3, 1, 1),\n            nn.ReLU(inplace=False),\n\n            ResidualBlock(256, 256),\n\n            nn.PixelShuffle(4),\n\n            nn.Conv2d(16, 32, 3, 1, 1),\n            nn.ReLU(inplace=False),\n            \n            nn.Conv2d(32, 3, 1, 1, 0)   \n        ]\n        return nn.Sequential(*layers)    \nFor clarity, I omitted some code.\nIf you need more relevant code to understand or reproduce, comment to tell me.\nI know it is a little bit long, but I don't know where is the matter. Thanks for your attention.", "body": "I have tried to train an autoencoder-like network with pytorch based on py3, its loss decreased normally on my local machine. However, when I moved the code to my training server whose pytorch is based on py2 , I found the loss was never converge. I debugged it for a long time, and suspected it's because of the framework itself finally, because when I changed the server's environment to pytorch based on py3, the bug disappeared and behaved the same as my local machine, the loss could be declined quickly.\r\n\r\nAll the training data and code are the same, following is my network's definition and training code.\r\n\r\nIn summary, I found the bug appear's version is **0.3.0.post4 of python2**.\r\nI found the bug disappear's version is **0.3.0.post4 of python3(my local machine)**, and **'0.3.1.post2 of python3(my server's new environment)**.\r\n\r\n**trainning code**\r\n```python\r\n#coding:utf8\r\nfrom __future__ import print_function\r\nfrom config import opt\r\nimport os\r\nimport numpy as np\r\nimport math\r\nimport torch as t\r\nimport models\r\n\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.autograd import Variable\r\nfrom torchnet import meter\r\nfrom utils import AverageValueMeter\r\nfrom torchvision import transforms, datasets\r\nimport pdb\r\n\r\nfrom utils.visualize import Visualizer, PlotSaver\r\nfrom extend import RateLoss, LimuRateLoss\r\nimport torch.backends.cudnn as cudnn\r\nimport time\r\n\r\n\r\ndef train(**kwargs):\r\n    opt.parse(kwargs)\r\n    ps = PlotSaver(\"TrainImageNet12WithoutImpMap_\"+time.strftime(\"%m_%d_%H:%M:%S\")+\".log\")\r\n\r\n\r\n    # step1: Model\r\n    model = getattr(models, opt.model)(use_imp = opt.use_imp, model_name=\"CWCNN_limu_ImageNet_imp_r={r}_\u03b3={w}\".format(\r\n                                                                r=opt.rate_loss_threshold,\r\n                                                                w=opt.rate_loss_weight)\r\n                                                                if opt.use_imp else None)\r\n\r\n    if opt.use_gpu:\r\n        model.cuda()\r\n\r\n   \r\n    normalize = transforms.Normalize(\r\n                mean=[0.5, 0.5, 0.5],\r\n                std=[0.5, 0.5, 0.5]\r\n                )\r\n\r\n    train_data_transforms = transforms.Compose(\r\n        [\r\n            transforms.ToTensor(),\r\n            normalize\r\n        ]\r\n    )\r\n    val_data_transforms = transforms.Compose(\r\n        [\r\n            transforms.ToTensor(),\r\n            normalize\r\n        ]\r\n    )\r\n\r\n    train_data = datasets.ImageFolder(\r\n        opt.train_data_root,\r\n        train_data_transforms\r\n    )\r\n    val_data = datasets.ImageFolder(\r\n        opt.val_data_root,\r\n        val_data_transforms\r\n    )\r\n \r\n    train_dataloader = DataLoader(train_data, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\r\n    val_dataloader = DataLoader(val_data, opt.batch_size, shuffle=False, num_workers=opt.num_workers)\r\n\r\n    # step3: criterion and optimizer\r\n\r\n    mse_loss = t.nn.MSELoss(size_average = False)\r\n\r\n    lr = opt.lr\r\n\r\n    optimizer = t.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\r\n\r\n    start_epoch = 0\r\n\r\n    # step4: meters\r\n    mse_loss_meter = AverageValueMeter()\r\n  \r\n    previous_loss = 1e100\r\n    tolerant_now = 0\r\n    same_lr_epoch = 0\r\n\r\n\r\n    for epoch in range(start_epoch+1, opt.max_epoch+1):\r\n        same_lr_epoch += 1\r\n  \r\n        mse_loss_meter.reset()\r\n        total_loss_meter = mse_loss_meter\r\n        model.train()\r\n\r\n        for idx, (data, _) in enumerate(train_dataloader):\r\n            ipt = Variable(data)\r\n\r\n            if opt.use_gpu:\r\n                ipt = ipt.cuda()\r\n\r\n            optimizer.zero_grad()  \r\n            reconstructed = model(ipt)\r\n\r\n            loss = mse_loss(reconstructed, ipt)\r\n            caffe_loss = loss / (2 * opt.batch_size)\r\n\r\n            total_loss = caffe_loss\r\n\r\n            total_loss.backward()\r\n            optimizer.step()\r\n\r\n            mse_loss_meter.add(caffe_loss.data[0])\r\n\r\n            if idx % opt.print_freq == opt.print_freq - 1:\r\n                pass    # print sth. I omitted it for clarity\r\n\r\n       \r\n        model.save(optimizer, epoch)\r\n\r\n        # valdation \r\n        val()\r\n\r\n        if opt.use_early_adjust:\r\n            if total_loss_meter.value()[0] > previous_loss:\r\n                tolerant_now += 1\r\n                if tolerant_now == opt.tolerant_max:\r\n                    tolerant_now = 0\r\n                    same_lr_epoch = 0\r\n                    lr = lr * opt.lr_decay\r\n                    for param_group in optimizer.param_groups:\r\n                        param_group['lr'] = lr\r\n                    print ('Due to early stop anneal lr to',lr,'at epoch',epoch)\r\n            else:\r\n                tolerant_now -= 1\r\n\r\n        if same_lr_epoch and same_lr_epoch % opt.lr_anneal_epochs == 0:\r\n            same_lr_epoch = 0\r\n            tolerant_now = 0\r\n            lr = lr * opt.lr_decay\r\n            for param_group in optimizer.param_groups:\r\n                param_group['lr'] = lr\r\n            print ('Due to full epochs anneal lr to',lr,'at epoch',epoch)\r\n\r\n        previous_loss = total_loss_meter.value()[0]\r\n\r\n\r\n```\r\n \r\n**Model definition**\r\n\r\n```python\r\n#coding:utf-8\r\nfrom .BasicModule import BasicModule\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\nimport sys\r\nimport math\r\n\r\n\r\nfrom extend import Round\r\nfrom torch.nn.init import xavier_uniform\r\n\r\nimport pdb\r\n\r\nclass ResidualBlock(nn.Module):\r\n    '''\r\n    Residual Module\r\n        input(nchw) -> Conv(3x3x128, pad 1) -> ReLu -> Conv(3x3xc, pad 1) -> ReLU\r\n        input(nchw)                                                                +  = output(nchw)\r\n    '''\r\n    def __init__(self, ch_in, ch_out, shortcut = None):\r\n        super(ResidualBlock, self).__init__()\r\n        self.left = nn.Sequential(\r\n            nn.Conv2d(ch_in, 128, 3, 1, 1),  # ch_in, ch_out, kernel_size, stride, pad\r\n            nn.ReLU(inplace=False),\r\n            nn.Conv2d(128, ch_out, 3, 1, 1),\r\n            nn.ReLU(inplace=False),\r\n        )\r\n        self.right = shortcut\r\n    \r\n    def forward(self, x):\r\n        out = self.left(x)\r\n        residual = x if self.right is None else self.right(x)\r\n        out += residual\r\n        return F.relu(out, inplace=False)\r\n\r\n\r\ndef xavier_init(data):\r\n    fan_in = data.numel() / data.size(0)\r\n    scale = math.sqrt(3 / fan_in)\r\n    data.uniform_(-scale, scale)\r\n\r\n\r\ndef weights_initialization(m):\r\n    classname = m.__class__.__name__\r\n    if classname.find('Conv') != -1:\r\n        xavier_init(m.weight.data)\r\n        m.bias.data.fill_(0)\r\n\r\n\r\n# \r\nclass ContentWeightedCNN(BasicModule):\r\n    '''\r\n    BasicModule is just a Module with save() and load() convenient method, I don't think it matters, so I ommited the relevant files.\r\n    '''\r\n    def __init__(self, use_imp = True, model_name = None):\r\n        super(ContentWeightedCNN, self).__init__()\r\n        self.model_name = model_name if model_name else 'ContentWeightedCNN'\r\n        self.encoder = self.make_encoder()\r\n        self.decoder = self.make_decoder()\r\n        self.reset_parameters()\r\n    \r\n    def reset_parameters(self):\r\n        self.apply(weights_initialization)\r\n    \r\n\r\n\r\n    def forward(self, x):\r\n        mgdata = self.encoder(x)\r\n        enc_data = mgdata\r\n        dec_data = self.decoder(enc_data)\r\n        return dec_data\r\n\r\n    def make_encoder(self):\r\n        layers = [\r\n            nn.Conv2d(3, 128, 8, 4, 2),\r\n            nn.ReLU(inplace=False), # 54\r\n\r\n            ResidualBlock(128, 128),\r\n\r\n            nn.Conv2d(128, 256, 4, 2, 1), # 115\r\n            nn.ReLU(inplace=False),\r\n\r\n            ResidualBlock(256, 256),\r\n\r\n            nn.Conv2d(256, 256, 3, 1, 1), #192\r\n            nn.ReLU(inplace=False),\r\n\r\n            ResidualBlock(256, 256),\r\n\r\n            nn.Conv2d(256, 64, 1, 1, 0),   \r\n            nn.Sigmoid(),                    \r\n        ]\r\n        return nn.Sequential(*layers)\r\n\r\n\r\n\r\n    def make_decoder(self):\r\n        layers = [\r\n            nn.Conv2d(64, 512, 3, 1, 1),\r\n            nn.ReLU(inplace=False),\r\n\r\n            ResidualBlock(512, 512),\r\n\r\n            nn.Conv2d(512, 512, 3, 1, 1),\r\n            nn.ReLU(inplace=False),\r\n\r\n            ResidualBlock(512, 512),\r\n\r\n            nn.PixelShuffle(2),\r\n\r\n            nn.Conv2d(128, 256, 3, 1, 1),\r\n            nn.ReLU(inplace=False),\r\n\r\n            ResidualBlock(256, 256),\r\n\r\n            nn.PixelShuffle(4),\r\n\r\n            nn.Conv2d(16, 32, 3, 1, 1),\r\n            nn.ReLU(inplace=False),\r\n            \r\n            nn.Conv2d(32, 3, 1, 1, 0)   \r\n        ]\r\n        return nn.Sequential(*layers)    \r\n```\r\n\r\nFor clarity, I omitted some code.\r\nIf you need more relevant code to understand or reproduce, comment to tell me.\r\n\r\nI know it is a little bit long, but I don't know where is the matter. Thanks for your attention."}