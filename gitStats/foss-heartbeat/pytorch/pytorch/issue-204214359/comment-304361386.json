{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/304361386", "html_url": "https://github.com/pytorch/pytorch/issues/653#issuecomment-304361386", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/653", "id": 304361386, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDM2MTM4Ng==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-26T18:52:56Z", "updated_at": "2017-05-26T18:52:56Z", "author_association": "MEMBER", "body_html": "<p>Here is a workaround implementation:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.legacy.nn <span class=\"pl-k\">import</span> SpatialCrossMapLRN <span class=\"pl-k\">as</span> SpatialCrossMapLRNOld\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Function, Variable\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> Module\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> function interface, internal, do not use this one!!!</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">SpatialCrossMapLRNFunc</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>, <span class=\"pl-smi\">beta</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.75</span>, <span class=\"pl-smi\">k</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        <span class=\"pl-c1\">self</span>.size <span class=\"pl-k\">=</span> size\n        <span class=\"pl-c1\">self</span>.alpha <span class=\"pl-k\">=</span> alpha\n        <span class=\"pl-c1\">self</span>.beta <span class=\"pl-k\">=</span> beta\n        <span class=\"pl-c1\">self</span>.k <span class=\"pl-k\">=</span> k\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-c1\">self</span>.save_for_backward(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-c1\">self</span>.lrn <span class=\"pl-k\">=</span> SpatialCrossMapLRNOld(<span class=\"pl-c1\">self</span>.size, <span class=\"pl-c1\">self</span>.alpha, <span class=\"pl-c1\">self</span>.beta, <span class=\"pl-c1\">self</span>.k)\n        <span class=\"pl-c1\">self</span>.lrn.type(<span class=\"pl-c1\">input</span>.type())\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.lrn.forward(<span class=\"pl-c1\">input</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        <span class=\"pl-c1\">input</span>, <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.saved_tensors\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.lrn.backward(<span class=\"pl-c1\">input</span>, grad_output)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> use this one instead</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">SpatialCrossMapLRN</span>(<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>, <span class=\"pl-smi\">beta</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.75</span>, <span class=\"pl-smi\">k</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        <span class=\"pl-c1\">super</span>(SpatialCrossMapLRN, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.size <span class=\"pl-k\">=</span> size\n        <span class=\"pl-c1\">self</span>.alpha <span class=\"pl-k\">=</span> alpha\n        <span class=\"pl-c1\">self</span>.beta <span class=\"pl-k\">=</span> beta\n        <span class=\"pl-c1\">self</span>.k <span class=\"pl-k\">=</span> k\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">return</span> SpatialCrossMapLRNFunc(<span class=\"pl-c1\">self</span>.size, <span class=\"pl-c1\">self</span>.alpha, <span class=\"pl-c1\">self</span>.beta, <span class=\"pl-c1\">self</span>.k)(<span class=\"pl-c1\">input</span>)</pre></div>\n<p>It uses the legacy implementation, and might not be great because it might keep some tensors around. But seems to work and passes grad-checks (but was not extensively tested). Use with care :)</p>", "body_text": "Here is a workaround implementation:\nimport torch\nfrom torch.legacy.nn import SpatialCrossMapLRN as SpatialCrossMapLRNOld\nfrom torch.autograd import Function, Variable\nfrom torch.nn import Module\n\n# function interface, internal, do not use this one!!!\nclass SpatialCrossMapLRNFunc(Function):\n    def __init__(self, size, alpha=1e-4, beta=0.75, k=1):\n        self.size = size\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n\n    def forward(self, input):\n        self.save_for_backward(input)\n        self.lrn = SpatialCrossMapLRNOld(self.size, self.alpha, self.beta, self.k)\n        self.lrn.type(input.type())\n        return self.lrn.forward(input)\n\n    def backward(self, grad_output):\n        input, = self.saved_tensors\n        return self.lrn.backward(input, grad_output)\n\n# use this one instead\nclass SpatialCrossMapLRN(Module):\n    def __init__(self, size, alpha=1e-4, beta=0.75, k=1):\n        super(SpatialCrossMapLRN, self).__init__()\n        self.size = size\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n\n    def forward(self, input):\n        return SpatialCrossMapLRNFunc(self.size, self.alpha, self.beta, self.k)(input)\nIt uses the legacy implementation, and might not be great because it might keep some tensors around. But seems to work and passes grad-checks (but was not extensively tested). Use with care :)", "body": "Here is a workaround implementation:\r\n```python\r\nimport torch\r\nfrom torch.legacy.nn import SpatialCrossMapLRN as SpatialCrossMapLRNOld\r\nfrom torch.autograd import Function, Variable\r\nfrom torch.nn import Module\r\n\r\n# function interface, internal, do not use this one!!!\r\nclass SpatialCrossMapLRNFunc(Function):\r\n    def __init__(self, size, alpha=1e-4, beta=0.75, k=1):\r\n        self.size = size\r\n        self.alpha = alpha\r\n        self.beta = beta\r\n        self.k = k\r\n\r\n    def forward(self, input):\r\n        self.save_for_backward(input)\r\n        self.lrn = SpatialCrossMapLRNOld(self.size, self.alpha, self.beta, self.k)\r\n        self.lrn.type(input.type())\r\n        return self.lrn.forward(input)\r\n\r\n    def backward(self, grad_output):\r\n        input, = self.saved_tensors\r\n        return self.lrn.backward(input, grad_output)\r\n\r\n# use this one instead\r\nclass SpatialCrossMapLRN(Module):\r\n    def __init__(self, size, alpha=1e-4, beta=0.75, k=1):\r\n        super(SpatialCrossMapLRN, self).__init__()\r\n        self.size = size\r\n        self.alpha = alpha\r\n        self.beta = beta\r\n        self.k = k\r\n\r\n    def forward(self, input):\r\n        return SpatialCrossMapLRNFunc(self.size, self.alpha, self.beta, self.k)(input)\r\n```\r\n\r\nIt uses the legacy implementation, and might not be great because it might keep some tensors around. But seems to work and passes grad-checks (but was not extensively tested). Use with care :)"}