{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165859759", "pull_request_review_id": 93859702, "id": 165859759, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NTg1OTc1OQ==", "diff_hunk": "@@ -0,0 +1,53 @@\n+#pragma once\n+\n+#include <cstdint>\n+#include <functional>\n+#include <memory>\n+\n+#include \"torch/csrc/utils/hash.h\"\n+\n+namespace torch {\n+namespace autograd {\n+\n+class Function;\n+\n+/// Represents a particular input of a function.\n+struct Edge {\n+  explicit Edge(\n+      const std::shared_ptr<Function>& function_ = nullptr,\n+      uint32_t input_nr_ = 0)", "path": "torch/csrc/autograd/edge.h", "position": null, "original_position": 18, "commit_id": "62b9f8f91783f6a3fc51a5447e0e94e49d8e15de", "original_commit_id": "a5410f7184dfdb2bc926dfb70bbbad7d03263885", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "I removed the default arguments. I think taking the shared pointer by value in the constructor would create a copy here:\r\n```\r\nif (const auto& gradient = grad_fn()) {\r\n      return Edge(gradient, output_nr());\r\n                           ^^^^^^^^\r\n    }\r\n```\r\nwhile it would turn a copy into a move for:\r\n```\r\nreturn Edge(grad_accumulator(), 0);\r\n                      ^^^^^^^^^^^^^^^^^^\r\n```\r\nso the choice is between (ref+copy, ref+copy) vs. (copy+move, move+move).\r\nMaybe we could store the `grad_accumulator` inside `Variable` and have `grad_accumulator()` return a reference?\r\n\r\nFYI, I just spent quite a decent amount of time trying to cache `gradient_edge` inside `VariableImpl` and only creating it the first time we call it and then returning a reference. However, there are too many places in the codebase where random code grabs the internals of `Variable` and assigns to the gradient function, breaking this caching. I believe `Variable` needs a proper API where you can `set_` things so that changes to internal state are encapsulated, allowing implementation changes like this, for example. I will leave it like this for now.", "created_at": "2018-02-04T21:59:11Z", "updated_at": "2018-11-23T15:39:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/5030#discussion_r165859759", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5030", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165859759"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5030#discussion_r165859759"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5030"}}, "body_html": "<p>I removed the default arguments. I think taking the shared pointer by value in the constructor would create a copy here:</p>\n<pre><code>if (const auto&amp; gradient = grad_fn()) {\n      return Edge(gradient, output_nr());\n                           ^^^^^^^^\n    }\n</code></pre>\n<p>while it would turn a copy into a move for:</p>\n<pre><code>return Edge(grad_accumulator(), 0);\n                      ^^^^^^^^^^^^^^^^^^\n</code></pre>\n<p>so the choice is between (ref+copy, ref+copy) vs. (copy+move, move+move).<br>\nMaybe we could store the <code>grad_accumulator</code> inside <code>Variable</code> and have <code>grad_accumulator()</code> return a reference?</p>\n<p>FYI, I just spent quite a decent amount of time trying to cache <code>gradient_edge</code> inside <code>VariableImpl</code> and only creating it the first time we call it and then returning a reference. However, there are too many places in the codebase where random code grabs the internals of <code>Variable</code> and assigns to the gradient function, breaking this caching. I believe <code>Variable</code> needs a proper API where you can <code>set_</code> things so that changes to internal state are encapsulated, allowing implementation changes like this, for example. I will leave it like this for now.</p>", "body_text": "I removed the default arguments. I think taking the shared pointer by value in the constructor would create a copy here:\nif (const auto& gradient = grad_fn()) {\n      return Edge(gradient, output_nr());\n                           ^^^^^^^^\n    }\n\nwhile it would turn a copy into a move for:\nreturn Edge(grad_accumulator(), 0);\n                      ^^^^^^^^^^^^^^^^^^\n\nso the choice is between (ref+copy, ref+copy) vs. (copy+move, move+move).\nMaybe we could store the grad_accumulator inside Variable and have grad_accumulator() return a reference?\nFYI, I just spent quite a decent amount of time trying to cache gradient_edge inside VariableImpl and only creating it the first time we call it and then returning a reference. However, there are too many places in the codebase where random code grabs the internals of Variable and assigns to the gradient function, breaking this caching. I believe Variable needs a proper API where you can set_ things so that changes to internal state are encapsulated, allowing implementation changes like this, for example. I will leave it like this for now.", "in_reply_to_id": 165841739}