{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2647", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2647/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2647/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2647/events", "html_url": "https://github.com/pytorch/pytorch/issues/2647", "id": 255747383, "node_id": "MDU6SXNzdWUyNTU3NDczODM=", "number": 2647, "title": "Error when loading an \"empty\" state_dict of torch.optim.Adam", "user": {"login": "Cadene", "id": 4681518, "node_id": "MDQ6VXNlcjQ2ODE1MTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/4681518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cadene", "html_url": "https://github.com/Cadene", "followers_url": "https://api.github.com/users/Cadene/followers", "following_url": "https://api.github.com/users/Cadene/following{/other_user}", "gists_url": "https://api.github.com/users/Cadene/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cadene/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cadene/subscriptions", "organizations_url": "https://api.github.com/users/Cadene/orgs", "repos_url": "https://api.github.com/users/Cadene/repos", "events_url": "https://api.github.com/users/Cadene/events{/privacy}", "received_events_url": "https://api.github.com/users/Cadene/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-06T21:25:27Z", "updated_at": "2017-10-08T16:08:18Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I know it is not a common use case, but I encounter an issue after having loaded the state_dict of Adam if the <code>step</code> method of the optimizer has not been executed.</p>\n<p>However it works well with SGD.</p>\n<p>You need to run this code twice (first time to save the state_dict) to reproduce the error.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\n<span class=\"pl-c1\">BUG</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\noptim_name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Adam<span class=\"pl-pds\">'</span></span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> works with SGD</span>\n\nmodel <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>)\noptimizer <span class=\"pl-k\">=</span> torch.optim.<span class=\"pl-c1\">__dict__</span>[optim_name](model.parameters(), <span class=\"pl-c1\">1e-4</span>)\nstate_dict <span class=\"pl-k\">=</span> optimizer.state_dict()\npath_state <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>state_dict_<span class=\"pl-c1\">{}</span>.pth<span class=\"pl-pds\">'</span></span>.format(optim_name)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.isfile(path_state):\n    torch.save(state_dict, path_state)\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">BUG</span>:\n        optimizer.load_state_dict(torch.load(path_state))\n    loss <span class=\"pl-k\">=</span> model(torch.autograd.Variable(torch.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>)))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()</pre></div>\n<pre><code>Traceback (most recent call last):\n  File \"optimizer.py\", line 21, in &lt;module&gt;\n    optimizer.step()\n  File \"/home/cadene/anaconda3/lib/python3.5/site-packages/torch/optim/adam.py\", line 46, in step\n    state = self.state[p]\nKeyError: Parameter containing:\n-0.0645  0.1660\n[torch.FloatTensor of size 1x2]\n</code></pre>\n<p>I solved the problem with a little hack:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">...</span>\nstate_dict <span class=\"pl-k\">=</span> torch.load(path_state)\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(state_dict[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>state<span class=\"pl-pds\">'</span></span>]) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>:\n    optimizer.load_state_dict(state_dict\n<span class=\"pl-c1\">...</span></pre></div>", "body_text": "Hello,\nI know it is not a common use case, but I encounter an issue after having loaded the state_dict of Adam if the step method of the optimizer has not been executed.\nHowever it works well with SGD.\nYou need to run this code twice (first time to save the state_dict) to reproduce the error.\nimport os\nimport torch\nimport torch.nn as nn\n\nBUG = True\noptim_name = 'Adam' # works with SGD\n\nmodel = nn.Linear(2,1)\noptimizer = torch.optim.__dict__[optim_name](model.parameters(), 1e-4)\nstate_dict = optimizer.state_dict()\npath_state = 'state_dict_{}.pth'.format(optim_name)\n\nif not os.path.isfile(path_state):\n    torch.save(state_dict, path_state)\nelse:\n    if BUG:\n        optimizer.load_state_dict(torch.load(path_state))\n    loss = model(torch.autograd.Variable(torch.randn(1,2)))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\nTraceback (most recent call last):\n  File \"optimizer.py\", line 21, in <module>\n    optimizer.step()\n  File \"/home/cadene/anaconda3/lib/python3.5/site-packages/torch/optim/adam.py\", line 46, in step\n    state = self.state[p]\nKeyError: Parameter containing:\n-0.0645  0.1660\n[torch.FloatTensor of size 1x2]\n\nI solved the problem with a little hack:\n...\nstate_dict = torch.load(path_state)\nif len(state_dict['state']) != 0:\n    optimizer.load_state_dict(state_dict\n...", "body": "Hello,\r\n\r\nI know it is not a common use case, but I encounter an issue after having loaded the state_dict of Adam if the `step` method of the optimizer has not been executed.\r\n\r\nHowever it works well with SGD.\r\n\r\nYou need to run this code twice (first time to save the state_dict) to reproduce the error.\r\n\r\n```python\r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nBUG = True\r\noptim_name = 'Adam' # works with SGD\r\n\r\nmodel = nn.Linear(2,1)\r\noptimizer = torch.optim.__dict__[optim_name](model.parameters(), 1e-4)\r\nstate_dict = optimizer.state_dict()\r\npath_state = 'state_dict_{}.pth'.format(optim_name)\r\n\r\nif not os.path.isfile(path_state):\r\n    torch.save(state_dict, path_state)\r\nelse:\r\n    if BUG:\r\n        optimizer.load_state_dict(torch.load(path_state))\r\n    loss = model(torch.autograd.Variable(torch.randn(1,2)))\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"optimizer.py\", line 21, in <module>\r\n    optimizer.step()\r\n  File \"/home/cadene/anaconda3/lib/python3.5/site-packages/torch/optim/adam.py\", line 46, in step\r\n    state = self.state[p]\r\nKeyError: Parameter containing:\r\n-0.0645  0.1660\r\n[torch.FloatTensor of size 1x2]\r\n```\r\n\r\nI solved the problem with a little hack:\r\n\r\n```python\r\n...\r\nstate_dict = torch.load(path_state)\r\nif len(state_dict['state']) != 0:\r\n    optimizer.load_state_dict(state_dict\r\n...\r\n```"}