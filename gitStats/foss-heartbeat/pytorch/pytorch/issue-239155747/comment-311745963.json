{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/311745963", "html_url": "https://github.com/pytorch/pytorch/issues/1927#issuecomment-311745963", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1927", "id": 311745963, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTc0NTk2Mw==", "user": {"login": "pranv", "id": 8753078, "node_id": "MDQ6VXNlcjg3NTMwNzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/8753078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pranv", "html_url": "https://github.com/pranv", "followers_url": "https://api.github.com/users/pranv/followers", "following_url": "https://api.github.com/users/pranv/following{/other_user}", "gists_url": "https://api.github.com/users/pranv/gists{/gist_id}", "starred_url": "https://api.github.com/users/pranv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pranv/subscriptions", "organizations_url": "https://api.github.com/users/pranv/orgs", "repos_url": "https://api.github.com/users/pranv/repos", "events_url": "https://api.github.com/users/pranv/events{/privacy}", "received_events_url": "https://api.github.com/users/pranv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-28T18:25:12Z", "updated_at": "2017-06-28T18:26:05Z", "author_association": "NONE", "body_html": "<p>I have thought about such an API a lot, both with respect to PyTorch and (earlier) with respect to Keras. Here are my thoughts:</p>\n<p>What you've proposed in the code here is an abstraction of what seems to be the ubiquitous pattern used when implementing RNNs. The RNN/LSTM implementation always outputs a 3D tensor and if we want to apply softmax over it, we need to apply the same projection for each samples in the batch, at each time-step. There is a lot of utility for a wrapper that takes in the <code>Linear</code> layer and applies it over a 3D tensor. PyTorch's own examples have used this in a couple of places.</p>\n<p>But I would argue that this is still a very limited use case and doesn't warrant a generic container. We can have a <code>LinearOverTime</code> layer or even extend the <code>Linear</code> layer to work with 3D inputs (implicitly flattening across one of the dimensions).</p>\n<p>However, there are a lot of use cases where a collection of layers have to be applied repeatedly over an axis. For making such a container for this generic case, we have to consider 2 cases:</p>\n<ol>\n<li><strong>Stateful Recurrence</strong>: We have some state that is carried as we index along a particular axis. This cannot be parallelized, but the container can provide some utilities that manage the state transmissions.</li>\n<li><strong>Stateless Recurrence</strong>: In this case we can do something similar to what you've proposed -- flatten along some axis (user provided) and do the computation in parallel. (This is for speed only, as stateless recurrence with no state is a strict superset of this).</li>\n</ol>\n<p>Common use cases for the first category are attention models, fancy memory augmented networks and novel RNN formulations (ex. Recurrent Highway Networks, Unitary Evolution RNNs etc.,). Common use cases for the second category are the time distributed (or axis distributed) versions of any cascade of layers.</p>\n<p>Basically, I am imagining PyTorch's take on <code>theano.scan</code>: <a href=\"http://deeplearning.net/software/theano/library/scan.html\" rel=\"nofollow\">http://deeplearning.net/software/theano/library/scan.html</a> . I think it'll be much more user friendly and powerful with PyTorch's dynamic paradigm.</p>\n<p>If more people think that this is a good idea, I can work on it :)</p>", "body_text": "I have thought about such an API a lot, both with respect to PyTorch and (earlier) with respect to Keras. Here are my thoughts:\nWhat you've proposed in the code here is an abstraction of what seems to be the ubiquitous pattern used when implementing RNNs. The RNN/LSTM implementation always outputs a 3D tensor and if we want to apply softmax over it, we need to apply the same projection for each samples in the batch, at each time-step. There is a lot of utility for a wrapper that takes in the Linear layer and applies it over a 3D tensor. PyTorch's own examples have used this in a couple of places.\nBut I would argue that this is still a very limited use case and doesn't warrant a generic container. We can have a LinearOverTime layer or even extend the Linear layer to work with 3D inputs (implicitly flattening across one of the dimensions).\nHowever, there are a lot of use cases where a collection of layers have to be applied repeatedly over an axis. For making such a container for this generic case, we have to consider 2 cases:\n\nStateful Recurrence: We have some state that is carried as we index along a particular axis. This cannot be parallelized, but the container can provide some utilities that manage the state transmissions.\nStateless Recurrence: In this case we can do something similar to what you've proposed -- flatten along some axis (user provided) and do the computation in parallel. (This is for speed only, as stateless recurrence with no state is a strict superset of this).\n\nCommon use cases for the first category are attention models, fancy memory augmented networks and novel RNN formulations (ex. Recurrent Highway Networks, Unitary Evolution RNNs etc.,). Common use cases for the second category are the time distributed (or axis distributed) versions of any cascade of layers.\nBasically, I am imagining PyTorch's take on theano.scan: http://deeplearning.net/software/theano/library/scan.html . I think it'll be much more user friendly and powerful with PyTorch's dynamic paradigm.\nIf more people think that this is a good idea, I can work on it :)", "body": "I have thought about such an API a lot, both with respect to PyTorch and (earlier) with respect to Keras. Here are my thoughts:\r\n\r\nWhat you've proposed in the code here is an abstraction of what seems to be the ubiquitous pattern used when implementing RNNs. The RNN/LSTM implementation always outputs a 3D tensor and if we want to apply softmax over it, we need to apply the same projection for each samples in the batch, at each time-step. There is a lot of utility for a wrapper that takes in the `Linear` layer and applies it over a 3D tensor. PyTorch's own examples have used this in a couple of places. \r\n\r\nBut I would argue that this is still a very limited use case and doesn't warrant a generic container. We can have a `LinearOverTime` layer or even extend the `Linear` layer to work with 3D inputs (implicitly flattening across one of the dimensions).\r\n\r\nHowever, there are a lot of use cases where a collection of layers have to be applied repeatedly over an axis. For making such a container for this generic case, we have to consider 2 cases:\r\n\r\n1. **Stateful Recurrence**: We have some state that is carried as we index along a particular axis. This cannot be parallelized, but the container can provide some utilities that manage the state transmissions.\r\n2. **Stateless Recurrence**: In this case we can do something similar to what you've proposed -- flatten along some axis (user provided) and do the computation in parallel. (This is for speed only, as stateless recurrence with no state is a strict superset of this).\r\n\r\nCommon use cases for the first category are attention models, fancy memory augmented networks and novel RNN formulations (ex. Recurrent Highway Networks, Unitary Evolution RNNs etc.,). Common use cases for the second category are the time distributed (or axis distributed) versions of any cascade of layers. \r\n\r\nBasically, I am imagining PyTorch's take on `theano.scan`: http://deeplearning.net/software/theano/library/scan.html . I think it'll be much more user friendly and powerful with PyTorch's dynamic paradigm.\r\n\r\nIf more people think that this is a good idea, I can work on it :)"}