{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/312142846", "html_url": "https://github.com/pytorch/pytorch/issues/1927#issuecomment-312142846", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1927", "id": 312142846, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjE0Mjg0Ng==", "user": {"login": "erogol", "id": 1402048, "node_id": "MDQ6VXNlcjE0MDIwNDg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1402048?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erogol", "html_url": "https://github.com/erogol", "followers_url": "https://api.github.com/users/erogol/followers", "following_url": "https://api.github.com/users/erogol/following{/other_user}", "gists_url": "https://api.github.com/users/erogol/gists{/gist_id}", "starred_url": "https://api.github.com/users/erogol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erogol/subscriptions", "organizations_url": "https://api.github.com/users/erogol/orgs", "repos_url": "https://api.github.com/users/erogol/repos", "events_url": "https://api.github.com/users/erogol/events{/privacy}", "received_events_url": "https://api.github.com/users/erogol/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-30T00:27:54Z", "updated_at": "2017-06-30T00:27:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11729078\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jekbradbury\">@jekbradbury</a> but my feeling is (not verified) using only tensor operations without explicit loop would be faster. For example, assume that we have 12x8x3x224x224 input s.t. 8 is seq length. Looping means 8 times layer call, otherwise, if we just merge first two dimensions, this is only one pass on GPU (if memory allows you). And the complexity increases as we increase number of time-distributed layers. Correct me I am wrong.</p>", "body_text": "@jekbradbury but my feeling is (not verified) using only tensor operations without explicit loop would be faster. For example, assume that we have 12x8x3x224x224 input s.t. 8 is seq length. Looping means 8 times layer call, otherwise, if we just merge first two dimensions, this is only one pass on GPU (if memory allows you). And the complexity increases as we increase number of time-distributed layers. Correct me I am wrong.", "body": "@jekbradbury but my feeling is (not verified) using only tensor operations without explicit loop would be faster. For example, assume that we have 12x8x3x224x224 input s.t. 8 is seq length. Looping means 8 times layer call, otherwise, if we just merge first two dimensions, this is only one pass on GPU (if memory allows you). And the complexity increases as we increase number of time-distributed layers. Correct me I am wrong."}