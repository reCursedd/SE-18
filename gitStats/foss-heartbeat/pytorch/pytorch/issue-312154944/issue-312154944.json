{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6373", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6373/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6373/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6373/events", "html_url": "https://github.com/pytorch/pytorch/issues/6373", "id": 312154944, "node_id": "MDU6SXNzdWUzMTIxNTQ5NDQ=", "number": 6373, "title": "torch.nn.Linear() behaves randomly", "user": {"login": "HMEIatJHU", "id": 19693633, "node_id": "MDQ6VXNlcjE5NjkzNjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/19693633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HMEIatJHU", "html_url": "https://github.com/HMEIatJHU", "followers_url": "https://api.github.com/users/HMEIatJHU/followers", "following_url": "https://api.github.com/users/HMEIatJHU/following{/other_user}", "gists_url": "https://api.github.com/users/HMEIatJHU/gists{/gist_id}", "starred_url": "https://api.github.com/users/HMEIatJHU/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HMEIatJHU/subscriptions", "organizations_url": "https://api.github.com/users/HMEIatJHU/orgs", "repos_url": "https://api.github.com/users/HMEIatJHU/repos", "events_url": "https://api.github.com/users/HMEIatJHU/events{/privacy}", "received_events_url": "https://api.github.com/users/HMEIatJHU/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-04-07T00:29:22Z", "updated_at": "2018-04-10T17:35:22Z", "closed_at": "2018-04-10T17:35:22Z", "author_association": "NONE", "body_html": "<h2>Linear layer torch.nn.Linear() behaves randomly<br>\nTwo passes of exactly the same input returns different outputs</h2>\n<p>I would like to report a bug:</p>\n<ul>\n<li>\n<p>PyTorch or Caffe2: PyTorch</p>\n</li>\n<li>\n<p>OS: OS X Yosemite 10.10.2</p>\n</li>\n<li>\n<p>PyTorch version: 0.4.0a0+64e94f0</p>\n</li>\n<li>\n<p>How you installed PyTorch (conda, pip, source): source</p>\n</li>\n<li>\n<p>Python version: Python 3.6.3</p>\n</li>\n<li>\n<p>CUDA/cuDNN version: NO</p>\n</li>\n<li>\n<p>GPU models and configuration: NO</p>\n</li>\n<li>\n<p>GCC version (if compiling from source): GCC 7.2.0</p>\n</li>\n<li>\n<p>CMake version: 3.9.4</p>\n</li>\n<li>\n<p>A script to reproduce the bug.</p>\n</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\n\ndim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\nlin1 <span class=\"pl-k\">=</span> torch.nn.Linear(dim,<span class=\"pl-c1\">10</span>)\ninfeat <span class=\"pl-k\">=</span> torch.FloatTensor(<span class=\"pl-c1\">1</span>,dim).fill_(<span class=\"pl-c1\">1.0</span>)\n\ninfeat <span class=\"pl-k\">=</span> Variable(infeat)\n\nout <span class=\"pl-k\">=</span> lin1(infeat)\nout_new <span class=\"pl-k\">=</span> lin1(infeat)\n\n<span class=\"pl-c1\">print</span>(out)\n<span class=\"pl-c1\">print</span>(out_new)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sum of abs diff is <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format((out <span class=\"pl-k\">-</span> out_new).abs().sum()))\n\nlin2 <span class=\"pl-k\">=</span> torch.nn.Linear(dim,<span class=\"pl-c1\">100</span>)\n\nout <span class=\"pl-k\">=</span> lin2(infeat)\nout_new <span class=\"pl-k\">=</span> lin2(infeat)\n\n<span class=\"pl-c1\">print</span>(out)\n<span class=\"pl-c1\">print</span>(out_new)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sum of abs diff is <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format((out <span class=\"pl-k\">-</span> out_new).abs().sum()))</pre></div>", "body_text": "Linear layer torch.nn.Linear() behaves randomly\nTwo passes of exactly the same input returns different outputs\nI would like to report a bug:\n\n\nPyTorch or Caffe2: PyTorch\n\n\nOS: OS X Yosemite 10.10.2\n\n\nPyTorch version: 0.4.0a0+64e94f0\n\n\nHow you installed PyTorch (conda, pip, source): source\n\n\nPython version: Python 3.6.3\n\n\nCUDA/cuDNN version: NO\n\n\nGPU models and configuration: NO\n\n\nGCC version (if compiling from source): GCC 7.2.0\n\n\nCMake version: 3.9.4\n\n\nA script to reproduce the bug.\n\n\nimport torch\nfrom torch.autograd import Variable\n\ntorch.manual_seed(0)\n\ndim = 10000\nlin1 = torch.nn.Linear(dim,10)\ninfeat = torch.FloatTensor(1,dim).fill_(1.0)\n\ninfeat = Variable(infeat)\n\nout = lin1(infeat)\nout_new = lin1(infeat)\n\nprint(out)\nprint(out_new)\nprint(\"sum of abs diff is {}\".format((out - out_new).abs().sum()))\n\nlin2 = torch.nn.Linear(dim,100)\n\nout = lin2(infeat)\nout_new = lin2(infeat)\n\nprint(out)\nprint(out_new)\nprint(\"sum of abs diff is {}\".format((out - out_new).abs().sum()))", "body": "Linear layer torch.nn.Linear() behaves randomly\r\nTwo passes of exactly the same input returns different outputs\r\n--------------------------------\r\nI would like to report a bug:\r\n- PyTorch or Caffe2: PyTorch\r\n- OS: OS X Yosemite 10.10.2\r\n- PyTorch version: 0.4.0a0+64e94f0\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: Python 3.6.3\r\n- CUDA/cuDNN version: NO\r\n- GPU models and configuration: NO\r\n- GCC version (if compiling from source): GCC 7.2.0\r\n- CMake version: 3.9.4\r\n\r\n- A script to reproduce the bug. \r\n```py\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ntorch.manual_seed(0)\r\n\r\ndim = 10000\r\nlin1 = torch.nn.Linear(dim,10)\r\ninfeat = torch.FloatTensor(1,dim).fill_(1.0)\r\n\r\ninfeat = Variable(infeat)\r\n\r\nout = lin1(infeat)\r\nout_new = lin1(infeat)\r\n\r\nprint(out)\r\nprint(out_new)\r\nprint(\"sum of abs diff is {}\".format((out - out_new).abs().sum()))\r\n\r\nlin2 = torch.nn.Linear(dim,100)\r\n\r\nout = lin2(infeat)\r\nout_new = lin2(infeat)\r\n\r\nprint(out)\r\nprint(out_new)\r\nprint(\"sum of abs diff is {}\".format((out - out_new).abs().sum()))\r\n```\r\n\r\n"}