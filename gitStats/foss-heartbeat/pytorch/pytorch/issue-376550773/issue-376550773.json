{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13471", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13471/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13471/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13471/events", "html_url": "https://github.com/pytorch/pytorch/issues/13471", "id": 376550773, "node_id": "MDU6SXNzdWUzNzY1NTA3NzM=", "number": 13471, "title": "Seemingly Random success rate of DistributedDataParallel (1.0.0.dev20181029 - 1.0.0.dev20181105) ", "user": {"login": "SinghJasdeep", "id": 33911313, "node_id": "MDQ6VXNlcjMzOTExMzEz", "avatar_url": "https://avatars2.githubusercontent.com/u/33911313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SinghJasdeep", "html_url": "https://github.com/SinghJasdeep", "followers_url": "https://api.github.com/users/SinghJasdeep/followers", "following_url": "https://api.github.com/users/SinghJasdeep/following{/other_user}", "gists_url": "https://api.github.com/users/SinghJasdeep/gists{/gist_id}", "starred_url": "https://api.github.com/users/SinghJasdeep/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SinghJasdeep/subscriptions", "organizations_url": "https://api.github.com/users/SinghJasdeep/orgs", "repos_url": "https://api.github.com/users/SinghJasdeep/repos", "events_url": "https://api.github.com/users/SinghJasdeep/events{/privacy}", "received_events_url": "https://api.github.com/users/SinghJasdeep/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-11-01T20:09:28Z", "updated_at": "2018-11-21T00:00:24Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>Dear Pytorch team,</p>\n<p>I am trying to run a multi-gpu job using torch.nn.parallel.distributed. I would like to use pytorch 1.0+ because of: (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"373590347\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13056\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13056/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/13056\">#13056</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"312477828\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6419\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6419/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6419\">#6419</a>) and well because its awesome :)</p>\n<p>I am running the following commands in separate tmux sessions on the same cloud machine with 8 GPUs:</p>\n<pre><code>1. nvidia-docker ... \"CUDA ... DEVICES=0 python train.py  ... --devices 0 --rank_ 0 --world_size 8\"\n2. nvidia-docker ... \"CUDA ... DEVICES=1 python train.py  ... --devices 0 --rank_ 1 --world_size 8\"\n.\n.\n.\n8. nvidia-docker ... \"CUDA ... DEVICES=7 python train.py  ... --devices 0 --rank_ 7 --world_size 8\"\n</code></pre>\n<p>The programs execute as expected up until:</p>\n<pre><code>line xxx:   torch.distributed.init_process_group(...)\n</code></pre>\n<p>At which point they (<strong>usually</strong>) hang. I have checked to make sure that all of the programs are getting the same file for torch.distributed.init_process_group(init_method= ...)</p>\n<p>I have yet to get this working with a world_size of 8, but I can get this working with smaller world_size's (2-7). To do so I just run the exact same command a number of times until the programs just (seemingly) randomly get past torch.distributed.init_process_group(...). At first glance the average number failed tries before a success seems to increase with the world_size.</p>\n<p>This code was working consistently on 0.4.1 however the gloo timeout needed to be larger for my application.</p>\n<p>Do not know what is causing this. Would really appreciate you hep.</p>\n<p>Thank you!</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<ol>\n<li>Run the following commands in separate tmux sessions on the same cloud machine with 8 GPUs:</li>\n</ol>\n<pre><code>1. nvidia-docker ... \"CUDA ... DEVICES=0 python train.py  ... --devices 0 --rank_ 0 --world_size 8\"\n2. nvidia-docker ... \"CUDA ... DEVICES=1 python train.py  ... --devices 0 --rank_ 1 --world_size 8\"\n.\n.\n.\n8. nvidia-docker ... \"CUDA ... DEVICES=7 python train.py  ... --devices 0 --rank_ 7 --world_size 8\"\n</code></pre>\n<ol start=\"2\">\n<li>If the programs get hung on the line with:</li>\n</ol>\n<pre><code>torch.distributed.init_process_group(...)\n</code></pre>\n<p>You have reproduced the issue.</p>\n<h2>Expected behavior</h2>\n<p>I expected the programs to continue training more than ~ 10% of the time.<br>\nAlso would be nice to be able to use a world_size of 8</p>\n<h2>Environment</h2>\n<p>PyTorch version: 1.0.0.dev20181029<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.5 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0: Tesla V100-SXM2-16GB<br>\nGPU 1: Tesla V100-SXM2-16GB<br>\nGPU 2: Tesla V100-SXM2-16GB<br>\nGPU 3: Tesla V100-SXM2-16GB<br>\nGPU 4: Tesla V100-SXM2-16GB<br>\nGPU 5: Tesla V100-SXM2-16GB<br>\nGPU 6: Tesla V100-SXM2-16GB<br>\nGPU 7: Tesla V100-SXM2-16GB</p>\n<p>Nvidia driver version: 396.44<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.15.2)<br>\n[pip] torch (1.0.0.dev20181029)<br>\n[conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch</p>\n<h2>Additional context</h2>\n<p>Please feel free to ask for more :)</p>", "body_text": "\ud83d\udc1b Bug\nDear Pytorch team,\nI am trying to run a multi-gpu job using torch.nn.parallel.distributed. I would like to use pytorch 1.0+ because of: (#13056, #6419) and well because its awesome :)\nI am running the following commands in separate tmux sessions on the same cloud machine with 8 GPUs:\n1. nvidia-docker ... \"CUDA ... DEVICES=0 python train.py  ... --devices 0 --rank_ 0 --world_size 8\"\n2. nvidia-docker ... \"CUDA ... DEVICES=1 python train.py  ... --devices 0 --rank_ 1 --world_size 8\"\n.\n.\n.\n8. nvidia-docker ... \"CUDA ... DEVICES=7 python train.py  ... --devices 0 --rank_ 7 --world_size 8\"\n\nThe programs execute as expected up until:\nline xxx:   torch.distributed.init_process_group(...)\n\nAt which point they (usually) hang. I have checked to make sure that all of the programs are getting the same file for torch.distributed.init_process_group(init_method= ...)\nI have yet to get this working with a world_size of 8, but I can get this working with smaller world_size's (2-7). To do so I just run the exact same command a number of times until the programs just (seemingly) randomly get past torch.distributed.init_process_group(...). At first glance the average number failed tries before a success seems to increase with the world_size.\nThis code was working consistently on 0.4.1 however the gloo timeout needed to be larger for my application.\nDo not know what is causing this. Would really appreciate you hep.\nThank you!\nTo Reproduce\nSteps to reproduce the behavior:\n\nRun the following commands in separate tmux sessions on the same cloud machine with 8 GPUs:\n\n1. nvidia-docker ... \"CUDA ... DEVICES=0 python train.py  ... --devices 0 --rank_ 0 --world_size 8\"\n2. nvidia-docker ... \"CUDA ... DEVICES=1 python train.py  ... --devices 0 --rank_ 1 --world_size 8\"\n.\n.\n.\n8. nvidia-docker ... \"CUDA ... DEVICES=7 python train.py  ... --devices 0 --rank_ 7 --world_size 8\"\n\n\nIf the programs get hung on the line with:\n\ntorch.distributed.init_process_group(...)\n\nYou have reproduced the issue.\nExpected behavior\nI expected the programs to continue training more than ~ 10% of the time.\nAlso would be nice to be able to use a world_size of 8\nEnvironment\nPyTorch version: 1.0.0.dev20181029\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: Could not collect\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: Tesla V100-SXM2-16GB\nGPU 1: Tesla V100-SXM2-16GB\nGPU 2: Tesla V100-SXM2-16GB\nGPU 3: Tesla V100-SXM2-16GB\nGPU 4: Tesla V100-SXM2-16GB\nGPU 5: Tesla V100-SXM2-16GB\nGPU 6: Tesla V100-SXM2-16GB\nGPU 7: Tesla V100-SXM2-16GB\nNvidia driver version: 396.44\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\nVersions of relevant libraries:\n[pip] numpy (1.15.2)\n[pip] torch (1.0.0.dev20181029)\n[conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\nAdditional context\nPlease feel free to ask for more :)", "body": "## \ud83d\udc1b Bug\r\nDear Pytorch team, \r\n\r\nI am trying to run a multi-gpu job using torch.nn.parallel.distributed. I would like to use pytorch 1.0+ because of: (https://github.com/pytorch/pytorch/pull/13056, https://github.com/pytorch/pytorch/issues/6419) and well because its awesome :) \r\n\r\nI am running the following commands in separate tmux sessions on the same cloud machine with 8 GPUs: \r\n\r\n```\r\n1. nvidia-docker ... \"CUDA ... DEVICES=0 python train.py  ... --devices 0 --rank_ 0 --world_size 8\"\r\n2. nvidia-docker ... \"CUDA ... DEVICES=1 python train.py  ... --devices 0 --rank_ 1 --world_size 8\"\r\n.\r\n.\r\n.\r\n8. nvidia-docker ... \"CUDA ... DEVICES=7 python train.py  ... --devices 0 --rank_ 7 --world_size 8\"\r\n```\r\n\r\nThe programs execute as expected up until:\r\n```\r\nline xxx:   torch.distributed.init_process_group(...)\r\n```\r\n\r\nAt which point they (**usually**) hang. I have checked to make sure that all of the programs are getting the same file for torch.distributed.init_process_group(init_method= ...) \r\n\r\nI have yet to get this working with a world_size of 8, but I can get this working with smaller world_size's (2-7). To do so I just run the exact same command a number of times until the programs just (seemingly) randomly get past torch.distributed.init_process_group(...). At first glance the average number failed tries before a success seems to increase with the world_size. \r\n\r\nThis code was working consistently on 0.4.1 however the gloo timeout needed to be larger for my application. \r\n\r\nDo not know what is causing this. Would really appreciate you hep. \r\n\r\nThank you!\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1) Run the following commands in separate tmux sessions on the same cloud machine with 8 GPUs: \r\n\r\n```\r\n1. nvidia-docker ... \"CUDA ... DEVICES=0 python train.py  ... --devices 0 --rank_ 0 --world_size 8\"\r\n2. nvidia-docker ... \"CUDA ... DEVICES=1 python train.py  ... --devices 0 --rank_ 1 --world_size 8\"\r\n.\r\n.\r\n.\r\n8. nvidia-docker ... \"CUDA ... DEVICES=7 python train.py  ... --devices 0 --rank_ 7 --world_size 8\"\r\n```\r\n2) If the programs get hung on the line with:\r\n```\r\ntorch.distributed.init_process_group(...)\r\n```\r\nYou have reproduced the issue. \r\n\r\n## Expected behavior\r\n\r\nI expected the programs to continue training more than ~ 10% of the time. \r\nAlso would be nice to be able to use a world_size of 8\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20181029\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (1.0.0.dev20181029)\r\n[conda] pytorch-nightly           1.0.0.dev20181029 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n\r\n## Additional context\r\n\r\nPlease feel free to ask for more :) "}