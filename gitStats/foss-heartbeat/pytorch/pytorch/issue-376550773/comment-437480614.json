{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/437480614", "html_url": "https://github.com/pytorch/pytorch/issues/13471#issuecomment-437480614", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13471", "id": 437480614, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzQ4MDYxNA==", "user": {"login": "SinghJasdeep", "id": 33911313, "node_id": "MDQ6VXNlcjMzOTExMzEz", "avatar_url": "https://avatars2.githubusercontent.com/u/33911313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SinghJasdeep", "html_url": "https://github.com/SinghJasdeep", "followers_url": "https://api.github.com/users/SinghJasdeep/followers", "following_url": "https://api.github.com/users/SinghJasdeep/following{/other_user}", "gists_url": "https://api.github.com/users/SinghJasdeep/gists{/gist_id}", "starred_url": "https://api.github.com/users/SinghJasdeep/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SinghJasdeep/subscriptions", "organizations_url": "https://api.github.com/users/SinghJasdeep/orgs", "repos_url": "https://api.github.com/users/SinghJasdeep/repos", "events_url": "https://api.github.com/users/SinghJasdeep/events{/privacy}", "received_events_url": "https://api.github.com/users/SinghJasdeep/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-09T20:09:06Z", "updated_at": "2018-11-09T20:09:06Z", "author_association": "NONE", "body_html": "<p>This is still an issue in <strong>1.0.0.dev20181105</strong>-py3.6_cuda9.0.176_cudnn7.1.2_0 pytorch</p>\n<p>This usually does not happen for 4 gpus and below. But usually always occurs for 7+</p>\n<p>I have noticed that for 4 gpus if the processes do hang they will still recover after ~5 minutes. This leads me to believe perhaps there is some communication issue between the GPU's and coordinating more GPU's is simply more likely to get at least one of them out of sync, therefore, the much longer (possibly infinite) wait times for 8+ gpus.</p>", "body_text": "This is still an issue in 1.0.0.dev20181105-py3.6_cuda9.0.176_cudnn7.1.2_0 pytorch\nThis usually does not happen for 4 gpus and below. But usually always occurs for 7+\nI have noticed that for 4 gpus if the processes do hang they will still recover after ~5 minutes. This leads me to believe perhaps there is some communication issue between the GPU's and coordinating more GPU's is simply more likely to get at least one of them out of sync, therefore, the much longer (possibly infinite) wait times for 8+ gpus.", "body": "This is still an issue in **1.0.0.dev20181105**-py3.6_cuda9.0.176_cudnn7.1.2_0 pytorch\r\n\r\nThis usually does not happen for 4 gpus and below. But usually always occurs for 7+ \r\n\r\nI have noticed that for 4 gpus if the processes do hang they will still recover after ~5 minutes. This leads me to believe perhaps there is some communication issue between the GPU's and coordinating more GPU's is simply more likely to get at least one of them out of sync, therefore, the much longer (possibly infinite) wait times for 8+ gpus.  "}