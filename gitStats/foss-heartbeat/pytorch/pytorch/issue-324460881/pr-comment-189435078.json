{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189435078", "pull_request_review_id": 121619708, "id": 189435078, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQzNTA3OA==", "diff_hunk": "@@ -0,0 +1,99 @@\n+import torch\n+\n+\n+class detect_anomaly(object):\n+    r\"\"\"Context-manager that enable anomaly detection for the autograd engine.\n+\n+    This does two things:\n+    - Running the forward pass with detection enabled will allow the backward\n+    pass to print the traceback of the forward operation that created the failing\n+    backward function.\n+    - Any backward computation that generate \"nan\" value will raise an error.\n+\n+    Example:\n+\n+        >>> import torch\n+        >>> from torch import autograd\n+        >>> class MyFunc(autograd.Function):\n+        ...     @staticmethod\n+        ...     def forward(ctx, inp):\n+        ...         return inp.clone()\n+        ...     @staticmethod\n+        ...     def backward(ctx, gO):\n+        ...         # Error during the backward pass\n+        ...         raise RuntimeError(\"Some error in backward\")\n+        ...         return gO.clone()\n+        >>> def run_fn(a):\n+        ...     out = MyFunc.apply(a)\n+        ...     return out.sum()\n+        >>> inp = torch.rand(10, 10, requires_grad=True)\n+        >>> out = run_fn(inp)\n+        >>> out.backward()\n+            Traceback (most recent call last):\n+              File \"<stdin>\", line 1, in <module>\n+              File \"/your/pytorch/intall/torch/tensor.py\", line 93, in backward\n+                torch.autograd.backward(self, gradient, retain_graph, create_graph)\n+              File \"/your/pytorch/intall/torch/autograd/__init__.py\", line 90, in backward\n+                allow_unreachable=True)  # allow_unreachable flag\n+              File \"/your/pytorch/intall/torch/autograd/function.py\", line 76, in apply\n+                return self._forward_cls.backward(self, *args)\n+              File \"<stdin>\", line 8, in backward\n+            RuntimeError: Some error in backward\n+        >>> with autograd.detect_anomaly():\n+        ...     inp = torch.rand(10, 10, requires_grad=True)\n+        ...     out = run_fn(inp)\n+        ...     out.backward()\n+            Traceback of forward call that caused the error:", "path": "torch/autograd/anomaly_mode.py", "position": 46, "original_position": 46, "commit_id": "ff608fec1da7f98b52ff290917865e6fd6fa8ec0", "original_commit_id": "312e4ab87895811bdef0eb71cc9dfdaa9df71e9c", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "Interesting, I usually read traceback from bottom to top. So I did not want to confuse the user to see a traceback in the forward and think that the problem is in the forward.\r\nBasically, you first see the backward error, and going up you see the extra information about the forward. Not sure which one is better?", "created_at": "2018-05-19T13:55:41Z", "updated_at": "2018-11-23T15:44:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/7677#discussion_r189435078", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7677", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189435078"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7677#discussion_r189435078"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7677"}}, "body_html": "<p>Interesting, I usually read traceback from bottom to top. So I did not want to confuse the user to see a traceback in the forward and think that the problem is in the forward.<br>\nBasically, you first see the backward error, and going up you see the extra information about the forward. Not sure which one is better?</p>", "body_text": "Interesting, I usually read traceback from bottom to top. So I did not want to confuse the user to see a traceback in the forward and think that the problem is in the forward.\nBasically, you first see the backward error, and going up you see the extra information about the forward. Not sure which one is better?", "in_reply_to_id": 189362379}