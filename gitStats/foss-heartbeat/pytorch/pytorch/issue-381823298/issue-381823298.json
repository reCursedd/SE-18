{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14142", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14142/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14142/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14142/events", "html_url": "https://github.com/pytorch/pytorch/pull/14142", "id": 381823298, "node_id": "MDExOlB1bGxSZXF1ZXN0MjMxNzAzNjQz", "number": 14142, "title": "[c10d] Make NCCL backend support barrier op", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-17T03:29:48Z", "updated_at": "2018-11-23T15:55:12Z", "closed_at": "2018-11-21T05:13:51Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/14142", "html_url": "https://github.com/pytorch/pytorch/pull/14142", "diff_url": "https://github.com/pytorch/pytorch/pull/14142.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/14142.patch"}, "body_html": "<p>This is a feature request from: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"377470335\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13573\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13573/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13573\">#13573</a></p>\n<p>As the title says, this PR makes NCCL backend support barrier op.</p>\n<p>There are a couple scenarios that need to be addressed:<br>\n(1) When there is already a NCCL op happened, we need to record what GPU device(s)  the previous op happened and queue the allreduce barrier op on the same GPU device<br>\n(2) When there is no NCCL op yet, we will try to use a single GPU and separate each process from a single GPU as the best effort.</p>\n<p>As for the async work, during wait, we would like not just wait on the NCCL kernel to be completed, but also block the thread until the current stream and nccl stream return.</p>\n<p><code>test_distributed</code> should cover the test. I also manually tested both scenarios.</p>", "body_text": "This is a feature request from: #13573\nAs the title says, this PR makes NCCL backend support barrier op.\nThere are a couple scenarios that need to be addressed:\n(1) When there is already a NCCL op happened, we need to record what GPU device(s)  the previous op happened and queue the allreduce barrier op on the same GPU device\n(2) When there is no NCCL op yet, we will try to use a single GPU and separate each process from a single GPU as the best effort.\nAs for the async work, during wait, we would like not just wait on the NCCL kernel to be completed, but also block the thread until the current stream and nccl stream return.\ntest_distributed should cover the test. I also manually tested both scenarios.", "body": "This is a feature request from: https://github.com/pytorch/pytorch/issues/13573\r\n\r\nAs the title says, this PR makes NCCL backend support barrier op.\r\n\r\nThere are a couple scenarios that need to be addressed:\r\n(1) When there is already a NCCL op happened, we need to record what GPU device(s)  the previous op happened and queue the allreduce barrier op on the same GPU device\r\n(2) When there is no NCCL op yet, we will try to use a single GPU and separate each process from a single GPU as the best effort.\r\n\r\nAs for the async work, during wait, we would like not just wait on the NCCL kernel to be completed, but also block the thread until the current stream and nccl stream return.\r\n\r\n`test_distributed` should cover the test. I also manually tested both scenarios. "}