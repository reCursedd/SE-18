{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/440524384", "html_url": "https://github.com/pytorch/pytorch/pull/14142#issuecomment-440524384", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/14142", "id": 440524384, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDUyNDM4NA==", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-21T04:19:09Z", "updated_at": "2018-11-21T04:19:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> It is only the same for MPI because everything is serialized. There is simply no alternative.</p>\n<p>I'm fine with these semantics, and I agree they are useful (less code to wait for everything to complete). BUT, then it should also be documented as such, and tested as such. In a test you can kick off some async work, run a barrier, and check that all tensors have the expected results.</p>\n<p>I also think there is an alternative approach to this, given the semantic you're looking for, that is more robust. The last devices list assumes that you're using a single NCCL communicator, and that you ensure everything is completed by queuing something against that same communicator. This is only true if you have a single communicator, but breaks down when you have multiple. To fix this, you can keep a map with a single CUDA event per NCCL communicator, that you record immediately after queuing some operation on that communicator. Then when you execute a barrier, you can use a single device communicator, grab a stream, wait for the CUDA events of all other communicators, and then run the allreduce. With this approach nothing can fall through the cracks when using multiple communicators.</p>\n<p>This semantic should also be implemented for the Gloo backend.</p>", "body_text": "@teng-li It is only the same for MPI because everything is serialized. There is simply no alternative.\nI'm fine with these semantics, and I agree they are useful (less code to wait for everything to complete). BUT, then it should also be documented as such, and tested as such. In a test you can kick off some async work, run a barrier, and check that all tensors have the expected results.\nI also think there is an alternative approach to this, given the semantic you're looking for, that is more robust. The last devices list assumes that you're using a single NCCL communicator, and that you ensure everything is completed by queuing something against that same communicator. This is only true if you have a single communicator, but breaks down when you have multiple. To fix this, you can keep a map with a single CUDA event per NCCL communicator, that you record immediately after queuing some operation on that communicator. Then when you execute a barrier, you can use a single device communicator, grab a stream, wait for the CUDA events of all other communicators, and then run the allreduce. With this approach nothing can fall through the cracks when using multiple communicators.\nThis semantic should also be implemented for the Gloo backend.", "body": "@teng-li It is only the same for MPI because everything is serialized. There is simply no alternative.\r\n\r\nI'm fine with these semantics, and I agree they are useful (less code to wait for everything to complete). BUT, then it should also be documented as such, and tested as such. In a test you can kick off some async work, run a barrier, and check that all tensors have the expected results.\r\n\r\nI also think there is an alternative approach to this, given the semantic you're looking for, that is more robust. The last devices list assumes that you're using a single NCCL communicator, and that you ensure everything is completed by queuing something against that same communicator. This is only true if you have a single communicator, but breaks down when you have multiple. To fix this, you can keep a map with a single CUDA event per NCCL communicator, that you record immediately after queuing some operation on that communicator. Then when you execute a barrier, you can use a single device communicator, grab a stream, wait for the CUDA events of all other communicators, and then run the allreduce. With this approach nothing can fall through the cracks when using multiple communicators.\r\n\r\nThis semantic should also be implemented for the Gloo backend."}