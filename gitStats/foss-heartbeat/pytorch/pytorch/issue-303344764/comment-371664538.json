{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/371664538", "html_url": "https://github.com/pytorch/pytorch/issues/5628#issuecomment-371664538", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5628", "id": 371664538, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTY2NDUzOA==", "user": {"login": "ProGamerGov", "id": 10626398, "node_id": "MDQ6VXNlcjEwNjI2Mzk4", "avatar_url": "https://avatars1.githubusercontent.com/u/10626398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ProGamerGov", "html_url": "https://github.com/ProGamerGov", "followers_url": "https://api.github.com/users/ProGamerGov/followers", "following_url": "https://api.github.com/users/ProGamerGov/following{/other_user}", "gists_url": "https://api.github.com/users/ProGamerGov/gists{/gist_id}", "starred_url": "https://api.github.com/users/ProGamerGov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ProGamerGov/subscriptions", "organizations_url": "https://api.github.com/users/ProGamerGov/orgs", "repos_url": "https://api.github.com/users/ProGamerGov/repos", "events_url": "https://api.github.com/users/ProGamerGov/events{/privacy}", "received_events_url": "https://api.github.com/users/ProGamerGov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-08T23:46:30Z", "updated_at": "2018-03-09T01:06:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> Another quick question, it seems that I am having issues with the legacy lbfgs function and my feval function.</p>\n<p>Lua:    <code> return loss, grad:view(grad:nElement())</code></p>\n<p>Python: <code>return loss, grad.view(grad.nelement())</code></p>\n<p>My Python translation results in this error:</p>\n<pre><code>    x, losses = optim.lbfgs(feval, img, optim_state)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/lbfgs.py\", line 197, in lbfgs\n    x.add_(t, d)\nRuntimeError: The expanded size of the tensor (64) must match the existing size (12288) at non-singleton dimension 2\n</code></pre>\n<p>Even just running,  <code>grad = grad.view(grad.nelement()</code> will get the error:</p>\n<pre><code>RuntimeError: The expanded size of the tensor (64) must match the existing size (12288) at non-singleton dimension 2\n</code></pre>\n<p>Is there some difference between Torch7 and PyTorch here that I am missing?</p>\n<p>Edit:</p>\n<p>Looking at the error when I test adam, I think the issue could be the <code>img</code> value:</p>\n<pre><code>    x, losses = optim.adam(feval, img, optim_state)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/adam.py\", line 37, in adam\n    fx, dfdx = opfunc(x)\n</code></pre>\n<p>Maybe it's an issue with my <code>optim_state</code> value:</p>\n<pre><code>  optim_state = {\n    \"maxIter\": params.num_iterations,\n    \"verbose\": True,\n    \"tolX\":-1,\n    \"tolFun\":-1,\n  }\n</code></pre>\n<p>Or something else?</p>\n<p><a href=\"https://github.com/torch/optim/blob/master/lbfgs.lua\">https://github.com/torch/optim/blob/master/lbfgs.lua</a><br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/lbfgs.py\">https://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/lbfgs.py</a></p>", "body_text": "@SsnL Another quick question, it seems that I am having issues with the legacy lbfgs function and my feval function.\nLua:     return loss, grad:view(grad:nElement())\nPython: return loss, grad.view(grad.nelement())\nMy Python translation results in this error:\n    x, losses = optim.lbfgs(feval, img, optim_state)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/lbfgs.py\", line 197, in lbfgs\n    x.add_(t, d)\nRuntimeError: The expanded size of the tensor (64) must match the existing size (12288) at non-singleton dimension 2\n\nEven just running,  grad = grad.view(grad.nelement() will get the error:\nRuntimeError: The expanded size of the tensor (64) must match the existing size (12288) at non-singleton dimension 2\n\nIs there some difference between Torch7 and PyTorch here that I am missing?\nEdit:\nLooking at the error when I test adam, I think the issue could be the img value:\n    x, losses = optim.adam(feval, img, optim_state)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/adam.py\", line 37, in adam\n    fx, dfdx = opfunc(x)\n\nMaybe it's an issue with my optim_state value:\n  optim_state = {\n    \"maxIter\": params.num_iterations,\n    \"verbose\": True,\n    \"tolX\":-1,\n    \"tolFun\":-1,\n  }\n\nOr something else?\nhttps://github.com/torch/optim/blob/master/lbfgs.lua\nhttps://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/lbfgs.py", "body": "@SsnL Another quick question, it seems that I am having issues with the legacy lbfgs function and my feval function.\r\n\r\nLua:    ` return loss, grad:view(grad:nElement())`\r\n\r\nPython: `return loss, grad.view(grad.nelement())`\r\n\r\nMy Python translation results in this error: \r\n\r\n```\r\n    x, losses = optim.lbfgs(feval, img, optim_state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/lbfgs.py\", line 197, in lbfgs\r\n    x.add_(t, d)\r\nRuntimeError: The expanded size of the tensor (64) must match the existing size (12288) at non-singleton dimension 2\r\n```\r\n\r\nEven just running,  `grad = grad.view(grad.nelement()` will get the error: \r\n\r\n```\r\nRuntimeError: The expanded size of the tensor (64) must match the existing size (12288) at non-singleton dimension 2\r\n```\r\n\r\nIs there some difference between Torch7 and PyTorch here that I am missing?\r\n\r\n\r\nEdit: \r\n\r\nLooking at the error when I test adam, I think the issue could be the `img` value:\r\n\r\n```\r\n    x, losses = optim.adam(feval, img, optim_state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/adam.py\", line 37, in adam\r\n    fx, dfdx = opfunc(x)\r\n```\r\n\r\nMaybe it's an issue with my `optim_state` value: \r\n\r\n\r\n```\r\n  optim_state = {\r\n    \"maxIter\": params.num_iterations,\r\n    \"verbose\": True,\r\n    \"tolX\":-1,\r\n    \"tolFun\":-1,\r\n  }\r\n```\r\n\r\nOr something else? \r\n\r\nhttps://github.com/torch/optim/blob/master/lbfgs.lua\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/lbfgs.py"}