{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5865", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5865/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5865/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5865/events", "html_url": "https://github.com/pytorch/pytorch/issues/5865", "id": 306310987, "node_id": "MDU6SXNzdWUzMDYzMTA5ODc=", "number": 5865, "title": "About the weight normalization?", "user": {"login": "PkuRainBow", "id": 4639578, "node_id": "MDQ6VXNlcjQ2Mzk1Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/4639578?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PkuRainBow", "html_url": "https://github.com/PkuRainBow", "followers_url": "https://api.github.com/users/PkuRainBow/followers", "following_url": "https://api.github.com/users/PkuRainBow/following{/other_user}", "gists_url": "https://api.github.com/users/PkuRainBow/gists{/gist_id}", "starred_url": "https://api.github.com/users/PkuRainBow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PkuRainBow/subscriptions", "organizations_url": "https://api.github.com/users/PkuRainBow/orgs", "repos_url": "https://api.github.com/users/PkuRainBow/repos", "events_url": "https://api.github.com/users/PkuRainBow/events{/privacy}", "received_events_url": "https://api.github.com/users/PkuRainBow/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-19T02:06:06Z", "updated_at": "2018-03-19T18:06:05Z", "closed_at": "2018-03-19T18:06:05Z", "author_association": "NONE", "body_html": "<p>I have one question about the implementation for</p>\n<p><code>nn.utils.weight_norm()</code></p>\n<p>I read the doc, it says that,</p>\n<blockquote>\n<p>Weight normalization is a reparameterization that decouples the magnitude<br>\nof a weight tensor from its direction. This replaces the parameter specified<br>\nby <code>name</code> (e.g. \"weight\") with two parameters: one specifying the magnitude<br>\n(e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").</p>\n</blockquote>\n<p>In fact, I want to normalize the weight vectors(L2 norm=1), so I am wondering whether the following function call can ensure that the weights' L2 norm equal 1 or it will learn a magnitude factor \"weight_g\"? So How can I ensure that the \"weight_g\" equal 1?</p>\n<p><code>nn.utils.weight_norm(nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=1, dilation=1, bias=False), name='weight')</code></p>", "body_text": "I have one question about the implementation for\nnn.utils.weight_norm()\nI read the doc, it says that,\n\nWeight normalization is a reparameterization that decouples the magnitude\nof a weight tensor from its direction. This replaces the parameter specified\nby name (e.g. \"weight\") with two parameters: one specifying the magnitude\n(e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").\n\nIn fact, I want to normalize the weight vectors(L2 norm=1), so I am wondering whether the following function call can ensure that the weights' L2 norm equal 1 or it will learn a magnitude factor \"weight_g\"? So How can I ensure that the \"weight_g\" equal 1?\nnn.utils.weight_norm(nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=1, dilation=1, bias=False), name='weight')", "body": "I have one question about the implementation for \r\n\r\n`nn.utils.weight_norm()`\r\n\r\nI read the doc, it says that, \r\n\r\n>   Weight normalization is a reparameterization that decouples the magnitude\r\n>     of a weight tensor from its direction. This replaces the parameter specified\r\n>     by `name` (e.g. \"weight\") with two parameters: one specifying the magnitude\r\n>     (e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").\r\n\r\nIn fact, I want to normalize the weight vectors(L2 norm=1), so I am wondering whether the following function call can ensure that the weights' L2 norm equal 1 or it will learn a magnitude factor \"weight_g\"? So How can I ensure that the \"weight_g\" equal 1?\r\n\r\n`nn.utils.weight_norm(nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=1, dilation=1, bias=False), name='weight')`"}