{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/279220356", "html_url": "https://github.com/pytorch/pytorch/pull/658#issuecomment-279220356", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/658", "id": 279220356, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTIyMDM1Ng==", "user": {"login": "SeanNaren", "id": 6707363, "node_id": "MDQ6VXNlcjY3MDczNjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6707363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SeanNaren", "html_url": "https://github.com/SeanNaren", "followers_url": "https://api.github.com/users/SeanNaren/followers", "following_url": "https://api.github.com/users/SeanNaren/following{/other_user}", "gists_url": "https://api.github.com/users/SeanNaren/gists{/gist_id}", "starred_url": "https://api.github.com/users/SeanNaren/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SeanNaren/subscriptions", "organizations_url": "https://api.github.com/users/SeanNaren/orgs", "repos_url": "https://api.github.com/users/SeanNaren/repos", "events_url": "https://api.github.com/users/SeanNaren/events{/privacy}", "received_events_url": "https://api.github.com/users/SeanNaren/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-12T13:59:44Z", "updated_at": "2017-02-12T13:59:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm having trouble making <code>w_ih</code> and <code>b_ih</code> optional, mainly because of the weights being positional arguments, e.g:</p>\n<p><code>def RNNReLUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, skip_input=False)</code></p>\n<p>I could make them <code>None</code>, however this becomes tricky when the tensors are saved for the backward step in cuDNN, because non-tensors are filtered out via the <code>_iter_tensors</code> when saved <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L182\">here</a>.</p>\n<p>I'd prefer not to change the core function for saving tensors, but trying to support not allocating the input weight and bias may be trickier than expected. Any advice to approaching this?</p>", "body_text": "I'm having trouble making w_ih and b_ih optional, mainly because of the weights being positional arguments, e.g:\ndef RNNReLUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, skip_input=False)\nI could make them None, however this becomes tricky when the tensors are saved for the backward step in cuDNN, because non-tensors are filtered out via the _iter_tensors when saved here.\nI'd prefer not to change the core function for saving tensors, but trying to support not allocating the input weight and bias may be trickier than expected. Any advice to approaching this?", "body": "I'm having trouble making `w_ih` and `b_ih` optional, mainly because of the weights being positional arguments, e.g:\r\n\r\n`def RNNReLUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None, skip_input=False)`\r\n\r\nI could make them `None`, however this becomes tricky when the tensors are saved for the backward step in cuDNN, because non-tensors are filtered out via the `_iter_tensors` when saved [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L182).\r\n\r\nI'd prefer not to change the core function for saving tensors, but trying to support not allocating the input weight and bias may be trickier than expected. Any advice to approaching this?\r\n\r\n\r\n"}