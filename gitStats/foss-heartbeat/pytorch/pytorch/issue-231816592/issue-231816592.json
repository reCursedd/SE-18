{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1667", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1667/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1667/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1667/events", "html_url": "https://github.com/pytorch/pytorch/issues/1667", "id": 231816592, "node_id": "MDU6SXNzdWUyMzE4MTY1OTI=", "number": 1667, "title": "reset_parameters in various Modules", "user": {"login": "koz4k", "id": 1211366, "node_id": "MDQ6VXNlcjEyMTEzNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1211366?v=4", "gravatar_id": "", "url": "https://api.github.com/users/koz4k", "html_url": "https://github.com/koz4k", "followers_url": "https://api.github.com/users/koz4k/followers", "following_url": "https://api.github.com/users/koz4k/following{/other_user}", "gists_url": "https://api.github.com/users/koz4k/gists{/gist_id}", "starred_url": "https://api.github.com/users/koz4k/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/koz4k/subscriptions", "organizations_url": "https://api.github.com/users/koz4k/orgs", "repos_url": "https://api.github.com/users/koz4k/repos", "events_url": "https://api.github.com/users/koz4k/events{/privacy}", "received_events_url": "https://api.github.com/users/koz4k/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-27T17:35:07Z", "updated_at": "2017-06-07T19:10:35Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm concerned with the implementation of reset_parameters in many Modules, for example in Linear:</p>\n<pre><code>    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n</code></pre>\n<p>It seems quite arbitrary to me. If the weights were drawn from the uniform distribution with zero mean and standard deviation <code>stdv</code>, it would be LeCun initialization, a sensible choice. But <code>Tensor.uniform_(-stdv, stdv)</code> samples from the uniform distribution on the <code>(-stdv, stdv)</code> interval, which has standard deviation of <code>stdv/sqrt(3)</code>.<br>\nI think that <code>stdv</code> should be scaled by <code>sqrt(3)</code>, then we would get LeCun initialization, which would deal better with vanishing gradients.<br>\nI realize that this is only a default initalization and it can be easily overriden, but if it can work better out-o-the-box, I think it should.</p>", "body_text": "I'm concerned with the implementation of reset_parameters in many Modules, for example in Linear:\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\nIt seems quite arbitrary to me. If the weights were drawn from the uniform distribution with zero mean and standard deviation stdv, it would be LeCun initialization, a sensible choice. But Tensor.uniform_(-stdv, stdv) samples from the uniform distribution on the (-stdv, stdv) interval, which has standard deviation of stdv/sqrt(3).\nI think that stdv should be scaled by sqrt(3), then we would get LeCun initialization, which would deal better with vanishing gradients.\nI realize that this is only a default initalization and it can be easily overriden, but if it can work better out-o-the-box, I think it should.", "body": "I'm concerned with the implementation of reset_parameters in many Modules, for example in Linear:\r\n```\r\n    def reset_parameters(self):\r\n        stdv = 1. / math.sqrt(self.weight.size(1))\r\n        self.weight.data.uniform_(-stdv, stdv)\r\n        if self.bias is not None:\r\n            self.bias.data.uniform_(-stdv, stdv)\r\n```\r\nIt seems quite arbitrary to me. If the weights were drawn from the uniform distribution with zero mean and standard deviation `stdv`, it would be LeCun initialization, a sensible choice. But `Tensor.uniform_(-stdv, stdv)` samples from the uniform distribution on the `(-stdv, stdv)` interval, which has standard deviation of `stdv/sqrt(3)`.\r\nI think that `stdv` should be scaled by `sqrt(3)`, then we would get LeCun initialization, which would deal better with vanishing gradients.\r\nI realize that this is only a default initalization and it can be easily overriden, but if it can work better out-o-the-box, I think it should."}