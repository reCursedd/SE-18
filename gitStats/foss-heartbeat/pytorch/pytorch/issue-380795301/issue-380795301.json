{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13974", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13974/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13974/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13974/events", "html_url": "https://github.com/pytorch/pytorch/issues/13974", "id": 380795301, "node_id": "MDU6SXNzdWUzODA3OTUzMDE=", "number": 13974, "title": "[study] Are we calling DeviceGuard too much?", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-14T16:59:51Z", "updated_at": "2018-11-15T16:25:54Z", "closed_at": "2018-11-15T16:25:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is a followup to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"373523929\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13049\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13049/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13049\">#13049</a>, where I discovered that one big aten overhead is calling DeviceGuard a lot. I've been removing unnecessary DeviceGuard calls, but reached a point where it is not obvious what can be removed. In this issue I investigate exactly where DeviceGuard is being called to come up with potential solutions to improve the overhead.</p>\n<h3>Some numbers</h3>\n<ul>\n<li>benchmarking DeviceGuard with gbenchmark: ~200ns per DeviceGuard. Some DeviceGuards don't actually cost anything, but let's ignore those for now.</li>\n<li>cudnn/aten fused implementation/JIT LSTM with typical sizes (100 seq_len, 64 batch_size, 512 input_size, 512 hidden_size) take the following times on my machine:</li>\n</ul>\n<pre><code>Benchmarking LSTMs... (ms)\n            name          avg_fwd          std_fwd          avg_bwd          std_bwd\n           cudnn            5.549           0.1538            10.09           0.2957\n            aten            6.822            0.337            20.07            0.335\n             jit            9.152           0.1486            15.49          0.06995\n</code></pre>\n<table>\n<thead>\n<tr>\n<th>LSTM</th>\n<th># DeviceGuard calls</th>\n<th>Total theoretical overhead (us)</th>\n<th>% of total runtime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>jit</td>\n<td>2580</td>\n<td>516</td>\n<td>2.5</td>\n</tr>\n<tr>\n<td>aten</td>\n<td>3643</td>\n<td>728</td>\n<td>2.6</td>\n</tr>\n</tbody>\n</table>\n<h3>Breakdown of DeviceGuard top callsites</h3>\n<p>aten:</p>\n<pre><code>100     at::CUDAFloatType::_thnn_fused_lstm_cell(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;) const\n100     at::CUDAFloatType::_thnn_fused_lstm_cell_backward(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, bool)\nconst\n100     at::TypeDefault::sum(at::Tensor const&amp;, at::ArrayRef&lt;long&gt;, bool) const\n200     at::TypeDefault::matmul(at::Tensor const&amp;, at::Tensor const&amp;) const\n495     torch::autograd::InputBuffer::add(unsigned long, torch::autograd::Variable)\n495     at::TypeDefault::add(at::Tensor const&amp;, at::Tensor const&amp;, at::Scalar) const\n495     at::TensorIterator::allocate_outputs()\n499     at::TypeDefault::mm(at::Tensor const&amp;, at::Tensor const&amp;) const\n499     at::native::resize_cuda_(at::Tensor&amp;, at::ArrayRef&lt;long&gt;)\n600     at::CUDAFloatType::empty(at::ArrayRef&lt;long&gt;, at::TensorOptions const&amp;) const\n</code></pre>\n<p>jit:</p>\n<pre><code>32      at::TypeDefault::sum(at::Tensor const&amp;, at::ArrayRef&lt;long&gt;, bool) const\n49      at::TypeDefault::cat(at::ArrayRef&lt;at::Tensor&gt;, long) const\n75      torch::autograd::InputBuffer::add(unsigned long, torch::autograd::Variable)\n75      at::TypeDefault::add(at::Tensor const&amp;, at::Tensor const&amp;, at::Scalar) const\n75      at::TensorIterator::allocate_outputs()\n312     torch::jit::fuser::cuda::FusedKernelCUDA::launch_raw(unsigned int, std::vector&lt;void*, std::allocator&lt;void*&gt; &gt;&amp;) const\n432     at::TypeDefault::mm(at::Tensor const&amp;, at::Tensor const&amp;) const\n432     at::native::resize_cuda_(at::Tensor&amp;, at::ArrayRef&lt;long&gt;)\n1045    at::CUDAFloatType::empty(at::ArrayRef&lt;long&gt;, at::TensorOptions const&amp;) const\n</code></pre>\n<p><a href=\"https://gist.github.com/zou3519/d042e5280e150033adfabf9da71c7afc\">DeviceGuard callsites for aten LSTM forward pass + backward pass full data</a><br>\n<a href=\"https://gist.github.com/zou3519/4c878d18158fbd3050f647eefb373981\">DeviceGuard callsites for JIT LSTM forward pass + backward pass full data</a></p>\n<h3>Analysis</h3>\n<p>Since these are all running on the same device, we don't need all the DeviceGuard calls in theory.</p>\n<p>There used to be around 3x as many DeviceGuard calls for JIT LSTM. However, since the overhead is only 2.5% of the runtime now, one solution is to just do nothing about the DeviceGuard calls and leave them as-is.</p>", "body_text": "This is a followup to #13049, where I discovered that one big aten overhead is calling DeviceGuard a lot. I've been removing unnecessary DeviceGuard calls, but reached a point where it is not obvious what can be removed. In this issue I investigate exactly where DeviceGuard is being called to come up with potential solutions to improve the overhead.\nSome numbers\n\nbenchmarking DeviceGuard with gbenchmark: ~200ns per DeviceGuard. Some DeviceGuards don't actually cost anything, but let's ignore those for now.\ncudnn/aten fused implementation/JIT LSTM with typical sizes (100 seq_len, 64 batch_size, 512 input_size, 512 hidden_size) take the following times on my machine:\n\nBenchmarking LSTMs... (ms)\n            name          avg_fwd          std_fwd          avg_bwd          std_bwd\n           cudnn            5.549           0.1538            10.09           0.2957\n            aten            6.822            0.337            20.07            0.335\n             jit            9.152           0.1486            15.49          0.06995\n\n\n\n\nLSTM\n# DeviceGuard calls\nTotal theoretical overhead (us)\n% of total runtime\n\n\n\n\njit\n2580\n516\n2.5\n\n\naten\n3643\n728\n2.6\n\n\n\nBreakdown of DeviceGuard top callsites\naten:\n100     at::CUDAFloatType::_thnn_fused_lstm_cell(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&) const\n100     at::CUDAFloatType::_thnn_fused_lstm_cell_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, bool)\nconst\n100     at::TypeDefault::sum(at::Tensor const&, at::ArrayRef<long>, bool) const\n200     at::TypeDefault::matmul(at::Tensor const&, at::Tensor const&) const\n495     torch::autograd::InputBuffer::add(unsigned long, torch::autograd::Variable)\n495     at::TypeDefault::add(at::Tensor const&, at::Tensor const&, at::Scalar) const\n495     at::TensorIterator::allocate_outputs()\n499     at::TypeDefault::mm(at::Tensor const&, at::Tensor const&) const\n499     at::native::resize_cuda_(at::Tensor&, at::ArrayRef<long>)\n600     at::CUDAFloatType::empty(at::ArrayRef<long>, at::TensorOptions const&) const\n\njit:\n32      at::TypeDefault::sum(at::Tensor const&, at::ArrayRef<long>, bool) const\n49      at::TypeDefault::cat(at::ArrayRef<at::Tensor>, long) const\n75      torch::autograd::InputBuffer::add(unsigned long, torch::autograd::Variable)\n75      at::TypeDefault::add(at::Tensor const&, at::Tensor const&, at::Scalar) const\n75      at::TensorIterator::allocate_outputs()\n312     torch::jit::fuser::cuda::FusedKernelCUDA::launch_raw(unsigned int, std::vector<void*, std::allocator<void*> >&) const\n432     at::TypeDefault::mm(at::Tensor const&, at::Tensor const&) const\n432     at::native::resize_cuda_(at::Tensor&, at::ArrayRef<long>)\n1045    at::CUDAFloatType::empty(at::ArrayRef<long>, at::TensorOptions const&) const\n\nDeviceGuard callsites for aten LSTM forward pass + backward pass full data\nDeviceGuard callsites for JIT LSTM forward pass + backward pass full data\nAnalysis\nSince these are all running on the same device, we don't need all the DeviceGuard calls in theory.\nThere used to be around 3x as many DeviceGuard calls for JIT LSTM. However, since the overhead is only 2.5% of the runtime now, one solution is to just do nothing about the DeviceGuard calls and leave them as-is.", "body": "This is a followup to #13049, where I discovered that one big aten overhead is calling DeviceGuard a lot. I've been removing unnecessary DeviceGuard calls, but reached a point where it is not obvious what can be removed. In this issue I investigate exactly where DeviceGuard is being called to come up with potential solutions to improve the overhead.\r\n\r\n### Some numbers\r\n- benchmarking DeviceGuard with gbenchmark: ~200ns per DeviceGuard. Some DeviceGuards don't actually cost anything, but let's ignore those for now.\r\n- cudnn/aten fused implementation/JIT LSTM with typical sizes (100 seq_len, 64 batch_size, 512 input_size, 512 hidden_size) take the following times on my machine:\r\n```\r\nBenchmarking LSTMs... (ms)\r\n            name          avg_fwd          std_fwd          avg_bwd          std_bwd\r\n           cudnn            5.549           0.1538            10.09           0.2957\r\n            aten            6.822            0.337            20.07            0.335\r\n             jit            9.152           0.1486            15.49          0.06995\r\n```\r\n\r\n\r\n| LSTM | # DeviceGuard calls | Total theoretical overhead (us) | % of total runtime |\r\n| ------------- | ------------- | ---- | ---- |\r\n| jit  | 2580  | 516 | 2.5 |\r\n| aten  | 3643  | 728 | 2.6 |\r\n\r\n### Breakdown of DeviceGuard top callsites\r\naten:\r\n```\r\n100     at::CUDAFloatType::_thnn_fused_lstm_cell(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&) const\r\n100     at::CUDAFloatType::_thnn_fused_lstm_cell_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, bool)\r\nconst\r\n100     at::TypeDefault::sum(at::Tensor const&, at::ArrayRef<long>, bool) const\r\n200     at::TypeDefault::matmul(at::Tensor const&, at::Tensor const&) const\r\n495     torch::autograd::InputBuffer::add(unsigned long, torch::autograd::Variable)\r\n495     at::TypeDefault::add(at::Tensor const&, at::Tensor const&, at::Scalar) const\r\n495     at::TensorIterator::allocate_outputs()\r\n499     at::TypeDefault::mm(at::Tensor const&, at::Tensor const&) const\r\n499     at::native::resize_cuda_(at::Tensor&, at::ArrayRef<long>)\r\n600     at::CUDAFloatType::empty(at::ArrayRef<long>, at::TensorOptions const&) const\r\n```\r\njit:\r\n```\r\n32      at::TypeDefault::sum(at::Tensor const&, at::ArrayRef<long>, bool) const\r\n49      at::TypeDefault::cat(at::ArrayRef<at::Tensor>, long) const\r\n75      torch::autograd::InputBuffer::add(unsigned long, torch::autograd::Variable)\r\n75      at::TypeDefault::add(at::Tensor const&, at::Tensor const&, at::Scalar) const\r\n75      at::TensorIterator::allocate_outputs()\r\n312     torch::jit::fuser::cuda::FusedKernelCUDA::launch_raw(unsigned int, std::vector<void*, std::allocator<void*> >&) const\r\n432     at::TypeDefault::mm(at::Tensor const&, at::Tensor const&) const\r\n432     at::native::resize_cuda_(at::Tensor&, at::ArrayRef<long>)\r\n1045    at::CUDAFloatType::empty(at::ArrayRef<long>, at::TensorOptions const&) const\r\n```\r\n\r\n[DeviceGuard callsites for aten LSTM forward pass + backward pass full data](https://gist.github.com/zou3519/d042e5280e150033adfabf9da71c7afc)\r\n[DeviceGuard callsites for JIT LSTM forward pass + backward pass full data](\r\nhttps://gist.github.com/zou3519/4c878d18158fbd3050f647eefb373981)\r\n\r\n### Analysis\r\nSince these are all running on the same device, we don't need all the DeviceGuard calls in theory. \r\n\r\nThere used to be around 3x as many DeviceGuard calls for JIT LSTM. However, since the overhead is only 2.5% of the runtime now, one solution is to just do nothing about the DeviceGuard calls and leave them as-is.\r\n"}