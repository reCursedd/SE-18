{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/366749409", "html_url": "https://github.com/pytorch/pytorch/pull/3043#issuecomment-366749409", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3043", "id": 366749409, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Njc0OTQwOQ==", "user": {"login": "vfdev-5", "id": 2459423, "node_id": "MDQ6VXNlcjI0NTk0MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2459423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vfdev-5", "html_url": "https://github.com/vfdev-5", "followers_url": "https://api.github.com/users/vfdev-5/followers", "following_url": "https://api.github.com/users/vfdev-5/following{/other_user}", "gists_url": "https://api.github.com/users/vfdev-5/gists{/gist_id}", "starred_url": "https://api.github.com/users/vfdev-5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vfdev-5/subscriptions", "organizations_url": "https://api.github.com/users/vfdev-5/orgs", "repos_url": "https://api.github.com/users/vfdev-5/repos", "events_url": "https://api.github.com/users/vfdev-5/events{/privacy}", "received_events_url": "https://api.github.com/users/vfdev-5/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-19T16:53:36Z", "updated_at": "2018-02-19T16:54:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10509755\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/isaykatsman\">@isaykatsman</a> there is also a problem with</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> (i, l_type), l_name <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(<span class=\"pl-c1\">enumerate</span>(summary), names):</pre></div>\n<p>as names can be not 'aligned' with modules firstly appeared in forward. For example,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">TestModel</span>(<span class=\"pl-e\">Module</span>):\n   <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(TestModel, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()                \n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> POOL IS DECLARED AS THE LAST MODULE</span>\n        <span class=\"pl-c1\">self</span>.pool <span class=\"pl-k\">=</span> MaxPool2d(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n        \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> POOL IS CALLED THE FIRST</span>\n        x2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(x)\n        x1 <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv1(x))\n        x1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv2(x1)\n        x2 <span class=\"pl-k\">=</span> x2 <span class=\"pl-k\">+</span> x1\n        <span class=\"pl-k\">return</span> x2</pre></div>\n<p>this gives the following summary:</p>\n<pre><code>________________________________________________________________________________________________________\nLayer (type)               Output Shape               Param #                   \n========================================================================================================\nconv1 (MaxPool2d)        [None, 3, 112, 112]        0                         \n________________________________________________________________________________________________________\nconv2 (Conv2d)             [None, 3, 224, 224]        84                        \n________________________________________________________________________________________________________\npool (Conv2d)                [None, 3, 112, 112]        84                        \n========================================================================================================\nTotal params: 168\nTrainable params: 168\nNon-trainable params: 0\n</code></pre>", "body_text": "@isaykatsman there is also a problem with\nfor (i, l_type), l_name in zip(enumerate(summary), names):\nas names can be not 'aligned' with modules firstly appeared in forward. For example,\nclass TestModel(Module):\n   def __init__(self):\n        super(TestModel, self).__init__()                \n        self.conv1 = Conv2d(3, 3, kernel_size=(3, 3), padding=1)\n        self.conv2 = Conv2d(3, 3, kernel_size=(3, 3), padding=1, stride=2)\n        # POOL IS DECLARED AS THE LAST MODULE\n        self.pool = MaxPool2d(2, 2)\n        \n    def forward(self, x):\n        # POOL IS CALLED THE FIRST\n        x2 = self.pool(x)\n        x1 = F.relu(self.conv1(x))\n        x1 = self.conv2(x1)\n        x2 = x2 + x1\n        return x2\nthis gives the following summary:\n________________________________________________________________________________________________________\nLayer (type)               Output Shape               Param #                   \n========================================================================================================\nconv1 (MaxPool2d)        [None, 3, 112, 112]        0                         \n________________________________________________________________________________________________________\nconv2 (Conv2d)             [None, 3, 224, 224]        84                        \n________________________________________________________________________________________________________\npool (Conv2d)                [None, 3, 112, 112]        84                        \n========================================================================================================\nTotal params: 168\nTrainable params: 168\nNon-trainable params: 0", "body": "@isaykatsman there is also a problem with \r\n```python\r\nfor (i, l_type), l_name in zip(enumerate(summary), names):\r\n```\r\nas names can be not 'aligned' with modules firstly appeared in forward. For example, \r\n```python\r\nclass TestModel(Module):\r\n   def __init__(self):\r\n        super(TestModel, self).__init__()                \r\n        self.conv1 = Conv2d(3, 3, kernel_size=(3, 3), padding=1)\r\n        self.conv2 = Conv2d(3, 3, kernel_size=(3, 3), padding=1, stride=2)\r\n        # POOL IS DECLARED AS THE LAST MODULE\r\n        self.pool = MaxPool2d(2, 2)\r\n        \r\n    def forward(self, x):\r\n        # POOL IS CALLED THE FIRST\r\n        x2 = self.pool(x)\r\n        x1 = F.relu(self.conv1(x))\r\n        x1 = self.conv2(x1)\r\n        x2 = x2 + x1\r\n        return x2\r\n```\r\nthis gives the following summary:\r\n```\r\n________________________________________________________________________________________________________\r\nLayer (type)               Output Shape               Param #                   \r\n========================================================================================================\r\nconv1 (MaxPool2d)        [None, 3, 112, 112]        0                         \r\n________________________________________________________________________________________________________\r\nconv2 (Conv2d)             [None, 3, 224, 224]        84                        \r\n________________________________________________________________________________________________________\r\npool (Conv2d)                [None, 3, 112, 112]        84                        \r\n========================================================================================================\r\nTotal params: 168\r\nTrainable params: 168\r\nNon-trainable params: 0\r\n```"}