{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367193200", "html_url": "https://github.com/pytorch/pytorch/pull/3043#issuecomment-367193200", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3043", "id": 367193200, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzE5MzIwMA==", "user": {"login": "Erotemic", "id": 3186211, "node_id": "MDQ6VXNlcjMxODYyMTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3186211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erotemic", "html_url": "https://github.com/Erotemic", "followers_url": "https://api.github.com/users/Erotemic/followers", "following_url": "https://api.github.com/users/Erotemic/following{/other_user}", "gists_url": "https://api.github.com/users/Erotemic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erotemic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erotemic/subscriptions", "organizations_url": "https://api.github.com/users/Erotemic/orgs", "repos_url": "https://api.github.com/users/Erotemic/repos", "events_url": "https://api.github.com/users/Erotemic/events{/privacy}", "received_events_url": "https://api.github.com/users/Erotemic/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-21T02:27:48Z", "updated_at": "2018-02-21T02:27:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10509755\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/isaykatsman\">@isaykatsman</a> I think it still might be useful to implement a <code>get_output_shape_for</code> method for classes like <code>Conv2d</code>, <code>Linear</code>, etc. Would something like that be in the scope of the core library?</p>\n<p>For example adding the function to <code>_ConvNd</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_output_shape_for</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_shape</span>, <span class=\"pl-smi\">n</span>):\n        N, C_in, <span class=\"pl-k\">*</span><span class=\"pl-c1\">DIMS_in</span> <span class=\"pl-k\">=</span> input_shape\n        C_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.out_channels\n        padding <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.padding\n        stride <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.stride\n        dilation <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.dilation\n        kernel_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.kernel_size\n\n        <span class=\"pl-c1\">DIMS_out</span> <span class=\"pl-k\">=</span> [\n            math.floor((D_in  <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> padding[i] <span class=\"pl-k\">-</span> dilation[i] <span class=\"pl-k\">*</span> (kernel_size[i] <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">/</span> stride[i] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)\n            <span class=\"pl-k\">for</span> i, D_in <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">DIMS_in</span>)\n        ]\n        output_shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>([N, C_out] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">DIMS_out</span>)\n        <span class=\"pl-k\">return</span> output_shape</pre></div>\n<p>and then adding the function to <code>Conv2d</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">get_output_shape_for</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_shape</span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">super</span>(Conv2d, <span class=\"pl-c1\">self</span>).get_output_shape_for(input_shape, <span class=\"pl-c1\">2</span>)</pre></div>", "body_text": "@apaszke and @isaykatsman I think it still might be useful to implement a get_output_shape_for method for classes like Conv2d, Linear, etc. Would something like that be in the scope of the core library?\nFor example adding the function to _ConvNd:\n    def get_output_shape_for(self, input_shape, n):\n        N, C_in, *DIMS_in = input_shape\n        C_out = self.out_channels\n        padding = self.padding\n        stride = self.stride\n        dilation = self.dilation\n        kernel_size = self.kernel_size\n\n        DIMS_out = [\n            math.floor((D_in  + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1) / stride[i] + 1)\n            for i, D_in in enumerate(DIMS_in)\n        ]\n        output_shape = tuple([N, C_out] + DIMS_out)\n        return output_shape\nand then adding the function to Conv2d:\ndef get_output_shape_for(self, input_shape):\n    return super(Conv2d, self).get_output_shape_for(input_shape, 2)", "body": "@apaszke and @isaykatsman I think it still might be useful to implement a `get_output_shape_for` method for classes like `Conv2d`, `Linear`, etc. Would something like that be in the scope of the core library?\r\n\r\nFor example adding the function to `_ConvNd`:\r\n\r\n```python\r\n    def get_output_shape_for(self, input_shape, n):\r\n        N, C_in, *DIMS_in = input_shape\r\n        C_out = self.out_channels\r\n        padding = self.padding\r\n        stride = self.stride\r\n        dilation = self.dilation\r\n        kernel_size = self.kernel_size\r\n\r\n        DIMS_out = [\r\n            math.floor((D_in  + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1) / stride[i] + 1)\r\n            for i, D_in in enumerate(DIMS_in)\r\n        ]\r\n        output_shape = tuple([N, C_out] + DIMS_out)\r\n        return output_shape\r\n```\r\n\r\nand then adding the function to `Conv2d`: \r\n```python\r\n\r\ndef get_output_shape_for(self, input_shape):\r\n    return super(Conv2d, self).get_output_shape_for(input_shape, 2)\r\n```\r\n"}