{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367014512", "html_url": "https://github.com/pytorch/pytorch/pull/3043#issuecomment-367014512", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3043", "id": 367014512, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzAxNDUxMg==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-20T15:30:15Z", "updated_at": "2018-02-20T15:30:52Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10509755\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/isaykatsman\">@isaykatsman</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2459423\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vfdev-5\">@vfdev-5</a> thanks for working on this, but I really feel like this extension is not in the scope of the core library. Keras has a much simpler offline/symbolic model of computation, where all operations are pre-declared as modules before hand, and their connections are fully specified <em>before the run time</em> (define-and-run). This allows it to go over the whole network and print a complete summary. In fact this was the design created in Lua Torch, and it was limiting in a number of applications, which is why we've decided to relax those constraints when designing PyTorch.</p>\n<p>We're aware of the implications it has (tracing is hard, see our JIT compiler effort), but recent models rarely are simple sequential stacks of layers, and have much more complicated and rich connectivity patterns and use a variety of ops. This is why most frameworks refer to them as graphs. This linear summary is fine for very very simple convnets, but in any other case I don't think it's a good representation, and you're much better off using a tool like <a href=\"https://github.com/szagoruyko/pytorchviz\">pytorchviz</a>, or <a href=\"https://github.com/lanpa/tensorboard-pytorch\">TensorBoard bindings</a>. Additionally, <code>nn</code> is not longer the \"dominant\" layer of abstraction (as it was in Lua Torch or is in Keras) - in PyTorch it's only a bit of syntatic sugar over the autograd layer. It is unable to see a lot of operations (e.g. a <code>+</code> written inline in <code>forward</code>, or any application of the <code>functional</code> API).</p>\n<p>If this is useful to you/a larger group of users, I'd encourage you to package it separately as an extension package. Hope you understand.</p>", "body_text": "@isaykatsman @vfdev-5 thanks for working on this, but I really feel like this extension is not in the scope of the core library. Keras has a much simpler offline/symbolic model of computation, where all operations are pre-declared as modules before hand, and their connections are fully specified before the run time (define-and-run). This allows it to go over the whole network and print a complete summary. In fact this was the design created in Lua Torch, and it was limiting in a number of applications, which is why we've decided to relax those constraints when designing PyTorch.\nWe're aware of the implications it has (tracing is hard, see our JIT compiler effort), but recent models rarely are simple sequential stacks of layers, and have much more complicated and rich connectivity patterns and use a variety of ops. This is why most frameworks refer to them as graphs. This linear summary is fine for very very simple convnets, but in any other case I don't think it's a good representation, and you're much better off using a tool like pytorchviz, or TensorBoard bindings. Additionally, nn is not longer the \"dominant\" layer of abstraction (as it was in Lua Torch or is in Keras) - in PyTorch it's only a bit of syntatic sugar over the autograd layer. It is unable to see a lot of operations (e.g. a + written inline in forward, or any application of the functional API).\nIf this is useful to you/a larger group of users, I'd encourage you to package it separately as an extension package. Hope you understand.", "body": "@isaykatsman @vfdev-5 thanks for working on this, but I really feel like this extension is not in the scope of the core library. Keras has a much simpler offline/symbolic model of computation, where all operations are pre-declared as modules before hand, and their connections are fully specified *before the run time* (define-and-run). This allows it to go over the whole network and print a complete summary. In fact this was the design created in Lua Torch, and it was limiting in a number of applications, which is why we've decided to relax those constraints when designing PyTorch.\r\n\r\nWe're aware of the implications it has (tracing is hard, see our JIT compiler effort), but recent models rarely are simple sequential stacks of layers, and have much more complicated and rich connectivity patterns and use a variety of ops. This is why most frameworks refer to them as graphs. This linear summary is fine for very very simple convnets, but in any other case I don't think it's a good representation, and you're much better off using a tool like [pytorchviz](https://github.com/szagoruyko/pytorchviz), or [TensorBoard bindings](https://github.com/lanpa/tensorboard-pytorch). Additionally, `nn` is not longer the \"dominant\" layer of abstraction (as it was in Lua Torch or is in Keras) - in PyTorch it's only a bit of syntatic sugar over the autograd layer. It is unable to see a lot of operations (e.g. a `+` written inline in `forward`, or any application of the `functional` API).\r\n\r\nIf this is useful to you/a larger group of users, I'd encourage you to package it separately as an extension package. Hope you understand."}