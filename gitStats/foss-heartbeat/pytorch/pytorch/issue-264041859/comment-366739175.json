{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/366739175", "html_url": "https://github.com/pytorch/pytorch/pull/3043#issuecomment-366739175", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3043", "id": 366739175, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjczOTE3NQ==", "user": {"login": "vfdev-5", "id": 2459423, "node_id": "MDQ6VXNlcjI0NTk0MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2459423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vfdev-5", "html_url": "https://github.com/vfdev-5", "followers_url": "https://api.github.com/users/vfdev-5/followers", "following_url": "https://api.github.com/users/vfdev-5/following{/other_user}", "gists_url": "https://api.github.com/users/vfdev-5/gists{/gist_id}", "starred_url": "https://api.github.com/users/vfdev-5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vfdev-5/subscriptions", "organizations_url": "https://api.github.com/users/vfdev-5/orgs", "repos_url": "https://api.github.com/users/vfdev-5/repos", "events_url": "https://api.github.com/users/vfdev-5/events{/privacy}", "received_events_url": "https://api.github.com/users/vfdev-5/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-19T16:15:34Z", "updated_at": "2018-02-19T16:16:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10509755\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/isaykatsman\">@isaykatsman</a> concerning multiple heads in keras, probably you saw, that they use <a href=\"https://github.com/keras-team/keras/blob/master/keras/utils/layer_utils.py#L84\">'Connected to'</a> to show connections between layers. For example, ResNet50</p>\n<pre><code>__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n__________________________________________________________________________________________________\nconv1 (Conv2D)                  (None, 112, 112, 64) 9472        input_2[0][0]                    \n__________________________________________________________________________________________________\nbn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_1[0][0]               \n__________________________________________________________________________________________________\nres2a_branch2a (Conv2D)         (None, 55, 55, 64)   4160        max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nbn2a_branch2a (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 55, 55, 64)   0           bn2a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2a_branch2b (Conv2D)         (None, 55, 55, 64)   36928       activation_2[0][0]               \n__________________________________________________________________________________________________\nbn2a_branch2b (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 55, 55, 64)   0           bn2a_branch2b[0][0]              \n__________________________________________________________________________________________________\n....\n</code></pre>\n<p>If you could integrate this too, will be cool :)</p>", "body_text": "@isaykatsman concerning multiple heads in keras, probably you saw, that they use 'Connected to' to show connections between layers. For example, ResNet50\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n__________________________________________________________________________________________________\nconv1 (Conv2D)                  (None, 112, 112, 64) 9472        input_2[0][0]                    \n__________________________________________________________________________________________________\nbn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_1[0][0]               \n__________________________________________________________________________________________________\nres2a_branch2a (Conv2D)         (None, 55, 55, 64)   4160        max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nbn2a_branch2a (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 55, 55, 64)   0           bn2a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2a_branch2b (Conv2D)         (None, 55, 55, 64)   36928       activation_2[0][0]               \n__________________________________________________________________________________________________\nbn2a_branch2b (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 55, 55, 64)   0           bn2a_branch2b[0][0]              \n__________________________________________________________________________________________________\n....\n\nIf you could integrate this too, will be cool :)", "body": "@isaykatsman concerning multiple heads in keras, probably you saw, that they use ['Connected to'](https://github.com/keras-team/keras/blob/master/keras/utils/layer_utils.py#L84) to show connections between layers. For example, ResNet50\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_2 (InputLayer)            (None, 224, 224, 3)  0                                            \r\n__________________________________________________________________________________________________\r\nconv1 (Conv2D)                  (None, 112, 112, 64) 9472        input_2[0][0]                    \r\n__________________________________________________________________________________________________\r\nbn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \r\n__________________________________________________________________________________________________\r\nactivation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_1[0][0]               \r\n__________________________________________________________________________________________________\r\nres2a_branch2a (Conv2D)         (None, 55, 55, 64)   4160        max_pooling2d_1[0][0]            \r\n__________________________________________________________________________________________________\r\nbn2a_branch2a (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2a[0][0]             \r\n__________________________________________________________________________________________________\r\nactivation_2 (Activation)       (None, 55, 55, 64)   0           bn2a_branch2a[0][0]              \r\n__________________________________________________________________________________________________\r\nres2a_branch2b (Conv2D)         (None, 55, 55, 64)   36928       activation_2[0][0]               \r\n__________________________________________________________________________________________________\r\nbn2a_branch2b (BatchNormalizati (None, 55, 55, 64)   256         res2a_branch2b[0][0]             \r\n__________________________________________________________________________________________________\r\nactivation_3 (Activation)       (None, 55, 55, 64)   0           bn2a_branch2b[0][0]              \r\n__________________________________________________________________________________________________\r\n....\r\n```\r\nIf you could integrate this too, will be cool :)\r\n"}