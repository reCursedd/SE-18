{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2068", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2068/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2068/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2068/events", "html_url": "https://github.com/pytorch/pytorch/issues/2068", "id": 242457666, "node_id": "MDU6SXNzdWUyNDI0NTc2NjY=", "number": 2068, "title": "Memory leak after index_select and assignment", "user": {"login": "hongzimao", "id": 5906100, "node_id": "MDQ6VXNlcjU5MDYxMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5906100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongzimao", "html_url": "https://github.com/hongzimao", "followers_url": "https://api.github.com/users/hongzimao/followers", "following_url": "https://api.github.com/users/hongzimao/following{/other_user}", "gists_url": "https://api.github.com/users/hongzimao/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongzimao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongzimao/subscriptions", "organizations_url": "https://api.github.com/users/hongzimao/orgs", "repos_url": "https://api.github.com/users/hongzimao/repos", "events_url": "https://api.github.com/users/hongzimao/events{/privacy}", "received_events_url": "https://api.github.com/users/hongzimao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-12T17:31:52Z", "updated_at": "2017-07-13T05:43:23Z", "closed_at": "2017-07-13T05:43:23Z", "author_association": "NONE", "body_html": "<p>It seems the memory quickly gets occupied when I run the following testing code</p>\n<pre><code>import numpy as np\nimport torch\nfrom torch.autograd import Variable\n\na = np.random.rand(1000, 1000)\n\nfor i in xrange(10000):\n    b = Variable(torch.from_numpy(a).clone(), requires_grad=False)\n    c = torch.index_select(b, 0, Variable(torch.LongTensor([4])))\n\n    for t in xrange(1000):\n        b[t, :] = c\n</code></pre>\n<p>within 2 minutes it eats up 128GB of my memory.</p>\n<p>Basically what I wanted to do is dynamically modify parts of a tensor from some other parts of the same tensor. There's a severe memory leak when I do that. I may be very likely doing something not intended for the pytorch framework but not sure what the workaround is.</p>\n<p>I'm using pytorch version 0.1.12_2, installed from pip. OS is Ubuntu 16.04.2 LTS.</p>\n<p>Thanks for the help!</p>", "body_text": "It seems the memory quickly gets occupied when I run the following testing code\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\na = np.random.rand(1000, 1000)\n\nfor i in xrange(10000):\n    b = Variable(torch.from_numpy(a).clone(), requires_grad=False)\n    c = torch.index_select(b, 0, Variable(torch.LongTensor([4])))\n\n    for t in xrange(1000):\n        b[t, :] = c\n\nwithin 2 minutes it eats up 128GB of my memory.\nBasically what I wanted to do is dynamically modify parts of a tensor from some other parts of the same tensor. There's a severe memory leak when I do that. I may be very likely doing something not intended for the pytorch framework but not sure what the workaround is.\nI'm using pytorch version 0.1.12_2, installed from pip. OS is Ubuntu 16.04.2 LTS.\nThanks for the help!", "body": "It seems the memory quickly gets occupied when I run the following testing code\r\n\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\na = np.random.rand(1000, 1000)\r\n\r\nfor i in xrange(10000):\r\n    b = Variable(torch.from_numpy(a).clone(), requires_grad=False)\r\n    c = torch.index_select(b, 0, Variable(torch.LongTensor([4])))\r\n\r\n    for t in xrange(1000):\r\n        b[t, :] = c\r\n```\r\n\r\nwithin 2 minutes it eats up 128GB of my memory. \r\n\r\nBasically what I wanted to do is dynamically modify parts of a tensor from some other parts of the same tensor. There's a severe memory leak when I do that. I may be very likely doing something not intended for the pytorch framework but not sure what the workaround is. \r\n\r\nI'm using pytorch version 0.1.12_2, installed from pip. OS is Ubuntu 16.04.2 LTS.\r\n\r\nThanks for the help!"}