{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6347", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6347/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6347/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6347/events", "html_url": "https://github.com/pytorch/pytorch/issues/6347", "id": 311916544, "node_id": "MDU6SXNzdWUzMTE5MTY1NDQ=", "number": 6347, "title": "multi-gpu error sgd backward pass learning rate reduced by half", "user": {"login": "kirk86", "id": 2902390, "node_id": "MDQ6VXNlcjI5MDIzOTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2902390?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kirk86", "html_url": "https://github.com/kirk86", "followers_url": "https://api.github.com/users/kirk86/followers", "following_url": "https://api.github.com/users/kirk86/following{/other_user}", "gists_url": "https://api.github.com/users/kirk86/gists{/gist_id}", "starred_url": "https://api.github.com/users/kirk86/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kirk86/subscriptions", "organizations_url": "https://api.github.com/users/kirk86/orgs", "repos_url": "https://api.github.com/users/kirk86/repos", "events_url": "https://api.github.com/users/kirk86/events{/privacy}", "received_events_url": "https://api.github.com/users/kirk86/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-04-06T09:56:26Z", "updated_at": "2018-06-11T18:14:42Z", "closed_at": "2018-06-11T18:14:42Z", "author_association": "NONE", "body_html": "<h2>PyTorch GitHub Issues Guidelines</h2>\n<ul>\n<li>PyTorch</li>\n<li>OS: ubuntu 16.04</li>\n<li>PyTorch version: 0.4</li>\n<li>How you installed PyTorch (source):</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: cuda8/cudnn6</li>\n<li>GPU models and configuration: 1080Ti</li>\n<li>GCC version (if compiling from source): gcc-7</li>\n<li>CMake version: 3.5.1</li>\n<li>Build command you used (if compiling from source):</li>\n</ul>\n<pre><code>export CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n\n# Install basic dependencies\nconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing\n\n# Add LAPACK support for the GPU\nconda install -c pytorch magma-cuda80 \npython setup.py install\n</code></pre>\n<p>In addition, including the following information will also be very helpful for us to diagnose the problem:</p>\n<ul>\n<li>A script to reproduce the bug. Please try to provide as minimal of a test case as possible.</li>\n</ul>\n<pre><code>import torch\nimport torchvision\nfrom torchvision.models import densenet201\n\nmodel = densenet201()\n# set the optimizer to SGD\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n                         lr=lr/2 if (num_iters % iter_smooth == 0) else lr, momentum=0.9, weight_decay=0.0001)\nrate = get_learning_rate(optimizer)  #load all except learning rate\noptimizer.load_state_dict(checkpoint['optimizer'])\nadjust_learning_rate(optimizer, rate)\n\n# usual training step follows:\n</code></pre>\n<ul>\n<li>Error messages and/or stack traces of the bug</li>\n</ul>\n<pre><code>in  40638,138,Traceback (most recent call last):\n  File \"train_0.py\", line 516, in &lt;module&gt;\n    run_train()\n  File \"train_0.py\", line 352, in run_train\n    loss.backward()\n  File \"/home/user/miniconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\", line 120, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/user/miniconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 81, in backward\n    variables, grad_variables, retain_graph, create_graph)\nRuntimeError: arguments are located on different GPUs at /home/user/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:231\n</code></pre>\n<ul>\n<li>Context around what you are trying to do</li>\n</ul>\n<p>Train a model on multiple gpus with SGD and reduce learning rate by half every x epochs.</p>", "body_text": "PyTorch GitHub Issues Guidelines\n\nPyTorch\nOS: ubuntu 16.04\nPyTorch version: 0.4\nHow you installed PyTorch (source):\nPython version: 3.6\nCUDA/cuDNN version: cuda8/cudnn6\nGPU models and configuration: 1080Ti\nGCC version (if compiling from source): gcc-7\nCMake version: 3.5.1\nBuild command you used (if compiling from source):\n\nexport CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n\n# Install basic dependencies\nconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing\n\n# Add LAPACK support for the GPU\nconda install -c pytorch magma-cuda80 \npython setup.py install\n\nIn addition, including the following information will also be very helpful for us to diagnose the problem:\n\nA script to reproduce the bug. Please try to provide as minimal of a test case as possible.\n\nimport torch\nimport torchvision\nfrom torchvision.models import densenet201\n\nmodel = densenet201()\n# set the optimizer to SGD\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n                         lr=lr/2 if (num_iters % iter_smooth == 0) else lr, momentum=0.9, weight_decay=0.0001)\nrate = get_learning_rate(optimizer)  #load all except learning rate\noptimizer.load_state_dict(checkpoint['optimizer'])\nadjust_learning_rate(optimizer, rate)\n\n# usual training step follows:\n\n\nError messages and/or stack traces of the bug\n\nin  40638,138,Traceback (most recent call last):\n  File \"train_0.py\", line 516, in <module>\n    run_train()\n  File \"train_0.py\", line 352, in run_train\n    loss.backward()\n  File \"/home/user/miniconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\", line 120, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/user/miniconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 81, in backward\n    variables, grad_variables, retain_graph, create_graph)\nRuntimeError: arguments are located on different GPUs at /home/user/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:231\n\n\nContext around what you are trying to do\n\nTrain a model on multiple gpus with SGD and reduce learning rate by half every x epochs.", "body": "PyTorch GitHub Issues Guidelines\r\n--------------------------------\r\n- PyTorch\r\n- OS: ubuntu 16.04\r\n- PyTorch version: 0.4\r\n- How you installed PyTorch (source):\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: cuda8/cudnn6\r\n- GPU models and configuration: 1080Ti\r\n- GCC version (if compiling from source): gcc-7\r\n- CMake version: 3.5.1\r\n- Build command you used (if compiling from source):\r\n```\r\nexport CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\r\n\r\n# Install basic dependencies\r\nconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing\r\n\r\n# Add LAPACK support for the GPU\r\nconda install -c pytorch magma-cuda80 \r\npython setup.py install\r\n```\r\n\r\nIn addition, including the following information will also be very helpful for us to diagnose the problem:\r\n- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.\r\n```\r\nimport torch\r\nimport torchvision\r\nfrom torchvision.models import densenet201\r\n\r\nmodel = densenet201()\r\n# set the optimizer to SGD\r\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\r\n                         lr=lr/2 if (num_iters % iter_smooth == 0) else lr, momentum=0.9, weight_decay=0.0001)\r\nrate = get_learning_rate(optimizer)  #load all except learning rate\r\noptimizer.load_state_dict(checkpoint['optimizer'])\r\nadjust_learning_rate(optimizer, rate)\r\n\r\n# usual training step follows:\r\n```\r\n\r\n- Error messages and/or stack traces of the bug\r\n```\r\nin  40638,138,Traceback (most recent call last):\r\n  File \"train_0.py\", line 516, in <module>\r\n    run_train()\r\n  File \"train_0.py\", line 352, in run_train\r\n    loss.backward()\r\n  File \"/home/user/miniconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\", line 120, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/user/miniconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 81, in backward\r\n    variables, grad_variables, retain_graph, create_graph)\r\nRuntimeError: arguments are located on different GPUs at /home/user/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:231\r\n```\r\n- Context around what you are trying to do\r\n\r\nTrain a model on multiple gpus with SGD and reduce learning rate by half every x epochs.\r\n"}