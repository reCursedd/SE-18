{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/438239061", "html_url": "https://github.com/pytorch/pytorch/issues/13850#issuecomment-438239061", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13850", "id": 438239061, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODIzOTA2MQ==", "user": {"login": "toemm", "id": 15850161, "node_id": "MDQ6VXNlcjE1ODUwMTYx", "avatar_url": "https://avatars3.githubusercontent.com/u/15850161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/toemm", "html_url": "https://github.com/toemm", "followers_url": "https://api.github.com/users/toemm/followers", "following_url": "https://api.github.com/users/toemm/following{/other_user}", "gists_url": "https://api.github.com/users/toemm/gists{/gist_id}", "starred_url": "https://api.github.com/users/toemm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/toemm/subscriptions", "organizations_url": "https://api.github.com/users/toemm/orgs", "repos_url": "https://api.github.com/users/toemm/repos", "events_url": "https://api.github.com/users/toemm/events{/privacy}", "received_events_url": "https://api.github.com/users/toemm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-13T11:49:56Z", "updated_at": "2018-11-13T11:49:56Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Is there any code I can try here?</p>\n</blockquote>\n<p>I managed to fix it by putting my <strong>whole</strong> training code into main().<br>\nBefore that I was only doing the training loop in main while keeping other functions outside. This led to memory buildup as soon as more worker processed were spawned that executed the code that comes before main() repeatedly.</p>\n<p>I'm not a Windows/Python guy so this wasn't obvious behaviour for me. I took the code snippet from the Windows FAQs from the Pytorch site: <a href=\"https://pytorch.org/docs/stable/notes/windows.html\" rel=\"nofollow\">https://pytorch.org/docs/stable/notes/windows.html</a></p>\n<p>Is this normal procedure to put the whole code into main() when using num_workers multiprocessing?</p>", "body_text": "Is there any code I can try here?\n\nI managed to fix it by putting my whole training code into main().\nBefore that I was only doing the training loop in main while keeping other functions outside. This led to memory buildup as soon as more worker processed were spawned that executed the code that comes before main() repeatedly.\nI'm not a Windows/Python guy so this wasn't obvious behaviour for me. I took the code snippet from the Windows FAQs from the Pytorch site: https://pytorch.org/docs/stable/notes/windows.html\nIs this normal procedure to put the whole code into main() when using num_workers multiprocessing?", "body": "> Is there any code I can try here?\r\n\r\nI managed to fix it by putting my **whole** training code into main().\r\nBefore that I was only doing the training loop in main while keeping other functions outside. This led to memory buildup as soon as more worker processed were spawned that executed the code that comes before main() repeatedly.\r\n\r\nI'm not a Windows/Python guy so this wasn't obvious behaviour for me. I took the code snippet from the Windows FAQs from the Pytorch site: https://pytorch.org/docs/stable/notes/windows.html\r\n\r\nIs this normal procedure to put the whole code into main() when using num_workers multiprocessing?"}