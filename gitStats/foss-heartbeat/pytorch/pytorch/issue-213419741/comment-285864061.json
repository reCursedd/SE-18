{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/285864061", "html_url": "https://github.com/pytorch/pytorch/issues/975#issuecomment-285864061", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/975", "id": 285864061, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTg2NDA2MQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-11T12:36:21Z", "updated_at": "2017-03-11T12:46:30Z", "author_association": "MEMBER", "body_html": "<p>Setting <code>OMP_NUM_THREADS</code> helps. I've looked into the issue and it seems that the problem is caused by <code>zero</code> calls in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/methods/TensorMath.cwrap#L1445\"><code>mm</code></a> and <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/methods/TensorMath.cwrap#L1424\"><code>mv</code></a>. Removing them speeds up the backward from 15.6s to 4.4s (forward takes 3.5s), without touching <code>OMP</code> or <code>MKL_NUM_THREADS</code> (so using all 40 cores).</p>\n<p>A recent PR that added support for AVX instructions made <code>fill</code> use OpenMP, and this is why all these threads are spawned, but I'm not sure why does it slow down everything so much. Each of them is pre-assigned a single contiguous chunk of size <code>numel / omp_num_threads</code> to process, so they shouldn't be thrashing the cache (maybe it's just too small for them?). What's more, if I time a call to <code>tensor.zero_()</code> in an interpreter, it never gets slower when I increase the number of threads (however the speedups are quite small after reaching 20 threads). <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> any ideas what might be going on?</p>\n<p>I think the problem with <code>set_num_threads</code> might have been caused by <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/ff5fa111299bbe3aa47fd55bd94d971b90cda646/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/ff5fa111299bbe3aa47fd55bd94d971b90cda646\"><tt>ff5fa11</tt></a> that, as I understand, will additionally link Intel OpenMP. Since we're still going to be adding <code>-fopenmp</code> to the compile flags a guess would be that GNU OpenMP would still be linked, leading to a symbol conflict, so <code>omp_set_num_threads</code> gets resolved to whichever library is loaded first, and it makes it possible to control how many threads will be used only by MKL, or only by torch libs. Output of <code>ldd</code> seems to confirm that theory, and since <code>iomp5</code> appears before <code>gomop</code>, all omp symbols are resolved form it:</p>\n<pre><code>ldd torch/lib/libTH.so.1\n    ...\n    libiomp5.so =&gt; /.../miniconda3/lib/libiomp5.so (0x00007f10f15af000)\n    ...\n    libgomp.so.1 =&gt; /.../miniconda3/lib/libgomp.so.1 (0x00007f10f0cf7000)\n...\n</code></pre>\n<p>To sum up I think there are two problems:</p>\n<ol>\n<li>We're suffering from having two omp libraries linked at the same time</li>\n<li>For some reason <code>zero</code> calls in our bindings are very very slow</li>\n</ol>", "body_text": "Setting OMP_NUM_THREADS helps. I've looked into the issue and it seems that the problem is caused by zero calls in mm and mv. Removing them speeds up the backward from 15.6s to 4.4s (forward takes 3.5s), without touching OMP or MKL_NUM_THREADS (so using all 40 cores).\nA recent PR that added support for AVX instructions made fill use OpenMP, and this is why all these threads are spawned, but I'm not sure why does it slow down everything so much. Each of them is pre-assigned a single contiguous chunk of size numel / omp_num_threads to process, so they shouldn't be thrashing the cache (maybe it's just too small for them?). What's more, if I time a call to tensor.zero_() in an interpreter, it never gets slower when I increase the number of threads (however the speedups are quite small after reaching 20 threads). @adamlerer any ideas what might be going on?\nI think the problem with set_num_threads might have been caused by ff5fa11 that, as I understand, will additionally link Intel OpenMP. Since we're still going to be adding -fopenmp to the compile flags a guess would be that GNU OpenMP would still be linked, leading to a symbol conflict, so omp_set_num_threads gets resolved to whichever library is loaded first, and it makes it possible to control how many threads will be used only by MKL, or only by torch libs. Output of ldd seems to confirm that theory, and since iomp5 appears before gomop, all omp symbols are resolved form it:\nldd torch/lib/libTH.so.1\n    ...\n    libiomp5.so => /.../miniconda3/lib/libiomp5.so (0x00007f10f15af000)\n    ...\n    libgomp.so.1 => /.../miniconda3/lib/libgomp.so.1 (0x00007f10f0cf7000)\n...\n\nTo sum up I think there are two problems:\n\nWe're suffering from having two omp libraries linked at the same time\nFor some reason zero calls in our bindings are very very slow", "body": "Setting `OMP_NUM_THREADS` helps. I've looked into the issue and it seems that the problem is caused by `zero` calls in [`mm`](https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/methods/TensorMath.cwrap#L1445) and [`mv`](https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/methods/TensorMath.cwrap#L1424). Removing them speeds up the backward from 15.6s to 4.4s (forward takes 3.5s), without touching `OMP` or `MKL_NUM_THREADS` (so using all 40 cores).\r\n\r\nA recent PR that added support for AVX instructions made `fill` use OpenMP, and this is why all these threads are spawned, but I'm not sure why does it slow down everything so much. Each of them is pre-assigned a single contiguous chunk of size `numel / omp_num_threads` to process, so they shouldn't be thrashing the cache (maybe it's just too small for them?). What's more, if I time a call to `tensor.zero_()` in an interpreter, it never gets slower when I increase the number of threads (however the speedups are quite small after reaching 20 threads). @adamlerer any ideas what might be going on?\r\n\r\nI think the problem with `set_num_threads` might have been caused by ff5fa11 that, as I understand, will additionally link Intel OpenMP. Since we're still going to be adding `-fopenmp` to the compile flags a guess would be that GNU OpenMP would still be linked, leading to a symbol conflict, so `omp_set_num_threads` gets resolved to whichever library is loaded first, and it makes it possible to control how many threads will be used only by MKL, or only by torch libs. Output of `ldd` seems to confirm that theory, and since `iomp5` appears before `gomop`, all omp symbols are resolved form it:\r\n\r\n```\r\nldd torch/lib/libTH.so.1\r\n    ...\r\n    libiomp5.so => /.../miniconda3/lib/libiomp5.so (0x00007f10f15af000)\r\n    ...\r\n    libgomp.so.1 => /.../miniconda3/lib/libgomp.so.1 (0x00007f10f0cf7000)\r\n...\r\n```\r\n\r\n\r\nTo sum up I think there are two problems:\r\n1. We're suffering from having two omp libraries linked at the same time\r\n2. For some reason `zero` calls in our bindings are very very slow"}