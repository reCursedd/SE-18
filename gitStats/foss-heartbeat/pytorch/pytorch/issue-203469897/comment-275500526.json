{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/275500526", "html_url": "https://github.com/pytorch/pytorch/issues/599#issuecomment-275500526", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/599", "id": 275500526, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTUwMDUyNg==", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-26T20:14:39Z", "updated_at": "2017-01-26T20:14:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Tried the same RNN in torch and am getting ~2.6GiB on nvidia-smi.</p>\n<pre><code>require 'cudnn'\nrequire 'cunn'\n\nlocal miniBatch = 512\nlocal seqLength = 300\nlocal hiddenSize = 256\nlocal numberOfLayers = 1\nlocal biDirectionalScale = 1\nlocal rnn = cudnn.LSTM(hiddenSize, hiddenSize, numberOfLayers):cuda()\n\nrnn.hiddenInput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\nrnn.cellInput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\nrnn.gradHiddenOutput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\nrnn.gradCellOutput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\ninput = torch.CudaTensor(seqLength, miniBatch, hiddenSize):fill(1)\n\nfor layer=1,200 do\n   print(layer)\n   local testOutputi = rnn:forward(input)\n   -- gradInput set to 1s.\n   local gradInput\n   if(batchFirst) then\n      gradInput = torch.CudaTensor(miniBatch, seqLength, hiddenSize * biDirectionalScale):fill(1)\n   else\n      gradInput = torch.CudaTensor(seqLength, miniBatch, hiddenSize * biDirectionalScale):fill(1)\n   end\n   rnn:backward(input, gradInput)\nend\n\n</code></pre>", "body_text": "Tried the same RNN in torch and am getting ~2.6GiB on nvidia-smi.\nrequire 'cudnn'\nrequire 'cunn'\n\nlocal miniBatch = 512\nlocal seqLength = 300\nlocal hiddenSize = 256\nlocal numberOfLayers = 1\nlocal biDirectionalScale = 1\nlocal rnn = cudnn.LSTM(hiddenSize, hiddenSize, numberOfLayers):cuda()\n\nrnn.hiddenInput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\nrnn.cellInput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\nrnn.gradHiddenOutput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\nrnn.gradCellOutput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\ninput = torch.CudaTensor(seqLength, miniBatch, hiddenSize):fill(1)\n\nfor layer=1,200 do\n   print(layer)\n   local testOutputi = rnn:forward(input)\n   -- gradInput set to 1s.\n   local gradInput\n   if(batchFirst) then\n      gradInput = torch.CudaTensor(miniBatch, seqLength, hiddenSize * biDirectionalScale):fill(1)\n   else\n      gradInput = torch.CudaTensor(seqLength, miniBatch, hiddenSize * biDirectionalScale):fill(1)\n   end\n   rnn:backward(input, gradInput)\nend", "body": "Tried the same RNN in torch and am getting ~2.6GiB on nvidia-smi.\r\n\r\n```\r\nrequire 'cudnn'\r\nrequire 'cunn'\r\n\r\nlocal miniBatch = 512\r\nlocal seqLength = 300\r\nlocal hiddenSize = 256\r\nlocal numberOfLayers = 1\r\nlocal biDirectionalScale = 1\r\nlocal rnn = cudnn.LSTM(hiddenSize, hiddenSize, numberOfLayers):cuda()\r\n\r\nrnn.hiddenInput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\r\nrnn.cellInput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\r\nrnn.gradHiddenOutput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\r\nrnn.gradCellOutput = torch.CudaTensor(numberOfLayers * biDirectionalScale, miniBatch, hiddenSize):fill(1)\r\ninput = torch.CudaTensor(seqLength, miniBatch, hiddenSize):fill(1)\r\n\r\nfor layer=1,200 do\r\n   print(layer)\r\n   local testOutputi = rnn:forward(input)\r\n   -- gradInput set to 1s.\r\n   local gradInput\r\n   if(batchFirst) then\r\n      gradInput = torch.CudaTensor(miniBatch, seqLength, hiddenSize * biDirectionalScale):fill(1)\r\n   else\r\n      gradInput = torch.CudaTensor(seqLength, miniBatch, hiddenSize * biDirectionalScale):fill(1)\r\n   end\r\n   rnn:backward(input, gradInput)\r\nend\r\n\r\n```"}