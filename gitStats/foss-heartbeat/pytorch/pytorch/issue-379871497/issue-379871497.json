{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13843", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13843/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13843/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13843/events", "html_url": "https://github.com/pytorch/pytorch/issues/13843", "id": 379871497, "node_id": "MDU6SXNzdWUzNzk4NzE0OTc=", "number": 13843, "title": "Possible CPU-side memory leak even when fitting on GPU", "user": {"login": "trias702", "id": 25867060, "node_id": "MDQ6VXNlcjI1ODY3MDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/25867060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trias702", "html_url": "https://github.com/trias702", "followers_url": "https://api.github.com/users/trias702/followers", "following_url": "https://api.github.com/users/trias702/following{/other_user}", "gists_url": "https://api.github.com/users/trias702/gists{/gist_id}", "starred_url": "https://api.github.com/users/trias702/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trias702/subscriptions", "organizations_url": "https://api.github.com/users/trias702/orgs", "repos_url": "https://api.github.com/users/trias702/repos", "events_url": "https://api.github.com/users/trias702/events{/privacy}", "received_events_url": "https://api.github.com/users/trias702/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-12T17:07:05Z", "updated_at": "2018-11-13T17:13:28Z", "closed_at": "2018-11-13T17:13:28Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>A possible CPU-side memory leak even when fitting on the GPU using PyTorch 0.4.1.</p>\n<h2>To Reproduce</h2>\n<p>I am quite new to PyTorch, having used TF/Keras extensively in the past, but am now trying to use PyTorch as a replacement. I decided to start small with a seq2seq Skip-Thought model, cobbled together using the PyTorch NLP tutorials. Everything seems to work fine when I run small scale tests, however, when I use the code to run a large scale fit on 3000 separate paragraphs (each paragraph having a variable number of sentences) I notice that my system RAM usage slowly goes up as the script runs, until eventually it hits 100% and the box becomes unresponsive and has to be force rebooted. The Linux box has 64 GB of RAM, and when the script starts, usage is 4.7% but this climbs steadily over time to 100%, at which point the box becomes unresponsive.</p>\n<p>Since I'm new to PyTorch, I'm not sure if perhaps I'm doing something blatantly wrong in my code which would account for this behaviour?</p>\n<p>This is my core fitting logic, which runs as a script on linux:</p>\n<pre><code>device = 'cuda'\nmodel = SkipThought(len(text_dictionary.token2id), 128, 256, nn.NLLLoss()).to(device)\n\n// corpus_orig is a list of lists where each list-element is a text paragraph with multiple sentences\nn_iters=len(corpus_orig)\n\nstart = time.time()\nprint_loss_total = 0\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nfor xi in range(1, n_iters + 1):\n    x = corpus_orig[xi - 1]\n    sents = [x for x in map(str.strip, x.split('. ')) if len(x) &gt; 0]\n    for i in range(1, len(sents) - 1):\n        input_tensor, prev_tensor, next_tensor = tensorsFromPair( (sents[i], sents[i - 1], sents[i + 1]) )\n\n        optimizer.zero_grad()\n        \n        loss, prev_output, next_output = model(input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True)\n        \n        loss.backward(retain_graph=True)    // pytorch says I need retain_graph=True, otherwise I get an error here\n        optimizer.step()\n        \n        print_loss_total += loss.item()\n\n    print_loss_avg = print_loss_total\n    print_loss_total = 0\n    print('%s (%d %d%%) %.4f' % (timeSince(start, xi / n_iters),\n                         xi, xi / n_iters * 100, print_loss_avg), flush=True)\n</code></pre>\n<p>Here are the helper functions which create the tensors passed to the model by converting all text words into indices from a predefined gensim dictionary:</p>\n<pre><code>def indexesFromSentence(sentence):\n    return text_dictionary.doc2idx(sentence.split())\n\ndef tensorFromSentence(sentence):\n    indexes = indexesFromSentence(sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(pair[0])\n    prev_tensor = tensorFromSentence(pair[1])\n    next_tensor = tensorFromSentence(pair[2])\n    return (input_tensor, prev_tensor, next_tensor)\n</code></pre>\n<p>And, finally, here is the SkipThought model itself, with all its helper classes (apologies for the wall of code):</p>\n<pre><code>class EncoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_size, gru_size):\n        super(EncoderRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.gru_size = gru_size\n\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.gru = nn.GRU(embedding_size, gru_size)\n\n    def forward(self, sentence, hidden):\n        embeddings = self.embedding(sentence)\n        embeddings = F.tanh(embeddings)\n        output, hidden = self.gru(embeddings, hidden)\n        \n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.gru_size, device=device)\n    \n    def getEmbedding(self, sentence):\n        return F.tanh(self.embedding(sentence))\n\nclass LocalAttention(nn.Module):\n    def __init__(self, dim):\n        super(LocalAttention, self).__init__()\n        self.W = nn.Linear(dim, dim, bias=False)\n\n    def score(self, decoder_hidden, encoder_out):\n        encoder_out = self.W(encoder_out)\n        encoder_out = encoder_out.permute(1, 0, 2)\n        return encoder_out @ decoder_hidden.permute(1, 2, 0)\n\n    def forward(self, decoder_hidden, encoder_out):\n        energies = self.score(decoder_hidden, encoder_out)\n        mask = F.softmax(energies, dim=1)\n        context = encoder_out.permute(1, 2, 0) @ mask\n        context = context.permute(2, 0, 1)\n        mask = mask.permute(2, 0, 1)\n        return context, mask\n\n\nclass SkipThought(nn.Module):\n    def __init__(self, vocab_size, embedding_size, gru_size, criterion):\n        super(SkipThought, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.gru_size = gru_size\n        self.criterion = criterion\n        \n        self.encoder = EncoderRNN(vocab_size, embedding_size, gru_size)\n        self.prev_gru = nn.GRU(embedding_size + gru_size, gru_size)\n        self.next_gru = nn.GRU(embedding_size + gru_size, gru_size)\n        self.attention = LocalAttention(gru_size)\n        self.worder = nn.Linear(gru_size * 2, vocab_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n        self.encoder_hidden = self.encoder.initHidden()\n    \n    def forward(self, input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True):        \n        encoder_hidden = self.encoder_hidden\n        \n        prev_length = prev_tensor.size(0)\n        next_length = next_tensor.size(0)\n    \n        loss = 0\n        \n        encoder_output, encoder_hidden = self.encoder(input_tensor, encoder_hidden)\n    \n        prev_input = torch.tensor([[SOS_token]], device=device)\n        next_input = torch.tensor([[SOS_token]], device=device)\n    \n        prev_hidden = encoder_hidden\n        next_hidden = encoder_hidden\n        \n        self.encoder_hidden = encoder_hidden\n    \n        prev_output = []\n        next_output = []\n    \n        if use_teacher_forcing:\n            # Teacher forcing: Feed the target as the next input\n            for di in range(prev_length):\n                embedded = self.encoder.getEmbedding(prev_input)\n                context, _ = self.attention(prev_hidden, encoder_output)\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                loss += self.criterion(decoder_output, prev_tensor[di])\n                prev_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\n                prev_input = prev_tensor[di].unsqueeze(0)  # Teacher forcing\n            \n            for di in range(next_length):\n                embedded = self.encoder.getEmbedding(next_input)\n                context, _ = self.attention(next_hidden, encoder_output)\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                loss += self.criterion(decoder_output, next_tensor[di])\n                next_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\n                next_input = next_tensor[di].unsqueeze(0)  # Teacher forcing\n    \n        else:\n            # Without teacher forcing: use its own predictions as the next input\n            for di in range(prev_length):\n                embedded = self.encoder.getEmbedding(prev_input)\n                context, _ = self.attention(prev_hidden, encoder_output)\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                topv, topi = decoder_output.topk(1)\n                prev_input = topi.squeeze().detach()  # detach from history as input\n                \n                loss += self.criterion(decoder_output, prev_tensor[di])\n                prev_output.append( prev_input.item() )\n                if prev_input.item() == EOS_token:\n                    break\n                prev_input = prev_input.unsqueeze(0).unsqueeze(0)\n            \n            for di in range(next_length):\n                embedded = self.encoder.getEmbedding(next_input)\n                context, _ = self.attention(next_hidden, encoder_output)\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                topv, topi = decoder_output.topk(1)\n                next_input = topi.squeeze().detach()  # detach from history as input\n    \n                loss += self.criterion(decoder_output, next_tensor[di])\n                next_output.append( next_input.item() )\n                if next_input.item() == EOS_token:\n                    break\n                next_input = next_input.unsqueeze(0).unsqueeze(0)\n        \n        return loss, prev_output, next_output\n</code></pre>\n<h2>Expected behavior</h2>\n<p>Memory usage in Linux should not linearly increase as the fitting script runs, especially not to the point at which the box dies.</p>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 0.4.1</li>\n<li>OS (e.g., Linux): Linux (Ubuntu 16.04)</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): pip install torch</li>\n<li>Build command you used (if compiling from source): N/A</li>\n<li>Python version: 3.6.6</li>\n<li>CUDA/cuDNN version: CUDA 9.0.176 / CuDNN 7.4.1.5</li>\n<li>GPU models and configuration: Tesla V100</li>\n<li>Any other relevant information:</li>\n</ul>", "body_text": "\ud83d\udc1b Bug\nA possible CPU-side memory leak even when fitting on the GPU using PyTorch 0.4.1.\nTo Reproduce\nI am quite new to PyTorch, having used TF/Keras extensively in the past, but am now trying to use PyTorch as a replacement. I decided to start small with a seq2seq Skip-Thought model, cobbled together using the PyTorch NLP tutorials. Everything seems to work fine when I run small scale tests, however, when I use the code to run a large scale fit on 3000 separate paragraphs (each paragraph having a variable number of sentences) I notice that my system RAM usage slowly goes up as the script runs, until eventually it hits 100% and the box becomes unresponsive and has to be force rebooted. The Linux box has 64 GB of RAM, and when the script starts, usage is 4.7% but this climbs steadily over time to 100%, at which point the box becomes unresponsive.\nSince I'm new to PyTorch, I'm not sure if perhaps I'm doing something blatantly wrong in my code which would account for this behaviour?\nThis is my core fitting logic, which runs as a script on linux:\ndevice = 'cuda'\nmodel = SkipThought(len(text_dictionary.token2id), 128, 256, nn.NLLLoss()).to(device)\n\n// corpus_orig is a list of lists where each list-element is a text paragraph with multiple sentences\nn_iters=len(corpus_orig)\n\nstart = time.time()\nprint_loss_total = 0\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nfor xi in range(1, n_iters + 1):\n    x = corpus_orig[xi - 1]\n    sents = [x for x in map(str.strip, x.split('. ')) if len(x) > 0]\n    for i in range(1, len(sents) - 1):\n        input_tensor, prev_tensor, next_tensor = tensorsFromPair( (sents[i], sents[i - 1], sents[i + 1]) )\n\n        optimizer.zero_grad()\n        \n        loss, prev_output, next_output = model(input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True)\n        \n        loss.backward(retain_graph=True)    // pytorch says I need retain_graph=True, otherwise I get an error here\n        optimizer.step()\n        \n        print_loss_total += loss.item()\n\n    print_loss_avg = print_loss_total\n    print_loss_total = 0\n    print('%s (%d %d%%) %.4f' % (timeSince(start, xi / n_iters),\n                         xi, xi / n_iters * 100, print_loss_avg), flush=True)\n\nHere are the helper functions which create the tensors passed to the model by converting all text words into indices from a predefined gensim dictionary:\ndef indexesFromSentence(sentence):\n    return text_dictionary.doc2idx(sentence.split())\n\ndef tensorFromSentence(sentence):\n    indexes = indexesFromSentence(sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(pair[0])\n    prev_tensor = tensorFromSentence(pair[1])\n    next_tensor = tensorFromSentence(pair[2])\n    return (input_tensor, prev_tensor, next_tensor)\n\nAnd, finally, here is the SkipThought model itself, with all its helper classes (apologies for the wall of code):\nclass EncoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_size, gru_size):\n        super(EncoderRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.gru_size = gru_size\n\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.gru = nn.GRU(embedding_size, gru_size)\n\n    def forward(self, sentence, hidden):\n        embeddings = self.embedding(sentence)\n        embeddings = F.tanh(embeddings)\n        output, hidden = self.gru(embeddings, hidden)\n        \n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.gru_size, device=device)\n    \n    def getEmbedding(self, sentence):\n        return F.tanh(self.embedding(sentence))\n\nclass LocalAttention(nn.Module):\n    def __init__(self, dim):\n        super(LocalAttention, self).__init__()\n        self.W = nn.Linear(dim, dim, bias=False)\n\n    def score(self, decoder_hidden, encoder_out):\n        encoder_out = self.W(encoder_out)\n        encoder_out = encoder_out.permute(1, 0, 2)\n        return encoder_out @ decoder_hidden.permute(1, 2, 0)\n\n    def forward(self, decoder_hidden, encoder_out):\n        energies = self.score(decoder_hidden, encoder_out)\n        mask = F.softmax(energies, dim=1)\n        context = encoder_out.permute(1, 2, 0) @ mask\n        context = context.permute(2, 0, 1)\n        mask = mask.permute(2, 0, 1)\n        return context, mask\n\n\nclass SkipThought(nn.Module):\n    def __init__(self, vocab_size, embedding_size, gru_size, criterion):\n        super(SkipThought, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.gru_size = gru_size\n        self.criterion = criterion\n        \n        self.encoder = EncoderRNN(vocab_size, embedding_size, gru_size)\n        self.prev_gru = nn.GRU(embedding_size + gru_size, gru_size)\n        self.next_gru = nn.GRU(embedding_size + gru_size, gru_size)\n        self.attention = LocalAttention(gru_size)\n        self.worder = nn.Linear(gru_size * 2, vocab_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n        self.encoder_hidden = self.encoder.initHidden()\n    \n    def forward(self, input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True):        \n        encoder_hidden = self.encoder_hidden\n        \n        prev_length = prev_tensor.size(0)\n        next_length = next_tensor.size(0)\n    \n        loss = 0\n        \n        encoder_output, encoder_hidden = self.encoder(input_tensor, encoder_hidden)\n    \n        prev_input = torch.tensor([[SOS_token]], device=device)\n        next_input = torch.tensor([[SOS_token]], device=device)\n    \n        prev_hidden = encoder_hidden\n        next_hidden = encoder_hidden\n        \n        self.encoder_hidden = encoder_hidden\n    \n        prev_output = []\n        next_output = []\n    \n        if use_teacher_forcing:\n            # Teacher forcing: Feed the target as the next input\n            for di in range(prev_length):\n                embedded = self.encoder.getEmbedding(prev_input)\n                context, _ = self.attention(prev_hidden, encoder_output)\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                loss += self.criterion(decoder_output, prev_tensor[di])\n                prev_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\n                prev_input = prev_tensor[di].unsqueeze(0)  # Teacher forcing\n            \n            for di in range(next_length):\n                embedded = self.encoder.getEmbedding(next_input)\n                context, _ = self.attention(next_hidden, encoder_output)\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                loss += self.criterion(decoder_output, next_tensor[di])\n                next_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\n                next_input = next_tensor[di].unsqueeze(0)  # Teacher forcing\n    \n        else:\n            # Without teacher forcing: use its own predictions as the next input\n            for di in range(prev_length):\n                embedded = self.encoder.getEmbedding(prev_input)\n                context, _ = self.attention(prev_hidden, encoder_output)\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                topv, topi = decoder_output.topk(1)\n                prev_input = topi.squeeze().detach()  # detach from history as input\n                \n                loss += self.criterion(decoder_output, prev_tensor[di])\n                prev_output.append( prev_input.item() )\n                if prev_input.item() == EOS_token:\n                    break\n                prev_input = prev_input.unsqueeze(0).unsqueeze(0)\n            \n            for di in range(next_length):\n                embedded = self.encoder.getEmbedding(next_input)\n                context, _ = self.attention(next_hidden, encoder_output)\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\n                topv, topi = decoder_output.topk(1)\n                next_input = topi.squeeze().detach()  # detach from history as input\n    \n                loss += self.criterion(decoder_output, next_tensor[di])\n                next_output.append( next_input.item() )\n                if next_input.item() == EOS_token:\n                    break\n                next_input = next_input.unsqueeze(0).unsqueeze(0)\n        \n        return loss, prev_output, next_output\n\nExpected behavior\nMemory usage in Linux should not linearly increase as the fitting script runs, especially not to the point at which the box dies.\nEnvironment\n\nPyTorch Version (e.g., 1.0): 0.4.1\nOS (e.g., Linux): Linux (Ubuntu 16.04)\nHow you installed PyTorch (conda, pip, source): pip install torch\nBuild command you used (if compiling from source): N/A\nPython version: 3.6.6\nCUDA/cuDNN version: CUDA 9.0.176 / CuDNN 7.4.1.5\nGPU models and configuration: Tesla V100\nAny other relevant information:", "body": "## \ud83d\udc1b Bug\r\n\r\nA possible CPU-side memory leak even when fitting on the GPU using PyTorch 0.4.1.\r\n\r\n## To Reproduce\r\n\r\nI am quite new to PyTorch, having used TF/Keras extensively in the past, but am now trying to use PyTorch as a replacement. I decided to start small with a seq2seq Skip-Thought model, cobbled together using the PyTorch NLP tutorials. Everything seems to work fine when I run small scale tests, however, when I use the code to run a large scale fit on 3000 separate paragraphs (each paragraph having a variable number of sentences) I notice that my system RAM usage slowly goes up as the script runs, until eventually it hits 100% and the box becomes unresponsive and has to be force rebooted. The Linux box has 64 GB of RAM, and when the script starts, usage is 4.7% but this climbs steadily over time to 100%, at which point the box becomes unresponsive.\r\n\r\nSince I'm new to PyTorch, I'm not sure if perhaps I'm doing something blatantly wrong in my code which would account for this behaviour?\r\n\r\nThis is my core fitting logic, which runs as a script on linux:\r\n\r\n```\r\ndevice = 'cuda'\r\nmodel = SkipThought(len(text_dictionary.token2id), 128, 256, nn.NLLLoss()).to(device)\r\n\r\n// corpus_orig is a list of lists where each list-element is a text paragraph with multiple sentences\r\nn_iters=len(corpus_orig)\r\n\r\nstart = time.time()\r\nprint_loss_total = 0\r\n\r\noptimizer = optim.SGD(model.parameters(), lr=0.01)\r\n\r\nfor xi in range(1, n_iters + 1):\r\n    x = corpus_orig[xi - 1]\r\n    sents = [x for x in map(str.strip, x.split('. ')) if len(x) > 0]\r\n    for i in range(1, len(sents) - 1):\r\n        input_tensor, prev_tensor, next_tensor = tensorsFromPair( (sents[i], sents[i - 1], sents[i + 1]) )\r\n\r\n        optimizer.zero_grad()\r\n        \r\n        loss, prev_output, next_output = model(input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True)\r\n        \r\n        loss.backward(retain_graph=True)    // pytorch says I need retain_graph=True, otherwise I get an error here\r\n        optimizer.step()\r\n        \r\n        print_loss_total += loss.item()\r\n\r\n    print_loss_avg = print_loss_total\r\n    print_loss_total = 0\r\n    print('%s (%d %d%%) %.4f' % (timeSince(start, xi / n_iters),\r\n                         xi, xi / n_iters * 100, print_loss_avg), flush=True)\r\n```\r\n\r\nHere are the helper functions which create the tensors passed to the model by converting all text words into indices from a predefined gensim dictionary:\r\n\r\n```\r\ndef indexesFromSentence(sentence):\r\n    return text_dictionary.doc2idx(sentence.split())\r\n\r\ndef tensorFromSentence(sentence):\r\n    indexes = indexesFromSentence(sentence)\r\n    indexes.append(EOS_token)\r\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\r\n\r\ndef tensorsFromPair(pair):\r\n    input_tensor = tensorFromSentence(pair[0])\r\n    prev_tensor = tensorFromSentence(pair[1])\r\n    next_tensor = tensorFromSentence(pair[2])\r\n    return (input_tensor, prev_tensor, next_tensor)\r\n```\r\n\r\nAnd, finally, here is the SkipThought model itself, with all its helper classes (apologies for the wall of code):\r\n\r\n```\r\nclass EncoderRNN(nn.Module):\r\n    def __init__(self, vocab_size, embedding_size, gru_size):\r\n        super(EncoderRNN, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embedding_size = embedding_size\r\n        self.gru_size = gru_size\r\n\r\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n        self.gru = nn.GRU(embedding_size, gru_size)\r\n\r\n    def forward(self, sentence, hidden):\r\n        embeddings = self.embedding(sentence)\r\n        embeddings = F.tanh(embeddings)\r\n        output, hidden = self.gru(embeddings, hidden)\r\n        \r\n        return output, hidden\r\n\r\n    def initHidden(self):\r\n        return torch.zeros(1, 1, self.gru_size, device=device)\r\n    \r\n    def getEmbedding(self, sentence):\r\n        return F.tanh(self.embedding(sentence))\r\n\r\nclass LocalAttention(nn.Module):\r\n    def __init__(self, dim):\r\n        super(LocalAttention, self).__init__()\r\n        self.W = nn.Linear(dim, dim, bias=False)\r\n\r\n    def score(self, decoder_hidden, encoder_out):\r\n        encoder_out = self.W(encoder_out)\r\n        encoder_out = encoder_out.permute(1, 0, 2)\r\n        return encoder_out @ decoder_hidden.permute(1, 2, 0)\r\n\r\n    def forward(self, decoder_hidden, encoder_out):\r\n        energies = self.score(decoder_hidden, encoder_out)\r\n        mask = F.softmax(energies, dim=1)\r\n        context = encoder_out.permute(1, 2, 0) @ mask\r\n        context = context.permute(2, 0, 1)\r\n        mask = mask.permute(2, 0, 1)\r\n        return context, mask\r\n\r\n\r\nclass SkipThought(nn.Module):\r\n    def __init__(self, vocab_size, embedding_size, gru_size, criterion):\r\n        super(SkipThought, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embedding_size = embedding_size\r\n        self.gru_size = gru_size\r\n        self.criterion = criterion\r\n        \r\n        self.encoder = EncoderRNN(vocab_size, embedding_size, gru_size)\r\n        self.prev_gru = nn.GRU(embedding_size + gru_size, gru_size)\r\n        self.next_gru = nn.GRU(embedding_size + gru_size, gru_size)\r\n        self.attention = LocalAttention(gru_size)\r\n        self.worder = nn.Linear(gru_size * 2, vocab_size)\r\n        self.softmax = nn.LogSoftmax(dim=1)\r\n        \r\n        self.encoder_hidden = self.encoder.initHidden()\r\n    \r\n    def forward(self, input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True):        \r\n        encoder_hidden = self.encoder_hidden\r\n        \r\n        prev_length = prev_tensor.size(0)\r\n        next_length = next_tensor.size(0)\r\n    \r\n        loss = 0\r\n        \r\n        encoder_output, encoder_hidden = self.encoder(input_tensor, encoder_hidden)\r\n    \r\n        prev_input = torch.tensor([[SOS_token]], device=device)\r\n        next_input = torch.tensor([[SOS_token]], device=device)\r\n    \r\n        prev_hidden = encoder_hidden\r\n        next_hidden = encoder_hidden\r\n        \r\n        self.encoder_hidden = encoder_hidden\r\n    \r\n        prev_output = []\r\n        next_output = []\r\n    \r\n        if use_teacher_forcing:\r\n            # Teacher forcing: Feed the target as the next input\r\n            for di in range(prev_length):\r\n                embedded = self.encoder.getEmbedding(prev_input)\r\n                context, _ = self.attention(prev_hidden, encoder_output)\r\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                loss += self.criterion(decoder_output, prev_tensor[di])\r\n                prev_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\r\n                prev_input = prev_tensor[di].unsqueeze(0)  # Teacher forcing\r\n            \r\n            for di in range(next_length):\r\n                embedded = self.encoder.getEmbedding(next_input)\r\n                context, _ = self.attention(next_hidden, encoder_output)\r\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                loss += self.criterion(decoder_output, next_tensor[di])\r\n                next_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\r\n                next_input = next_tensor[di].unsqueeze(0)  # Teacher forcing\r\n    \r\n        else:\r\n            # Without teacher forcing: use its own predictions as the next input\r\n            for di in range(prev_length):\r\n                embedded = self.encoder.getEmbedding(prev_input)\r\n                context, _ = self.attention(prev_hidden, encoder_output)\r\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                topv, topi = decoder_output.topk(1)\r\n                prev_input = topi.squeeze().detach()  # detach from history as input\r\n                \r\n                loss += self.criterion(decoder_output, prev_tensor[di])\r\n                prev_output.append( prev_input.item() )\r\n                if prev_input.item() == EOS_token:\r\n                    break\r\n                prev_input = prev_input.unsqueeze(0).unsqueeze(0)\r\n            \r\n            for di in range(next_length):\r\n                embedded = self.encoder.getEmbedding(next_input)\r\n                context, _ = self.attention(next_hidden, encoder_output)\r\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                topv, topi = decoder_output.topk(1)\r\n                next_input = topi.squeeze().detach()  # detach from history as input\r\n    \r\n                loss += self.criterion(decoder_output, next_tensor[di])\r\n                next_output.append( next_input.item() )\r\n                if next_input.item() == EOS_token:\r\n                    break\r\n                next_input = next_input.unsqueeze(0).unsqueeze(0)\r\n        \r\n        return loss, prev_output, next_output\r\n```\r\n\r\n## Expected behavior\r\n\r\nMemory usage in Linux should not linearly increase as the fitting script runs, especially not to the point at which the box dies.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 0.4.1\r\n - OS (e.g., Linux): Linux (Ubuntu 16.04)\r\n - How you installed PyTorch (`conda`, `pip`, source): pip install torch\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.6.6\r\n - CUDA/cuDNN version: CUDA 9.0.176 / CuDNN 7.4.1.5\r\n - GPU models and configuration: Tesla V100\r\n - Any other relevant information:\r\n\r\n"}