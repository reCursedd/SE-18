{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7069", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7069/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7069/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7069/events", "html_url": "https://github.com/pytorch/pytorch/issues/7069", "id": 318634945, "node_id": "MDU6SXNzdWUzMTg2MzQ5NDU=", "number": 7069, "title": "JIT error with dropout: Function.apply returned the wrong number of outputs", "user": {"login": "deepbrain", "id": 10003025, "node_id": "MDQ6VXNlcjEwMDAzMDI1", "avatar_url": "https://avatars3.githubusercontent.com/u/10003025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepbrain", "html_url": "https://github.com/deepbrain", "followers_url": "https://api.github.com/users/deepbrain/followers", "following_url": "https://api.github.com/users/deepbrain/following{/other_user}", "gists_url": "https://api.github.com/users/deepbrain/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepbrain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepbrain/subscriptions", "organizations_url": "https://api.github.com/users/deepbrain/orgs", "repos_url": "https://api.github.com/users/deepbrain/repos", "events_url": "https://api.github.com/users/deepbrain/events{/privacy}", "received_events_url": "https://api.github.com/users/deepbrain/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-28T13:03:43Z", "updated_at": "2018-04-28T16:39:46Z", "closed_at": "2018-04-28T16:39:46Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>in a simple test network I get an error when dropout layer is included (error does not happen when the dropout layer is excluded):</p>\n<p>Traceback (most recent call last):<br>\nFile \"testnet.py\", line 48, in <br>\nr = net(V)<br>\nRuntimeError:<br>\nFunction.apply returned the wrong number of outputs.:<br>\noperation failed in interpreter:<br>\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py(554): dropout<br>\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/modules/dropout.py(53): forward<br>\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(491): <strong>call</strong><br>\ntestnet.py(25): forward<br>\ntestnet.py(48): </p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> jit\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim<span class=\"pl-c1\">......</span>..\n\n  \n<span class=\"pl-en\">@torch.jit.compile</span>(<span class=\"pl-v\">nderivs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TestNet</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(TestNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.net1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">200</span>)\n        <span class=\"pl-c1\">self</span>.net2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.bn <span class=\"pl-k\">=</span> nn.BatchNorm1d(<span class=\"pl-c1\">200</span>)\n        <span class=\"pl-c1\">self</span>.sigmoid <span class=\"pl-k\">=</span> nn.Sigmoid()\n        <span class=\"pl-c1\">self</span>.ReLU <span class=\"pl-k\">=</span> nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c1\">self</span>.drop <span class=\"pl-k\">=</span> nn.Dropout(<span class=\"pl-c1\">0.5</span>)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">V</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.sigmoid(<span class=\"pl-c1\">self</span>.net2(<span class=\"pl-c1\">self</span>.drop(<span class=\"pl-c1\">self</span>.ReLU(<span class=\"pl-c1\">self</span>.bn(<span class=\"pl-c1\">self</span>.net1(V)))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>        return self.sigmoid(self.net2(self.ReLU(self.bn(self.net1(V))))).squeez</span>\n\n\n<span class=\"pl-v\">use_cuda</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n<span class=\"pl-v\">net</span> <span class=\"pl-k\">=</span> TestNet()\n<span class=\"pl-v\">criterion</span> <span class=\"pl-k\">=</span> nn.BCELoss()\n<span class=\"pl-k\">if</span> use_cuda:\n    net.cuda()\n    criterion.cuda()\n    <span class=\"pl-v\">V</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)).cuda()\n    <span class=\"pl-v\">label</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>)).cuda()\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-v\">V</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>))\n    <span class=\"pl-v\">label</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>))\n\n<span class=\"pl-v\">optim_betas</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">0.9</span>, <span class=\"pl-c1\">0.999</span>)\n<span class=\"pl-v\">optimizer</span> <span class=\"pl-k\">=</span> optim.Adam(<span class=\"pl-c1\">filter</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">p</span>: p.requires_grad, net.parameters()), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">betas</span><span class=\"pl-k\">=</span>optim_betas)\n<span class=\"pl-v\">criterion</span> <span class=\"pl-k\">=</span> nn.BCELoss()\n\nnet.train()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1000000</span>):\n    net.zero_grad()\n    <span class=\"pl-v\">r</span> <span class=\"pl-k\">=</span> net(V)\n    <span class=\"pl-v\">err</span> <span class=\"pl-k\">=</span> criterion(r, label)\n    err.backward()<span class=\"pl-c1\">...</span>\n</pre></div>\n<h2>System Info</h2>\n<p>PyTorch version: 0.5.0a0+59f5f9a<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: None</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.9.4</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: No CUDA<br>\nGPU models and configuration: No CUDA<br>\nNvidia driver version: No CUDA<br>\ncuDNN version: No CUDA</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.0)<br>\n[pip] numpydoc (0.7.0)<br>\n[pip] torch (0.5.0a0+59f5f9a)<br>\n[conda] magma-cuda80              2.3.0                         1    pytorch<br>\n[conda] torch                     0.5.0a0+59f5f9a           </p>", "body_text": "Issue description\nin a simple test network I get an error when dropout layer is included (error does not happen when the dropout layer is excluded):\nTraceback (most recent call last):\nFile \"testnet.py\", line 48, in \nr = net(V)\nRuntimeError:\nFunction.apply returned the wrong number of outputs.:\noperation failed in interpreter:\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py(554): dropout\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/modules/dropout.py(53): forward\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(491): call\ntestnet.py(25): forward\ntestnet.py(48): \nCode example\nimport torch\nfrom torch import jit\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim........\n\n  \n@torch.jit.compile(nderivs=1)\nclass TestNet(nn.Module):\n    def __init__(self):\n        super(TestNet, self).__init__()\n        self.net1 = nn.Linear(100, 200)\n        self.net2 = nn.Linear(200, 1)\n        self.bn = nn.BatchNorm1d(200)\n        self.sigmoid = nn.Sigmoid()\n        self.ReLU = nn.ReLU(inplace=False)\n        self.drop = nn.Dropout(0.5)\n    def forward(self, V):\n        return self.sigmoid(self.net2(self.drop(self.ReLU(self.bn(self.net1(V)))\n#        return self.sigmoid(self.net2(self.ReLU(self.bn(self.net1(V))))).squeez\n\n\nuse_cuda = False\nnet = TestNet()\ncriterion = nn.BCELoss()\nif use_cuda:\n    net.cuda()\n    criterion.cuda()\n    V = Variable(torch.randn(100, 100)).cuda()\n    label = Variable(torch.randn(100)).cuda()\nelse:\n    V = Variable(torch.randn(100, 100))\n    label = Variable(torch.randn(100))\n\noptim_betas = (0.9, 0.999)\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01, betas=optim_betas)\ncriterion = nn.BCELoss()\n\nnet.train()\nfor i in range(0,1000000):\n    net.zero_grad()\n    r = net(V)\n    err = criterion(r, label)\n    err.backward()...\n\nSystem Info\nPyTorch version: 0.5.0a0+59f5f9a\nIs debug build: No\nCUDA used to build PyTorch: None\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.9.4\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nVersions of relevant libraries:\n[pip] numpy (1.14.0)\n[pip] numpydoc (0.7.0)\n[pip] torch (0.5.0a0+59f5f9a)\n[conda] magma-cuda80              2.3.0                         1    pytorch\n[conda] torch                     0.5.0a0+59f5f9a", "body": "## Issue description\r\n\r\nin a simple test network I get an error when dropout layer is included (error does not happen when the dropout layer is excluded):\r\n\r\nTraceback (most recent call last):\r\n  File \"testnet.py\", line 48, in <module>\r\n    r = net(V)\r\nRuntimeError: \r\nFunction.apply returned the wrong number of outputs.:\r\noperation failed in interpreter:\r\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py(554): dropout\r\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/modules/dropout.py(53): forward\r\n/home/tester/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(491): __call__\r\ntestnet.py(25): forward\r\ntestnet.py(48): <module>\r\n\r\n## Code example\r\n\r\n```python\r\nimport torch\r\nfrom torch import jit\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport torch.optim as optim........\r\n\r\n  \r\n@torch.jit.compile(nderivs=1)\r\nclass TestNet(nn.Module):\r\n    def __init__(self):\r\n        super(TestNet, self).__init__()\r\n        self.net1 = nn.Linear(100, 200)\r\n        self.net2 = nn.Linear(200, 1)\r\n        self.bn = nn.BatchNorm1d(200)\r\n        self.sigmoid = nn.Sigmoid()\r\n        self.ReLU = nn.ReLU(inplace=False)\r\n        self.drop = nn.Dropout(0.5)\r\n    def forward(self, V):\r\n        return self.sigmoid(self.net2(self.drop(self.ReLU(self.bn(self.net1(V)))\r\n#        return self.sigmoid(self.net2(self.ReLU(self.bn(self.net1(V))))).squeez\r\n\r\n\r\nuse_cuda = False\r\nnet = TestNet()\r\ncriterion = nn.BCELoss()\r\nif use_cuda:\r\n    net.cuda()\r\n    criterion.cuda()\r\n    V = Variable(torch.randn(100, 100)).cuda()\r\n    label = Variable(torch.randn(100)).cuda()\r\nelse:\r\n    V = Variable(torch.randn(100, 100))\r\n    label = Variable(torch.randn(100))\r\n\r\noptim_betas = (0.9, 0.999)\r\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01, betas=optim_betas)\r\ncriterion = nn.BCELoss()\r\n\r\nnet.train()\r\nfor i in range(0,1000000):\r\n    net.zero_grad()\r\n    r = net(V)\r\n    err = criterion(r, label)\r\n    err.backward()...\r\n\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.5.0a0+59f5f9a\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.0)\r\n[pip] numpydoc (0.7.0)\r\n[pip] torch (0.5.0a0+59f5f9a)\r\n[conda] magma-cuda80              2.3.0                         1    pytorch\r\n[conda] torch                     0.5.0a0+59f5f9a           <pip>\r\n\r\n"}