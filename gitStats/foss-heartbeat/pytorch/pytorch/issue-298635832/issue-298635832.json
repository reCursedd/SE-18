{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5309", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5309/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5309/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5309/events", "html_url": "https://github.com/pytorch/pytorch/issues/5309", "id": 298635832, "node_id": "MDU6SXNzdWUyOTg2MzU4MzI=", "number": 5309, "title": "Slow optimization with large vocabulary size embedding matrices", "user": {"login": "ngarneau", "id": 665101, "node_id": "MDQ6VXNlcjY2NTEwMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/665101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngarneau", "html_url": "https://github.com/ngarneau", "followers_url": "https://api.github.com/users/ngarneau/followers", "following_url": "https://api.github.com/users/ngarneau/following{/other_user}", "gists_url": "https://api.github.com/users/ngarneau/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngarneau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngarneau/subscriptions", "organizations_url": "https://api.github.com/users/ngarneau/orgs", "repos_url": "https://api.github.com/users/ngarneau/repos", "events_url": "https://api.github.com/users/ngarneau/events{/privacy}", "received_events_url": "https://api.github.com/users/ngarneau/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-20T15:27:00Z", "updated_at": "2018-02-20T16:19:09Z", "closed_at": "2018-02-20T16:19:09Z", "author_association": "NONE", "body_html": "<p>Hey there,</p>\n<p>I am experiencing some issues with larger embedding matrices (500K) which takes a long time to optimize. I ran a couple of test where you can see some results in this <a href=\"https://gist.github.com/ngarneau/f66829cc4c1ff99beb9a2b7c206bf6bd\">gist</a></p>\n<p>I did some experiment on CPU and GPU and we had same conclusions.<br>\nMy OS is Linux Ubuntu 17.04<br>\nPython 3.6.3<br>\nPytorch version: 0.3.0.post4 installed with pip<br>\nBatch size of 1, embedding size of 100</p>\n<p>Having 500K vocabulary in NLP is a common thing and I wish it could scale to this point.</p>\n<p>Many thanks,<br>\nNicolas</p>", "body_text": "Hey there,\nI am experiencing some issues with larger embedding matrices (500K) which takes a long time to optimize. I ran a couple of test where you can see some results in this gist\nI did some experiment on CPU and GPU and we had same conclusions.\nMy OS is Linux Ubuntu 17.04\nPython 3.6.3\nPytorch version: 0.3.0.post4 installed with pip\nBatch size of 1, embedding size of 100\nHaving 500K vocabulary in NLP is a common thing and I wish it could scale to this point.\nMany thanks,\nNicolas", "body": "Hey there,\r\n\r\nI am experiencing some issues with larger embedding matrices (500K) which takes a long time to optimize. I ran a couple of test where you can see some results in this [gist](https://gist.github.com/ngarneau/f66829cc4c1ff99beb9a2b7c206bf6bd)\r\n\r\nI did some experiment on CPU and GPU and we had same conclusions.\r\nMy OS is Linux Ubuntu 17.04\r\nPython 3.6.3\r\nPytorch version: 0.3.0.post4 installed with pip\r\nBatch size of 1, embedding size of 100\r\n\r\nHaving 500K vocabulary in NLP is a common thing and I wish it could scale to this point.\r\n\r\nMany thanks,\r\nNicolas\r\n"}