{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201850129", "pull_request_review_id": 136384362, "id": 201850129, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTg1MDEyOQ==", "diff_hunk": "@@ -0,0 +1,161 @@\n+#pragma once\n+\n+#include <torch/cuda.h>\n+#include <torch/nn/module.h>\n+#include <torch/nn/pimpl.h>\n+#include <torch/tensor.h>\n+\n+#include <torch/csrc/autograd/functions/comm.h>\n+#include <torch/csrc/cuda/comm.h>\n+\n+#include <ATen/Device.h>\n+#include <ATen/Error.h>\n+#include <ATen/OptionsGuard.h>\n+#include <ATen/Parallel.h>\n+#include <ATen/TensorOptions.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <exception>\n+#include <iostream>\n+#include <memory>\n+#include <mutex>\n+#include <vector>\n+\n+namespace torch {\n+namespace nn {\n+namespace parallel {\n+\n+/// Replicates a module on the given list of devices.\n+/// A replica is created by calling `clone()` on the module. For this, the\n+/// module must inherit from `nn::Cloneable`, or define its own `clone()`\n+/// method, which is expected to perform a deep copy of the module.\n+template <typename ModuleType>\n+std::vector<std::shared_ptr<ModuleType>> replicate(\n+    const std::shared_ptr<ModuleType>& module,\n+    const std::vector<Device>& devices) {\n+  std::vector<std::shared_ptr<ModuleType>> replicas;\n+  replicas.reserve(devices.size());\n+  for (const auto& device : devices) {\n+    // Here we rely on the property tensors are never (or should never be)\n+    // allocated on any particular device, but always the default device, e.g.\n+    // in `torch::ones({3, 4})`, the device is unspecified and pulled from the\n+    // current thread local default options. As such, we can here modify these\n+    // thread local default options and thereby cause all tensors in the cloned\n+    // module to be constructed directly on the device we want.\n+    OptionsGuard guard(device);\n+    replicas.push_back(std::static_pointer_cast<ModuleType>(module->clone()));\n+  }\n+  return replicas;\n+}\n+\n+/// Replicates a module holder on the given list of devices.\n+/// This method allows calling `replicate()` with a module holder, such as\n+/// `Linear`.\n+template <typename ModuleType>\n+std::vector<ModuleHolder<ModuleType>> replicate(\n+    const ModuleHolder<ModuleType>& module,\n+    const std::vector<Device>& devices) {\n+  auto ptrs = replicate(module.ptr(), devices);\n+  return std::vector<ModuleHolder<ModuleType>>(ptrs.begin(), ptrs.end());\n+}\n+\n+/// Applies the given inputs to the given modules in a parallel fashion.\n+/// Conceptually, a thread is spawned for each `(module, input)` pair, in which\n+/// `forward()` is called on the module with its corresponding input. The\n+/// outputs of the individual calls are stored in a vector and returned.\n+///\n+/// Further remarks:\n+/// 1. The length of the module container must match the length of the inputs.\n+/// 2. If a list of devices is supplied, it must match the list of modules in\n+/// length. Each device will be set to the current default device during the\n+/// invocation of the respective module. This means any tensors allocated on the\n+/// default device inside the module will be constructed on this device.\n+template <typename ModuleType>\n+std::vector<Tensor> parallel_apply(\n+    std::vector<ModuleType>& modules,\n+    const std::vector<Tensor>& inputs,\n+    const at::optional<std::vector<Device>>& devices = at::nullopt) {\n+  AT_CHECK(\n+      modules.size() == inputs.size(), \"Must have as many inputs as modules\");\n+  if (devices) {\n+    AT_CHECK(\n+        modules.size() == devices->size(),\n+        \"Must have as many devices as modules\");\n+  }\n+\n+  std::vector<Tensor> outputs(modules.size());\n+  std::mutex mutex;\n+\n+  at::parallel_for(\n+      /*begin=*/0,\n+      /*end=*/modules.size(),\n+      /*grain_size=*/1,\n+      [&modules, &inputs, &devices, &outputs, &mutex](\n+          int64_t index, int64_t stop) {\n+        for (; index < stop; ++index) {\n+          try {\n+            torch::OptionsGuard options_guard(\n+                devices ? (*devices)[index] : inputs[index].device());\n+            auto output = modules[index]->forward(inputs[index]);\n+            std::lock_guard<std::mutex> lock(mutex);\n+            outputs[index] = output;\n+          } catch (const std::exception& exception) {\n+            std::cerr << \"Replica #\" << index\n+                      << \" terminated in parallel_apply() with exception: \"\n+                      << exception.what();\n+            std::terminate();\n+          }\n+        }\n+      });\n+\n+  return outputs;\n+}\n+\n+/// Evaluates `module(input)` in parallel across the given `devices`. If\n+/// `devices` is not supplied, the invocation is parallelized across all\n+/// available CUDA devices. If `output_device` is supplied, the final, combined\n+/// tensor will be placed on this device. If not, it defaults to the first\n+/// device in `devices`.\n+///\n+/// In detail, this method performs the following four distinct steps:\n+/// 1. *Scatter* the input to the given devices,\n+/// 2. *Replicate* (deep clone) the model on each device,\n+/// 3. *Evaluate* each module with its input on its device,\n+/// 4. *Gather* the outputs of each replica into a single output tensor, located\n+/// on the `output_device`.\n+template <typename ModuleType>\n+Tensor data_parallel(\n+    ModuleType module,\n+    Tensor input,\n+    at::optional<std::vector<Device>> devices = at::nullopt,\n+    at::optional<Device> output_device = at::nullopt,\n+    int64_t dim = 0) {\n+  if (!devices) {\n+    const auto device_count = torch::cuda::device_count();\n+    AT_CHECK(device_count > 0, \"Expected at least one CUDA device\");\n+    devices.emplace();\n+    devices->reserve(device_count);\n+    for (size_t index = 0; index < device_count; ++index) {\n+      devices->emplace_back(kCUDA, index);\n+    }\n+  }\n+  if (!output_device) {\n+    output_device = devices->front();\n+  }\n+\n+  autograd::Scatter scatter(*devices, /*chunk_sizes=*/at::nullopt, dim);\n+  auto scattered_inputs = scatter.apply({input});\n+\n+  if (devices->size() == 1) {\n+    return module->forward(input);", "path": "torch/csrc/api/include/torch/nn/parallel/data_parallel.h", "position": null, "original_position": 151, "commit_id": "bcaacf8be9df7fd32007e3528295943a9be46e14", "original_commit_id": "ee79272a6c9444f83f74b66005f86369e9120edf", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can't you move this before the scatter?", "created_at": "2018-07-11T21:46:30Z", "updated_at": "2018-11-23T15:47:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201850129", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201850129"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201850129"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9234"}}, "body_html": "<p>Can't you move this before the scatter?</p>", "body_text": "Can't you move this before the scatter?"}