{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202188639", "pull_request_review_id": 136840808, "id": 202188639, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjE4ODYzOQ==", "diff_hunk": "@@ -0,0 +1,179 @@\n+#include <catch.hpp>\n+\n+#include <torch/csrc/autograd/functions/comm.h>\n+#include <torch/nn/module.h>\n+#include <torch/nn/modules/linear.h>\n+#include <torch/nn/parallel/data_parallel.h>\n+#include <torch/nn/pimpl.h>\n+#include <torch/tensor.h>\n+\n+#include <iostream>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+using namespace torch::autograd;\n+using namespace torch::nn;\n+\n+TEST_CASE(\"Parallel/DifferentiableScatter\", \"[cuda]\") {\n+  Scatter scatter(\n+      {torch::Device(torch::kCUDA, 0), torch::Device(torch::kCUDA, 1)});\n+\n+  auto input = torch::ones(10, torch::requires_grad(true));\n+  auto output = scatter.apply({input});\n+\n+  REQUIRE(output.size() == 2);\n+  REQUIRE(output[0].size(0) == 5);\n+  REQUIRE(output[1].size(0) == 5);\n+\n+  auto sum = output[0].to({torch::kCUDA, 1}) + output[1];\n+  sum.backward();\n+\n+  REQUIRE(input.grad().defined());\n+  REQUIRE(input.grad().device().is_cpu());\n+  REQUIRE(input.grad().sum().toCInt() == 10);\n+}\n+\n+TEST_CASE(\"Parallel/DifferentiableGather\", \"[cuda]\") {\n+  Gather gather(torch::Device(torch::kCUDA, 1));\n+\n+  auto a = torch::ones(5, torch::requires_grad(true).device({torch::kCUDA, 0}));\n+  auto b = torch::ones(5, torch::requires_grad(true).device({torch::kCUDA, 1}));\n+  auto output = gather.apply({a, b});\n+\n+  REQUIRE(output.size() == 1);\n+  REQUIRE(output[0].size(0) == 10);\n+  REQUIRE(output[0].device() == torch::Device(torch::kCUDA, 1));\n+\n+  output[0].backward();\n+\n+  REQUIRE(a.grad().defined());\n+  REQUIRE(a.grad().device() == torch::Device(torch::kCUDA, 0));\n+  REQUIRE(a.grad().sum().toCInt() == 5);\n+\n+  REQUIRE(b.grad().defined());\n+  REQUIRE(b.grad().device() == torch::Device(torch::kCUDA, 1));\n+  REQUIRE(b.grad().sum().toCInt() == 5);\n+}\n+\n+TEST_CASE(\"Parallel/Replicate\", \"[cuda]\") {\n+  Linear linear(3, 4);\n+  auto replicas = parallel::replicate(\n+      linear, {torch::Device(torch::kCUDA, 0), torch::Device(torch::kCUDA, 1)});", "path": "test/cpp/api/parallel.cpp", "position": null, "original_position": 62, "commit_id": "bcaacf8be9df7fd32007e3528295943a9be46e14", "original_commit_id": "ee79272a6c9444f83f74b66005f86369e9120edf", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "Yes good point", "created_at": "2018-07-12T21:43:08Z", "updated_at": "2018-11-23T15:47:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/9234#discussion_r202188639", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202188639"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9234#discussion_r202188639"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9234"}}, "body_html": "<p>Yes good point</p>", "body_text": "Yes good point", "in_reply_to_id": 201850896}