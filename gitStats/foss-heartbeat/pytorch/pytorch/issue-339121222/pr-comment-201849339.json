{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201849339", "pull_request_review_id": 136384362, "id": 201849339, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTg0OTMzOQ==", "diff_hunk": "@@ -0,0 +1,161 @@\n+#pragma once\n+\n+#include <torch/cuda.h>\n+#include <torch/nn/module.h>\n+#include <torch/nn/pimpl.h>\n+#include <torch/tensor.h>\n+\n+#include <torch/csrc/autograd/functions/comm.h>\n+#include <torch/csrc/cuda/comm.h>\n+\n+#include <ATen/Device.h>\n+#include <ATen/Error.h>\n+#include <ATen/OptionsGuard.h>\n+#include <ATen/Parallel.h>\n+#include <ATen/TensorOptions.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <exception>\n+#include <iostream>\n+#include <memory>\n+#include <mutex>\n+#include <vector>\n+\n+namespace torch {\n+namespace nn {\n+namespace parallel {\n+\n+/// Replicates a module on the given list of devices.\n+/// A replica is created by calling `clone()` on the module. For this, the\n+/// module must inherit from `nn::Cloneable`, or define its own `clone()`\n+/// method, which is expected to perform a deep copy of the module.\n+template <typename ModuleType>\n+std::vector<std::shared_ptr<ModuleType>> replicate(\n+    const std::shared_ptr<ModuleType>& module,\n+    const std::vector<Device>& devices) {\n+  std::vector<std::shared_ptr<ModuleType>> replicas;\n+  replicas.reserve(devices.size());\n+  for (const auto& device : devices) {\n+    // Here we rely on the property tensors are never (or should never be)\n+    // allocated on any particular device, but always the default device, e.g.\n+    // in `torch::ones({3, 4})`, the device is unspecified and pulled from the\n+    // current thread local default options. As such, we can here modify these\n+    // thread local default options and thereby cause all tensors in the cloned\n+    // module to be constructed directly on the device we want.\n+    OptionsGuard guard(device);\n+    replicas.push_back(std::static_pointer_cast<ModuleType>(module->clone()));\n+  }\n+  return replicas;\n+}\n+\n+/// Replicates a module holder on the given list of devices.\n+/// This method allows calling `replicate()` with a module holder, such as\n+/// `Linear`.\n+template <typename ModuleType>\n+std::vector<ModuleHolder<ModuleType>> replicate(\n+    const ModuleHolder<ModuleType>& module,\n+    const std::vector<Device>& devices) {\n+  auto ptrs = replicate(module.ptr(), devices);\n+  return std::vector<ModuleHolder<ModuleType>>(ptrs.begin(), ptrs.end());\n+}\n+\n+/// Applies the given inputs to the given modules in a parallel fashion.\n+/// Conceptually, a thread is spawned for each `(module, input)` pair, in which\n+/// `forward()` is called on the module with its corresponding input. The\n+/// outputs of the individual calls are stored in a vector and returned.\n+///\n+/// Further remarks:\n+/// 1. The length of the module container must match the length of the inputs.\n+/// 2. If a list of devices is supplied, it must match the list of modules in\n+/// length. Each device will be set to the current default device during the\n+/// invocation of the respective module. This means any tensors allocated on the\n+/// default device inside the module will be constructed on this device.\n+template <typename ModuleType>\n+std::vector<Tensor> parallel_apply(\n+    std::vector<ModuleType>& modules,\n+    const std::vector<Tensor>& inputs,\n+    const at::optional<std::vector<Device>>& devices = at::nullopt) {\n+  AT_CHECK(\n+      modules.size() == inputs.size(), \"Must have as many inputs as modules\");\n+  if (devices) {\n+    AT_CHECK(\n+        modules.size() == devices->size(),\n+        \"Must have as many devices as modules\");\n+  }\n+\n+  std::vector<Tensor> outputs(modules.size());\n+  std::mutex mutex;\n+\n+  at::parallel_for(\n+      /*begin=*/0,\n+      /*end=*/modules.size(),\n+      /*grain_size=*/1,\n+      [&modules, &inputs, &devices, &outputs, &mutex](\n+          int64_t index, int64_t stop) {\n+        for (; index < stop; ++index) {\n+          try {\n+            torch::OptionsGuard options_guard(\n+                devices ? (*devices)[index] : inputs[index].device());\n+            auto output = modules[index]->forward(inputs[index]);\n+            std::lock_guard<std::mutex> lock(mutex);\n+            outputs[index] = output;\n+          } catch (const std::exception& exception) {\n+            std::cerr << \"Replica #\" << index\n+                      << \" terminated in parallel_apply() with exception: \"\n+                      << exception.what();\n+            std::terminate();", "path": "torch/csrc/api/include/torch/nn/parallel/data_parallel.h", "position": null, "original_position": 107, "commit_id": "bcaacf8be9df7fd32007e3528295943a9be46e14", "original_commit_id": "ee79272a6c9444f83f74b66005f86369e9120edf", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "That's not too elegant (both printing and terminating unconditionally). We stash the exception in Python land and reraise it later. I think it's possible even in C++ using `std::exception_ptr`", "created_at": "2018-07-11T21:43:31Z", "updated_at": "2018-11-23T15:47:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201849339", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201849339"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201849339"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9234"}}, "body_html": "<p>That's not too elegant (both printing and terminating unconditionally). We stash the exception in Python land and reraise it later. I think it's possible even in C++ using <code>std::exception_ptr</code></p>", "body_text": "That's not too elegant (both printing and terminating unconditionally). We stash the exception in Python land and reraise it later. I think it's possible even in C++ using std::exception_ptr"}