{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202188277", "pull_request_review_id": 136840387, "id": 202188277, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjE4ODI3Nw==", "diff_hunk": "@@ -0,0 +1,179 @@\n+#include <catch.hpp>\n+\n+#include <torch/csrc/autograd/functions/comm.h>\n+#include <torch/nn/module.h>\n+#include <torch/nn/modules/linear.h>\n+#include <torch/nn/parallel/data_parallel.h>\n+#include <torch/nn/pimpl.h>\n+#include <torch/tensor.h>\n+\n+#include <iostream>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+using namespace torch::autograd;\n+using namespace torch::nn;\n+\n+TEST_CASE(\"Parallel/DifferentiableScatter\", \"[cuda]\") {\n+  Scatter scatter(\n+      {torch::Device(torch::kCUDA, 0), torch::Device(torch::kCUDA, 1)});\n+\n+  auto input = torch::ones(10, torch::requires_grad(true));\n+  auto output = scatter.apply({input});\n+\n+  REQUIRE(output.size() == 2);\n+  REQUIRE(output[0].size(0) == 5);\n+  REQUIRE(output[1].size(0) == 5);\n+\n+  auto sum = output[0].to({torch::kCUDA, 1}) + output[1];\n+  sum.backward();\n+\n+  REQUIRE(input.grad().defined());\n+  REQUIRE(input.grad().device().is_cpu());\n+  REQUIRE(input.grad().sum().toCInt() == 10);\n+}\n+\n+TEST_CASE(\"Parallel/DifferentiableGather\", \"[cuda]\") {\n+  Gather gather(torch::Device(torch::kCUDA, 1));\n+\n+  auto a = torch::ones(5, torch::requires_grad(true).device({torch::kCUDA, 0}));\n+  auto b = torch::ones(5, torch::requires_grad(true).device({torch::kCUDA, 1}));\n+  auto output = gather.apply({a, b});\n+\n+  REQUIRE(output.size() == 1);\n+  REQUIRE(output[0].size(0) == 10);\n+  REQUIRE(output[0].device() == torch::Device(torch::kCUDA, 1));\n+\n+  output[0].backward();\n+\n+  REQUIRE(a.grad().defined());\n+  REQUIRE(a.grad().device() == torch::Device(torch::kCUDA, 0));\n+  REQUIRE(a.grad().sum().toCInt() == 5);\n+\n+  REQUIRE(b.grad().defined());\n+  REQUIRE(b.grad().device() == torch::Device(torch::kCUDA, 1));\n+  REQUIRE(b.grad().sum().toCInt() == 5);\n+}\n+\n+TEST_CASE(\"Parallel/Replicate\", \"[cuda]\") {\n+  Linear linear(3, 4);\n+  auto replicas = parallel::replicate(\n+      linear, {torch::Device(torch::kCUDA, 0), torch::Device(torch::kCUDA, 1)});\n+  REQUIRE(replicas.size() == 2);\n+\n+  auto original_parameters = linear->parameters();\n+\n+  auto replica1_parameters = replicas[0]->parameters();\n+  for (auto& parameter : replica1_parameters) {\n+    REQUIRE(parameter->device() == torch::Device(torch::kCUDA, 0));\n+  }\n+  replicas[0]->to(torch::kCPU);\n+  REQUIRE(replica1_parameters.size() == original_parameters.size());\n+  for (size_t i = 0; i < original_parameters.size(); ++i) {\n+    REQUIRE(replica1_parameters[i]->allclose(*original_parameters[i]));\n+    REQUIRE(\n+        replica1_parameters[i]->data().data<float>() !=\n+        original_parameters[i]->data().data<float>());\n+  }\n+\n+  auto replica2_parameters = replicas[1]->parameters();\n+  for (auto& parameter : replica2_parameters) {\n+    REQUIRE(parameter->device() == torch::Device(torch::kCUDA, 1));\n+  }\n+  replicas[1]->to(torch::kCPU);\n+  REQUIRE(replica2_parameters.size() == original_parameters.size());\n+  for (size_t i = 0; i < original_parameters.size(); ++i) {\n+    REQUIRE(replica2_parameters[i]->allclose(*original_parameters[i]));\n+    REQUIRE(\n+        replica2_parameters[i]->data().data<float>() !=\n+        original_parameters[i]->data().data<float>());\n+  }\n+}", "path": "test/cpp/api/parallel.cpp", "position": 103, "original_position": 92, "commit_id": "bcaacf8be9df7fd32007e3528295943a9be46e14", "original_commit_id": "ee79272a6c9444f83f74b66005f86369e9120edf", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "I personally prefer to do this kind of duplication in test code as long as its not unreasonable, it makes debugging easier (e.g. the line information about where the assertion hit tells you more than if it was in a loop)", "created_at": "2018-07-12T21:41:39Z", "updated_at": "2018-11-23T15:47:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/9234#discussion_r202188277", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202188277"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9234#discussion_r202188277"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9234"}}, "body_html": "<p>I personally prefer to do this kind of duplication in test code as long as its not unreasonable, it makes debugging easier (e.g. the line information about where the assertion hit tells you more than if it was in a loop)</p>", "body_text": "I personally prefer to do this kind of duplication in test code as long as its not unreasonable, it makes debugging easier (e.g. the line information about where the assertion hit tells you more than if it was in a loop)", "in_reply_to_id": 201851037}