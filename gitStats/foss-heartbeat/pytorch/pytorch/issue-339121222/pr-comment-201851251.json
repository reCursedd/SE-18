{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201851251", "pull_request_review_id": 136384362, "id": 201851251, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTg1MTI1MQ==", "diff_hunk": "@@ -0,0 +1,179 @@\n+#include <catch.hpp>\n+\n+#include <torch/csrc/autograd/functions/comm.h>\n+#include <torch/nn/module.h>\n+#include <torch/nn/modules/linear.h>\n+#include <torch/nn/parallel/data_parallel.h>\n+#include <torch/nn/pimpl.h>\n+#include <torch/tensor.h>\n+\n+#include <iostream>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+using namespace torch::autograd;\n+using namespace torch::nn;\n+\n+TEST_CASE(\"Parallel/DifferentiableScatter\", \"[cuda]\") {\n+  Scatter scatter(\n+      {torch::Device(torch::kCUDA, 0), torch::Device(torch::kCUDA, 1)});\n+\n+  auto input = torch::ones(10, torch::requires_grad(true));\n+  auto output = scatter.apply({input});\n+\n+  REQUIRE(output.size() == 2);\n+  REQUIRE(output[0].size(0) == 5);\n+  REQUIRE(output[1].size(0) == 5);\n+\n+  auto sum = output[0].to({torch::kCUDA, 1}) + output[1];\n+  sum.backward();\n+\n+  REQUIRE(input.grad().defined());\n+  REQUIRE(input.grad().device().is_cpu());\n+  REQUIRE(input.grad().sum().toCInt() == 10);\n+}\n+\n+TEST_CASE(\"Parallel/DifferentiableGather\", \"[cuda]\") {\n+  Gather gather(torch::Device(torch::kCUDA, 1));\n+\n+  auto a = torch::ones(5, torch::requires_grad(true).device({torch::kCUDA, 0}));\n+  auto b = torch::ones(5, torch::requires_grad(true).device({torch::kCUDA, 1}));\n+  auto output = gather.apply({a, b});\n+\n+  REQUIRE(output.size() == 1);\n+  REQUIRE(output[0].size(0) == 10);\n+  REQUIRE(output[0].device() == torch::Device(torch::kCUDA, 1));\n+\n+  output[0].backward();\n+\n+  REQUIRE(a.grad().defined());\n+  REQUIRE(a.grad().device() == torch::Device(torch::kCUDA, 0));\n+  REQUIRE(a.grad().sum().toCInt() == 5);\n+\n+  REQUIRE(b.grad().defined());\n+  REQUIRE(b.grad().device() == torch::Device(torch::kCUDA, 1));\n+  REQUIRE(b.grad().sum().toCInt() == 5);\n+}\n+\n+TEST_CASE(\"Parallel/Replicate\", \"[cuda]\") {\n+  Linear linear(3, 4);\n+  auto replicas = parallel::replicate(\n+      linear, {torch::Device(torch::kCUDA, 0), torch::Device(torch::kCUDA, 1)});\n+  REQUIRE(replicas.size() == 2);\n+\n+  auto original_parameters = linear->parameters();\n+\n+  auto replica1_parameters = replicas[0]->parameters();\n+  for (auto& parameter : replica1_parameters) {\n+    REQUIRE(parameter->device() == torch::Device(torch::kCUDA, 0));\n+  }\n+  replicas[0]->to(torch::kCPU);\n+  REQUIRE(replica1_parameters.size() == original_parameters.size());\n+  for (size_t i = 0; i < original_parameters.size(); ++i) {\n+    REQUIRE(replica1_parameters[i]->allclose(*original_parameters[i]));\n+    REQUIRE(\n+        replica1_parameters[i]->data().data<float>() !=\n+        original_parameters[i]->data().data<float>());\n+  }\n+\n+  auto replica2_parameters = replicas[1]->parameters();\n+  for (auto& parameter : replica2_parameters) {\n+    REQUIRE(parameter->device() == torch::Device(torch::kCUDA, 1));\n+  }\n+  replicas[1]->to(torch::kCPU);\n+  REQUIRE(replica2_parameters.size() == original_parameters.size());\n+  for (size_t i = 0; i < original_parameters.size(); ++i) {\n+    REQUIRE(replica2_parameters[i]->allclose(*original_parameters[i]));\n+    REQUIRE(\n+        replica2_parameters[i]->data().data<float>() !=\n+        original_parameters[i]->data().data<float>());\n+  }\n+}\n+\n+TEST_CASE(\"Parallel/ParallelApply\", \"[cuda]\") {\n+  Linear a(3, 4);\n+\n+  Linear b(std::static_pointer_cast<LinearImpl>(a->clone()));\n+  b->to({torch::kCUDA, 0});\n+\n+  Linear c(std::static_pointer_cast<LinearImpl>(a->clone()));\n+  c->to({torch::kCUDA, 1});\n+\n+  std::vector<Linear> modules = {a, b, c};\n+  std::vector<torch::Tensor> inputs = {\n+      torch::ones({2, 3}),\n+      torch::ones({2, 3}, torch::device({torch::kCUDA, 0})),\n+      torch::ones({2, 3}, torch::device({torch::kCUDA, 1}))};\n+\n+  auto outputs = parallel::parallel_apply(modules, inputs);\n+\n+  REQUIRE(outputs.size() == 3);\n+  REQUIRE(outputs[0].device().is_cpu());\n+\n+  REQUIRE(outputs[1].device() == torch::Device(torch::kCUDA, 0));\n+  REQUIRE(outputs[1].to(torch::kCPU).allclose(outputs[0]));\n+\n+  REQUIRE(outputs[2].device() == torch::Device(torch::kCUDA, 1));\n+  REQUIRE(outputs[2].to(torch::kCPU).allclose(outputs[0]));\n+}\n+\n+TEST_CASE(\"Parallel/ParallelApplyWithDifferentOutputDevice\", \"[cuda]\") {\n+  struct M : torch::nn::Module {\n+    torch::Tensor forward(torch::Tensor input) {\n+      return torch::ones({5}, torch::dtype(torch::kInt32));\n+    }\n+  };\n+\n+  std::vector<std::shared_ptr<M>> modules = {\n+      std::make_shared<M>(), std::make_shared<M>(), std::make_shared<M>()};\n+  std::vector<torch::Tensor> inputs = {\n+      torch::empty({}), torch::empty({}), torch::empty({})};\n+  std::vector<torch::Device> devices = {\n+      {torch::kCUDA, 1}, {torch::kCUDA, 0}, {torch::kCPU}};\n+\n+  auto outputs = parallel::parallel_apply(modules, inputs, devices);\n+\n+  REQUIRE(outputs.size() == 3);\n+  REQUIRE(outputs[0].device().is_cuda());\n+  REQUIRE(outputs[0].device() == torch::Device(torch::kCUDA, 1));\n+\n+  REQUIRE(outputs[1].device().is_cuda());\n+  REQUIRE(outputs[1].device() == torch::Device(torch::kCUDA, 0));\n+\n+  REQUIRE(outputs[2].device().is_cpu());\n+}\n+\n+TEST_CASE(\n+    \"Parallel/DataParallelPlacesTheOutputOnTheRequestedDevice\",\n+    \"[cuda]\") {\n+  Linear linear(3, 4);\n+  auto input = torch::ones({10, 3});\n+  auto output = parallel::data_parallel(\n+      linear,\n+      input,\n+      /*devices=*/at::nullopt,\n+      /*output_device=*/torch::Device(torch::kCUDA, 1));", "path": "test/cpp/api/parallel.cpp", "position": null, "original_position": 156, "commit_id": "bcaacf8be9df7fd32007e3528295943a9be46e14", "original_commit_id": "ee79272a6c9444f83f74b66005f86369e9120edf", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Again, ignore if don't have 2 GPUs?", "created_at": "2018-07-11T21:51:12Z", "updated_at": "2018-11-23T15:47:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201851251", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201851251"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201851251"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9234"}}, "body_html": "<p>Again, ignore if don't have 2 GPUs?</p>", "body_text": "Again, ignore if don't have 2 GPUs?"}