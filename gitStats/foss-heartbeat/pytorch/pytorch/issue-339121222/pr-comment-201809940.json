{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201809940", "pull_request_review_id": 136384362, "id": 201809940, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTgwOTk0MA==", "diff_hunk": "@@ -0,0 +1,131 @@\n+#include <torch/csrc/autograd/functions/comm.h>\n+\n+#include <torch/csrc/autograd/function.h>\n+#include <torch/csrc/autograd/functions/utils.h>\n+#include <torch/csrc/autograd/variable.h>\n+#include <torch/csrc/cuda/comm.h>\n+#include <torch/csrc/utils/functional.h>\n+\n+#include <ATen/ATen.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <memory>\n+#include <vector>\n+\n+namespace torch {\n+namespace autograd {\n+Scatter::Scatter(\n+    std::vector<at::Device> devices,\n+    const at::optional<std::vector<int64_t>>& chunk_sizes,\n+    int64_t dim,\n+    const at::optional<std::vector<at::CUDAStream>>& streams,\n+    bool unsqueeze_scalars)\n+    : devices_(std::move(devices)),\n+      chunk_sizes_(chunk_sizes),\n+      dim_(dim),\n+      streams_(streams),\n+      unsqueeze_scalars_(unsqueeze_scalars) {}\n+\n+variable_list Scatter::apply(const variable_list& inputs) {\n+  AT_ASSERT(inputs.size() == 1);\n+  auto& input = inputs.front();\n+\n+  std::shared_ptr<Function> grad_fn;\n+  if (compute_requires_grad(input)) {\n+    grad_fn =\n+        std::make_shared<Gather>(/*destination_device=*/input.device(), dim_);\n+    grad_fn->set_next_edges(collect_next_edges(input));\n+  }\n+\n+#ifdef USE_CUDA\n+  auto device_indices = fmap(devices_, [](const at::Device& device) -> int64_t {\n+    return device.index();\n+  });\n+  std::vector<at::Tensor> tensors;\n+  tensors =\n+      torch::cuda::scatter(input, device_indices, chunk_sizes_, dim_, streams_);\n+\n+  std::vector<Variable> variables;\n+  for (auto& tensor : tensors) {\n+    AT_ASSERT(tensor.defined());\n+    if (unsqueeze_scalars_) {\n+      AT_ASSERT(tensor.dim() == 1 && tensor.numel() == 1);\n+      variables.push_back(tensor[0]);\n+    } else {\n+      variables.push_back(tensor);\n+    }\n+  }\n+\n+  set_history(variables, grad_fn);\n+\n+  return variables;\n+#else\n+  AT_ERROR(\"Scatter is only supported in CUDA environments\");\n+#endif\n+}\n+\n+Gather::Gather(const at::Device& destination_device, int64_t dim)\n+    : destination_device_(destination_device), dim_(dim) {}\n+\n+variable_list Gather::apply(const variable_list& inputs) {\n+  bool all_are_zero_dim = true;\n+  for (auto& input : inputs) {\n+    AT_CHECK(\n+        input.is_cuda(),\n+        \"All inputs to Gather must be CUDA tensors, got \",\n+        input.type());\n+    if (input.dim() > 0) {\n+      all_are_zero_dim = false;\n+    }\n+  }\n+\n+  bool unsqueeze_scalars = all_are_zero_dim && dim_ == 0;\n+  if (unsqueeze_scalars) {\n+    AT_WARN(\n+        \"Was asked to gather along dimension 0, but all \"\n+        \"input tensors were scalars; will instead unsqueeze \"\n+        \"and return a vector.\");\n+  }\n+\n+  std::vector<at::Tensor> tensors;", "path": "torch/csrc/autograd/functions/comm.cpp", "position": 92, "original_position": 91, "commit_id": "bcaacf8be9df7fd32007e3528295943a9be46e14", "original_commit_id": "ee79272a6c9444f83f74b66005f86369e9120edf", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "`.reserve()` please", "created_at": "2018-07-11T19:16:09Z", "updated_at": "2018-11-23T15:47:10Z", "html_url": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201809940", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201809940"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201809940"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9234"}}, "body_html": "<p><code>.reserve()</code> please</p>", "body_text": ".reserve() please"}