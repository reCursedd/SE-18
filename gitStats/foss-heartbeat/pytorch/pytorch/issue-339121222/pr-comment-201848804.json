{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201848804", "pull_request_review_id": 136384362, "id": 201848804, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTg0ODgwNA==", "diff_hunk": "@@ -0,0 +1,161 @@\n+#pragma once\n+\n+#include <torch/cuda.h>\n+#include <torch/nn/module.h>\n+#include <torch/nn/pimpl.h>\n+#include <torch/tensor.h>\n+\n+#include <torch/csrc/autograd/functions/comm.h>\n+#include <torch/csrc/cuda/comm.h>\n+\n+#include <ATen/Device.h>\n+#include <ATen/Error.h>\n+#include <ATen/OptionsGuard.h>\n+#include <ATen/Parallel.h>\n+#include <ATen/TensorOptions.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <exception>\n+#include <iostream>\n+#include <memory>\n+#include <mutex>\n+#include <vector>\n+\n+namespace torch {\n+namespace nn {\n+namespace parallel {\n+\n+/// Replicates a module on the given list of devices.\n+/// A replica is created by calling `clone()` on the module. For this, the\n+/// module must inherit from `nn::Cloneable`, or define its own `clone()`\n+/// method, which is expected to perform a deep copy of the module.\n+template <typename ModuleType>\n+std::vector<std::shared_ptr<ModuleType>> replicate(\n+    const std::shared_ptr<ModuleType>& module,\n+    const std::vector<Device>& devices) {\n+  std::vector<std::shared_ptr<ModuleType>> replicas;\n+  replicas.reserve(devices.size());\n+  for (const auto& device : devices) {\n+    // Here we rely on the property tensors are never (or should never be)\n+    // allocated on any particular device, but always the default device, e.g.\n+    // in `torch::ones({3, 4})`, the device is unspecified and pulled from the\n+    // current thread local default options. As such, we can here modify these\n+    // thread local default options and thereby cause all tensors in the cloned\n+    // module to be constructed directly on the device we want.\n+    OptionsGuard guard(device);\n+    replicas.push_back(std::static_pointer_cast<ModuleType>(module->clone()));", "path": "torch/csrc/api/include/torch/nn/parallel/data_parallel.h", "position": 46, "original_position": 47, "commit_id": "bcaacf8be9df7fd32007e3528295943a9be46e14", "original_commit_id": "ee79272a6c9444f83f74b66005f86369e9120edf", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Hmm that's a bit surprising. I'd expect `clone()` to completely ignore the thread local device and just put them wherever the original params where. This way makes sense too I guess \ud83d\ude15 ", "created_at": "2018-07-11T21:41:28Z", "updated_at": "2018-11-23T15:47:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201848804", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201848804"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9234#discussion_r201848804"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9234"}}, "body_html": "<p>Hmm that's a bit surprising. I'd expect <code>clone()</code> to completely ignore the thread local device and just put them wherever the original params where. This way makes sense too I guess <g-emoji class=\"g-emoji\" alias=\"confused\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f615.png\">\ud83d\ude15</g-emoji></p>", "body_text": "Hmm that's a bit surprising. I'd expect clone() to completely ignore the thread local device and just put them wherever the original params where. This way makes sense too I guess \ud83d\ude15"}