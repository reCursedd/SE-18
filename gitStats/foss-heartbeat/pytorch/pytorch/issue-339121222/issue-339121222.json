{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9234", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9234/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9234/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9234/events", "html_url": "https://github.com/pytorch/pytorch/pull/9234", "id": 339121222, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk5ODgyOTMw", "number": 9234, "title": "[C++ API] Functional DataParallel", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-07T06:18:19Z", "updated_at": "2018-11-23T15:47:27Z", "closed_at": "2018-07-19T23:13:08Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9234", "html_url": "https://github.com/pytorch/pytorch/pull/9234", "diff_url": "https://github.com/pytorch/pytorch/pull/9234.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9234.patch"}, "body_html": "<p>This PR adds the functional version of <code>DataParallel</code> (i.e. <code>data_parallel</code>) to the C++ frontend.</p>\n<p>For this, I had to:</p>\n<ol>\n<li>Add \"differentiable\" versions of scatter and gather, which perform their inverse operation in the backward pass, to C++. I've added them under <code>torch/csrc/autograd/functions/comm.{h,cpp}</code>. I had to move some utilities from <code>VariableType.cpp</code> into <code>torch/csrc/autograd/functions/utils.h</code>, and changed them a bit to fix the <code>const_cast</code>s for which there were <code>TODO</code>s,</li>\n<li>Implement the <code>replicate</code>, <code>parallel_apply</code> and the combining <code>data_parallel</code> functions in C++.</li>\n</ol>\n<p><code>replicate</code> is implemented based on our existing <code>clone()</code> interface, along with the ability to set the current device via <code>at::OptionsGuard</code> (so nice).</p>\n<p><code>parallel_apply</code> is implemented using <code>at::parallel_for</code> (CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a>) and <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/parallel_apply.py\">follows the code from PyTorch</a>.</p>\n<p>Added lots of tests for these things.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3605224\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebetica\">@ebetica</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>", "body_text": "This PR adds the functional version of DataParallel (i.e. data_parallel) to the C++ frontend.\nFor this, I had to:\n\nAdd \"differentiable\" versions of scatter and gather, which perform their inverse operation in the backward pass, to C++. I've added them under torch/csrc/autograd/functions/comm.{h,cpp}. I had to move some utilities from VariableType.cpp into torch/csrc/autograd/functions/utils.h, and changed them a bit to fix the const_casts for which there were TODOs,\nImplement the replicate, parallel_apply and the combining data_parallel functions in C++.\n\nreplicate is implemented based on our existing clone() interface, along with the ability to set the current device via at::OptionsGuard (so nice).\nparallel_apply is implemented using at::parallel_for (CC @cpuhrsch) and follows the code from PyTorch.\nAdded lots of tests for these things.\n@apaszke @ezyang @ebetica @colesbury", "body": "This PR adds the functional version of `DataParallel` (i.e. `data_parallel`) to the C++ frontend.\r\n\r\nFor this, I had to:\r\n1. Add \"differentiable\" versions of scatter and gather, which perform their inverse operation in the backward pass, to C++. I've added them under `torch/csrc/autograd/functions/comm.{h,cpp}`. I had to move some utilities from `VariableType.cpp` into `torch/csrc/autograd/functions/utils.h`, and changed them a bit to fix the `const_cast`s for which there were `TODO`s,\r\n2. Implement the `replicate`, `parallel_apply` and the combining `data_parallel` functions in C++.\r\n\r\n`replicate` is implemented based on our existing `clone()` interface, along with the ability to set the current device via `at::OptionsGuard` (so nice).\r\n\r\n`parallel_apply` is implemented using `at::parallel_for` (CC @cpuhrsch) and [follows the code from PyTorch](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/parallel_apply.py).\r\n\r\nAdded lots of tests for these things.\r\n\r\n@apaszke @ezyang @ebetica @colesbury "}