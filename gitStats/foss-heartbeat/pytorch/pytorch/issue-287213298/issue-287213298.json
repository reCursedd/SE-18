{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4564", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4564/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4564/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4564/events", "html_url": "https://github.com/pytorch/pytorch/issues/4564", "id": 287213298, "node_id": "MDU6SXNzdWUyODcyMTMyOTg=", "number": 4564, "title": "Mixed Tensor/TensorList arguments in ATen functions with explicit derivatives", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-01-09T20:07:29Z", "updated_at": "2018-01-19T01:19:11Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Today, you cannot write a derivative for a function with signature <code>f(Tensor, TensorList)</code> (or any combination thereof) because our code generation does not support it.</p>\n<p>The primary technical barrier to implementing this is the fact that autograd internals works with a completely flat list of tensors. To support mixed Tensor/TensorList arguments in general, one would also have to record enough metadata to reconstruct the intended groupings. This is only required when there are multiple TensorList arguments; with a single TensorList argument the correct grouping can be unambiguously inferred (though it takes some work to do so.)</p>\n<p>We should decide what we want to do long term. Is it:</p>\n<ol>\n<li>Never support mixed Tensor/TensorList; instead, require users to somehow work around the restriction in some way. (For example, <code>index(Tensor, TensorList)</code> can be decomposed into a series of differentiable operations that don't require mixed arguments.)</li>\n<li>Support mixed Tensor plus a single TensorList. In this regime no special metadata is needed, and the wrapping code can look solely at the calling convention to determine how to unwrap</li>\n<li>Support arbitrary mixed Tensor and TensorList. In this regime some extra metadata is needed to resolve the wrapping/unwrapping. If written in a robust way, this regime could be generalized to support arbitrary nested structures.</li>\n</ol>\n<p>An ancillary consideration is ONNX, which also adopts the \"single, flat list of tensors\" model. ONNX has effectively bet that operators with complicated input tensor structures are unlikely, and you pay a big complexity cost if this is not the case.</p>", "body_text": "Today, you cannot write a derivative for a function with signature f(Tensor, TensorList) (or any combination thereof) because our code generation does not support it.\nThe primary technical barrier to implementing this is the fact that autograd internals works with a completely flat list of tensors. To support mixed Tensor/TensorList arguments in general, one would also have to record enough metadata to reconstruct the intended groupings. This is only required when there are multiple TensorList arguments; with a single TensorList argument the correct grouping can be unambiguously inferred (though it takes some work to do so.)\nWe should decide what we want to do long term. Is it:\n\nNever support mixed Tensor/TensorList; instead, require users to somehow work around the restriction in some way. (For example, index(Tensor, TensorList) can be decomposed into a series of differentiable operations that don't require mixed arguments.)\nSupport mixed Tensor plus a single TensorList. In this regime no special metadata is needed, and the wrapping code can look solely at the calling convention to determine how to unwrap\nSupport arbitrary mixed Tensor and TensorList. In this regime some extra metadata is needed to resolve the wrapping/unwrapping. If written in a robust way, this regime could be generalized to support arbitrary nested structures.\n\nAn ancillary consideration is ONNX, which also adopts the \"single, flat list of tensors\" model. ONNX has effectively bet that operators with complicated input tensor structures are unlikely, and you pay a big complexity cost if this is not the case.", "body": "Today, you cannot write a derivative for a function with signature `f(Tensor, TensorList)` (or any combination thereof) because our code generation does not support it.\r\n\r\nThe primary technical barrier to implementing this is the fact that autograd internals works with a completely flat list of tensors. To support mixed Tensor/TensorList arguments in general, one would also have to record enough metadata to reconstruct the intended groupings. This is only required when there are multiple TensorList arguments; with a single TensorList argument the correct grouping can be unambiguously inferred (though it takes some work to do so.)\r\n\r\nWe should decide what we want to do long term. Is it:\r\n\r\n1. Never support mixed Tensor/TensorList; instead, require users to somehow work around the restriction in some way. (For example, `index(Tensor, TensorList)` can be decomposed into a series of differentiable operations that don't require mixed arguments.)\r\n2. Support mixed Tensor plus a single TensorList. In this regime no special metadata is needed, and the wrapping code can look solely at the calling convention to determine how to unwrap\r\n3. Support arbitrary mixed Tensor and TensorList. In this regime some extra metadata is needed to resolve the wrapping/unwrapping. If written in a robust way, this regime could be generalized to support arbitrary nested structures.\r\n\r\nAn ancillary consideration is ONNX, which also adopts the \"single, flat list of tensors\" model. ONNX has effectively bet that operators with complicated input tensor structures are unlikely, and you pay a big complexity cost if this is not the case."}