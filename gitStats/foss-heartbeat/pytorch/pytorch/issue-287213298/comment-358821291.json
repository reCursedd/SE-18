{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358821291", "html_url": "https://github.com/pytorch/pytorch/issues/4564#issuecomment-358821291", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4564", "id": 358821291, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODgyMTI5MQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-18T23:53:22Z", "updated_at": "2018-01-19T01:19:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>Step 2: handle AutogradFunction generation for mixed outputs in backwards.</strong></p>\n<p>Here's what the codegen produces today:</p>\n<pre><code>variable_list SmoketestConsBackward::apply(const variable_list&amp; grads) {\n  variable_list grad_inputs{};\n  if (should_compute_output({ 0, * })) {\n    std::tie(grad_inputs[0], grad_inputs[*]) = smoketest_cons_backward(grads);\n  }\n  return grad_inputs;      \n}\n</code></pre>\n<p>This doesn't even syntax check.</p>\n<ol>\n<li>\n<p>We should assume that <code>smoketest_cons_backward</code> returns <code>std::tuple&lt;Tensor, std::vector&lt;Tensor&gt;&gt;</code>. Consequently, we need to flatten this tuple, feeding it into the <code>grad_inputs</code>. If we want to live fast and loose, we can just walk the tuple, writing the outputs into <code>grad_inputs</code>. A safer approach is to verify that the returned vectors have the correct sizes (otherwise tensors afterwards could be offset, leading to very strange bugs.)</p>\n</li>\n<li>\n<p>We need to determine if we should compute output. Here, once again, we need to know what the shapes of the returned vectors are so that we can run <code>should_compute_output</code> on them.</p>\n</li>\n</ol>\n<p>We need to save information about the sizes of TensorList inputs.  So we will need a struct like this:</p>\n<pre><code>struct SmoketestConsBackward : public TraceableFunction {\n  using TraceableFunction::TraceableFunction;\n  int64_t xs_size;\n  ...\n};\n</code></pre>\n<p>A direct rewrite of the code now looks like this:</p>\n<pre><code>variable_list SmoketestConsBackward::apply(const variable_list&amp; grads) {\n  variable_list grad_inputs{};\n  if (should_compute_output({ 0 }) &amp;&amp; should_compute_output_range(1, 1 + xs_size)) {\n    auto result = smoketest_cons_backward(grads);\n    grad_inputs[0] = std::get&lt;0&gt;(result);\n    copy_range(grad_inputs, 1, xs_size, std::get&lt;1&gt;(result))\n  }\n  return grad_inputs;      \n}\n</code></pre>\n<p>One irritating thing about the code generation scheme above is that we have to compute offsets for every Tensor/TensorList after the first one, leading to expressions like <code>xs_size + 2 + ys_size + 3 + ...</code>. A modest new abstraction can help avoid this situation:</p>\n<pre><code>variable_list SmoketestConsBackward::apply(const variable_list&amp; grads) {\n  variable_list grad_inputs{};\n  IndexRangeGenerator igen;\n  std::pair&lt;size_t, size_t&gt; x_ix = igen.single();\n  std::pair&lt;size_t, size_t&gt; xs_ix = igen.range(xs_size);\n  if (should_compute_output({x_ix, xs_ix})) {\n    auto result = smoketest_cons_backward(grads);\n    copy_range(grad_inputs, x_ix, std::get&lt;0&gt;(result));\n    copy_range(grad_inputs, xs_ix, std::get&lt;1&gt;(result));\n  }\n  return grad_inputs;      \n}\n</code></pre>\n<p>After this is done, we have the first half of mixed Tensor/TensorList support, which supports mixed forward inputs / mixed backward outputs.</p>", "body_text": "Step 2: handle AutogradFunction generation for mixed outputs in backwards.\nHere's what the codegen produces today:\nvariable_list SmoketestConsBackward::apply(const variable_list& grads) {\n  variable_list grad_inputs{};\n  if (should_compute_output({ 0, * })) {\n    std::tie(grad_inputs[0], grad_inputs[*]) = smoketest_cons_backward(grads);\n  }\n  return grad_inputs;      \n}\n\nThis doesn't even syntax check.\n\n\nWe should assume that smoketest_cons_backward returns std::tuple<Tensor, std::vector<Tensor>>. Consequently, we need to flatten this tuple, feeding it into the grad_inputs. If we want to live fast and loose, we can just walk the tuple, writing the outputs into grad_inputs. A safer approach is to verify that the returned vectors have the correct sizes (otherwise tensors afterwards could be offset, leading to very strange bugs.)\n\n\nWe need to determine if we should compute output. Here, once again, we need to know what the shapes of the returned vectors are so that we can run should_compute_output on them.\n\n\nWe need to save information about the sizes of TensorList inputs.  So we will need a struct like this:\nstruct SmoketestConsBackward : public TraceableFunction {\n  using TraceableFunction::TraceableFunction;\n  int64_t xs_size;\n  ...\n};\n\nA direct rewrite of the code now looks like this:\nvariable_list SmoketestConsBackward::apply(const variable_list& grads) {\n  variable_list grad_inputs{};\n  if (should_compute_output({ 0 }) && should_compute_output_range(1, 1 + xs_size)) {\n    auto result = smoketest_cons_backward(grads);\n    grad_inputs[0] = std::get<0>(result);\n    copy_range(grad_inputs, 1, xs_size, std::get<1>(result))\n  }\n  return grad_inputs;      \n}\n\nOne irritating thing about the code generation scheme above is that we have to compute offsets for every Tensor/TensorList after the first one, leading to expressions like xs_size + 2 + ys_size + 3 + .... A modest new abstraction can help avoid this situation:\nvariable_list SmoketestConsBackward::apply(const variable_list& grads) {\n  variable_list grad_inputs{};\n  IndexRangeGenerator igen;\n  std::pair<size_t, size_t> x_ix = igen.single();\n  std::pair<size_t, size_t> xs_ix = igen.range(xs_size);\n  if (should_compute_output({x_ix, xs_ix})) {\n    auto result = smoketest_cons_backward(grads);\n    copy_range(grad_inputs, x_ix, std::get<0>(result));\n    copy_range(grad_inputs, xs_ix, std::get<1>(result));\n  }\n  return grad_inputs;      \n}\n\nAfter this is done, we have the first half of mixed Tensor/TensorList support, which supports mixed forward inputs / mixed backward outputs.", "body": "**Step 2: handle AutogradFunction generation for mixed outputs in backwards.**\r\n\r\nHere's what the codegen produces today:\r\n\r\n```\r\nvariable_list SmoketestConsBackward::apply(const variable_list& grads) {\r\n  variable_list grad_inputs{};\r\n  if (should_compute_output({ 0, * })) {\r\n    std::tie(grad_inputs[0], grad_inputs[*]) = smoketest_cons_backward(grads);\r\n  }\r\n  return grad_inputs;      \r\n}\r\n``` \r\n\r\nThis doesn't even syntax check.\r\n\r\n1. We should assume that `smoketest_cons_backward` returns `std::tuple<Tensor, std::vector<Tensor>>`. Consequently, we need to flatten this tuple, feeding it into the `grad_inputs`. If we want to live fast and loose, we can just walk the tuple, writing the outputs into `grad_inputs`. A safer approach is to verify that the returned vectors have the correct sizes (otherwise tensors afterwards could be offset, leading to very strange bugs.)\r\n\r\n2. We need to determine if we should compute output. Here, once again, we need to know what the shapes of the returned vectors are so that we can run `should_compute_output` on them.\r\n\r\nWe need to save information about the sizes of TensorList inputs.  So we will need a struct like this:\r\n\r\n```\r\nstruct SmoketestConsBackward : public TraceableFunction {\r\n  using TraceableFunction::TraceableFunction;\r\n  int64_t xs_size;\r\n  ...\r\n};\r\n```\r\n\r\nA direct rewrite of the code now looks like this:\r\n\r\n```\r\nvariable_list SmoketestConsBackward::apply(const variable_list& grads) {\r\n  variable_list grad_inputs{};\r\n  if (should_compute_output({ 0 }) && should_compute_output_range(1, 1 + xs_size)) {\r\n    auto result = smoketest_cons_backward(grads);\r\n    grad_inputs[0] = std::get<0>(result);\r\n    copy_range(grad_inputs, 1, xs_size, std::get<1>(result))\r\n  }\r\n  return grad_inputs;      \r\n}\r\n``` \r\n\r\nOne irritating thing about the code generation scheme above is that we have to compute offsets for every Tensor/TensorList after the first one, leading to expressions like `xs_size + 2 + ys_size + 3 + ...`. A modest new abstraction can help avoid this situation:\r\n\r\n```\r\nvariable_list SmoketestConsBackward::apply(const variable_list& grads) {\r\n  variable_list grad_inputs{};\r\n  IndexRangeGenerator igen;\r\n  std::pair<size_t, size_t> x_ix = igen.single();\r\n  std::pair<size_t, size_t> xs_ix = igen.range(xs_size);\r\n  if (should_compute_output({x_ix, xs_ix})) {\r\n    auto result = smoketest_cons_backward(grads);\r\n    copy_range(grad_inputs, x_ix, std::get<0>(result));\r\n    copy_range(grad_inputs, xs_ix, std::get<1>(result));\r\n  }\r\n  return grad_inputs;      \r\n}\r\n``` \r\n\r\nAfter this is done, we have the first half of mixed Tensor/TensorList support, which supports mixed forward inputs / mixed backward outputs."}