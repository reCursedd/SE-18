{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7851", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7851/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7851/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7851/events", "html_url": "https://github.com/pytorch/pytorch/issues/7851", "id": 326616804, "node_id": "MDU6SXNzdWUzMjY2MTY4MDQ=", "number": 7851, "title": "JIT test failure", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-25T18:07:34Z", "updated_at": "2018-07-11T17:21:33Z", "closed_at": "2018-07-11T17:21:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>On recent builds I've been getting the following jit error. Has anything changed that would change the produced jit code?</p>\n<pre><code>Running test_jit ...\nFs........s......x..x..x.....x..............s........s.........................................................................................\n======================================================================\nFAIL: test_alexnet (test_jit.TestJit)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/opt/pytorch/pytorch/test/test_jit.py\", line 649, in test_alexnet\n    self.assertExpectedGraph(trace)\n  File \"/opt/pytorch/pytorch/test/test_jit.py\", line 111, in assertExpectedGraph\n    self.assertExpected(str(graph), *args, **kwargs)\n  File \"/opt/pytorch/pytorch/test/common.py\", line 414, in assertExpected\n    self.assertMultiLineEqual(expected, s)\nAssertionError: 'grap[3314 chars]9216), %38 : Handle = ^Dropout(0.5, True, Fals[1460 chars]n}\\n' != 'grap[3314 chars]9216) = ^Dropout(0.5, True, False)(%36), scope[1432 chars]n}\\n'\n  graph(%0 : Double(1, 3, 224, 224)\n        %1 : Double(64, 3, 11, 11)\n        %2 : Double(64)\n        %3 : Double(192, 64, 5, 5)\n        %4 : Double(192)\n        %5 : Double(384, 192, 3, 3)\n        %6 : Double(384)\n        %7 : Double(256, 384, 3, 3)\n        %8 : Double(256)\n        %9 : Double(256, 256, 3, 3)\n        %10 : Double(256)\n        %11 : Double(4096, 9216)\n        %12 : Double(4096)\n        %13 : Double(4096, 4096)\n        %14 : Double(4096)\n        %15 : Double(1000, 4096)\n        %16 : Double(1000)) {\n    %17 : Double(1, 64, 55, 55) = aten::_convolution[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n    %18 : Double(1, 64, 55, 55) = aten::threshold[threshold={0}, value={0}](%17), scope: AlexNet/Sequential[features]/ReLU[1]\n    %19 : Double(1, 64, 27, 27), %20 : Long(1, 64, 27, 27) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n    %21 : Double(1, 192, 27, 27) = aten::_convolution[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%19, %3, %4), scope: AlexNet/Sequential[features]/Conv2d[3]\n    %22 : Double(1, 192, 27, 27) = aten::threshold[threshold={0}, value={0}](%21), scope: AlexNet/Sequential[features]/ReLU[4]\n    %23 : Double(1, 192, 13, 13), %24 : Long(1, 192, 13, 13) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%22), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n    %25 : Double(1, 384, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%23, %5, %6), scope: AlexNet/Sequential[features]/Conv2d[6]\n    %26 : Double(1, 384, 13, 13) = aten::threshold[threshold={0}, value={0}](%25), scope: AlexNet/Sequential[features]/ReLU[7]\n    %27 : Double(1, 256, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%26, %7, %8), scope: AlexNet/Sequential[features]/Conv2d[8]\n    %28 : Double(1, 256, 13, 13) = aten::threshold[threshold={0}, value={0}](%27), scope: AlexNet/Sequential[features]/ReLU[9]\n    %29 : Double(1, 256, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%28, %9, %10), scope: AlexNet/Sequential[features]/Conv2d[10]\n    %30 : Double(1, 256, 13, 13) = aten::threshold[threshold={0}, value={0}](%29), scope: AlexNet/Sequential[features]/ReLU[11]\n    %31 : Double(1, 256, 6, 6), %32 : Long(1, 256, 6, 6) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%30), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n    %33 : Long() = aten::size[dim=0](%31), scope: AlexNet\n    %34 : Long() = prim::Constant[value={9216}](), scope: AlexNet\n    %35 : Dynamic = aten::stack[dim=0](%33, %34), scope: AlexNet\n    %36 : Double(1, 9216) = aten::view(%31, %35), scope: AlexNet\n-   %37 : Double(1, 9216), %38 : Handle = ^Dropout(0.5, True, False)(%36), scope: AlexNet/Sequential[classifier]/Dropout[0]\n?                        --------------\n+   %37 : Double(1, 9216) = ^Dropout(0.5, True, False)(%36), scope: AlexNet/Sequential[classifier]/Dropout[0]\n-   %39 : Double(9216!, 4096!) = aten::t(%11), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^\n+   %38 : Double(9216!, 4096!) = aten::t(%11), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^\n-   %40 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%12), scope: AlexNet/Sequential[classifier]/Linear[1]\n?    ^^\n+   %39 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%12), scope: AlexNet/Sequential[classifier]/Linear[1]\n?    ^^\n-   %41 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%40, %37, %39), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^                                                       ^^         ^\n+   %40 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%39, %37, %38), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^                                                       ^^         ^\n-   %42 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%41), scope: AlexNet/Sequential[classifier]/ReLU[2]\n?     ^                                                                 ^\n+   %41 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%40), scope: AlexNet/Sequential[classifier]/ReLU[2]\n?     ^                                                                 ^\n-   %43 : Double(1, 4096), %44 : Handle = ^Dropout(0.5, True, False)(%42), scope: AlexNet/Sequential[classifier]/Dropout[3]\n?     ^                  --------------                                ^\n+   %42 : Double(1, 4096) = ^Dropout(0.5, True, False)(%41), scope: AlexNet/Sequential[classifier]/Dropout[3]\n?     ^                                                  ^\n-   %45 : Double(4096!, 4096!) = aten::t(%13), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n+   %43 : Double(4096!, 4096!) = aten::t(%13), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n-   %46 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%14), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n+   %44 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%14), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n-   %47 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%46, %43, %45), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^                                                        ^     -----\n+   %45 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%44, %42, %43), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^                                                        ^ +++++\n-   %48 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%47), scope: AlexNet/Sequential[classifier]/ReLU[5]\n?     ^                                                                 ^\n+   %46 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%45), scope: AlexNet/Sequential[classifier]/ReLU[5]\n?     ^                                                                 ^\n-   %49 : Double(4096!, 1000!) = aten::t(%15), scope: AlexNet/Sequential[classifier]/Linear[6]\n?     ^\n+   %47 : Double(4096!, 1000!) = aten::t(%15), scope: AlexNet/Sequential[classifier]/Linear[6]\n?     ^\n-   %50 : Double(1, 1000) = aten::expand[size=[1, 1000], implicit=1](%16), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^\n+   %48 : Double(1, 1000) = aten::expand[size=[1, 1000], implicit=1](%16), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^\n-   %51 : Double(1, 1000) = aten::addmm[beta={1}, alpha={1}](%50, %48, %49), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^                                                       -----      ^\n+   %49 : Double(1, 1000) = aten::addmm[beta={1}, alpha={1}](%48, %46, %47), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^                                                             ^^^^^^\n-   return (%51);\n?            ^^\n+   return (%49);\n?            ^^\n  }\n\n\n----------------------------------------------------------------------\nRan 143 tests in 22.279s\n\nFAILED (failures=1, skipped=4, expected failures=4)\nTraceback (most recent call last):\n  File \"run_test.py\", line 342, in &lt;module&gt;\n    main()\n  File \"run_test.py\", line 334, in main\n    raise RuntimeError(message)\nRuntimeError: test_jit failed!\n</code></pre>", "body_text": "On recent builds I've been getting the following jit error. Has anything changed that would change the produced jit code?\nRunning test_jit ...\nFs........s......x..x..x.....x..............s........s.........................................................................................\n======================================================================\nFAIL: test_alexnet (test_jit.TestJit)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/opt/pytorch/pytorch/test/test_jit.py\", line 649, in test_alexnet\n    self.assertExpectedGraph(trace)\n  File \"/opt/pytorch/pytorch/test/test_jit.py\", line 111, in assertExpectedGraph\n    self.assertExpected(str(graph), *args, **kwargs)\n  File \"/opt/pytorch/pytorch/test/common.py\", line 414, in assertExpected\n    self.assertMultiLineEqual(expected, s)\nAssertionError: 'grap[3314 chars]9216), %38 : Handle = ^Dropout(0.5, True, Fals[1460 chars]n}\\n' != 'grap[3314 chars]9216) = ^Dropout(0.5, True, False)(%36), scope[1432 chars]n}\\n'\n  graph(%0 : Double(1, 3, 224, 224)\n        %1 : Double(64, 3, 11, 11)\n        %2 : Double(64)\n        %3 : Double(192, 64, 5, 5)\n        %4 : Double(192)\n        %5 : Double(384, 192, 3, 3)\n        %6 : Double(384)\n        %7 : Double(256, 384, 3, 3)\n        %8 : Double(256)\n        %9 : Double(256, 256, 3, 3)\n        %10 : Double(256)\n        %11 : Double(4096, 9216)\n        %12 : Double(4096)\n        %13 : Double(4096, 4096)\n        %14 : Double(4096)\n        %15 : Double(1000, 4096)\n        %16 : Double(1000)) {\n    %17 : Double(1, 64, 55, 55) = aten::_convolution[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n    %18 : Double(1, 64, 55, 55) = aten::threshold[threshold={0}, value={0}](%17), scope: AlexNet/Sequential[features]/ReLU[1]\n    %19 : Double(1, 64, 27, 27), %20 : Long(1, 64, 27, 27) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n    %21 : Double(1, 192, 27, 27) = aten::_convolution[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%19, %3, %4), scope: AlexNet/Sequential[features]/Conv2d[3]\n    %22 : Double(1, 192, 27, 27) = aten::threshold[threshold={0}, value={0}](%21), scope: AlexNet/Sequential[features]/ReLU[4]\n    %23 : Double(1, 192, 13, 13), %24 : Long(1, 192, 13, 13) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%22), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n    %25 : Double(1, 384, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%23, %5, %6), scope: AlexNet/Sequential[features]/Conv2d[6]\n    %26 : Double(1, 384, 13, 13) = aten::threshold[threshold={0}, value={0}](%25), scope: AlexNet/Sequential[features]/ReLU[7]\n    %27 : Double(1, 256, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%26, %7, %8), scope: AlexNet/Sequential[features]/Conv2d[8]\n    %28 : Double(1, 256, 13, 13) = aten::threshold[threshold={0}, value={0}](%27), scope: AlexNet/Sequential[features]/ReLU[9]\n    %29 : Double(1, 256, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%28, %9, %10), scope: AlexNet/Sequential[features]/Conv2d[10]\n    %30 : Double(1, 256, 13, 13) = aten::threshold[threshold={0}, value={0}](%29), scope: AlexNet/Sequential[features]/ReLU[11]\n    %31 : Double(1, 256, 6, 6), %32 : Long(1, 256, 6, 6) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%30), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n    %33 : Long() = aten::size[dim=0](%31), scope: AlexNet\n    %34 : Long() = prim::Constant[value={9216}](), scope: AlexNet\n    %35 : Dynamic = aten::stack[dim=0](%33, %34), scope: AlexNet\n    %36 : Double(1, 9216) = aten::view(%31, %35), scope: AlexNet\n-   %37 : Double(1, 9216), %38 : Handle = ^Dropout(0.5, True, False)(%36), scope: AlexNet/Sequential[classifier]/Dropout[0]\n?                        --------------\n+   %37 : Double(1, 9216) = ^Dropout(0.5, True, False)(%36), scope: AlexNet/Sequential[classifier]/Dropout[0]\n-   %39 : Double(9216!, 4096!) = aten::t(%11), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^\n+   %38 : Double(9216!, 4096!) = aten::t(%11), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^\n-   %40 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%12), scope: AlexNet/Sequential[classifier]/Linear[1]\n?    ^^\n+   %39 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%12), scope: AlexNet/Sequential[classifier]/Linear[1]\n?    ^^\n-   %41 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%40, %37, %39), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^                                                       ^^         ^\n+   %40 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%39, %37, %38), scope: AlexNet/Sequential[classifier]/Linear[1]\n?     ^                                                       ^^         ^\n-   %42 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%41), scope: AlexNet/Sequential[classifier]/ReLU[2]\n?     ^                                                                 ^\n+   %41 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%40), scope: AlexNet/Sequential[classifier]/ReLU[2]\n?     ^                                                                 ^\n-   %43 : Double(1, 4096), %44 : Handle = ^Dropout(0.5, True, False)(%42), scope: AlexNet/Sequential[classifier]/Dropout[3]\n?     ^                  --------------                                ^\n+   %42 : Double(1, 4096) = ^Dropout(0.5, True, False)(%41), scope: AlexNet/Sequential[classifier]/Dropout[3]\n?     ^                                                  ^\n-   %45 : Double(4096!, 4096!) = aten::t(%13), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n+   %43 : Double(4096!, 4096!) = aten::t(%13), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n-   %46 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%14), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n+   %44 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%14), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^\n-   %47 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%46, %43, %45), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^                                                        ^     -----\n+   %45 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%44, %42, %43), scope: AlexNet/Sequential[classifier]/Linear[4]\n?     ^                                                        ^ +++++\n-   %48 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%47), scope: AlexNet/Sequential[classifier]/ReLU[5]\n?     ^                                                                 ^\n+   %46 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%45), scope: AlexNet/Sequential[classifier]/ReLU[5]\n?     ^                                                                 ^\n-   %49 : Double(4096!, 1000!) = aten::t(%15), scope: AlexNet/Sequential[classifier]/Linear[6]\n?     ^\n+   %47 : Double(4096!, 1000!) = aten::t(%15), scope: AlexNet/Sequential[classifier]/Linear[6]\n?     ^\n-   %50 : Double(1, 1000) = aten::expand[size=[1, 1000], implicit=1](%16), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^\n+   %48 : Double(1, 1000) = aten::expand[size=[1, 1000], implicit=1](%16), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^\n-   %51 : Double(1, 1000) = aten::addmm[beta={1}, alpha={1}](%50, %48, %49), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^                                                       -----      ^\n+   %49 : Double(1, 1000) = aten::addmm[beta={1}, alpha={1}](%48, %46, %47), scope: AlexNet/Sequential[classifier]/Linear[6]\n?    ^^                                                             ^^^^^^\n-   return (%51);\n?            ^^\n+   return (%49);\n?            ^^\n  }\n\n\n----------------------------------------------------------------------\nRan 143 tests in 22.279s\n\nFAILED (failures=1, skipped=4, expected failures=4)\nTraceback (most recent call last):\n  File \"run_test.py\", line 342, in <module>\n    main()\n  File \"run_test.py\", line 334, in main\n    raise RuntimeError(message)\nRuntimeError: test_jit failed!", "body": "On recent builds I've been getting the following jit error. Has anything changed that would change the produced jit code?\r\n\r\n```\r\nRunning test_jit ...\r\nFs........s......x..x..x.....x..............s........s.........................................................................................\r\n======================================================================\r\nFAIL: test_alexnet (test_jit.TestJit)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/opt/pytorch/pytorch/test/test_jit.py\", line 649, in test_alexnet\r\n    self.assertExpectedGraph(trace)\r\n  File \"/opt/pytorch/pytorch/test/test_jit.py\", line 111, in assertExpectedGraph\r\n    self.assertExpected(str(graph), *args, **kwargs)\r\n  File \"/opt/pytorch/pytorch/test/common.py\", line 414, in assertExpected\r\n    self.assertMultiLineEqual(expected, s)\r\nAssertionError: 'grap[3314 chars]9216), %38 : Handle = ^Dropout(0.5, True, Fals[1460 chars]n}\\n' != 'grap[3314 chars]9216) = ^Dropout(0.5, True, False)(%36), scope[1432 chars]n}\\n'\r\n  graph(%0 : Double(1, 3, 224, 224)\r\n        %1 : Double(64, 3, 11, 11)\r\n        %2 : Double(64)\r\n        %3 : Double(192, 64, 5, 5)\r\n        %4 : Double(192)\r\n        %5 : Double(384, 192, 3, 3)\r\n        %6 : Double(384)\r\n        %7 : Double(256, 384, 3, 3)\r\n        %8 : Double(256)\r\n        %9 : Double(256, 256, 3, 3)\r\n        %10 : Double(256)\r\n        %11 : Double(4096, 9216)\r\n        %12 : Double(4096)\r\n        %13 : Double(4096, 4096)\r\n        %14 : Double(4096)\r\n        %15 : Double(1000, 4096)\r\n        %16 : Double(1000)) {\r\n    %17 : Double(1, 64, 55, 55) = aten::_convolution[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n    %18 : Double(1, 64, 55, 55) = aten::threshold[threshold={0}, value={0}](%17), scope: AlexNet/Sequential[features]/ReLU[1]\r\n    %19 : Double(1, 64, 27, 27), %20 : Long(1, 64, 27, 27) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\r\n    %21 : Double(1, 192, 27, 27) = aten::_convolution[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%19, %3, %4), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n    %22 : Double(1, 192, 27, 27) = aten::threshold[threshold={0}, value={0}](%21), scope: AlexNet/Sequential[features]/ReLU[4]\r\n    %23 : Double(1, 192, 13, 13), %24 : Long(1, 192, 13, 13) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%22), scope: AlexNet/Sequential[features]/MaxPool2d[5]\r\n    %25 : Double(1, 384, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%23, %5, %6), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n    %26 : Double(1, 384, 13, 13) = aten::threshold[threshold={0}, value={0}](%25), scope: AlexNet/Sequential[features]/ReLU[7]\r\n    %27 : Double(1, 256, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%26, %7, %8), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n    %28 : Double(1, 256, 13, 13) = aten::threshold[threshold={0}, value={0}](%27), scope: AlexNet/Sequential[features]/ReLU[9]\r\n    %29 : Double(1, 256, 13, 13) = aten::_convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%28, %9, %10), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n    %30 : Double(1, 256, 13, 13) = aten::threshold[threshold={0}, value={0}](%29), scope: AlexNet/Sequential[features]/ReLU[11]\r\n    %31 : Double(1, 256, 6, 6), %32 : Long(1, 256, 6, 6) = aten::max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%30), scope: AlexNet/Sequential[features]/MaxPool2d[12]\r\n    %33 : Long() = aten::size[dim=0](%31), scope: AlexNet\r\n    %34 : Long() = prim::Constant[value={9216}](), scope: AlexNet\r\n    %35 : Dynamic = aten::stack[dim=0](%33, %34), scope: AlexNet\r\n    %36 : Double(1, 9216) = aten::view(%31, %35), scope: AlexNet\r\n-   %37 : Double(1, 9216), %38 : Handle = ^Dropout(0.5, True, False)(%36), scope: AlexNet/Sequential[classifier]/Dropout[0]\r\n?                        --------------\r\n+   %37 : Double(1, 9216) = ^Dropout(0.5, True, False)(%36), scope: AlexNet/Sequential[classifier]/Dropout[0]\r\n-   %39 : Double(9216!, 4096!) = aten::t(%11), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n?     ^\r\n+   %38 : Double(9216!, 4096!) = aten::t(%11), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n?     ^\r\n-   %40 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%12), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n?    ^^\r\n+   %39 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%12), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n?    ^^\r\n-   %41 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%40, %37, %39), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n?     ^                                                       ^^         ^\r\n+   %40 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%39, %37, %38), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n?     ^                                                       ^^         ^\r\n-   %42 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%41), scope: AlexNet/Sequential[classifier]/ReLU[2]\r\n?     ^                                                                 ^\r\n+   %41 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%40), scope: AlexNet/Sequential[classifier]/ReLU[2]\r\n?     ^                                                                 ^\r\n-   %43 : Double(1, 4096), %44 : Handle = ^Dropout(0.5, True, False)(%42), scope: AlexNet/Sequential[classifier]/Dropout[3]\r\n?     ^                  --------------                                ^\r\n+   %42 : Double(1, 4096) = ^Dropout(0.5, True, False)(%41), scope: AlexNet/Sequential[classifier]/Dropout[3]\r\n?     ^                                                  ^\r\n-   %45 : Double(4096!, 4096!) = aten::t(%13), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n?     ^\r\n+   %43 : Double(4096!, 4096!) = aten::t(%13), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n?     ^\r\n-   %46 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%14), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n?     ^\r\n+   %44 : Double(1, 4096) = aten::expand[size=[1, 4096], implicit=1](%14), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n?     ^\r\n-   %47 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%46, %43, %45), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n?     ^                                                        ^     -----\r\n+   %45 : Double(1, 4096) = aten::addmm[beta={1}, alpha={1}](%44, %42, %43), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n?     ^                                                        ^ +++++\r\n-   %48 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%47), scope: AlexNet/Sequential[classifier]/ReLU[5]\r\n?     ^                                                                 ^\r\n+   %46 : Double(1, 4096) = aten::threshold[threshold={0}, value={0}](%45), scope: AlexNet/Sequential[classifier]/ReLU[5]\r\n?     ^                                                                 ^\r\n-   %49 : Double(4096!, 1000!) = aten::t(%15), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n?     ^\r\n+   %47 : Double(4096!, 1000!) = aten::t(%15), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n?     ^\r\n-   %50 : Double(1, 1000) = aten::expand[size=[1, 1000], implicit=1](%16), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n?    ^^\r\n+   %48 : Double(1, 1000) = aten::expand[size=[1, 1000], implicit=1](%16), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n?    ^^\r\n-   %51 : Double(1, 1000) = aten::addmm[beta={1}, alpha={1}](%50, %48, %49), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n?    ^^                                                       -----      ^\r\n+   %49 : Double(1, 1000) = aten::addmm[beta={1}, alpha={1}](%48, %46, %47), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n?    ^^                                                             ^^^^^^\r\n-   return (%51);\r\n?            ^^\r\n+   return (%49);\r\n?            ^^\r\n  }\r\n\r\n\r\n----------------------------------------------------------------------\r\nRan 143 tests in 22.279s\r\n\r\nFAILED (failures=1, skipped=4, expected failures=4)\r\nTraceback (most recent call last):\r\n  File \"run_test.py\", line 342, in <module>\r\n    main()\r\n  File \"run_test.py\", line 334, in main\r\n    raise RuntimeError(message)\r\nRuntimeError: test_jit failed!\r\n```"}