{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409645223", "html_url": "https://github.com/pytorch/pytorch/issues/10096#issuecomment-409645223", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10096", "id": 409645223, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTY0NTIyMw==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-01T16:55:30Z", "updated_at": "2018-08-01T16:55:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There used to be explicit expands before relevant binary operations. Something changed somewhere (my guess is the TensorIterator got rid of the necessity for the explicit expand?) and now there are no explicit expands:</p>\n<pre><code>In [1]: import torch\n\nIn [2]: @torch.jit.trace(torch.rand(4,4),  torch.rand(4), torch.rand(4))\n   ...: def scaleshift(x, scale, shift):\n   ...:     return x * scale + shift\n   ...:\n   ...:\n\nIn [3]: scaleshift.graph\nOut[3]:\ngraph(%0 : Float(4, 4)\n      %1 : Float(4)\n      %2 : Float(4)) {\n  %3 : Float(4, 4) = aten::mul(%0, %1)\n  %4 : int = prim::Constant[value=1]()\n  %5 : Float(4, 4) = aten::add(%3, %2, %4)\n  return (%5);\n}\n\nIn [4]: x = torch.randn(4, 4)\n\nIn [5]: y = torch.randn(4)\n\nIn [6]: z = torch.randn(4)\n\nIn [7]: scaleshift(x, y, z)\nOut[7]:\ntensor([[-5.4599, -0.7210,  0.1930,  1.1432],\n        [-0.5724,  3.6090,  0.0220,  1.2254],\n        [ 1.6685,  2.4105,  0.5106,  1.0230],\n        [-2.2619,  3.7919,  0.1854,  1.2218]])\n\nIn [8]: scaleshift.graph_for(x, y, z)\nOut[8]:\ngraph(%0 : Float(4, 4)\n      %1 : Float(4)\n      %2 : Float(4)) {\n  %3 : Float(4, 4) = aten::mul(%0, %1)\n  %4 : int = prim::Constant[value=1]()\n  %5 : Float(4, 4) = aten::add(%3, %2, %4)\n  return (%5);\n}\n</code></pre>\n<p>Without explicit expands, the inputs don't have the same sizes anymore so no fusion occurs.</p>", "body_text": "There used to be explicit expands before relevant binary operations. Something changed somewhere (my guess is the TensorIterator got rid of the necessity for the explicit expand?) and now there are no explicit expands:\nIn [1]: import torch\n\nIn [2]: @torch.jit.trace(torch.rand(4,4),  torch.rand(4), torch.rand(4))\n   ...: def scaleshift(x, scale, shift):\n   ...:     return x * scale + shift\n   ...:\n   ...:\n\nIn [3]: scaleshift.graph\nOut[3]:\ngraph(%0 : Float(4, 4)\n      %1 : Float(4)\n      %2 : Float(4)) {\n  %3 : Float(4, 4) = aten::mul(%0, %1)\n  %4 : int = prim::Constant[value=1]()\n  %5 : Float(4, 4) = aten::add(%3, %2, %4)\n  return (%5);\n}\n\nIn [4]: x = torch.randn(4, 4)\n\nIn [5]: y = torch.randn(4)\n\nIn [6]: z = torch.randn(4)\n\nIn [7]: scaleshift(x, y, z)\nOut[7]:\ntensor([[-5.4599, -0.7210,  0.1930,  1.1432],\n        [-0.5724,  3.6090,  0.0220,  1.2254],\n        [ 1.6685,  2.4105,  0.5106,  1.0230],\n        [-2.2619,  3.7919,  0.1854,  1.2218]])\n\nIn [8]: scaleshift.graph_for(x, y, z)\nOut[8]:\ngraph(%0 : Float(4, 4)\n      %1 : Float(4)\n      %2 : Float(4)) {\n  %3 : Float(4, 4) = aten::mul(%0, %1)\n  %4 : int = prim::Constant[value=1]()\n  %5 : Float(4, 4) = aten::add(%3, %2, %4)\n  return (%5);\n}\n\nWithout explicit expands, the inputs don't have the same sizes anymore so no fusion occurs.", "body": "There used to be explicit expands before relevant binary operations. Something changed somewhere (my guess is the TensorIterator got rid of the necessity for the explicit expand?) and now there are no explicit expands:\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: @torch.jit.trace(torch.rand(4,4),  torch.rand(4), torch.rand(4))\r\n   ...: def scaleshift(x, scale, shift):\r\n   ...:     return x * scale + shift\r\n   ...:\r\n   ...:\r\n\r\nIn [3]: scaleshift.graph\r\nOut[3]:\r\ngraph(%0 : Float(4, 4)\r\n      %1 : Float(4)\r\n      %2 : Float(4)) {\r\n  %3 : Float(4, 4) = aten::mul(%0, %1)\r\n  %4 : int = prim::Constant[value=1]()\r\n  %5 : Float(4, 4) = aten::add(%3, %2, %4)\r\n  return (%5);\r\n}\r\n\r\nIn [4]: x = torch.randn(4, 4)\r\n\r\nIn [5]: y = torch.randn(4)\r\n\r\nIn [6]: z = torch.randn(4)\r\n\r\nIn [7]: scaleshift(x, y, z)\r\nOut[7]:\r\ntensor([[-5.4599, -0.7210,  0.1930,  1.1432],\r\n        [-0.5724,  3.6090,  0.0220,  1.2254],\r\n        [ 1.6685,  2.4105,  0.5106,  1.0230],\r\n        [-2.2619,  3.7919,  0.1854,  1.2218]])\r\n\r\nIn [8]: scaleshift.graph_for(x, y, z)\r\nOut[8]:\r\ngraph(%0 : Float(4, 4)\r\n      %1 : Float(4)\r\n      %2 : Float(4)) {\r\n  %3 : Float(4, 4) = aten::mul(%0, %1)\r\n  %4 : int = prim::Constant[value=1]()\r\n  %5 : Float(4, 4) = aten::add(%3, %2, %4)\r\n  return (%5);\r\n}\r\n```\r\n\r\nWithout explicit expands, the inputs don't have the same sizes anymore so no fusion occurs."}