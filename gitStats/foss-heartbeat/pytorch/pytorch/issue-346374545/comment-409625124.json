{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409625124", "html_url": "https://github.com/pytorch/pytorch/issues/10096#issuecomment-409625124", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10096", "id": 409625124, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTYyNTEyNA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-01T15:58:08Z", "updated_at": "2018-08-01T15:58:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>That's strange, as of a few days back the following function</p>\n<pre><code>@torch.jit.trace(torch.rand(4,4),  torch.rand(4), torch.rand(4))\ndef scaleshift(x, scale, shift):\n    return x * scale + shift\n</code></pre>\n<p>where <code>scale</code> and <code>shift</code> are broadcastable to x was properly fused into a single kernel for me. Important difference is that in my case <code>scale</code> and <code>shift</code> did not require gradient, which is not the case for adding bias in LSTM. For a function to be put in a fusion block it has to be symbolically differentiable, IIRC, and expand is not.</p>", "body_text": "That's strange, as of a few days back the following function\n@torch.jit.trace(torch.rand(4,4),  torch.rand(4), torch.rand(4))\ndef scaleshift(x, scale, shift):\n    return x * scale + shift\n\nwhere scale and shift are broadcastable to x was properly fused into a single kernel for me. Important difference is that in my case scale and shift did not require gradient, which is not the case for adding bias in LSTM. For a function to be put in a fusion block it has to be symbolically differentiable, IIRC, and expand is not.", "body": "That's strange, as of a few days back the following function\r\n```\r\n@torch.jit.trace(torch.rand(4,4),  torch.rand(4), torch.rand(4))\r\ndef scaleshift(x, scale, shift):\r\n    return x * scale + shift\r\n```\r\nwhere `scale` and `shift` are broadcastable to x was properly fused into a single kernel for me. Important difference is that in my case `scale` and `shift` did not require gradient, which is not the case for adding bias in LSTM. For a function to be put in a fusion block it has to be symbolically differentiable, IIRC, and expand is not. "}