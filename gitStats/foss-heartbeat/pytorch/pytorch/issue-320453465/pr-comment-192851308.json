{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/192851308", "pull_request_review_id": 125707050, "id": 192851308, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5Mjg1MTMwOA==", "diff_hunk": "@@ -558,31 +559,25 @@ static PyObject * THPVariable_storage_type(PyObject* self, PyObject* arg)\n static PyObject * THPVariable_to(PyObject* self, PyObject* args, PyObject* kwargs)\n {\n   HANDLE_TH_ERRORS\n-  static PythonArgParser parser({\n-    \"to(Device device, ScalarType dtype=None)\",\n-    \"to(ScalarType dtype)\",\n-    \"to(Tensor other)\",\n-  });\n-  auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;\n-  ParsedArgs<2> parsed_args;\n-  auto r = parser.parse(args, kwargs, parsed_args);\n-  if (r.idx == 0) {\n-    auto device = r.device(0);\n-    auto deviceAutoGPU = device.deviceInt64();\n-    auto scalarType = r.scalartypeWithDefault(1, self_.type().scalarType());\n-    auto& layout = *torch::getLayout(self_.type().backend());\n-    auto& type = torch::getType(scalarType, layout, device.type);\n-    return THPVariable_Wrap(torch::utils::dispatch_type_conversion(self_, type, deviceAutoGPU, false));\n-  } else if (r.idx == 1) {\n-    auto scalarType = r.scalartype(0);\n-    auto& type = self_.type().toScalarType(scalarType);\n+  auto parsed = parse_to_conversion(args, kwargs);\n+  auto& device = std::get<0>(parsed);\n+  auto& scalarType = std::get<1>(parsed);\n+  auto non_blocking = std::get<2>(parsed);\n+  if (!device && scalarType) {\n+    // only dtype given\n+    auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;\n+    auto& type = self_.type().toScalarType(*scalarType);\n     return THPVariable_Wrap(torch::utils::dispatch_type_conversion(self_, type));\n-  } else if (r.idx == 2) {\n-    auto other = r.tensor(0);\n-    auto& type = other.type();\n-    auto deviceType = torch::getDeviceType(type);\n-    auto deviceAutoGPU = (deviceType == DeviceType::CPU) ? -1 : other.get_device();\n-    return THPVariable_Wrap(torch::utils::dispatch_type_conversion(self_, type, deviceAutoGPU, false));\n+  } else if (device) {\n+    // device and maybe dtype given\n+    auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;\n+    auto deviceAutoGPU = device->deviceInt64();\n+    auto& layout = *torch::getLayout(self_.type().backend());\n+    auto& type = torch::getType(scalarType.value_or(self_.type().scalarType()), layout, device->type);\n+    return THPVariable_Wrap(torch::utils::dispatch_type_conversion(self_, type, deviceAutoGPU, non_blocking));\n+  } else {\n+    Py_INCREF(self);", "path": "tools/autograd/templates/python_variable_methods.cpp", "position": null, "original_position": 53, "commit_id": "036f618a8df265c928ff96d43295df2f9de7141a", "original_commit_id": "1d729cf4e77bc425e71c052514c53438c24b500e", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "nit: can you fold this case into the !device case (so there is no !device && scalarType case)?  type should be the same as in the below case, scalarType.value_or(self_.type().scalarType()), right?", "created_at": "2018-06-04T19:16:57Z", "updated_at": "2018-11-23T15:44:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/7312#discussion_r192851308", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7312", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/192851308"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7312#discussion_r192851308"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7312"}}, "body_html": "<p>nit: can you fold this case into the !device case (so there is no !device &amp;&amp; scalarType case)?  type should be the same as in the below case, scalarType.value_or(self_.type().scalarType()), right?</p>", "body_text": "nit: can you fold this case into the !device case (so there is no !device && scalarType case)?  type should be the same as in the below case, scalarType.value_or(self_.type().scalarType()), right?"}