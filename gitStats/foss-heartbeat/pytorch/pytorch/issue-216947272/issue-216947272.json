{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1099", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1099/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1099/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1099/events", "html_url": "https://github.com/pytorch/pytorch/issues/1099", "id": 216947272, "node_id": "MDU6SXNzdWUyMTY5NDcyNzI=", "number": 1099, "title": "Add a note in the docs about the momentum formulation used in optim", "user": {"login": "keskarnitish", "id": 5945552, "node_id": "MDQ6VXNlcjU5NDU1NTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/5945552?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keskarnitish", "html_url": "https://github.com/keskarnitish", "followers_url": "https://api.github.com/users/keskarnitish/followers", "following_url": "https://api.github.com/users/keskarnitish/following{/other_user}", "gists_url": "https://api.github.com/users/keskarnitish/gists{/gist_id}", "starred_url": "https://api.github.com/users/keskarnitish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keskarnitish/subscriptions", "organizations_url": "https://api.github.com/users/keskarnitish/orgs", "repos_url": "https://api.github.com/users/keskarnitish/repos", "events_url": "https://api.github.com/users/keskarnitish/events{/privacy}", "received_events_url": "https://api.github.com/users/keskarnitish/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-25T03:55:51Z", "updated_at": "2017-04-06T00:45:41Z", "closed_at": "2017-04-06T00:45:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have been looking at the implementation of SGD + Momentum in PyTorch and noticed something a bit different from how other packages (and papers) describe it. For the moment, let's focus solely on (classical) momentum and not Nesterov's version.</p>\n<p>At the time of writing, the implementation reads:</p>\n<pre><code>              if momentum != 0:\n                   param_state = self.state[p]\n                   if 'momentum_buffer' not in param_state:\n                       buf = param_state['momentum_buffer'] = d_p.clone()\n                   else:\n                       buf = param_state['momentum_buffer']\n                       buf.mul_(momentum).add_(1 - dampening, d_p)\n                   if nesterov:\n                       d_p = d_p.add(momentum, buf)\n                   else:\n                       d_p = buf\n\n               p.data.add_(-group['lr'], d_p)\n</code></pre>\n<p>Mathematically, if we denote the momentum buffer by <code>v</code> and assume that <code>dampening=0</code>, at every iteration, the buffer is updated as <code>v = m*v + g</code> and the step is <code>\u2206x = lr * v</code>. Notice that the learning rate <code>lr</code> hits the momentum term <code>v</code> as well as the gradient. To me, this is different from what classical momentum is, and also differs from how other packages implement SGD+M.</p>\n<p>Let us contrast this with the Sutskever et. al. paper and other commonly used pacakges such as Lasagne, Keras, Neon, etc.</p>\n<h2><a href=\"http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf\" rel=\"nofollow\">Sutskever et. al.</a></h2>\n<p>The snippet of the relevant section is pasted below.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/6970e4b9cf615a90d561e4b3454a4188ade70237/687474703a2f2f692e696d6775722e636f6d2f514a656c6f64452e706e67\"><img src=\"https://camo.githubusercontent.com/6970e4b9cf615a90d561e4b3454a4188ade70237/687474703a2f2f692e696d6775722e636f6d2f514a656c6f64452e706e67\" alt=\"Sutskever et. al.\" data-canonical-src=\"http://i.imgur.com/QJelodE.png\" style=\"max-width:100%;\"></a></p>\n<p>Retaining the syntax from above, the algorithm updates <code>v</code> as <code>v = m*v - lr * g</code> with the step <code>\u2206x = v</code>. So, the learning rate <code>lr</code> only hits the gradient. It does not (explicitly) influence the effect of the momentum term which is in contrast with PyTorch's implementation.</p>\n<h1><a href=\"https://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py#L217\">Lasagne</a></h1>\n<p>Lasagne employs the same rule as suggested in Sutskever for momentum.</p>\n<pre><code>    for param in params:\n        value = param.get_value(borrow=True)\n        velocity = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n                                 broadcastable=param.broadcastable)\n        x = momentum * velocity + updates[param]\n        updates[velocity] = x - param\n</code></pre>\n<h1><a href=\"https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L141\">Keras</a></h1>\n<p>Same for Keras:</p>\n<pre><code>       for p, g, m in zip(params, grads, moments):\n            v = self.momentum * m - lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n</code></pre>\n<h1><a href=\"https://github.com/NervanaSystems/neon/blob/master/neon/optimizers/optimizer.py#L520\">Neon</a></h1>\n<p>and Neon.</p>\n<pre><code>                velocity[:] = self.momentum_coef * velocity - lrate * grad\n\n                # Nesterov accelerated gradient (NAG) is implemented the same\n                # as in torch's \"sgd.lua\". It's a reformulation of Sutskever's\n                # NAG equation found in \"On the importance of initialization\n                # and momentum in deep learning\".\n                if self.nesterov:\n                    param[:] = param + self.momentum_coef * velocity -\\\n                               lrate * grad\n                else:\n                    param[:] = param + velocity\n</code></pre>\n<p>Is the disparity true or am I missing something important?</p>\n<p>The difference between the two implementations is not insignificant and especially so when <code>lr</code> is reduced along the way. If my claim is true, maybe we could update the reference (I'm not sure what that would be) or include the above version in the SGD code (I can take this up if necessary)?</p>", "body_text": "I have been looking at the implementation of SGD + Momentum in PyTorch and noticed something a bit different from how other packages (and papers) describe it. For the moment, let's focus solely on (classical) momentum and not Nesterov's version.\nAt the time of writing, the implementation reads:\n              if momentum != 0:\n                   param_state = self.state[p]\n                   if 'momentum_buffer' not in param_state:\n                       buf = param_state['momentum_buffer'] = d_p.clone()\n                   else:\n                       buf = param_state['momentum_buffer']\n                       buf.mul_(momentum).add_(1 - dampening, d_p)\n                   if nesterov:\n                       d_p = d_p.add(momentum, buf)\n                   else:\n                       d_p = buf\n\n               p.data.add_(-group['lr'], d_p)\n\nMathematically, if we denote the momentum buffer by v and assume that dampening=0, at every iteration, the buffer is updated as v = m*v + g and the step is \u2206x = lr * v. Notice that the learning rate lr hits the momentum term v as well as the gradient. To me, this is different from what classical momentum is, and also differs from how other packages implement SGD+M.\nLet us contrast this with the Sutskever et. al. paper and other commonly used pacakges such as Lasagne, Keras, Neon, etc.\nSutskever et. al.\nThe snippet of the relevant section is pasted below.\n\nRetaining the syntax from above, the algorithm updates v as v = m*v - lr * g with the step \u2206x = v. So, the learning rate lr only hits the gradient. It does not (explicitly) influence the effect of the momentum term which is in contrast with PyTorch's implementation.\nLasagne\nLasagne employs the same rule as suggested in Sutskever for momentum.\n    for param in params:\n        value = param.get_value(borrow=True)\n        velocity = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n                                 broadcastable=param.broadcastable)\n        x = momentum * velocity + updates[param]\n        updates[velocity] = x - param\n\nKeras\nSame for Keras:\n       for p, g, m in zip(params, grads, moments):\n            v = self.momentum * m - lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n\nNeon\nand Neon.\n                velocity[:] = self.momentum_coef * velocity - lrate * grad\n\n                # Nesterov accelerated gradient (NAG) is implemented the same\n                # as in torch's \"sgd.lua\". It's a reformulation of Sutskever's\n                # NAG equation found in \"On the importance of initialization\n                # and momentum in deep learning\".\n                if self.nesterov:\n                    param[:] = param + self.momentum_coef * velocity -\\\n                               lrate * grad\n                else:\n                    param[:] = param + velocity\n\nIs the disparity true or am I missing something important?\nThe difference between the two implementations is not insignificant and especially so when lr is reduced along the way. If my claim is true, maybe we could update the reference (I'm not sure what that would be) or include the above version in the SGD code (I can take this up if necessary)?", "body": "I have been looking at the implementation of SGD + Momentum in PyTorch and noticed something a bit different from how other packages (and papers) describe it. For the moment, let's focus solely on (classical) momentum and not Nesterov's version.  \r\n\r\nAt the time of writing, the implementation reads:\r\n\r\n ```\r\n               if momentum != 0:\r\n                    param_state = self.state[p]\r\n                    if 'momentum_buffer' not in param_state:\r\n                        buf = param_state['momentum_buffer'] = d_p.clone()\r\n                    else:\r\n                        buf = param_state['momentum_buffer']\r\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\r\n                    if nesterov:\r\n                        d_p = d_p.add(momentum, buf)\r\n                    else:\r\n                        d_p = buf\r\n\r\n                p.data.add_(-group['lr'], d_p)\r\n``` \r\n\r\nMathematically, if we denote the momentum buffer by `v` and assume that `dampening=0`, at every iteration, the buffer is updated as `v = m*v + g` and the step is `\u2206x = lr * v`. Notice that the learning rate `lr` hits the momentum term `v` as well as the gradient. To me, this is different from what classical momentum is, and also differs from how other packages implement SGD+M.\r\n\r\nLet us contrast this with the Sutskever et. al. paper and other commonly used pacakges such as Lasagne, Keras, Neon, etc.\r\n\r\n## [Sutskever et. al.](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf)\r\nThe snippet of the relevant section is pasted below. \r\n![Sutskever et. al.](http://i.imgur.com/QJelodE.png)\r\n\r\nRetaining the syntax from above, the algorithm updates `v` as `v = m*v - lr * g` with the step `\u2206x = v`. So, the learning rate `lr` only hits the gradient. It does not (explicitly) influence the effect of the momentum term which is in contrast with PyTorch's implementation. \r\n\r\n# [Lasagne](https://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py#L217)\r\n\r\nLasagne employs the same rule as suggested in Sutskever for momentum. \r\n\r\n```\r\n    for param in params:\r\n        value = param.get_value(borrow=True)\r\n        velocity = theano.shared(np.zeros(value.shape, dtype=value.dtype),\r\n                                 broadcastable=param.broadcastable)\r\n        x = momentum * velocity + updates[param]\r\n        updates[velocity] = x - param\r\n```\r\n\r\n# [Keras](https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L141)\r\n\r\nSame for Keras:\r\n```\r\n       for p, g, m in zip(params, grads, moments):\r\n            v = self.momentum * m - lr * g  # velocity\r\n            self.updates.append(K.update(m, v))\r\n\r\n            if self.nesterov:\r\n                new_p = p + self.momentum * v - lr * g\r\n            else:\r\n                new_p = p + v\r\n```\r\n\r\n# [Neon](https://github.com/NervanaSystems/neon/blob/master/neon/optimizers/optimizer.py#L520)\r\n\r\nand Neon.\r\n```\r\n                velocity[:] = self.momentum_coef * velocity - lrate * grad\r\n\r\n                # Nesterov accelerated gradient (NAG) is implemented the same\r\n                # as in torch's \"sgd.lua\". It's a reformulation of Sutskever's\r\n                # NAG equation found in \"On the importance of initialization\r\n                # and momentum in deep learning\".\r\n                if self.nesterov:\r\n                    param[:] = param + self.momentum_coef * velocity -\\\r\n                               lrate * grad\r\n                else:\r\n                    param[:] = param + velocity\r\n```\r\nIs the disparity true or am I missing something important?\r\n\r\nThe difference between the two implementations is not insignificant and especially so when `lr` is reduced along the way. If my claim is true, maybe we could update the reference (I'm not sure what that would be) or include the above version in the SGD code (I can take this up if necessary)? "}