{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/361464009", "html_url": "https://github.com/pytorch/pytorch/pull/4881#issuecomment-361464009", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4881", "id": 361464009, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQ2NDAwOQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T03:16:44Z", "updated_at": "2018-01-30T03:16:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I took another look at my Python code, and I noticed I was using a doubly-nested list comprehension to flatten weight lists. According to Stack Overflow, <code>itertools.chain</code> is twice as fast. So I tried the following patch:</p>\n<pre><code>diff --git a/torch/nn/_functions/rnn.py b/torch/nn/_functions/rnn.py\nindex 4af5fda..b046a64 100644\n--- a/torch/nn/_functions/rnn.py\n+++ b/torch/nn/_functions/rnn.py\n@@ -3,6 +3,7 @@ from torch.autograd import NestedIOFunction, Variable\n import torch.backends.cudnn as cudnn\n from .. import functional as F\n from .thnn import rnnFusedPointwise as fusedBackend\n+import itertools\n \n try:\n     import torch.backends.cudnn.rnn\n@@ -273,10 +274,8 @@ def CudnnRNN(mode, input_size, hidden_size, num_layers=1,\n         handle = cudnn.get_handle()\n         dropout_desc = cudnn.rnn.init_dropout_descriptor(handle, dropout, train, dropout_seed, dropout_state)\n \n-        weight_arr = [w for ws in weight for w in ws]\n+        weight_arr = list(itertools.chain.from_iterable(weight))\n         weight_stride0 = len(weight[0])\n-        for ws in weight:\n-            assert len(ws) == weight_stride0\n \n         output, hy, cy, reserve, new_weight_buf = torch._C._VariableFunctions._cudnn_rnn(\n             input, weight_arr, weight_stride0,\ndiff --git a/torch/nn/modules/rnn.py b/torch/nn/modules/rnn.py\nindex 2b3770d..9ab6932 100644\n--- a/torch/nn/modules/rnn.py\n+++ b/torch/nn/modules/rnn.py\n@@ -1,6 +1,7 @@\n import math\n import torch\n import warnings\n+import itertools\n \n from .module import Module\n from ..parameter import Parameter\n@@ -78,7 +79,7 @@ class RNNBase(Module):\n         with torch.cuda.device_of(any_param):\n             import torch.backends.cudnn.rnn as rnn\n \n-            weight_arr = [w for ws in self.all_weights for w in ws]\n+            weight_arr = list(itertools.chain.from_iterable(self.all_weights))\n             weight_stride0 = len(self.all_weights[0])\n \n             # NB: This is a temporary hack while we still don't have Tensor\n</code></pre>\n<p>End result: no difference.</p>\n<pre><code>-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 74.00s | valid loss  5.53 | valid ppl   252.75\n-----------------------------------------------------------------------------------------\n=========================================================================================\n| End of training | test loss  5.44 | test ppl   231.46\n=========================================================================================\n\nreal    1m25.777s\nuser    1m10.042s\nsys     0m14.932s\n</code></pre>", "body_text": "I took another look at my Python code, and I noticed I was using a doubly-nested list comprehension to flatten weight lists. According to Stack Overflow, itertools.chain is twice as fast. So I tried the following patch:\ndiff --git a/torch/nn/_functions/rnn.py b/torch/nn/_functions/rnn.py\nindex 4af5fda..b046a64 100644\n--- a/torch/nn/_functions/rnn.py\n+++ b/torch/nn/_functions/rnn.py\n@@ -3,6 +3,7 @@ from torch.autograd import NestedIOFunction, Variable\n import torch.backends.cudnn as cudnn\n from .. import functional as F\n from .thnn import rnnFusedPointwise as fusedBackend\n+import itertools\n \n try:\n     import torch.backends.cudnn.rnn\n@@ -273,10 +274,8 @@ def CudnnRNN(mode, input_size, hidden_size, num_layers=1,\n         handle = cudnn.get_handle()\n         dropout_desc = cudnn.rnn.init_dropout_descriptor(handle, dropout, train, dropout_seed, dropout_state)\n \n-        weight_arr = [w for ws in weight for w in ws]\n+        weight_arr = list(itertools.chain.from_iterable(weight))\n         weight_stride0 = len(weight[0])\n-        for ws in weight:\n-            assert len(ws) == weight_stride0\n \n         output, hy, cy, reserve, new_weight_buf = torch._C._VariableFunctions._cudnn_rnn(\n             input, weight_arr, weight_stride0,\ndiff --git a/torch/nn/modules/rnn.py b/torch/nn/modules/rnn.py\nindex 2b3770d..9ab6932 100644\n--- a/torch/nn/modules/rnn.py\n+++ b/torch/nn/modules/rnn.py\n@@ -1,6 +1,7 @@\n import math\n import torch\n import warnings\n+import itertools\n \n from .module import Module\n from ..parameter import Parameter\n@@ -78,7 +79,7 @@ class RNNBase(Module):\n         with torch.cuda.device_of(any_param):\n             import torch.backends.cudnn.rnn as rnn\n \n-            weight_arr = [w for ws in self.all_weights for w in ws]\n+            weight_arr = list(itertools.chain.from_iterable(self.all_weights))\n             weight_stride0 = len(self.all_weights[0])\n \n             # NB: This is a temporary hack while we still don't have Tensor\n\nEnd result: no difference.\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 74.00s | valid loss  5.53 | valid ppl   252.75\n-----------------------------------------------------------------------------------------\n=========================================================================================\n| End of training | test loss  5.44 | test ppl   231.46\n=========================================================================================\n\nreal    1m25.777s\nuser    1m10.042s\nsys     0m14.932s", "body": " I took another look at my Python code, and I noticed I was using a doubly-nested list comprehension to flatten weight lists. According to Stack Overflow, `itertools.chain` is twice as fast. So I tried the following patch:\r\n\r\n```\r\ndiff --git a/torch/nn/_functions/rnn.py b/torch/nn/_functions/rnn.py\r\nindex 4af5fda..b046a64 100644\r\n--- a/torch/nn/_functions/rnn.py\r\n+++ b/torch/nn/_functions/rnn.py\r\n@@ -3,6 +3,7 @@ from torch.autograd import NestedIOFunction, Variable\r\n import torch.backends.cudnn as cudnn\r\n from .. import functional as F\r\n from .thnn import rnnFusedPointwise as fusedBackend\r\n+import itertools\r\n \r\n try:\r\n     import torch.backends.cudnn.rnn\r\n@@ -273,10 +274,8 @@ def CudnnRNN(mode, input_size, hidden_size, num_layers=1,\r\n         handle = cudnn.get_handle()\r\n         dropout_desc = cudnn.rnn.init_dropout_descriptor(handle, dropout, train, dropout_seed, dropout_state)\r\n \r\n-        weight_arr = [w for ws in weight for w in ws]\r\n+        weight_arr = list(itertools.chain.from_iterable(weight))\r\n         weight_stride0 = len(weight[0])\r\n-        for ws in weight:\r\n-            assert len(ws) == weight_stride0\r\n \r\n         output, hy, cy, reserve, new_weight_buf = torch._C._VariableFunctions._cudnn_rnn(\r\n             input, weight_arr, weight_stride0,\r\ndiff --git a/torch/nn/modules/rnn.py b/torch/nn/modules/rnn.py\r\nindex 2b3770d..9ab6932 100644\r\n--- a/torch/nn/modules/rnn.py\r\n+++ b/torch/nn/modules/rnn.py\r\n@@ -1,6 +1,7 @@\r\n import math\r\n import torch\r\n import warnings\r\n+import itertools\r\n \r\n from .module import Module\r\n from ..parameter import Parameter\r\n@@ -78,7 +79,7 @@ class RNNBase(Module):\r\n         with torch.cuda.device_of(any_param):\r\n             import torch.backends.cudnn.rnn as rnn\r\n \r\n-            weight_arr = [w for ws in self.all_weights for w in ws]\r\n+            weight_arr = list(itertools.chain.from_iterable(self.all_weights))\r\n             weight_stride0 = len(self.all_weights[0])\r\n \r\n             # NB: This is a temporary hack while we still don't have Tensor\r\n```\r\n\r\nEnd result: no difference.\r\n\r\n```\r\n-----------------------------------------------------------------------------------------\r\n| end of epoch   1 | time: 74.00s | valid loss  5.53 | valid ppl   252.75\r\n-----------------------------------------------------------------------------------------\r\n=========================================================================================\r\n| End of training | test loss  5.44 | test ppl   231.46\r\n=========================================================================================\r\n\r\nreal    1m25.777s\r\nuser    1m10.042s\r\nsys     0m14.932s\r\n```"}