{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164912040", "pull_request_review_id": 92760300, "id": 164912040, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDkxMjA0MA==", "diff_hunk": "@@ -0,0 +1,896 @@\n+#include <ATen/ATen.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/Config.h>\n+#include <ATen/MatrixRef.h>\n+\n+#if !AT_CUDNN_ENABLED()\n+\n+namespace at { namespace native {\n+\n+// See Note [ATen preprocessor philosophy]\n+\n+Tensor _cudnn_rnn_flatten_weight(\n+    TensorList weight_arr, int64_t weight_stride0,\n+    int64_t input_size,\n+    int64_t fn_mode, int64_t fn_hidden_size,\n+    int64_t fn_num_layers, bool batch_first,\n+    bool fn_bidirectional\n+    ) {\n+  throw std::runtime_error(\"_cudnn_rnn_flatten_weight: ATen not compiled with cuDNN support\");\n+}\n+\n+std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> _cudnn_rnn(\n+    const Tensor& input_r,\n+    TensorList weight, int64_t weight_stride0,\n+    const Tensor& weight_buf_r, const Tensor& hx, const Tensor& cx,\n+    int64_t fn_mode, int64_t fn_hidden_size,\n+    int64_t fn_num_layers, bool batch_first, double fn_dropout,\n+    bool fn_train, bool fn_bidirectional, IntList fn_batch_sizes,\n+    const Tensor& fn_dropout_state\n+    ) {\n+  throw std::runtime_error(\"_cudnn_rnn: ATen not compiled with cuDNN support\");\n+}\n+\n+std::tuple<Tensor, Tensor, Tensor, std::vector<Tensor>> _cudnn_rnn_backward(\n+    const Tensor& input, TensorList weight, int64_t weight_stride0, const Tensor& weight_buf, const Tensor& hx, const Tensor& cx,\n+    const Tensor& output, const Tensor& grad_output_r, const Tensor& grad_hy_r,\n+    const Tensor& grad_cy_r,\n+    int64_t mode, int64_t hidden_size,\n+    int64_t num_layers, bool batch_first, double dropout,\n+    bool train, bool bidirectional, IntList batch_sizes,\n+    const Tensor& dropout_state, const Tensor& reserve,\n+    std::array<bool, 4> output_mask\n+    ) {\n+  throw std::runtime_error(\"_cudnn_rnn_backward: ATen not compiled with cuDNN support\");\n+}\n+\n+}} // namespace at::native\n+\n+#else // AT_CUDNN_ENABLED()\n+\n+#include <ATen/cudnn/cudnn-wrapper.h>\n+#include <ATen/cudnn/Descriptors.h>\n+#include <ATen/cudnn/Types.h>\n+#include <ATen/cudnn/Utils.h>\n+\n+namespace at { namespace native {\n+\n+namespace {\n+  // DropoutDescriptor\n+\n+  struct DropoutDescriptorParams {\n+    bool train;\n+    double dropout;\n+    Tensor dropout_state;\n+    DropoutDescriptorParams() {}\n+    void set(bool train_, double dropout_, Tensor dropout_state_) {\n+      train = train_;\n+      dropout = dropout_;\n+      dropout_state = dropout_state_;\n+    }\n+    DropoutDescriptor descriptor(cudnnHandle_t handle) const {\n+      // NB: dropout_seed passed dummy 0, because it isn't actually used\n+      // when dropout_state is defined.\n+      auto dropout_p = train ? dropout : 0;\n+      DropoutDescriptor dropout_desc;\n+      dropout_desc.set(handle, dropout_p, dropout_state, 0);\n+      return dropout_desc;\n+    }\n+  };\n+\n+  // RNNDescriptor\n+\n+  struct RNNDescriptorParams {\n+    int64_t hidden_size;\n+    int64_t num_layers;\n+    cudnnDirectionMode_t bidirectional;\n+    cudnnRNNMode_t mode;\n+    cudnnDataType_t datatype;\n+\n+    cudnnRNNInputMode_t input_mode = CUDNN_LINEAR_INPUT;\n+\n+    int64_t num_directions() const {\n+      return bidirectional ? 2 : 1;\n+    }\n+\n+    void set_mode(int64_t fn_mode) {\n+      switch (fn_mode) {\n+        case CUDNN_RNN_RELU:\n+          mode = CUDNN_RNN_RELU;\n+          break;\n+        case CUDNN_RNN_TANH:\n+          mode = CUDNN_RNN_TANH;\n+          break;\n+        case CUDNN_LSTM:\n+          mode = CUDNN_LSTM;\n+          break;\n+        case CUDNN_GRU:\n+          mode = CUDNN_GRU;\n+          break;\n+        default:\n+          throw std::runtime_error(\"unrecognized mode\"); // TODO", "path": "aten/src/ATen/native/cudnn/RNN.cpp", "position": null, "original_position": 111, "commit_id": "ae78d3290563d944d5e884f6bfbaba8f7000d2d2", "original_commit_id": "b5bd576921dacf1771688493014b39e5faf5f6e3", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "TODO?", "created_at": "2018-01-30T23:30:02Z", "updated_at": "2018-11-23T15:38:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/4881#discussion_r164912040", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4881", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164912040"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4881#discussion_r164912040"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4881"}}, "body_html": "<p>TODO?</p>", "body_text": "TODO?"}