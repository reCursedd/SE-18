{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164897806", "pull_request_review_id": 92040114, "id": 164897806, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDg5NzgwNg==", "diff_hunk": "@@ -251,95 +251,48 @@ def forward(input, weight, hidden):\n     return forward\n \n \n-class CudnnRNN(NestedIOFunction):\n-\n-    def __init__(self, mode, input_size, hidden_size, num_layers=1,\n-                 batch_first=False, dropout=0, train=True, bidirectional=False,\n-                 batch_sizes=None, dropout_state=None, flat_weight=None):\n-        super(CudnnRNN, self).__init__()\n-        if dropout_state is None:\n-            dropout_state = {}\n-        self.mode = cudnn.rnn.get_cudnn_mode(mode)\n-        self.input_mode = cudnn.CUDNN_LINEAR_INPUT\n-        self.input_size = input_size\n-        self.hidden_size = hidden_size\n-        self.num_layers = num_layers\n-        self.batch_first = batch_first\n-        self.dropout = dropout\n-        self.train = train\n-        self.bidirectional = 1 if bidirectional else 0\n-        self.num_directions = 2 if bidirectional else 1\n-        self.batch_sizes = batch_sizes\n-        self.dropout_seed = torch.IntTensor(1).random_()[0]\n-        self.dropout_state = dropout_state\n-        self.weight_buf = flat_weight\n-        if flat_weight is None:\n-            warnings.warn(\"RNN module weights are not part of single contiguous \"\n-                          \"chunk of memory. This means they need to be compacted \"\n-                          \"at every call, possibly greatly increasing memory usage. \"\n-                          \"To compact weights again call flatten_parameters().\", stacklevel=5)\n-\n-    def forward_extended(self, input, weight, hx):\n-        assert cudnn.is_acceptable(input)\n-        # TODO: raise a warning if weight_data_ptr is None\n-\n-        output = input.new()\n-\n-        if torch.is_tensor(hx):\n-            hy = hx.new()\n+def CudnnRNN(mode, input_size, hidden_size, num_layers=1,\n+             batch_first=False, dropout=0, train=True, bidirectional=False,\n+             batch_sizes=None, dropout_state=None, flat_weight=None):\n+    if dropout_state is None:\n+        dropout_state = {}\n+    mode = cudnn.rnn.get_cudnn_mode(mode)\n+    dropout_seed = torch.IntTensor(1).random_()[0]\n+    if flat_weight is None:\n+        warnings.warn(\"RNN module weights are not part of single contiguous \"\n+                      \"chunk of memory. This means they need to be compacted \"\n+                      \"at every call, possibly greatly increasing memory usage. \"\n+                      \"To compact weights again call flatten_parameters().\", stacklevel=5)\n+\n+    def forward(input, weight, hx):\n+        if mode == cudnn.CUDNN_LSTM:\n+            hx, cx = hx\n         else:\n-            hy = tuple(h.new() for h in hx)\n-\n-        cudnn.rnn.forward(self, input, hx, weight, output, hy)\n-\n-        self.save_for_backward(input, hx, weight, output)\n-        return output, hy\n-\n-    def backward_extended(self, grad_output, grad_hy):\n-        input, hx, weight, output = self.saved_tensors\n-        input = input.contiguous()\n-\n-        grad_input, grad_weight, grad_hx = None, None, None\n-\n-        assert cudnn.is_acceptable(input)\n-\n-        grad_input = input.new()\n-        if torch.is_tensor(hx):\n-            grad_hx = input.new()\n-        else:\n-            grad_hx = tuple(h.new() for h in hx)\n-\n-        if self.retain_variables:\n-            self._reserve_clone = self.reserve.clone()\n-\n-        cudnn.rnn.backward_grad(\n-            self,\n-            input,\n-            hx,\n-            weight,\n-            output,\n-            grad_output,\n-            grad_hy,\n-            grad_input,\n-            grad_hx)\n-\n-        if any(self.needs_input_grad[1:]):\n-            grad_weight = [tuple(w.new().resize_as_(w) for w in layer_weight) for layer_weight in weight]\n-            cudnn.rnn.backward_weight(\n-                self,\n-                input,\n-                hx,\n-                output,\n-                weight,\n-                grad_weight)\n+            cx = None\n+\n+        handle = cudnn.get_handle()\n+        dropout_desc = cudnn.rnn.init_dropout_descriptor(handle, dropout, train, dropout_seed, dropout_state)", "path": "torch/nn/_functions/rnn.py", "position": null, "original_position": 111, "commit_id": "ae78d3290563d944d5e884f6bfbaba8f7000d2d2", "original_commit_id": "b5bd576921dacf1771688493014b39e5faf5f6e3", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why do we initialize the descriptor here (and at every forward!)? I thought you had a bunch of code dedicated to handling dropout descriptors in C++", "created_at": "2018-01-30T22:22:41Z", "updated_at": "2018-11-23T15:38:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/4881#discussion_r164897806", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4881", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164897806"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4881#discussion_r164897806"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4881"}}, "body_html": "<p>Why do we initialize the descriptor here (and at every forward!)? I thought you had a bunch of code dedicated to handling dropout descriptors in C++</p>", "body_text": "Why do we initialize the descriptor here (and at every forward!)? I thought you had a bunch of code dedicated to handling dropout descriptors in C++"}