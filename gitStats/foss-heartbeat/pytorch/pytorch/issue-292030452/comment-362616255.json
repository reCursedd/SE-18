{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362616255", "html_url": "https://github.com/pytorch/pytorch/pull/4881#issuecomment-362616255", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4881", "id": 362616255, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjYxNjI1NQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-02T15:24:00Z", "updated_at": "2018-02-02T15:24:00Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">I guess I must have lost the relevant comment when porting, but this behavior is intentional; if the user provides no biases we just initialize the weights. That is what the old code said, anyway!</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On February 2, 2018 9:17:52 AM EST, Adam Paszke ***@***.***&gt; wrote:\napaszke commented on this pull request.\n\n\n\n&gt; +      if (layer == 0) {\n+        global_layer_params_count = layer_params_count;\n+      } else {\n+        AT_ASSERT(global_layer_params_count == layer_params_count, \"%d\n(global) != %d\", global_layer_params_count, layer_params_count);\n+      }\n+    } // for layer\n+    return std::make_pair(params, global_layer_params_count);\n+  }\n+\n+  void _copyParams(MatrixRef&lt;Tensor&gt; params_from, MatrixRef&lt;Tensor&gt;\nparams_to) {\n+    AT_ASSERT(params_from.size(0) == params_to.size(0), \"number of\nlayers mismatch\");\n+    for (size_t i = 0; i &lt; params_from.size(0); i++) {\n+      auto layer_params_from = params_from[i];\n+      auto layer_params_to = params_to[i];\n+      for (auto a = layer_params_from.begin(), b =\nlayer_params_to.begin();\n+           a != layer_params_from.end() &amp;&amp; b != layer_params_to.end();\n\nYes, but it should never happen. What you're doing is safe, but will\nsilently ignore the fact that the sequences have different lengths\n(assert that they are equal at the end of the loop would be nice)\n\n--\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"292030452\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4881\" href=\"https://github.com/pytorch/pytorch/pull/4881#discussion_r165655567\">#4881 (comment)</a>\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.</div>\n</div>", "body_text": "I guess I must have lost the relevant comment when porting, but this behavior is intentional; if the user provides no biases we just initialize the weights. That is what the old code said, anyway!\n\u2026\nOn February 2, 2018 9:17:52 AM EST, Adam Paszke ***@***.***> wrote:\napaszke commented on this pull request.\n\n\n\n> +      if (layer == 0) {\n+        global_layer_params_count = layer_params_count;\n+      } else {\n+        AT_ASSERT(global_layer_params_count == layer_params_count, \"%d\n(global) != %d\", global_layer_params_count, layer_params_count);\n+      }\n+    } // for layer\n+    return std::make_pair(params, global_layer_params_count);\n+  }\n+\n+  void _copyParams(MatrixRef<Tensor> params_from, MatrixRef<Tensor>\nparams_to) {\n+    AT_ASSERT(params_from.size(0) == params_to.size(0), \"number of\nlayers mismatch\");\n+    for (size_t i = 0; i < params_from.size(0); i++) {\n+      auto layer_params_from = params_from[i];\n+      auto layer_params_to = params_to[i];\n+      for (auto a = layer_params_from.begin(), b =\nlayer_params_to.begin();\n+           a != layer_params_from.end() && b != layer_params_to.end();\n\nYes, but it should never happen. What you're doing is safe, but will\nsilently ignore the fact that the sequences have different lengths\n(assert that they are equal at the end of the loop would be nice)\n\n--\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\n#4881 (comment)\n\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.", "body": "I guess I must have lost the relevant comment when porting, but this behavior is intentional; if the user provides no biases we just initialize the weights. That is what the old code said, anyway!\n\nOn February 2, 2018 9:17:52 AM EST, Adam Paszke <notifications@github.com> wrote:\n>apaszke commented on this pull request.\n>\n>\n>\n>> +      if (layer == 0) {\n>+        global_layer_params_count = layer_params_count;\n>+      } else {\n>+        AT_ASSERT(global_layer_params_count == layer_params_count, \"%d\n>(global) != %d\", global_layer_params_count, layer_params_count);\n>+      }\n>+    } // for layer\n>+    return std::make_pair(params, global_layer_params_count);\n>+  }\n>+\n>+  void _copyParams(MatrixRef<Tensor> params_from, MatrixRef<Tensor>\n>params_to) {\n>+    AT_ASSERT(params_from.size(0) == params_to.size(0), \"number of\n>layers mismatch\");\n>+    for (size_t i = 0; i < params_from.size(0); i++) {\n>+      auto layer_params_from = params_from[i];\n>+      auto layer_params_to = params_to[i];\n>+      for (auto a = layer_params_from.begin(), b =\n>layer_params_to.begin();\n>+           a != layer_params_from.end() && b != layer_params_to.end();\n>\n>Yes, but it should never happen. What you're doing is safe, but will\n>silently ignore the fact that the sequences have different lengths\n>(assert that they are equal at the end of the loop would be nice)\n>\n>-- \n>You are receiving this because you were mentioned.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/pytorch/pytorch/pull/4881#discussion_r165655567\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity."}