{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195565041", "pull_request_review_id": 128961784, "id": 195565041, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTU2NTA0MQ==", "diff_hunk": "@@ -0,0 +1,98 @@\n+#pragma once\n+\n+/**\n+ * To register your own tensor types, do in a header file:\n+ *   C10_DECLARE_TENSOR_TYPE(MY_TENSOR)\n+ * and in one (!) cpp file:\n+ *   C10_DEFINE_TENSOR_TYPE(MY_TENSOR)\n+ * Both must be in the same namespace.\n+ */\n+\n+#include \"caffe2/core/dispatch/TensorTypeId.h\"\n+#include \"caffe2/core/common.h\"\n+#include <atomic>\n+#include \"flat_hash_map/flat_hash_map.h\"", "path": "caffe2/core/dispatch/TensorTypeIdRegistration.h", "position": null, "original_position": 14, "commit_id": "e98041959adca7661a009c729d6a1699a81b19aa", "original_commit_id": "e412d966fa9b56ac481082cad9619b4528e30c2b", "user": {"login": "smessmer", "id": 2373925, "node_id": "MDQ6VXNlcjIzNzM5MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2373925?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smessmer", "html_url": "https://github.com/smessmer", "followers_url": "https://api.github.com/users/smessmer/followers", "following_url": "https://api.github.com/users/smessmer/following{/other_user}", "gists_url": "https://api.github.com/users/smessmer/gists{/gist_id}", "starred_url": "https://api.github.com/users/smessmer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smessmer/subscriptions", "organizations_url": "https://api.github.com/users/smessmer/orgs", "repos_url": "https://api.github.com/users/smessmer/repos", "events_url": "https://api.github.com/users/smessmer/events{/privacy}", "received_events_url": "https://api.github.com/users/smessmer/received_events", "type": "User", "site_admin": false}, "body": "If flat_hash_map turns out to be too large (binary size), we can consider using one of the F14 tables instead. std::vector would here probably be fine too because there aren't too many tensor type ids, but I'd still expect a small perf hit with it. We need a hash map (either flat_hash_map or F14) for the dispatch table anyhow.", "created_at": "2018-06-14T20:44:44Z", "updated_at": "2018-11-23T15:45:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/8389#discussion_r195565041", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8389", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195565041"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8389#discussion_r195565041"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8389"}}, "body_html": "<p>If flat_hash_map turns out to be too large (binary size), we can consider using one of the F14 tables instead. std::vector would here probably be fine too because there aren't too many tensor type ids, but I'd still expect a small perf hit with it. We need a hash map (either flat_hash_map or F14) for the dispatch table anyhow.</p>", "body_text": "If flat_hash_map turns out to be too large (binary size), we can consider using one of the F14 tables instead. std::vector would here probably be fine too because there aren't too many tensor type ids, but I'd still expect a small perf hit with it. We need a hash map (either flat_hash_map or F14) for the dispatch table anyhow.", "in_reply_to_id": 194966498}