{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12190", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12190/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12190/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12190/events", "html_url": "https://github.com/pytorch/pytorch/issues/12190", "id": 365078926, "node_id": "MDU6SXNzdWUzNjUwNzg5MjY=", "number": 12190, "title": "Slowdown in distributions log_prob methods", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/performance", "name": "performance", "color": "f9d0c4", "default": false}], "state": "open", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-09-29T00:31:22Z", "updated_at": "2018-11-05T18:21:08Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>One of Pyro's branches (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"366588295\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/uber/pyro/issues/1431\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/uber/pyro/pull/1431/hovercard\" href=\"https://github.com/uber/pyro/pull/1431\">uber/pyro#1431</a>) is running against PyTorch's nightly build, and we noticed that the unit test stage in CI is almost twice as slow as compared to the 0.4.0 release. Many of the slow tests turn out to be HMC tests (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"365017016\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/uber/pyro/issues/1421\" data-hovercard-type=\"issue\" data-hovercard-url=\"/uber/pyro/issues/1421/hovercard\" href=\"https://github.com/uber/pyro/issues/1421\">uber/pyro#1421</a>), and the slowdown seems to mostly be in the distribution's <code>log_prob</code> methods. Pasting the results below for the normal distribution, but I am seeing this for other distributions too.</p>\n<h2>To Reproduce</h2>\n<p><strong>version: 0.4.0</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.distributions <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.<span class=\"pl-c1\">__version__</span>\n <span class=\"pl-s\"><span class=\"pl-pds\">'</span>0.4.0<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> d <span class=\"pl-k\">=</span> dist.Normal(torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>), torch.ones(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>))\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>timeit torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>)\n<span class=\"pl-c1\">16.2</span> \u00b5s \u00b1 <span class=\"pl-c1\">342</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100000</span> loops each)\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>timeit [d.log_prob(torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>)) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>)]\n<span class=\"pl-c1\">45.8</span> ms \u00b1 <span class=\"pl-c1\">1.6</span> ms per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10</span> loops each)\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>lprun <span class=\"pl-k\">-</span>f d.log_prob [d.log_prob(torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>)) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>)]\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.476309</span> s\nFile: <span class=\"pl-k\">/</span>Users<span class=\"pl-k\">/</span>npradhan<span class=\"pl-k\">/</span>miniconda2<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">-</span><span class=\"pl-c1\">36</span><span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>distributions<span class=\"pl-k\">/</span>normal.py\nFunction: log_prob at line <span class=\"pl-c1\">62</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">62</span>                                               <span class=\"pl-k\">def</span> <span class=\"pl-en\">log_prob</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">value</span>):\n    <span class=\"pl-c1\">63</span>     <span class=\"pl-c1\">10000</span>       <span class=\"pl-c1\">5511.0</span>      <span class=\"pl-c1\">0.6</span>      <span class=\"pl-c1\">1.2</span>          <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._validate_args:\n    <span class=\"pl-c1\">64</span>                                                       <span class=\"pl-c1\">self</span>._validate_sample(value)\n    <span class=\"pl-c1\">65</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute the variance</span>\n    <span class=\"pl-c1\">66</span>     <span class=\"pl-c1\">10000</span>      <span class=\"pl-c1\">23427.0</span>      <span class=\"pl-c1\">2.3</span>      <span class=\"pl-c1\">4.9</span>          var <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">self</span>.scale <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>)\n    <span class=\"pl-c1\">67</span>     <span class=\"pl-c1\">10000</span>     <span class=\"pl-c1\">180867.0</span>     <span class=\"pl-c1\">18.1</span>     <span class=\"pl-c1\">38.0</span>          log_scale <span class=\"pl-k\">=</span> math.log(<span class=\"pl-c1\">self</span>.scale) <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(<span class=\"pl-c1\">self</span>.scale, Number) <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>.scale.log()\n    <span class=\"pl-c1\">68</span>     <span class=\"pl-c1\">10000</span>     <span class=\"pl-c1\">266504.0</span>     <span class=\"pl-c1\">26.7</span>     <span class=\"pl-c1\">56.0</span>          <span class=\"pl-k\">return</span> <span class=\"pl-k\">-</span>((value <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>.loc) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> var) <span class=\"pl-k\">-</span> log_scale <span class=\"pl-k\">-</span> math.log(math.sqrt(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> math.pi))</pre></div>\n<p><strong>master (1.0.0a0+ab9a597)</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.distributions <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.<span class=\"pl-c1\">__version__</span>\n <span class=\"pl-s\"><span class=\"pl-pds\">'</span>1.0.0a0+ab9a597<span class=\"pl-pds\">'</span></span>\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> d <span class=\"pl-k\">=</span> dist.Normal(torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>), torch.ones(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>))\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>timeit torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>)\n<span class=\"pl-c1\">17.8</span> \u00b5s \u00b1 <span class=\"pl-c1\">132</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100000</span> loops each)\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>timeit [d.log_prob(torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>)) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>)]\n<span class=\"pl-c1\">72.1</span> ms \u00b1 <span class=\"pl-c1\">1.78</span> ms per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10</span> loops each)\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>lprun <span class=\"pl-k\">-</span>f d.log_prob [d.log_prob(torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">2</span>)) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>)]\nTimer unit: <span class=\"pl-c1\">1e-06</span> s\n\nTotal time: <span class=\"pl-c1\">0.782675</span> s\nFile: <span class=\"pl-k\">/</span>Users<span class=\"pl-k\">/</span>npradhan<span class=\"pl-k\">/</span>miniconda2<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">-</span>master<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>distributions<span class=\"pl-k\">/</span>normal.py\nFunction: log_prob at line <span class=\"pl-c1\">70</span>\n\nLine <span class=\"pl-c\"><span class=\"pl-c\">#</span>      Hits         Time  Per Hit   % Time  Line Contents</span>\n<span class=\"pl-k\">==============================================================</span>\n    <span class=\"pl-c1\">70</span>                                               <span class=\"pl-k\">def</span> <span class=\"pl-en\">log_prob</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">value</span>):\n    <span class=\"pl-c1\">71</span>     <span class=\"pl-c1\">10000</span>       <span class=\"pl-c1\">8708.0</span>      <span class=\"pl-c1\">0.9</span>      <span class=\"pl-c1\">1.1</span>          <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._validate_args:\n    <span class=\"pl-c1\">72</span>                                                       <span class=\"pl-c1\">self</span>._validate_sample(value)\n    <span class=\"pl-c1\">73</span>                                                   <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute the variance</span>\n    <span class=\"pl-c1\">74</span>     <span class=\"pl-c1\">10000</span>      <span class=\"pl-c1\">46140.0</span>      <span class=\"pl-c1\">4.6</span>      <span class=\"pl-c1\">5.9</span>          var <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">self</span>.scale <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>)\n    <span class=\"pl-c1\">75</span>     <span class=\"pl-c1\">10000</span>     <span class=\"pl-c1\">135462.0</span>     <span class=\"pl-c1\">13.5</span>     <span class=\"pl-c1\">17.3</span>          log_scale <span class=\"pl-k\">=</span> math.log(<span class=\"pl-c1\">self</span>.scale) <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(<span class=\"pl-c1\">self</span>.scale, Number) <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>.scale.log()\n    <span class=\"pl-c1\">76</span>     <span class=\"pl-c1\">10000</span>     <span class=\"pl-c1\">592365.0</span>     <span class=\"pl-c1\">59.2</span>     <span class=\"pl-c1\">75.7</span>          <span class=\"pl-k\">return</span> <span class=\"pl-k\">-</span>((value <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>.loc) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> var) <span class=\"pl-k\">-</span> log_scale <span class=\"pl-k\">-</span> math.log(math.sqrt(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> math.pi))</pre></div>\n<p>Note that the <code>log_prob</code> method hasn't changed, but the last line takes almost twice as long. The only thing I can think of is that the <code>broadcast_all</code> method in the constructor is now different, and the expanded instances are somehow slower. I am still investigating this.</p>\n<h2>Environment</h2>\n<pre><code>  $ python collect_env.py\nCollecting environment information...\nPyTorch version: 1.0.0a0+ab9a597\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.0)\n[pip] torch (1.0.0a0+ab9a597, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] torch                     1.0.0a0+ab9a597           &lt;pip&gt;\n[conda] torch                     0.5.0a0+2431eac           &lt;pip&gt;\n[conda] torch                     1.0.0a0+6ff568d           &lt;pip&gt;\n[conda] torch                     0.5.0a0+6660a12           &lt;pip&gt;\n[conda] torch                     0.5.0a0+35d52db           &lt;pip&gt;\n[conda] torch                     0.5.0a0+6c3792b           &lt;pip&gt;\n[conda] torchfile                 0.1.0                     &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n</code></pre>\n<h2>Additional context</h2>\n<p>I also noticed that certain functions like <code>torch.randn</code> in the nightly build are almost 2X slower than compiling the source code on my system locally. That's the reason why I am benchmarking against the local build and not the pytorch nightly build.</p>", "body_text": "\ud83d\udc1b Bug\nOne of Pyro's branches (uber/pyro#1431) is running against PyTorch's nightly build, and we noticed that the unit test stage in CI is almost twice as slow as compared to the 0.4.0 release. Many of the slow tests turn out to be HMC tests (uber/pyro#1421), and the slowdown seems to mostly be in the distribution's log_prob methods. Pasting the results below for the normal distribution, but I am seeing this for other distributions too.\nTo Reproduce\nversion: 0.4.0\n>>> import torch\n>>> import torch.distributions as dist\n>>> torch.__version__\n '0.4.0'\n>>> d = dist.Normal(torch.zeros(1000, 2), torch.ones(1000, 2))\n\n>>> %timeit torch.randn(1000, 2)\n16.2 \u00b5s \u00b1 342 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n>>> %timeit [d.log_prob(torch.randn(1000, 2)) for _ in range(1000)]\n45.8 ms \u00b1 1.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n>>> %lprun -f d.log_prob [d.log_prob(torch.randn(1000, 2)) for _ in range(10000)]\nTimer unit: 1e-06 s\n\nTotal time: 0.476309 s\nFile: /Users/npradhan/miniconda2/envs/pytorch-36/lib/python3.6/site-packages/torch/distributions/normal.py\nFunction: log_prob at line 62\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    62                                               def log_prob(self, value):\n    63     10000       5511.0      0.6      1.2          if self._validate_args:\n    64                                                       self._validate_sample(value)\n    65                                                   # compute the variance\n    66     10000      23427.0      2.3      4.9          var = (self.scale ** 2)\n    67     10000     180867.0     18.1     38.0          log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()\n    68     10000     266504.0     26.7     56.0          return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\nmaster (1.0.0a0+ab9a597)\n>>> import torch\n>>> import torch.distributions as dist\n>>> torch.__version__\n '1.0.0a0+ab9a597'\n\n>>> d = dist.Normal(torch.zeros(1000, 2), torch.ones(1000, 2))\n\n>>> %timeit torch.randn(1000, 2)\n17.8 \u00b5s \u00b1 132 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n>>> %timeit [d.log_prob(torch.randn(1000, 2)) for _ in range(1000)]\n72.1 ms \u00b1 1.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n>>> %lprun -f d.log_prob [d.log_prob(torch.randn(1000, 2)) for _ in range(10000)]\nTimer unit: 1e-06 s\n\nTotal time: 0.782675 s\nFile: /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/distributions/normal.py\nFunction: log_prob at line 70\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    70                                               def log_prob(self, value):\n    71     10000       8708.0      0.9      1.1          if self._validate_args:\n    72                                                       self._validate_sample(value)\n    73                                                   # compute the variance\n    74     10000      46140.0      4.6      5.9          var = (self.scale ** 2)\n    75     10000     135462.0     13.5     17.3          log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()\n    76     10000     592365.0     59.2     75.7          return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\nNote that the log_prob method hasn't changed, but the last line takes almost twice as long. The only thing I can think of is that the broadcast_all method in the constructor is now different, and the expanded instances are somehow slower. I am still investigating this.\nEnvironment\n  $ python collect_env.py\nCollecting environment information...\nPyTorch version: 1.0.0a0+ab9a597\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.0)\n[pip] torch (1.0.0a0+ab9a597, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] torch                     1.0.0a0+ab9a597           <pip>\n[conda] torch                     0.5.0a0+2431eac           <pip>\n[conda] torch                     1.0.0a0+6ff568d           <pip>\n[conda] torch                     0.5.0a0+6660a12           <pip>\n[conda] torch                     0.5.0a0+35d52db           <pip>\n[conda] torch                     0.5.0a0+6c3792b           <pip>\n[conda] torchfile                 0.1.0                     <pip>\n[conda] torchvision               0.2.1                     <pip>\n\nAdditional context\nI also noticed that certain functions like torch.randn in the nightly build are almost 2X slower than compiling the source code on my system locally. That's the reason why I am benchmarking against the local build and not the pytorch nightly build.", "body": "## \ud83d\udc1b Bug\r\n\r\nOne of Pyro's branches (https://github.com/uber/pyro/pull/1431) is running against PyTorch's nightly build, and we noticed that the unit test stage in CI is almost twice as slow as compared to the 0.4.0 release. Many of the slow tests turn out to be HMC tests (https://github.com/uber/pyro/issues/1421), and the slowdown seems to mostly be in the distribution's `log_prob` methods. Pasting the results below for the normal distribution, but I am seeing this for other distributions too.\r\n\r\n## To Reproduce\r\n\r\n**version: 0.4.0**\r\n```python\r\n>>> import torch\r\n>>> import torch.distributions as dist\r\n>>> torch.__version__\r\n '0.4.0'\r\n>>> d = dist.Normal(torch.zeros(1000, 2), torch.ones(1000, 2))\r\n\r\n>>> %timeit torch.randn(1000, 2)\r\n16.2 \u00b5s \u00b1 342 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n>>> %timeit [d.log_prob(torch.randn(1000, 2)) for _ in range(1000)]\r\n45.8 ms \u00b1 1.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n>>> %lprun -f d.log_prob [d.log_prob(torch.randn(1000, 2)) for _ in range(10000)]\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.476309 s\r\nFile: /Users/npradhan/miniconda2/envs/pytorch-36/lib/python3.6/site-packages/torch/distributions/normal.py\r\nFunction: log_prob at line 62\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    62                                               def log_prob(self, value):\r\n    63     10000       5511.0      0.6      1.2          if self._validate_args:\r\n    64                                                       self._validate_sample(value)\r\n    65                                                   # compute the variance\r\n    66     10000      23427.0      2.3      4.9          var = (self.scale ** 2)\r\n    67     10000     180867.0     18.1     38.0          log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()\r\n    68     10000     266504.0     26.7     56.0          return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\r\n```\r\n\r\n**master (1.0.0a0+ab9a597)**\r\n```python\r\n>>> import torch\r\n>>> import torch.distributions as dist\r\n>>> torch.__version__\r\n '1.0.0a0+ab9a597'\r\n\r\n>>> d = dist.Normal(torch.zeros(1000, 2), torch.ones(1000, 2))\r\n\r\n>>> %timeit torch.randn(1000, 2)\r\n17.8 \u00b5s \u00b1 132 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n>>> %timeit [d.log_prob(torch.randn(1000, 2)) for _ in range(1000)]\r\n72.1 ms \u00b1 1.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n>>> %lprun -f d.log_prob [d.log_prob(torch.randn(1000, 2)) for _ in range(10000)]\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.782675 s\r\nFile: /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/distributions/normal.py\r\nFunction: log_prob at line 70\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    70                                               def log_prob(self, value):\r\n    71     10000       8708.0      0.9      1.1          if self._validate_args:\r\n    72                                                       self._validate_sample(value)\r\n    73                                                   # compute the variance\r\n    74     10000      46140.0      4.6      5.9          var = (self.scale ** 2)\r\n    75     10000     135462.0     13.5     17.3          log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()\r\n    76     10000     592365.0     59.2     75.7          return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\r\n```\r\n\r\nNote that the `log_prob` method hasn't changed, but the last line takes almost twice as long. The only thing I can think of is that the `broadcast_all` method in the constructor is now different, and the expanded instances are somehow slower. I am still investigating this.\r\n\r\n## Environment\r\n\r\n```\r\n  $ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+ab9a597\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.0)\r\n[pip] torch (1.0.0a0+ab9a597, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] torch                     1.0.0a0+ab9a597           <pip>\r\n[conda] torch                     0.5.0a0+2431eac           <pip>\r\n[conda] torch                     1.0.0a0+6ff568d           <pip>\r\n[conda] torch                     0.5.0a0+6660a12           <pip>\r\n[conda] torch                     0.5.0a0+35d52db           <pip>\r\n[conda] torch                     0.5.0a0+6c3792b           <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\n## Additional context\r\n\r\nI also noticed that certain functions like `torch.randn` in the nightly build are almost 2X slower than compiling the source code on my system locally. That's the reason why I am benchmarking against the local build and not the pytorch nightly build."}