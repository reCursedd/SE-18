{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119918381", "pull_request_review_id": 41834513, "id": 119918381, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExOTkxODM4MQ==", "diff_hunk": "@@ -300,7 +302,24 @@ def __matmul__(self, other):\n             return self.unsqueeze(0).mm(other).squeeze(0)\n         elif dim_self == 2 and dim_other == 2:\n             return self.mm(other)\n-        raise ValueError(\"both arguments to __matmul__ need to be 1D or 2D, \"\n+        elif dim_self >= 2 and dim_other >= 2:\n+            # ensure each tensor is at least 3-dimensional\n+            self_exp_size = torch.Size((1,) * max(3 - self.dim(), 0) + self.size())\n+            other_exp_size = torch.Size((1,) * max(3 - other.dim(), 0) + other.size())\n+\n+            # expand the batch portion (i.e. cut off matrix dimensions and expand rest)\n+            expand_batch_portion = torch._C._infer_size(self_exp_size[:-2], other_exp_size[:-2])\n+\n+            # flatten expanded batches\n+            self_expanded = self.expand(*(expand_batch_portion + self_exp_size[-2:])) \\\n+                .contiguous().view(reduce(mul, expand_batch_portion), *self_exp_size[-2:])\n+            other_expanded = other.expand(*(expand_batch_portion + other_exp_size[-2:])) \\\n+                .contiguous().view(reduce(mul, expand_batch_portion), *other_exp_size[-2:])\n+\n+            # reshape batches back into result\n+            total_expansion = expand_batch_portion + (self_exp_size[-2], other_exp_size[-1])\n+            return self_expanded.bmm(other_expanded).view(*(total_expansion))", "path": "torch/tensor.py", "position": 30, "original_position": 30, "commit_id": "ca546930a8c9f31eab0a6fb79154ad4e985882e3", "original_commit_id": "0fb1f36420a17612e870b8c5c1851e7e85912ba3", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "No, see https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#matmul.  This is of course a somewhat arbitrary choice, but I took the view that matmul currently potentially changes the size of tensors and dispatches to non-broadcasting functions (you see this in the \"dim_self == 1 and dim_other == 2\" case).  So, we just continue that here: matmul dispatches to non-broadcasting bmm.", "created_at": "2017-06-02T18:05:35Z", "updated_at": "2018-11-23T15:33:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/1563#discussion_r119918381", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1563", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119918381"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1563#discussion_r119918381"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1563"}}, "body_html": "<p>No, see <a href=\"https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#matmul\">https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#matmul</a>.  This is of course a somewhat arbitrary choice, but I took the view that matmul currently potentially changes the size of tensors and dispatches to non-broadcasting functions (you see this in the \"dim_self == 1 and dim_other == 2\" case).  So, we just continue that here: matmul dispatches to non-broadcasting bmm.</p>", "body_text": "No, see https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#matmul.  This is of course a somewhat arbitrary choice, but I took the view that matmul currently potentially changes the size of tensors and dispatches to non-broadcasting functions (you see this in the \"dim_self == 1 and dim_other == 2\" case).  So, we just continue that here: matmul dispatches to non-broadcasting bmm.", "in_reply_to_id": 119902826}