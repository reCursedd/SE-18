{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119916867", "pull_request_review_id": 41834513, "id": 119916867, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExOTkxNjg2Nw==", "diff_hunk": "@@ -0,0 +1,13 @@\n+#ifndef TH_SIZE_INC\n+#define TH_SIZE_INC\n+\n+#include \"THGeneral.h\"\n+#include <stddef.h>\n+\n+// THTensor functions that would work on a THSize if we had such a class in C++,\n+// i.e. THTensor functions that depend only on the shape of the tensor, not the type.", "path": "torch/lib/TH/THSize.h", "position": 8, "original_position": 8, "commit_id": "ca546930a8c9f31eab0a6fb79154ad4e985882e3", "original_commit_id": "0fb1f36420a17612e870b8c5c1851e7e85912ba3", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "Yes, you are right.  Those functions use a THLongStorage somewhat naturally, though (e.g. they are called by functions that take a torch.Size at the python level).  Here I'd just be adding a THLongStorage type so it could \"belong\" in THLongStorage.  But using THLongStorage this way is terrible:\r\n1) I can't call THTensor_(newSizeOf) without doing the template specialization, because this has to work for arbitrary templated tensor types.\r\n2) I can manually construct a THLongStorage, but it has to copy, and the names are confusing: the size isn't the size, it's the nDimension, and the data is the size.\r\n\r\nI think clearly the ideal thing to do here is to have a first-class THSize class at the C/C++ level, as we do in python.  But that would obviously break a lot of code at the C/C++ level, so the next best thing is just to pretend we have that and treat torch.sizes as a dimension and sizes, which is what I've done here.\r\n\r\nThat being said, I don't think it's unreasonable to argue that using THLongStorage consistently is worth the tradeoff in clarity, usability, etc, so let me know if you want me to change this.", "created_at": "2017-06-02T17:58:36Z", "updated_at": "2018-11-23T15:33:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/1563#discussion_r119916867", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1563", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119916867"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1563#discussion_r119916867"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1563"}}, "body_html": "<p>Yes, you are right.  Those functions use a THLongStorage somewhat naturally, though (e.g. they are called by functions that take a torch.Size at the python level).  Here I'd just be adding a THLongStorage type so it could \"belong\" in THLongStorage.  But using THLongStorage this way is terrible:</p>\n<ol>\n<li>I can't call THTensor_(newSizeOf) without doing the template specialization, because this has to work for arbitrary templated tensor types.</li>\n<li>I can manually construct a THLongStorage, but it has to copy, and the names are confusing: the size isn't the size, it's the nDimension, and the data is the size.</li>\n</ol>\n<p>I think clearly the ideal thing to do here is to have a first-class THSize class at the C/C++ level, as we do in python.  But that would obviously break a lot of code at the C/C++ level, so the next best thing is just to pretend we have that and treat torch.sizes as a dimension and sizes, which is what I've done here.</p>\n<p>That being said, I don't think it's unreasonable to argue that using THLongStorage consistently is worth the tradeoff in clarity, usability, etc, so let me know if you want me to change this.</p>", "body_text": "Yes, you are right.  Those functions use a THLongStorage somewhat naturally, though (e.g. they are called by functions that take a torch.Size at the python level).  Here I'd just be adding a THLongStorage type so it could \"belong\" in THLongStorage.  But using THLongStorage this way is terrible:\n\nI can't call THTensor_(newSizeOf) without doing the template specialization, because this has to work for arbitrary templated tensor types.\nI can manually construct a THLongStorage, but it has to copy, and the names are confusing: the size isn't the size, it's the nDimension, and the data is the size.\n\nI think clearly the ideal thing to do here is to have a first-class THSize class at the C/C++ level, as we do in python.  But that would obviously break a lot of code at the C/C++ level, so the next best thing is just to pretend we have that and treat torch.sizes as a dimension and sizes, which is what I've done here.\nThat being said, I don't think it's unreasonable to argue that using THLongStorage consistently is worth the tradeoff in clarity, usability, etc, so let me know if you want me to change this.", "in_reply_to_id": 119901253}