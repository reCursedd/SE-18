{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119991517", "pull_request_review_id": 41893924, "id": 119991517, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExOTk5MTUxNw==", "diff_hunk": "@@ -939,6 +944,385 @@ def test_arange(self):\n         self.assertEqual(r1, r2, 0)\n         self.assertEqual(r2, r3[:-1], 0)\n \n+    @staticmethod\n+    def _select_broadcastable_dims(self, dims_full=None):\n+        # select full dimensionality\n+        if dims_full is None:\n+            dims_full = []\n+            ndims = random.randint(1, 4)\n+            for _ in range(ndims):\n+                dims_full = dims_full + [random.randint(1, 8)]\n+        else:\n+            ndims = len(dims_full)\n+\n+        # select actual dimensions for ops:\n+        # larger: full ndims, individual sizes may be reduced\n+        # smaller: possibly reduced ndims, sizes may be reduced\n+        smaller_ndims = random.randint(1, ndims)\n+        dims_small = []\n+        dims_large = []\n+        for i in range(ndims - 1, -1, -1):\n+            j = random.randint(1, 3)\n+            if j == 1:  # no reduced singleton dimension\n+                ds = dims_full[i]\n+                dl = dims_full[i]\n+            elif j == 2:  # larger may have reduced singleton dimension\n+                ds = dims_full[i]\n+                dl = 1 if len(dims_small) < smaller_ndims else dims_full[i]\n+            elif j == 3:  # smaller may have reduced singleton dimension\n+                ds = 1\n+                dl = dims_full[i]\n+            dims_large = [dl] + dims_large\n+            if len(dims_small) < smaller_ndims:\n+                dims_small = [ds] + dims_small\n+        return (dims_small, dims_large, dims_full)\n+\n+    @staticmethod\n+    def _test_broadcast(self, cast):\n+\n+        # all functions\n+        fns = [\n+            \"dist\", \"atan2\", \"pow\", \"lerp\", \"add\",\n+            \"sub\", \"mul\", \"div\", \"fmod\", \"remainder\",\n+            \"eq\", \"ge\", \"gt\", \"le\", \"lt\", \"max\", \"min\", \"ne\",\n+            \"addcdiv\", \"addcmul\", \"masked_copy\", \"masked_fill\",\n+            \"map\", \"map2\", \"copy\"\n+        ]\n+        # functions with no torch. equivalent\n+        fns_no_torch = [\"sub\", \"masked_copy\", \"masked_fill\", \"map\", \"map2\", \"copy\"]\n+        # functions with no inplace equivalent\n+        fns_no_inplace = [\"dist\", \"max\", \"min\"]\n+        # functions with no inplace cuda implementation\n+        fns_no_inplace_cuda = [\"map\", \"map2\"]\n+        # functions with no out-of-place tensor version\n+        fns_no_out_place = [\"masked_copy\", \"masked_fill\", \"map\", \"map2\", \"copy\"]\n+        # functions with fallback to equal nElem behavior\n+        fns_fallback = [\"add\", \"sub\", \"div\", \"mul\", \"pow\", \"fmod\", \"remainder\",\n+                        \"eq\", \"ge\", \"gt\", \"le\", \"lt\", \"max\", \"min\", \"ne\",\n+                        \"addcdiv\", \"addcmul\", \"masked_copy\", \"masked_fill\",\n+                        \"map\", \"map2\", \"copy\"]\n+        # functions with three tensor arguments\n+        fns_3_args = [\"addcdiv\", \"addcmul\", \"map2\"]\n+\n+        for fn in fns:\n+            (dims_small, dims_large, dims_full) = TestTorch._select_broadcastable_dims(self)\n+            small = torch.randn(*dims_small).float()\n+            small = cast(small)\n+            large = torch.randn(*dims_large).float()\n+            large = cast(large)\n+            smallExpanded = small.expand(*dims_full)\n+            largeExpanded = large.expand(*dims_full)\n+            small2 = None\n+            small2Expanded = None\n+            if fn in fns_3_args:\n+                # create another smaller tensor\n+                (dims_small2, _, _) = TestTorch._select_broadcastable_dims(self, dims_full)\n+                small2 = torch.randn(*dims_small2).float()\n+                small2 = cast(small2)\n+                small2Expanded = small2.expand(*dims_full)\n+\n+            if fn not in fns_no_out_place:\n+                # run through tensor versions of functions\n+                # and verify fully expanded inputs give same results\n+                fntensor_large_expanded = getattr(largeExpanded, fn)\n+                fntensor_large_non_expanded = getattr(large, fn)\n+\n+                def tensorfn(myfn, t1, t2):\n+                    if fn == \"lerp\":\n+                        return myfn(t1, 0.5)\n+                    elif fn in fns_3_args:\n+                        return myfn(1, t1, t2)\n+                    else:\n+                        return myfn(t1)\n+                r1 = tensorfn(fntensor_large_expanded, smallExpanded, small2Expanded)\n+                r2 = tensorfn(fntensor_large_non_expanded, small, small2)\n+                self.assertEqual(r1, r2)\n+                # other order\n+                fntensor_small_expanded = getattr(smallExpanded, fn)\n+                fntensor_small_non_expanded = getattr(small, fn)\n+                r1 = tensorfn(fntensor_small_expanded, largeExpanded, small2Expanded)\n+                r2 = tensorfn(fntensor_small_non_expanded, large, small2)\n+                self.assertEqual(r1, r2)\n+                if fn in fns_3_args:\n+                    fntensor_small2_expanded = getattr(small2Expanded, fn)\n+                    fntensor_small2_non_expanded = getattr(small2, fn)\n+                    r1 = tensorfn(fntensor_small2_expanded, smallExpanded, largeExpanded)\n+                    r2 = tensorfn(fntensor_small2_non_expanded, small, large)\n+                    self.assertEqual(r1, r2)\n+                    r1 = tensorfn(fntensor_small2_expanded, largeExpanded, smallExpanded)\n+                    r2 = tensorfn(fntensor_small2_non_expanded, large, small)\n+                    self.assertEqual(r1, r2)\n+\n+            # now for torch. versions of functions\n+            if fn not in fns_no_torch:\n+                fntorch = getattr(torch, fn)\n+\n+                def torchfn(t1, t2, t3):\n+                    if fn == \"lerp\":\n+                        return fntorch(t1, t2, 0.5)\n+                    elif fn in fns_3_args:\n+                        return fntorch(t1, 1.0, t2, t3)\n+                    else:\n+                        return fntorch(t1, t2)\n+                r1 = torchfn(large, small, small2)\n+                r2 = torchfn(largeExpanded, smallExpanded, small2Expanded)\n+                self.assertEqual(r1, r2)\n+                # other order\n+                r1 = torchfn(small, large, small2)\n+                r2 = torchfn(smallExpanded, largeExpanded, small2Expanded)\n+                self.assertEqual(r1, r2)\n+                if fn in fns_3_args:\n+                    r1 = torchfn(small2, small, large)\n+                    r2 = torchfn(small2Expanded, smallExpanded, largeExpanded)\n+                    self.assertEqual(r1, r2)\n+                    r1 = torchfn(small2, large, small)\n+                    r2 = torchfn(small2Expanded, largeExpanded, smallExpanded)\n+                    self.assertEqual(r1, r2)\n+\n+            # now for in place functions\n+            if fn not in fns_no_inplace and (fn not in fns_no_inplace_cuda or not largeExpanded.is_cuda):\n+                # in-place tensor is not broadcastable; test only guaranteed\n+                # to work by broadcasting other argument(s)\n+\n+                # need to clone largeExpanded so we can reuse, since functions are in-place\n+                largeExpandedClone = largeExpanded.clone()\n+\n+                def tensorfn_inplace(t0, t1, t2=None):\n+                    t0_fn = getattr(t0, fn + \"_\")\n+                    if fn == \"lerp\":\n+                        return t0_fn(t1, 0.5)\n+                    elif fn == \"masked_copy\":\n+                        return t0_fn(t1 < 0.5, t1.expand_as(t0))", "path": "test/test_torch.py", "position": null, "original_position": 210, "commit_id": "ca546930a8c9f31eab0a6fb79154ad4e985882e3", "original_commit_id": "0fb1f36420a17612e870b8c5c1851e7e85912ba3", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "No, because it's not really masked_copy, more like masked scatter and the shape of the source has nothing really to do with the shape of the underlying tensor, so we don't broadcast it.\r\n\r\nIt's just unnecessarily confusing for me to have written the test to use the expanded/non-expanded tensors, though; I'll just use a 1-d arange of the proper length or something.", "created_at": "2017-06-03T18:09:16Z", "updated_at": "2018-11-23T15:33:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/1563#discussion_r119991517", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1563", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119991517"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1563#discussion_r119991517"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1563"}}, "body_html": "<p>No, because it's not really masked_copy, more like masked scatter and the shape of the source has nothing really to do with the shape of the underlying tensor, so we don't broadcast it.</p>\n<p>It's just unnecessarily confusing for me to have written the test to use the expanded/non-expanded tensors, though; I'll just use a 1-d arange of the proper length or something.</p>", "body_text": "No, because it's not really masked_copy, more like masked scatter and the shape of the source has nothing really to do with the shape of the underlying tensor, so we don't broadcast it.\nIt's just unnecessarily confusing for me to have written the test to use the expanded/non-expanded tensors, though; I'll just use a 1-d arange of the proper length or something.", "in_reply_to_id": 119745221}