{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/120843916", "pull_request_review_id": 42840303, "id": 120843916, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMDg0MzkxNg==", "diff_hunk": "@@ -300,7 +302,24 @@ def __matmul__(self, other):\n             return self.unsqueeze(0).mm(other).squeeze(0)\n         elif dim_self == 2 and dim_other == 2:\n             return self.mm(other)\n-        raise ValueError(\"both arguments to __matmul__ need to be 1D or 2D, \"\n+        elif dim_self >= 2 and dim_other >= 2:\n+            # ensure each tensor is at least 3-dimensional\n+            self_exp_size = torch.Size((1,) * max(3 - self.dim(), 0) + self.size())\n+            other_exp_size = torch.Size((1,) * max(3 - other.dim(), 0) + other.size())\n+\n+            # expand the batch portion (i.e. cut off matrix dimensions and expand rest)\n+            expand_batch_portion = torch._C._infer_size(self_exp_size[:-2], other_exp_size[:-2])\n+\n+            # flatten expanded batches\n+            self_expanded = self.expand(*(expand_batch_portion + self_exp_size[-2:])) \\\n+                .contiguous().view(reduce(mul, expand_batch_portion), *self_exp_size[-2:])\n+            other_expanded = other.expand(*(expand_batch_portion + other_exp_size[-2:])) \\\n+                .contiguous().view(reduce(mul, expand_batch_portion), *other_exp_size[-2:])\n+\n+            # reshape batches back into result\n+            total_expansion = expand_batch_portion + (self_exp_size[-2], other_exp_size[-1])\n+            return self_expanded.bmm(other_expanded).view(*(total_expansion))", "path": "torch/tensor.py", "position": 30, "original_position": 30, "commit_id": "ca546930a8c9f31eab0a6fb79154ad4e985882e3", "original_commit_id": "0fb1f36420a17612e870b8c5c1851e7e85912ba3", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "Given that `mm`, `bmm`, `ger` are low-level functions that do not perform broadcasting, what about explicitly stating that in the docs, and add a `torch.matmul` function equivalent to `__matmul__`, with proper documentation? For the moment, the `@` operator is not documented, and only potentially used by py3 users.", "created_at": "2017-06-08T09:46:29Z", "updated_at": "2018-11-23T15:33:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/1563#discussion_r120843916", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1563", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/120843916"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1563#discussion_r120843916"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1563"}}, "body_html": "<p>Given that <code>mm</code>, <code>bmm</code>, <code>ger</code> are low-level functions that do not perform broadcasting, what about explicitly stating that in the docs, and add a <code>torch.matmul</code> function equivalent to <code>__matmul__</code>, with proper documentation? For the moment, the <code>@</code> operator is not documented, and only potentially used by py3 users.</p>", "body_text": "Given that mm, bmm, ger are low-level functions that do not perform broadcasting, what about explicitly stating that in the docs, and add a torch.matmul function equivalent to __matmul__, with proper documentation? For the moment, the @ operator is not documented, and only potentially used by py3 users.", "in_reply_to_id": 119902826}