{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9563", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9563/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9563/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9563/events", "html_url": "https://github.com/pytorch/pytorch/issues/9563", "id": 342489303, "node_id": "MDU6SXNzdWUzNDI0ODkzMDM=", "number": 9563, "title": "batch_sampler/test_worker_seed intermittently fails with address already in use on OS X", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-18T20:57:32Z", "updated_at": "2018-07-24T16:28:22Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Sample log:</p>\n<pre><code>19:26:36 test_add_dataset (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_raises_index_error (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_two_non_singletons (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_two_non_singletons_with_empty (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_two_singletons (__main__.TestConcatDataset) ... ok\n19:26:36 test_batch_sampler (__main__.TestDataLoader) ... libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: Address already in use\n19:26:36 ERROR\n19:26:36 test_default_colate_bad_numpy_types (__main__.TestDataLoader) ... ok\n19:26:36 test_error (__main__.TestDataLoader) ... ok\n19:26:36 test_error_workers (__main__.TestDataLoader) ... ok\n19:26:36 test_growing_dataset (__main__.TestDataLoader) ... ok\n19:26:36 test_invalid_assign_after_init (__main__.TestDataLoader) ... ok\n19:26:36 test_len (__main__.TestDataLoader) ... ok\n19:26:36 test_manager_unclean_exit (__main__.TestDataLoader)\n19:26:36 there might be ConnectionResetError or leaked semaphore warning (due to dirty process exit), but they are all safe to ignore ... skipped 'CUDA unavailable'\n19:26:36 test_multiple_dataloaders (__main__.TestDataLoader) ... ok\n19:26:36 test_numpy (__main__.TestDataLoader) ... ok\n19:26:36 test_numpy_scalars (__main__.TestDataLoader) ... ok\n19:26:36 test_partial_workers (__main__.TestDataLoader)\n19:26:36 check that workers exit even if the iterator is not exhausted ... skipped 'CUDA unavailable'\n19:26:36 test_segfault (__main__.TestDataLoader) ... skipped 'temporarily disable until flaky failures are fixed'\n19:26:36 test_seqential_batch_workers (__main__.TestDataLoader) ... ok\n19:26:36 test_sequential (__main__.TestDataLoader) ... ok\n19:26:36 test_sequential_batch (__main__.TestDataLoader) ... ok\n19:26:36 test_sequential_pin_memory (__main__.TestDataLoader) ... skipped 'CUDA unavailable'\n19:26:36 test_sequential_workers (__main__.TestDataLoader) ... ok\n19:26:36 test_shuffle (__main__.TestDataLoader) ... ok\n19:26:36 test_shuffle_batch (__main__.TestDataLoader) ... ok\n19:26:37 test_shuffle_batch_workers (__main__.TestDataLoader) ... ok\n19:26:37 test_shuffle_pin_memory (__main__.TestDataLoader) ... skipped 'CUDA unavailable'\n19:26:37 test_shuffle_workers (__main__.TestDataLoader) ... ok\n19:26:37 test_timeout (__main__.TestDataLoader) ... ok\n19:26:38 test_worker_init_fn (__main__.TestDataLoader) ... ok\n19:26:38 test_worker_seed (__main__.TestDataLoader) ... ok\n19:26:38 test_lengths_must_equal_datset_size (__main__.TestDatasetRandomSplit) ... ok\n19:26:38 test_splits_are_mutually_exclusive (__main__.TestDatasetRandomSplit) ... ok\n19:26:38 test_splits_have_correct_size (__main__.TestDatasetRandomSplit) ... ok\n19:26:38 test_pin_memory (__main__.TestDictDataLoader) ... skipped 'CUDA unavailable'\n19:26:38 test_sequential_batch (__main__.TestDictDataLoader) ... ok\n19:26:38 test_ind_worker_queue (__main__.TestIndividualWorkerQueue) ... ok\n19:26:39 test_shuffle_pin_memory (__main__.TestStringDataLoader) ... skipped 'CUDA unavailable'\n19:26:39 test_getitem (__main__.TestTensorDataset) ... ok\n19:26:39 test_getitem_1d (__main__.TestTensorDataset) ... ok\n19:26:39 test_len (__main__.TestTensorDataset) ... ok\n19:26:39 test_many_tensors (__main__.TestTensorDataset) ... ok\n19:26:39 test_single_tensor (__main__.TestTensorDataset) ... ok\n19:26:39 \n19:26:39 ======================================================================\n19:26:39 ERROR: test_batch_sampler (__main__.TestDataLoader)\n19:26:39 ----------------------------------------------------------------------\n19:26:39 Traceback (most recent call last):\n19:26:39   File \"test_dataloader.py\", line 441, in test_batch_sampler\n19:26:39     self._test_batch_sampler(num_workers=4)\n19:26:39   File \"test_dataloader.py\", line 427, in _test_batch_sampler\n19:26:39     for i, (input, _target) in enumerate(dl):\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 336, in __next__\n19:26:39     return self._process_next_batch(batch)\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 357, in _process_next_batch\n19:26:39     raise batch.exc_type(batch.exc_msg)\n19:26:39 RuntimeError: Traceback (most recent call last):\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n19:26:39     samples = collate_fn([dataset[i] for i in batch_indices])\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in default_collate\n19:26:39     return [default_collate(samples) for samples in transposed]\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in &lt;listcomp&gt;\n19:26:39     return [default_collate(samples) for samples in transposed]\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 162, in default_collate\n19:26:39     storage = batch[0].storage()._new_shared(numel)\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/storage.py\", line 118, in _new_shared\n19:26:39     return cls._new_using_filename(size)\n19:26:39 RuntimeError: std::exception at /private/var/lib/jenkins/workspace/pytorch-builds/pytorch-macos-10.13-py3-build/torch/lib/libshm/core.cpp:99\n</code></pre>\n<p>log: <a href=\"https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-macos-10.13-py3-test1/4002/console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-macos-10.13-py3-test1/4002/console</a></p>", "body_text": "Sample log:\n19:26:36 test_add_dataset (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_raises_index_error (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_two_non_singletons (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_two_non_singletons_with_empty (__main__.TestConcatDataset) ... ok\n19:26:36 test_concat_two_singletons (__main__.TestConcatDataset) ... ok\n19:26:36 test_batch_sampler (__main__.TestDataLoader) ... libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: Address already in use\n19:26:36 ERROR\n19:26:36 test_default_colate_bad_numpy_types (__main__.TestDataLoader) ... ok\n19:26:36 test_error (__main__.TestDataLoader) ... ok\n19:26:36 test_error_workers (__main__.TestDataLoader) ... ok\n19:26:36 test_growing_dataset (__main__.TestDataLoader) ... ok\n19:26:36 test_invalid_assign_after_init (__main__.TestDataLoader) ... ok\n19:26:36 test_len (__main__.TestDataLoader) ... ok\n19:26:36 test_manager_unclean_exit (__main__.TestDataLoader)\n19:26:36 there might be ConnectionResetError or leaked semaphore warning (due to dirty process exit), but they are all safe to ignore ... skipped 'CUDA unavailable'\n19:26:36 test_multiple_dataloaders (__main__.TestDataLoader) ... ok\n19:26:36 test_numpy (__main__.TestDataLoader) ... ok\n19:26:36 test_numpy_scalars (__main__.TestDataLoader) ... ok\n19:26:36 test_partial_workers (__main__.TestDataLoader)\n19:26:36 check that workers exit even if the iterator is not exhausted ... skipped 'CUDA unavailable'\n19:26:36 test_segfault (__main__.TestDataLoader) ... skipped 'temporarily disable until flaky failures are fixed'\n19:26:36 test_seqential_batch_workers (__main__.TestDataLoader) ... ok\n19:26:36 test_sequential (__main__.TestDataLoader) ... ok\n19:26:36 test_sequential_batch (__main__.TestDataLoader) ... ok\n19:26:36 test_sequential_pin_memory (__main__.TestDataLoader) ... skipped 'CUDA unavailable'\n19:26:36 test_sequential_workers (__main__.TestDataLoader) ... ok\n19:26:36 test_shuffle (__main__.TestDataLoader) ... ok\n19:26:36 test_shuffle_batch (__main__.TestDataLoader) ... ok\n19:26:37 test_shuffle_batch_workers (__main__.TestDataLoader) ... ok\n19:26:37 test_shuffle_pin_memory (__main__.TestDataLoader) ... skipped 'CUDA unavailable'\n19:26:37 test_shuffle_workers (__main__.TestDataLoader) ... ok\n19:26:37 test_timeout (__main__.TestDataLoader) ... ok\n19:26:38 test_worker_init_fn (__main__.TestDataLoader) ... ok\n19:26:38 test_worker_seed (__main__.TestDataLoader) ... ok\n19:26:38 test_lengths_must_equal_datset_size (__main__.TestDatasetRandomSplit) ... ok\n19:26:38 test_splits_are_mutually_exclusive (__main__.TestDatasetRandomSplit) ... ok\n19:26:38 test_splits_have_correct_size (__main__.TestDatasetRandomSplit) ... ok\n19:26:38 test_pin_memory (__main__.TestDictDataLoader) ... skipped 'CUDA unavailable'\n19:26:38 test_sequential_batch (__main__.TestDictDataLoader) ... ok\n19:26:38 test_ind_worker_queue (__main__.TestIndividualWorkerQueue) ... ok\n19:26:39 test_shuffle_pin_memory (__main__.TestStringDataLoader) ... skipped 'CUDA unavailable'\n19:26:39 test_getitem (__main__.TestTensorDataset) ... ok\n19:26:39 test_getitem_1d (__main__.TestTensorDataset) ... ok\n19:26:39 test_len (__main__.TestTensorDataset) ... ok\n19:26:39 test_many_tensors (__main__.TestTensorDataset) ... ok\n19:26:39 test_single_tensor (__main__.TestTensorDataset) ... ok\n19:26:39 \n19:26:39 ======================================================================\n19:26:39 ERROR: test_batch_sampler (__main__.TestDataLoader)\n19:26:39 ----------------------------------------------------------------------\n19:26:39 Traceback (most recent call last):\n19:26:39   File \"test_dataloader.py\", line 441, in test_batch_sampler\n19:26:39     self._test_batch_sampler(num_workers=4)\n19:26:39   File \"test_dataloader.py\", line 427, in _test_batch_sampler\n19:26:39     for i, (input, _target) in enumerate(dl):\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 336, in __next__\n19:26:39     return self._process_next_batch(batch)\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 357, in _process_next_batch\n19:26:39     raise batch.exc_type(batch.exc_msg)\n19:26:39 RuntimeError: Traceback (most recent call last):\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n19:26:39     samples = collate_fn([dataset[i] for i in batch_indices])\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in default_collate\n19:26:39     return [default_collate(samples) for samples in transposed]\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in <listcomp>\n19:26:39     return [default_collate(samples) for samples in transposed]\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 162, in default_collate\n19:26:39     storage = batch[0].storage()._new_shared(numel)\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/storage.py\", line 118, in _new_shared\n19:26:39     return cls._new_using_filename(size)\n19:26:39 RuntimeError: std::exception at /private/var/lib/jenkins/workspace/pytorch-builds/pytorch-macos-10.13-py3-build/torch/lib/libshm/core.cpp:99\n\nlog: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-macos-10.13-py3-test1/4002/console", "body": "Sample log:\r\n\r\n```\r\n19:26:36 test_add_dataset (__main__.TestConcatDataset) ... ok\r\n19:26:36 test_concat_raises_index_error (__main__.TestConcatDataset) ... ok\r\n19:26:36 test_concat_two_non_singletons (__main__.TestConcatDataset) ... ok\r\n19:26:36 test_concat_two_non_singletons_with_empty (__main__.TestConcatDataset) ... ok\r\n19:26:36 test_concat_two_singletons (__main__.TestConcatDataset) ... ok\r\n19:26:36 test_batch_sampler (__main__.TestDataLoader) ... libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: Address already in use\r\n19:26:36 ERROR\r\n19:26:36 test_default_colate_bad_numpy_types (__main__.TestDataLoader) ... ok\r\n19:26:36 test_error (__main__.TestDataLoader) ... ok\r\n19:26:36 test_error_workers (__main__.TestDataLoader) ... ok\r\n19:26:36 test_growing_dataset (__main__.TestDataLoader) ... ok\r\n19:26:36 test_invalid_assign_after_init (__main__.TestDataLoader) ... ok\r\n19:26:36 test_len (__main__.TestDataLoader) ... ok\r\n19:26:36 test_manager_unclean_exit (__main__.TestDataLoader)\r\n19:26:36 there might be ConnectionResetError or leaked semaphore warning (due to dirty process exit), but they are all safe to ignore ... skipped 'CUDA unavailable'\r\n19:26:36 test_multiple_dataloaders (__main__.TestDataLoader) ... ok\r\n19:26:36 test_numpy (__main__.TestDataLoader) ... ok\r\n19:26:36 test_numpy_scalars (__main__.TestDataLoader) ... ok\r\n19:26:36 test_partial_workers (__main__.TestDataLoader)\r\n19:26:36 check that workers exit even if the iterator is not exhausted ... skipped 'CUDA unavailable'\r\n19:26:36 test_segfault (__main__.TestDataLoader) ... skipped 'temporarily disable until flaky failures are fixed'\r\n19:26:36 test_seqential_batch_workers (__main__.TestDataLoader) ... ok\r\n19:26:36 test_sequential (__main__.TestDataLoader) ... ok\r\n19:26:36 test_sequential_batch (__main__.TestDataLoader) ... ok\r\n19:26:36 test_sequential_pin_memory (__main__.TestDataLoader) ... skipped 'CUDA unavailable'\r\n19:26:36 test_sequential_workers (__main__.TestDataLoader) ... ok\r\n19:26:36 test_shuffle (__main__.TestDataLoader) ... ok\r\n19:26:36 test_shuffle_batch (__main__.TestDataLoader) ... ok\r\n19:26:37 test_shuffle_batch_workers (__main__.TestDataLoader) ... ok\r\n19:26:37 test_shuffle_pin_memory (__main__.TestDataLoader) ... skipped 'CUDA unavailable'\r\n19:26:37 test_shuffle_workers (__main__.TestDataLoader) ... ok\r\n19:26:37 test_timeout (__main__.TestDataLoader) ... ok\r\n19:26:38 test_worker_init_fn (__main__.TestDataLoader) ... ok\r\n19:26:38 test_worker_seed (__main__.TestDataLoader) ... ok\r\n19:26:38 test_lengths_must_equal_datset_size (__main__.TestDatasetRandomSplit) ... ok\r\n19:26:38 test_splits_are_mutually_exclusive (__main__.TestDatasetRandomSplit) ... ok\r\n19:26:38 test_splits_have_correct_size (__main__.TestDatasetRandomSplit) ... ok\r\n19:26:38 test_pin_memory (__main__.TestDictDataLoader) ... skipped 'CUDA unavailable'\r\n19:26:38 test_sequential_batch (__main__.TestDictDataLoader) ... ok\r\n19:26:38 test_ind_worker_queue (__main__.TestIndividualWorkerQueue) ... ok\r\n19:26:39 test_shuffle_pin_memory (__main__.TestStringDataLoader) ... skipped 'CUDA unavailable'\r\n19:26:39 test_getitem (__main__.TestTensorDataset) ... ok\r\n19:26:39 test_getitem_1d (__main__.TestTensorDataset) ... ok\r\n19:26:39 test_len (__main__.TestTensorDataset) ... ok\r\n19:26:39 test_many_tensors (__main__.TestTensorDataset) ... ok\r\n19:26:39 test_single_tensor (__main__.TestTensorDataset) ... ok\r\n19:26:39 \r\n19:26:39 ======================================================================\r\n19:26:39 ERROR: test_batch_sampler (__main__.TestDataLoader)\r\n19:26:39 ----------------------------------------------------------------------\r\n19:26:39 Traceback (most recent call last):\r\n19:26:39   File \"test_dataloader.py\", line 441, in test_batch_sampler\r\n19:26:39     self._test_batch_sampler(num_workers=4)\r\n19:26:39   File \"test_dataloader.py\", line 427, in _test_batch_sampler\r\n19:26:39     for i, (input, _target) in enumerate(dl):\r\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 336, in __next__\r\n19:26:39     return self._process_next_batch(batch)\r\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 357, in _process_next_batch\r\n19:26:39     raise batch.exc_type(batch.exc_msg)\r\n19:26:39 RuntimeError: Traceback (most recent call last):\r\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\r\n19:26:39     samples = collate_fn([dataset[i] for i in batch_indices])\r\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in default_collate\r\n19:26:39     return [default_collate(samples) for samples in transposed]\r\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in <listcomp>\r\n19:26:39     return [default_collate(samples) for samples in transposed]\r\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 162, in default_collate\r\n19:26:39     storage = batch[0].storage()._new_shared(numel)\r\n19:26:39   File \"/var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/storage.py\", line 118, in _new_shared\r\n19:26:39     return cls._new_using_filename(size)\r\n19:26:39 RuntimeError: std::exception at /private/var/lib/jenkins/workspace/pytorch-builds/pytorch-macos-10.13-py3-build/torch/lib/libshm/core.cpp:99\r\n```\r\n\r\nlog: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-macos-10.13-py3-test1/4002/console"}