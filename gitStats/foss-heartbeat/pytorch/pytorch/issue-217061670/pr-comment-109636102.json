{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109636102", "pull_request_review_id": 30749176, "id": 109636102, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTYzNjEwMg==", "diff_hunk": "@@ -1239,41 +1250,52 @@ def unpack_variables(args):\n \n for test in function_tests:\n     cls, constructor_args, call_args = test[:3]\n-    test_name = 'test_{}Function'.format(cls.__name__)\n-    if len(test) == 4:\n-        test_name += '_' + test[3]\n-\n-    def do_test(self, cls=cls, constructor_args=constructor_args,\n-                call_args=call_args, test_name=test_name):\n-        input = create_input(call_args)\n-        self.assertEqual(gradcheck(cls(*constructor_args), input, eps=1e-6, atol=PRECISION), True)\n-\n-        if test_name not in ignore_inplace and issubclass(cls, InplaceFunction):\n-            output = cls(*constructor_args)(*input)\n-            if not isinstance(output, tuple):\n-                output = (output,)\n-            inplace_input = deepcopy(input)\n-            inplace_input_copy = tuple(i + 0 for i in inplace_input)\n-            fn = cls(*constructor_args, inplace=True)\n-            inplace_output = fn(*inplace_input_copy)\n-            if not isinstance(inplace_output, tuple):\n-                inplace_output = (inplace_output,)\n-            self.assertEqual(inplace_output, output)\n-            # Check that gradient is the same\n-            for inp_i, i in zip(inplace_input, input):\n-                if inp_i.grad is not None:\n-                    inp_i.grad.data.zero_()\n-                if i.grad is not None:\n-                    i.grad.data.zero_()\n-            for io, o in zip(inplace_output, output):\n-                grad = torch.randn(*io.size()).double()\n-                io.backward(grad)\n-                o.backward(grad)\n-            for inp_i, i in zip(inplace_input, input):\n-                self.assertEqual(inp_i.grad, i.grad)\n-\n-    assert not hasattr(TestAutograd, test_name), 'Two tests have the same name: ' + test_name\n-    setattr(TestAutograd, test_name, do_test)\n+    basic_test_name = 'test_{}Function'.format(cls.__name__)\n+    if len(test) >= 4:\n+        basic_test_name += '_' + test[3]\n+\n+    dim_args_idx = test[4] if len(test) == 5 else []\n+\n+    for dim_perm in product([-1, 1], repeat=len(dim_args_idx)):\n+        test_name = basic_test_name\n+        new_constructor_args = list(constructor_args)\n+        for i, arg_idx in enumerate(dim_args_idx):\n+            new_constructor_args[arg_idx] *= dim_perm[i]\n+            if dim_perm[i] == -1:\n+                test_name += \"_negdimarg\" + str(arg_idx)\n+        new_constructor_args = tuple(new_constructor_args)", "path": "test/test_autograd.py", "position": null, "original_position": 252, "commit_id": "37d95687c47beab841cd67003de3390414db0dca", "original_commit_id": "f6a50c2a5406a6acb1c06d1679edff3caf6de494", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "yes,\r\nThe first one liner does not work though because not all args are possible dimensions.\r\nI modified it to work, but I am not sure if its clearer than the for loop, let me know.", "created_at": "2017-04-04T11:21:24Z", "updated_at": "2018-11-23T15:32:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/1108#discussion_r109636102", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1108", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109636102"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1108#discussion_r109636102"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1108"}}, "body_html": "<p>yes,<br>\nThe first one liner does not work though because not all args are possible dimensions.<br>\nI modified it to work, but I am not sure if its clearer than the for loop, let me know.</p>", "body_text": "yes,\nThe first one liner does not work though because not all args are possible dimensions.\nI modified it to work, but I am not sure if its clearer than the for loop, let me know.", "in_reply_to_id": 109315162}