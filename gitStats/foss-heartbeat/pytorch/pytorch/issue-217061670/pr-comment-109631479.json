{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109631479", "pull_request_review_id": 30744171, "id": 109631479, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTYzMTQ3OQ==", "diff_hunk": "@@ -3068,23 +3069,107 @@ def test_Size(self):\n         self.assertIsInstance(x[:-1], torch.Size)\n         self.assertIsInstance(x + x, torch.Size)\n \n-    def test_transpose_neg(self):\n-        x = torch.randn(10, 20, 30)\n-        ndim = 3\n+# Functions to test negative dimension wrapping\n+METHOD = 1\n+INPLACE_METHOD = 2\n+FUNCTIONAL = 4\n+DIM_ARG = None\n \n-        for i, j in combinations(range(ndim), 2):\n-            a = x.transpose(i, j)\n-            b = x.transpose(i - ndim, j - ndim)\n-            self.assertEqual(a, b)\n \n-            a = torch.transpose(x, i, j)\n-            b = torch.transpose(x, i - ndim, j - ndim)\n-            self.assertEqual(a, b)\n-\n-            a = x.clone()\n-            x.transpose_(i, j)\n-            x.transpose_(i - ndim, j - ndim)\n-            self.assertEqual(a, x)\n+def make_neg_dim_test(name, tensor_arg, arg_constr, types, extra_dim=0):\n+    def neg_dim_test(self):\n+        if isinstance(tensor_arg, list):\n+            assert 0 not in types and 1 not in types\n+            x = [torch.randn(arg) for arg in tensor_arg]\n+            ndim = len(tensor_arg[-1])\n+        else:\n+            x = torch.randn(*tensor_arg)\n+            ndim = len(tensor_arg)\n+        ndim += extra_dim\n+\n+        n_dim_to_test = sum(map(lambda e: e is DIM_ARG, arg_constr()))\n+\n+        for dims_val in combinations(range(ndim), n_dim_to_test):\n+            arg = arg_constr()\n+            arg_neg = copy.deepcopy(arg)\n+            idx = 0\n+            for i, v in enumerate(arg):\n+                if v is DIM_ARG:\n+                    arg[i] = dims_val[idx]\n+                    arg_neg[i] = dims_val[idx] - ndim\n+                    idx += 1\n+\n+            if METHOD in types:\n+                a = getattr(x, name)(*arg)\n+                b = getattr(x, name)(*arg_neg)\n+                self.assertEqual(a, b)\n+\n+            if INPLACE_METHOD in types:\n+                a = x.clone()\n+                getattr(a, name + '_')(*arg)\n+                b = x.clone()\n+                getattr(b, name + '_')(*arg_neg)\n+                self.assertEqual(a, b)\n+\n+            if FUNCTIONAL in types:\n+                a = getattr(torch, name)(x, *arg)\n+                b = getattr(torch, name)(x, *arg_neg)\n+                self.assertEqual(a, b)\n+\n+    return neg_dim_test\n+\n+\n+def idx_tensor(size, max_val):\n+    return torch.LongTensor(*size).random_(0, max_val - 1)\n+\n+neg_dim_tests = [\n+    ('narrow', (10, 20, 30), lambda: [DIM_ARG, 0, 5], [METHOD]),\n+    ('transpose', (10, 20, 30), lambda: [DIM_ARG, DIM_ARG], [METHOD, INPLACE_METHOD, FUNCTIONAL]),\n+    ('size', (10, 20, 30), lambda: [DIM_ARG], [METHOD]),\n+    ('cat', [(2, 3, 4), (2, 3, 4)], lambda: [DIM_ARG], [FUNCTIONAL]),\n+    ('chunk', (10, 20, 30), lambda: [5, DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('gather', (10, 20), lambda: [DIM_ARG, idx_tensor((10, 20), 10)], [METHOD, FUNCTIONAL]),\n+    ('index_select', (10, 10), lambda: [DIM_ARG, idx_tensor((10,), 10)], [METHOD, FUNCTIONAL]),\n+    ('split', (10, 20), lambda: [5, DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('squeeze', (10, 1, 20, 1), lambda: [DIM_ARG], [METHOD, INPLACE_METHOD, FUNCTIONAL]),\n+    ('stack', [(2, 3, 4), (2, 3, 4)], lambda: [DIM_ARG], [FUNCTIONAL]),\n+    ('unbind', (2, 3, 4), lambda: [DIM_ARG], [FUNCTIONAL]),\n+    ('unsqueeze', (10, 20), lambda: [DIM_ARG], [METHOD, INPLACE_METHOD, FUNCTIONAL], 1),\n+    ('cumprod', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('cumsum', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('mean', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('median', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('mode', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('norm', (10, 20), lambda: [2, DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('prod', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('std', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('sum', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('var', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('kthvalue', (10, 20), lambda: [3, DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('max', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('min', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('sort', (10, 20), lambda: [DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('topk', (10, 20), lambda: [5, DIM_ARG], [METHOD, FUNCTIONAL]),\n+    ('renorm', (10, 20), lambda: [2, DIM_ARG, 1], [METHOD, INPLACE_METHOD, FUNCTIONAL]),\n+    ('index_add', (10, 10), lambda: [DIM_ARG, idx_tensor((10,), 10), torch.randn(10, 10)], [INPLACE_METHOD]),\n+    ('index_copy', (10, 10), lambda: [DIM_ARG, idx_tensor((10,), 10), torch.randn(10, 10)], [INPLACE_METHOD]),\n+    ('index_fill', (10, 10), lambda: [DIM_ARG, idx_tensor((10,), 10), 12], [INPLACE_METHOD]),\n+    ('scatter', (10, 10), lambda: [DIM_ARG, idx_tensor((10, 10), 10), torch.randn(10, 10)], [INPLACE_METHOD]),\n+    ('select', (10, 20), lambda: [DIM_ARG, 3], [METHOD]),\n+    ('unfold', (10, 20), lambda: [DIM_ARG, 5, 2], [METHOD]),\n+]\n+\n+for decl in neg_dim_tests:\n+    if len(decl) == 4:\n+        name, tensor_arg, arg_constr, types = decl\n+        extra_dim = 0", "path": "test/test_torch.py", "position": 120, "original_position": 120, "commit_id": "37d95687c47beab841cd67003de3390414db0dca", "original_commit_id": "f6a50c2a5406a6acb1c06d1679edff3caf6de494", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "Yes, for `unsqueeze` which should wrap at ndim+1 instead of ndim ", "created_at": "2017-04-04T10:53:18Z", "updated_at": "2018-11-23T15:32:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/1108#discussion_r109631479", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1108", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109631479"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1108#discussion_r109631479"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1108"}}, "body_html": "<p>Yes, for <code>unsqueeze</code> which should wrap at ndim+1 instead of ndim</p>", "body_text": "Yes, for unsqueeze which should wrap at ndim+1 instead of ndim", "in_reply_to_id": 109315490}