{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/182373123", "pull_request_review_id": 113152980, "id": 182373123, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MjM3MzEyMw==", "diff_hunk": "@@ -1489,6 +1489,67 @@\n         - **v** (*Tensor*): the eigenvectors of ``a`` if ``eigenvectors`` is ``True``; otherwise an empty tensor\n \"\"\")\n \n+add_docstr(torch.einsum,\n+           r\"\"\"\n+einsum(equation, operands) -> Tensor\n+\n+This function provides a way of computing multilinear expressions (i.e. sums of products) using the\n+Einstein summation convention.\n+\n+Args:\n+    equation (string): The equation is given in terms of lower case letters (indices) to be associated\n+           with each dimension of the operands and result. The left hand side lists the operands\n+           dimensions, separated by commas. There should be one index letter per tensor dimension.\n+           The right hand side follows after `->` and gives the indices for the output.\n+           If the `->` and right hand side are omitted, it implicitly defined as the alphabetically\n+           sorted list of all indices appearing exactly once in the left hand side.\n+           The indices not apprearing in the output are summed over after multiplying the operands\n+           entries.\n+           `einsum` does not implement diagonals (multiple occurences of a single index for one tensor,\n+           e.g. `ii->i`) and ellipses (`...`).\n+    operands (list of Tensors): The operands to compute the Einstein sum of.\n+           Note that the operands are passed as a list, not as individual arguments.\n+\n+Examples::\n+\n+    >>> x = torch.randn(5)\n+    >>> y = torch.randn(4)\n+    >>> torch.einsum('i,j->ij', (x,y))  # outer product\n+\n+    -1.0066 -2.0433 -0.8290  0.8429", "path": "torch/_torch_docs.py", "position": 31, "original_position": 31, "commit_id": "daf3c25ee5e9a1a8fe038ddc416761646410b199", "original_commit_id": "d7c7ebe6d0a6513ea95fb356abe751ca4893b8ac", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "      Is there a reason we need to pass arguments as tuples?\r\n\r\nGreat comment, thank you!\r\nThe parsing of arguments is in [`torch/csrc/utils/python_arg_parser.cpp`, in `FunctionSignature::parse`](https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/torch/csrc/utils/python_arg_parser.cpp#L363). Varargs are only supported if there is a single parameter and that is of type IntList. This is because in the arg parsing [the entire argument tuple is assigned as the argument](https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/torch/csrc/utils/python_arg_parser.cpp#L409). Now, if we wanted to have a slice there (which would be the way to support it generically), we would need to construct one using `PyTuple_GetSlice`. But that would be a new reference, and suddenly we would need to release the reference in the indirect caller - who typically used [`PythonArgParse.parse`](https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/torch/csrc/utils/python_arg_parser.h#L64) . So the way to support it would be to return something like \"vararg-tuple\" and deallocate that or have `PythonArgs` do that automatically when it goes out of scope - but I'm not sure whether the latter is a good idea if the caller does have the `ParsedArgs` struct as well. All this seems to be fairly invasive and also I don't understand the rationale of exposing `ParsedArgs` to callers (rather than only showing the `PythonArgs`) to not have that be a fairly strong distraction to the einsum implementation.\r\n\r\nThe obvious alternative is to do our own arg parsing and blacklist it in automatic generation - similar to what is [done for clamp](https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/tools/autograd/templates/python_torch_functions.cpp#L51). The additional complication would be that we cannot use `PythonArgParse` - not that bad if we can still use `PythonArgs`.\r\n\r\nAs such, I would tentatively not want to burden the reviewers with this. However if they would like to review that along with the other bits, I would be happy to include that in the PR as well.", "created_at": "2018-04-18T09:58:53Z", "updated_at": "2018-11-23T15:42:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/6307#discussion_r182373123", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6307", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/182373123"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6307#discussion_r182373123"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6307"}}, "body_html": "<pre><code>  Is there a reason we need to pass arguments as tuples?\n</code></pre>\n<p>Great comment, thank you!<br>\nThe parsing of arguments is in <a href=\"https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/torch/csrc/utils/python_arg_parser.cpp#L363\"><code>torch/csrc/utils/python_arg_parser.cpp</code>, in <code>FunctionSignature::parse</code></a>. Varargs are only supported if there is a single parameter and that is of type IntList. This is because in the arg parsing <a href=\"https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/torch/csrc/utils/python_arg_parser.cpp#L409\">the entire argument tuple is assigned as the argument</a>. Now, if we wanted to have a slice there (which would be the way to support it generically), we would need to construct one using <code>PyTuple_GetSlice</code>. But that would be a new reference, and suddenly we would need to release the reference in the indirect caller - who typically used <a href=\"https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/torch/csrc/utils/python_arg_parser.h#L64\"><code>PythonArgParse.parse</code></a> . So the way to support it would be to return something like \"vararg-tuple\" and deallocate that or have <code>PythonArgs</code> do that automatically when it goes out of scope - but I'm not sure whether the latter is a good idea if the caller does have the <code>ParsedArgs</code> struct as well. All this seems to be fairly invasive and also I don't understand the rationale of exposing <code>ParsedArgs</code> to callers (rather than only showing the <code>PythonArgs</code>) to not have that be a fairly strong distraction to the einsum implementation.</p>\n<p>The obvious alternative is to do our own arg parsing and blacklist it in automatic generation - similar to what is <a href=\"https://github.com/pytorch/pytorch/blob/187955b9599c162f75d19ceb65fde02bd85253b3/tools/autograd/templates/python_torch_functions.cpp#L51\">done for clamp</a>. The additional complication would be that we cannot use <code>PythonArgParse</code> - not that bad if we can still use <code>PythonArgs</code>.</p>\n<p>As such, I would tentatively not want to burden the reviewers with this. However if they would like to review that along with the other bits, I would be happy to include that in the PR as well.</p>", "body_text": "Is there a reason we need to pass arguments as tuples?\n\nGreat comment, thank you!\nThe parsing of arguments is in torch/csrc/utils/python_arg_parser.cpp, in FunctionSignature::parse. Varargs are only supported if there is a single parameter and that is of type IntList. This is because in the arg parsing the entire argument tuple is assigned as the argument. Now, if we wanted to have a slice there (which would be the way to support it generically), we would need to construct one using PyTuple_GetSlice. But that would be a new reference, and suddenly we would need to release the reference in the indirect caller - who typically used PythonArgParse.parse . So the way to support it would be to return something like \"vararg-tuple\" and deallocate that or have PythonArgs do that automatically when it goes out of scope - but I'm not sure whether the latter is a good idea if the caller does have the ParsedArgs struct as well. All this seems to be fairly invasive and also I don't understand the rationale of exposing ParsedArgs to callers (rather than only showing the PythonArgs) to not have that be a fairly strong distraction to the einsum implementation.\nThe obvious alternative is to do our own arg parsing and blacklist it in automatic generation - similar to what is done for clamp. The additional complication would be that we cannot use PythonArgParse - not that bad if we can still use PythonArgs.\nAs such, I would tentatively not want to burden the reviewers with this. However if they would like to review that along with the other bits, I would be happy to include that in the PR as well.", "in_reply_to_id": 181868931}