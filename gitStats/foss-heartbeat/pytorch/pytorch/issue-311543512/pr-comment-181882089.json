{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181882089", "pull_request_review_id": 112576648, "id": 181882089, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTg4MjA4OQ==", "diff_hunk": "@@ -1,9 +1,202 @@\n #include \"ATen/ATen.h\"\n #include \"ATen/NativeFunctions.h\"\n+#include \"ATen/WrapDimUtilsMulti.h\"\n \n \n namespace at { namespace native {\n \n+Tensor sumproduct_pair(const Tensor& left_, const Tensor& right_, IntList sum_dims_, bool keepdim) {\n+  // assumes that tensors have been pre-unsqueezed\n+  AT_ASSERT(left_.dim()==right_.dim(), \"number of dimensions must match\");\n+  if (sum_dims_.size() == 0)\n+    return at::mul(left_, right_);\n+  int64_t dim = left_.dim();\n+  auto sum_dims = dim_list_to_vector(sum_dims_, dim);\n+  std::vector<int64_t> lro, lo, ro;\n+  int64_t lro_size = 1, lo_size = 1, ro_size = 1, sum_size = 1;\n+  Tensor left = left_;\n+  Tensor right = right_;\n+  for (int64_t i = 0; i < dim; i++) {\n+    auto sl = left.size(i)>1;\n+    auto sr = right.size(i)>1;\n+    if (sum_dims[i]) {\n+      if (sl && sr) {\n+\tAT_ASSERT(left.size(i)==right.size(i), \"sum indexes must match\");\n+\tsum_size *= left.size(i);\n+      } else if (sl) {\n+\tleft = left.sum(i, true);\n+      } else if (sr) {\n+\tright = right.sum(i, true);\n+      }\n+    } else if (sl && sr) {\n+      AT_ASSERT(left.size(i)==right.size(i), \"non-broadcast dimensions must match\");\n+      lro.push_back(i);\n+      lro_size *= left.size(i);\n+    } else if (sl) {\n+      lo.push_back(i);\n+      lo_size *= left.size(i);\n+    } else {\n+      ro.push_back(i);\n+      ro_size *= right.size(i);\n+    }\n+  }\n+  std::vector<int64_t> out_size;\n+  for (auto& d : lro) out_size.push_back(left.size(d));\n+  for (auto& d : lo) out_size.push_back(left.size(d));\n+  for (auto& d : sum_dims_) { out_size.push_back(1); (void)(d); }; // avoid warining about not using d\n+  for (auto& d : ro) out_size.push_back(right.size(d));\n+\n+  std::vector<int64_t> lpermutation(lro);\n+  lpermutation.insert(lpermutation.end(), lo.begin(), lo.end());\n+  lpermutation.insert(lpermutation.end(), sum_dims_.begin(), sum_dims_.end());\n+  lpermutation.insert(lpermutation.end(), ro.begin(), ro.end());\n+\n+  std::vector<int64_t> rpermutation(lro);\n+  rpermutation.insert(rpermutation.end(), sum_dims_.begin(), sum_dims_.end());\n+  rpermutation.insert(rpermutation.end(), ro.begin(), ro.end());\n+  rpermutation.insert(rpermutation.end(), lo.begin(), lo.end());\n+\n+  std::vector<int64_t> opermutation(lro.size()+lo.size()+sum_dims_.size()+ro.size(), -1);\n+  {\n+  int64_t i = 0;\n+\n+  for (auto it = lro.begin(); it != lro.end(); i++, it++) {\n+    opermutation[*it] = i;\n+  }\n+  for (auto it = lo.begin(); it != lo.end(); i++, it++) {\n+    opermutation[*it] = i;\n+  }\n+  for (auto it = sum_dims_.begin(); it != sum_dims_.end(); i++, it++) {\n+    opermutation[*it] = i;\n+  }\n+  for (auto it = ro.begin(); it != ro.end(); i++, it++) {\n+    opermutation[*it] = i;\n+  }\n+  }\n+\n+  left = left.permute(lpermutation).reshape({lro_size, lo_size, sum_size});\n+  right = right.permute(rpermutation).reshape({lro_size, sum_size, ro_size});\n+  Tensor result = at::bmm(left, right);\n+  result = result.view(out_size).permute(opermutation);\n+  if (! keepdim) {\n+    for (int i = dim-1; i>=0; i--)\n+      if (sum_dims[i])\n+\tresult.squeeze_(i);\n+  }\n+  return result;\n+}\n+\n+\n+Tensor einsum(String eqn, TensorList tensors) {\n+  std::string in_eqn;\n+  size_t pos;\n+  std::vector<std::int64_t> number_of_occurences(26, 0);\n+  std::vector<std::int64_t> last_occurence(26, -1);\n+  std::vector<std::int64_t> sorted_position(26, -1);\n+  if ((pos = eqn.find(\"->\")) != std::string::npos) {\n+    in_eqn = eqn.substr(0, pos);\n+  } else {\n+    in_eqn = eqn;\n+  }\n+\n+  int64_t operand = 0;\n+  std::stringstream eqn_stream(in_eqn);\n+  std::string term;\n+  while (! eqn_stream.eof()) {\n+    std::getline(eqn_stream, term, ',');\n+    int64_t dims_in_operand = 0;\n+    for (auto &c : term) {\n+      AT_ASSERT(('a' <= c) && (c <= 'z'), \"only lowercase letters a-z allowed as indices\");\n+      int64_t index_num = c-'a';\n+      number_of_occurences[index_num]++;\n+      AT_ASSERT(last_occurence[index_num] < operand, \"diagonals (multiple occurences of the same index for one tensor) not implemented yet\")\n+      last_occurence[index_num] = operand;\n+      dims_in_operand++;\n+    }\n+    AT_ASSERT((int64_t) tensors.size()>operand, \"more operands in equation than tensors\");\n+    AT_ASSERT(dims_in_operand == tensors[operand].dim(), \"dimension mismatch for operand %zd: equation %zd, tensor %zd\", operand, dims_in_operand, tensors[operand].dim());\n+    operand++;\n+  }\n+  AT_ASSERT((int64_t) tensors.size()==operand, \"more tensors than operands in equation\");\n+\n+  int64_t num_outputs = 0;\n+  std::vector<int64_t> position_labels;\n+  if (pos != std::string::npos) {\n+    for (auto &c : eqn.substr(pos+2)) {\n+      AT_ASSERT(('a' <= c) && (c <= 'z'), \"only lowercase letters a-z allowed as indices\");\n+      int64_t index_num = c-'a';\n+      AT_ASSERT(sorted_position[index_num] == -1, \"index %c occurs twice in output\", c);\n+      sorted_position[index_num] = num_outputs;\n+      position_labels.push_back(index_num);\n+      num_outputs++;\n+    }\n+  } else {\n+    for (size_t idx = 0; idx < 26; idx++) {\n+      if (number_of_occurences[idx] == 1) {\n+\tsorted_position[idx] = num_outputs;\n+\tposition_labels.push_back(idx);\n+\tnum_outputs++;\n+      }\n+    }\n+  }\n+  int64_t position = num_outputs;\n+  for (int64_t idx = 0; idx < 26; idx++) {\n+    if ((number_of_occurences[idx] > 0) && (sorted_position[idx]==-1)) {\n+      sorted_position[idx] = position;\n+      position_labels.push_back(idx);\n+      position++;\n+    }\n+  }\n+  std::vector<Tensor> permuted_ops;\n+  eqn_stream.clear();\n+  eqn_stream.seekg(0, std::ios_base::beg);\n+  for (int64_t op = 0; op < (int64_t) tensors.size(); op++) {\n+    std::vector<int64_t> axes(26, -1);\n+    std::vector<int64_t> permutation;\n+    std::getline(eqn_stream, term, ',');\n+    int64_t dim = 0;\n+    for (auto &c : term) {\n+      int64_t index_num = c-'a';\n+      axes[index_num] = dim;\n+      dim++;\n+    }\n+    for (auto &c : position_labels) {\n+      if (axes[c] > -1) {\n+\tpermutation.push_back(axes[c]);\n+      }\n+    }\n+    permuted_ops.push_back(tensors[op].permute(permutation));\n+    for (int64_t dim = 0; dim < (int64_t) position_labels.size(); dim++) {\n+      auto c = position_labels[dim];\n+      if (axes[c] == -1) {\n+\tpermuted_ops.back().unsqueeze_(dim);\n+      }\n+    }\n+  }\n+  // we just go left to right. numpy allows to optimize the path...\n+  Tensor result = permuted_ops[0];\n+  for (int64_t idx = 0; idx < 26; idx++) {\n+    if ((last_occurence[idx] == 0)\n+\t&& (sorted_position[idx]>=num_outputs)) {\n+      result = result.sum(sorted_position[idx], true);", "path": "aten/src/ATen/native/Linear.cpp", "position": null, "original_position": 181, "commit_id": "daf3c25ee5e9a1a8fe038ddc416761646410b199", "original_commit_id": "d7c7ebe6d0a6513ea95fb356abe751ca4893b8ac", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Yep, let's wait until the multi-dim sum gets in, and use it there!", "created_at": "2018-04-16T20:55:27Z", "updated_at": "2018-11-23T15:42:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/6307#discussion_r181882089", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6307", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181882089"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6307#discussion_r181882089"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6307"}}, "body_html": "<p>Yep, let's wait until the multi-dim sum gets in, and use it there!</p>", "body_text": "Yep, let's wait until the multi-dim sum gets in, and use it there!", "in_reply_to_id": 181867667}