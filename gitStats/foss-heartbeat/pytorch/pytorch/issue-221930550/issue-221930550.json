{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1266", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1266/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1266/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1266/events", "html_url": "https://github.com/pytorch/pytorch/issues/1266", "id": 221930550, "node_id": "MDU6SXNzdWUyMjE5MzA1NTA=", "number": 1266, "title": "It is kind of strict to require every param to be `requires_grad=True`", "user": {"login": "chenzhekl", "id": 1201055, "node_id": "MDQ6VXNlcjEyMDEwNTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/1201055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenzhekl", "html_url": "https://github.com/chenzhekl", "followers_url": "https://api.github.com/users/chenzhekl/followers", "following_url": "https://api.github.com/users/chenzhekl/following{/other_user}", "gists_url": "https://api.github.com/users/chenzhekl/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenzhekl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenzhekl/subscriptions", "organizations_url": "https://api.github.com/users/chenzhekl/orgs", "repos_url": "https://api.github.com/users/chenzhekl/repos", "events_url": "https://api.github.com/users/chenzhekl/events{/privacy}", "received_events_url": "https://api.github.com/users/chenzhekl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-04-15T06:28:27Z", "updated_at": "2017-04-15T17:07:02Z", "closed_at": "2017-04-15T17:07:02Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I find that optimizer enforces every parameter to be <code>requires_grad=True</code>. However, if I used such a network where the first layer was a pre-trained <code>AlexNet</code> with every parameter set to <code>requires_grad = False</code>, and rest layers were left <code>requires_grad = True</code>, pytorch would complain <code>optimizing a parameter that doesn't require gradients</code>. Because in <code>model.parameters()</code>, some parameters do not require gradients.</p>\n<p>Is there anything wrong with my method of freezing part of the model? or would it be better to enforce that not all parameters are set <code>requires_grad=False</code> .</p>", "body_text": "Hi,\nI find that optimizer enforces every parameter to be requires_grad=True. However, if I used such a network where the first layer was a pre-trained AlexNet with every parameter set to requires_grad = False, and rest layers were left requires_grad = True, pytorch would complain optimizing a parameter that doesn't require gradients. Because in model.parameters(), some parameters do not require gradients.\nIs there anything wrong with my method of freezing part of the model? or would it be better to enforce that not all parameters are set requires_grad=False .", "body": "Hi,\r\n\r\nI find that optimizer enforces every parameter to be `requires_grad=True`. However, if I used such a network where the first layer was a pre-trained `AlexNet` with every parameter set to `requires_grad = False`, and rest layers were left `requires_grad = True`, pytorch would complain `optimizing a parameter that doesn't require gradients`. Because in `model.parameters()`, some parameters do not require gradients.\r\n\r\nIs there anything wrong with my method of freezing part of the model? or would it be better to enforce that not all parameters are set `requires_grad=False` ."}