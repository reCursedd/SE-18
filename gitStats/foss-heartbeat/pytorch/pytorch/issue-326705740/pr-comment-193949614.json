{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193949614", "pull_request_review_id": 127026150, "id": 193949614, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5Mzk0OTYxNA==", "diff_hunk": "@@ -0,0 +1,135 @@\n+#pragma once\n+\n+#include <ATen/Device.h>\n+#include <ATen/Layout.h>\n+#include <ATen/ScalarType.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+\n+namespace at {\n+struct Type;\n+struct Tensor;\n+} // namespace at\n+\n+namespace at {\n+\n+/// A class to encapsulate construction axes of a `Tensor`.\n+/// `TensorOptions` is a virtual class to enable overriding of certain methods\n+/// by subclasses in other libraries, such as PyTorch. In PyTorch, there is a\n+/// `torch::TensorOptions` subclass of this `TensorOptions`, which changes\n+/// `type()` to return a variable type instead of a tensor type, such that\n+/// variables are created inside factory methods, instead of tensors.\n+/// Furthermore, it changes `apply` to perform certain post-creation steps, such\n+/// as setting the `requires_grad` property of a `Variable`.\n+struct TensorOptions {\n+  /// Constructs the `TensorOptions` with valid defaults, which are:\n+  /// - dtype: float\n+  /// - device: CPU\n+  /// - layout: strided\n+  TensorOptions() = default;\n+\n+  /// Constructs the `TensorOptions` from the type of the given `Tensor`.\n+  /// If the `Tensor` has a CUDA type, the `device_index` will match that of the\n+  /// tensor. See the constructor from `Type` for the semantics w.r.t. the\n+  /// `type()` method.\n+  explicit TensorOptions(Tensor tensor);\n+\n+  /// Constructs the `TensorOptions` from a type and optional `device_index`.\n+  ///\n+  /// NOTE: This changes the behavior of `TensorOptions::type()` in that it will\n+  /// always return this `type`, irrespective of any `device` or `dtype` or\n+  /// `layout` specified at a later time. This is to ensure that when a\n+  /// `TensorOptions` object is constructed from a tensor's type, and that type\n+  /// has a dynamic type other than `at::Type` (e.g.\n+  /// `torch::autograd::VariableType`), constructing a new tensor from this\n+  /// `TensorOptions` will use this same derived type. If instead the given\n+  /// `type` were destructured into its components (backend, dtype and layout),\n+  /// information about the runtime type of the `Type` would be lost.\n+  /* implicit */ TensorOptions(\n+      const Type& type,\n+      optional<int32_t> device_index = nullopt);\n+\n+  /// Constructs a `TensorOptions` object with the given layout.\n+  /* implicit */ TensorOptions(Layout layout) : TensorOptions() {\n+    this->layout(layout);\n+  }\n+\n+  /// Constructs a `TensorOptions` object with the given device.\n+  /* implicit */ TensorOptions(Device device) : TensorOptions() {\n+    this->device(device);\n+  }\n+\n+  /// Constructs a `TensorOptions` object with the given dtype.\n+  /* implicit */ TensorOptions(ScalarType dtype) : TensorOptions() {\n+    this->dtype(dtype);\n+  }\n+\n+  virtual ~TensorOptions() = default;\n+\n+  // NOTE: These methods are defined in TensorOptions.cpp because I get funny\n+  // linker errors for their missing definition if they're defined in the\n+  // header. Who knows why?\n+\n+  /// Sets the device of the `TensorOptions`.\n+  virtual TensorOptions& device(Device device);", "path": "aten/src/ATen/TensorOptions.h", "position": null, "original_position": 75, "commit_id": "c5b2af6a950b67445ad8916d364fd24456d7aa39", "original_commit_id": "c760d1cdac573d758f59013d06d1ae773a91bd26", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Why are these one liners virtual? I see the linker error comments, but introducing jumps for every one of these functions is going to be bad. Most of the time these are able to be constant propagated into a full TensorOptions argument, but this prevents all of that from happening. This will make tensor allocation, already a source of overhead from small tensors, even worse. It is worth sorting out the linker errors -- we have equivalent things in other files.", "created_at": "2018-06-08T04:22:40Z", "updated_at": "2018-11-23T15:45:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/7869#discussion_r193949614", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7869", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193949614"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7869#discussion_r193949614"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7869"}}, "body_html": "<p>Why are these one liners virtual? I see the linker error comments, but introducing jumps for every one of these functions is going to be bad. Most of the time these are able to be constant propagated into a full TensorOptions argument, but this prevents all of that from happening. This will make tensor allocation, already a source of overhead from small tensors, even worse. It is worth sorting out the linker errors -- we have equivalent things in other files.</p>", "body_text": "Why are these one liners virtual? I see the linker error comments, but introducing jumps for every one of these functions is going to be bad. Most of the time these are able to be constant propagated into a full TensorOptions argument, but this prevents all of that from happening. This will make tensor allocation, already a source of overhead from small tensors, even worse. It is worth sorting out the linker errors -- we have equivalent things in other files."}