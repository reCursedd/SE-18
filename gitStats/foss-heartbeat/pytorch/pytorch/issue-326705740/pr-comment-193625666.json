{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193625666", "pull_request_review_id": 126633269, "id": 193625666, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MzYyNTY2Ng==", "diff_hunk": "@@ -0,0 +1,96 @@\n+#pragma once\n+\n+#include <ATen/Error.h>\n+#include <ATen/ScalarType.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/Type.h>\n+#include <ATen/detail/CUDAHooksInterface.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+\n+namespace at {\n+/// RAII guard that sets the CUDA device index in its constructor, and changes\n+/// it back to the index that was originally active upon destruction.\n+///\n+/// The index is always reset to the one that was active at the time of\n+/// construction of the guard. Even if you `set_index` after construction, the\n+/// destructor will still reset the index to the one that was active at\n+/// construction time.\n+///\n+/// The index is represented by an `optional<int32_t>`. Both `nullopt` and `-1`\n+/// represent the default device. `nullopt` should be preferred, support for\n+/// `-1` is kept for legacy reasons.\n+struct AutoGPU {", "path": "aten/src/ATen/AutoGPU.h", "position": null, "original_position": 24, "commit_id": "c5b2af6a950b67445ad8916d364fd24456d7aa39", "original_commit_id": "c760d1cdac573d758f59013d06d1ae773a91bd26", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "Note for C2 reference - this is basically equivalent of SwitchToDevice (https://github.com/pytorch/pytorch/blob/master/caffe2/core/context.h#L80)\r\n\r\nDepending on how generated wrappers evolve there might be a value in abstracting it out as \"general device abstraction\" (and empty CPU implementation of \"switch to device\"). Then it'd work nicely if we need to add HIP or something even more weird.\r\n\r\nDo you think it makes sense to make it something like DeviceGuard in this diff or later?\r\n\r\nWe are also introducing StaticContext (or shall it be called DeviceContext) which is basically an singleton for all device-level stuff. So this thing can live eventually in DeviceContext::Guard(Device) or something like this.", "created_at": "2018-06-07T04:55:25Z", "updated_at": "2018-11-23T15:45:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/7869#discussion_r193625666", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7869", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193625666"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7869#discussion_r193625666"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7869"}}, "body_html": "<p>Note for C2 reference - this is basically equivalent of SwitchToDevice (<a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/core/context.h#L80\">https://github.com/pytorch/pytorch/blob/master/caffe2/core/context.h#L80</a>)</p>\n<p>Depending on how generated wrappers evolve there might be a value in abstracting it out as \"general device abstraction\" (and empty CPU implementation of \"switch to device\"). Then it'd work nicely if we need to add HIP or something even more weird.</p>\n<p>Do you think it makes sense to make it something like DeviceGuard in this diff or later?</p>\n<p>We are also introducing StaticContext (or shall it be called DeviceContext) which is basically an singleton for all device-level stuff. So this thing can live eventually in DeviceContext::Guard(Device) or something like this.</p>", "body_text": "Note for C2 reference - this is basically equivalent of SwitchToDevice (https://github.com/pytorch/pytorch/blob/master/caffe2/core/context.h#L80)\nDepending on how generated wrappers evolve there might be a value in abstracting it out as \"general device abstraction\" (and empty CPU implementation of \"switch to device\"). Then it'd work nicely if we need to add HIP or something even more weird.\nDo you think it makes sense to make it something like DeviceGuard in this diff or later?\nWe are also introducing StaticContext (or shall it be called DeviceContext) which is basically an singleton for all device-level stuff. So this thing can live eventually in DeviceContext::Guard(Device) or something like this."}