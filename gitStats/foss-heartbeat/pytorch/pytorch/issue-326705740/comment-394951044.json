{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/394951044", "html_url": "https://github.com/pytorch/pytorch/pull/7869#issuecomment-394951044", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7869", "id": 394951044, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDk1MTA0NA==", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-06T06:07:40Z", "updated_at": "2018-06-06T06:12:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hello <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>\n<p>This PR is now ready, and it is fat. Tests all passed locally (though probably issues will arise in CI), and all major functionality is implemented.</p>\n<p>Unfortunately, this PR is a mix of important changes, and codemods to update all uses of factory methods to the new API. Therefore, here is a rough list of changes I have made. Please start by reviewing these files, then maybe glance over all the other files. You may also want to review individual commits if that helps.</p>\n<p><strong>Classes I have created</strong>:</p>\n<ul>\n<li><code>at::Device</code> in <code>aten/src/ATen/Device.{h,cpp}</code></li>\n<li><code>at::Layout</code> in <code>aten/src/ATen/Layout.{h,cpp}</code></li>\n<li><code>at::AutoGPU</code> in <code>aten/src/ATen/AutoGPU.{h,cpp}</code></li>\n<li><code>at::TensorOptions</code> in <code>aten/src/ATen/TensorOptions.{h,cpp}</code> and <code>torch/csrc/TensorOptions.h</code></li>\n<li><code>at::detail::DynamicCUDAInterface</code> in <code>aten/src/ATen/detail/CUDAHooksInterface.h</code> and <code>aten/src/ATen/detail/CUDAHooks.cpp</code></li>\n</ul>\n<p><strong>Important files I have modified</strong>:</p>\n<ul>\n<li><code>aten/src/ATen/native/TensorFactories.cpp</code></li>\n<li><code>aten/src/ATen/native/native_functions.yaml</code></li>\n<li><code>tools/autograd/gen_python_functions.py</code></li>\n<li><code>aten/src/ATen/function_wrapper.py</code></li>\n<li><code>tools/autograd/gen_variable_type.py</code></li>\n<li><code>tools/jit/gen_jit_dispatch.py</code></li>\n<li><code>torch/csrc/utils/tensor_new.cpp</code></li>\n</ul>\n<p><strong>Some notes</strong>:</p>\n<p>Factory functions (like <code>at::ones</code>) used to take the type as their first argument and then the size second. This is now changed that the size list is always first, and TensorOptions follow. This is like to PyTorch.</p>\n<p>To allow default arguments, while also have the <code>TensorOptions</code> always be the last argument, I have changed all factory methods that took default arguments to follow this kind of scheme, e.g. for a function that used to be <code>func(arg1, arg2, default_arg1=..., default_arg2=...)</code>:</p>\n<ol>\n<li><code>func(arg1, arg2, options)</code></li>\n<li><code>func(arg1, arg2, default_arg1, options)</code></li>\n<li><code>func(arg1, arg2, default_arg1, default_arg2, options)</code><br>\netc. This is a small hassle to implement in <code>TensorFactories.cpp</code>, but leads to a nice, uniform API.</li>\n</ol>\n<p>AutoGPU has now moved entirely into ATen, and is placed inside e.g. <code>at::ones</code> before it calls into <code>at::native::ones</code>.</p>\n<p>I have opted for <code>at::optional&lt;int32_t&gt;</code> as the device index type instead of <code>int32_t</code>, such that the default device is represented by an empty optional instead of <code>-1</code>. This is more principled and makes for nicer and safer (!) code. To not disturb other codepaths too much, I made AutoGPU nevertheless respect -1 as the default device.</p>\n<p><strong>TODO in follow-up PRs</strong>:</p>\n<ul>\n<li>A mechanism to default tensor options thread locally, like in PyTorch (e.g. <code>with torch.cuda.device()</code>). See <code>test/cpp/api/tensor.cpp</code> for notes on this.</li>\n</ul>", "body_text": "Hello @zdevito @colesbury @gchanan @apaszke @ezyang\nThis PR is now ready, and it is fat. Tests all passed locally (though probably issues will arise in CI), and all major functionality is implemented.\nUnfortunately, this PR is a mix of important changes, and codemods to update all uses of factory methods to the new API. Therefore, here is a rough list of changes I have made. Please start by reviewing these files, then maybe glance over all the other files. You may also want to review individual commits if that helps.\nClasses I have created:\n\nat::Device in aten/src/ATen/Device.{h,cpp}\nat::Layout in aten/src/ATen/Layout.{h,cpp}\nat::AutoGPU in aten/src/ATen/AutoGPU.{h,cpp}\nat::TensorOptions in aten/src/ATen/TensorOptions.{h,cpp} and torch/csrc/TensorOptions.h\nat::detail::DynamicCUDAInterface in aten/src/ATen/detail/CUDAHooksInterface.h and aten/src/ATen/detail/CUDAHooks.cpp\n\nImportant files I have modified:\n\naten/src/ATen/native/TensorFactories.cpp\naten/src/ATen/native/native_functions.yaml\ntools/autograd/gen_python_functions.py\naten/src/ATen/function_wrapper.py\ntools/autograd/gen_variable_type.py\ntools/jit/gen_jit_dispatch.py\ntorch/csrc/utils/tensor_new.cpp\n\nSome notes:\nFactory functions (like at::ones) used to take the type as their first argument and then the size second. This is now changed that the size list is always first, and TensorOptions follow. This is like to PyTorch.\nTo allow default arguments, while also have the TensorOptions always be the last argument, I have changed all factory methods that took default arguments to follow this kind of scheme, e.g. for a function that used to be func(arg1, arg2, default_arg1=..., default_arg2=...):\n\nfunc(arg1, arg2, options)\nfunc(arg1, arg2, default_arg1, options)\nfunc(arg1, arg2, default_arg1, default_arg2, options)\netc. This is a small hassle to implement in TensorFactories.cpp, but leads to a nice, uniform API.\n\nAutoGPU has now moved entirely into ATen, and is placed inside e.g. at::ones before it calls into at::native::ones.\nI have opted for at::optional<int32_t> as the device index type instead of int32_t, such that the default device is represented by an empty optional instead of -1. This is more principled and makes for nicer and safer (!) code. To not disturb other codepaths too much, I made AutoGPU nevertheless respect -1 as the default device.\nTODO in follow-up PRs:\n\nA mechanism to default tensor options thread locally, like in PyTorch (e.g. with torch.cuda.device()). See test/cpp/api/tensor.cpp for notes on this.", "body": "Hello @zdevito @colesbury @gchanan @apaszke @ezyang\r\n\r\nThis PR is now ready, and it is fat. Tests all passed locally (though probably issues will arise in CI), and all major functionality is implemented.\r\n\r\nUnfortunately, this PR is a mix of important changes, and codemods to update all uses of factory methods to the new API. Therefore, here is a rough list of changes I have made. Please start by reviewing these files, then maybe glance over all the other files. You may also want to review individual commits if that helps.\r\n\r\n__Classes I have created__:\r\n- `at::Device` in `aten/src/ATen/Device.{h,cpp}`\r\n- `at::Layout` in `aten/src/ATen/Layout.{h,cpp}`\r\n- `at::AutoGPU` in `aten/src/ATen/AutoGPU.{h,cpp}`\r\n- `at::TensorOptions` in `aten/src/ATen/TensorOptions.{h,cpp}` and `torch/csrc/TensorOptions.h`\r\n- `at::detail::DynamicCUDAInterface` in `aten/src/ATen/detail/CUDAHooksInterface.h` and `aten/src/ATen/detail/CUDAHooks.cpp`\r\n\r\n__Important files I have modified__:\r\n- `aten/src/ATen/native/TensorFactories.cpp`\r\n- `aten/src/ATen/native/native_functions.yaml`\r\n- `tools/autograd/gen_python_functions.py`\r\n- `aten/src/ATen/function_wrapper.py`\r\n- `tools/autograd/gen_variable_type.py`\r\n- `tools/jit/gen_jit_dispatch.py`\r\n- `torch/csrc/utils/tensor_new.cpp`\r\n\r\n__Some notes__:\r\n\r\nFactory functions (like `at::ones`) used to take the type as their first argument and then the size second. This is now changed that the size list is always first, and TensorOptions follow. This is like to PyTorch.\r\n\r\nTo allow default arguments, while also have the `TensorOptions` always be the last argument, I have changed all factory methods that took default arguments to follow this kind of scheme, e.g. for a function that used to be `func(arg1, arg2, default_arg1=..., default_arg2=...)`:\r\n1. `func(arg1, arg2, options)`\r\n2. `func(arg1, arg2, default_arg1, options)`\r\n3. `func(arg1, arg2, default_arg1, default_arg2, options)`\r\netc. This is a small hassle to implement in `TensorFactories.cpp`, but leads to a nice, uniform API.\r\n\r\nAutoGPU has now moved entirely into ATen, and is placed inside e.g. `at::ones` before it calls into `at::native::ones`.\r\n\r\nI have opted for `at::optional<int32_t>` as the device index type instead of `int32_t`, such that the default device is represented by an empty optional instead of `-1`. This is more principled and makes for nicer and safer (!) code. To not disturb other codepaths too much, I made AutoGPU nevertheless respect -1 as the default device.\r\n\r\n__TODO in follow-up PRs__:\r\n- A mechanism to default tensor options thread locally, like in PyTorch (e.g. `with torch.cuda.device()`). See `test/cpp/api/tensor.cpp` for notes on this."}