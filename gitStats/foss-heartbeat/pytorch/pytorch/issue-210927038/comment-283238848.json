{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/283238848", "html_url": "https://github.com/pytorch/pytorch/issues/877#issuecomment-283238848", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/877", "id": 283238848, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzIzODg0OA==", "user": {"login": "chahuja", "id": 6669653, "node_id": "MDQ6VXNlcjY2Njk2NTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6669653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chahuja", "html_url": "https://github.com/chahuja", "followers_url": "https://api.github.com/users/chahuja/followers", "following_url": "https://api.github.com/users/chahuja/following{/other_user}", "gists_url": "https://api.github.com/users/chahuja/gists{/gist_id}", "starred_url": "https://api.github.com/users/chahuja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chahuja/subscriptions", "organizations_url": "https://api.github.com/users/chahuja/orgs", "repos_url": "https://api.github.com/users/chahuja/repos", "events_url": "https://api.github.com/users/chahuja/events{/privacy}", "received_events_url": "https://api.github.com/users/chahuja/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-01T04:03:06Z", "updated_at": "2017-03-01T04:03:06Z", "author_association": "NONE", "body_html": "<p>Following is the script I use. You would need a text file(any text file with a lot of characters will do) of the name <code>input.txt</code> in a directory named <code>dataset</code>. Check the variable <code>data_file </code>.</p>\n<p>Currently the variable <code>num_layers</code> is set to 10. This causes my system to crash.</p>\n<p>I use the conda version of pytorch and  TitanX (12GB) for running this code.</p>\n<pre><code>import numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport pdb\nfrom tqdm import tqdm\n\n# Data Loader\nclass BatchGenerator(object):\n  def __init__(self, text, batch_size, num_unrolling, vocabulary_size, char2id):\n    self._text = text\n    self._batch_size = batch_size\n    self._num_unrolling = num_unrolling\n    self._vocabulary_size = vocabulary_size\n    self._char2id = char2id\n\n  def __call__(self):\n    ## cut extra data at the end\n    text_len = (len(self._text)//self._batch_size) * self._batch_size\n    text_len = (((text_len/self._batch_size -1) // self._num_unrolling) * self._num_unrolling + 1) * self._batch_size\n    self._text = self._text[:text_len]\n\n    ## create batches\n    batches = list()\n    for step in range(self._num_unrolling + 1):\n      segment = text_len/self._batch_size\n      arr_len = text_len/self._num_unrolling\n      arr = np.zeros((arr_len, self._vocabulary_size))\n      sample_num = 0\n      for c in range(step,segment-1, self._num_unrolling):\n        for b in range(self._batch_size):\n          arr[sample_num, self._char2id(self._text[b*segment+c])] = 1\n          sample_num+=1\n      batches.append(arr)\n\n    return batches\n\nclass Data(object):\n  def __init__(self, filename, valid_frac=0.1, batch_size=100, num_unrolling=10, criterion='char', delimiter=None):\n    '''\n    Data handler for character/word based models.\n    criterion: char/word based spliting\n    '''\n\n    self.unk_count = 0\n\n    if(delimiter==None):\n      if(criterion=='word'):\n        ## search for existing tokenised file\n        if(not os.path.isfile(filename + '.tokens')):\n          subprocess.call('./tokenizer.perl -l en -no-escape' + ' &lt; ' + filename + ' &gt; ' + filename+'.tokens', shell=True)\n\n        filename += '.tokens'\n        delimiter = ' '\n      elif(criterion=='char'):\n        delimiter = ''\n      else:\n        assert (0), 'Incorrect criterion. Choose from char or word'\n    \n    ## read text from a given file\n    text = self.read_data(filename)\n    if(criterion=='word'):\n      ## tokenzer.perl does not put spaces between escape sequences\n      text = text.replace('\\n',' \\n ').replace('\\n  \\n','\\n \\n') ## hardcoded for shakespere dataset\n\n    if(delimiter == ''):\n      text = list(text)\n    else:\n      text=text.split(delimiter)\n\n    self.delimiter = delimiter\n      \n    ## split text into train and test\n    train_text, valid_text, test_text = self.split_data(text, valid_frac)\n\n    ## create vocabulary based on the data\n    self.vocab, self.rev_vocab, self.vocabulary_size = self.create_vocab(train_text)\n\n    train = BatchGenerator(train_text, batch_size, num_unrolling, self.vocabulary_size, self.char2id)\n    val = BatchGenerator(valid_text, batch_size, 1, self.vocabulary_size, self.char2id)\n    test = BatchGenerator(test_text, batch_size, 1, self.vocabulary_size, self.char2id)\n    self.train_batches = train()\n    self.val_batches = val()\n    print('Unknown Tokens in val-set: %d') %(self.unk_count)\n    self.unk_count = 0\n    self.test_batches = test()\n    print('Unknown Tokens in test-set: %d') %(self.unk_count)\n    \n  ## Read file as a single string\n  def read_data(self, filename):\n    f = open(filename, 'rb')\n    data=f.read()\n    f.close()\n    return data\n\n  ## Split given string into train and test\n  def split_data(self, text, valid_frac):\n    \"\"\"Split the given data into train and validation based on\n    the valid_frac\"\"\"\n    valid_size = int(np.ceil(len(text)*valid_frac))\n    test_size = int(np.ceil(len(text)*valid_frac*3)) ## hardcoded to 3 times valid frac\n    valid_text = text[-valid_size:]\n    test_text = text[-valid_size-test_size:-valid_size]\n    train_text = text[:-valid_size-test_size]\n    train_size = len(train_text)\n    print(train_size, train_text[:64])\n    print(valid_size, valid_text[:64])\n    print(test_size, test_text[:64])\n    return train_text, valid_text, test_text\n\n  ## Given a string find a character/word based vocabulary\n  def create_vocab(self, text):\n    \"\"\"Create a vocabulary for a given text and return \n    vocab, rev_vocab and vocabulary size\n    \"\"\"\n    \n    vocabulary_size = 1\n    vocab = {'_UNK':0}\n    rev_vocab = ['_UNK']\n    for char in text:\n      if char not in vocab.keys():\n        vocab[char] = vocabulary_size\n        rev_vocab += [char]\n        vocabulary_size += 1\n    print('Vocabulary Size: %d') % (vocabulary_size)\n    return vocab, rev_vocab, vocabulary_size\n\n  def char2id(self,char):\n    \"\"\"Convert a character to its correspoding id in the dictionary\"\"\"\n    if char in self.vocab.keys():\n      return self.vocab[char]\n    else:\n      self.unk_count += 1\n      return 0\n\n  def id2char(self,dictid):\n    \"\"\"Convert id(int) to the corresponding character\"\"\"\n    if dictid &gt;=0 :\n      return self.rev_vocab[dictid]\n    else:\n      print ('Invalid ID: %d' % dictid)\n      return 0\n\n  def characters(self,probabilities):\n    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n    characters back into its (most likely) character representation.\"\"\"\n\n\n## Variables\ndata_file = './dataset/input.txt'\nnum_unrolling = 100\nbatch_size = 100\nnum_layers = 10\nrnn_size = 418\nNUM_EPOCHS = 100\n\ndata = Data(data_file, num_unrolling=num_unrolling, batch_size=batch_size)\n\ntrain = torch.Tensor(np.stack(np.asarray(data.train_batches, dtype=np.int), axis=1)[:,:-1,:])\ny_t = torch.LongTensor(np.argmax(np.stack(np.asarray(data.train_batches, dtype=np.int), axis=1)[:,1:,:], axis=2))\n\nclass seq2seq(torch.nn.Module):\n  def __init__(self, vocab_size, batch_size, hidden_state, num_layers):\n    super(seq2seq,self).__init__()\n    self.vocab_size = vocab_size\n    self.batch_size = batch_size\n    self.hidden_state = hidden_state\n    self.num_layers = num_layers\n    self.model = torch.nn.LSTM(input_size=vocab_size, hidden_size=hidden_state, num_layers=num_layers, batch_first=True)\n    self.predict = torch.nn.Linear(hidden_state,vocab_size)\n    #self.Wo = Variable(torch.FloatTensor(np.random.uniform(0.01,-0.01,size=(self.vocab_size, self.hidden_state))),requires_grad=True).cuda()\n    #self.bo = Variable(torch.FloatTensor(np.zeros((self.vocab_size))),requires_grad=True).cuda()\n    \n  def forward(self,x):\n    zeros = torch.autograd.Variable(torch.zeros(*(self.num_layers, self.batch_size, self.hidden_state)))\n    h, _ = self.model(x, None)\n    #pdb.set_trace()\n    #y = h.contiguous().view(-1,self.hidden_state).mm(self.Wo) + self.bo\n    #y = torch.nn.functional.linear(h.contiguous().view(-1,self.hidden_state),self.Wo,self.bo)\n    y = self.predict(h.contiguous().view(-1,self.hidden_state))\n    return y\n    \n\nmodel = seq2seq(vocab_size=train.size(2), batch_size=batch_size, hidden_state=rnn_size, num_layers=num_layers)\nmodel = model.cuda()\n\n\ncriterion = torch.nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\nindices = range(0,train.size(0)+1,batch_size)\nfor i in tqdm(range(NUM_EPOCHS)):\n  running_loss = 0.0\n  for i, (start,end) in enumerate(zip(indices[:-1],indices[1:])):\n    inputs, labels = torch.autograd.Variable(train[start:end,:,:]).cuda(), torch.autograd.Variable(y_t[start:end,:]).cuda()\n    \n    # zero the parameter gradients\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    \n    loss = criterion(outputs, labels.view(-1))\n    \n    loss.backward()        \n    running_loss += loss.data[0]\n    optimizer.step()\n  tqdm.write('Loss:%f'%(running_loss/(i+1)))\n\n</code></pre>", "body_text": "Following is the script I use. You would need a text file(any text file with a lot of characters will do) of the name input.txt in a directory named dataset. Check the variable data_file .\nCurrently the variable num_layers is set to 10. This causes my system to crash.\nI use the conda version of pytorch and  TitanX (12GB) for running this code.\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport pdb\nfrom tqdm import tqdm\n\n# Data Loader\nclass BatchGenerator(object):\n  def __init__(self, text, batch_size, num_unrolling, vocabulary_size, char2id):\n    self._text = text\n    self._batch_size = batch_size\n    self._num_unrolling = num_unrolling\n    self._vocabulary_size = vocabulary_size\n    self._char2id = char2id\n\n  def __call__(self):\n    ## cut extra data at the end\n    text_len = (len(self._text)//self._batch_size) * self._batch_size\n    text_len = (((text_len/self._batch_size -1) // self._num_unrolling) * self._num_unrolling + 1) * self._batch_size\n    self._text = self._text[:text_len]\n\n    ## create batches\n    batches = list()\n    for step in range(self._num_unrolling + 1):\n      segment = text_len/self._batch_size\n      arr_len = text_len/self._num_unrolling\n      arr = np.zeros((arr_len, self._vocabulary_size))\n      sample_num = 0\n      for c in range(step,segment-1, self._num_unrolling):\n        for b in range(self._batch_size):\n          arr[sample_num, self._char2id(self._text[b*segment+c])] = 1\n          sample_num+=1\n      batches.append(arr)\n\n    return batches\n\nclass Data(object):\n  def __init__(self, filename, valid_frac=0.1, batch_size=100, num_unrolling=10, criterion='char', delimiter=None):\n    '''\n    Data handler for character/word based models.\n    criterion: char/word based spliting\n    '''\n\n    self.unk_count = 0\n\n    if(delimiter==None):\n      if(criterion=='word'):\n        ## search for existing tokenised file\n        if(not os.path.isfile(filename + '.tokens')):\n          subprocess.call('./tokenizer.perl -l en -no-escape' + ' < ' + filename + ' > ' + filename+'.tokens', shell=True)\n\n        filename += '.tokens'\n        delimiter = ' '\n      elif(criterion=='char'):\n        delimiter = ''\n      else:\n        assert (0), 'Incorrect criterion. Choose from char or word'\n    \n    ## read text from a given file\n    text = self.read_data(filename)\n    if(criterion=='word'):\n      ## tokenzer.perl does not put spaces between escape sequences\n      text = text.replace('\\n',' \\n ').replace('\\n  \\n','\\n \\n') ## hardcoded for shakespere dataset\n\n    if(delimiter == ''):\n      text = list(text)\n    else:\n      text=text.split(delimiter)\n\n    self.delimiter = delimiter\n      \n    ## split text into train and test\n    train_text, valid_text, test_text = self.split_data(text, valid_frac)\n\n    ## create vocabulary based on the data\n    self.vocab, self.rev_vocab, self.vocabulary_size = self.create_vocab(train_text)\n\n    train = BatchGenerator(train_text, batch_size, num_unrolling, self.vocabulary_size, self.char2id)\n    val = BatchGenerator(valid_text, batch_size, 1, self.vocabulary_size, self.char2id)\n    test = BatchGenerator(test_text, batch_size, 1, self.vocabulary_size, self.char2id)\n    self.train_batches = train()\n    self.val_batches = val()\n    print('Unknown Tokens in val-set: %d') %(self.unk_count)\n    self.unk_count = 0\n    self.test_batches = test()\n    print('Unknown Tokens in test-set: %d') %(self.unk_count)\n    \n  ## Read file as a single string\n  def read_data(self, filename):\n    f = open(filename, 'rb')\n    data=f.read()\n    f.close()\n    return data\n\n  ## Split given string into train and test\n  def split_data(self, text, valid_frac):\n    \"\"\"Split the given data into train and validation based on\n    the valid_frac\"\"\"\n    valid_size = int(np.ceil(len(text)*valid_frac))\n    test_size = int(np.ceil(len(text)*valid_frac*3)) ## hardcoded to 3 times valid frac\n    valid_text = text[-valid_size:]\n    test_text = text[-valid_size-test_size:-valid_size]\n    train_text = text[:-valid_size-test_size]\n    train_size = len(train_text)\n    print(train_size, train_text[:64])\n    print(valid_size, valid_text[:64])\n    print(test_size, test_text[:64])\n    return train_text, valid_text, test_text\n\n  ## Given a string find a character/word based vocabulary\n  def create_vocab(self, text):\n    \"\"\"Create a vocabulary for a given text and return \n    vocab, rev_vocab and vocabulary size\n    \"\"\"\n    \n    vocabulary_size = 1\n    vocab = {'_UNK':0}\n    rev_vocab = ['_UNK']\n    for char in text:\n      if char not in vocab.keys():\n        vocab[char] = vocabulary_size\n        rev_vocab += [char]\n        vocabulary_size += 1\n    print('Vocabulary Size: %d') % (vocabulary_size)\n    return vocab, rev_vocab, vocabulary_size\n\n  def char2id(self,char):\n    \"\"\"Convert a character to its correspoding id in the dictionary\"\"\"\n    if char in self.vocab.keys():\n      return self.vocab[char]\n    else:\n      self.unk_count += 1\n      return 0\n\n  def id2char(self,dictid):\n    \"\"\"Convert id(int) to the corresponding character\"\"\"\n    if dictid >=0 :\n      return self.rev_vocab[dictid]\n    else:\n      print ('Invalid ID: %d' % dictid)\n      return 0\n\n  def characters(self,probabilities):\n    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n    characters back into its (most likely) character representation.\"\"\"\n\n\n## Variables\ndata_file = './dataset/input.txt'\nnum_unrolling = 100\nbatch_size = 100\nnum_layers = 10\nrnn_size = 418\nNUM_EPOCHS = 100\n\ndata = Data(data_file, num_unrolling=num_unrolling, batch_size=batch_size)\n\ntrain = torch.Tensor(np.stack(np.asarray(data.train_batches, dtype=np.int), axis=1)[:,:-1,:])\ny_t = torch.LongTensor(np.argmax(np.stack(np.asarray(data.train_batches, dtype=np.int), axis=1)[:,1:,:], axis=2))\n\nclass seq2seq(torch.nn.Module):\n  def __init__(self, vocab_size, batch_size, hidden_state, num_layers):\n    super(seq2seq,self).__init__()\n    self.vocab_size = vocab_size\n    self.batch_size = batch_size\n    self.hidden_state = hidden_state\n    self.num_layers = num_layers\n    self.model = torch.nn.LSTM(input_size=vocab_size, hidden_size=hidden_state, num_layers=num_layers, batch_first=True)\n    self.predict = torch.nn.Linear(hidden_state,vocab_size)\n    #self.Wo = Variable(torch.FloatTensor(np.random.uniform(0.01,-0.01,size=(self.vocab_size, self.hidden_state))),requires_grad=True).cuda()\n    #self.bo = Variable(torch.FloatTensor(np.zeros((self.vocab_size))),requires_grad=True).cuda()\n    \n  def forward(self,x):\n    zeros = torch.autograd.Variable(torch.zeros(*(self.num_layers, self.batch_size, self.hidden_state)))\n    h, _ = self.model(x, None)\n    #pdb.set_trace()\n    #y = h.contiguous().view(-1,self.hidden_state).mm(self.Wo) + self.bo\n    #y = torch.nn.functional.linear(h.contiguous().view(-1,self.hidden_state),self.Wo,self.bo)\n    y = self.predict(h.contiguous().view(-1,self.hidden_state))\n    return y\n    \n\nmodel = seq2seq(vocab_size=train.size(2), batch_size=batch_size, hidden_state=rnn_size, num_layers=num_layers)\nmodel = model.cuda()\n\n\ncriterion = torch.nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\nindices = range(0,train.size(0)+1,batch_size)\nfor i in tqdm(range(NUM_EPOCHS)):\n  running_loss = 0.0\n  for i, (start,end) in enumerate(zip(indices[:-1],indices[1:])):\n    inputs, labels = torch.autograd.Variable(train[start:end,:,:]).cuda(), torch.autograd.Variable(y_t[start:end,:]).cuda()\n    \n    # zero the parameter gradients\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    \n    loss = criterion(outputs, labels.view(-1))\n    \n    loss.backward()        \n    running_loss += loss.data[0]\n    optimizer.step()\n  tqdm.write('Loss:%f'%(running_loss/(i+1)))", "body": "Following is the script I use. You would need a text file(any text file with a lot of characters will do) of the name `input.txt` in a directory named `dataset`. Check the variable `data_file `.\r\n\r\nCurrently the variable `num_layers` is set to 10. This causes my system to crash. \r\n\r\nI use the conda version of pytorch and  TitanX (12GB) for running this code.\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport pdb\r\nfrom tqdm import tqdm\r\n\r\n# Data Loader\r\nclass BatchGenerator(object):\r\n  def __init__(self, text, batch_size, num_unrolling, vocabulary_size, char2id):\r\n    self._text = text\r\n    self._batch_size = batch_size\r\n    self._num_unrolling = num_unrolling\r\n    self._vocabulary_size = vocabulary_size\r\n    self._char2id = char2id\r\n\r\n  def __call__(self):\r\n    ## cut extra data at the end\r\n    text_len = (len(self._text)//self._batch_size) * self._batch_size\r\n    text_len = (((text_len/self._batch_size -1) // self._num_unrolling) * self._num_unrolling + 1) * self._batch_size\r\n    self._text = self._text[:text_len]\r\n\r\n    ## create batches\r\n    batches = list()\r\n    for step in range(self._num_unrolling + 1):\r\n      segment = text_len/self._batch_size\r\n      arr_len = text_len/self._num_unrolling\r\n      arr = np.zeros((arr_len, self._vocabulary_size))\r\n      sample_num = 0\r\n      for c in range(step,segment-1, self._num_unrolling):\r\n        for b in range(self._batch_size):\r\n          arr[sample_num, self._char2id(self._text[b*segment+c])] = 1\r\n          sample_num+=1\r\n      batches.append(arr)\r\n\r\n    return batches\r\n\r\nclass Data(object):\r\n  def __init__(self, filename, valid_frac=0.1, batch_size=100, num_unrolling=10, criterion='char', delimiter=None):\r\n    '''\r\n    Data handler for character/word based models.\r\n    criterion: char/word based spliting\r\n    '''\r\n\r\n    self.unk_count = 0\r\n\r\n    if(delimiter==None):\r\n      if(criterion=='word'):\r\n        ## search for existing tokenised file\r\n        if(not os.path.isfile(filename + '.tokens')):\r\n          subprocess.call('./tokenizer.perl -l en -no-escape' + ' < ' + filename + ' > ' + filename+'.tokens', shell=True)\r\n\r\n        filename += '.tokens'\r\n        delimiter = ' '\r\n      elif(criterion=='char'):\r\n        delimiter = ''\r\n      else:\r\n        assert (0), 'Incorrect criterion. Choose from char or word'\r\n    \r\n    ## read text from a given file\r\n    text = self.read_data(filename)\r\n    if(criterion=='word'):\r\n      ## tokenzer.perl does not put spaces between escape sequences\r\n      text = text.replace('\\n',' \\n ').replace('\\n  \\n','\\n \\n') ## hardcoded for shakespere dataset\r\n\r\n    if(delimiter == ''):\r\n      text = list(text)\r\n    else:\r\n      text=text.split(delimiter)\r\n\r\n    self.delimiter = delimiter\r\n      \r\n    ## split text into train and test\r\n    train_text, valid_text, test_text = self.split_data(text, valid_frac)\r\n\r\n    ## create vocabulary based on the data\r\n    self.vocab, self.rev_vocab, self.vocabulary_size = self.create_vocab(train_text)\r\n\r\n    train = BatchGenerator(train_text, batch_size, num_unrolling, self.vocabulary_size, self.char2id)\r\n    val = BatchGenerator(valid_text, batch_size, 1, self.vocabulary_size, self.char2id)\r\n    test = BatchGenerator(test_text, batch_size, 1, self.vocabulary_size, self.char2id)\r\n    self.train_batches = train()\r\n    self.val_batches = val()\r\n    print('Unknown Tokens in val-set: %d') %(self.unk_count)\r\n    self.unk_count = 0\r\n    self.test_batches = test()\r\n    print('Unknown Tokens in test-set: %d') %(self.unk_count)\r\n    \r\n  ## Read file as a single string\r\n  def read_data(self, filename):\r\n    f = open(filename, 'rb')\r\n    data=f.read()\r\n    f.close()\r\n    return data\r\n\r\n  ## Split given string into train and test\r\n  def split_data(self, text, valid_frac):\r\n    \"\"\"Split the given data into train and validation based on\r\n    the valid_frac\"\"\"\r\n    valid_size = int(np.ceil(len(text)*valid_frac))\r\n    test_size = int(np.ceil(len(text)*valid_frac*3)) ## hardcoded to 3 times valid frac\r\n    valid_text = text[-valid_size:]\r\n    test_text = text[-valid_size-test_size:-valid_size]\r\n    train_text = text[:-valid_size-test_size]\r\n    train_size = len(train_text)\r\n    print(train_size, train_text[:64])\r\n    print(valid_size, valid_text[:64])\r\n    print(test_size, test_text[:64])\r\n    return train_text, valid_text, test_text\r\n\r\n  ## Given a string find a character/word based vocabulary\r\n  def create_vocab(self, text):\r\n    \"\"\"Create a vocabulary for a given text and return \r\n    vocab, rev_vocab and vocabulary size\r\n    \"\"\"\r\n    \r\n    vocabulary_size = 1\r\n    vocab = {'_UNK':0}\r\n    rev_vocab = ['_UNK']\r\n    for char in text:\r\n      if char not in vocab.keys():\r\n        vocab[char] = vocabulary_size\r\n        rev_vocab += [char]\r\n        vocabulary_size += 1\r\n    print('Vocabulary Size: %d') % (vocabulary_size)\r\n    return vocab, rev_vocab, vocabulary_size\r\n\r\n  def char2id(self,char):\r\n    \"\"\"Convert a character to its correspoding id in the dictionary\"\"\"\r\n    if char in self.vocab.keys():\r\n      return self.vocab[char]\r\n    else:\r\n      self.unk_count += 1\r\n      return 0\r\n\r\n  def id2char(self,dictid):\r\n    \"\"\"Convert id(int) to the corresponding character\"\"\"\r\n    if dictid >=0 :\r\n      return self.rev_vocab[dictid]\r\n    else:\r\n      print ('Invalid ID: %d' % dictid)\r\n      return 0\r\n\r\n  def characters(self,probabilities):\r\n    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\r\n    characters back into its (most likely) character representation.\"\"\"\r\n\r\n\r\n## Variables\r\ndata_file = './dataset/input.txt'\r\nnum_unrolling = 100\r\nbatch_size = 100\r\nnum_layers = 10\r\nrnn_size = 418\r\nNUM_EPOCHS = 100\r\n\r\ndata = Data(data_file, num_unrolling=num_unrolling, batch_size=batch_size)\r\n\r\ntrain = torch.Tensor(np.stack(np.asarray(data.train_batches, dtype=np.int), axis=1)[:,:-1,:])\r\ny_t = torch.LongTensor(np.argmax(np.stack(np.asarray(data.train_batches, dtype=np.int), axis=1)[:,1:,:], axis=2))\r\n\r\nclass seq2seq(torch.nn.Module):\r\n  def __init__(self, vocab_size, batch_size, hidden_state, num_layers):\r\n    super(seq2seq,self).__init__()\r\n    self.vocab_size = vocab_size\r\n    self.batch_size = batch_size\r\n    self.hidden_state = hidden_state\r\n    self.num_layers = num_layers\r\n    self.model = torch.nn.LSTM(input_size=vocab_size, hidden_size=hidden_state, num_layers=num_layers, batch_first=True)\r\n    self.predict = torch.nn.Linear(hidden_state,vocab_size)\r\n    #self.Wo = Variable(torch.FloatTensor(np.random.uniform(0.01,-0.01,size=(self.vocab_size, self.hidden_state))),requires_grad=True).cuda()\r\n    #self.bo = Variable(torch.FloatTensor(np.zeros((self.vocab_size))),requires_grad=True).cuda()\r\n    \r\n  def forward(self,x):\r\n    zeros = torch.autograd.Variable(torch.zeros(*(self.num_layers, self.batch_size, self.hidden_state)))\r\n    h, _ = self.model(x, None)\r\n    #pdb.set_trace()\r\n    #y = h.contiguous().view(-1,self.hidden_state).mm(self.Wo) + self.bo\r\n    #y = torch.nn.functional.linear(h.contiguous().view(-1,self.hidden_state),self.Wo,self.bo)\r\n    y = self.predict(h.contiguous().view(-1,self.hidden_state))\r\n    return y\r\n    \r\n\r\nmodel = seq2seq(vocab_size=train.size(2), batch_size=batch_size, hidden_state=rnn_size, num_layers=num_layers)\r\nmodel = model.cuda()\r\n\r\n\r\ncriterion = torch.nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n\r\n\r\nindices = range(0,train.size(0)+1,batch_size)\r\nfor i in tqdm(range(NUM_EPOCHS)):\r\n  running_loss = 0.0\r\n  for i, (start,end) in enumerate(zip(indices[:-1],indices[1:])):\r\n    inputs, labels = torch.autograd.Variable(train[start:end,:,:]).cuda(), torch.autograd.Variable(y_t[start:end,:]).cuda()\r\n    \r\n    # zero the parameter gradients\r\n    optimizer.zero_grad()\r\n    outputs = model(inputs)\r\n    \r\n    loss = criterion(outputs, labels.view(-1))\r\n    \r\n    loss.backward()        \r\n    running_loss += loss.data[0]\r\n    optimizer.step()\r\n  tqdm.write('Loss:%f'%(running_loss/(i+1)))\r\n\r\n```"}