{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13242", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13242/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13242/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13242/events", "html_url": "https://github.com/pytorch/pytorch/issues/13242", "id": 374901424, "node_id": "MDU6SXNzdWUzNzQ5MDE0MjQ=", "number": 13242, "title": "Output from network inconsistent after save/load", "user": {"login": "zmonoid", "id": 16687307, "node_id": "MDQ6VXNlcjE2Njg3MzA3", "avatar_url": "https://avatars3.githubusercontent.com/u/16687307?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zmonoid", "html_url": "https://github.com/zmonoid", "followers_url": "https://api.github.com/users/zmonoid/followers", "following_url": "https://api.github.com/users/zmonoid/following{/other_user}", "gists_url": "https://api.github.com/users/zmonoid/gists{/gist_id}", "starred_url": "https://api.github.com/users/zmonoid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zmonoid/subscriptions", "organizations_url": "https://api.github.com/users/zmonoid/orgs", "repos_url": "https://api.github.com/users/zmonoid/repos", "events_url": "https://api.github.com/users/zmonoid/events{/privacy}", "received_events_url": "https://api.github.com/users/zmonoid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-29T08:13:59Z", "updated_at": "2018-10-29T12:39:42Z", "closed_at": "2018-10-29T12:39:41Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>We fix the random seed, save and reload the network during training, but found the network output is different although the network parameters and input is exactly the same. See below code modified from pytorch example mnist. We insert save/load code during training:</p>\n<div class=\"highlight highlight-source-python\"><pre>        <span class=\"pl-k\">if</span> batch_idx <span class=\"pl-k\">==</span> <span class=\"pl-c1\">55</span>:\n            torch.save({\n                    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>model<span class=\"pl-pds\">'</span></span>: model.state_dict(),\n                    }, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>save.pth<span class=\"pl-pds\">'</span></span>)\n\n            xmodel <span class=\"pl-k\">=</span> Net()\n            xdata <span class=\"pl-k\">=</span> torch.load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>save.pth<span class=\"pl-pds\">'</span></span>)\n            xmodel.load_state_dict(xdata[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model<span class=\"pl-pds\">'</span></span>])\n            xmodel.to(device)\n\n            yhat <span class=\"pl-k\">=</span> xmodel(data)\n\n            model_conv1w_diff <span class=\"pl-k\">=</span> np.abs(model.conv1.weight.cpu().detach().numpy() <span class=\"pl-k\">-</span> xmodel.conv1.weight.cpu().detach().numpy()).max()\n            model_fc2w_diff <span class=\"pl-k\">=</span> np.abs(model.fc2.weight.cpu().detach().numpy() <span class=\"pl-k\">-</span> xmodel.fc2.weight.cpu().detach().numpy()).max()\n            model_fc2b_diff <span class=\"pl-k\">=</span> np.abs(model.fc2.bias.cpu().detach().numpy() <span class=\"pl-k\">-</span> xmodel.fc2.bias.cpu().detach().numpy()).max()\n\n            y_diff <span class=\"pl-k\">=</span> np.abs(output.cpu().detach().numpy() <span class=\"pl-k\">-</span> yhat.cpu().detach().numpy()).max()\n            <span class=\"pl-c1\">print</span>(model_conv1w_diff, model_fc2w_diff, model_fc2b_diff, y_diff)</pre></div>\n<p>The print output shows</p>\n<pre><code>0.0 0.0 0.0 0.82663274\n</code></pre>\n<h2>To Reproduce</h2>\n<p>Run the following code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> datasets, transforms\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> pdb\n\ntorch.manual_seed(<span class=\"pl-c1\">7</span>)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n        <span class=\"pl-c1\">self</span>.conv2_drop <span class=\"pl-k\">=</span> nn.Dropout2d()\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">320</span>, <span class=\"pl-c1\">50</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> F.relu(F.max_pool2d(<span class=\"pl-c1\">self</span>.conv1(x), <span class=\"pl-c1\">2</span>))\n        x <span class=\"pl-k\">=</span> F.relu(F.max_pool2d(<span class=\"pl-c1\">self</span>.conv2_drop(<span class=\"pl-c1\">self</span>.conv2(x)), <span class=\"pl-c1\">2</span>))\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">320</span>)\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n        x <span class=\"pl-k\">=</span> F.dropout(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.training)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc2(x)\n        <span class=\"pl-k\">return</span> F.log_softmax(x, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">args</span>, <span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">device</span>, <span class=\"pl-smi\">train_loader</span>, <span class=\"pl-smi\">optimizer</span>, <span class=\"pl-smi\">epoch</span>):\n    model.train()\n    <span class=\"pl-k\">for</span> batch_idx, (data, target) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n        data, target <span class=\"pl-k\">=</span> data.to(device), target.to(device)\n        data.requires_grad_()\n        optimizer.zero_grad()\n        output <span class=\"pl-k\">=</span> model(data)\n        loss <span class=\"pl-k\">=</span> F.nll_loss(output, target)\n        loss.backward()\n\n        <span class=\"pl-k\">if</span> batch_idx <span class=\"pl-k\">==</span> <span class=\"pl-c1\">55</span>:\n            torch.save({\n                    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>model<span class=\"pl-pds\">'</span></span>: model.state_dict(),\n                    }, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>save.pth<span class=\"pl-pds\">'</span></span>)\n\n            xmodel <span class=\"pl-k\">=</span> Net()\n            xdata <span class=\"pl-k\">=</span> torch.load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>save.pth<span class=\"pl-pds\">'</span></span>)\n            xmodel.load_state_dict(xdata[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model<span class=\"pl-pds\">'</span></span>])\n            xmodel.to(device)\n\n            yhat <span class=\"pl-k\">=</span> xmodel(data)\n\n            model_conv1w_diff <span class=\"pl-k\">=</span> np.abs(model.conv1.weight.cpu().detach().numpy() <span class=\"pl-k\">-</span> xmodel.conv1.weight.cpu().detach().numpy()).max()\n            model_fc2w_diff <span class=\"pl-k\">=</span> np.abs(model.fc2.weight.cpu().detach().numpy() <span class=\"pl-k\">-</span> xmodel.fc2.weight.cpu().detach().numpy()).max()\n            model_fc2b_diff <span class=\"pl-k\">=</span> np.abs(model.fc2.bias.cpu().detach().numpy() <span class=\"pl-k\">-</span> xmodel.fc2.bias.cpu().detach().numpy()).max()\n\n            y_diff <span class=\"pl-k\">=</span> np.abs(output.cpu().detach().numpy() <span class=\"pl-k\">-</span> yhat.cpu().detach().numpy()).max()\n            <span class=\"pl-c1\">print</span>(model_conv1w_diff, model_fc2w_diff, model_fc2b_diff, y_diff)\n\n            pdb.set_trace()\n\n        optimizer.step()\n\n        <span class=\"pl-k\">if</span> batch_idx <span class=\"pl-k\">%</span> args.log_interval <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Train Epoch: <span class=\"pl-c1\">{}</span> [<span class=\"pl-c1\">{}</span>/<span class=\"pl-c1\">{}</span> (<span class=\"pl-c1\">{<span class=\"pl-k\">:.0f</span>}</span>%)]<span class=\"pl-cce\">\\t</span>Loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span><span class=\"pl-pds\">'</span></span>.format(\n                epoch, batch_idx <span class=\"pl-k\">*</span> <span class=\"pl-c1\">len</span>(data), <span class=\"pl-c1\">len</span>(train_loader.dataset),\n                <span class=\"pl-c1\">100</span>. <span class=\"pl-k\">*</span> batch_idx <span class=\"pl-k\">/</span> <span class=\"pl-c1\">len</span>(train_loader), loss.item()))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">args</span>, <span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">device</span>, <span class=\"pl-smi\">test_loader</span>):\n    model.eval()\n    test_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    correct <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">with</span> torch.no_grad():\n        <span class=\"pl-k\">for</span> data, target <span class=\"pl-k\">in</span> test_loader:\n            data, target <span class=\"pl-k\">=</span> data.to(device), target.to(device)\n            output <span class=\"pl-k\">=</span> model(data)\n            test_loss <span class=\"pl-k\">+=</span> F.nll_loss(output, target, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>).item() <span class=\"pl-c\"><span class=\"pl-c\">#</span> sum up batch loss</span>\n            pred <span class=\"pl-k\">=</span> output.max(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">keepdim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">1</span>] <span class=\"pl-c\"><span class=\"pl-c\">#</span> get the index of the max log-probability</span>\n            correct <span class=\"pl-k\">+=</span> pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss <span class=\"pl-k\">/=</span> <span class=\"pl-c1\">len</span>(test_loader.dataset)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span>Test set: Average loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span>, Accuracy: <span class=\"pl-c1\">{}</span>/<span class=\"pl-c1\">{}</span> (<span class=\"pl-c1\">{<span class=\"pl-k\">:.0f</span>}</span>%)<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>.format(\n        test_loss, correct, <span class=\"pl-c1\">len</span>(test_loader.dataset),\n        <span class=\"pl-c1\">100</span>. <span class=\"pl-k\">*</span> correct <span class=\"pl-k\">/</span> <span class=\"pl-c1\">len</span>(test_loader.dataset)))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Training settings</span>\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser(<span class=\"pl-v\">description</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>PyTorch MNIST Example<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--batch-size<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input batch size for training (default: 64)<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--test-batch-size<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input batch size for testing (default: 1000)<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--epochs<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>number of epochs to train (default: 10)<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--lr<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>LR<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning rate (default: 0.01)<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--momentum<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>M<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SGD momentum (default: 0.5)<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--no-cuda<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>disables CUDA training<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--seed<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>S<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>random seed (default: 1)<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--log-interval<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>N<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>how many batches to wait before logging training status<span class=\"pl-pds\">'</span></span>)\n    args <span class=\"pl-k\">=</span> parser.parse_args()\n    use_cuda <span class=\"pl-k\">=</span> <span class=\"pl-k\">not</span> args.no_cuda <span class=\"pl-k\">and</span> torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n\n    device <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> use_cuda <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu<span class=\"pl-pds\">\"</span></span>)\n\n    kwargs <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>num_workers<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>pin_memory<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">True</span>} <span class=\"pl-k\">if</span> use_cuda <span class=\"pl-k\">else</span> {}\n    train_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n        datasets.MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>../data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                       <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((<span class=\"pl-c1\">0.1307</span>,), (<span class=\"pl-c1\">0.3081</span>,))\n                       ])),\n        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>args.batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-k\">**</span>kwargs)\n    test_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n        datasets.MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>../data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((<span class=\"pl-c1\">0.1307</span>,), (<span class=\"pl-c1\">0.3081</span>,))\n                       ])),\n        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>args.test_batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-k\">**</span>kwargs)\n\n\n    model <span class=\"pl-k\">=</span> Net().to(device)\n    optimizer <span class=\"pl-k\">=</span> optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>args.lr, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>args.momentum)\n\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, args.epochs <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(args, model, device, test_loader)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()\n</pre></div>\n<h2>Expected behavior</h2>\n<p>Expected that printed output to be zero for all.</p>\n<h2>Environment</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<pre><code>Collecting environment information...\nPyTorch version: 0.4.1.post2\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\nCMake version: version 3.10.2\n\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: GeForce GTX 1070\nNvidia driver version: 390.48\ncuDNN version: Probably one of the following:\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3\n/usr/local/cudnn7-cu90/lib64/libcudnn.so.7.2.1\n/usr/local/cudnn7-cu90/lib64/libcudnn_static.a\n/usr/local/cudnn7005-cu90/lib64/libcudnn.so.7.0.5\n/usr/local/cudnn7005-cu90/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     0.4.1.post2               &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n\n</code></pre>\n<h2>Additional context</h2>\n", "body_text": "\ud83d\udc1b Bug\nWe fix the random seed, save and reload the network during training, but found the network output is different although the network parameters and input is exactly the same. See below code modified from pytorch example mnist. We insert save/load code during training:\n        if batch_idx == 55:\n            torch.save({\n                    'model': model.state_dict(),\n                    }, 'save.pth')\n\n            xmodel = Net()\n            xdata = torch.load('save.pth')\n            xmodel.load_state_dict(xdata['model'])\n            xmodel.to(device)\n\n            yhat = xmodel(data)\n\n            model_conv1w_diff = np.abs(model.conv1.weight.cpu().detach().numpy() - xmodel.conv1.weight.cpu().detach().numpy()).max()\n            model_fc2w_diff = np.abs(model.fc2.weight.cpu().detach().numpy() - xmodel.fc2.weight.cpu().detach().numpy()).max()\n            model_fc2b_diff = np.abs(model.fc2.bias.cpu().detach().numpy() - xmodel.fc2.bias.cpu().detach().numpy()).max()\n\n            y_diff = np.abs(output.cpu().detach().numpy() - yhat.cpu().detach().numpy()).max()\n            print(model_conv1w_diff, model_fc2w_diff, model_fc2b_diff, y_diff)\nThe print output shows\n0.0 0.0 0.0 0.82663274\n\nTo Reproduce\nRun the following code\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport pdb\n\ntorch.manual_seed(7)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        data.requires_grad_()\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n\n        if batch_idx == 55:\n            torch.save({\n                    'model': model.state_dict(),\n                    }, 'save.pth')\n\n            xmodel = Net()\n            xdata = torch.load('save.pth')\n            xmodel.load_state_dict(xdata['model'])\n            xmodel.to(device)\n\n            yhat = xmodel(data)\n\n            model_conv1w_diff = np.abs(model.conv1.weight.cpu().detach().numpy() - xmodel.conv1.weight.cpu().detach().numpy()).max()\n            model_fc2w_diff = np.abs(model.fc2.weight.cpu().detach().numpy() - xmodel.fc2.weight.cpu().detach().numpy()).max()\n            model_fc2b_diff = np.abs(model.fc2.bias.cpu().detach().numpy() - xmodel.fc2.bias.cpu().detach().numpy()).max()\n\n            y_diff = np.abs(output.cpu().detach().numpy() - yhat.cpu().detach().numpy()).max()\n            print(model_conv1w_diff, model_fc2w_diff, model_fc2b_diff, y_diff)\n\n            pdb.set_trace()\n\n        optimizer.step()\n\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(args, model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 10)')\n    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                        help='learning rate (default: 0.01)')\n    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                        help='SGD momentum (default: 0.5)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(args, model, device, test_loader)\n\n\nif __name__ == '__main__':\n    main()\n\nExpected behavior\nExpected that printed output to be zero for all.\nEnvironment\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\nCollecting environment information...\nPyTorch version: 0.4.1.post2\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\nCMake version: version 3.10.2\n\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: GeForce GTX 1070\nNvidia driver version: 390.48\ncuDNN version: Probably one of the following:\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3\n/usr/local/cudnn7-cu90/lib64/libcudnn.so.7.2.1\n/usr/local/cudnn7-cu90/lib64/libcudnn_static.a\n/usr/local/cudnn7005-cu90/lib64/libcudnn.so.7.0.5\n/usr/local/cudnn7005-cu90/lib64/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     0.4.1.post2               <pip>\n[conda] torchvision               0.2.1                     <pip>\n\n\nAdditional context", "body": "## \ud83d\udc1b Bug\r\n\r\nWe fix the random seed, save and reload the network during training, but found the network output is different although the network parameters and input is exactly the same. See below code modified from pytorch example mnist. We insert save/load code during training:\r\n```python\r\n        if batch_idx == 55:\r\n            torch.save({\r\n                    'model': model.state_dict(),\r\n                    }, 'save.pth')\r\n\r\n            xmodel = Net()\r\n            xdata = torch.load('save.pth')\r\n            xmodel.load_state_dict(xdata['model'])\r\n            xmodel.to(device)\r\n\r\n            yhat = xmodel(data)\r\n\r\n            model_conv1w_diff = np.abs(model.conv1.weight.cpu().detach().numpy() - xmodel.conv1.weight.cpu().detach().numpy()).max()\r\n            model_fc2w_diff = np.abs(model.fc2.weight.cpu().detach().numpy() - xmodel.fc2.weight.cpu().detach().numpy()).max()\r\n            model_fc2b_diff = np.abs(model.fc2.bias.cpu().detach().numpy() - xmodel.fc2.bias.cpu().detach().numpy()).max()\r\n\r\n            y_diff = np.abs(output.cpu().detach().numpy() - yhat.cpu().detach().numpy()).max()\r\n            print(model_conv1w_diff, model_fc2w_diff, model_fc2b_diff, y_diff)\r\n```\r\n\r\nThe print output shows \r\n```\r\n0.0 0.0 0.0 0.82663274\r\n```\r\n\r\n## To Reproduce\r\nRun the following code\r\n```python\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nimport numpy as np\r\nimport pdb\r\n\r\ntorch.manual_seed(7)\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x, dim=1)\r\n\r\ndef train(args, model, device, train_loader, optimizer, epoch):\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data, target = data.to(device), target.to(device)\r\n        data.requires_grad_()\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n\r\n        if batch_idx == 55:\r\n            torch.save({\r\n                    'model': model.state_dict(),\r\n                    }, 'save.pth')\r\n\r\n            xmodel = Net()\r\n            xdata = torch.load('save.pth')\r\n            xmodel.load_state_dict(xdata['model'])\r\n            xmodel.to(device)\r\n\r\n            yhat = xmodel(data)\r\n\r\n            model_conv1w_diff = np.abs(model.conv1.weight.cpu().detach().numpy() - xmodel.conv1.weight.cpu().detach().numpy()).max()\r\n            model_fc2w_diff = np.abs(model.fc2.weight.cpu().detach().numpy() - xmodel.fc2.weight.cpu().detach().numpy()).max()\r\n            model_fc2b_diff = np.abs(model.fc2.bias.cpu().detach().numpy() - xmodel.fc2.bias.cpu().detach().numpy()).max()\r\n\r\n            y_diff = np.abs(output.cpu().detach().numpy() - yhat.cpu().detach().numpy()).max()\r\n            print(model_conv1w_diff, model_fc2w_diff, model_fc2b_diff, y_diff)\r\n\r\n            pdb.set_trace()\r\n\r\n        optimizer.step()\r\n\r\n        if batch_idx % args.log_interval == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                100. * batch_idx / len(train_loader), loss.item()))\r\n\r\ndef test(args, model, device, test_loader):\r\n    model.eval()\r\n    test_loss = 0\r\n    correct = 0\r\n    with torch.no_grad():\r\n        for data, target in test_loader:\r\n            data, target = data.to(device), target.to(device)\r\n            output = model(data)\r\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\r\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n            correct += pred.eq(target.view_as(pred)).sum().item()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n\r\ndef main():\r\n    # Training settings\r\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\r\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\r\n                        help='input batch size for training (default: 64)')\r\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\r\n                        help='input batch size for testing (default: 1000)')\r\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\r\n                        help='number of epochs to train (default: 10)')\r\n    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\r\n                        help='learning rate (default: 0.01)')\r\n    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\r\n                        help='SGD momentum (default: 0.5)')\r\n    parser.add_argument('--no-cuda', action='store_true', default=False,\r\n                        help='disables CUDA training')\r\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\r\n                        help='random seed (default: 1)')\r\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\r\n                        help='how many batches to wait before logging training status')\r\n    args = parser.parse_args()\r\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\r\n\r\n    torch.manual_seed(args.seed)\r\n\r\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n\r\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\r\n    train_loader = torch.utils.data.DataLoader(\r\n        datasets.MNIST('../data', train=True, download=True,\r\n                       transform=transforms.Compose([\r\n                           transforms.ToTensor(),\r\n                           transforms.Normalize((0.1307,), (0.3081,))\r\n                       ])),\r\n        batch_size=args.batch_size, shuffle=True, **kwargs)\r\n    test_loader = torch.utils.data.DataLoader(\r\n        datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n                           transforms.ToTensor(),\r\n                           transforms.Normalize((0.1307,), (0.3081,))\r\n                       ])),\r\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\r\n\r\n\r\n    model = Net().to(device)\r\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\r\n\r\n    for epoch in range(1, args.epochs + 1):\r\n        train(args, model, device, train_loader, optimizer, epoch)\r\n        test(args, model, device, test_loader)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nExpected that printed output to be zero for all.\r\n\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3\r\n/usr/local/cudnn7-cu90/lib64/libcudnn.so.7.2.1\r\n/usr/local/cudnn7-cu90/lib64/libcudnn_static.a\r\n/usr/local/cudnn7005-cu90/lib64/libcudnn.so.7.0.5\r\n/usr/local/cudnn7005-cu90/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] torch                     0.4.1.post2               <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n\r\n```\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}