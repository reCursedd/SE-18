{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342349522", "html_url": "https://github.com/pytorch/pytorch/issues/2389#issuecomment-342349522", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2389", "id": 342349522, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjM0OTUyMg==", "user": {"login": "jdily", "id": 304749, "node_id": "MDQ6VXNlcjMwNDc0OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/304749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdily", "html_url": "https://github.com/jdily", "followers_url": "https://api.github.com/users/jdily/followers", "following_url": "https://api.github.com/users/jdily/following{/other_user}", "gists_url": "https://api.github.com/users/jdily/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdily/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdily/subscriptions", "organizations_url": "https://api.github.com/users/jdily/orgs", "repos_url": "https://api.github.com/users/jdily/repos", "events_url": "https://api.github.com/users/jdily/events{/privacy}", "received_events_url": "https://api.github.com/users/jdily/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-07T01:50:58Z", "updated_at": "2017-11-07T01:50:58Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> No problem. I am just thinking if it is implemented, because as discussed in<br>\n<a href=\"https://discuss.pytorch.org/t/does-pytorch-support-autograd-on-sparse-matrix/6156\" rel=\"nofollow\">https://discuss.pytorch.org/t/does-pytorch-support-autograd-on-sparse-matrix/6156</a><br>\nit looks like there should be a PR about it?<br>\nIf so, where can I find it?</p>\n<p>Meanwhile, I found out some interesting results about this feature.<br>\nLet's take a look at the example above,</p>\n<pre><code>x = torch.sparse.FloatTensor(5, 5)\ny = torch.FloatTensor(5, 5)\nxx = torch.autograd.Variable(x)\nxy = torch.autograd.Variable(y)\nprint(type(x), type(xx))\ntorch.mm(xx, xy) # I think here we want to check if backward things  \n</code></pre>\n<p>it will give us <code>RuntimeError: Expected object of type Variable[Sparsetorch.FloatTensor] but found type Variable[torch.FloatTensor]  for argument #1 'mat2'</code>.<br>\nIf I understand correctly,  it seems to me that it ask me to change the type of mat2 from <code>Variable[torch.FloatTensor]</code> to <code>Variable[Sparsetorch.FloatTensor]</code>, i.e. from dense to sparse.<br>\nSo I change the type of <code>y</code> into sparse so the next test:</p>\n<pre><code>x = torch.sparse.FloatTensor(5, 5)\ny = torch.sparse.FloatTensor(5, 5)\nxx = torch.autograd.Variable(x)\nxy = torch.autograd.Variable(y)\nprint(type(x), type(xx))\ntorch.mm(xx, xy) \n</code></pre>\n<p>and this time I got</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/ichao/project/deep_dci/code/sparse_util.py\", line 64, in &lt;module&gt;\n    torch.mm(xx, xy)\nRuntimeError: mm is not implemented for type torch.sparse.FloatTensor\n</code></pre>\n<p>So I am a bit confused about does it even support sparse x sparse ??</p>\n<p>thx for the help. :)</p>", "body_text": "@apaszke No problem. I am just thinking if it is implemented, because as discussed in\nhttps://discuss.pytorch.org/t/does-pytorch-support-autograd-on-sparse-matrix/6156\nit looks like there should be a PR about it?\nIf so, where can I find it?\nMeanwhile, I found out some interesting results about this feature.\nLet's take a look at the example above,\nx = torch.sparse.FloatTensor(5, 5)\ny = torch.FloatTensor(5, 5)\nxx = torch.autograd.Variable(x)\nxy = torch.autograd.Variable(y)\nprint(type(x), type(xx))\ntorch.mm(xx, xy) # I think here we want to check if backward things  \n\nit will give us RuntimeError: Expected object of type Variable[Sparsetorch.FloatTensor] but found type Variable[torch.FloatTensor]  for argument #1 'mat2'.\nIf I understand correctly,  it seems to me that it ask me to change the type of mat2 from Variable[torch.FloatTensor] to Variable[Sparsetorch.FloatTensor], i.e. from dense to sparse.\nSo I change the type of y into sparse so the next test:\nx = torch.sparse.FloatTensor(5, 5)\ny = torch.sparse.FloatTensor(5, 5)\nxx = torch.autograd.Variable(x)\nxy = torch.autograd.Variable(y)\nprint(type(x), type(xx))\ntorch.mm(xx, xy) \n\nand this time I got\nTraceback (most recent call last):\n  File \"/home/ichao/project/deep_dci/code/sparse_util.py\", line 64, in <module>\n    torch.mm(xx, xy)\nRuntimeError: mm is not implemented for type torch.sparse.FloatTensor\n\nSo I am a bit confused about does it even support sparse x sparse ??\nthx for the help. :)", "body": "@apaszke No problem. I am just thinking if it is implemented, because as discussed in \r\nhttps://discuss.pytorch.org/t/does-pytorch-support-autograd-on-sparse-matrix/6156\r\nit looks like there should be a PR about it?\r\nIf so, where can I find it?\r\n\r\nMeanwhile, I found out some interesting results about this feature.\r\nLet's take a look at the example above, \r\n```\r\nx = torch.sparse.FloatTensor(5, 5)\r\ny = torch.FloatTensor(5, 5)\r\nxx = torch.autograd.Variable(x)\r\nxy = torch.autograd.Variable(y)\r\nprint(type(x), type(xx))\r\ntorch.mm(xx, xy) # I think here we want to check if backward things  \r\n```\r\nit will give us ` RuntimeError: Expected object of type Variable[Sparsetorch.FloatTensor] but found type Variable[torch.FloatTensor]  for argument #1 'mat2' `.\r\nIf I understand correctly,  it seems to me that it ask me to change the type of mat2 from `Variable[torch.FloatTensor]` to `Variable[Sparsetorch.FloatTensor]`, i.e. from dense to sparse.\r\nSo I change the type of `y` into sparse so the next test:\r\n```\r\nx = torch.sparse.FloatTensor(5, 5)\r\ny = torch.sparse.FloatTensor(5, 5)\r\nxx = torch.autograd.Variable(x)\r\nxy = torch.autograd.Variable(y)\r\nprint(type(x), type(xx))\r\ntorch.mm(xx, xy) \r\n```\r\nand this time I got \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ichao/project/deep_dci/code/sparse_util.py\", line 64, in <module>\r\n    torch.mm(xx, xy)\r\nRuntimeError: mm is not implemented for type torch.sparse.FloatTensor\r\n```\r\nSo I am a bit confused about does it even support sparse x sparse ?? \r\n\r\nthx for the help. :)\r\n"}