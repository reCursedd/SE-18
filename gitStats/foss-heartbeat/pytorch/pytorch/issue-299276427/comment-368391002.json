{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/368391002", "html_url": "https://github.com/pytorch/pytorch/issues/5349#issuecomment-368391002", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5349", "id": 368391002, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODM5MTAwMg==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-26T05:03:34Z", "updated_at": "2018-02-26T05:03:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So... maybe the thing we're missing is a <code>torch.autograd.backward</code> variant that takes both inputs and outputs (like <code>torch.autograd.grad</code>) but accumulates gradients imperatively in <code>grad</code>. Then this would be a drop-in replacement at any point you want to call <code>loss.backward()</code> but train only certain parameters. The context manager is just icing on the API cake (for conveniently transforming <code>loss.backward()</code> calls into the elaborated version with explicit inputs).</p>\n<p>Avoiding creating graphs in all cases seems difficult, because there's no convenient way to tell what leaves contributed to the history of a variable. I suppose you can make a modest improvement by <em>only</em> clipping the graph when a parameter is directly used, and conservatively build graphs for any variables that come in requiring gradient. This would still be a win if most of your graph management happens inside the context manager.</p>", "body_text": "So... maybe the thing we're missing is a torch.autograd.backward variant that takes both inputs and outputs (like torch.autograd.grad) but accumulates gradients imperatively in grad. Then this would be a drop-in replacement at any point you want to call loss.backward() but train only certain parameters. The context manager is just icing on the API cake (for conveniently transforming loss.backward() calls into the elaborated version with explicit inputs).\nAvoiding creating graphs in all cases seems difficult, because there's no convenient way to tell what leaves contributed to the history of a variable. I suppose you can make a modest improvement by only clipping the graph when a parameter is directly used, and conservatively build graphs for any variables that come in requiring gradient. This would still be a win if most of your graph management happens inside the context manager.", "body": "So... maybe the thing we're missing is a `torch.autograd.backward` variant that takes both inputs and outputs (like `torch.autograd.grad`) but accumulates gradients imperatively in `grad`. Then this would be a drop-in replacement at any point you want to call `loss.backward()` but train only certain parameters. The context manager is just icing on the API cake (for conveniently transforming `loss.backward()` calls into the elaborated version with explicit inputs).\r\n\r\nAvoiding creating graphs in all cases seems difficult, because there's no convenient way to tell what leaves contributed to the history of a variable. I suppose you can make a modest improvement by *only* clipping the graph when a parameter is directly used, and conservatively build graphs for any variables that come in requiring gradient. This would still be a win if most of your graph management happens inside the context manager."}