{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5349", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5349/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5349/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5349/events", "html_url": "https://github.com/pytorch/pytorch/issues/5349", "id": 299276427, "node_id": "MDU6SXNzdWUyOTkyNzY0Mjc=", "number": 5349, "title": "Determining `requires_grad` automatically", "user": {"login": "netheril96", "id": 836839, "node_id": "MDQ6VXNlcjgzNjgzOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/836839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/netheril96", "html_url": "https://github.com/netheril96", "followers_url": "https://api.github.com/users/netheril96/followers", "following_url": "https://api.github.com/users/netheril96/following{/other_user}", "gists_url": "https://api.github.com/users/netheril96/gists{/gist_id}", "starred_url": "https://api.github.com/users/netheril96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/netheril96/subscriptions", "organizations_url": "https://api.github.com/users/netheril96/orgs", "repos_url": "https://api.github.com/users/netheril96/repos", "events_url": "https://api.github.com/users/netheril96/events{/privacy}", "received_events_url": "https://api.github.com/users/netheril96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-02-22T09:24:39Z", "updated_at": "2018-02-27T13:52:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.3.1</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version: Python 3.5.2</li>\n<li>CUDA/cuDNN version: 9.0/7.0</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n</ul>\n<p>Suppose we are training a GAN, with a very simple generator G(z) = tanh(W\u2081z + b\u2081) and discriminator D(x) = W\u2082x + b\u2082. The loss is a function of D(G(z)), and when calling <code>loss.backward()</code>, all of the gradients in the network are computed. However, during GAN training, the generator and discriminator are trained separately in alternation, therefore</p>\n<ol>\n<li>When training D, the gradients do not need to back propagate to G.</li>\n<li>When training G, we need only \u2202D/\u2202x, but not \u2202D/\u2202W\u2082 or \u2202D/\u2202b\u2082.</li>\n</ol>\n<p>The larger the network, the more unnecessary computation is done if we do not set <code>requires_grad</code> and <code>volatile</code> appropriately. According to my observation, few people understand the need to do this in PyTorch, and if they do, they will likely realize point (1) but miss point (2). If the network is even more complex (e.g. ACGAN + WGAN-GP), it will be even harder for a human to analyze.</p>\n<p>TensorFlow does not have the same problem, because its optimizers receive a list of variables to optimize, and with that information they can determine exactly what computation are needed.</p>", "body_text": "OS: Ubuntu 16.04\nPyTorch version: 0.3.1\nHow you installed PyTorch (conda, pip, source): pip\nPython version: Python 3.5.2\nCUDA/cuDNN version: 9.0/7.0\nGPU models and configuration:\nGCC version (if compiling from source):\n\nSuppose we are training a GAN, with a very simple generator G(z) = tanh(W\u2081z + b\u2081) and discriminator D(x) = W\u2082x + b\u2082. The loss is a function of D(G(z)), and when calling loss.backward(), all of the gradients in the network are computed. However, during GAN training, the generator and discriminator are trained separately in alternation, therefore\n\nWhen training D, the gradients do not need to back propagate to G.\nWhen training G, we need only \u2202D/\u2202x, but not \u2202D/\u2202W\u2082 or \u2202D/\u2202b\u2082.\n\nThe larger the network, the more unnecessary computation is done if we do not set requires_grad and volatile appropriately. According to my observation, few people understand the need to do this in PyTorch, and if they do, they will likely realize point (1) but miss point (2). If the network is even more complex (e.g. ACGAN + WGAN-GP), it will be even harder for a human to analyze.\nTensorFlow does not have the same problem, because its optimizers receive a list of variables to optimize, and with that information they can determine exactly what computation are needed.", "body": "- OS: Ubuntu 16.04\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version: Python 3.5.2\r\n- CUDA/cuDNN version: 9.0/7.0\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n\r\nSuppose we are training a GAN, with a very simple generator G(z) = tanh(W\u2081z + b\u2081) and discriminator D(x) = W\u2082x + b\u2082. The loss is a function of D(G(z)), and when calling `loss.backward()`, all of the gradients in the network are computed. However, during GAN training, the generator and discriminator are trained separately in alternation, therefore\r\n\r\n1. When training D, the gradients do not need to back propagate to G.\r\n2. When training G, we need only \u2202D/\u2202x, but not \u2202D/\u2202W\u2082 or \u2202D/\u2202b\u2082.\r\n\r\nThe larger the network, the more unnecessary computation is done if we do not set `requires_grad` and `volatile` appropriately. According to my observation, few people understand the need to do this in PyTorch, and if they do, they will likely realize point (1) but miss point (2). If the network is even more complex (e.g. ACGAN + WGAN-GP), it will be even harder for a human to analyze.\r\n\r\nTensorFlow does not have the same problem, because its optimizers receive a list of variables to optimize, and with that information they can determine exactly what computation are needed. "}