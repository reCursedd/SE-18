{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3863", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3863/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3863/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3863/events", "html_url": "https://github.com/pytorch/pytorch/issues/3863", "id": 276697414, "node_id": "MDU6SXNzdWUyNzY2OTc0MTQ=", "number": 3863, "title": "Considerable slowdown in Adam.step after a number of epochs with multiple losses", "user": {"login": "emanjavacas", "id": 6195107, "node_id": "MDQ6VXNlcjYxOTUxMDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6195107?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emanjavacas", "html_url": "https://github.com/emanjavacas", "followers_url": "https://api.github.com/users/emanjavacas/followers", "following_url": "https://api.github.com/users/emanjavacas/following{/other_user}", "gists_url": "https://api.github.com/users/emanjavacas/gists{/gist_id}", "starred_url": "https://api.github.com/users/emanjavacas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emanjavacas/subscriptions", "organizations_url": "https://api.github.com/users/emanjavacas/orgs", "repos_url": "https://api.github.com/users/emanjavacas/repos", "events_url": "https://api.github.com/users/emanjavacas/events{/privacy}", "received_events_url": "https://api.github.com/users/emanjavacas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-24T20:53:31Z", "updated_at": "2018-11-23T15:29:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I have a model with multiple outputs and, therefore, multiple losses. When training I accumulate the losses using retain_graph. Something along the lines of:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">self</span>.zero_grad()\n<span class=\"pl-k\">for</span> output_label, output <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>(<span class=\"pl-c1\">input</span>, target).items():\n    loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.loss(output, target[output_label])\n    loss.backward(<span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">self</span>.optimizer.step()</pre></div>\n<p>where input, output and target are dictionaries with the respective data for the different inputs and losses.</p>\n<p>I am using Adam for the optimization.<br>\nI've noticed that after a number of epochs, the running time of an epoch goes suddenly up from 7sec to 34sec.<br>\nI also noticed a slowdown of CPU usage in my computer (I haven't test this yet on the GPU). Memory usage doesn't seem to increase.</p>\n<p>I profiled the code and I saw this (output from cProfile):</p>\n<p>Normal epoch:</p>\n<pre><code>       52    0.012    0.000    3.538    0.068 adam.py:30(step)\n      624    0.646    0.001    0.646    0.001 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\n</code></pre>\n<p>Slow epoch:</p>\n<pre><code>       52    0.013    0.000   24.576    0.473 adam.py:30(step)\n      624   21.469    0.034   21.469    0.034 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\n</code></pre>\n<p>I've tested with other adaptive losses like Adagrad, and there I can't see the issue.<br>\nIt seems to be related to this line of code in Adam.step():</p>\n<pre><code>p.data.addcdiv_(-step_size, exp_avg, denom)\n</code></pre>\n<p>Any ideas about why this is happening? It seems like suddenly the size of the accumulated gradient explodes, but  I can't see why.</p>", "body_text": "I have a model with multiple outputs and, therefore, multiple losses. When training I accumulate the losses using retain_graph. Something along the lines of:\nself.zero_grad()\nfor output_label, output in self(input, target).items():\n    loss = self.loss(output, target[output_label])\n    loss.backward(retain_graph=True)\nself.optimizer.step()\nwhere input, output and target are dictionaries with the respective data for the different inputs and losses.\nI am using Adam for the optimization.\nI've noticed that after a number of epochs, the running time of an epoch goes suddenly up from 7sec to 34sec.\nI also noticed a slowdown of CPU usage in my computer (I haven't test this yet on the GPU). Memory usage doesn't seem to increase.\nI profiled the code and I saw this (output from cProfile):\nNormal epoch:\n       52    0.012    0.000    3.538    0.068 adam.py:30(step)\n      624    0.646    0.001    0.646    0.001 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\n\nSlow epoch:\n       52    0.013    0.000   24.576    0.473 adam.py:30(step)\n      624   21.469    0.034   21.469    0.034 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\n\nI've tested with other adaptive losses like Adagrad, and there I can't see the issue.\nIt seems to be related to this line of code in Adam.step():\np.data.addcdiv_(-step_size, exp_avg, denom)\n\nAny ideas about why this is happening? It seems like suddenly the size of the accumulated gradient explodes, but  I can't see why.", "body": "I have a model with multiple outputs and, therefore, multiple losses. When training I accumulate the losses using retain_graph. Something along the lines of:\r\n\r\n```python\r\nself.zero_grad()\r\nfor output_label, output in self(input, target).items():\r\n    loss = self.loss(output, target[output_label])\r\n    loss.backward(retain_graph=True)\r\nself.optimizer.step()\r\n```\r\n\r\nwhere input, output and target are dictionaries with the respective data for the different inputs and losses.\r\n\r\nI am using Adam for the optimization. \r\nI've noticed that after a number of epochs, the running time of an epoch goes suddenly up from 7sec to 34sec.\r\nI also noticed a slowdown of CPU usage in my computer (I haven't test this yet on the GPU). Memory usage doesn't seem to increase.\r\n\r\nI profiled the code and I saw this (output from cProfile):\r\n\r\nNormal epoch:\r\n```\r\n       52    0.012    0.000    3.538    0.068 adam.py:30(step)\r\n      624    0.646    0.001    0.646    0.001 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\r\n```\r\nSlow epoch:\r\n```\r\n       52    0.013    0.000   24.576    0.473 adam.py:30(step)\r\n      624   21.469    0.034   21.469    0.034 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\r\n```\r\nI've tested with other adaptive losses like Adagrad, and there I can't see the issue.\r\nIt seems to be related to this line of code in Adam.step():\r\n```\r\np.data.addcdiv_(-step_size, exp_avg, denom)\r\n```\r\nAny ideas about why this is happening? It seems like suddenly the size of the accumulated gradient explodes, but  I can't see why."}