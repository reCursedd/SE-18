{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9940", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9940/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9940/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9940/events", "html_url": "https://github.com/pytorch/pytorch/issues/9940", "id": 345310317, "node_id": "MDU6SXNzdWUzNDUzMTAzMTc=", "number": 9940, "title": "JIT fuser can't handle scalar ops", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-27T17:42:34Z", "updated_at": "2018-07-27T19:07:19Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336004262\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8919\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8919/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8919\">#8919</a> changes the semantics of the most basic binary ops, and we need to update the JIT code to handle that.</p>\n<p>Previously, the fuser depended on shape propagation to insert numerous expands, making the sizes of inputs to pointwise ops match (this is a hard requirement of our codegen). However, this is no longer a sound transformation, because the type inference rules depend on input ranks and the \"scalar-auto-wrapping status\" of input tensors.</p>\n<p>Since all this is fundamentally only a requirement of the fuser, it makes sense to handle it there. We will need to special case the pointwise operators between regular tensors and (possibly CPU, possibly differently typed) tensors representing scalars. There are two ways to approach this:</p>\n<ol>\n<li>Allow the codegen to take scalars as plain kernel arguments. This even gives us some nice perf benefits we didn't have before.</li>\n<li>If the <code>TensorIterator</code> actually allows to easily write CUDA kernels, and handles type promotion and expands for us, we might want to use that in our codegen and relax some constraints. I'm not very familiar with this functionality, so I don't know how feasible this is.</li>\n<li>(inferior to previous) insert <code>type_as</code> and <code>expand</code> nodes, making the inputs amenable to fusion</li>\n</ol>", "body_text": "#8919 changes the semantics of the most basic binary ops, and we need to update the JIT code to handle that.\nPreviously, the fuser depended on shape propagation to insert numerous expands, making the sizes of inputs to pointwise ops match (this is a hard requirement of our codegen). However, this is no longer a sound transformation, because the type inference rules depend on input ranks and the \"scalar-auto-wrapping status\" of input tensors.\nSince all this is fundamentally only a requirement of the fuser, it makes sense to handle it there. We will need to special case the pointwise operators between regular tensors and (possibly CPU, possibly differently typed) tensors representing scalars. There are two ways to approach this:\n\nAllow the codegen to take scalars as plain kernel arguments. This even gives us some nice perf benefits we didn't have before.\nIf the TensorIterator actually allows to easily write CUDA kernels, and handles type promotion and expands for us, we might want to use that in our codegen and relax some constraints. I'm not very familiar with this functionality, so I don't know how feasible this is.\n(inferior to previous) insert type_as and expand nodes, making the inputs amenable to fusion", "body": "#8919 changes the semantics of the most basic binary ops, and we need to update the JIT code to handle that.\r\n\r\nPreviously, the fuser depended on shape propagation to insert numerous expands, making the sizes of inputs to pointwise ops match (this is a hard requirement of our codegen). However, this is no longer a sound transformation, because the type inference rules depend on input ranks and the \"scalar-auto-wrapping status\" of input tensors.\r\n\r\nSince all this is fundamentally only a requirement of the fuser, it makes sense to handle it there. We will need to special case the pointwise operators between regular tensors and (possibly CPU, possibly differently typed) tensors representing scalars. There are two ways to approach this:\r\n\r\n1. Allow the codegen to take scalars as plain kernel arguments. This even gives us some nice perf benefits we didn't have before.\r\n2. If the `TensorIterator` actually allows to easily write CUDA kernels, and handles type promotion and expands for us, we might want to use that in our codegen and relax some constraints. I'm not very familiar with this functionality, so I don't know how feasible this is.\r\n3. (inferior to previous) insert `type_as` and `expand` nodes, making the inputs amenable to fusion"}