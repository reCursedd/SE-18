{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4805", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4805/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4805/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4805/events", "html_url": "https://github.com/pytorch/pytorch/issues/4805", "id": 290933652, "node_id": "MDU6SXNzdWUyOTA5MzM2NTI=", "number": 4805, "title": "Bugged behaviour of pytorch [v0.2.0, v0.3.0, v0.4.0] with joblib ", "user": {"login": "lpuglia", "id": 2184440, "node_id": "MDQ6VXNlcjIxODQ0NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/2184440?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lpuglia", "html_url": "https://github.com/lpuglia", "followers_url": "https://api.github.com/users/lpuglia/followers", "following_url": "https://api.github.com/users/lpuglia/following{/other_user}", "gists_url": "https://api.github.com/users/lpuglia/gists{/gist_id}", "starred_url": "https://api.github.com/users/lpuglia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lpuglia/subscriptions", "organizations_url": "https://api.github.com/users/lpuglia/orgs", "repos_url": "https://api.github.com/users/lpuglia/repos", "events_url": "https://api.github.com/users/lpuglia/events{/privacy}", "received_events_url": "https://api.github.com/users/lpuglia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 838476895, "node_id": "MDU6TGFiZWw4Mzg0NzY4OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/hackamonth", "name": "hackamonth", "color": "0e8a16", "default": false}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-23T17:49:18Z", "updated_at": "2018-04-10T14:20:47Z", "closed_at": "2018-04-10T14:20:47Z", "author_association": "NONE", "body_html": "<p>I was playing with torch and joblib and i incurred in something weird. This is the smallest snippet of code to reproduce the BUG:</p>\n<pre><code>from joblib import Parallel, delayed\nimport torch\n\ndef worker(local_perm):\n    print(\"start \"+str(local_perm))\n    torch.zeros(100001)\n    \ntorch.zeros(100001)\nParallel(n_jobs=2, verbose=51)(delayed(worker)(1) for perm in [1,2])\n</code></pre>\n<p>In this case in fact the two (or more) jobs hang. And weirdly enough, if you change the size of the two tensors from 100001 to 100000 they end without hanging.</p>\n<p>This happens with the 0.3.0 and 0.2.0 versions, doesn't happen for v0.1.12<br>\nTested on two machines with different setup and with both python 2.7 and 3.5.</p>", "body_text": "I was playing with torch and joblib and i incurred in something weird. This is the smallest snippet of code to reproduce the BUG:\nfrom joblib import Parallel, delayed\nimport torch\n\ndef worker(local_perm):\n    print(\"start \"+str(local_perm))\n    torch.zeros(100001)\n    \ntorch.zeros(100001)\nParallel(n_jobs=2, verbose=51)(delayed(worker)(1) for perm in [1,2])\n\nIn this case in fact the two (or more) jobs hang. And weirdly enough, if you change the size of the two tensors from 100001 to 100000 they end without hanging.\nThis happens with the 0.3.0 and 0.2.0 versions, doesn't happen for v0.1.12\nTested on two machines with different setup and with both python 2.7 and 3.5.", "body": "I was playing with torch and joblib and i incurred in something weird. This is the smallest snippet of code to reproduce the BUG:\r\n```\r\nfrom joblib import Parallel, delayed\r\nimport torch\r\n\r\ndef worker(local_perm):\r\n    print(\"start \"+str(local_perm))\r\n    torch.zeros(100001)\r\n    \r\ntorch.zeros(100001)\r\nParallel(n_jobs=2, verbose=51)(delayed(worker)(1) for perm in [1,2])\r\n```\r\nIn this case in fact the two (or more) jobs hang. And weirdly enough, if you change the size of the two tensors from 100001 to 100000 they end without hanging.\r\n\r\nThis happens with the 0.3.0 and 0.2.0 versions, doesn't happen for v0.1.12\r\nTested on two machines with different setup and with both python 2.7 and 3.5."}