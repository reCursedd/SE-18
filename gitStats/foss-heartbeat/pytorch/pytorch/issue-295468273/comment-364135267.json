{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/364135267", "html_url": "https://github.com/pytorch/pytorch/issues/5132#issuecomment-364135267", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5132", "id": 364135267, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDEzNTI2Nw==", "user": {"login": "dasguptar", "id": 1309728, "node_id": "MDQ6VXNlcjEzMDk3Mjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1309728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dasguptar", "html_url": "https://github.com/dasguptar", "followers_url": "https://api.github.com/users/dasguptar/followers", "following_url": "https://api.github.com/users/dasguptar/following{/other_user}", "gists_url": "https://api.github.com/users/dasguptar/gists{/gist_id}", "starred_url": "https://api.github.com/users/dasguptar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dasguptar/subscriptions", "organizations_url": "https://api.github.com/users/dasguptar/orgs", "repos_url": "https://api.github.com/users/dasguptar/repos", "events_url": "https://api.github.com/users/dasguptar/events{/privacy}", "received_events_url": "https://api.github.com/users/dasguptar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-08T14:52:05Z", "updated_at": "2018-02-08T14:52:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9799395\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/saitarslanboun\">@saitarslanboun</a><br>\nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for <code>LSTM</code> and <code>LSTMCell</code>. Note that by default, <code>LSTM</code> takes as input a tensor of shape <code>(seq_len, batch, input_size)</code>, whereas <code>LSTMCell</code> takes as input a tensor of shape <code>(batch, input_size)</code>.</p>\n<p>In your code snippet, you seem to have <code>x</code> where <code>x.size(1)</code> specifies the number of timesteps. If I assume that <code>x</code> is then of the form <code>(batch_size, seq_len, input_size)</code>, the first thing we might want to do is to transpose the first 2 dimensions, by <code>x.transpose_(0,1)</code>, to make things <code>seq_len</code> first, or <code>batch_size</code> second. This is only because recurrent layers like to have the timesteps in the first dimension.</p>\n<p>Now, ideally, with a tensor <code>x</code> in such a shape, you do not need to use either a for loop, or a <code>LSTMCell</code>, and can simply pass it to a <code>LSTM</code> module to get the desired output. You can find the documentation for LSTM <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.LSTM\" rel=\"nofollow\">here</a>. Your entire code will look like this:</p>\n<pre><code># create lstmcell module\nlstm = nn.LSTM(embed_size * 2, hidden_size)\n\n# transpose from batch first to batch second for recurrent layers\nx = x.transpose(0, 1).contiguous()\noutput, (h_t, c_t) = lstm(x)\n</code></pre>\n<p>Now let us assume that for some reason, you must use a <code>LSTMCell</code> instead of a <code>LSTM</code>. In this case, if you go through the documentation for <code>LSTMCell</code> <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell\" rel=\"nofollow\">here</a>, you will see that the input needs to be passed 1 time step at a time. So, assuming your <code>x</code> is of the form <code>(batch_size, seq_len, input_size)</code>, your entire code will look like this:</p>\n<pre><code># create lstmcell module\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\n\n# initialize h,c outside for loop\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\n\n# loop over time steps\nfor time_step in range(x.size(1)):\n    x_t = x[:, time_step, :]\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\n</code></pre>\n<p>From what I have understood from your snippet, there are a few potential mistakes:</p>\n<ul>\n<li>Wrong sizes to initialize <code>hidden</code> and <code>cell</code>, which should be <code>(batch_size, input_size)</code> for <code>LSTMCell</code></li>\n<li>Two nested for loops, first over time steps, and second possibly over batch size, which is wrong</li>\n</ul>\n<p>Let me know if this helps you and/or if there are any details in your question that I have misunderstood.</p>", "body_text": "Hi @saitarslanboun\nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for LSTM and LSTMCell. Note that by default, LSTM takes as input a tensor of shape (seq_len, batch, input_size), whereas LSTMCell takes as input a tensor of shape (batch, input_size).\nIn your code snippet, you seem to have x where x.size(1) specifies the number of timesteps. If I assume that x is then of the form (batch_size, seq_len, input_size), the first thing we might want to do is to transpose the first 2 dimensions, by x.transpose_(0,1), to make things seq_len first, or batch_size second. This is only because recurrent layers like to have the timesteps in the first dimension.\nNow, ideally, with a tensor x in such a shape, you do not need to use either a for loop, or a LSTMCell, and can simply pass it to a LSTM module to get the desired output. You can find the documentation for LSTM here. Your entire code will look like this:\n# create lstmcell module\nlstm = nn.LSTM(embed_size * 2, hidden_size)\n\n# transpose from batch first to batch second for recurrent layers\nx = x.transpose(0, 1).contiguous()\noutput, (h_t, c_t) = lstm(x)\n\nNow let us assume that for some reason, you must use a LSTMCell instead of a LSTM. In this case, if you go through the documentation for LSTMCell here, you will see that the input needs to be passed 1 time step at a time. So, assuming your x is of the form (batch_size, seq_len, input_size), your entire code will look like this:\n# create lstmcell module\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\n\n# initialize h,c outside for loop\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\n\n# loop over time steps\nfor time_step in range(x.size(1)):\n    x_t = x[:, time_step, :]\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\n\nFrom what I have understood from your snippet, there are a few potential mistakes:\n\nWrong sizes to initialize hidden and cell, which should be (batch_size, input_size) for LSTMCell\nTwo nested for loops, first over time steps, and second possibly over batch size, which is wrong\n\nLet me know if this helps you and/or if there are any details in your question that I have misunderstood.", "body": "Hi @saitarslanboun \r\nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.\r\n\r\nIn your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.\r\n\r\nNow, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:\r\n```\r\n# create lstmcell module\r\nlstm = nn.LSTM(embed_size * 2, hidden_size)\r\n\r\n# transpose from batch first to batch second for recurrent layers\r\nx = x.transpose(0, 1).contiguous()\r\noutput, (h_t, c_t) = lstm(x)\r\n```\r\n\r\nNow let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:\r\n```\r\n# create lstmcell module\r\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\r\n\r\n# initialize h,c outside for loop\r\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\r\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\r\n\r\n# loop over time steps\r\nfor time_step in range(x.size(1)):\r\n    x_t = x[:, time_step, :]\r\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\r\n```\r\n\r\nFrom what I have understood from your snippet, there are a few potential mistakes:\r\n- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`\r\n- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong\r\n\r\nLet me know if this helps you and/or if there are any details in your question that I have misunderstood."}