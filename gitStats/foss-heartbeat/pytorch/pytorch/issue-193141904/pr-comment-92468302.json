{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/92468302", "pull_request_review_id": 12984930, "id": 92468302, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDkyNDY4MzAy", "diff_hunk": "@@ -94,35 +94,42 @@ def backward(self, gradient=None, retain_variables=False):\n     def __repr__(self):\n         return 'Variable containing:' + self.data.__repr__()\n \n+    # It's safer to make this a static method, so we can be sure that\n+    # the closure holds no references to self\n+    @staticmethod\n+    def _build_hook(hook, output_nr):\n+        def wrapper(grad_input, grad_output):\n+            result = hook(grad_output[output_nr])\n+            if result is None:\n+                return None\n+            return grad_output[:output_nr] + (result,) + grad_output[output_nr+1:]\n+        return wrapper\n+\n     def register_hook(self, name, hook):\n         if self.volatile:\n-            raise RuntimeError('registering hook on a volatile variable')\n+            raise RuntimeError(\"registering hook on a volatile variable\")\n         if not self.requires_grad:\n             raise RuntimeError(\"registering hook on a variable that doesn't require gradient\")\n-        if self.creator is not None:\n-            self.creator.register_hook(name, lambda gi, go: hook(go[self.output_nr]))\n-        else:\n-            self.backward_hooks = self.backward_hooks or OrderedDict()\n-            assert name not in self.backward_hooks, \\\n-                \"Trying to register a second hook with name {}\".format(name)\n-            self.backward_hooks[name] = hook\n+        if self._backward_hooks is None:\n+            self._backward_hooks = OrderedDict()\n+            self.creator._register_hook_dict(self)\n+        assert name not in self._backward_hooks, \\\n+            \"Trying to register a second hook with name {}\".format(name)\n+        self._backward_hooks[name] = hook\n \n     def remove_hook(self, name):\n         if self.volatile:\n             raise RuntimeError(\"volatile variables don't support hooks\")\n-        if self.creator is not None:\n-            self.creator.remove_hook(name)\n-        else:\n-            assert self.backward_hooks and name in self.backward_hooks, \\\n-                \"Trying to remove an inexistent hook with name {}\".format(name)\n-            del self.backward_hooks[name]\n+        assert self._backward_hooks and name in self._backward_hooks, \\\n+            \"Trying to remove an inexistent hook with name {}\".format(name)\n+        del self._backward_hooks[name]\n \n     def _do_backward(self, grad_output, retain_variables):\n         assert len(grad_output) == 1\n         assert self._version == 0 and self.creator is None, \\\n             \"leaf variable was used in an inplace operation\"\n-        if self.backward_hooks:\n-            for hook in self.backward_hooks.values():\n+        if self._backward_hooks:\n+            for hook in self._backward_hooks.values():", "path": "torch/autograd/variable.py", "position": null, "original_position": 55, "commit_id": "04ddd0c65ce1a683b29a47bbbabdff5e42776e72", "original_commit_id": "fe8d55c78ff3bafdf5e3d4c8d4ce2f130f2695cc", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "body": "shouldn't the logic here be: grad_output[0] = hook(grad_output[0]) ? otherwise what's the point of the hook returning the modified gradient as opposed to changing them in-place...", "created_at": "2016-12-14T19:23:17Z", "updated_at": "2018-11-23T15:31:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/290#discussion_r92468302", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/290", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/92468302"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/290#discussion_r92468302"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/290"}}, "body_html": "<p>shouldn't the logic here be: grad_output[0] = hook(grad_output[0]) ? otherwise what's the point of the hook returning the modified gradient as opposed to changing them in-place...</p>", "body_text": "shouldn't the logic here be: grad_output[0] = hook(grad_output[0]) ? otherwise what's the point of the hook returning the modified gradient as opposed to changing them in-place..."}