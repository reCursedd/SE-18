{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/320815566", "html_url": "https://github.com/pytorch/pytorch/issues/2124#issuecomment-320815566", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2124", "id": 320815566, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDgxNTU2Ng==", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-08T00:22:38Z", "updated_at": "2017-08-08T00:22:38Z", "author_association": "COLLABORATOR", "body_html": "<p>The thing is that, given</p>\n<pre><code>a = torch.ByteTensor([1,2])\n</code></pre>\n<p><code>sum</code> returns a raw number if called without arguments (it sums all the elements of the tensor)</p>\n<pre><code>&gt;&gt;&gt; type(torch.sum(a))\n&lt;type 'int'&gt;\n</code></pre>\n<p>while it returns a new <code>ByteTensor</code> when <code>dim</code> is specified</p>\n<pre><code>&gt;&gt;&gt; type(torch.sum(a,dim=0))\n&lt;type 'torch.ByteTensor'&gt;\n</code></pre>\n<p>so it would overflow if <code>dim</code> is provided.</p>\n<p>On the other hand, <code>sum</code> on a <code>Variable</code> always wraps the results in a <code>ByteTensor</code> variable, with our without <code>dim</code>.</p>\n<p>Looking at the API I would say that <code>torch.sum(a)</code> is the exception here, although the <code>a==b</code> case is admittedly a big gotcha and probably a common use case.</p>", "body_text": "The thing is that, given\na = torch.ByteTensor([1,2])\n\nsum returns a raw number if called without arguments (it sums all the elements of the tensor)\n>>> type(torch.sum(a))\n<type 'int'>\n\nwhile it returns a new ByteTensor when dim is specified\n>>> type(torch.sum(a,dim=0))\n<type 'torch.ByteTensor'>\n\nso it would overflow if dim is provided.\nOn the other hand, sum on a Variable always wraps the results in a ByteTensor variable, with our without dim.\nLooking at the API I would say that torch.sum(a) is the exception here, although the a==b case is admittedly a big gotcha and probably a common use case.", "body": "The thing is that, given\r\n```\r\na = torch.ByteTensor([1,2])\r\n```\r\n`sum` returns a raw number if called without arguments (it sums all the elements of the tensor)\r\n```\r\n>>> type(torch.sum(a))\r\n<type 'int'>\r\n```\r\nwhile it returns a new `ByteTensor` when `dim` is specified\r\n```\r\n>>> type(torch.sum(a,dim=0))\r\n<type 'torch.ByteTensor'>\r\n```\r\nso it would overflow if `dim` is provided.\r\n\r\nOn the other hand, `sum` on a `Variable` always wraps the results in a `ByteTensor` variable, with our without `dim`.\r\n\r\nLooking at the API I would say that `torch.sum(a)` is the exception here, although the `a==b` case is admittedly a big gotcha and probably a common use case."}