{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200529539", "pull_request_review_id": 134864328, "id": 200529539, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDUyOTUzOQ==", "diff_hunk": "@@ -203,49 +207,179 @@ struct SchemaParser {\n   }\n   Lexer L;\n   bool kwarg_only;\n-  at::Tensor one = at::full({}, 1, at::kLong);\n-  at::Tensor zero = at::full({}, 0, at::kLong);\n+  static at::Tensor one() {\n+    static at::Tensor v = at::full({}, 1, at::kLong);\n+    return v;\n+  }\n+  static at::Tensor zero() {\n+    static at::Tensor v = at::full({}, 0, at::kLong);\n+    return v;\n+  }\n };\n }\n \n-using SchemaMap = std::unordered_map<std::string, std::vector<FunctionSchema>>;\n \n-// defined in aten_schema_declarations.cpp\n-extern const char * schema_declarations;\n+namespace {\n+\n+using OperatorMap = std::unordered_map<Symbol, std::vector<std::shared_ptr<Operator>>>;\n+struct OperatorRegistry  {\n+  OperatorMap operators;\n+  std::mutex lock;\n+  void registerOperator(Operator&& op){\n+    std::lock_guard<std::mutex> guard(lock);\n+    Symbol sym = Symbol::fromQualString(op.schema.name);\n+    auto it = operators.find(sym);\n+    if(it == operators.end()) {\n+      it = operators.insert({sym, {}}).first;\n+    }\n+    it->second.push_back(std::make_shared<Operator>(std::move(op)));\n+  }\n+  const std::vector<std::shared_ptr<Operator>>& getOperators(Symbol name) {\n+    std::lock_guard<std::mutex> guard(lock);\n+    static std::vector<std::shared_ptr<Operator>> empty;\n+    auto it = operators.find(name);\n+    if(it != operators.end())\n+      return it->second;\n+    return empty;\n+  }\n+};\n \n-std::vector<FunctionSchema> createOperatorSchemas() {\n-  return script::SchemaParser(schema_declarations).parseDeclarations();\n+OperatorRegistry& getRegsitry() {\n+  static OperatorRegistry r;\n+  return r;\n }\n \n-std::vector<FunctionSchema> & getOperatorSchemas() {\n-  static std::vector<FunctionSchema> schema = createOperatorSchemas();\n-  return schema;\n }\n \n-static SchemaMap createSchemaMap() {\n-  auto& schemas = getOperatorSchemas();\n-  SchemaMap result;\n-  for(auto & schema : schemas) {\n-    auto it = result.find(schema.name);\n-    if(it == result.end()) {\n-      it = result.insert({schema.name, {}}).first;\n-    }\n-    it->second.push_back(std::move(schema));\n-  }\n-  return result;\n+void registerOperator(Operator&& op) {\n+  getRegsitry().registerOperator(std::move(op));\n }\n \n-const std::vector<FunctionSchema>& getOperatorSchema(const std::string& name) {\n-  static SchemaMap map = createSchemaMap();\n-  static std::vector<FunctionSchema> empty;\n-  auto it = map.find(name);\n-  if(it != map.end())\n-    return it->second;\n-  return empty;\n+const std::vector<std::shared_ptr<Operator>>& getOperatorsFor(Symbol name) {\n+  return getRegsitry().getOperators(name);\n }\n \n FunctionSchema parseSchema(const std::string& schema) {\n   return script::SchemaParser(schema).parseDeclarations().at(0);\n }\n \n+at::optional<AttributeKind> attributeKindOf(TypePtr type) {\n+  switch(type->kind()) {\n+    case TypeKind::IntType: return AttributeKind::i;\n+    case TypeKind::FloatType: return AttributeKind::f;\n+    case TypeKind::NumberType: return AttributeKind::t;\n+    case TypeKind::ListType:\n+      if(type->isSubtypeOf(*ListType::ofInts()))\n+        return AttributeKind::is;\n+      else\n+        return at::nullopt;\n+    default:\n+      return at::nullopt;\n+  }\n+}\n+\n+bool typeMatches(TypePtr actual, TypePtr formal) {\n+  if(actual->isSubtypeOf(*formal))\n+    return true;\n+\n+  // XXX - this is here because we allow tensors to be used in place of numbers\n+  // or lists of numbers in the script because of the restriction that all inputs to script must be tensors.\n+  // Once numbers are always treated as seperate types from Tensors, this line\n+  // should be removed, since it opens up the possibility of ambigous declarations\n+  // dispatching to the wrong implementation.\n+  if ((formal->isSubtypeOf(*NumberType::get()) ||\n+       formal->isSubtypeOf(*ListType::ofInts())) &&\n+      actual->isSubtypeOf(*DynamicType::get()))\n+    return true;\n+\n+  return false;\n+}\n+\n+bool Operator::matchesNode(Node* node) const {\n+  size_t attributes_size = node->numAttributes();\n+  size_t attributes_seen = 0;\n+  auto inputs_size = node->inputs().size();\n+  size_t input_i = 0;\n+  for(size_t arg_i = 0; arg_i < schema.arguments.size(); ++arg_i) {\n+    at::optional<AttributeKind> attribute_kind;\n+    const Argument& arg = schema.arguments[arg_i];\n+    if(attributes_size > 0 && (attribute_kind = attributeKindOf(arg.type))) {\n+      auto name = Symbol::fromQualString(\"attr::\" + arg.name);\n+      if(!node->hasAttribute(name) || node->kindOf(name) != *attribute_kind) {\n+        // std::cout << \"missing attribute: \" << name << \"\\n\";\n+        return false;\n+      }\n+      attributes_seen++;\n+    } else if(*arg.type == *ListType::ofTensors()) {\n+      // Tensor[] is handled as varargs, consume inputs until the remaining required arguments\n+      // XXX - there can only be a single Tensor[] in a declaration\n+      size_t remaining_required = 0;\n+      for(size_t j = arg_i + 1; j < schema.arguments.size(); ++j){\n+        // remaining arguments are only those that won't be consumed from attributes\n+        if(attributes_size == 0 || !attributeKindOf(schema.arguments[j].type))\n+          remaining_required++;\n+      }\n+      while(inputs_size - input_i > remaining_required) {\n+        auto input = node->inputs()[input_i++];\n+        if(!typeMatches(input->type(), DynamicType::get())) {\n+          // std::cout << \"vararg argument is not Dynamic\\n\";\n+          return false;\n+        }\n+      }\n+    } else {\n+      if(input_i == inputs_size) {\n+        // std::cout << \"not enough inputs\\n\";\n+        return false;\n+      }\n+      auto input = node->inputs()[input_i++];\n+      if(!typeMatches(input->type(), arg.type)) {\n+        // std::cout << \"argument \" << arg_i << \" has the wrong type\\n\";\n+        return false;\n+      }\n+    }\n+  }\n+\n+  if(!schema.is_vararg && input_i != inputs_size) {\n+    // std::cout << \"not all inputs used\\n\" << input_i << \" \" << inputs_size << \"\\n\";\n+    return false;\n+  }\n+  if(!schema.is_vararg && attributes_seen != attributes_size) {\n+    // std::cout << \"not all attributes used\\n\" << attributes_seen << \" \" << attributes_size << \"\\n\";\n+    return false;\n+  }\n+  return true;\n+}\n+\n+std::shared_ptr<Operator> findOperatorFor(Node* node) {\n+  const auto& candidates = getOperatorsFor(node->kind());\n+  for(const auto& candidate : candidates) {\n+    if(candidate->matchesNode(node)) {\n+      return candidate;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+const Operator& getOperatorFor(Node* node) {", "path": "torch/csrc/jit/operator.cpp", "position": 361, "original_position": 218, "commit_id": "1efd5a0aff5427596ececb2c851b1ef6291a707b", "original_commit_id": "1715f95129be14a369d9c5324f601abdcabc6a77", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Change the name of getOperatorsFor to getAllOperatorsFor  to make the distinction clearer.", "created_at": "2018-07-06T01:27:50Z", "updated_at": "2018-11-23T15:46:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/8885#discussion_r200529539", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8885", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200529539"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8885#discussion_r200529539"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8885"}}, "body_html": "<p>Change the name of getOperatorsFor to getAllOperatorsFor  to make the distinction clearer.</p>", "body_text": "Change the name of getOperatorsFor to getAllOperatorsFor  to make the distinction clearer.", "in_reply_to_id": 200312978}