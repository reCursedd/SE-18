{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200527022", "pull_request_review_id": 134861508, "id": 200527022, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDUyNzAyMg==", "diff_hunk": "@@ -0,0 +1,251 @@\n+#include \"ATen/ATen.h\"\n+#include \"torch/csrc/jit/script/lexer.h\"\n+#include \"torch/csrc/jit/script/tree.h\"\n+#include \"torch/csrc/jit/aten_schema.h\"\n+#include \"torch/csrc/jit/tensor_conversions.h\"\n+#include \"torch/csrc/jit/script/error_report.h\"\n+\n+namespace torch { namespace jit {\n+\n+namespace script {\n+struct SchemaParser {\n+  SchemaParser(const std::string& str)\n+  : L(str) {}\n+\n+  FunctionSchema parseDeclaration() {\n+    auto name = L.expect(TK_IDENT).text();\n+    std::vector<Argument> arguments;\n+    std::vector<Argument> returns;\n+    kwarg_only = false;\n+    parseList('(', ',', ')', arguments, &SchemaParser::parseArgument);\n+    L.expect(TK_ARROW);\n+    if(L.cur().kind == '(') {\n+      parseList('(', ',', ')', returns, &SchemaParser::parseReturn);\n+    } else {\n+      parseReturn(returns);\n+    }\n+    return FunctionSchema { name, arguments, returns };\n+  }\n+\n+  std::vector<FunctionSchema> parseDeclarations() {\n+    std::vector<FunctionSchema> results;\n+    do {\n+      results.push_back(parseDeclaration());\n+    } while(L.nextIf(TK_NEWLINE));\n+    L.expect(TK_EOF);\n+    return results;\n+  }\n+\n+  TreeRef parseIdent() {\n+    return String::create(L.expect(TK_IDENT).text());\n+  }\n+  TypePtr parseBaseType() {\n+    static std::unordered_map<std::string, TypePtr> type_map = {\n+      {\"Tensor\", DynamicType::get() },\n+      {\"Generator\", DynamicType::get() },\n+      {\"ScalarType\", IntType::get() },\n+      {\"Layout\", IntType::get() },\n+      {\"Device\", ListType::ofInts() },\n+      {\"Scalar\", NumberType::get() },\n+    };\n+    switch(L.cur().kind) {\n+      case TK_FLOAT:\n+        L.next();\n+        return FloatType::get();\n+      case TK_INT:\n+      case TK_BOOL: // TODO: add separate bool type\n+        L.next();\n+        return IntType::get();\n+      default:\n+        auto tok = L.expect(TK_IDENT);\n+        auto text = tok.text();\n+        auto it = type_map.find(text);\n+        if(it == type_map.end())\n+          throw ErrorReport(tok.range) << \"unknown type specifier\";\n+        return it->second;\n+    }\n+  }\n+  void parseType(Argument& arg) {\n+    arg.type = parseBaseType();\n+    if(L.nextIf('[')) {\n+      arg.type = std::make_shared<ListType>(arg.type);\n+      if(L.cur().kind == TK_NUMBER) {\n+        arg.N = std::stoll(L.next().text());\n+      }\n+      L.expect(']');\n+    }\n+  }\n+\n+  void parseArgument(std::vector<Argument>& arguments) {\n+    // varargs\n+    if(L.nextIf('*')) {\n+      kwarg_only = true;\n+      return;\n+    }\n+    Argument arg;\n+    parseType(arg);\n+\n+    // nullability is ignored for now, since the JIT never cares about it\n+    L.nextIf('?');\n+    arg.name = L.expect(TK_IDENT).text();\n+    if(L.nextIf('=')) {\n+      parseDefaultValue(arg);\n+    }\n+    arg.kwarg_only = kwarg_only;\n+    arguments.push_back(std::move(arg));\n+  }\n+  void parseReturn(std::vector<Argument>& args) {\n+    Argument arg(\"ret\" + std::to_string(args.size()));\n+    parseType(arg);\n+    args.push_back(std::move(arg));\n+  }\n+  at::Tensor parseSingleConstant(TypeKind kind) {\n+    switch(L.cur().kind) {\n+      case TK_TRUE:\n+        L.next();\n+        return one;\n+      case TK_FALSE:\n+        L.next();\n+        return zero;\n+      case TK_FLOAT:\n+        L.next();\n+        return as_tensor(static_cast<int64_t>(at::kFloat));\n+      case TK_IDENT: {\n+        auto tok = L.next();\n+        auto text = tok.text();\n+        if(\"cpu\" == text) {\n+          return as_tensor(static_cast<int64_t>(at::Device::Type::CPU));\n+        } else if(\"strided\" == text) {\n+          return as_tensor(static_cast<int64_t>(at::kStrided));\n+        } else {\n+          throw ErrorReport() << \"invalid numeric default value\";\n+        }\n+      } default:\n+        std::string n;\n+        if(L.nextIf('-'))\n+          n = \"-\" + L.expect(TK_NUMBER).text();\n+        else\n+          n = L.expect(TK_NUMBER).text();\n+        if(kind == TypeKind::FloatType || n.find(\".\") != std::string::npos || n.find(\"e\") != std::string::npos) {\n+          return at::full({}, std::stod(n), at::kDouble); // float?\n+        } else {\n+          int64_t v = std::stoll(n);\n+          return at::full({}, v, at::kLong);\n+        }\n+    }\n+  }\n+  at::Tensor parseConstantList(TypeKind kind) {\n+    auto tok = L.expect('[');\n+    std::vector<at::Tensor> vs;\n+    if(L.cur().kind != ']') {\n+      do {\n+        vs.push_back(parseSingleConstant(kind));\n+      } while(L.nextIf(','));\n+    }\n+    L.expect(']');\n+    if(vs.size() == 0) {\n+      switch(kind) {\n+        case TypeKind::FloatType:\n+          return at::empty({}, at::kFloat);\n+        case TypeKind::IntType:\n+          return at::empty({}, at::kLong);\n+        default:\n+          throw ErrorReport(tok) << \"empty lists are only supported for float or int types.\";\n+      }\n+    }\n+    return at::stack(vs);\n+  }\n+  at::Tensor parseTensorDefault(const SourceRange& range) {\n+    if(\"None\" == L.expect(TK_IDENT).text()) {\n+      return at::Tensor();\n+    } else {\n+      throw ErrorReport(range) << \"invalid tensor default value\";\n+    }\n+  }\n+  void parseDefaultValue(Argument& arg) {\n+    auto range = L.cur().range;\n+    switch(arg.type->kind()) {\n+      case TypeKind::DynamicType: {\n+        arg.default_value = parseTensorDefault(range);\n+      }  break;\n+      case TypeKind::NumberType:\n+      case TypeKind::IntType:\n+      case TypeKind::FloatType:\n+        arg.default_value = parseSingleConstant(arg.type->kind());\n+        break;\n+      case TypeKind::ListType: {\n+        auto elem_kind = arg.type->cast<ListType>()->getElementType();\n+        if(L.cur().kind == TK_IDENT) {\n+          arg.default_value = parseTensorDefault(range);\n+        } else if(arg.N && L.cur().kind != '[') {\n+          arg.default_value = parseSingleConstant(elem_kind->kind()).expand({*arg.N});\n+        } else {\n+          arg.default_value = parseConstantList(elem_kind->kind());\n+        }\n+      } break;\n+      default:\n+        throw ErrorReport(range) << \"unexpected type, file a bug report\";\n+    }\n+  }\n+\n+  template<typename T>\n+  void parseList(int begin, int sep, int end, std::vector<T>& result, void (SchemaParser::*parse)(std::vector<T>&)) {\n+    auto r = L.cur().range;\n+    if (begin != TK_NOTHING)\n+      L.expect(begin);\n+    if (L.cur().kind != end) {\n+      do {\n+        (this->*parse)(result);\n+      } while (L.nextIf(sep));\n+    }\n+    if (end != TK_NOTHING)\n+      L.expect(end);\n+  }\n+  Lexer L;\n+  bool kwarg_only;\n+  at::Tensor one = at::full({}, 1, at::kLong);\n+  at::Tensor zero = at::full({}, 0, at::kLong);\n+};\n+}\n+\n+using SchemaMap = std::unordered_map<std::string, std::vector<FunctionSchema>>;\n+\n+// defined in aten_schema_declarations.cpp\n+extern const char * schema_declarations;\n+\n+std::vector<FunctionSchema> createOperatorSchemas() {\n+  return script::SchemaParser(schema_declarations).parseDeclarations();\n+}\n+\n+std::vector<FunctionSchema> & getOperatorSchemas() {\n+  static std::vector<FunctionSchema> schema = createOperatorSchemas();\n+  return schema;\n+}\n+\n+static SchemaMap createSchemaMap() {\n+  auto& schemas = getOperatorSchemas();\n+  SchemaMap result;\n+  for(auto & schema : schemas) {\n+    auto it = result.find(schema.name);\n+    if(it == result.end()) {\n+      it = result.insert({schema.name, {}}).first;\n+    }\n+    it->second.push_back(std::move(schema));", "path": "torch/csrc/jit/aten_schema.cpp", "position": null, "original_position": 233, "commit_id": "1efd5a0aff5427596ececb2c851b1ef6291a707b", "original_commit_id": "717abe9074bdb912aef72a8e19122fa4a6c3d89d", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "I fixed this in `operator.cpp` where this code ended up.", "created_at": "2018-07-06T01:03:46Z", "updated_at": "2018-11-23T15:46:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/8885#discussion_r200527022", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8885", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/200527022"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8885#discussion_r200527022"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8885"}}, "body_html": "<p>I fixed this in <code>operator.cpp</code> where this code ended up.</p>", "body_text": "I fixed this in operator.cpp where this code ended up.", "in_reply_to_id": 200143857}