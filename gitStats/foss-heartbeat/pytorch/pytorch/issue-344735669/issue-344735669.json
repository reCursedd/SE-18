{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9869", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9869/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9869/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9869/events", "html_url": "https://github.com/pytorch/pytorch/issues/9869", "id": 344735669, "node_id": "MDU6SXNzdWUzNDQ3MzU2Njk=", "number": 9869, "title": "RuntimeError: Expected object of type Variable[torch.FloatTensor] but found type Variable[torch.cuda.FloatTensor] for argument #1 'mat2'", "user": {"login": "xhran2010", "id": 34118915, "node_id": "MDQ6VXNlcjM0MTE4OTE1", "avatar_url": "https://avatars0.githubusercontent.com/u/34118915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xhran2010", "html_url": "https://github.com/xhran2010", "followers_url": "https://api.github.com/users/xhran2010/followers", "following_url": "https://api.github.com/users/xhran2010/following{/other_user}", "gists_url": "https://api.github.com/users/xhran2010/gists{/gist_id}", "starred_url": "https://api.github.com/users/xhran2010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xhran2010/subscriptions", "organizations_url": "https://api.github.com/users/xhran2010/orgs", "repos_url": "https://api.github.com/users/xhran2010/repos", "events_url": "https://api.github.com/users/xhran2010/events{/privacy}", "received_events_url": "https://api.github.com/users/xhran2010/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-26T08:00:00Z", "updated_at": "2018-07-27T03:57:19Z", "closed_at": "2018-07-27T03:57:19Z", "author_association": "NONE", "body_html": "<p>Hey guys! When I'm running a pytorch code:<br>\n`class BatchRNN(nn.Module):<br>\ndef <strong>init</strong>(self, input_size, hidden_size, rnn_type=nn.LSTM,<br>\nbidirectional=False, batch_norm=True, dropout = 0.1):<br>\nsuper(BatchRNN, self).<strong>init</strong>()<br>\nself.input_size = input_size<br>\nself.hidden_size = hidden_size<br>\nself.bidirectional = bidirectional<br>\nself.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)).cuda() if batch_norm else None<br>\nself.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,<br>\nbidirectional=bidirectional, dropout = dropout, bias=False,batch_first=True).cuda()</p>\n<p>def forward(self, x):<br>\nif self.batch_norm is not None:<br>\nx = self.batch_norm(x).cuda()<br>\nx, _ = self.rnn(x)<br>\nself.rnn.cuda().flatten_parameters()<br>\nreturn x</p>\n<p>class CTC_RNN(nn.Module):<br>\ndef <strong>init</strong>(self, rnn_input_size, rnn_hidden_size, rnn_layers=1,<br>\nrnn_type=nn.LSTM, bidirectional=True,<br>\nbatch_norm=True, num_class=1232, drop_out = 0.1):<br>\nsuper(CTC_RNN, self).<strong>init</strong>()<br>\nself.rnn_input_size = rnn_input_size<br>\nself.rnn_hidden_size = rnn_hidden_size<br>\nself.rnn_layers = rnn_layers<br>\nself.rnn_type = rnn_type<br>\nself.num_class = num_class<br>\nself.num_directions = 2 if bidirectional else 1<br>\n#cnn<br>\nself.conv1_cnn=nn.Conv2d(1,256,(2,rnn_input_size)).cuda()<br>\nself.conv2_cnn=nn.Conv2d(1,256,(2,256)).cuda()<br>\nself.fc_cnn=nn.Linear(256,rnn_hidden_size).cuda()<br>\nself.softmax_cnn=torch.nn.Softmax().cuda()<br>\nrnns = []<br>\nrnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size,<br>\nrnn_type=rnn_type, bidirectional=bidirectional,<br>\nbatch_norm=False).cuda()<br>\nrnns.append(('0', rnn))<br>\nfor i in range(rnn_layers-1):<br>\nrnn = BatchRNN(input_size=self.num_directions*rnn_hidden_size,<br>\nhidden_size=rnn_hidden_size, rnn_type=rnn_type,<br>\nbidirectional=bidirectional, dropout = drop_out, batch_norm = batch_norm).cuda()<br>\nrnns.append(('%d' % (i+1), rnn))</p>\n<pre><code>self.rnns = nn.Sequential(OrderedDict(rnns)).cuda()\n\nif batch_norm :\n    fc = nn.Sequential(nn.BatchNorm1d(self.num_directions*rnn_hidden_size).cuda(),\n                nn.Linear(self.num_directions*rnn_hidden_size, rnn_hidden_size, bias=False).cuda()).cuda()\nelse:\n    fc = nn.Linear(self.num_directions*rnn_hidden_size, rnn_hidden_size, bias=False).cuda()\nself.fc = SequenceWise(fc).cuda()\nself.inference_log_softmax = InferenceBatchLogSoftmax().cuda()\nself.softmax=torch.nn.Softmax().cuda()\n#self.inference_softmax = InferenceBatchSoftmax()  \n#tddf fusion lstm\nself.tddf_lstm=nn.LSTMCell(rnn_hidden_size,rnn_hidden_size).cuda()\nself.fc_s=nn.Linear(rnn_hidden_size,2,bias=True).cuda()\nself.fc_c=nn.Linear(rnn_hidden_size,2,bias=True) .cuda()\nself.hx=Variable(torch.zeros(100,rnn_hidden_size),requires_grad=True).cuda()\nself.cx=Variable(torch.zeros(100,rnn_hidden_size),requires_grad=True).cuda()\nself.fc_tddf=nn.Linear(rnn_hidden_size,num_class).cuda()\n</code></pre>\n<p>def forward(self, x,y):  #x: packed padded sequence [x.data: the origin data] [x.batch_sizes: the batch_size of each frames] [x_len: type:list not torch.IntTensor]<br>\n#ipdb.set_trace()<br>\nx = self.rnns(x)<br>\nx = self.fc(x)<br>\nx = self.inference_log_softmax(x)#(max_step,batch_size,dim)<br>\nx=x.transpose(0,1)<br>\n#x = self.inference_softmax(x)<br>\ny=self.conv1_cnn(y)<br>\n#banben 2_relu<br>\ny=F.relu(y)<br>\ny=self.conv2_cnn(torch.transpose(y,1,3))<br>\ny=F.relu(y)<br>\ny=self.fc_cnn(torch.transpose(y,1,3))#(batch_size,1,max_step,dim)<br>\n#y=torch.transpose(y,1,3)<br>\ny=y.view(100,-1,self.rnn_hidden_size)<br>\ny=torch.transpose(y,0,1)</p>\n<pre><code>output=Variable(torch.zeros(x.cpu().data.numpy().shape[0],100,self.rnn_hidden_size)).cuda()\nfor i in range(x.cpu().data.numpy().shape[0]):\n    #ipdb.set_trace()\n    if i==0:\n        st=F.softmax(self.fc_s(self.hx))\n        ct=F.sigmoid(self.fc_c(self.hx))\n        at=st*ct\n        tddf_input_i_x=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i_y=y[i]*at[:,1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i=tddf_input_i_x+tddf_input_i_y\n        hx,cx=self.tddf_lstm(tddf_input_i,(self.hx,self.cx))\n        output[i]=hx\n    else:\n        st=F.softmax(self.fc_s(hx))\n        ct=F.sigmoid(self.fc_c(hx))\n        at=st*ct\n        tddf_input_i_x=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i_y=y[i]*at[:,1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i=tddf_input_i_x+tddf_input_i_y\n        #tddf_input_i=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)+y[i]*at[:1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        hx,cx=self.tddf_lstm(tddf_input_i,(hx,cx))\n        output[i]=hx\nreturn self.fc_tddf(output)`\n</code></pre>\n<p>I got an error shows like:<br>\n<code>Traceback (most recent call last): File \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 358, in &lt;module&gt; main() File \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 353, in main train() File \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 256, in train probs = model(feats,feats_cnn).cuda(async=True) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/PycharmProjects/TDDF/model_tddf.py\", line 167, in forward x = self.rnns(x) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 67, in forward input = module(input) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/PycharmProjects/TDDF/model_tddf.py\", line 115, in forward x, _ = self.rnn(x).cuda() File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 204, in forward output, hidden = func(input, self.all_weights, hx) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 385, in forward return func(input, *fargs, **fkwargs) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 245, in forward nexth, output = func(input, hidden, weight) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 85, in forward hy, output = inner(input, hidden[l], weight[l]) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 158, in forward hidden = inner(step_input, hidden, *weight) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 32, in LSTMCell gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 837, in linear output = input.matmul(weight.t()) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 386, in matmul return torch.matmul(self, other) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/functional.py\", line 174, in matmul return torch.mm(tensor1, tensor2) RuntimeError: Expected object of type Variable[torch.FloatTensor] but found type Variable[torch.cuda.FloatTensor] for argument #1 'mat2'</code></p>\n<p>I'd appreciate it if someone could help me out!!!~~ For the record, this seems to work on pytorch0.2.0(I'm using 0.3.0 now)</p>", "body_text": "Hey guys! When I'm running a pytorch code:\n`class BatchRNN(nn.Module):\ndef init(self, input_size, hidden_size, rnn_type=nn.LSTM,\nbidirectional=False, batch_norm=True, dropout = 0.1):\nsuper(BatchRNN, self).init()\nself.input_size = input_size\nself.hidden_size = hidden_size\nself.bidirectional = bidirectional\nself.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)).cuda() if batch_norm else None\nself.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\nbidirectional=bidirectional, dropout = dropout, bias=False,batch_first=True).cuda()\ndef forward(self, x):\nif self.batch_norm is not None:\nx = self.batch_norm(x).cuda()\nx, _ = self.rnn(x)\nself.rnn.cuda().flatten_parameters()\nreturn x\nclass CTC_RNN(nn.Module):\ndef init(self, rnn_input_size, rnn_hidden_size, rnn_layers=1,\nrnn_type=nn.LSTM, bidirectional=True,\nbatch_norm=True, num_class=1232, drop_out = 0.1):\nsuper(CTC_RNN, self).init()\nself.rnn_input_size = rnn_input_size\nself.rnn_hidden_size = rnn_hidden_size\nself.rnn_layers = rnn_layers\nself.rnn_type = rnn_type\nself.num_class = num_class\nself.num_directions = 2 if bidirectional else 1\n#cnn\nself.conv1_cnn=nn.Conv2d(1,256,(2,rnn_input_size)).cuda()\nself.conv2_cnn=nn.Conv2d(1,256,(2,256)).cuda()\nself.fc_cnn=nn.Linear(256,rnn_hidden_size).cuda()\nself.softmax_cnn=torch.nn.Softmax().cuda()\nrnns = []\nrnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size,\nrnn_type=rnn_type, bidirectional=bidirectional,\nbatch_norm=False).cuda()\nrnns.append(('0', rnn))\nfor i in range(rnn_layers-1):\nrnn = BatchRNN(input_size=self.num_directions*rnn_hidden_size,\nhidden_size=rnn_hidden_size, rnn_type=rnn_type,\nbidirectional=bidirectional, dropout = drop_out, batch_norm = batch_norm).cuda()\nrnns.append(('%d' % (i+1), rnn))\nself.rnns = nn.Sequential(OrderedDict(rnns)).cuda()\n\nif batch_norm :\n    fc = nn.Sequential(nn.BatchNorm1d(self.num_directions*rnn_hidden_size).cuda(),\n                nn.Linear(self.num_directions*rnn_hidden_size, rnn_hidden_size, bias=False).cuda()).cuda()\nelse:\n    fc = nn.Linear(self.num_directions*rnn_hidden_size, rnn_hidden_size, bias=False).cuda()\nself.fc = SequenceWise(fc).cuda()\nself.inference_log_softmax = InferenceBatchLogSoftmax().cuda()\nself.softmax=torch.nn.Softmax().cuda()\n#self.inference_softmax = InferenceBatchSoftmax()  \n#tddf fusion lstm\nself.tddf_lstm=nn.LSTMCell(rnn_hidden_size,rnn_hidden_size).cuda()\nself.fc_s=nn.Linear(rnn_hidden_size,2,bias=True).cuda()\nself.fc_c=nn.Linear(rnn_hidden_size,2,bias=True) .cuda()\nself.hx=Variable(torch.zeros(100,rnn_hidden_size),requires_grad=True).cuda()\nself.cx=Variable(torch.zeros(100,rnn_hidden_size),requires_grad=True).cuda()\nself.fc_tddf=nn.Linear(rnn_hidden_size,num_class).cuda()\n\ndef forward(self, x,y):  #x: packed padded sequence [x.data: the origin data] [x.batch_sizes: the batch_size of each frames] [x_len: type:list not torch.IntTensor]\n#ipdb.set_trace()\nx = self.rnns(x)\nx = self.fc(x)\nx = self.inference_log_softmax(x)#(max_step,batch_size,dim)\nx=x.transpose(0,1)\n#x = self.inference_softmax(x)\ny=self.conv1_cnn(y)\n#banben 2_relu\ny=F.relu(y)\ny=self.conv2_cnn(torch.transpose(y,1,3))\ny=F.relu(y)\ny=self.fc_cnn(torch.transpose(y,1,3))#(batch_size,1,max_step,dim)\n#y=torch.transpose(y,1,3)\ny=y.view(100,-1,self.rnn_hidden_size)\ny=torch.transpose(y,0,1)\noutput=Variable(torch.zeros(x.cpu().data.numpy().shape[0],100,self.rnn_hidden_size)).cuda()\nfor i in range(x.cpu().data.numpy().shape[0]):\n    #ipdb.set_trace()\n    if i==0:\n        st=F.softmax(self.fc_s(self.hx))\n        ct=F.sigmoid(self.fc_c(self.hx))\n        at=st*ct\n        tddf_input_i_x=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i_y=y[i]*at[:,1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i=tddf_input_i_x+tddf_input_i_y\n        hx,cx=self.tddf_lstm(tddf_input_i,(self.hx,self.cx))\n        output[i]=hx\n    else:\n        st=F.softmax(self.fc_s(hx))\n        ct=F.sigmoid(self.fc_c(hx))\n        at=st*ct\n        tddf_input_i_x=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i_y=y[i]*at[:,1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        tddf_input_i=tddf_input_i_x+tddf_input_i_y\n        #tddf_input_i=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)+y[i]*at[:1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\n        hx,cx=self.tddf_lstm(tddf_input_i,(hx,cx))\n        output[i]=hx\nreturn self.fc_tddf(output)`\n\nI got an error shows like:\nTraceback (most recent call last): File \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 358, in <module> main() File \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 353, in main train() File \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 256, in train probs = model(feats,feats_cnn).cuda(async=True) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/PycharmProjects/TDDF/model_tddf.py\", line 167, in forward x = self.rnns(x) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 67, in forward input = module(input) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/PycharmProjects/TDDF/model_tddf.py\", line 115, in forward x, _ = self.rnn(x).cuda() File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__ result = self.forward(*input, **kwargs) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 204, in forward output, hidden = func(input, self.all_weights, hx) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 385, in forward return func(input, *fargs, **fkwargs) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 245, in forward nexth, output = func(input, hidden, weight) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 85, in forward hy, output = inner(input, hidden[l], weight[l]) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 158, in forward hidden = inner(step_input, hidden, *weight) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 32, in LSTMCell gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 837, in linear output = input.matmul(weight.t()) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 386, in matmul return torch.matmul(self, other) File \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/functional.py\", line 174, in matmul return torch.mm(tensor1, tensor2) RuntimeError: Expected object of type Variable[torch.FloatTensor] but found type Variable[torch.cuda.FloatTensor] for argument #1 'mat2'\nI'd appreciate it if someone could help me out!!!~~ For the record, this seems to work on pytorch0.2.0(I'm using 0.3.0 now)", "body": "Hey guys! When I'm running a pytorch code:\r\n`class BatchRNN(nn.Module):\r\ndef __init__(self, input_size, hidden_size, rnn_type=nn.LSTM, \r\n        bidirectional=False, batch_norm=True, dropout = 0.1):\r\n    super(BatchRNN, self).__init__()\r\n    self.input_size = input_size\r\n    self.hidden_size = hidden_size\r\n    self.bidirectional = bidirectional\r\n    self.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)).cuda() if batch_norm else None\r\n    self.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\r\n                        bidirectional=bidirectional, dropout = dropout, bias=False,batch_first=True).cuda()\r\n\r\ndef forward(self, x):\r\n    if self.batch_norm is not None:\r\n        x = self.batch_norm(x).cuda()\r\n    x, _ = self.rnn(x)\r\n    self.rnn.cuda().flatten_parameters()\r\n    return x\r\n\r\nclass CTC_RNN(nn.Module):\r\ndef __init__(self, rnn_input_size, rnn_hidden_size, rnn_layers=1,\r\n        rnn_type=nn.LSTM, bidirectional=True, \r\n        batch_norm=True, num_class=1232, drop_out = 0.1):\r\n    super(CTC_RNN, self).__init__()\r\n    self.rnn_input_size = rnn_input_size\r\n    self.rnn_hidden_size = rnn_hidden_size\r\n    self.rnn_layers = rnn_layers\r\n    self.rnn_type = rnn_type\r\n    self.num_class = num_class\r\n    self.num_directions = 2 if bidirectional else 1\r\n    #cnn\r\n    self.conv1_cnn=nn.Conv2d(1,256,(2,rnn_input_size)).cuda()\r\n    self.conv2_cnn=nn.Conv2d(1,256,(2,256)).cuda()\r\n    self.fc_cnn=nn.Linear(256,rnn_hidden_size).cuda()\r\n    self.softmax_cnn=torch.nn.Softmax().cuda()\r\n    rnns = []\r\n    rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, \r\n                    rnn_type=rnn_type, bidirectional=bidirectional, \r\n                    batch_norm=False).cuda()\r\n    rnns.append(('0', rnn))\r\n    for i in range(rnn_layers-1):\r\n        rnn = BatchRNN(input_size=self.num_directions*rnn_hidden_size, \r\n                hidden_size=rnn_hidden_size, rnn_type=rnn_type, \r\n                bidirectional=bidirectional, dropout = drop_out, batch_norm = batch_norm).cuda()\r\n        rnns.append(('%d' % (i+1), rnn))\r\n\r\n    self.rnns = nn.Sequential(OrderedDict(rnns)).cuda()\r\n\r\n    if batch_norm :\r\n        fc = nn.Sequential(nn.BatchNorm1d(self.num_directions*rnn_hidden_size).cuda(),\r\n                    nn.Linear(self.num_directions*rnn_hidden_size, rnn_hidden_size, bias=False).cuda()).cuda()\r\n    else:\r\n        fc = nn.Linear(self.num_directions*rnn_hidden_size, rnn_hidden_size, bias=False).cuda()\r\n    self.fc = SequenceWise(fc).cuda()\r\n    self.inference_log_softmax = InferenceBatchLogSoftmax().cuda()\r\n    self.softmax=torch.nn.Softmax().cuda()\r\n    #self.inference_softmax = InferenceBatchSoftmax()  \r\n    #tddf fusion lstm\r\n    self.tddf_lstm=nn.LSTMCell(rnn_hidden_size,rnn_hidden_size).cuda()\r\n    self.fc_s=nn.Linear(rnn_hidden_size,2,bias=True).cuda()\r\n    self.fc_c=nn.Linear(rnn_hidden_size,2,bias=True) .cuda()\r\n    self.hx=Variable(torch.zeros(100,rnn_hidden_size),requires_grad=True).cuda()\r\n    self.cx=Variable(torch.zeros(100,rnn_hidden_size),requires_grad=True).cuda()\r\n    self.fc_tddf=nn.Linear(rnn_hidden_size,num_class).cuda()\r\ndef forward(self, x,y):  #x: packed padded sequence [x.data: the origin data] [x.batch_sizes: the batch_size of each frames] [x_len: type:list not torch.IntTensor] \r\n    #ipdb.set_trace()   \r\n    x = self.rnns(x)\r\n    x = self.fc(x)\r\n    x = self.inference_log_softmax(x)#(max_step,batch_size,dim)\r\n    x=x.transpose(0,1)\r\n    #x = self.inference_softmax(x)\r\n    y=self.conv1_cnn(y)\r\n    #banben 2_relu\r\n    y=F.relu(y)\r\n    y=self.conv2_cnn(torch.transpose(y,1,3))\r\n    y=F.relu(y)\r\n    y=self.fc_cnn(torch.transpose(y,1,3))#(batch_size,1,max_step,dim)\r\n    #y=torch.transpose(y,1,3)\r\n    y=y.view(100,-1,self.rnn_hidden_size)\r\n    y=torch.transpose(y,0,1)\r\n\r\n    output=Variable(torch.zeros(x.cpu().data.numpy().shape[0],100,self.rnn_hidden_size)).cuda()\r\n    for i in range(x.cpu().data.numpy().shape[0]):\r\n        #ipdb.set_trace()\r\n        if i==0:\r\n            st=F.softmax(self.fc_s(self.hx))\r\n            ct=F.sigmoid(self.fc_c(self.hx))\r\n            at=st*ct\r\n            tddf_input_i_x=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\r\n            tddf_input_i_y=y[i]*at[:,1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\r\n            tddf_input_i=tddf_input_i_x+tddf_input_i_y\r\n            hx,cx=self.tddf_lstm(tddf_input_i,(self.hx,self.cx))\r\n            output[i]=hx\r\n        else:\r\n            st=F.softmax(self.fc_s(hx))\r\n            ct=F.sigmoid(self.fc_c(hx))\r\n            at=st*ct\r\n            tddf_input_i_x=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\r\n            tddf_input_i_y=y[i]*at[:,1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\r\n            tddf_input_i=tddf_input_i_x+tddf_input_i_y\r\n            #tddf_input_i=x[i]*at[:,0].contiguous().view(100,1).expand(100,self.rnn_hidden_size)+y[i]*at[:1].contiguous().view(100,1).expand(100,self.rnn_hidden_size)\r\n            hx,cx=self.tddf_lstm(tddf_input_i,(hx,cx))\r\n            output[i]=hx\r\n    return self.fc_tddf(output)`\r\n\r\nI got an error shows like:\r\n`Traceback (most recent call last):\r\nFile \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 358, in <module>\r\nmain()\r\nFile \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 353, in main\r\ntrain()\r\nFile \"/home/xinhaoran/PycharmProjects/TDDF/PH_ctc_cnn_tddf.py\", line 256, in train\r\nprobs = model(feats,feats_cnn).cuda(async=True)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/xinhaoran/PycharmProjects/TDDF/model_tddf.py\", line 167, in forward\r\nx = self.rnns(x)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 67, in forward\r\ninput = module(input)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/xinhaoran/PycharmProjects/TDDF/model_tddf.py\", line 115, in forward\r\nx, _ = self.rnn(x).cuda()\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\nresult = self.forward(*input, **kwargs)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\", line 204, in forward\r\noutput, hidden = func(input, self.all_weights, hx)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 385, in forward\r\nreturn func(input, *fargs, **fkwargs)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 245, in forward\r\nnexth, output = func(input, hidden, weight)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 85, in forward\r\nhy, output = inner(input, hidden[l], weight[l])\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 158, in forward\r\nhidden = inner(step_input, hidden, *weight)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 32, in LSTMCell\r\ngates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 837, in linear\r\noutput = input.matmul(weight.t())\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 386, in matmul\r\nreturn torch.matmul(self, other)\r\nFile \"/home/xinhaoran/anaconda3/lib/python3.6/site-packages/torch/functional.py\", line 174, in matmul\r\nreturn torch.mm(tensor1, tensor2)\r\nRuntimeError: Expected object of type Variable[torch.FloatTensor] but found type Variable[torch.cuda.FloatTensor] for argument #1 'mat2'`\r\n\r\nI'd appreciate it if someone could help me out!!!~~ For the record, this seems to work on pytorch0.2.0(I'm using 0.3.0 now)"}