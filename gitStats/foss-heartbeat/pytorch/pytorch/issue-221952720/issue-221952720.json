{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1267", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1267/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1267/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1267/events", "html_url": "https://github.com/pytorch/pytorch/issues/1267", "id": 221952720, "node_id": "MDU6SXNzdWUyMjE5NTI3MjA=", "number": 1267, "title": "Handling the error message of cudaGetDeviceCount", "user": {"login": "wddabc", "id": 5722427, "node_id": "MDQ6VXNlcjU3MjI0Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5722427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wddabc", "html_url": "https://github.com/wddabc", "followers_url": "https://api.github.com/users/wddabc/followers", "following_url": "https://api.github.com/users/wddabc/following{/other_user}", "gists_url": "https://api.github.com/users/wddabc/gists{/gist_id}", "starred_url": "https://api.github.com/users/wddabc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wddabc/subscriptions", "organizations_url": "https://api.github.com/users/wddabc/orgs", "repos_url": "https://api.github.com/users/wddabc/repos", "events_url": "https://api.github.com/users/wddabc/events{/privacy}", "received_events_url": "https://api.github.com/users/wddabc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2017-04-15T14:49:38Z", "updated_at": "2017-04-23T12:45:28Z", "closed_at": "2017-04-23T12:45:28Z", "author_association": "NONE", "body_html": "<p>Summary of the suggestion: The correct behavior of <code>is_available()</code> is it should return False whenever <code>cudaGetDeviceCount(&amp;count) != cudaSuccess || count == 0</code>, also for the check in <code>start_threads</code></p>\n<p>Details:<br>\nThis issue is strongly related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"218478856\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1154\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1154/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1154\">#1154</a>, but I think is should be more appropriate as a new issue. A snapshot of this issue is the pytorch crashes when the <code>cudaGetDeviceCount</code> throw some unknown exceptions. This unknown behavior happens when the cuda or device is improperly installed. For example, the author of that issue, get things fixed by uninstalling an old version of cuda.</p>\n<p>But I'm wondering whether Pytorch should handle this by backoff to CPU whenever it happens. The current code looks inconsistent for handling this. For example, in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L24-L34\">https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L24-L34</a><br>\nIt catches two types of exceptions, the <code>cudaErrorInsufficientDriver</code> and <code>cudaErrorNoDevice</code>, which seems a little bit limited. (A side note: catching <code>cudaErrorNoDevice</code> by string matching at <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L32\">https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L32</a> looks a little bit hacky to me :-) )</p>\n<p>I think the correct behavior of <code>is_available()</code> is it should return False whenever <code>cudaGetDeviceCount(&amp;count) != cudaSuccess || count == 0</code>. This will make the code more robust to the unexpected error exception throw by some incorrectly configure CUDA. For my own case, the the CUDA returns <code>cudaErrorUnknown</code> that crashes <code>is_available()</code>. I asked my admins to fix, but they think it is extensive and unnecessary \"because the CPU nodes don't support gpu jobs, the particular errors that they throw are irrelevant.\"</p>\n<p>The same thing happens at (as far as I can tell) <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L353-L361\">https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L353-L361</a> as well, where it only checks <code>cudaErrorInsufficientDriver</code>. (somehow it doesn't crashes on <code>cudaErrorNoDevice</code>, for which I don't know why). I think it also should be a no-success-then-false behavior as I suggested in <code>is_available()</code>. A less important questions is why the cuda check happens here? If I've called <code>is_available()</code> before then call <code>.backward()</code>, Pytorch seems detect the device twice. Which looks unnecessary to me. This flag should be set when launching the pytorch I guess.</p>", "body_text": "Summary of the suggestion: The correct behavior of is_available() is it should return False whenever cudaGetDeviceCount(&count) != cudaSuccess || count == 0, also for the check in start_threads\nDetails:\nThis issue is strongly related to #1154, but I think is should be more appropriate as a new issue. A snapshot of this issue is the pytorch crashes when the cudaGetDeviceCount throw some unknown exceptions. This unknown behavior happens when the cuda or device is improperly installed. For example, the author of that issue, get things fixed by uninstalling an old version of cuda.\nBut I'm wondering whether Pytorch should handle this by backoff to CPU whenever it happens. The current code looks inconsistent for handling this. For example, in https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L24-L34\nIt catches two types of exceptions, the cudaErrorInsufficientDriver and cudaErrorNoDevice, which seems a little bit limited. (A side note: catching cudaErrorNoDevice by string matching at https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L32 looks a little bit hacky to me :-) )\nI think the correct behavior of is_available() is it should return False whenever cudaGetDeviceCount(&count) != cudaSuccess || count == 0. This will make the code more robust to the unexpected error exception throw by some incorrectly configure CUDA. For my own case, the the CUDA returns cudaErrorUnknown that crashes is_available(). I asked my admins to fix, but they think it is extensive and unnecessary \"because the CPU nodes don't support gpu jobs, the particular errors that they throw are irrelevant.\"\nThe same thing happens at (as far as I can tell) https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L353-L361 as well, where it only checks cudaErrorInsufficientDriver. (somehow it doesn't crashes on cudaErrorNoDevice, for which I don't know why). I think it also should be a no-success-then-false behavior as I suggested in is_available(). A less important questions is why the cuda check happens here? If I've called is_available() before then call .backward(), Pytorch seems detect the device twice. Which looks unnecessary to me. This flag should be set when launching the pytorch I guess.", "body": "Summary of the suggestion: The correct behavior of `is_available()` is it should return False whenever `cudaGetDeviceCount(&count) != cudaSuccess || count == 0`, also for the check in `start_threads`\r\n\r\nDetails:\r\nThis issue is strongly related to #1154, but I think is should be more appropriate as a new issue. A snapshot of this issue is the pytorch crashes when the `cudaGetDeviceCount` throw some unknown exceptions. This unknown behavior happens when the cuda or device is improperly installed. For example, the author of that issue, get things fixed by uninstalling an old version of cuda.\r\n\r\nBut I'm wondering whether Pytorch should handle this by backoff to CPU whenever it happens. The current code looks inconsistent for handling this. For example, in https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L24-L34\r\nIt catches two types of exceptions, the `cudaErrorInsufficientDriver` and `cudaErrorNoDevice`, which seems a little bit limited. (A side note: catching `cudaErrorNoDevice` by string matching at https://github.com/pytorch/pytorch/blob/master/torch/cuda/__init__.py#L32 looks a little bit hacky to me :-) ) \r\n\r\nI think the correct behavior of `is_available()` is it should return False whenever `cudaGetDeviceCount(&count) != cudaSuccess || count == 0`. This will make the code more robust to the unexpected error exception throw by some incorrectly configure CUDA. For my own case, the the CUDA returns `cudaErrorUnknown` that crashes `is_available()`. I asked my admins to fix, but they think it is extensive and unnecessary \"because the CPU nodes don't support gpu jobs, the particular errors that they throw are irrelevant.\"\r\n\r\nThe same thing happens at (as far as I can tell) https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L353-L361 as well, where it only checks `cudaErrorInsufficientDriver`. (somehow it doesn't crashes on `cudaErrorNoDevice`, for which I don't know why). I think it also should be a no-success-then-false behavior as I suggested in `is_available()`. A less important questions is why the cuda check happens here? If I've called `is_available()` before then call `.backward()`, Pytorch seems detect the device twice. Which looks unnecessary to me. This flag should be set when launching the pytorch I guess."}