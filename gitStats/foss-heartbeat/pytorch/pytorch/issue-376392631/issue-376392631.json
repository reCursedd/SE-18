{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13447", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13447/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13447/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13447/events", "html_url": "https://github.com/pytorch/pytorch/issues/13447", "id": 376392631, "node_id": "MDU6SXNzdWUzNzYzOTI2MzE=", "number": 13447, "title": "Support gathering nested lists in DataParallel ", "user": {"login": "nikhilweee", "id": 8774119, "node_id": "MDQ6VXNlcjg3NzQxMTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/8774119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikhilweee", "html_url": "https://github.com/nikhilweee", "followers_url": "https://api.github.com/users/nikhilweee/followers", "following_url": "https://api.github.com/users/nikhilweee/following{/other_user}", "gists_url": "https://api.github.com/users/nikhilweee/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikhilweee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikhilweee/subscriptions", "organizations_url": "https://api.github.com/users/nikhilweee/orgs", "repos_url": "https://api.github.com/users/nikhilweee/repos", "events_url": "https://api.github.com/users/nikhilweee/events{/privacy}", "received_events_url": "https://api.github.com/users/nikhilweee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-01T13:29:38Z", "updated_at": "2018-11-01T13:29:38Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n\n<p>The <a href=\"https://github.com/pytorch/pytorch/blob/ee628d64b9d2bc8ab26655982aa3a158859a99c0/torch/nn/parallel/scatter_gather.py#L46\">current implementation</a> of the <code>gather</code> function supports simple data structures like tensors and dictionaries but can not handle nested structures such as a list of lists.<br>\nHow about supporting them as well?</p>\n<h2>Motivation</h2>\n\n<p>Complex networks don't always return simple outputs such as tensors. Consider the following situation where the outputs from the forward pass of a question answering network is a dictionary with keys <code>loss</code>, <code>answers</code> and <code>qids</code>. Notice that <code>answers</code> and <code>qids</code> are a list of lists.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Output from forward pass</span>\n {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>: tensor(<span class=\"pl-c1\">8.6422</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">grad_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">&lt;</span>ThAddBackward<span class=\"pl-k\">&gt;</span>),\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>answers<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>July 4<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>American Independence<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>July 1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Canadian Independence<span class=\"pl-pds\">'</span></span>]],\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>qids<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfa5abq#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfa5abq#2<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c6ca8b089q#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c6ca8b089q#2<span class=\"pl-pds\">'</span></span>]}</pre></div>\n<p>While running this model over multiple GPUs using <code>nn.DataParallel</code>, the following is passed as input to the <a href=\"https://github.com/pytorch/pytorch/blob/ee628d64b9d2bc8ab26655982aa3a158859a99c0/torch/nn/parallel/scatter_gather.py#L46\">gather function</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Input to the `gather` function</span>\n[{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>: tensor(<span class=\"pl-c1\">8.6422</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">grad_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">&lt;</span>ThAddBackward<span class=\"pl-k\">&gt;</span>),\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>answers<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>July 4<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>American Independence<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>July 1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Canadian Independence<span class=\"pl-pds\">'</span></span>]],\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>qids<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfa5abq#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfa5abq#2<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c6ca8b089q#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c6ca8b089q#2<span class=\"pl-pds\">'</span></span>]},\n {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>: tensor(<span class=\"pl-c1\">8.7826</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">grad_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">&lt;</span>ThAddBackward<span class=\"pl-k\">&gt;</span>),\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>answers<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>September 16<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Mexican Independence<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>November 3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Panamanian Independence<span class=\"pl-pds\">'</span></span>]],\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>qids<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfcdefab#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfcdefab#2<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c623ef089q#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c623ef089q#2<span class=\"pl-pds\">'</span></span>]}]</pre></div>\n<p>Here's the expected output where the tensors and lists are concatenated together.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Expected behaviour</span>\n{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>: tensor([<span class=\"pl-c1\">8.6422</span>, <span class=\"pl-c1\">8.7826</span>], <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">grad_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">&lt;</span>ThAddBackward<span class=\"pl-k\">&gt;</span>),\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>answers<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>July 4<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>American Independence<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>July 1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Canadian Independence<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>September 16<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Mexican Independence<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>November 3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Panamanian Independence<span class=\"pl-pds\">'</span></span>]],\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>qids<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfa5abq#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfa5abq#2<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c6ca8b089q#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c6ca8b089q#2<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfcdefab#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C28fbfcdefab#2<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c623ef089q#1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>C2c623ef089q#2<span class=\"pl-pds\">'</span></span>]]}</pre></div>\n<p>But this is what happens right now. The <code>map</code> clause cannot handle nested lists.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Current behaviour</span>\n{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>: tensor([<span class=\"pl-c1\">8.6422</span>, <span class=\"pl-c1\">8.7826</span>], <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">grad_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">&lt;</span>ThAddBackward<span class=\"pl-k\">&gt;</span>),\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>answers<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2ad2445a7f60&gt;<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2ad2445a75f8&gt;<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2b6862d87358&gt;<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2b6862d87668&gt;<span class=\"pl-pds\">'</span></span>]],\n  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>qids<span class=\"pl-pds\">'</span></span>: [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2b6862d87668&gt;<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2b6862d875c0&gt;<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2b6862d87438&gt;<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;map object at 0x2b6862d87828&gt;<span class=\"pl-pds\">'</span></span>]]}</pre></div>\n<h2>Pitch</h2>\n\n<p>One solution is to explicitly handle merging of lists together. This basically boils down to adding an extra condition for handling lists in the <code>gather</code> function <a href=\"https://github.com/pytorch/pytorch/blob/ee628d64b9d2bc8ab26655982aa3a158859a99c0/torch/nn/parallel/scatter_gather.py#L62\">here</a>.</p>\n<div class=\"highlight highlight-source-python\"><pre> <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(out, <span class=\"pl-c1\">list</span>):\n     <span class=\"pl-k\">return</span> [item <span class=\"pl-k\">for</span> output <span class=\"pl-k\">in</span> outputs <span class=\"pl-k\">for</span> item <span class=\"pl-k\">in</span> output]</pre></div>\n<h2>Alternatives</h2>\n\n<p>One workaround is to avoid nested outputs from the forward pass in the first place. This is still a workaround and not a solution.</p>\n<h2>Additional context</h2>\n\n<p>I'm not sure if this is the right way to handle the larger problem of handling complex data structures.<br>\nFor example one can always come up with a more nested structure which might not work with the proposed solution either. Should pytorch allow the provision for custom <code>gather</code> and <code>scatter</code> functions as it has done with <code>collate_fn</code>?</p>", "body_text": "\ud83d\ude80 Feature\n\nThe current implementation of the gather function supports simple data structures like tensors and dictionaries but can not handle nested structures such as a list of lists.\nHow about supporting them as well?\nMotivation\n\nComplex networks don't always return simple outputs such as tensors. Consider the following situation where the outputs from the forward pass of a question answering network is a dictionary with keys loss, answers and qids. Notice that answers and qids are a list of lists.\n# Output from forward pass\n {'loss': tensor(8.6422, device='cuda:0', grad_fn=<ThAddBackward>),\n  'answers': [['July 4', 'American Independence'], ['July 1', 'Canadian Independence']],\n  'qids': [['C28fbfa5abq#1', 'C28fbfa5abq#2'], ['C2c6ca8b089q#1', 'C2c6ca8b089q#2']}\nWhile running this model over multiple GPUs using nn.DataParallel, the following is passed as input to the gather function\n# Input to the `gather` function\n[{'loss': tensor(8.6422, device='cuda:0', grad_fn=<ThAddBackward>),\n  'answers': [['July 4', 'American Independence'], ['July 1', 'Canadian Independence']],\n  'qids': [['C28fbfa5abq#1', 'C28fbfa5abq#2'], ['C2c6ca8b089q#1', 'C2c6ca8b089q#2']},\n {'loss': tensor(8.7826, device='cuda:0', grad_fn=<ThAddBackward>),\n  'answers': [['September 16', 'Mexican Independence'], ['November 3', 'Panamanian Independence']],\n  'qids': [['C28fbfcdefab#1', 'C28fbfcdefab#2'], ['C2c623ef089q#1', 'C2c623ef089q#2']}]\nHere's the expected output where the tensors and lists are concatenated together.\n# Expected behaviour\n{'loss': tensor([8.6422, 8.7826], device='cuda:0', grad_fn=<ThAddBackward>),\n  'answers': [['July 4', 'American Independence'], ['July 1', 'Canadian Independence'], ['September 16', 'Mexican Independence'], ['November 3', 'Panamanian Independence']],\n  'qids': [['C28fbfa5abq#1', 'C28fbfa5abq#2'], ['C2c6ca8b089q#1', 'C2c6ca8b089q#2'], ['C28fbfcdefab#1', 'C28fbfcdefab#2'], ['C2c623ef089q#1', 'C2c623ef089q#2']]}\nBut this is what happens right now. The map clause cannot handle nested lists.\n# Current behaviour\n{'loss': tensor([8.6422, 8.7826], device='cuda:0', grad_fn=<ThAddBackward>),\n  'answers': [['<map object at 0x2ad2445a7f60>', '<map object at 0x2ad2445a75f8>'], ['<map object at 0x2b6862d87358>', '<map object at 0x2b6862d87668>']],\n  'qids': [['<map object at 0x2b6862d87668>', '<map object at 0x2b6862d875c0>'], ['<map object at 0x2b6862d87438>', '<map object at 0x2b6862d87828>']]}\nPitch\n\nOne solution is to explicitly handle merging of lists together. This basically boils down to adding an extra condition for handling lists in the gather function here.\n if isinstance(out, list):\n     return [item for output in outputs for item in output]\nAlternatives\n\nOne workaround is to avoid nested outputs from the forward pass in the first place. This is still a workaround and not a solution.\nAdditional context\n\nI'm not sure if this is the right way to handle the larger problem of handling complex data structures.\nFor example one can always come up with a more nested structure which might not work with the proposed solution either. Should pytorch allow the provision for custom gather and scatter functions as it has done with collate_fn?", "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nThe [current implementation](https://github.com/pytorch/pytorch/blob/ee628d64b9d2bc8ab26655982aa3a158859a99c0/torch/nn/parallel/scatter_gather.py#L46) of the `gather` function supports simple data structures like tensors and dictionaries but can not handle nested structures such as a list of lists. \r\nHow about supporting them as well?\r\n\r\n## Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nComplex networks don't always return simple outputs such as tensors. Consider the following situation where the outputs from the forward pass of a question answering network is a dictionary with keys `loss`, `answers` and `qids`. Notice that `answers` and `qids` are a list of lists. \r\n```python\r\n# Output from forward pass\r\n {'loss': tensor(8.6422, device='cuda:0', grad_fn=<ThAddBackward>),\r\n  'answers': [['July 4', 'American Independence'], ['July 1', 'Canadian Independence']],\r\n  'qids': [['C28fbfa5abq#1', 'C28fbfa5abq#2'], ['C2c6ca8b089q#1', 'C2c6ca8b089q#2']}\r\n```\r\n\r\nWhile running this model over multiple GPUs using `nn.DataParallel`, the following is passed as input to the [gather function](https://github.com/pytorch/pytorch/blob/ee628d64b9d2bc8ab26655982aa3a158859a99c0/torch/nn/parallel/scatter_gather.py#L46)\r\n\r\n```python\r\n# Input to the `gather` function\r\n[{'loss': tensor(8.6422, device='cuda:0', grad_fn=<ThAddBackward>),\r\n  'answers': [['July 4', 'American Independence'], ['July 1', 'Canadian Independence']],\r\n  'qids': [['C28fbfa5abq#1', 'C28fbfa5abq#2'], ['C2c6ca8b089q#1', 'C2c6ca8b089q#2']},\r\n {'loss': tensor(8.7826, device='cuda:0', grad_fn=<ThAddBackward>),\r\n  'answers': [['September 16', 'Mexican Independence'], ['November 3', 'Panamanian Independence']],\r\n  'qids': [['C28fbfcdefab#1', 'C28fbfcdefab#2'], ['C2c623ef089q#1', 'C2c623ef089q#2']}]\r\n```\r\nHere's the expected output where the tensors and lists are concatenated together.\r\n\r\n```python\r\n# Expected behaviour\r\n{'loss': tensor([8.6422, 8.7826], device='cuda:0', grad_fn=<ThAddBackward>),\r\n  'answers': [['July 4', 'American Independence'], ['July 1', 'Canadian Independence'], ['September 16', 'Mexican Independence'], ['November 3', 'Panamanian Independence']],\r\n  'qids': [['C28fbfa5abq#1', 'C28fbfa5abq#2'], ['C2c6ca8b089q#1', 'C2c6ca8b089q#2'], ['C28fbfcdefab#1', 'C28fbfcdefab#2'], ['C2c623ef089q#1', 'C2c623ef089q#2']]}\r\n```\r\n\r\nBut this is what happens right now. The `map` clause cannot handle nested lists.\r\n\r\n```python\r\n# Current behaviour\r\n{'loss': tensor([8.6422, 8.7826], device='cuda:0', grad_fn=<ThAddBackward>),\r\n  'answers': [['<map object at 0x2ad2445a7f60>', '<map object at 0x2ad2445a75f8>'], ['<map object at 0x2b6862d87358>', '<map object at 0x2b6862d87668>']],\r\n  'qids': [['<map object at 0x2b6862d87668>', '<map object at 0x2b6862d875c0>'], ['<map object at 0x2b6862d87438>', '<map object at 0x2b6862d87828>']]}\r\n```\r\n\r\n## Pitch\r\n<!-- A clear and concise description of what you want to happen. -->\r\nOne solution is to explicitly handle merging of lists together. This basically boils down to adding an extra condition for handling lists in the `gather` function [here](https://github.com/pytorch/pytorch/blob/ee628d64b9d2bc8ab26655982aa3a158859a99c0/torch/nn/parallel/scatter_gather.py#L62).\r\n\r\n```python\r\n if isinstance(out, list):\r\n     return [item for output in outputs for item in output]\r\n```\r\n\r\n## Alternatives\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nOne workaround is to avoid nested outputs from the forward pass in the first place. This is still a workaround and not a solution.\r\n\r\n## Additional context\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nI'm not sure if this is the right way to handle the larger problem of handling complex data structures.\r\nFor example one can always come up with a more nested structure which might not work with the proposed solution either. Should pytorch allow the provision for custom `gather` and `scatter` functions as it has done with `collate_fn`?"}