{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2736", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2736/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2736/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2736/events", "html_url": "https://github.com/pytorch/pytorch/issues/2736", "id": 257795930, "node_id": "MDU6SXNzdWUyNTc3OTU5MzA=", "number": 2736, "title": "backward of backwards multigpu failure with Conv2d", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-09-14T17:23:15Z", "updated_at": "2017-09-15T00:31:31Z", "closed_at": "2017-09-15T00:31:31Z", "author_association": "MEMBER", "body_html": "<p>Reported by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2237879\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lopezpaz\">@lopezpaz</a><br>\nThis mwe fails:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable, grad\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> nn.Linear works</span>\nnet <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">3</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">32</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">1</span>)\nnet <span class=\"pl-k\">=</span> torch.nn.DataParallel(net, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">range</span>(torch.cuda.device_count()))\nnet.cuda()\n\ninputs <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">32</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">32</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrads <span class=\"pl-k\">=</span> grad(net(inputs).sum(), inputs, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\ngrads.view(inputs.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>).norm(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>).mean().backward()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>works<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> nn.Conv2d fails</span>\nnet <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>)\nnet <span class=\"pl-k\">=</span> torch.nn.DataParallel(net, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">range</span>(torch.cuda.device_count()))\nnet.cuda()\n\ninputs <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrads <span class=\"pl-k\">=</span> grad(net(inputs).sum(), inputs, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\ngrads.view(inputs.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>).norm(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>).mean().backward()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>works2<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "Reported by @lopezpaz\nThis mwe fails:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable, grad\n\n# nn.Linear works\nnet = nn.Linear(3 * 32 * 32, 1)\nnet = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\nnet.cuda()\n\ninputs = Variable(torch.randn(32, 3 * 32 * 32).cuda(), requires_grad=True)\ngrads = grad(net(inputs).sum(), inputs, create_graph=True)[0]\ngrads.view(inputs.size(0), -1).norm(2, 1).mean().backward()\n\nprint('works')\n\n# nn.Conv2d fails\nnet = nn.Conv2d(3, 64, 3)\nnet = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\nnet.cuda()\n\ninputs = Variable(torch.randn(32, 3, 32, 32).cuda(), requires_grad=True)\ngrads = grad(net(inputs).sum(), inputs, create_graph=True)[0]\ngrads.view(inputs.size(0), -1).norm(2, 1).mean().backward()\n\nprint('works2')", "body": "Reported by @lopezpaz\r\nThis mwe fails:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable, grad\r\n\r\n# nn.Linear works\r\nnet = nn.Linear(3 * 32 * 32, 1)\r\nnet = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\r\nnet.cuda()\r\n\r\ninputs = Variable(torch.randn(32, 3 * 32 * 32).cuda(), requires_grad=True)\r\ngrads = grad(net(inputs).sum(), inputs, create_graph=True)[0]\r\ngrads.view(inputs.size(0), -1).norm(2, 1).mean().backward()\r\n\r\nprint('works')\r\n\r\n# nn.Conv2d fails\r\nnet = nn.Conv2d(3, 64, 3)\r\nnet = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\r\nnet.cuda()\r\n\r\ninputs = Variable(torch.randn(32, 3, 32, 32).cuda(), requires_grad=True)\r\ngrads = grad(net(inputs).sum(), inputs, create_graph=True)[0]\r\ngrads.view(inputs.size(0), -1).norm(2, 1).mean().backward()\r\n\r\nprint('works2')\r\n```"}