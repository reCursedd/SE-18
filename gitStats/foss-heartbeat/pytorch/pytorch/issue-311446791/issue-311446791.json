{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6298", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6298/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6298/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6298/events", "html_url": "https://github.com/pytorch/pytorch/pull/6298", "id": 311446791, "node_id": "MDExOlB1bGxSZXF1ZXN0MTc5NTU0NDEx", "number": 6298, "title": "[caffe2] Refactor Predictor to support CUDA", "user": {"login": "harrysummer", "id": 1215413, "node_id": "MDQ6VXNlcjEyMTU0MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1215413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harrysummer", "html_url": "https://github.com/harrysummer", "followers_url": "https://api.github.com/users/harrysummer/followers", "following_url": "https://api.github.com/users/harrysummer/following{/other_user}", "gists_url": "https://api.github.com/users/harrysummer/gists{/gist_id}", "starred_url": "https://api.github.com/users/harrysummer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harrysummer/subscriptions", "organizations_url": "https://api.github.com/users/harrysummer/orgs", "repos_url": "https://api.github.com/users/harrysummer/repos", "events_url": "https://api.github.com/users/harrysummer/events{/privacy}", "received_events_url": "https://api.github.com/users/harrysummer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-04-05T01:49:20Z", "updated_at": "2018-11-23T15:41:59Z", "closed_at": "2018-06-01T18:40:39Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6298", "html_url": "https://github.com/pytorch/pytorch/pull/6298", "diff_url": "https://github.com/pytorch/pytorch/pull/6298.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6298.patch"}, "body_html": "<p>Update 2:<br>\nThe pull request now removes the thread-safe implementation. Only predictor CUDA support is included. See <a href=\"https://github.com/pytorch/pytorch/pull/6298#issuecomment-380674538\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6298/hovercard\">comment</a> below.</p>\n<hr>\n<p>Update:<br>\nThe commit now put the threadsafe implementation into <code>Predictor</code>, and also includes Predictor CUDA support.</p>\n<hr>\n<p>Original:</p>\n<p>The ONNX interface is not thread safe. All threading using the same ONNX backend would share the same Caffe2 workspace. When multiple evaluation are doing concurrently, they would write to the same blobs.</p>\n<p>This pull request deals with the problem by adding an interface method <code>run_in_new_workspace</code> to Caffe2 ONNX backend. When evaluating an image, we create a new workspace to store the intermediate and final results, while sharing the network weights by inheriting those blobs in init_net from the default workspace.</p>\n<p>Note that this might not work for RNN case. An ultimate solution might be some sort of copy-on-write mechanism.</p>", "body_text": "Update 2:\nThe pull request now removes the thread-safe implementation. Only predictor CUDA support is included. See comment below.\n\nUpdate:\nThe commit now put the threadsafe implementation into Predictor, and also includes Predictor CUDA support.\n\nOriginal:\nThe ONNX interface is not thread safe. All threading using the same ONNX backend would share the same Caffe2 workspace. When multiple evaluation are doing concurrently, they would write to the same blobs.\nThis pull request deals with the problem by adding an interface method run_in_new_workspace to Caffe2 ONNX backend. When evaluating an image, we create a new workspace to store the intermediate and final results, while sharing the network weights by inheriting those blobs in init_net from the default workspace.\nNote that this might not work for RNN case. An ultimate solution might be some sort of copy-on-write mechanism.", "body": "Update 2:\r\nThe pull request now removes the thread-safe implementation. Only predictor CUDA support is included. See [comment](https://github.com/pytorch/pytorch/pull/6298#issuecomment-380674538) below.\r\n\r\n---\r\n\r\nUpdate:\r\nThe commit now put the threadsafe implementation into `Predictor`, and also includes Predictor CUDA support.\r\n\r\n---\r\nOriginal:\r\n\r\nThe ONNX interface is not thread safe. All threading using the same ONNX backend would share the same Caffe2 workspace. When multiple evaluation are doing concurrently, they would write to the same blobs.\r\n\r\nThis pull request deals with the problem by adding an interface method `run_in_new_workspace` to Caffe2 ONNX backend. When evaluating an image, we create a new workspace to store the intermediate and final results, while sharing the network weights by inheriting those blobs in init_net from the default workspace.\r\n\r\nNote that this might not work for RNN case. An ultimate solution might be some sort of copy-on-write mechanism."}