{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380674538", "html_url": "https://github.com/pytorch/pytorch/pull/6298#issuecomment-380674538", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6298", "id": 380674538, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDY3NDUzOA==", "user": {"login": "harrysummer", "id": 1215413, "node_id": "MDQ6VXNlcjEyMTU0MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1215413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harrysummer", "html_url": "https://github.com/harrysummer", "followers_url": "https://api.github.com/users/harrysummer/followers", "following_url": "https://api.github.com/users/harrysummer/following{/other_user}", "gists_url": "https://api.github.com/users/harrysummer/gists{/gist_id}", "starred_url": "https://api.github.com/users/harrysummer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harrysummer/subscriptions", "organizations_url": "https://api.github.com/users/harrysummer/orgs", "repos_url": "https://api.github.com/users/harrysummer/repos", "events_url": "https://api.github.com/users/harrysummer/events{/privacy}", "received_events_url": "https://api.github.com/users/harrysummer/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T04:29:26Z", "updated_at": "2018-04-12T04:29:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When I was playing with my thread-safe implementation, I found it has a serious performance drop to create new workspace every time predicting a new image. I have switched to instantiate multiple predictors in one onnx backend instead. Therefore, I will only preserve predictor CUDA support part of the change. As for multi-threading, we are looking forward to the official ThreadLocalPtr solution.</p>", "body_text": "When I was playing with my thread-safe implementation, I found it has a serious performance drop to create new workspace every time predicting a new image. I have switched to instantiate multiple predictors in one onnx backend instead. Therefore, I will only preserve predictor CUDA support part of the change. As for multi-threading, we are looking forward to the official ThreadLocalPtr solution.", "body": "When I was playing with my thread-safe implementation, I found it has a serious performance drop to create new workspace every time predicting a new image. I have switched to instantiate multiple predictors in one onnx backend instead. Therefore, I will only preserve predictor CUDA support part of the change. As for multi-threading, we are looking forward to the official ThreadLocalPtr solution."}