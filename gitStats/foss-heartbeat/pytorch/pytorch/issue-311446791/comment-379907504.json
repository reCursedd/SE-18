{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379907504", "html_url": "https://github.com/pytorch/pytorch/pull/6298#issuecomment-379907504", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6298", "id": 379907504, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTkwNzUwNA==", "user": {"login": "harrysummer", "id": 1215413, "node_id": "MDQ6VXNlcjEyMTU0MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1215413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harrysummer", "html_url": "https://github.com/harrysummer", "followers_url": "https://api.github.com/users/harrysummer/followers", "following_url": "https://api.github.com/users/harrysummer/following{/other_user}", "gists_url": "https://api.github.com/users/harrysummer/gists{/gist_id}", "starred_url": "https://api.github.com/users/harrysummer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harrysummer/subscriptions", "organizations_url": "https://api.github.com/users/harrysummer/orgs", "repos_url": "https://api.github.com/users/harrysummer/repos", "events_url": "https://api.github.com/users/harrysummer/events{/privacy}", "received_events_url": "https://api.github.com/users/harrysummer/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T22:01:05Z", "updated_at": "2018-04-09T22:01:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20307328\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/salexspb\">@salexspb</a> for your clarifying of the official plan. To my understanding, this pull request is not completely conflicts with your plan. It consists of two features:</p>\n<ol>\n<li><strong>Make <code>Predictor</code>, as its name indicates, independent to the underlying device.</strong> Both CPU and CUDA should be support. If your plan is making predictor a mobile one (CPU only), why not making it a <code>Predictor&lt;CPUContext&gt;</code> and inherit the <code>PredictorBase</code>? I don't think we should limit the potential of <code>Predictor</code>.</li>\n<li><strong>Make network prediction thread-safe by allowing predictor to create and run net in a temporary workspace.</strong> This might have the similar user-case as your about-to-ship feature of thread-safety. It sounds to me that your <code>folly::ThreadLocalPtr</code> would be used to create multiple predictor instances for each thread. This is almost as heavy as running different Caffe2 instances in terms of memory consumption. This pull request deals with multi-thread evalution in one predictor. This would also be very helpful when comes to Python-side evaluation with multiple threads from a single python Predictor instance. The implementation should be parallel with your about-to-ship feature.</li>\n</ol>\n<p>BTW, I think current <code>Predictor</code> already support <code>MetaNetDef</code> format in its constructor.</p>\n<p>I will stay with my own implementation, which works fine for me, at the moment, and keep up to the official design.</p>\n<p>Best Regards.</p>", "body_text": "Thanks @salexspb for your clarifying of the official plan. To my understanding, this pull request is not completely conflicts with your plan. It consists of two features:\n\nMake Predictor, as its name indicates, independent to the underlying device. Both CPU and CUDA should be support. If your plan is making predictor a mobile one (CPU only), why not making it a Predictor<CPUContext> and inherit the PredictorBase? I don't think we should limit the potential of Predictor.\nMake network prediction thread-safe by allowing predictor to create and run net in a temporary workspace. This might have the similar user-case as your about-to-ship feature of thread-safety. It sounds to me that your folly::ThreadLocalPtr would be used to create multiple predictor instances for each thread. This is almost as heavy as running different Caffe2 instances in terms of memory consumption. This pull request deals with multi-thread evalution in one predictor. This would also be very helpful when comes to Python-side evaluation with multiple threads from a single python Predictor instance. The implementation should be parallel with your about-to-ship feature.\n\nBTW, I think current Predictor already support MetaNetDef format in its constructor.\nI will stay with my own implementation, which works fine for me, at the moment, and keep up to the official design.\nBest Regards.", "body": "Thanks @salexspb for your clarifying of the official plan. To my understanding, this pull request is not completely conflicts with your plan. It consists of two features:\r\n1. **Make `Predictor`, as its name indicates, independent to the underlying device.** Both CPU and CUDA should be support. If your plan is making predictor a mobile one (CPU only), why not making it a `Predictor<CPUContext>` and inherit the `PredictorBase`? I don't think we should limit the potential of `Predictor`.\r\n2. **Make network prediction thread-safe by allowing predictor to create and run net in a temporary workspace.** This might have the similar user-case as your about-to-ship feature of thread-safety. It sounds to me that your `folly::ThreadLocalPtr` would be used to create multiple predictor instances for each thread. This is almost as heavy as running different Caffe2 instances in terms of memory consumption. This pull request deals with multi-thread evalution in one predictor. This would also be very helpful when comes to Python-side evaluation with multiple threads from a single python Predictor instance. The implementation should be parallel with your about-to-ship feature.\r\n\r\nBTW, I think current `Predictor` already support `MetaNetDef` format in its constructor.\r\n\r\nI will stay with my own implementation, which works fine for me, at the moment, and keep up to the official design.\r\n\r\nBest Regards."}