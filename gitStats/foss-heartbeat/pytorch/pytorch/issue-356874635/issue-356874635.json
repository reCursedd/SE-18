{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11223", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11223/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11223/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11223/events", "html_url": "https://github.com/pytorch/pytorch/issues/11223", "id": 356874635, "node_id": "MDU6SXNzdWUzNTY4NzQ2MzU=", "number": 11223, "title": "index_copy inconsistent behavior with duplicated indices on CPU and GPU", "user": {"login": "jadore801120", "id": 3960874, "node_id": "MDQ6VXNlcjM5NjA4NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/3960874?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jadore801120", "html_url": "https://github.com/jadore801120", "followers_url": "https://api.github.com/users/jadore801120/followers", "following_url": "https://api.github.com/users/jadore801120/following{/other_user}", "gists_url": "https://api.github.com/users/jadore801120/gists{/gist_id}", "starred_url": "https://api.github.com/users/jadore801120/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jadore801120/subscriptions", "organizations_url": "https://api.github.com/users/jadore801120/orgs", "repos_url": "https://api.github.com/users/jadore801120/repos", "events_url": "https://api.github.com/users/jadore801120/events{/privacy}", "received_events_url": "https://api.github.com/users/jadore801120/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-04T15:54:59Z", "updated_at": "2018-09-04T16:05:12Z", "closed_at": "2018-09-04T16:05:12Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Hello all, I just found out that to do <code>index_copy</code> with duplicated elements in the index tensor will result in different results on CPU and GPU.<br>\nTo give a little context, I am trying to perform <a href=\"https://www.tensorflow.org/api_docs/python/tf/segment_max\" rel=\"nofollow\"><code>tf.segment_max</code></a> with pytorch.<br>\nThe elements in a tensor are associated to segments, and I would like to get all the max value in all the segments.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/6ace4f9df6092782e555dd1b494345f5b45e78fa/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f5365676d656e744d61782e706e67\"><img src=\"https://camo.githubusercontent.com/6ace4f9df6092782e555dd1b494345f5b45e78fa/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f5365676d656e744d61782e706e67\" alt=\"tf.segment_max\" data-canonical-src=\"https://www.tensorflow.org/images/SegmentMax.png\" style=\"max-width:100%;\"></a></p>\n<p>Then I found out the api <code>index_copy</code> and test its behavior on cpu.<br>\nI realized that it will place the last segment value associated with the last segment index into the output tensor.<br>\nTherefore, I just need to sort the input tensor and make the segment Index tensor reorder as the sorted input tensor.<br>\nThe maximum in every segment shall be the last one to copy, and the other values in the segments will be ignored. Please refer to following function <code>_seg_max</code>.</p>\n<p>Running on CPU, the code just run smoothly and perfectly worked as the <code>tf.segment_max</code>.<br>\nHowever, on GPU, I found out that it will not keep the last value a the final output value.<br>\n(The <code>torch.sort</code> works correctly.)</p>\n<p>I found that if there is no duplicated index in index tensor, the <code>index_copy</code> works just fine also on GPU.</p>\n<h3>TL;DR</h3>\n<p><code>index_copy</code> will copy the last value if it meets duplicated indices on CPU. On GPU, it behaves differently.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> random\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_seg_max</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">seg</span>, <span class=\"pl-smi\">n_seg</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Perform segment max<span class=\"pl-pds\">'</span></span>\n    _, sorted_id <span class=\"pl-k\">=</span> torch.sort(data)\n    sorted_seg <span class=\"pl-k\">=</span> seg[sorted_id].detach()\n    sorted_data <span class=\"pl-k\">=</span> data[sorted_id].detach()\n    seg_max <span class=\"pl-k\">=</span> torch.zeros((n_seg,), <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>data.device).index_copy(<span class=\"pl-c1\">0</span>, sorted_seg, sorted_data).detach()\n    <span class=\"pl-k\">return</span> seg_max\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">random_seg</span>(<span class=\"pl-smi\">n_ele</span>, <span class=\"pl-smi\">n_seg</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Create a random segment idx list<span class=\"pl-pds\">'</span></span>\n    seg_list <span class=\"pl-k\">=</span> [random.randint(<span class=\"pl-c1\">0</span>, n_seg <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n_ele <span class=\"pl-k\">-</span> n_seg)] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(n_seg))\n    random.shuffle(seg_list)\n    <span class=\"pl-k\">return</span> seg_list\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">n_ele</span>, <span class=\"pl-smi\">n_seg</span>, <span class=\"pl-smi\">device</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Perform `reduce max` on <span class=\"pl-c1\">{}</span>-dim vector into <span class=\"pl-c1\">{}</span> segments on <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(n_ele, n_seg, device))\n    <span class=\"pl-k\">with</span> torch.no_grad():\n        <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n            data <span class=\"pl-k\">=</span> torch.FloatTensor(n_ele).to(device)\n            nn.init.uniform_(data)\n            segment_ids <span class=\"pl-k\">=</span> torch.LongTensor(random_seg(n_ele, n_seg)).to(device)\n            seg_max <span class=\"pl-k\">=</span> _seg_max(data, segment_ids, n_seg)\n            a <span class=\"pl-k\">=</span> seg_max.max().item()\n            b <span class=\"pl-k\">=</span> data.max().item()\n            <span class=\"pl-k\">assert</span> b <span class=\"pl-k\">==</span> a, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{}</span> != <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(a, b)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> -- Passed.<span class=\"pl-pds\">'</span></span>)\n\n\ngpu <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)\ncpu <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>)\n\nrun(<span class=\"pl-v\">n_ele</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">40</span>, <span class=\"pl-v\">n_seg</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">40</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>cpu)\nrun(<span class=\"pl-v\">n_ele</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">40</span>, <span class=\"pl-v\">n_seg</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>cpu)\nrun(<span class=\"pl-v\">n_ele</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">40</span>, <span class=\"pl-v\">n_seg</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">40</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>gpu)\nrun(<span class=\"pl-v\">n_ele</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">40</span>, <span class=\"pl-v\">n_seg</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>gpu)</pre></div>\n<p>Output:</p>\n<div class=\"highlight highlight-source-shell\"><pre>Perform <span class=\"pl-s\"><span class=\"pl-pds\">`</span>segment max<span class=\"pl-pds\">`</span></span> on 40-dim vector into 40 segments on cpu\n -- Passed.\nPerform <span class=\"pl-s\"><span class=\"pl-pds\">`</span>segment max<span class=\"pl-pds\">`</span></span> on 40-dim vector into 10 segments on cpu\n -- Passed.\nPerform <span class=\"pl-s\"><span class=\"pl-pds\">`</span>segment max<span class=\"pl-pds\">`</span></span> on 40-dim vector into 40 segments on cuda\n -- Passed.\nPerform <span class=\"pl-s\"><span class=\"pl-pds\">`</span>segment max<span class=\"pl-pds\">`</span></span> on 40-dim vector into 10 segments on cuda\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dbg.py<span class=\"pl-pds\">\"</span></span>, line 37, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    run(n_ele=40, n_seg=10, device=gpu)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dbg.py<span class=\"pl-pds\">\"</span></span>, line 28, <span class=\"pl-k\">in</span> run\n    assert b == a, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>{} != {}<span class=\"pl-pds\">\"</span></span>.format(a, b)\nAssertionError: 0.7791520357131958 <span class=\"pl-k\">!</span>= 0.8709555864334106</pre></div>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Debian GNU/Linux 8.10 (jessie)<br>\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2<br>\nCMake version: version 3.0.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: GeForce GTX TITAN X<br>\nNvidia driver version: 387.26<br>\nVersions of relevant libraries:</p>\n<p>[pip3] msgpack-numpy (0.4.1)<br>\n[pip3] numpy (1.14.5)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] torch (0.4.1)<br>\n[pip3] torchfile (0.1.0)<br>\n[pip3] torchtext (0.2.3)<br>\n[pip3] torchvision (0.2.0)<br>\n[conda] cuda80                    1.0                           0    soumith<br>\n[conda] pytorch                   0.4.1            py36ha74772b_0<br>\n[conda] torchfile                 0.1.0                     <br>\n[conda] torchtext                 0.2.3                     <br>\n[conda] torchvision               0.2.0                     <br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "Issue description\nHello all, I just found out that to do index_copy with duplicated elements in the index tensor will result in different results on CPU and GPU.\nTo give a little context, I am trying to perform tf.segment_max with pytorch.\nThe elements in a tensor are associated to segments, and I would like to get all the max value in all the segments.\n\nThen I found out the api index_copy and test its behavior on cpu.\nI realized that it will place the last segment value associated with the last segment index into the output tensor.\nTherefore, I just need to sort the input tensor and make the segment Index tensor reorder as the sorted input tensor.\nThe maximum in every segment shall be the last one to copy, and the other values in the segments will be ignored. Please refer to following function _seg_max.\nRunning on CPU, the code just run smoothly and perfectly worked as the tf.segment_max.\nHowever, on GPU, I found out that it will not keep the last value a the final output value.\n(The torch.sort works correctly.)\nI found that if there is no duplicated index in index tensor, the index_copy works just fine also on GPU.\nTL;DR\nindex_copy will copy the last value if it meets duplicated indices on CPU. On GPU, it behaves differently.\nCode example\nimport torch\nimport torch.nn as nn\nimport random\n\ndef _seg_max(data, seg, n_seg):\n    'Perform segment max'\n    _, sorted_id = torch.sort(data)\n    sorted_seg = seg[sorted_id].detach()\n    sorted_data = data[sorted_id].detach()\n    seg_max = torch.zeros((n_seg,), device=data.device).index_copy(0, sorted_seg, sorted_data).detach()\n    return seg_max\n\ndef random_seg(n_ele, n_seg):\n    'Create a random segment idx list'\n    seg_list = [random.randint(0, n_seg - 1) for _ in range(n_ele - n_seg)] + list(range(n_seg))\n    random.shuffle(seg_list)\n    return seg_list\n\ndef run(n_ele, n_seg, device):\n    print('Perform `reduce max` on {}-dim vector into {} segments on {}'.format(n_ele, n_seg, device))\n    with torch.no_grad():\n        for _ in range(10000):\n            data = torch.FloatTensor(n_ele).to(device)\n            nn.init.uniform_(data)\n            segment_ids = torch.LongTensor(random_seg(n_ele, n_seg)).to(device)\n            seg_max = _seg_max(data, segment_ids, n_seg)\n            a = seg_max.max().item()\n            b = data.max().item()\n            assert b == a, \"{} != {}\".format(a, b)\n    print(' -- Passed.')\n\n\ngpu = torch.device('cuda')\ncpu = torch.device('cpu')\n\nrun(n_ele=40, n_seg=40, device=cpu)\nrun(n_ele=40, n_seg=10, device=cpu)\nrun(n_ele=40, n_seg=40, device=gpu)\nrun(n_ele=40, n_seg=10, device=gpu)\nOutput:\nPerform `segment max` on 40-dim vector into 40 segments on cpu\n -- Passed.\nPerform `segment max` on 40-dim vector into 10 segments on cpu\n -- Passed.\nPerform `segment max` on 40-dim vector into 40 segments on cuda\n -- Passed.\nPerform `segment max` on 40-dim vector into 10 segments on cuda\nTraceback (most recent call last):\n  File \"dbg.py\", line 37, in <module>\n    run(n_ele=40, n_seg=10, device=gpu)\n  File \"dbg.py\", line 28, in run\n    assert b == a, \"{} != {}\".format(a, b)\nAssertionError: 0.7791520357131958 != 0.8709555864334106\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Debian GNU/Linux 8.10 (jessie)\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2\nCMake version: version 3.0.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: GeForce GTX TITAN X\nNvidia driver version: 387.26\nVersions of relevant libraries:\n[pip3] msgpack-numpy (0.4.1)\n[pip3] numpy (1.14.5)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.4.1)\n[pip3] torchfile (0.1.0)\n[pip3] torchtext (0.2.3)\n[pip3] torchvision (0.2.0)\n[conda] cuda80                    1.0                           0    soumith\n[conda] pytorch                   0.4.1            py36ha74772b_0\n[conda] torchfile                 0.1.0                     \n[conda] torchtext                 0.2.3                     \n[conda] torchvision               0.2.0                     \n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "## Issue description\r\n\r\nHello all, I just found out that to do `index_copy` with duplicated elements in the index tensor will result in different results on CPU and GPU.\r\nTo give a little context, I am trying to perform [`tf.segment_max`](https://www.tensorflow.org/api_docs/python/tf/segment_max) with pytorch.\r\nThe elements in a tensor are associated to segments, and I would like to get all the max value in all the segments.\r\n\r\n![tf.segment_max](https://www.tensorflow.org/images/SegmentMax.png)\r\n\r\nThen I found out the api `index_copy` and test its behavior on cpu.\r\nI realized that it will place the last segment value associated with the last segment index into the output tensor.\r\nTherefore, I just need to sort the input tensor and make the segment Index tensor reorder as the sorted input tensor.\r\nThe maximum in every segment shall be the last one to copy, and the other values in the segments will be ignored. Please refer to following function `_seg_max`.\r\n\r\nRunning on CPU, the code just run smoothly and perfectly worked as the `tf.segment_max`.\r\nHowever, on GPU, I found out that it will not keep the last value a the final output value.\r\n(The `torch.sort` works correctly.)\r\n\r\nI found that if there is no duplicated index in index tensor, the `index_copy` works just fine also on GPU.\r\n\r\n\r\n### TL;DR\r\n`index_copy` will copy the last value if it meets duplicated indices on CPU. On GPU, it behaves differently.\r\n\r\n## Code example\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport random\r\n\r\ndef _seg_max(data, seg, n_seg):\r\n    'Perform segment max'\r\n    _, sorted_id = torch.sort(data)\r\n    sorted_seg = seg[sorted_id].detach()\r\n    sorted_data = data[sorted_id].detach()\r\n    seg_max = torch.zeros((n_seg,), device=data.device).index_copy(0, sorted_seg, sorted_data).detach()\r\n    return seg_max\r\n\r\ndef random_seg(n_ele, n_seg):\r\n    'Create a random segment idx list'\r\n    seg_list = [random.randint(0, n_seg - 1) for _ in range(n_ele - n_seg)] + list(range(n_seg))\r\n    random.shuffle(seg_list)\r\n    return seg_list\r\n\r\ndef run(n_ele, n_seg, device):\r\n    print('Perform `reduce max` on {}-dim vector into {} segments on {}'.format(n_ele, n_seg, device))\r\n    with torch.no_grad():\r\n        for _ in range(10000):\r\n            data = torch.FloatTensor(n_ele).to(device)\r\n            nn.init.uniform_(data)\r\n            segment_ids = torch.LongTensor(random_seg(n_ele, n_seg)).to(device)\r\n            seg_max = _seg_max(data, segment_ids, n_seg)\r\n            a = seg_max.max().item()\r\n            b = data.max().item()\r\n            assert b == a, \"{} != {}\".format(a, b)\r\n    print(' -- Passed.')\r\n\r\n\r\ngpu = torch.device('cuda')\r\ncpu = torch.device('cpu')\r\n\r\nrun(n_ele=40, n_seg=40, device=cpu)\r\nrun(n_ele=40, n_seg=10, device=cpu)\r\nrun(n_ele=40, n_seg=40, device=gpu)\r\nrun(n_ele=40, n_seg=10, device=gpu)\r\n```\r\n\r\nOutput:\r\n```bash\r\nPerform `segment max` on 40-dim vector into 40 segments on cpu\r\n -- Passed.\r\nPerform `segment max` on 40-dim vector into 10 segments on cpu\r\n -- Passed.\r\nPerform `segment max` on 40-dim vector into 40 segments on cuda\r\n -- Passed.\r\nPerform `segment max` on 40-dim vector into 10 segments on cuda\r\nTraceback (most recent call last):\r\n  File \"dbg.py\", line 37, in <module>\r\n    run(n_ele=40, n_seg=10, device=gpu)\r\n  File \"dbg.py\", line 28, in run\r\n    assert b == a, \"{} != {}\".format(a, b)\r\nAssertionError: 0.7791520357131958 != 0.8709555864334106\r\n```\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 8.10 (jessie)\r\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2\r\nCMake version: version 3.0.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX TITAN X\r\nNvidia driver version: 387.26\r\nVersions of relevant libraries:\r\n\r\n[pip3] msgpack-numpy (0.4.1)\r\n[pip3] numpy (1.14.5)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.1)\r\n[pip3] torchfile (0.1.0)\r\n[pip3] torchtext (0.2.3)\r\n[pip3] torchvision (0.2.0)\r\n[conda] cuda80                    1.0                           0    soumith\r\n[conda] pytorch                   0.4.1            py36ha74772b_0  \r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchtext                 0.2.3                     <pip>\r\n[conda] torchvision               0.2.0                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n"}