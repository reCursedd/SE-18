{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/315923893", "html_url": "https://github.com/pytorch/pytorch/pull/2112#issuecomment-315923893", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2112", "id": 315923893, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTkyMzg5Mw==", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-18T00:34:19Z", "updated_at": "2017-07-18T00:34:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> What would you suggest, considering <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/instancenorm.py#L23\">instance norm uses a batch size of 1</a> and <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L560-L563\"><code>batch_norm</code></a> doesn't (sensibly so) provide a way to turn off <code>cudnn</code>?</p>\n<p>I can also imagine situations where <code>batch_size</code> * <code>num_channels</code> &lt;&lt; <code>feature_size</code> - in the lower layers of a CNN where you're looking at activation statistics for style transfer for example. James may also be able to say more about his use cases.</p>", "body_text": "@ngimel What would you suggest, considering instance norm uses a batch size of 1 and batch_norm doesn't (sensibly so) provide a way to turn off cudnn?\nI can also imagine situations where batch_size * num_channels << feature_size - in the lower layers of a CNN where you're looking at activation statistics for style transfer for example. James may also be able to say more about his use cases.", "body": "@ngimel What would you suggest, considering [instance norm uses a batch size of 1](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/instancenorm.py#L23) and [`batch_norm`](https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L560-L563) doesn't (sensibly so) provide a way to turn off `cudnn`?\r\n\r\nI can also imagine situations where `batch_size` * `num_channels` << `feature_size` - in the lower layers of a CNN where you're looking at activation statistics for style transfer for example. James may also be able to say more about his use cases."}