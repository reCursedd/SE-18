{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338481388", "html_url": "https://github.com/pytorch/pytorch/pull/2112#issuecomment-338481388", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2112", "id": 338481388, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODQ4MTM4OA==", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-22T14:24:05Z", "updated_at": "2017-10-22T14:24:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Note for when coming back to this - may want to make LayerNorm the same for {1, 2, 3}D for the same reasons as underlined in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"255288401\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2628\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2628/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2628\">#2628</a>.</p>", "body_text": "Note for when coming back to this - may want to make LayerNorm the same for {1, 2, 3}D for the same reasons as underlined in #2628.", "body": "Note for when coming back to this - may want to make LayerNorm the same for {1, 2, 3}D for the same reasons as underlined in https://github.com/pytorch/pytorch/issues/2628."}