{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/315585749", "html_url": "https://github.com/pytorch/pytorch/pull/2112#issuecomment-315585749", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2112", "id": 315585749, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTU4NTc0OQ==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-16T04:58:51Z", "updated_at": "2017-07-16T04:58:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm not entirely sold on reusing the InstanceNorm code to do LayerNorm. I think Sam's comments are right, and additionally I'm afraid that keeping running statistics (which is inconsistent with the motivation of LayerNorm) would be unnecessary overhead. LayerNorm is a surprisingly significant fraction of compute time for Transformer-like networks already; it might be worth doing a microbenchmark to make sure that it's as fast as we can easily make it.</p>", "body_text": "I'm not entirely sold on reusing the InstanceNorm code to do LayerNorm. I think Sam's comments are right, and additionally I'm afraid that keeping running statistics (which is inconsistent with the motivation of LayerNorm) would be unnecessary overhead. LayerNorm is a surprisingly significant fraction of compute time for Transformer-like networks already; it might be worth doing a microbenchmark to make sure that it's as fast as we can easily make it.", "body": "I'm not entirely sold on reusing the InstanceNorm code to do LayerNorm. I think Sam's comments are right, and additionally I'm afraid that keeping running statistics (which is inconsistent with the motivation of LayerNorm) would be unnecessary overhead. LayerNorm is a surprisingly significant fraction of compute time for Transformer-like networks already; it might be worth doing a microbenchmark to make sure that it's as fast as we can easily make it."}