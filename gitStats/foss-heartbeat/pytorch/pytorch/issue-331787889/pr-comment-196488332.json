{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196488332", "pull_request_review_id": 130050128, "id": 196488332, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjQ4ODMzMg==", "diff_hunk": "@@ -0,0 +1,122 @@\n+#ifndef THC_GENERIC_FILE\n+#define THC_GENERIC_FILE \"generic/THCTensorKthValue.cu\"\n+#else\n+\n+THC_API void THCTensor_(kthvalue)(THCState* state,\n+                                  THCTensor* kthValue,\n+                                  THCudaLongTensor* indices,\n+                                  THCTensor* input,\n+                                  int64_t k, int dim, int keepDim) {\n+  THAssert(kthValue != NULL && indices != NULL && input != NULL);\n+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 3, kthValue, indices, input));\n+  THArgCheck(THCTensor_(nDimension)(state, kthValue) <= MAX_CUTORCH_DIMS, 2, CUTORCH_DIM_WARNING);\n+  int64_t dims = THCudaLongTensor_nDimension(state, indices);\n+  THArgCheck(dims <= MAX_CUTORCH_DIMS, 3, CUTORCH_DIM_WARNING);\n+  int numDims = THCTensor_(nDimension)(state, input);\n+  THArgCheck(numDims <= MAX_CUTORCH_DIMS, 4, CUTORCH_DIM_WARNING);\n+\n+  THArgCheck(dim >= 0 && dim < numDims, 6, \"dim not in range\");\n+\n+  int64_t sliceSize = THCTensor_(size)(state, input, dim);\n+  THArgCheck(k > 0 && k <= sliceSize, 5, \"k not in range for dimension\");\n+\n+  // Build the output size, which is the dim being selected set to\n+  // size 1\n+  THLongStorage* kthValueSize = THCTensor_(newSizeOf)(state, input);\n+  THLongStorage_set(kthValueSize, dim, 1);\n+  THCTensor_(resize)(state, kthValue, kthValueSize, NULL);\n+  THCudaLongTensor_resize(state, indices, kthValueSize, NULL);\n+  THLongStorage_free(kthValueSize);\n+\n+  #define RUN_K(INDEX_T, DIM)                                             \\\n+    gatherKthValue<real, INDEX_T, DIM>                                    \\\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \\\n+        inputInfo,                                                        \\\n+        sliceSize,                                                        \\\n+        k,                                                                \\\n+        inputSlices,                                                      \\\n+        /* The actual dimension that the k-selection is running in */     \\\n+        /* may have changed from collapseDims() */                        \\\n+        inputInfo.strides[collapseInputDim],                              \\\n+        kthValueInfo,                                                     \\\n+        indicesInfo)\n+\n+  #define RUN_DIM(INDEX_T)                        \\\n+    if (allDims == 1) {                           \\\n+      RUN_K(INDEX_T, 1);                          \\\n+    } else if (allDims == 2) {                    \\\n+      RUN_K(INDEX_T, 2);                          \\\n+    } else if (allDims == 3) {                    \\\n+      RUN_K(INDEX_T, 3);                          \\\n+    } else {                                      \\\n+      RUN_K(INDEX_T, -1);                         \\\n+    }\n+\n+  #define RUN_T(INDEX_T)                                                  \\\n+    TensorInfo<real, INDEX_T> inputInfo =                                 \\\n+      getTensorInfo<real, THCTensor, INDEX_T>(state, input);              \\\n+    TensorInfo<real, INDEX_T> kthValueInfo =                              \\\n+      getTensorInfo<real, THCTensor, INDEX_T>(state, kthValue);           \\\n+    TensorInfo<int64_t, INDEX_T> indicesInfo =                            \\\n+      getTensorInfo<int64_t, THCudaLongTensor, INDEX_T>(state, indices);  \\\n+                                                                          \\\n+    /* We use these structures solely to find the offset to */            \\\n+    /* each slice we are operating on */                                  \\\n+    inputInfo.sizes[dim] = 1;                                             \\\n+    kthValueInfo.sizes[dim] = 1;                                          \\\n+    indicesInfo.sizes[dim] = 1;                                           \\\n+                                                                          \\\n+    /* Collapse all other dims */                                         \\\n+    int collapseInputDim = inputInfo.collapseDims(dim);                   \\\n+    int collapseKthValueDim = kthValueInfo.collapseDims(dim);             \\\n+    int collapseIndicesDim = indicesInfo.collapseDims(dim);               \\\n+                                                                          \\\n+    int64_t inputSlices = 1;                                              \\\n+    for (int i = 0; i < inputInfo.dims; ++i) {                            \\\n+      inputSlices *= inputInfo.sizes[i];                                  \\\n+    }                                                                     \\\n+    int64_t kthValueSlices = 1;                                           \\\n+    for (int i = 0; i < kthValueInfo.dims; ++i) {                         \\\n+      kthValueSlices *= kthValueInfo.sizes[i];                            \\\n+    }                                                                     \\\n+                                                                          \\\n+    dim3 grid;                                                            \\\n+    if (!THC_getGridFromTiles(inputSlices, grid)) {                       \\\n+      THError(\"Slice to select is too large\");                            \\\n+    }                                                                     \\\n+                                                                          \\\n+    dim3 block(std::min(THCRoundUp(sliceSize, (int64_t) 32), (int64_t) 1024)); \\\n+                                                                          \\\n+    /* This is used as a template parameter to calculate indices. */      \\\n+    /* We only specialize it if all collapsed dim sizes are the */        \\\n+    /* same; otherwise, we use -1 which is the specialization */          \\\n+    /* parameter for arbitrary dimensions */                              \\\n+    int allDims = inputInfo.dims;                                         \\\n+    if (kthValueInfo.dims != allDims || indicesInfo.dims != allDims) {    \\\n+      allDims = -1;                                                       \\\n+    }                                                                     \\\n+                                                                          \\\n+    RUN_DIM(INDEX_T);\n+\n+    // Based on required index size, run the algorithm with the\n+    // appropriate index type\n+    if (THCTensor_canUse32BitIndexMath(state, input) &&\n+        THCTensor_canUse32BitIndexMath(state, kthValue) &&\n+        THCTensor_canUse32BitIndexMath(state, indices)) {\n+      RUN_T(uint32_t);", "path": "aten/src/THC/generic/THCTensorKthValue.cu", "position": null, "original_position": 106, "commit_id": "67204dac722d05387ca13d50382378d66e1c3814", "original_commit_id": "b88b13605d7ae48f3b14ef96200143b278cb14ea", "user": {"login": "pararthshah", "id": 1484859, "node_id": "MDQ6VXNlcjE0ODQ4NTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1484859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pararthshah", "html_url": "https://github.com/pararthshah", "followers_url": "https://api.github.com/users/pararthshah/followers", "following_url": "https://api.github.com/users/pararthshah/following{/other_user}", "gists_url": "https://api.github.com/users/pararthshah/gists{/gist_id}", "starred_url": "https://api.github.com/users/pararthshah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pararthshah/subscriptions", "organizations_url": "https://api.github.com/users/pararthshah/orgs", "repos_url": "https://api.github.com/users/pararthshah/repos", "events_url": "https://api.github.com/users/pararthshah/events{/privacy}", "received_events_url": "https://api.github.com/users/pararthshah/received_events", "type": "User", "site_admin": false}, "body": "I've followed the style in THCTensorTopK.cu, which defines these as macros. Are templates preferable to macros?", "created_at": "2018-06-19T16:09:23Z", "updated_at": "2018-11-23T15:45:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/8406#discussion_r196488332", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8406", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196488332"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8406#discussion_r196488332"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8406"}}, "body_html": "<p>I've followed the style in THCTensorTopK.cu, which defines these as macros. Are templates preferable to macros?</p>", "body_text": "I've followed the style in THCTensorTopK.cu, which defines these as macros. Are templates preferable to macros?", "in_reply_to_id": 195763061}