{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196490783", "pull_request_review_id": 130053205, "id": 196490783, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjQ5MDc4Mw==", "diff_hunk": "@@ -0,0 +1,122 @@\n+#ifndef THC_GENERIC_FILE\n+#define THC_GENERIC_FILE \"generic/THCTensorKthValue.cu\"\n+#else\n+\n+THC_API void THCTensor_(kthvalue)(THCState* state,\n+                                  THCTensor* kthValue,\n+                                  THCudaLongTensor* indices,\n+                                  THCTensor* input,\n+                                  int64_t k, int dim, int keepDim) {\n+  THAssert(kthValue != NULL && indices != NULL && input != NULL);\n+  THCAssertSameGPU(THCTensor_(checkGPU)(state, 3, kthValue, indices, input));\n+  THArgCheck(THCTensor_(nDimension)(state, kthValue) <= MAX_CUTORCH_DIMS, 2, CUTORCH_DIM_WARNING);\n+  int64_t dims = THCudaLongTensor_nDimension(state, indices);\n+  THArgCheck(dims <= MAX_CUTORCH_DIMS, 3, CUTORCH_DIM_WARNING);\n+  int numDims = THCTensor_(nDimension)(state, input);\n+  THArgCheck(numDims <= MAX_CUTORCH_DIMS, 4, CUTORCH_DIM_WARNING);\n+\n+  THArgCheck(dim >= 0 && dim < numDims, 6, \"dim not in range\");\n+\n+  int64_t sliceSize = THCTensor_(size)(state, input, dim);\n+  THArgCheck(k > 0 && k <= sliceSize, 5, \"k not in range for dimension\");\n+\n+  // Build the output size, which is the dim being selected set to\n+  // size 1\n+  THLongStorage* kthValueSize = THCTensor_(newSizeOf)(state, input);\n+  THLongStorage_set(kthValueSize, dim, 1);\n+  THCTensor_(resize)(state, kthValue, kthValueSize, NULL);\n+  THCudaLongTensor_resize(state, indices, kthValueSize, NULL);\n+  THLongStorage_free(kthValueSize);\n+\n+  #define RUN_K(INDEX_T, DIM)                                             \\\n+    gatherKthValue<real, INDEX_T, DIM>                                    \\\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(             \\\n+        inputInfo,                                                        \\\n+        sliceSize,                                                        \\\n+        k,                                                                \\\n+        inputSlices,                                                      \\\n+        /* The actual dimension that the k-selection is running in */     \\\n+        /* may have changed from collapseDims() */                        \\\n+        inputInfo.strides[collapseInputDim],                              \\\n+        kthValueInfo,                                                     \\\n+        indicesInfo)\n+\n+  #define RUN_DIM(INDEX_T)                        \\\n+    if (allDims == 1) {                           \\\n+      RUN_K(INDEX_T, 1);                          \\\n+    } else if (allDims == 2) {                    \\\n+      RUN_K(INDEX_T, 2);                          \\\n+    } else if (allDims == 3) {                    \\\n+      RUN_K(INDEX_T, 3);                          \\\n+    } else {                                      \\\n+      RUN_K(INDEX_T, -1);                         \\\n+    }\n+\n+  #define RUN_T(INDEX_T)                                                  \\\n+    TensorInfo<real, INDEX_T> inputInfo =                                 \\\n+      getTensorInfo<real, THCTensor, INDEX_T>(state, input);              \\\n+    TensorInfo<real, INDEX_T> kthValueInfo =                              \\\n+      getTensorInfo<real, THCTensor, INDEX_T>(state, kthValue);           \\\n+    TensorInfo<int64_t, INDEX_T> indicesInfo =                            \\\n+      getTensorInfo<int64_t, THCudaLongTensor, INDEX_T>(state, indices);  \\\n+                                                                          \\\n+    /* We use these structures solely to find the offset to */            \\\n+    /* each slice we are operating on */                                  \\\n+    inputInfo.sizes[dim] = 1;                                             \\\n+    kthValueInfo.sizes[dim] = 1;                                          \\\n+    indicesInfo.sizes[dim] = 1;                                           \\\n+                                                                          \\\n+    /* Collapse all other dims */                                         \\\n+    int collapseInputDim = inputInfo.collapseDims(dim);                   \\\n+    int collapseKthValueDim = kthValueInfo.collapseDims(dim);             \\\n+    int collapseIndicesDim = indicesInfo.collapseDims(dim);               \\\n+                                                                          \\\n+    int64_t inputSlices = 1;                                              \\", "path": "aten/src/THC/generic/THCTensorKthValue.cu", "position": 74, "original_position": 74, "commit_id": "67204dac722d05387ca13d50382378d66e1c3814", "original_commit_id": "b88b13605d7ae48f3b14ef96200143b278cb14ea", "user": {"login": "pararthshah", "id": 1484859, "node_id": "MDQ6VXNlcjE0ODQ4NTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1484859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pararthshah", "html_url": "https://github.com/pararthshah", "followers_url": "https://api.github.com/users/pararthshah/followers", "following_url": "https://api.github.com/users/pararthshah/following{/other_user}", "gists_url": "https://api.github.com/users/pararthshah/gists{/gist_id}", "starred_url": "https://api.github.com/users/pararthshah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pararthshah/subscriptions", "organizations_url": "https://api.github.com/users/pararthshah/orgs", "repos_url": "https://api.github.com/users/pararthshah/repos", "events_url": "https://api.github.com/users/pararthshah/events{/privacy}", "received_events_url": "https://api.github.com/users/pararthshah/received_events", "type": "User", "site_admin": false}, "body": "Suppose the input tensor shape is (M1, M2, M3, M4) and dim=2. We need to select the kth largest elements in the M3 dimension of every slice of the tensor, so there will be a total of M1xM2xM4 slices along which selection is to be done. We are parallelizing this across blocks by spawning a separate block for each slice along which we need to select, so number of blocks = inputSlices = M1xM2xM4.\r\n\r\nThis is the same logic as used in THCTensorTopK.cu, so I assumed it was self-explanatory. But I'm happy to add documentation to both files.", "created_at": "2018-06-19T16:17:25Z", "updated_at": "2018-11-23T15:45:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/8406#discussion_r196490783", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8406", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196490783"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8406#discussion_r196490783"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8406"}}, "body_html": "<p>Suppose the input tensor shape is (M1, M2, M3, M4) and dim=2. We need to select the kth largest elements in the M3 dimension of every slice of the tensor, so there will be a total of M1xM2xM4 slices along which selection is to be done. We are parallelizing this across blocks by spawning a separate block for each slice along which we need to select, so number of blocks = inputSlices = M1xM2xM4.</p>\n<p>This is the same logic as used in THCTensorTopK.cu, so I assumed it was self-explanatory. But I'm happy to add documentation to both files.</p>", "body_text": "Suppose the input tensor shape is (M1, M2, M3, M4) and dim=2. We need to select the kth largest elements in the M3 dimension of every slice of the tensor, so there will be a total of M1xM2xM4 slices along which selection is to be done. We are parallelizing this across blocks by spawning a separate block for each slice along which we need to select, so number of blocks = inputSlices = M1xM2xM4.\nThis is the same logic as used in THCTensorTopK.cu, so I assumed it was self-explanatory. But I'm happy to add documentation to both files.", "in_reply_to_id": 195771637}