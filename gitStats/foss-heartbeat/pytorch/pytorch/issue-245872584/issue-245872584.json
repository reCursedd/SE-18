{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2221", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2221/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2221/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2221/events", "html_url": "https://github.com/pytorch/pytorch/pull/2221", "id": 245872584, "node_id": "MDExOlB1bGxSZXF1ZXN0MTMyNjI4MDM4", "number": 2221, "title": "[bugfix] in bce_with_logits logsumexp calculation", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-07-26T22:24:18Z", "updated_at": "2017-07-27T00:28:57Z", "closed_at": "2017-07-27T00:28:57Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2221", "html_url": "https://github.com/pytorch/pytorch/pull/2221", "diff_url": "https://github.com/pytorch/pytorch/pull/2221.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2221.patch"}, "body_html": "<p>Hi,</p>\n<p>In <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"245191487\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2195\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/2195/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/2195\">#2195</a> I re-formulated the bce with logits to not use abs (so that it could provide a gradient at an input of 0). In this re-formulation, I missed a <code>-</code> sign on the <code>clamp</code> which leads to numerical instability with large negative input values - leading to the wrong output.</p>\n<p>This PR adds a fix for this, Sorry for introducing the bug in the first place.</p>\n<p>Details about the bug:</p>\n<p>For example,</p>\n<div class=\"highlight highlight-source-python\"><pre>inp <span class=\"pl-k\">=</span> Variable(torch.FloatTensor([<span class=\"pl-k\">-</span><span class=\"pl-c1\">100</span>]))\ntar <span class=\"pl-k\">=</span> Variable(torch.FloatTensor([<span class=\"pl-c1\">0</span>]))\nnn.BCELoss()(nn.Sigmoid()(inp), tar)   <span class=\"pl-c\"><span class=\"pl-c\">#</span> returns ~0 </span>\nnn.BCEWithLogitsLoss()(inp, tar)  <span class=\"pl-c\"><span class=\"pl-c\">#</span>return inf</span></pre></div>\n<p>This is because the (wrong) calculation was:</p>\n<pre><code>max_val = max(inp, 0)\nloss = inp - inp*tar + max_val + log(e^-max_val + e^(-inp -max_val))\n</code></pre>\n<p>whereas <code>max_val</code> should be <code>max_val = max(-inp, 0)</code> (missing <code>-</code> sign).</p>\n<p>This didn't present an issue until unless with large <code>inp</code> where <code>e(-inp -max_val)</code> overflows giving <code>inf</code>. For example:</p>\n<pre><code>nn.BCELoss()(nn.Sigmoid()(inp), tar)\n```python\ninp = Variable(torch.FloatTensor([-50]))\ntar = Variable(torch.FloatTensor([0]))\nnn.BCELoss()(nn.Sigmoid()(inp), tar)   # returns ~0 \nnn.BCEWithLogitsLoss()(inp, tar)  #returns 0\n</code></pre>\n<p>are equal (with reasonable precision).</p>\n<p>I have now added a test using FloatTensors explicitly that shows this overflow. And fixed it by adding the <code>-</code> sign.</p>", "body_text": "Hi,\nIn #2195 I re-formulated the bce with logits to not use abs (so that it could provide a gradient at an input of 0). In this re-formulation, I missed a - sign on the clamp which leads to numerical instability with large negative input values - leading to the wrong output.\nThis PR adds a fix for this, Sorry for introducing the bug in the first place.\nDetails about the bug:\nFor example,\ninp = Variable(torch.FloatTensor([-100]))\ntar = Variable(torch.FloatTensor([0]))\nnn.BCELoss()(nn.Sigmoid()(inp), tar)   # returns ~0 \nnn.BCEWithLogitsLoss()(inp, tar)  #return inf\nThis is because the (wrong) calculation was:\nmax_val = max(inp, 0)\nloss = inp - inp*tar + max_val + log(e^-max_val + e^(-inp -max_val))\n\nwhereas max_val should be max_val = max(-inp, 0) (missing - sign).\nThis didn't present an issue until unless with large inp where e(-inp -max_val) overflows giving inf. For example:\nnn.BCELoss()(nn.Sigmoid()(inp), tar)\n```python\ninp = Variable(torch.FloatTensor([-50]))\ntar = Variable(torch.FloatTensor([0]))\nnn.BCELoss()(nn.Sigmoid()(inp), tar)   # returns ~0 \nnn.BCEWithLogitsLoss()(inp, tar)  #returns 0\n\nare equal (with reasonable precision).\nI have now added a test using FloatTensors explicitly that shows this overflow. And fixed it by adding the - sign.", "body": "Hi, \r\n\r\nIn #2195 I re-formulated the bce with logits to not use abs (so that it could provide a gradient at an input of 0). In this re-formulation, I missed a `-` sign on the `clamp` which leads to numerical instability with large negative input values - leading to the wrong output. \r\n\r\nThis PR adds a fix for this, Sorry for introducing the bug in the first place.\r\n\r\nDetails about the bug:\r\n\r\nFor example,\r\n```python\r\ninp = Variable(torch.FloatTensor([-100]))\r\ntar = Variable(torch.FloatTensor([0]))\r\nnn.BCELoss()(nn.Sigmoid()(inp), tar)   # returns ~0 \r\nnn.BCEWithLogitsLoss()(inp, tar)  #return inf\r\n```\r\n\r\nThis is because the (wrong) calculation was:\r\n```\r\nmax_val = max(inp, 0)\r\nloss = inp - inp*tar + max_val + log(e^-max_val + e^(-inp -max_val))\r\n```\r\nwhereas `max_val` should be `max_val = max(-inp, 0)` (missing `-` sign). \r\n\r\nThis didn't present an issue until unless with large `inp` where `e(-inp -max_val)` overflows giving `inf`. For example:\r\n```\r\nnn.BCELoss()(nn.Sigmoid()(inp), tar)\r\n```python\r\ninp = Variable(torch.FloatTensor([-50]))\r\ntar = Variable(torch.FloatTensor([0]))\r\nnn.BCELoss()(nn.Sigmoid()(inp), tar)   # returns ~0 \r\nnn.BCEWithLogitsLoss()(inp, tar)  #returns 0\r\n```\r\nare equal (with reasonable precision).\r\n\r\nI have now added a test using FloatTensors explicitly that shows this overflow. And fixed it by adding the `-` sign. \r\n\r\n"}