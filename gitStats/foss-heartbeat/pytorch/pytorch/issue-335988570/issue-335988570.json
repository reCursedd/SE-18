{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8913", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8913/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8913/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8913/events", "html_url": "https://github.com/pytorch/pytorch/issues/8913", "id": 335988570, "node_id": "MDU6SXNzdWUzMzU5ODg1NzA=", "number": 8913, "title": "Multiprocessing Self Test Error", "user": {"login": "syed-ahmed", "id": 8906225, "node_id": "MDQ6VXNlcjg5MDYyMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8906225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syed-ahmed", "html_url": "https://github.com/syed-ahmed", "followers_url": "https://api.github.com/users/syed-ahmed/followers", "following_url": "https://api.github.com/users/syed-ahmed/following{/other_user}", "gists_url": "https://api.github.com/users/syed-ahmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/syed-ahmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syed-ahmed/subscriptions", "organizations_url": "https://api.github.com/users/syed-ahmed/orgs", "repos_url": "https://api.github.com/users/syed-ahmed/repos", "events_url": "https://api.github.com/users/syed-ahmed/events{/privacy}", "received_events_url": "https://api.github.com/users/syed-ahmed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-26T20:59:55Z", "updated_at": "2018-07-02T18:02:59Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Getting this error in multi processing self test. The bug surfaces when I issue the <code>run_test</code> command like two more times one after another.</p>\n<p>System:<br>\nPytorch (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/cca247635c6edb323176eeac7a18d3e9ab71c558/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/cca247635c6edb323176eeac7a18d3e9ab71c558\"><tt>cca2476</tt></a>), CUDA 9, V100</p>\n<pre><code>Running test_multiprocessing ...\ns..........FTraceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n    send_bytes(obj)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n    self._send(header + buf)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n    n = write(self._handle, buf)\nBrokenPipeError: [Errno 32] Broken pipe\n.EFTraceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 112, in reduce_storage\n    metadata = storage._share_filename_()\nRuntimeError: unable to open shared memory object &lt;/torch_34959_822569775&gt; in read-write mode at ../aten/src/TH/THAllocator.cpp:334\n</code></pre>", "body_text": "Getting this error in multi processing self test. The bug surfaces when I issue the run_test command like two more times one after another.\nSystem:\nPytorch (cca2476), CUDA 9, V100\nRunning test_multiprocessing ...\ns..........FTraceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n    send_bytes(obj)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n    self._send(header + buf)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n    n = write(self._handle, buf)\nBrokenPipeError: [Errno 32] Broken pipe\n.EFTraceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 112, in reduce_storage\n    metadata = storage._share_filename_()\nRuntimeError: unable to open shared memory object </torch_34959_822569775> in read-write mode at ../aten/src/TH/THAllocator.cpp:334", "body": "Getting this error in multi processing self test. The bug surfaces when I issue the `run_test` command like two more times one after another.\r\n\r\nSystem:\r\nPytorch (cca24763), CUDA 9, V100\r\n\r\n```\r\nRunning test_multiprocessing ...\r\ns..........FTraceback (most recent call last):\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\r\n    send_bytes(obj)\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\r\n    self._send_bytes(m[offset:offset + size])\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\r\n    self._send(header + buf)\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\r\n    n = write(self._handle, buf)\r\nBrokenPipeError: [Errno 32] Broken pipe\r\n.EFTraceback (most recent call last):\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 112, in reduce_storage\r\n    metadata = storage._share_filename_()\r\nRuntimeError: unable to open shared memory object </torch_34959_822569775> in read-write mode at ../aten/src/TH/THAllocator.cpp:334\r\n```"}