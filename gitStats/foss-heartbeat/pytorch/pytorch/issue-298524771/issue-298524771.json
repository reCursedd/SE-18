{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5305", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5305/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5305/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5305/events", "html_url": "https://github.com/pytorch/pytorch/issues/5305", "id": 298524771, "node_id": "MDU6SXNzdWUyOTg1MjQ3NzE=", "number": 5305, "title": "Unrelated error message when passing tensors to nn.Embedding", "user": {"login": "miguelvr", "id": 7456627, "node_id": "MDQ6VXNlcjc0NTY2Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7456627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miguelvr", "html_url": "https://github.com/miguelvr", "followers_url": "https://api.github.com/users/miguelvr/followers", "following_url": "https://api.github.com/users/miguelvr/following{/other_user}", "gists_url": "https://api.github.com/users/miguelvr/gists{/gist_id}", "starred_url": "https://api.github.com/users/miguelvr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miguelvr/subscriptions", "organizations_url": "https://api.github.com/users/miguelvr/orgs", "repos_url": "https://api.github.com/users/miguelvr/repos", "events_url": "https://api.github.com/users/miguelvr/events{/privacy}", "received_events_url": "https://api.github.com/users/miguelvr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-20T09:33:31Z", "updated_at": "2018-02-20T12:03:42Z", "closed_at": "2018-02-20T12:03:42Z", "author_association": "NONE", "body_html": "<h3>Summary</h3>\n<p>By mistake, I passed a tensor instead of a variable to the nn.Embedding layer. This usually yields and error message related to the wrong input format.</p>\n<p>However, this time I got a totally unrelated error message.<br>\nDetails bellow.</p>\n<h3>Installation Details</h3>\n<ul>\n<li>OS: macOS</li>\n<li>PyTorch version: 0.3.1</li>\n<li>PyTorch Installation: conda</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: None</li>\n</ul>\n<h3>Example script</h3>\n<pre><code>import torch\nimport torch.nn as nn\n\nemb = nn.Embedding(10, 64)\nx = torch.LongTensor([1,2,3])\nprint(emb(x))\n</code></pre>\n<h3>Error message</h3>\n<pre><code>RuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-7-fa4557a58eb8&gt; in &lt;module&gt;()\n----&gt; 1 emb(x)\n\n~/anaconda2/envs/oxford-deep-nlp/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    355             result = self._slow_forward(*input, **kwargs)\n    356         else:\n--&gt; 357             result = self.forward(*input, **kwargs)\n    358         for hook in self._forward_hooks.values():\n    359             hook_result = hook(self, input, result)\n\n~/anaconda2/envs/oxford-deep-nlp/lib/python3.6/site-packages/torch/nn/modules/sparse.py in forward(self, input)\n    101             input, self.weight,\n    102             padding_idx, self.max_norm, self.norm_type,\n--&gt; 103             self.scale_grad_by_freq, self.sparse\n    104         )\n    105\n</code></pre>", "body_text": "Summary\nBy mistake, I passed a tensor instead of a variable to the nn.Embedding layer. This usually yields and error message related to the wrong input format.\nHowever, this time I got a totally unrelated error message.\nDetails bellow.\nInstallation Details\n\nOS: macOS\nPyTorch version: 0.3.1\nPyTorch Installation: conda\nPython version: 3.6\nCUDA/cuDNN version: None\n\nExample script\nimport torch\nimport torch.nn as nn\n\nemb = nn.Embedding(10, 64)\nx = torch.LongTensor([1,2,3])\nprint(emb(x))\n\nError message\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-7-fa4557a58eb8> in <module>()\n----> 1 emb(x)\n\n~/anaconda2/envs/oxford-deep-nlp/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    355             result = self._slow_forward(*input, **kwargs)\n    356         else:\n--> 357             result = self.forward(*input, **kwargs)\n    358         for hook in self._forward_hooks.values():\n    359             hook_result = hook(self, input, result)\n\n~/anaconda2/envs/oxford-deep-nlp/lib/python3.6/site-packages/torch/nn/modules/sparse.py in forward(self, input)\n    101             input, self.weight,\n    102             padding_idx, self.max_norm, self.norm_type,\n--> 103             self.scale_grad_by_freq, self.sparse\n    104         )\n    105", "body": "### Summary\r\n\r\nBy mistake, I passed a tensor instead of a variable to the nn.Embedding layer. This usually yields and error message related to the wrong input format.\r\n\r\nHowever, this time I got a totally unrelated error message.\r\nDetails bellow.\r\n\r\n### Installation Details\r\n\r\n- OS: macOS\r\n- PyTorch version: 0.3.1\r\n- PyTorch Installation: conda\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: None\r\n\r\n### Example script\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nemb = nn.Embedding(10, 64)\r\nx = torch.LongTensor([1,2,3])\r\nprint(emb(x))\r\n```\r\n\r\n### Error message\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-fa4557a58eb8> in <module>()\r\n----> 1 emb(x)\r\n\r\n~/anaconda2/envs/oxford-deep-nlp/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    355             result = self._slow_forward(*input, **kwargs)\r\n    356         else:\r\n--> 357             result = self.forward(*input, **kwargs)\r\n    358         for hook in self._forward_hooks.values():\r\n    359             hook_result = hook(self, input, result)\r\n\r\n~/anaconda2/envs/oxford-deep-nlp/lib/python3.6/site-packages/torch/nn/modules/sparse.py in forward(self, input)\r\n    101             input, self.weight,\r\n    102             padding_idx, self.max_norm, self.norm_type,\r\n--> 103             self.scale_grad_by_freq, self.sparse\r\n    104         )\r\n    105\r\n```\r\n\r\n"}