{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/282886056", "html_url": "https://github.com/pytorch/pytorch/issues/871#issuecomment-282886056", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/871", "id": 282886056, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Mjg4NjA1Ng==", "user": {"login": "ankitkv", "id": 3175634, "node_id": "MDQ6VXNlcjMxNzU2MzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/3175634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankitkv", "html_url": "https://github.com/ankitkv", "followers_url": "https://api.github.com/users/ankitkv/followers", "following_url": "https://api.github.com/users/ankitkv/following{/other_user}", "gists_url": "https://api.github.com/users/ankitkv/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankitkv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankitkv/subscriptions", "organizations_url": "https://api.github.com/users/ankitkv/orgs", "repos_url": "https://api.github.com/users/ankitkv/repos", "events_url": "https://api.github.com/users/ankitkv/events{/privacy}", "received_events_url": "https://api.github.com/users/ankitkv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-27T23:03:56Z", "updated_at": "2017-02-28T06:51:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In case this is helpful to anyone, a possible temporary workaround is to use</p>\n<pre><code>_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\n</code></pre>\n<p>where <code>log_dist</code> is batchwise log probabilities (e.g. output of <code>F.log_softmax</code>).</p>", "body_text": "In case this is helpful to anyone, a possible temporary workaround is to use\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\n\nwhere log_dist is batchwise log probabilities (e.g. output of F.log_softmax).", "body": "In case this is helpful to anyone, a possible temporary workaround is to use\r\n```\r\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\r\n```\r\nwhere `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`)."}