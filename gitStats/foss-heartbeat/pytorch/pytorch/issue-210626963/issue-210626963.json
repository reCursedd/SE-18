{"url": "https://api.github.com/repos/pytorch/pytorch/issues/871", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/871/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/871/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/871/events", "html_url": "https://github.com/pytorch/pytorch/issues/871", "id": 210626963, "node_id": "MDU6SXNzdWUyMTA2MjY5NjM=", "number": 871, "title": "GPU torch.multinomial produces an out-of-bounds index", "user": {"login": "ankitkv", "id": 3175634, "node_id": "MDQ6VXNlcjMxNzU2MzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/3175634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankitkv", "html_url": "https://github.com/ankitkv", "followers_url": "https://api.github.com/users/ankitkv/followers", "following_url": "https://api.github.com/users/ankitkv/following{/other_user}", "gists_url": "https://api.github.com/users/ankitkv/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankitkv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankitkv/subscriptions", "organizations_url": "https://api.github.com/users/ankitkv/orgs", "repos_url": "https://api.github.com/users/ankitkv/repos", "events_url": "https://api.github.com/users/ankitkv/events{/privacy}", "received_events_url": "https://api.github.com/users/ankitkv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-02-27T22:27:10Z", "updated_at": "2017-03-16T06:04:02Z", "closed_at": "2017-03-01T17:30:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p><code>torch.multinomial</code> on the GPU can produce indices that are out of bounds.</p>\n<p>Consider the following code:</p>\n<pre><code>from __future__ import print_function\nimport torch\n\ni = 0\nwhile True:\n    logdist = torch.zeros(6, 6).cuda()  # no problem without .cuda()\n    # the following is not logically correct, but is the fastest way I've found to reproduce the bug\n    logdist.log_normal_(mean=0.0, std=2.5)\n    prob = torch.exp(logdist)\n    sampled = torch.multinomial(prob, 1)  # multinomial is supposed to reweight to get probabilities\n    if ((sampled &gt; 5).sum()):\n        print('\\niter', i)\n        print('\\nprob:', prob)\n        print('sampled:', sampled)\n        break\n    if i % 5000 == 0:\n        print('iter', i)\n    i += 1\n</code></pre>\n<p>Here's output from an example run:</p>\n<pre><code>iter 0\niter 5000\n\niter 6491\n\nprob: \n 3.0595e+38  1.2861e+00  5.2613e+37  1.3855e+00  2.9041e+00  1.2441e+01\n 1.0531e+01  1.0103e+00  2.0693e+00  1.8121e+00  1.6328e+00  6.0454e+10\n 1.5679e+03  1.5869e+00  1.5553e+03  5.1932e+03  5.0801e+01  1.3416e+00\n 2.2532e+00  1.5512e+00  3.1946e+01  1.5208e+00  8.8690e+00  1.5255e+00\n 1.6197e+02  1.0395e+00  1.1355e+38  3.9969e+00  8.2150e+00  1.5104e+05\n 1.1240e+00  1.1315e+00  7.9896e+00  6.8996e+00  2.1447e+00  1.3858e+00\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\n\nsampled: \n 9.0078e+18\n 5.0000e+00\n 3.0000e+00\n 3.0000e+00\n 2.0000e+00\n 1.0000e+00\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\n</code></pre>\n<p>Here, the sampled indices should be between 0 to 5, but one of the values is  <code>9.0078e+18</code>.</p>\n<p>The iteration at which this happens is random.</p>\n<p>The following code uses probabilities closer to actual probabilities</p>\n<pre><code>from __future__ import print_function\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ni = 0\nwhile True:\n    data = torch.zeros(6, 6).cuda()  # no problem without .cuda()\n    data.normal_(mean=0.0, std=2.5)\n    logdist = F.log_softmax(Variable(data)).data\n    prob = torch.exp(logdist)  # prob.sum(1) are 1s.\n    sampled = torch.multinomial(prob, 1)\n    if ((sampled &gt; 5).sum()):\n        print('\\niter', i)\n        print('\\nprob:', prob)\n        print('sampled:', sampled)\n        break\n    if i % 10000 == 0:\n        print('iter', i)\n    i += 1\n</code></pre>\n<p>but it has the same problem (the problem occurs much later on average though). Example output snippet:</p>\n<pre><code>...\niter 27310000\niter 27320000\niter 27330000\niter 27340000\n\niter 27346838\n\nprob: \n 0.0017  0.8274  0.1657  0.0004  0.0017  0.0031\n 0.0123  0.0157  0.0001  0.0020  0.0015  0.9683\n 0.0067  0.1369  0.8092  0.0422  0.0030  0.0021\n 0.1306  0.0887  0.7521  0.0024  0.0005  0.0258\n 0.0425  0.5981  0.0140  0.0565  0.1005  0.1885\n 0.0006  0.0017  0.9819  0.0114  0.0038  0.0007\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\n\nsampled: \n 1.0000e+00\n 4.1752e+18\n 1.0000e+00\n 2.0000e+00\n 1.0000e+00\n 2.0000e+00\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\n</code></pre>", "body_text": "torch.multinomial on the GPU can produce indices that are out of bounds.\nConsider the following code:\nfrom __future__ import print_function\nimport torch\n\ni = 0\nwhile True:\n    logdist = torch.zeros(6, 6).cuda()  # no problem without .cuda()\n    # the following is not logically correct, but is the fastest way I've found to reproduce the bug\n    logdist.log_normal_(mean=0.0, std=2.5)\n    prob = torch.exp(logdist)\n    sampled = torch.multinomial(prob, 1)  # multinomial is supposed to reweight to get probabilities\n    if ((sampled > 5).sum()):\n        print('\\niter', i)\n        print('\\nprob:', prob)\n        print('sampled:', sampled)\n        break\n    if i % 5000 == 0:\n        print('iter', i)\n    i += 1\n\nHere's output from an example run:\niter 0\niter 5000\n\niter 6491\n\nprob: \n 3.0595e+38  1.2861e+00  5.2613e+37  1.3855e+00  2.9041e+00  1.2441e+01\n 1.0531e+01  1.0103e+00  2.0693e+00  1.8121e+00  1.6328e+00  6.0454e+10\n 1.5679e+03  1.5869e+00  1.5553e+03  5.1932e+03  5.0801e+01  1.3416e+00\n 2.2532e+00  1.5512e+00  3.1946e+01  1.5208e+00  8.8690e+00  1.5255e+00\n 1.6197e+02  1.0395e+00  1.1355e+38  3.9969e+00  8.2150e+00  1.5104e+05\n 1.1240e+00  1.1315e+00  7.9896e+00  6.8996e+00  2.1447e+00  1.3858e+00\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\n\nsampled: \n 9.0078e+18\n 5.0000e+00\n 3.0000e+00\n 3.0000e+00\n 2.0000e+00\n 1.0000e+00\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\n\nHere, the sampled indices should be between 0 to 5, but one of the values is  9.0078e+18.\nThe iteration at which this happens is random.\nThe following code uses probabilities closer to actual probabilities\nfrom __future__ import print_function\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ni = 0\nwhile True:\n    data = torch.zeros(6, 6).cuda()  # no problem without .cuda()\n    data.normal_(mean=0.0, std=2.5)\n    logdist = F.log_softmax(Variable(data)).data\n    prob = torch.exp(logdist)  # prob.sum(1) are 1s.\n    sampled = torch.multinomial(prob, 1)\n    if ((sampled > 5).sum()):\n        print('\\niter', i)\n        print('\\nprob:', prob)\n        print('sampled:', sampled)\n        break\n    if i % 10000 == 0:\n        print('iter', i)\n    i += 1\n\nbut it has the same problem (the problem occurs much later on average though). Example output snippet:\n...\niter 27310000\niter 27320000\niter 27330000\niter 27340000\n\niter 27346838\n\nprob: \n 0.0017  0.8274  0.1657  0.0004  0.0017  0.0031\n 0.0123  0.0157  0.0001  0.0020  0.0015  0.9683\n 0.0067  0.1369  0.8092  0.0422  0.0030  0.0021\n 0.1306  0.0887  0.7521  0.0024  0.0005  0.0258\n 0.0425  0.5981  0.0140  0.0565  0.1005  0.1885\n 0.0006  0.0017  0.9819  0.0114  0.0038  0.0007\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\n\nsampled: \n 1.0000e+00\n 4.1752e+18\n 1.0000e+00\n 2.0000e+00\n 1.0000e+00\n 2.0000e+00\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]", "body": "`torch.multinomial` on the GPU can produce indices that are out of bounds.\r\n\r\nConsider the following code:\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport torch\r\n\r\ni = 0\r\nwhile True:\r\n    logdist = torch.zeros(6, 6).cuda()  # no problem without .cuda()\r\n    # the following is not logically correct, but is the fastest way I've found to reproduce the bug\r\n    logdist.log_normal_(mean=0.0, std=2.5)\r\n    prob = torch.exp(logdist)\r\n    sampled = torch.multinomial(prob, 1)  # multinomial is supposed to reweight to get probabilities\r\n    if ((sampled > 5).sum()):\r\n        print('\\niter', i)\r\n        print('\\nprob:', prob)\r\n        print('sampled:', sampled)\r\n        break\r\n    if i % 5000 == 0:\r\n        print('iter', i)\r\n    i += 1\r\n```\r\n\r\nHere's output from an example run:\r\n\r\n```\r\niter 0\r\niter 5000\r\n\r\niter 6491\r\n\r\nprob: \r\n 3.0595e+38  1.2861e+00  5.2613e+37  1.3855e+00  2.9041e+00  1.2441e+01\r\n 1.0531e+01  1.0103e+00  2.0693e+00  1.8121e+00  1.6328e+00  6.0454e+10\r\n 1.5679e+03  1.5869e+00  1.5553e+03  5.1932e+03  5.0801e+01  1.3416e+00\r\n 2.2532e+00  1.5512e+00  3.1946e+01  1.5208e+00  8.8690e+00  1.5255e+00\r\n 1.6197e+02  1.0395e+00  1.1355e+38  3.9969e+00  8.2150e+00  1.5104e+05\r\n 1.1240e+00  1.1315e+00  7.9896e+00  6.8996e+00  2.1447e+00  1.3858e+00\r\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\r\n\r\nsampled: \r\n 9.0078e+18\r\n 5.0000e+00\r\n 3.0000e+00\r\n 3.0000e+00\r\n 2.0000e+00\r\n 1.0000e+00\r\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\r\n```\r\n\r\nHere, the sampled indices should be between 0 to 5, but one of the values is  `9.0078e+18`.\r\n\r\nThe iteration at which this happens is random.\r\n\r\nThe following code uses probabilities closer to actual probabilities\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn.functional as F\r\n\r\ni = 0\r\nwhile True:\r\n    data = torch.zeros(6, 6).cuda()  # no problem without .cuda()\r\n    data.normal_(mean=0.0, std=2.5)\r\n    logdist = F.log_softmax(Variable(data)).data\r\n    prob = torch.exp(logdist)  # prob.sum(1) are 1s.\r\n    sampled = torch.multinomial(prob, 1)\r\n    if ((sampled > 5).sum()):\r\n        print('\\niter', i)\r\n        print('\\nprob:', prob)\r\n        print('sampled:', sampled)\r\n        break\r\n    if i % 10000 == 0:\r\n        print('iter', i)\r\n    i += 1\r\n```\r\n\r\nbut it has the same problem (the problem occurs much later on average though). Example output snippet:\r\n\r\n```\r\n...\r\niter 27310000\r\niter 27320000\r\niter 27330000\r\niter 27340000\r\n\r\niter 27346838\r\n\r\nprob: \r\n 0.0017  0.8274  0.1657  0.0004  0.0017  0.0031\r\n 0.0123  0.0157  0.0001  0.0020  0.0015  0.9683\r\n 0.0067  0.1369  0.8092  0.0422  0.0030  0.0021\r\n 0.1306  0.0887  0.7521  0.0024  0.0005  0.0258\r\n 0.0425  0.5981  0.0140  0.0565  0.1005  0.1885\r\n 0.0006  0.0017  0.9819  0.0114  0.0038  0.0007\r\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\r\n\r\nsampled: \r\n 1.0000e+00\r\n 4.1752e+18\r\n 1.0000e+00\r\n 2.0000e+00\r\n 1.0000e+00\r\n 2.0000e+00\r\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\r\n```"}