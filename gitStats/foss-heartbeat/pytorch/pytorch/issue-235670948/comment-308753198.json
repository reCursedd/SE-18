{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/308753198", "html_url": "https://github.com/pytorch/pytorch/issues/1794#issuecomment-308753198", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1794", "id": 308753198, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODc1MzE5OA==", "user": {"login": "rdipietro", "id": 5150559, "node_id": "MDQ6VXNlcjUxNTA1NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5150559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rdipietro", "html_url": "https://github.com/rdipietro", "followers_url": "https://api.github.com/users/rdipietro/followers", "following_url": "https://api.github.com/users/rdipietro/following{/other_user}", "gists_url": "https://api.github.com/users/rdipietro/gists{/gist_id}", "starred_url": "https://api.github.com/users/rdipietro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rdipietro/subscriptions", "organizations_url": "https://api.github.com/users/rdipietro/orgs", "repos_url": "https://api.github.com/users/rdipietro/repos", "events_url": "https://api.github.com/users/rdipietro/events{/privacy}", "received_events_url": "https://api.github.com/users/rdipietro/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-15T14:37:38Z", "updated_at": "2017-06-15T14:37:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>My use case would be bidirectional RNNs without using <code>PackedSequence</code>s or their surrounding processing.</p>\n<p>So for example consider</p>\n<ul>\n<li>Inputs and targets at every time step</li>\n<li>A custom RNN cell, without options like <code>num_layers</code> and <code>bidirectional</code> (which can be handled later)</li>\n<li>A bidirectional RNN with multiple layers created using this cell</li>\n</ul>\n<p>Here we could do something like</p>\n<ul>\n<li>obtain <code>inputs</code> from some other module (which we may want to backprop through)</li>\n<li>compute forward outputs over all inputs (even padding / unused values)</li>\n<li>compute backward outputs over all inputs via\n<ul>\n<li>reverse inputs with <code>reverse_padded_sequence</code></li>\n<li>compute forward</li>\n<li>reverse outputs with <code>reverse_padded_sequence</code></li>\n</ul>\n</li>\n<li>concat forward and reverse outputs</li>\n<li>(repeat this whole process for however many layers we'd like)</li>\n<li>compute final targets</li>\n<li>compute the loss using only valid outputs (via indexing or masking)</li>\n</ul>", "body_text": "My use case would be bidirectional RNNs without using PackedSequences or their surrounding processing.\nSo for example consider\n\nInputs and targets at every time step\nA custom RNN cell, without options like num_layers and bidirectional (which can be handled later)\nA bidirectional RNN with multiple layers created using this cell\n\nHere we could do something like\n\nobtain inputs from some other module (which we may want to backprop through)\ncompute forward outputs over all inputs (even padding / unused values)\ncompute backward outputs over all inputs via\n\nreverse inputs with reverse_padded_sequence\ncompute forward\nreverse outputs with reverse_padded_sequence\n\n\nconcat forward and reverse outputs\n(repeat this whole process for however many layers we'd like)\ncompute final targets\ncompute the loss using only valid outputs (via indexing or masking)", "body": "My use case would be bidirectional RNNs without using `PackedSequence`s or their surrounding processing.\r\n\r\nSo for example consider\r\n- Inputs and targets at every time step\r\n- A custom RNN cell, without options like `num_layers` and `bidirectional` (which can be handled later)\r\n- A bidirectional RNN with multiple layers created using this cell\r\n\r\nHere we could do something like\r\n- obtain `inputs` from some other module (which we may want to backprop through)\r\n- compute forward outputs over all inputs (even padding / unused values)\r\n- compute backward outputs over all inputs via\r\n  - reverse inputs with `reverse_padded_sequence`\r\n  - compute forward\r\n  - reverse outputs with `reverse_padded_sequence`\r\n- concat forward and reverse outputs\r\n- (repeat this whole process for however many layers we'd like)\r\n- compute final targets\r\n- compute the loss using only valid outputs (via indexing or masking)\r\n"}