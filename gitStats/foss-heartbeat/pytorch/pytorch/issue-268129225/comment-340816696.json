{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340816696", "html_url": "https://github.com/pytorch/pytorch/issues/3268#issuecomment-340816696", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3268", "id": 340816696, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDgxNjY5Ng==", "user": {"login": "JasonLC506", "id": 15163234, "node_id": "MDQ6VXNlcjE1MTYzMjM0", "avatar_url": "https://avatars1.githubusercontent.com/u/15163234?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JasonLC506", "html_url": "https://github.com/JasonLC506", "followers_url": "https://api.github.com/users/JasonLC506/followers", "following_url": "https://api.github.com/users/JasonLC506/following{/other_user}", "gists_url": "https://api.github.com/users/JasonLC506/gists{/gist_id}", "starred_url": "https://api.github.com/users/JasonLC506/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JasonLC506/subscriptions", "organizations_url": "https://api.github.com/users/JasonLC506/orgs", "repos_url": "https://api.github.com/users/JasonLC506/repos", "events_url": "https://api.github.com/users/JasonLC506/events{/privacy}", "received_events_url": "https://api.github.com/users/JasonLC506/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-31T16:19:26Z", "updated_at": "2017-10-31T16:19:26Z", "author_association": "NONE", "body_html": "<p>I encounter another problem with using Embedding.<br>\nActually if you replace<br>\n(user_emb * item_emb).sum(1) + user_bias.squeeze(1) + item_bias.squeeze(1)<br>\nwith<br>\n(user_emb * item_emb).sum(1) + user_bias + item_bias<br>\nthe dimension of output becomes odd.</p>\n<p>To temporarily solve the problem, following the Linear module defined in pytorch, use torch.nn.Parameter(torch.FloatTensor(shape)) to replace torch.nn.Embedding(shape)</p>", "body_text": "I encounter another problem with using Embedding.\nActually if you replace\n(user_emb * item_emb).sum(1) + user_bias.squeeze(1) + item_bias.squeeze(1)\nwith\n(user_emb * item_emb).sum(1) + user_bias + item_bias\nthe dimension of output becomes odd.\nTo temporarily solve the problem, following the Linear module defined in pytorch, use torch.nn.Parameter(torch.FloatTensor(shape)) to replace torch.nn.Embedding(shape)", "body": "I encounter another problem with using Embedding.\r\nActually if you replace\r\n(user_emb * item_emb).sum(1) + user_bias.squeeze(1) + item_bias.squeeze(1) \r\nwith\r\n(user_emb * item_emb).sum(1) + user_bias + item_bias\r\nthe dimension of output becomes odd.\r\n\r\nTo temporarily solve the problem, following the Linear module defined in pytorch, use torch.nn.Parameter(torch.FloatTensor(shape)) to replace torch.nn.Embedding(shape)"}