{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3268", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3268/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3268/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3268/events", "html_url": "https://github.com/pytorch/pytorch/issues/3268", "id": 268129225, "node_id": "MDU6SXNzdWUyNjgxMjkyMjU=", "number": 3268, "title": "Will squeeze() influence gradient?", "user": {"login": "wilsonlym", "id": 2420569, "node_id": "MDQ6VXNlcjI0MjA1Njk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2420569?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wilsonlym", "html_url": "https://github.com/wilsonlym", "followers_url": "https://api.github.com/users/wilsonlym/followers", "following_url": "https://api.github.com/users/wilsonlym/following{/other_user}", "gists_url": "https://api.github.com/users/wilsonlym/gists{/gist_id}", "starred_url": "https://api.github.com/users/wilsonlym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wilsonlym/subscriptions", "organizations_url": "https://api.github.com/users/wilsonlym/orgs", "repos_url": "https://api.github.com/users/wilsonlym/repos", "events_url": "https://api.github.com/users/wilsonlym/events{/privacy}", "received_events_url": "https://api.github.com/users/wilsonlym/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-10-24T17:58:47Z", "updated_at": "2017-12-13T15:44:24Z", "closed_at": "2017-12-12T22:50:36Z", "author_association": "NONE", "body_html": "<p>Hi all, recently I am implementing matrix factorization models in pytorch and find something interesting.<br>\nHere are two models:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Model1</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">n_users</span>, <span class=\"pl-smi\">n_items</span>, <span class=\"pl-smi\">user_emb</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-smi\">item_emb</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.user_emb <span class=\"pl-k\">=</span> nn.Embedding(n_users, user_emb)\n        <span class=\"pl-c1\">self</span>.item_emb <span class=\"pl-k\">=</span> nn.Embedding(n_items, item_emb)\n        <span class=\"pl-c1\">self</span>.user_bias <span class=\"pl-k\">=</span> nn.Embedding(n_users, <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.item_bias <span class=\"pl-k\">=</span> nn.Embedding(n_items, <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">users</span>, <span class=\"pl-smi\">items</span>):\n        user_emb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.user_emb(users)\n        item_emb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.item_emb(items)\n        user_bias <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.user_bias(users)\n        item_bias <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.item_bias(items)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>#######################</span>\n        out <span class=\"pl-k\">=</span> user_bias <span class=\"pl-k\">+</span> item_bias\n        out <span class=\"pl-k\">+=</span> (user_emb <span class=\"pl-k\">*</span> item_emb).sum(<span class=\"pl-c1\">1</span>)\n        <span class=\"pl-k\">return</span> out.squeeze(<span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>#######################</span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model2</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">n_users</span>, <span class=\"pl-smi\">n_items</span>, <span class=\"pl-smi\">user_emb</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-smi\">item_emb</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.user_emb <span class=\"pl-k\">=</span> nn.Embedding(n_users, user_emb)\n        <span class=\"pl-c1\">self</span>.item_emb <span class=\"pl-k\">=</span> nn.Embedding(n_items, item_emb)\n        <span class=\"pl-c1\">self</span>.user_bias <span class=\"pl-k\">=</span> nn.Embedding(n_users, <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.item_bias <span class=\"pl-k\">=</span> nn.Embedding(n_items, <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">users</span>, <span class=\"pl-smi\">items</span>):\n        user_emb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.user_emb(users)\n        item_emb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.item_emb(items)\n        user_bias <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.user_bias(users)\n        item_bias <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.item_bias(items)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>########################</span>\n        out <span class=\"pl-k\">=</span> (user_emb <span class=\"pl-k\">*</span> item_emb).sum(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> user_bias.squeeze(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> item_bias.squeeze(<span class=\"pl-c1\">1</span>)\n        <span class=\"pl-k\">return</span> out\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>########################</span></pre></div>\n<p>For Model1, we will get the following warning</p>\n<blockquote>\n<p>lib/python3.5/site-packages/torch/autograd/<em>functions/basic_ops.py:15: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.<br>\nreturn a.add</em>(b)</p>\n</blockquote>\n<p>Does it mean it will be reduced to pointwise add? So I tried the following code:</p>\n<pre><code>loss_func = torch.nn.MSELoss()\n\nmodel1 = Model1(10, 10)\nmodel2 = Model2(10, 10)\nmodel2.load_state_dict(model1.state_dict())\n\nX = autograd.Variable(torch.LongTensor([1, 2]))\nY1= autograd.Variable(torch.LongTensor([2, 3]))\nY2 = autograd.Variable(torch.FloatTensor([4, 5]))\n\ny1 = model1.forward(X, Y1)\nprint(loss_func(y1, Y2))\n\ny1 = model2.forward(X, Y1)\nprint(loss_func(y1, Y2))\n</code></pre>\n<p>And we got the same outputs:</p>\n<blockquote>\n<p>Variable containing:<br>\n12.9656<br>\n[torch.FloatTensor of size 1]</p>\n<p>Variable containing:<br>\n12.9656<br>\n[torch.FloatTensor of size 1]</p>\n</blockquote>\n<p>But for gradient,</p>\n<pre><code>y1 = model1.forward(X, Y1)\nmodel1.zero_grad()\nloss = loss_func(y1, Y2)\nloss.backward()\nfor p in list(model1.parameters()):\n    print(p.grad)\n\ny1 = model2.forward(X, Y1)\nmodel2.zero_grad()\nloss = loss_func(y1, Y2)\nloss.backward()\nfor p in list(model2.parameters()):\n    print(p.grad)\n</code></pre>\n<p>We got quite different results:<br>\nmodel1:</p>\n<blockquote>\n<p>Variable containing:<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n8.2215 -8.7861 -3.2524  5.8750  4.0597<br>\n9.4413  2.6375  0.7764 -4.0542  4.1427<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n[torch.FloatTensor of size 10x5]</p>\n<p>Variable containing:<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n8.4739  -2.0651  -8.1959  -6.7616  11.1125<br>\n6.7948   6.4563   3.0686  11.5589   3.6424<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n0.0000   0.0000   0.0000   0.0000   0.0000<br>\n[torch.FloatTensor of size 10x5]</p>\n<p>Variable containing:<br>\n0.0000<br>\n-3.1769<br>\n-3.9797<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n[torch.FloatTensor of size 10x1]</p>\n<p>Variable containing:<br>\n0.0000<br>\n0.0000<br>\n-3.1769<br>\n-3.9797<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n[torch.FloatTensor of size 10x1]</p>\n</blockquote>\n<p>model2:</p>\n<blockquote>\n<p>Variable containing:<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n3.6496 -3.9003 -1.4438  2.6080  1.8022<br>\n5.2502  1.4667  0.4317 -2.2545  2.3037<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n[torch.FloatTensor of size 10x5]</p>\n<p>Variable containing:<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n3.7617 -0.9167 -3.6383 -3.0016  4.9330<br>\n3.7785  3.5903  1.7064  6.4277  2.0255<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n0.0000  0.0000  0.0000  0.0000  0.0000<br>\n[torch.FloatTensor of size 10x5]</p>\n<p>Variable containing:<br>\n0.0000<br>\n-3.1769<br>\n-3.9797<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n[torch.FloatTensor of size 10x1]</p>\n<p>Variable containing:<br>\n0.0000<br>\n0.0000<br>\n-3.1769<br>\n-3.9797<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n0.0000<br>\n[torch.FloatTensor of size 10x1]</p>\n</blockquote>\n<p>Thus, I am wondering why there is difference in gradient?<br>\nBy the way, I am using pytorch 0.2.0_3 and python 3.5.2. Thanks for your time!</p>", "body_text": "Hi all, recently I am implementing matrix factorization models in pytorch and find something interesting.\nHere are two models:\nclass Model1(nn.Module):\n    def __init__(self, n_users, n_items, user_emb=5, item_emb=5):\n        super().__init__()\n        self.user_emb = nn.Embedding(n_users, user_emb)\n        self.item_emb = nn.Embedding(n_items, item_emb)\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.item_bias = nn.Embedding(n_items, 1)\n\n    def forward(self, users, items):\n        user_emb = self.user_emb(users)\n        item_emb = self.item_emb(items)\n        user_bias = self.user_bias(users)\n        item_bias = self.item_bias(items)\n        ########################\n        out = user_bias + item_bias\n        out += (user_emb * item_emb).sum(1)\n        return out.squeeze(1)\n        ########################\n\n\nclass Model2(nn.Module):\n    def __init__(self, n_users, n_items, user_emb=5, item_emb=5):\n        super().__init__()\n        self.user_emb = nn.Embedding(n_users, user_emb)\n        self.item_emb = nn.Embedding(n_items, item_emb)\n        self.user_bias = nn.Embedding(n_users, 1)\n        self.item_bias = nn.Embedding(n_items, 1)\n\n    def forward(self, users, items):\n        user_emb = self.user_emb(users)\n        item_emb = self.item_emb(items)\n        user_bias = self.user_bias(users)\n        item_bias = self.item_bias(items)\n        #########################\n        out = (user_emb * item_emb).sum(1) + user_bias.squeeze(1) + item_bias.squeeze(1)\n        return out\n        #########################\nFor Model1, we will get the following warning\n\nlib/python3.5/site-packages/torch/autograd/functions/basic_ops.py:15: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\nreturn a.add(b)\n\nDoes it mean it will be reduced to pointwise add? So I tried the following code:\nloss_func = torch.nn.MSELoss()\n\nmodel1 = Model1(10, 10)\nmodel2 = Model2(10, 10)\nmodel2.load_state_dict(model1.state_dict())\n\nX = autograd.Variable(torch.LongTensor([1, 2]))\nY1= autograd.Variable(torch.LongTensor([2, 3]))\nY2 = autograd.Variable(torch.FloatTensor([4, 5]))\n\ny1 = model1.forward(X, Y1)\nprint(loss_func(y1, Y2))\n\ny1 = model2.forward(X, Y1)\nprint(loss_func(y1, Y2))\n\nAnd we got the same outputs:\n\nVariable containing:\n12.9656\n[torch.FloatTensor of size 1]\nVariable containing:\n12.9656\n[torch.FloatTensor of size 1]\n\nBut for gradient,\ny1 = model1.forward(X, Y1)\nmodel1.zero_grad()\nloss = loss_func(y1, Y2)\nloss.backward()\nfor p in list(model1.parameters()):\n    print(p.grad)\n\ny1 = model2.forward(X, Y1)\nmodel2.zero_grad()\nloss = loss_func(y1, Y2)\nloss.backward()\nfor p in list(model2.parameters()):\n    print(p.grad)\n\nWe got quite different results:\nmodel1:\n\nVariable containing:\n0.0000  0.0000  0.0000  0.0000  0.0000\n8.2215 -8.7861 -3.2524  5.8750  4.0597\n9.4413  2.6375  0.7764 -4.0542  4.1427\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.FloatTensor of size 10x5]\nVariable containing:\n0.0000   0.0000   0.0000   0.0000   0.0000\n0.0000   0.0000   0.0000   0.0000   0.0000\n8.4739  -2.0651  -8.1959  -6.7616  11.1125\n6.7948   6.4563   3.0686  11.5589   3.6424\n0.0000   0.0000   0.0000   0.0000   0.0000\n0.0000   0.0000   0.0000   0.0000   0.0000\n0.0000   0.0000   0.0000   0.0000   0.0000\n0.0000   0.0000   0.0000   0.0000   0.0000\n0.0000   0.0000   0.0000   0.0000   0.0000\n0.0000   0.0000   0.0000   0.0000   0.0000\n[torch.FloatTensor of size 10x5]\nVariable containing:\n0.0000\n-3.1769\n-3.9797\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n[torch.FloatTensor of size 10x1]\nVariable containing:\n0.0000\n0.0000\n-3.1769\n-3.9797\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n[torch.FloatTensor of size 10x1]\n\nmodel2:\n\nVariable containing:\n0.0000  0.0000  0.0000  0.0000  0.0000\n3.6496 -3.9003 -1.4438  2.6080  1.8022\n5.2502  1.4667  0.4317 -2.2545  2.3037\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.FloatTensor of size 10x5]\nVariable containing:\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n3.7617 -0.9167 -3.6383 -3.0016  4.9330\n3.7785  3.5903  1.7064  6.4277  2.0255\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.0000  0.0000\n[torch.FloatTensor of size 10x5]\nVariable containing:\n0.0000\n-3.1769\n-3.9797\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n[torch.FloatTensor of size 10x1]\nVariable containing:\n0.0000\n0.0000\n-3.1769\n-3.9797\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n[torch.FloatTensor of size 10x1]\n\nThus, I am wondering why there is difference in gradient?\nBy the way, I am using pytorch 0.2.0_3 and python 3.5.2. Thanks for your time!", "body": "Hi all, recently I am implementing matrix factorization models in pytorch and find something interesting.\r\nHere are two models:\r\n```python\r\nclass Model1(nn.Module):\r\n    def __init__(self, n_users, n_items, user_emb=5, item_emb=5):\r\n        super().__init__()\r\n        self.user_emb = nn.Embedding(n_users, user_emb)\r\n        self.item_emb = nn.Embedding(n_items, item_emb)\r\n        self.user_bias = nn.Embedding(n_users, 1)\r\n        self.item_bias = nn.Embedding(n_items, 1)\r\n\r\n    def forward(self, users, items):\r\n        user_emb = self.user_emb(users)\r\n        item_emb = self.item_emb(items)\r\n        user_bias = self.user_bias(users)\r\n        item_bias = self.item_bias(items)\r\n        ########################\r\n        out = user_bias + item_bias\r\n        out += (user_emb * item_emb).sum(1)\r\n        return out.squeeze(1)\r\n        ########################\r\n\r\n\r\nclass Model2(nn.Module):\r\n    def __init__(self, n_users, n_items, user_emb=5, item_emb=5):\r\n        super().__init__()\r\n        self.user_emb = nn.Embedding(n_users, user_emb)\r\n        self.item_emb = nn.Embedding(n_items, item_emb)\r\n        self.user_bias = nn.Embedding(n_users, 1)\r\n        self.item_bias = nn.Embedding(n_items, 1)\r\n\r\n    def forward(self, users, items):\r\n        user_emb = self.user_emb(users)\r\n        item_emb = self.item_emb(items)\r\n        user_bias = self.user_bias(users)\r\n        item_bias = self.item_bias(items)\r\n        #########################\r\n        out = (user_emb * item_emb).sum(1) + user_bias.squeeze(1) + item_bias.squeeze(1)\r\n        return out\r\n        #########################\r\n```\r\nFor Model1, we will get the following warning\r\n\r\n> lib/python3.5/site-packages/torch/autograd/_functions/basic_ops.py:15: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\r\n>   return a.add_(b)\r\n\r\nDoes it mean it will be reduced to pointwise add? So I tried the following code:\r\n```\r\nloss_func = torch.nn.MSELoss()\r\n\r\nmodel1 = Model1(10, 10)\r\nmodel2 = Model2(10, 10)\r\nmodel2.load_state_dict(model1.state_dict())\r\n\r\nX = autograd.Variable(torch.LongTensor([1, 2]))\r\nY1= autograd.Variable(torch.LongTensor([2, 3]))\r\nY2 = autograd.Variable(torch.FloatTensor([4, 5]))\r\n\r\ny1 = model1.forward(X, Y1)\r\nprint(loss_func(y1, Y2))\r\n\r\ny1 = model2.forward(X, Y1)\r\nprint(loss_func(y1, Y2))\r\n```\r\nAnd we got the same outputs:\r\n\r\n> Variable containing:\r\n>  12.9656\r\n> [torch.FloatTensor of size 1]\r\n> \r\n> Variable containing:\r\n>  12.9656\r\n> [torch.FloatTensor of size 1]\r\n\r\nBut for gradient, \r\n```\r\ny1 = model1.forward(X, Y1)\r\nmodel1.zero_grad()\r\nloss = loss_func(y1, Y2)\r\nloss.backward()\r\nfor p in list(model1.parameters()):\r\n    print(p.grad)\r\n\r\ny1 = model2.forward(X, Y1)\r\nmodel2.zero_grad()\r\nloss = loss_func(y1, Y2)\r\nloss.backward()\r\nfor p in list(model2.parameters()):\r\n    print(p.grad)\r\n```\r\nWe got quite different results:\r\nmodel1:\r\n\r\n> Variable containing:\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  8.2215 -8.7861 -3.2524  5.8750  4.0597\r\n>  9.4413  2.6375  0.7764 -4.0542  4.1427\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n> [torch.FloatTensor of size 10x5]\r\n> \r\n> Variable containing:\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n>   8.4739  -2.0651  -8.1959  -6.7616  11.1125\r\n>   6.7948   6.4563   3.0686  11.5589   3.6424\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n>   0.0000   0.0000   0.0000   0.0000   0.0000\r\n> [torch.FloatTensor of size 10x5]\r\n> \r\n> Variable containing:\r\n>  0.0000\r\n> -3.1769\r\n> -3.9797\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n> [torch.FloatTensor of size 10x1]\r\n> \r\n> Variable containing:\r\n>  0.0000\r\n>  0.0000\r\n> -3.1769\r\n> -3.9797\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n> [torch.FloatTensor of size 10x1]\r\n\r\nmodel2:\r\n\r\n> Variable containing:\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  3.6496 -3.9003 -1.4438  2.6080  1.8022\r\n>  5.2502  1.4667  0.4317 -2.2545  2.3037\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n> [torch.FloatTensor of size 10x5]\r\n> \r\n> Variable containing:\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  3.7617 -0.9167 -3.6383 -3.0016  4.9330\r\n>  3.7785  3.5903  1.7064  6.4277  2.0255\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n>  0.0000  0.0000  0.0000  0.0000  0.0000\r\n> [torch.FloatTensor of size 10x5]\r\n> \r\n> Variable containing:\r\n>  0.0000\r\n> -3.1769\r\n> -3.9797\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n> [torch.FloatTensor of size 10x1]\r\n> \r\n> Variable containing:\r\n>  0.0000\r\n>  0.0000\r\n> -3.1769\r\n> -3.9797\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n>  0.0000\r\n> [torch.FloatTensor of size 10x1]\r\n\r\nThus, I am wondering why there is difference in gradient?\r\nBy the way, I am using pytorch 0.2.0_3 and python 3.5.2. Thanks for your time!"}