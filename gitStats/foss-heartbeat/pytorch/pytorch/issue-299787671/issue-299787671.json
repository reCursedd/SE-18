{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5376", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5376/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5376/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5376/events", "html_url": "https://github.com/pytorch/pytorch/pull/5376", "id": 299787671, "node_id": "MDExOlB1bGxSZXF1ZXN0MTcxMDcwODc5", "number": 5376, "title": "DataParallel: GPU imbalance warning", "user": {"login": "lemairecarl", "id": 13444373, "node_id": "MDQ6VXNlcjEzNDQ0Mzcz", "avatar_url": "https://avatars3.githubusercontent.com/u/13444373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lemairecarl", "html_url": "https://github.com/lemairecarl", "followers_url": "https://api.github.com/users/lemairecarl/followers", "following_url": "https://api.github.com/users/lemairecarl/following{/other_user}", "gists_url": "https://api.github.com/users/lemairecarl/gists{/gist_id}", "starred_url": "https://api.github.com/users/lemairecarl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lemairecarl/subscriptions", "organizations_url": "https://api.github.com/users/lemairecarl/orgs", "repos_url": "https://api.github.com/users/lemairecarl/repos", "events_url": "https://api.github.com/users/lemairecarl/events{/privacy}", "received_events_url": "https://api.github.com/users/lemairecarl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 28, "created_at": "2018-02-23T17:22:39Z", "updated_at": "2018-11-23T15:40:04Z", "closed_at": "2018-02-27T20:30:42Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5376", "html_url": "https://github.com/pytorch/pytorch/pull/5376", "diff_url": "https://github.com/pytorch/pytorch/pull/5376.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5376.patch"}, "body_html": "<p>I have 2 GPUs in my machine: a TITAN X with 12 GB of memory, and a GT710 with 1 GB, that is used for X server. Tensorflow automatically discards the GT710, based on the number of compute cores, if I recall correctly.</p>\n<p>I should have simply set <code>CUDA_VISIBLE_DEVICES</code> so that the GT710 is ignored. But I did not do so, thus when trying code that uses DataParallel, PyTorch wants to use the 1 GB GPU along with the 12 GB one.</p>\n<p>This gets me \"out of memory\" errors, unless I drastically reduce the batch size. Then, it begins training, but it sometimes randomly crashes with a \"illegal memory access\" error (though this may have been fixed in 0.3.1). The latter error might be related to the fact that the GT710 has compute capability 3.5 while the TITAN X has 5.2.</p>\n<p>For 1-2 months I thought that this code was buggy or incompatible with the last version of PyTorch. It's only yesterday, by looking at <code>nvidia-smi</code>, that I saw that data was sent to the GT710. I can be blamed for not having hidden the GT710. But I think this case should be handled better than by \"out of memory\" errors. I think this could save headaches to many folks that use enterprise/university hardware such as mine.</p>\n<p>Maybe we could do similar checking than what Tensorflow does. Or maybe we could do something like this (see proposed changes). Sadly, this won't work; <code>max_memory_allocated()</code> does not return the memory capacity of the GPU. Is there a method that does that?</p>", "body_text": "I have 2 GPUs in my machine: a TITAN X with 12 GB of memory, and a GT710 with 1 GB, that is used for X server. Tensorflow automatically discards the GT710, based on the number of compute cores, if I recall correctly.\nI should have simply set CUDA_VISIBLE_DEVICES so that the GT710 is ignored. But I did not do so, thus when trying code that uses DataParallel, PyTorch wants to use the 1 GB GPU along with the 12 GB one.\nThis gets me \"out of memory\" errors, unless I drastically reduce the batch size. Then, it begins training, but it sometimes randomly crashes with a \"illegal memory access\" error (though this may have been fixed in 0.3.1). The latter error might be related to the fact that the GT710 has compute capability 3.5 while the TITAN X has 5.2.\nFor 1-2 months I thought that this code was buggy or incompatible with the last version of PyTorch. It's only yesterday, by looking at nvidia-smi, that I saw that data was sent to the GT710. I can be blamed for not having hidden the GT710. But I think this case should be handled better than by \"out of memory\" errors. I think this could save headaches to many folks that use enterprise/university hardware such as mine.\nMaybe we could do similar checking than what Tensorflow does. Or maybe we could do something like this (see proposed changes). Sadly, this won't work; max_memory_allocated() does not return the memory capacity of the GPU. Is there a method that does that?", "body": "I have 2 GPUs in my machine: a TITAN X with 12 GB of memory, and a GT710 with 1 GB, that is used for X server. Tensorflow automatically discards the GT710, based on the number of compute cores, if I recall correctly.\r\n\r\nI should have simply set `CUDA_VISIBLE_DEVICES` so that the GT710 is ignored. But I did not do so, thus when trying code that uses DataParallel, PyTorch wants to use the 1 GB GPU along with the 12 GB one.\r\n\r\nThis gets me \"out of memory\" errors, unless I drastically reduce the batch size. Then, it begins training, but it sometimes randomly crashes with a \"illegal memory access\" error (though this may have been fixed in 0.3.1). The latter error might be related to the fact that the GT710 has compute capability 3.5 while the TITAN X has 5.2.\r\n\r\nFor 1-2 months I thought that this code was buggy or incompatible with the last version of PyTorch. It's only yesterday, by looking at `nvidia-smi`, that I saw that data was sent to the GT710. I can be blamed for not having hidden the GT710. But I think this case should be handled better than by \"out of memory\" errors. I think this could save headaches to many folks that use enterprise/university hardware such as mine.\r\n\r\nMaybe we could do similar checking than what Tensorflow does. Or maybe we could do something like this (see proposed changes). Sadly, this won't work; `max_memory_allocated()` does not return the memory capacity of the GPU. Is there a method that does that?"}