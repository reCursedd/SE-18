{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3929", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3929/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3929/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3929/events", "html_url": "https://github.com/pytorch/pytorch/issues/3929", "id": 277574346, "node_id": "MDU6SXNzdWUyNzc1NzQzNDY=", "number": 3929, "title": "Implement fused versions of optimizers", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-28T23:23:05Z", "updated_at": "2017-11-28T23:42:59Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>Our current optimizers are terribly slow when dealing with large sets of relatively small parameters. This is demonstrated in the code example below. The difference is 100x on a Pascal card (100ms vs 1ms). <code>len(params)</code> is approx the number of parameters in a ResNet1001, and sum of their sizes correspond approximately to the the total number of params in that network as well.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time                                                                               \n<span class=\"pl-k\">import</span> torch                                                                              \n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable                                                       \n<span class=\"pl-k\">from</span> torch.optim <span class=\"pl-k\">import</span> Adam                                                              \n                                                                                          \nparams <span class=\"pl-k\">=</span> [Variable(torch.cuda.FloatTensor(<span class=\"pl-c1\">3500</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3000</span>)]\noptimizer <span class=\"pl-k\">=</span> Adam(params)                                                                  \n<span class=\"pl-c1\">sum</span>(params).sum().backward() <span class=\"pl-c\"><span class=\"pl-c\">#</span> create dummy gradients                                     </span>\n                                                                                          \n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Separate params:<span class=\"pl-pds\">'</span></span>)                                                                 \n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):                                                                       \n    start <span class=\"pl-k\">=</span> time.time()                                                                   \n    optimizer.step()                                                                      \n    torch.cuda.synchronize()                                                              \n    end <span class=\"pl-k\">=</span> time.time()                                                                     \n    <span class=\"pl-c1\">print</span>(((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ms<span class=\"pl-pds\">'</span></span>)                                                   \n                                                                                          \n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">80</span>)                                                                           \n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Flat params:<span class=\"pl-pds\">'</span></span>)                                                                     \nsingle_param <span class=\"pl-k\">=</span> Variable(torch.cat([p.data <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> params]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)          \nsingle_optimizer <span class=\"pl-k\">=</span> Adam([single_param])                                                   \nsingle_param.sum().backward()                                                             \n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):                                                                       \n    start <span class=\"pl-k\">=</span> time.time()                                                                   \n    single_optimizer.step()                                                               \n    torch.cuda.synchronize()                                                              \n    end <span class=\"pl-k\">=</span> time.time()                                                                     \n    <span class=\"pl-c1\">print</span>(((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ms<span class=\"pl-pds\">'</span></span>)                                                   </pre></div>\n<hr>\n<p>To implement:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> SGD</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Adam</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> RMSProp</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Adadelta</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Adagrad</li>\n</ul>\n<hr>\n<p>Implementation idea: write a CUDA kernel that accepts a list of tensor descriptors (similarly to <code>torch.cat</code>) that fuses the outer loop of optimizer.</p>", "body_text": "Our current optimizers are terribly slow when dealing with large sets of relatively small parameters. This is demonstrated in the code example below. The difference is 100x on a Pascal card (100ms vs 1ms). len(params) is approx the number of parameters in a ResNet1001, and sum of their sizes correspond approximately to the the total number of params in that network as well.\nimport time                                                                               \nimport torch                                                                              \nfrom torch.autograd import Variable                                                       \nfrom torch.optim import Adam                                                              \n                                                                                          \nparams = [Variable(torch.cuda.FloatTensor(3500), requires_grad=True) for i in range(3000)]\noptimizer = Adam(params)                                                                  \nsum(params).sum().backward() # create dummy gradients                                     \n                                                                                          \nprint('Separate params:')                                                                 \nfor i in range(10):                                                                       \n    start = time.time()                                                                   \n    optimizer.step()                                                                      \n    torch.cuda.synchronize()                                                              \n    end = time.time()                                                                     \n    print(((end - start) * 1000), 'ms')                                                   \n                                                                                          \nprint('-' * 80)                                                                           \nprint('Flat params:')                                                                     \nsingle_param = Variable(torch.cat([p.data for p in params]), requires_grad=True)          \nsingle_optimizer = Adam([single_param])                                                   \nsingle_param.sum().backward()                                                             \nfor i in range(10):                                                                       \n    start = time.time()                                                                   \n    single_optimizer.step()                                                               \n    torch.cuda.synchronize()                                                              \n    end = time.time()                                                                     \n    print(((end - start) * 1000), 'ms')                                                   \n\nTo implement:\n\n SGD\n Adam\n RMSProp\n Adadelta\n Adagrad\n\n\nImplementation idea: write a CUDA kernel that accepts a list of tensor descriptors (similarly to torch.cat) that fuses the outer loop of optimizer.", "body": "Our current optimizers are terribly slow when dealing with large sets of relatively small parameters. This is demonstrated in the code example below. The difference is 100x on a Pascal card (100ms vs 1ms). `len(params)` is approx the number of parameters in a ResNet1001, and sum of their sizes correspond approximately to the the total number of params in that network as well.\r\n\r\n```python\r\nimport time                                                                               \r\nimport torch                                                                              \r\nfrom torch.autograd import Variable                                                       \r\nfrom torch.optim import Adam                                                              \r\n                                                                                          \r\nparams = [Variable(torch.cuda.FloatTensor(3500), requires_grad=True) for i in range(3000)]\r\noptimizer = Adam(params)                                                                  \r\nsum(params).sum().backward() # create dummy gradients                                     \r\n                                                                                          \r\nprint('Separate params:')                                                                 \r\nfor i in range(10):                                                                       \r\n    start = time.time()                                                                   \r\n    optimizer.step()                                                                      \r\n    torch.cuda.synchronize()                                                              \r\n    end = time.time()                                                                     \r\n    print(((end - start) * 1000), 'ms')                                                   \r\n                                                                                          \r\nprint('-' * 80)                                                                           \r\nprint('Flat params:')                                                                     \r\nsingle_param = Variable(torch.cat([p.data for p in params]), requires_grad=True)          \r\nsingle_optimizer = Adam([single_param])                                                   \r\nsingle_param.sum().backward()                                                             \r\nfor i in range(10):                                                                       \r\n    start = time.time()                                                                   \r\n    single_optimizer.step()                                                               \r\n    torch.cuda.synchronize()                                                              \r\n    end = time.time()                                                                     \r\n    print(((end - start) * 1000), 'ms')                                                   \r\n```\r\n\r\n---\r\n\r\nTo implement:\r\n- [ ] SGD\r\n- [ ] Adam\r\n- [ ] RMSProp\r\n- [ ] Adadelta\r\n- [ ] Adagrad\r\n\r\n---\r\n\r\nImplementation idea: write a CUDA kernel that accepts a list of tensor descriptors (similarly to `torch.cat`) that fuses the outer loop of optimizer.\r\n\r\n"}