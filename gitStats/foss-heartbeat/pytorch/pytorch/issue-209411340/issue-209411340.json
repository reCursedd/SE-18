{"url": "https://api.github.com/repos/pytorch/pytorch/issues/821", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/821/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/821/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/821/events", "html_url": "https://github.com/pytorch/pytorch/issues/821", "id": 209411340, "node_id": "MDU6SXNzdWUyMDk0MTEzNDA=", "number": 821, "title": "Custom functions and custom modules.", "user": {"login": "yjxiong", "id": 6830199, "node_id": "MDQ6VXNlcjY4MzAxOTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/6830199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yjxiong", "html_url": "https://github.com/yjxiong", "followers_url": "https://api.github.com/users/yjxiong/followers", "following_url": "https://api.github.com/users/yjxiong/following{/other_user}", "gists_url": "https://api.github.com/users/yjxiong/gists{/gist_id}", "starred_url": "https://api.github.com/users/yjxiong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yjxiong/subscriptions", "organizations_url": "https://api.github.com/users/yjxiong/orgs", "repos_url": "https://api.github.com/users/yjxiong/repos", "events_url": "https://api.github.com/users/yjxiong/events{/privacy}", "received_events_url": "https://api.github.com/users/yjxiong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-02-22T10:27:21Z", "updated_at": "2018-07-21T04:39:47Z", "closed_at": "2018-07-13T02:11:56Z", "author_association": "NONE", "body_html": "<p>I tried writing a custom op and defining a module to wrap it. However, I came across a tedious problem which I feel useful to document here and provide some suggestions.</p>\n<p>I started with a custom op that does very simple reduction as follows</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyFunction</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Function</span>):\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n\t\t<span class=\"pl-c1\">self</span>.shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size()\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">input</span>.mean(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_out</span>):\n\t\t<span class=\"pl-k\">return</span> grad_out.expand(<span class=\"pl-c1\">self</span>.shape) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">float</span>(<span class=\"pl-c1\">self</span>.shape[<span class=\"pl-c1\">1</span>])\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyModule</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\t\t<span class=\"pl-c1\">self</span>.op <span class=\"pl-k\">=</span> MyFunction()\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.op(<span class=\"pl-c1\">input</span>)         </pre></div>\n<p>There you may have noticed that I have made a mistake by saving the instance in the module, provided you have read the documentation carefully and found this line</p>\n<blockquote>\n<p>Each function is meant to be used only once (in the forward pass).</p>\n</blockquote>\n<p>at <a href=\"http://pytorch.org/docs/autograd.html#function\" rel=\"nofollow\">http://pytorch.org/docs/autograd.html#function</a></p>\n<p>However, the above snippet ran without any problem in my model under the single GPU mode, i.e. when no <code>torch.nn.DataParallel</code> is used.</p>\n<p>When I started adding <code>DataParallel</code> to my model, the nightmare began. Pytorch repeatedly threw error like this</p>\n<pre><code>RuntimeError: arguments are located on different GPUs at ...\n</code></pre>\n<p>Once I remove the <code>DataParallel</code> module, the model can run smoothly.</p>\n<p>Since this error happens in the C++ side, there is no stack trace to help debug this problem. I got no clue which arguments lead to the error and had to doubt whether it is a bug in the <code>DataParallel</code> module.</p>\n<p>After spending a lot of time digging into the sources of <code>DataParallel</code> and even <code>THC</code> , I finally realized the mistake I made. So I changed the code to</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyFunction</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Function</span>):\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n\t\t<span class=\"pl-c1\">self</span>.shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size()\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">input</span>.mean(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_out</span>):\n\t\t<span class=\"pl-k\">return</span> grad_out.expand(<span class=\"pl-c1\">self</span>.shape) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">float</span>(<span class=\"pl-c1\">self</span>.shape[<span class=\"pl-c1\">1</span>])\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyModule</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\t\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n\t\t<span class=\"pl-k\">return</span> MyFunction()(<span class=\"pl-c1\">input</span>)</pre></div>\n<p>Then everything began to work as expected.</p>\n<p>I have a few comments afterward:</p>\n<ol>\n<li>If it is bound to fail in the multi-GPU situation, maybe we should also let it fail in the single GPU case.</li>\n<li>The mentioned line in the Doc should state this clearly that if we reuse a <code>Function</code> instance there might be some serious problem.</li>\n<li>One step further, if an instance of <code>Function</code> is meant to be used only once, how about integrating it into the extension mechanism? This may potentially protect many new users from facing this kind of problems.</li>\n</ol>\n<p>After all, thanks to the team for developing this awesome toolbox!</p>\n<p>P.S. I apologize if you feel uncomfortable about the tab indent in the code snippet.</p>", "body_text": "I tried writing a custom op and defining a module to wrap it. However, I came across a tedious problem which I feel useful to document here and provide some suggestions.\nI started with a custom op that does very simple reduction as follows\nimport torch\n\nclass MyFunction(torch.nn.Function):\n\tdef forward(self, input):\n\t\tself.shape = input.size()\n\t\treturn input.mean(dim=1)\n\tdef backward(self, grad_out):\n\t\treturn grad_out.expand(self.shape) / float(self.shape[1])\n\nclass MyModule(torch.nn.Module):\n\tdef __init__(self):\n\t\tself.op = MyFunction()\n\n\tdef forward(self, input):\n\t\treturn self.op(input)         \nThere you may have noticed that I have made a mistake by saving the instance in the module, provided you have read the documentation carefully and found this line\n\nEach function is meant to be used only once (in the forward pass).\n\nat http://pytorch.org/docs/autograd.html#function\nHowever, the above snippet ran without any problem in my model under the single GPU mode, i.e. when no torch.nn.DataParallel is used.\nWhen I started adding DataParallel to my model, the nightmare began. Pytorch repeatedly threw error like this\nRuntimeError: arguments are located on different GPUs at ...\n\nOnce I remove the DataParallel module, the model can run smoothly.\nSince this error happens in the C++ side, there is no stack trace to help debug this problem. I got no clue which arguments lead to the error and had to doubt whether it is a bug in the DataParallel module.\nAfter spending a lot of time digging into the sources of DataParallel and even THC , I finally realized the mistake I made. So I changed the code to\nimport torch\n\nclass MyFunction(torch.nn.Function):\n\tdef forward(self, input):\n\t\tself.shape = input.size()\n\t\treturn input.mean(dim=1)\n\tdef backward(self, grad_out):\n\t\treturn grad_out.expand(self.shape) / float(self.shape[1])\n\nclass MyModule(torch.nn.Module):\n\t\n\tdef forward(self, input):\n\t\treturn MyFunction()(input)\nThen everything began to work as expected.\nI have a few comments afterward:\n\nIf it is bound to fail in the multi-GPU situation, maybe we should also let it fail in the single GPU case.\nThe mentioned line in the Doc should state this clearly that if we reuse a Function instance there might be some serious problem.\nOne step further, if an instance of Function is meant to be used only once, how about integrating it into the extension mechanism? This may potentially protect many new users from facing this kind of problems.\n\nAfter all, thanks to the team for developing this awesome toolbox!\nP.S. I apologize if you feel uncomfortable about the tab indent in the code snippet.", "body": "I tried writing a custom op and defining a module to wrap it. However, I came across a tedious problem which I feel useful to document here and provide some suggestions.\r\n\r\nI started with a custom op that does very simple reduction as follows\r\n```Python\r\nimport torch\r\n\r\nclass MyFunction(torch.nn.Function):\r\n\tdef forward(self, input):\r\n\t\tself.shape = input.size()\r\n\t\treturn input.mean(dim=1)\r\n\tdef backward(self, grad_out):\r\n\t\treturn grad_out.expand(self.shape) / float(self.shape[1])\r\n\r\nclass MyModule(torch.nn.Module):\r\n\tdef __init__(self):\r\n\t\tself.op = MyFunction()\r\n\r\n\tdef forward(self, input):\r\n\t\treturn self.op(input)         \r\n```\r\n\r\nThere you may have noticed that I have made a mistake by saving the instance in the module, provided you have read the documentation carefully and found this line\r\n\r\n> Each function is meant to be used only once (in the forward pass). \r\n\r\nat http://pytorch.org/docs/autograd.html#function\r\n\r\nHowever, the above snippet ran without any problem in my model under the single GPU mode, i.e. when no `torch.nn.DataParallel` is used.\r\n\r\nWhen I started adding `DataParallel` to my model, the nightmare began. Pytorch repeatedly threw error like this\r\n\r\n```\r\nRuntimeError: arguments are located on different GPUs at ...\r\n```\r\nOnce I remove the `DataParallel` module, the model can run smoothly.\r\n\r\nSince this error happens in the C++ side, there is no stack trace to help debug this problem. I got no clue which arguments lead to the error and had to doubt whether it is a bug in the `DataParallel` module.\r\n\r\nAfter spending a lot of time digging into the sources of `DataParallel` and even `THC` , I finally realized the mistake I made. So I changed the code to\r\n\r\n```Python\r\nimport torch\r\n\r\nclass MyFunction(torch.nn.Function):\r\n\tdef forward(self, input):\r\n\t\tself.shape = input.size()\r\n\t\treturn input.mean(dim=1)\r\n\tdef backward(self, grad_out):\r\n\t\treturn grad_out.expand(self.shape) / float(self.shape[1])\r\n\r\nclass MyModule(torch.nn.Module):\r\n\t\r\n\tdef forward(self, input):\r\n\t\treturn MyFunction()(input)\r\n```\r\nThen everything began to work as expected.\r\n\r\n\r\nI have a few comments afterward:\r\n1. If it is bound to fail in the multi-GPU situation, maybe we should also let it fail in the single GPU case.\r\n2. The mentioned line in the Doc should state this clearly that if we reuse a `Function` instance there might be some serious problem.\r\n3. One step further, if an instance of `Function` is meant to be used only once, how about integrating it into the extension mechanism? This may potentially protect many new users from facing this kind of problems.\r\n\r\nAfter all, thanks to the team for developing this awesome toolbox!\r\n\r\nP.S. I apologize if you feel uncomfortable about the tab indent in the code snippet."}