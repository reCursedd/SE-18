{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367373728", "html_url": "https://github.com/pytorch/pytorch/issues/821#issuecomment-367373728", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/821", "id": 367373728, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzM3MzcyOA==", "user": {"login": "liyangliu", "id": 5159489, "node_id": "MDQ6VXNlcjUxNTk0ODk=", "avatar_url": "https://avatars0.githubusercontent.com/u/5159489?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liyangliu", "html_url": "https://github.com/liyangliu", "followers_url": "https://api.github.com/users/liyangliu/followers", "following_url": "https://api.github.com/users/liyangliu/following{/other_user}", "gists_url": "https://api.github.com/users/liyangliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/liyangliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liyangliu/subscriptions", "organizations_url": "https://api.github.com/users/liyangliu/orgs", "repos_url": "https://api.github.com/users/liyangliu/repos", "events_url": "https://api.github.com/users/liyangliu/events{/privacy}", "received_events_url": "https://api.github.com/users/liyangliu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-21T15:58:05Z", "updated_at": "2018-02-21T16:14:58Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6830199\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yjxiong\">@yjxiong</a> Thanks for your post. I am facing a similar problem. I wonder whether it is OK to save a submodule instead of a function as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MySubModule</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n\t\t<span class=\"pl-c1\">self</span>.shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size()\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">input</span>.mean(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyModule</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\t\t<span class=\"pl-c1\">self</span>.op <span class=\"pl-k\">=</span> MySubModule()\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n\t\t<span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.op(<span class=\"pl-c1\">input</span>)</pre></div>\n<p>My code is something like above because I have some parameters in MySubModule so I have to save a instance of it and reuse the instance. The code works fine on single GPU but I also got the error when using data parallel (multiple GPUs):</p>\n<p>RuntimeError: arguments are located on different GPUs at THCTensorMathBlas.cu.</p>\n<p>Would you please give me some help? Thanks.</p>", "body_text": "@yjxiong Thanks for your post. I am facing a similar problem. I wonder whether it is OK to save a submodule instead of a function as follows:\nimport torch\n\nclass MySubModule(torch.nn.Module):\n\tdef forward(self, input):\n\t\tself.shape = input.size()\n\t\treturn input.mean(dim=1)\n\nclass MyModule(torch.nn.Module):\n\tdef __init__(self):\n\t\tself.op = MySubModule()\n\n\tdef forward(self, input):\n\t\treturn self.op(input)\nMy code is something like above because I have some parameters in MySubModule so I have to save a instance of it and reuse the instance. The code works fine on single GPU but I also got the error when using data parallel (multiple GPUs):\nRuntimeError: arguments are located on different GPUs at THCTensorMathBlas.cu.\nWould you please give me some help? Thanks.", "body": "@yjxiong Thanks for your post. I am facing a similar problem. I wonder whether it is OK to save a submodule instead of a function as follows:\r\n\r\n```python\r\nimport torch\r\n\r\nclass MySubModule(torch.nn.Module):\r\n\tdef forward(self, input):\r\n\t\tself.shape = input.size()\r\n\t\treturn input.mean(dim=1)\r\n\r\nclass MyModule(torch.nn.Module):\r\n\tdef __init__(self):\r\n\t\tself.op = MySubModule()\r\n\r\n\tdef forward(self, input):\r\n\t\treturn self.op(input)\r\n```\r\n\r\nMy code is something like above because I have some parameters in MySubModule so I have to save a instance of it and reuse the instance. The code works fine on single GPU but I also got the error when using data parallel (multiple GPUs):\r\n\r\nRuntimeError: arguments are located on different GPUs at THCTensorMathBlas.cu.\r\n\r\nWould you please give me some help? Thanks."}