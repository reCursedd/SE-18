{"url": "https://api.github.com/repos/pytorch/pytorch/issues/964", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/964/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/964/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/964/events", "html_url": "https://github.com/pytorch/pytorch/issues/964", "id": 213166497, "node_id": "MDU6SXNzdWUyMTMxNjY0OTc=", "number": 964, "title": "output_padding argument is ignored in deconvolution, leading to cudnn errors", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-03-09T21:11:56Z", "updated_at": "2017-03-22T23:42:12Z", "closed_at": "2017-03-22T23:42:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The following snippet produces an error in deconvolution:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nconv <span class=\"pl-k\">=</span> nn.Conv3d(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">8</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">stride</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>)\nconv <span class=\"pl-k\">=</span> conv.cuda()\ndeconv <span class=\"pl-k\">=</span> nn.ConvTranspose3d(<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">output_padding</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\ndeconv <span class=\"pl-k\">=</span> deconv.cuda()\n\nx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">64</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>ok</span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n    out <span class=\"pl-k\">=</span> conv(x)\n    err <span class=\"pl-k\">=</span> out.sum()\n    err.backward()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>deconv<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>error</span>\nx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n    out <span class=\"pl-k\">=</span> deconv(x)\n    err <span class=\"pl-k\">=</span> out.sum()\n    err.backward()</pre></div>\n<p>The issue is, in the deconvolution case padding is not passed to convolution descriptor, ltraces for the failing and passing cudnnGetConvolutionBackwardDataAlgorithm below, padding is 1 for passing and 0 for failing.</p>\n<pre><code>[pid 21910] _C.cpython-35m-x86_64-linux-gnu.so-&gt;cudnnGetConvolutionBackwardDataAlgorithm(0xffff7fffffff, { CUDNN_DATA_FLOAT, 0, [ 5, 8, 1, 3... ], CUDNN_TENSOR_NCHW }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 8... ], [ 0, 0, 262144, 32768... ] }, { CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT, 3, [ 0, 0, 0, 0... ], [ 2, 2, 2, 0... ], [ 1, 1, 1, 0... ] }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 1... ], [ 0, 0, 262144, 262144... ] }, 1, 0, 0 &lt;unfinished ...&gt;\n[pid 21917] _C.cpython-35m-x86_64-linux-gnu.so-&gt;cudnnGetConvolutionBackwardDataAlgorithm(0xffff7fffffff, { CUDNN_DATA_FLOAT, 0, [ 5, 8, 1, 3... ], CUDNN_TENSOR_NCHW }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 8... ], [ 0, 0, 262144, 32768... ] }, { CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT, 3, [ 1, 1, 1, 0... ], [ 2, 2, 2, 0... ], [ 1, 1, 1, 0... ] }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 1... ], [ 0, 0, 262144, 262144... ] }, 1, 0, 0 &lt;unfinished ...&gt;\n</code></pre>\n<p>Also, there seems to be no way of disabling cudnn at runtime, torch.backends.cudnn.enabled is no longer checked for convolutions: <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/convolution.cpp#L108-L110\">https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/convolution.cpp#L108-L110</a></p>", "body_text": "The following snippet produces an error in deconvolution:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nconv = nn.Conv3d(1,8, kernel_size=3, padding=1, stride = 2)\nconv = conv.cuda()\ndeconv = nn.ConvTranspose3d(8,1, kernel_size=3, stride=2, output_padding=-1)\ndeconv = deconv.cuda()\n\nx = Variable(torch.randn(8,1,64,64,64).cuda(), requires_grad=True)\n#ok\nfor i in range(10):\n    out = conv(x)\n    err = out.sum()\n    err.backward()\n\nprint(\"deconv\")\n\n#error\nx = Variable(torch.randn(8,8,32,32,32).cuda(), requires_grad=True)\n\nfor i in range(10):\n    out = deconv(x)\n    err = out.sum()\n    err.backward()\nThe issue is, in the deconvolution case padding is not passed to convolution descriptor, ltraces for the failing and passing cudnnGetConvolutionBackwardDataAlgorithm below, padding is 1 for passing and 0 for failing.\n[pid 21910] _C.cpython-35m-x86_64-linux-gnu.so->cudnnGetConvolutionBackwardDataAlgorithm(0xffff7fffffff, { CUDNN_DATA_FLOAT, 0, [ 5, 8, 1, 3... ], CUDNN_TENSOR_NCHW }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 8... ], [ 0, 0, 262144, 32768... ] }, { CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT, 3, [ 0, 0, 0, 0... ], [ 2, 2, 2, 0... ], [ 1, 1, 1, 0... ] }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 1... ], [ 0, 0, 262144, 262144... ] }, 1, 0, 0 <unfinished ...>\n[pid 21917] _C.cpython-35m-x86_64-linux-gnu.so->cudnnGetConvolutionBackwardDataAlgorithm(0xffff7fffffff, { CUDNN_DATA_FLOAT, 0, [ 5, 8, 1, 3... ], CUDNN_TENSOR_NCHW }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 8... ], [ 0, 0, 262144, 32768... ] }, { CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT, 3, [ 1, 1, 1, 0... ], [ 2, 2, 2, 0... ], [ 1, 1, 1, 0... ] }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 1... ], [ 0, 0, 262144, 262144... ] }, 1, 0, 0 <unfinished ...>\n\nAlso, there seems to be no way of disabling cudnn at runtime, torch.backends.cudnn.enabled is no longer checked for convolutions: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/convolution.cpp#L108-L110", "body": "The following snippet produces an error in deconvolution:\r\n```.py\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nconv = nn.Conv3d(1,8, kernel_size=3, padding=1, stride = 2)\r\nconv = conv.cuda()\r\ndeconv = nn.ConvTranspose3d(8,1, kernel_size=3, stride=2, output_padding=-1)\r\ndeconv = deconv.cuda()\r\n\r\nx = Variable(torch.randn(8,1,64,64,64).cuda(), requires_grad=True)\r\n#ok\r\nfor i in range(10):\r\n    out = conv(x)\r\n    err = out.sum()\r\n    err.backward()\r\n\r\nprint(\"deconv\")\r\n\r\n#error\r\nx = Variable(torch.randn(8,8,32,32,32).cuda(), requires_grad=True)\r\n\r\nfor i in range(10):\r\n    out = deconv(x)\r\n    err = out.sum()\r\n    err.backward()\r\n```\r\nThe issue is, in the deconvolution case padding is not passed to convolution descriptor, ltraces for the failing and passing cudnnGetConvolutionBackwardDataAlgorithm below, padding is 1 for passing and 0 for failing. \r\n```\r\n[pid 21910] _C.cpython-35m-x86_64-linux-gnu.so->cudnnGetConvolutionBackwardDataAlgorithm(0xffff7fffffff, { CUDNN_DATA_FLOAT, 0, [ 5, 8, 1, 3... ], CUDNN_TENSOR_NCHW }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 8... ], [ 0, 0, 262144, 32768... ] }, { CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT, 3, [ 0, 0, 0, 0... ], [ 2, 2, 2, 0... ], [ 1, 1, 1, 0... ] }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 1... ], [ 0, 0, 262144, 262144... ] }, 1, 0, 0 <unfinished ...>\r\n[pid 21917] _C.cpython-35m-x86_64-linux-gnu.so->cudnnGetConvolutionBackwardDataAlgorithm(0xffff7fffffff, { CUDNN_DATA_FLOAT, 0, [ 5, 8, 1, 3... ], CUDNN_TENSOR_NCHW }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 8... ], [ 0, 0, 262144, 32768... ] }, { CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT, 3, [ 1, 1, 1, 0... ], [ 2, 2, 2, 0... ], [ 1, 1, 1, 0... ] }, { CUDNN_DATA_FLOAT, 0, 5, [ 2097152, 0, 8, 1... ], [ 0, 0, 262144, 262144... ] }, 1, 0, 0 <unfinished ...>\r\n```\r\nAlso, there seems to be no way of disabling cudnn at runtime, torch.backends.cudnn.enabled is no longer checked for convolutions: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/convolution.cpp#L108-L110"}