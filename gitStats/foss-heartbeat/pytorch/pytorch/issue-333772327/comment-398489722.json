{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398489722", "html_url": "https://github.com/pytorch/pytorch/issues/8656#issuecomment-398489722", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8656", "id": 398489722, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODQ4OTcyMg==", "user": {"login": "krylea", "id": 30674826, "node_id": "MDQ6VXNlcjMwNjc0ODI2", "avatar_url": "https://avatars0.githubusercontent.com/u/30674826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krylea", "html_url": "https://github.com/krylea", "followers_url": "https://api.github.com/users/krylea/followers", "following_url": "https://api.github.com/users/krylea/following{/other_user}", "gists_url": "https://api.github.com/users/krylea/gists{/gist_id}", "starred_url": "https://api.github.com/users/krylea/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krylea/subscriptions", "organizations_url": "https://api.github.com/users/krylea/orgs", "repos_url": "https://api.github.com/users/krylea/repos", "events_url": "https://api.github.com/users/krylea/events{/privacy}", "received_events_url": "https://api.github.com/users/krylea/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-19T17:57:55Z", "updated_at": "2018-06-19T17:57:55Z", "author_association": "NONE", "body_html": "<p>The model is a standard naive sequence to sequence language model with Luong attention, using 1-layer GRUs as the base RNNs. Inputs are passed into an embedding layer, then an \"encoder\" RNN (1-layer bidirectional GRU). The output hidden state from this is then used as the initial hidden state for a \"decoder\" RNN (1-layer unidirectional GRU). The decoder is given a predefined token at each decoding step, and at each step the output of the decoder is concatenated with a context vector computed by Luong attention over the encoder outputs (weighted sum of bidirectional outputs at each encoding step, with weights determined by the dot product between the encoder's output at that step and the output of the decoder RNN), then passed through a linear layer with tanh activation, and finally passed into a last linear layer and softmaxed to produce a probability distribution over the vocabulary for selecting the next token.<br>\nTraining is done in a batched fashion, with masking, using a cross-entropy loss.</p>", "body_text": "The model is a standard naive sequence to sequence language model with Luong attention, using 1-layer GRUs as the base RNNs. Inputs are passed into an embedding layer, then an \"encoder\" RNN (1-layer bidirectional GRU). The output hidden state from this is then used as the initial hidden state for a \"decoder\" RNN (1-layer unidirectional GRU). The decoder is given a predefined token at each decoding step, and at each step the output of the decoder is concatenated with a context vector computed by Luong attention over the encoder outputs (weighted sum of bidirectional outputs at each encoding step, with weights determined by the dot product between the encoder's output at that step and the output of the decoder RNN), then passed through a linear layer with tanh activation, and finally passed into a last linear layer and softmaxed to produce a probability distribution over the vocabulary for selecting the next token.\nTraining is done in a batched fashion, with masking, using a cross-entropy loss.", "body": "The model is a standard naive sequence to sequence language model with Luong attention, using 1-layer GRUs as the base RNNs. Inputs are passed into an embedding layer, then an \"encoder\" RNN (1-layer bidirectional GRU). The output hidden state from this is then used as the initial hidden state for a \"decoder\" RNN (1-layer unidirectional GRU). The decoder is given a predefined token at each decoding step, and at each step the output of the decoder is concatenated with a context vector computed by Luong attention over the encoder outputs (weighted sum of bidirectional outputs at each encoding step, with weights determined by the dot product between the encoder's output at that step and the output of the decoder RNN), then passed through a linear layer with tanh activation, and finally passed into a last linear layer and softmaxed to produce a probability distribution over the vocabulary for selecting the next token.\r\nTraining is done in a batched fashion, with masking, using a cross-entropy loss."}