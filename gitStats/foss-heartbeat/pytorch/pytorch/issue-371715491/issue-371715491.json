{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12837", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12837/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12837/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12837/events", "html_url": "https://github.com/pytorch/pytorch/issues/12837", "id": 371715491, "node_id": "MDU6SXNzdWUzNzE3MTU0OTE=", "number": 12837, "title": "Planned Work: Unify StreamGuard/CUDAGuard/DeviceGuard", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-18T21:11:45Z", "updated_at": "2018-10-18T21:11:45Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><strong>Background.</strong> Here is the current state of RAII stream/device handling in PyTorch and Caffe2.</p>\n<p>PyTorch has the following implementations:</p>\n<ul>\n<li>DeviceGuard, is a class for managing cudaSetDevice/cudaGetDevice calls. It is accessible from the CPU library, and handles cross CPU-GPU calls using a table of function pointers in CUDAHooksInterface.</li>\n<li>CUDAGuard is a CUDA library only class that manages both device and stream. Desired stream is identified via the CUDAStream object.</li>\n<li>StreamGuard is a CUDA library only class in c10d that is basically an exact copy of CUDAGuard, but worse (it calls legacy THC APIs)</li>\n</ul>\n<p>Caffe2 has the following implementations:</p>\n<ul>\n<li>DeviceGuard, is a CUDA library only class that wraps calls to CaffeCudaSetDevice/CaffeCudaGetDevice. The primary difference here is that these functions are controlled by an option FLAGS_caffe2_cuda_full_device_control which, when true, assumes that we can maintain a shadow thread_local variable that accurately reflects what the current CUDA device is.</li>\n</ul>\n<p><strong>Requirements.</strong></p>\n<ul>\n<li>The guard must be device agnostic in some way\n<ul>\n<li>Caffe2 supports more backends than just CPU and CUDA. DeviceGuard must be able to handle these cases.</li>\n<li>Stream identification should be done as an int (or a small wrapper struct around the int), and not as a class, so that we can represent streams of different device types uniformly.</li>\n</ul>\n</li>\n<li>The guard must be dispatchable at both compile time and runtime:\n<ul>\n<li>The most common call sites for device switching (SwitchToDevice) in Caffe2 are in places templatized by a Context parameter; thus, Caffe2 knows at compile-time which device guard it wants; the only requirement is that the device guards for different devices have a uniform interface (which can be accessed from Context). Thus, it is not necessary to be run time device agnostic.</li>\n<li>Due to architectural decisions in PyTorch, PyTorch requires the ability to call a DeviceGuard from CPU code. This is because at the time we set device correctly, we have not performed dispatch yet, and are in device agnostic code. This is particularly appropriate for composite ops, where we want to change the device once for an entire set of operations, but the top-level of the composite op is backend independent and would work for CPU or CUDA.\n<ul>\n<li>It's possible that we could rework how this works, but it would be substantial work.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Performance is important, because DeviceGuard is called on entry to every operation.\n<ul>\n<li>We don't want a virtual dispatch to a function which itself performs a non-inlineable function call; this is why DeviceGuard hard codes function pointers to get_device/set_device, etc. (c.f. design discussion between Sam Gross and Peter Goldsborough.) Maintaining this design while being device agnostic may be a bit more difficult.</li>\n</ul>\n</li>\n<li>The device agnostic interface should live in c10.</li>\n<li>It must be possible to set the stream/device without relying on RAII. This keeps API parity with Caffe2's current SwitchToDevice()</li>\n</ul>", "body_text": "Background. Here is the current state of RAII stream/device handling in PyTorch and Caffe2.\nPyTorch has the following implementations:\n\nDeviceGuard, is a class for managing cudaSetDevice/cudaGetDevice calls. It is accessible from the CPU library, and handles cross CPU-GPU calls using a table of function pointers in CUDAHooksInterface.\nCUDAGuard is a CUDA library only class that manages both device and stream. Desired stream is identified via the CUDAStream object.\nStreamGuard is a CUDA library only class in c10d that is basically an exact copy of CUDAGuard, but worse (it calls legacy THC APIs)\n\nCaffe2 has the following implementations:\n\nDeviceGuard, is a CUDA library only class that wraps calls to CaffeCudaSetDevice/CaffeCudaGetDevice. The primary difference here is that these functions are controlled by an option FLAGS_caffe2_cuda_full_device_control which, when true, assumes that we can maintain a shadow thread_local variable that accurately reflects what the current CUDA device is.\n\nRequirements.\n\nThe guard must be device agnostic in some way\n\nCaffe2 supports more backends than just CPU and CUDA. DeviceGuard must be able to handle these cases.\nStream identification should be done as an int (or a small wrapper struct around the int), and not as a class, so that we can represent streams of different device types uniformly.\n\n\nThe guard must be dispatchable at both compile time and runtime:\n\nThe most common call sites for device switching (SwitchToDevice) in Caffe2 are in places templatized by a Context parameter; thus, Caffe2 knows at compile-time which device guard it wants; the only requirement is that the device guards for different devices have a uniform interface (which can be accessed from Context). Thus, it is not necessary to be run time device agnostic.\nDue to architectural decisions in PyTorch, PyTorch requires the ability to call a DeviceGuard from CPU code. This is because at the time we set device correctly, we have not performed dispatch yet, and are in device agnostic code. This is particularly appropriate for composite ops, where we want to change the device once for an entire set of operations, but the top-level of the composite op is backend independent and would work for CPU or CUDA.\n\nIt's possible that we could rework how this works, but it would be substantial work.\n\n\n\n\nPerformance is important, because DeviceGuard is called on entry to every operation.\n\nWe don't want a virtual dispatch to a function which itself performs a non-inlineable function call; this is why DeviceGuard hard codes function pointers to get_device/set_device, etc. (c.f. design discussion between Sam Gross and Peter Goldsborough.) Maintaining this design while being device agnostic may be a bit more difficult.\n\n\nThe device agnostic interface should live in c10.\nIt must be possible to set the stream/device without relying on RAII. This keeps API parity with Caffe2's current SwitchToDevice()", "body": "**Background.** Here is the current state of RAII stream/device handling in PyTorch and Caffe2.\r\n\r\nPyTorch has the following implementations:\r\n\r\n* DeviceGuard, is a class for managing cudaSetDevice/cudaGetDevice calls. It is accessible from the CPU library, and handles cross CPU-GPU calls using a table of function pointers in CUDAHooksInterface.\r\n* CUDAGuard is a CUDA library only class that manages both device and stream. Desired stream is identified via the CUDAStream object.\r\n* StreamGuard is a CUDA library only class in c10d that is basically an exact copy of CUDAGuard, but worse (it calls legacy THC APIs)\r\n\r\nCaffe2 has the following implementations:\r\n* DeviceGuard, is a CUDA library only class that wraps calls to CaffeCudaSetDevice/CaffeCudaGetDevice. The primary difference here is that these functions are controlled by an option FLAGS_caffe2_cuda_full_device_control which, when true, assumes that we can maintain a shadow thread_local variable that accurately reflects what the current CUDA device is.\r\n\r\n**Requirements.**\r\n\r\n* The guard must be device agnostic in some way\r\n  * Caffe2 supports more backends than just CPU and CUDA. DeviceGuard must be able to handle these cases.\r\n  * Stream identification should be done as an int (or a small wrapper struct around the int), and not as a class, so that we can represent streams of different device types uniformly.\r\n* The guard must be dispatchable at both compile time and runtime:\r\n  * The most common call sites for device switching (SwitchToDevice) in Caffe2 are in places templatized by a Context parameter; thus, Caffe2 knows at compile-time which device guard it wants; the only requirement is that the device guards for different devices have a uniform interface (which can be accessed from Context). Thus, it is not necessary to be run time device agnostic.\r\n  * Due to architectural decisions in PyTorch, PyTorch requires the ability to call a DeviceGuard from CPU code. This is because at the time we set device correctly, we have not performed dispatch yet, and are in device agnostic code. This is particularly appropriate for composite ops, where we want to change the device once for an entire set of operations, but the top-level of the composite op is backend independent and would work for CPU or CUDA.\r\n    * It's possible that we could rework how this works, but it would be substantial work.\r\n* Performance is important, because DeviceGuard is called on entry to every operation.\r\n  * We don't want a virtual dispatch to a function which itself performs a non-inlineable function call; this is why DeviceGuard hard codes function pointers to get_device/set_device, etc. (c.f. design discussion between Sam Gross and Peter Goldsborough.) Maintaining this design while being device agnostic may be a bit more difficult.\r\n* The device agnostic interface should live in c10.\r\n* It must be possible to set the stream/device without relying on RAII. This keeps API parity with Caffe2's current SwitchToDevice()"}