{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3464", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3464/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3464/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3464/events", "html_url": "https://github.com/pytorch/pytorch/issues/3464", "id": 270991907, "node_id": "MDU6SXNzdWUyNzA5OTE5MDc=", "number": 3464, "title": "Usage block in `__mul__` for Variables", "user": {"login": "okcd00", "id": 9395067, "node_id": "MDQ6VXNlcjkzOTUwNjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9395067?v=4", "gravatar_id": "", "url": "https://api.github.com/users/okcd00", "html_url": "https://github.com/okcd00", "followers_url": "https://api.github.com/users/okcd00/followers", "following_url": "https://api.github.com/users/okcd00/following{/other_user}", "gists_url": "https://api.github.com/users/okcd00/gists{/gist_id}", "starred_url": "https://api.github.com/users/okcd00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/okcd00/subscriptions", "organizations_url": "https://api.github.com/users/okcd00/orgs", "repos_url": "https://api.github.com/users/okcd00/repos", "events_url": "https://api.github.com/users/okcd00/events{/privacy}", "received_events_url": "https://api.github.com/users/okcd00/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-03T14:13:24Z", "updated_at": "2017-11-06T06:49:34Z", "closed_at": "2017-11-06T06:49:34Z", "author_association": "NONE", "body_html": "<p>I want to make an element-wise <code>mul</code> operation for <code>Variable</code> and a mask matrix,<br>\nbut the usage for this might be blocked, I guess.<br>\nHere's an example for detail:</p>\n<pre><code>import torch\nimport numpy as np\nfrom torch.autograd import Variable\n\nx = np.ones((2,3))\nf = torch.FloatTensor(x)\nt = Variable(torch.ones((2,3)))\nt2 = Variable(f) \n</code></pre>\n<p>when I try <code>t.mul(x)</code> , it shows types I can select from is <code>float</code> and <code>torch.FloatTensor</code>:</p>\n<pre><code>TypeError: mul received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (numpy.ndarray)\n * (torch.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (numpy.ndarray)\n</code></pre>\n<p>So, I tried <code>t.mul(f)</code>:</p>\n<pre><code>/usr/local/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc in mul(self, other)\n    339             return Mul.apply(self, other)\n    340         else:\n--&gt; 341             assert not torch.is_tensor(other)\n    342             return MulConstant.apply(self, other)\n    343 \n\nAssertionError: \n</code></pre>\n<p>Only when we use <code>Variable</code> as <code>other</code> (<code>t.mul(t2)</code>) it works,<br>\n(so does the function <code>masked_scatter_</code>, maybe.)</p>\n<p>Does it mean 'if <code>other</code> is a tensor, raise AssertError' ?<br>\nIf so, how can I use <code>mul</code> operation for the <code>Variable</code>?</p>", "body_text": "I want to make an element-wise mul operation for Variable and a mask matrix,\nbut the usage for this might be blocked, I guess.\nHere's an example for detail:\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\n\nx = np.ones((2,3))\nf = torch.FloatTensor(x)\nt = Variable(torch.ones((2,3)))\nt2 = Variable(f) \n\nwhen I try t.mul(x) , it shows types I can select from is float and torch.FloatTensor:\nTypeError: mul received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (numpy.ndarray)\n * (torch.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (numpy.ndarray)\n\nSo, I tried t.mul(f):\n/usr/local/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc in mul(self, other)\n    339             return Mul.apply(self, other)\n    340         else:\n--> 341             assert not torch.is_tensor(other)\n    342             return MulConstant.apply(self, other)\n    343 \n\nAssertionError: \n\nOnly when we use Variable as other (t.mul(t2)) it works,\n(so does the function masked_scatter_, maybe.)\nDoes it mean 'if other is a tensor, raise AssertError' ?\nIf so, how can I use mul operation for the Variable?", "body": "I want to make an element-wise `mul` operation for `Variable` and a mask matrix,\r\nbut the usage for this might be blocked, I guess.\r\nHere's an example for detail:\r\n\r\n```\r\nimport torch\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\n\r\nx = np.ones((2,3))\r\nf = torch.FloatTensor(x)\r\nt = Variable(torch.ones((2,3)))\r\nt2 = Variable(f) \r\n```\r\n\r\nwhen I try `t.mul(x)` , it shows types I can select from is `float` and `torch.FloatTensor`:\r\n```\r\nTypeError: mul received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\r\n * (float value)\r\n      didn't match because some of the arguments have invalid types: (numpy.ndarray)\r\n * (torch.FloatTensor other)\r\n      didn't match because some of the arguments have invalid types: (numpy.ndarray)\r\n```\r\n\r\nSo, I tried `t.mul(f)`:\r\n```\r\n/usr/local/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc in mul(self, other)\r\n    339             return Mul.apply(self, other)\r\n    340         else:\r\n--> 341             assert not torch.is_tensor(other)\r\n    342             return MulConstant.apply(self, other)\r\n    343 \r\n\r\nAssertionError: \r\n```\r\n\r\nOnly when we use `Variable` as `other` (`t.mul(t2)`) it works,\r\n(so does the function `masked_scatter_`, maybe.)\r\n\r\nDoes it mean 'if `other` is a tensor, raise AssertError' ?\r\nIf so, how can I use `mul` operation for the `Variable`?"}