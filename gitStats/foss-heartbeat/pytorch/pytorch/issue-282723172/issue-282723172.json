{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4216", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4216/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4216/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4216/events", "html_url": "https://github.com/pytorch/pytorch/pull/4216", "id": 282723172, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU4ODAzMjQy", "number": 4216, "title": "Cache DataParallel replicas", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-12-17T20:57:11Z", "updated_at": "2018-11-23T15:37:30Z", "closed_at": null, "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4216", "html_url": "https://github.com/pytorch/pytorch/pull/4216", "diff_url": "https://github.com/pytorch/pytorch/pull/4216.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4216.patch"}, "body_html": "<p>This commits makes DataParallel cache the wrapped module replicas, and helps bring down overhead of <code>replicate</code> on ResNet1001 from ~140ms to 42ms. Remaining time is spent mostly in <code>broadcast_coalesced</code>, so moving it to C++ is the next step.</p>\n<p>This commit really starts to push Python to the limit, which can be seen in two places:</p>\n<ul>\n<li>There's this ugly odict hack, because Python inheritance does weird things, and selects a much slower implementation of <code>__getitem__</code> for OrderedDict subclasses than it could. I'm going to post to Python's mailing lists and clarify why is this happening, but I'm not aware of any reason why this hack would not work. This might seem like a silly thing, but removing this hack costs us 30ms at each forward.</li>\n<li>I was forced to change the implementation of <code>torch.jit.compile</code> for modules, and implement this poor man's inheritance-like thing (including the <code>__instancecheck__</code> hack so these object still appear to belong to subclasses)... I've tried a few other things, but I can't come up with anything else that wouldn't break. The problem is that having one superclass with <code>__slots__</code>, and another one in C++ confuses Python, and it complains that it can't figure out how to lay them out in memory. I'm happy to discuss alternative solutions.</li>\n</ul>", "body_text": "This commits makes DataParallel cache the wrapped module replicas, and helps bring down overhead of replicate on ResNet1001 from ~140ms to 42ms. Remaining time is spent mostly in broadcast_coalesced, so moving it to C++ is the next step.\nThis commit really starts to push Python to the limit, which can be seen in two places:\n\nThere's this ugly odict hack, because Python inheritance does weird things, and selects a much slower implementation of __getitem__ for OrderedDict subclasses than it could. I'm going to post to Python's mailing lists and clarify why is this happening, but I'm not aware of any reason why this hack would not work. This might seem like a silly thing, but removing this hack costs us 30ms at each forward.\nI was forced to change the implementation of torch.jit.compile for modules, and implement this poor man's inheritance-like thing (including the __instancecheck__ hack so these object still appear to belong to subclasses)... I've tried a few other things, but I can't come up with anything else that wouldn't break. The problem is that having one superclass with __slots__, and another one in C++ confuses Python, and it complains that it can't figure out how to lay them out in memory. I'm happy to discuss alternative solutions.", "body": "This commits makes DataParallel cache the wrapped module replicas, and helps bring down overhead of `replicate` on ResNet1001 from ~140ms to 42ms. Remaining time is spent mostly in `broadcast_coalesced`, so moving it to C++ is the next step.\r\n\r\nThis commit really starts to push Python to the limit, which can be seen in two places:\r\n* There's this ugly odict hack, because Python inheritance does weird things, and selects a much slower implementation of `__getitem__` for OrderedDict subclasses than it could. I'm going to post to Python's mailing lists and clarify why is this happening, but I'm not aware of any reason why this hack would not work. This might seem like a silly thing, but removing this hack costs us 30ms at each forward.\r\n* I was forced to change the implementation of `torch.jit.compile` for modules, and implement this poor man's inheritance-like thing (including the `__instancecheck__` hack so these object still appear to belong to subclasses)... I've tried a few other things, but I can't come up with anything else that wouldn't break. The problem is that having one superclass with `__slots__`, and another one in C++ confuses Python, and it complains that it can't figure out how to lay them out in memory. I'm happy to discuss alternative solutions."}