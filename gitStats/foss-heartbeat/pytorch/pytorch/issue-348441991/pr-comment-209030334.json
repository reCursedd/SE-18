{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209030334", "pull_request_review_id": 144965024, "id": 209030334, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTAzMDMzNA==", "diff_hunk": "@@ -93,11 +93,10 @@ def sample(self, sample_shape=torch.Size()):\n     def log_prob(self, value):\n         if self._validate_args:\n             self._validate_sample(value)\n-        value_shape = torch._C._infer_size(value.size(), self.batch_shape) if self.batch_shape else value.size()\n-        param_shape = value_shape + (self._num_events,)\n-        value = value.expand(value_shape)\n-        log_pmf = self.logits.expand(param_shape)\n-        return log_pmf.gather(-1, value.unsqueeze(-1).long()).squeeze(-1)\n+        value = value.long().unsqueeze(-1)\n+        value, log_pmf = torch.broadcast_tensors(value, self.logits)\n+        value = value[..., :1]", "path": "torch/distributions/categorical.py", "position": null, "original_position": 11, "commit_id": "b6a8be63aaaf8be5470b6d195b05d07b7b4886d3", "original_commit_id": "a463dc9122df473f1f981fec028d9b18f50dfc56", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "The value has been expanded to shape `batch_shape + (n,)` but we want it to have shape `batch_shape + (1,)`. This line slices it down to the desired shape. Note that this is basically free because the stride is zero on the rightmost dimension.\r\n\r\nThe reason I'm using this trick is to be able to use `broadcast_tensors()` on two tensors with different target shape: `value` with shape `batch_shape + (1,)` and `logits` with target shape `batch_shape + (n,)`. It happens to work and it's cheap.", "created_at": "2018-08-09T18:16:59Z", "updated_at": "2018-11-23T15:49:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/10321#discussion_r209030334", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10321", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209030334"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10321#discussion_r209030334"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10321"}}, "body_html": "<p>The value has been expanded to shape <code>batch_shape + (n,)</code> but we want it to have shape <code>batch_shape + (1,)</code>. This line slices it down to the desired shape. Note that this is basically free because the stride is zero on the rightmost dimension.</p>\n<p>The reason I'm using this trick is to be able to use <code>broadcast_tensors()</code> on two tensors with different target shape: <code>value</code> with shape <code>batch_shape + (1,)</code> and <code>logits</code> with target shape <code>batch_shape + (n,)</code>. It happens to work and it's cheap.</p>", "body_text": "The value has been expanded to shape batch_shape + (n,) but we want it to have shape batch_shape + (1,). This line slices it down to the desired shape. Note that this is basically free because the stride is zero on the rightmost dimension.\nThe reason I'm using this trick is to be able to use broadcast_tensors() on two tensors with different target shape: value with shape batch_shape + (1,) and logits with target shape batch_shape + (n,). It happens to work and it's cheap.", "in_reply_to_id": 209028993}