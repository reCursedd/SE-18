{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10321", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10321/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10321/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10321/events", "html_url": "https://github.com/pytorch/pytorch/pull/10321", "id": 348441991, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA2NzkxNjI1", "number": 10321, "title": "[distributions] Make more distributions jittable", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 559719279, "node_id": "MDU6TGFiZWw1NTk3MTkyNzk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ready%20for%20review", "name": "ready for review", "color": "b60205", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-08-07T18:44:40Z", "updated_at": "2018-11-23T15:49:38Z", "closed_at": "2018-09-25T16:59:12Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10321", "html_url": "https://github.com/pytorch/pytorch/pull/10321", "diff_url": "https://github.com/pytorch/pytorch/pull/10321.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10321.patch"}, "body_html": "<p>This uses <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>'s new <code>torch.broadcast_tensors()</code> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"346243597\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10075\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10075/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/10075\">#10075</a> to make <code>Categorical.log_prob()</code> and the <code>*Normal.__init__()</code> methods jittable. Previously <code>.log_prob()</code> was failing due to calls to <code>torch._C.infer_size()</code> with errors like</p>\n<pre><code>    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n&gt;       value_shape = torch._C._infer_size(value.size(), self.batch_shape) if self.batch_shape else value.size()\nE       RuntimeError: expected int at position 0, but got: Tensor\n</code></pre>\n<p>After this change I'm able to jit many more of Pyro's tests.</p>\n<h2>Questions for reviewers</h2>\n<p>This assumes that <code>broadcast_tensors</code> will create a <code>value</code> with stride 0 in the rightmost dimension; that way we never create a huge tensor. Is this assumption valid?</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> Yes, broadcast_tensors will create a value with stride 0 in the rightmost dimension because broadcasting expands tensors (as opposed to using repeat, which creates new tensors).</p>\n</blockquote>", "body_text": "This uses @zou3519's new torch.broadcast_tensors() #10075 to make Categorical.log_prob() and the *Normal.__init__() methods jittable. Previously .log_prob() was failing due to calls to torch._C.infer_size() with errors like\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n>       value_shape = torch._C._infer_size(value.size(), self.batch_shape) if self.batch_shape else value.size()\nE       RuntimeError: expected int at position 0, but got: Tensor\n\nAfter this change I'm able to jit many more of Pyro's tests.\nQuestions for reviewers\nThis assumes that broadcast_tensors will create a value with stride 0 in the rightmost dimension; that way we never create a huge tensor. Is this assumption valid?\n\n@zou3519 Yes, broadcast_tensors will create a value with stride 0 in the rightmost dimension because broadcasting expands tensors (as opposed to using repeat, which creates new tensors).", "body": "This uses @zou3519's new `torch.broadcast_tensors()` #10075 to make `Categorical.log_prob()` and the `*Normal.__init__()` methods jittable. Previously `.log_prob()` was failing due to calls to `torch._C.infer_size()` with errors like\r\n```\r\n    def log_prob(self, value):\r\n        if self._validate_args:\r\n            self._validate_sample(value)\r\n>       value_shape = torch._C._infer_size(value.size(), self.batch_shape) if self.batch_shape else value.size()\r\nE       RuntimeError: expected int at position 0, but got: Tensor\r\n```\r\nAfter this change I'm able to jit many more of Pyro's tests.\r\n\r\n## Questions for reviewers\r\n\r\nThis assumes that `broadcast_tensors` will create a `value` with stride 0 in the rightmost dimension; that way we never create a huge tensor. Is this assumption valid?\r\n\r\n> @zou3519 Yes, broadcast_tensors will create a value with stride 0 in the rightmost dimension because broadcasting expands tensors (as opposed to using repeat, which creates new tensors)."}