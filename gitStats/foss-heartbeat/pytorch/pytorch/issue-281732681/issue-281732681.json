{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4147", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4147/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4147/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4147/events", "html_url": "https://github.com/pytorch/pytorch/issues/4147", "id": 281732681, "node_id": "MDU6SXNzdWUyODE3MzI2ODE=", "number": 4147, "title": "Parameter turns into Variable with multiple GPUs", "user": {"login": "WellYoungIOE", "id": 19660081, "node_id": "MDQ6VXNlcjE5NjYwMDgx", "avatar_url": "https://avatars2.githubusercontent.com/u/19660081?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WellYoungIOE", "html_url": "https://github.com/WellYoungIOE", "followers_url": "https://api.github.com/users/WellYoungIOE/followers", "following_url": "https://api.github.com/users/WellYoungIOE/following{/other_user}", "gists_url": "https://api.github.com/users/WellYoungIOE/gists{/gist_id}", "starred_url": "https://api.github.com/users/WellYoungIOE/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WellYoungIOE/subscriptions", "organizations_url": "https://api.github.com/users/WellYoungIOE/orgs", "repos_url": "https://api.github.com/users/WellYoungIOE/repos", "events_url": "https://api.github.com/users/WellYoungIOE/events{/privacy}", "received_events_url": "https://api.github.com/users/WellYoungIOE/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-13T12:30:35Z", "updated_at": "2017-12-13T13:57:57Z", "closed_at": "2017-12-13T13:57:57Z", "author_association": "NONE", "body_html": "<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nfrom torch.nn.modules import conv\nfrom torch.nn.modules.utils import _pair\nfrom torch.autograd import Variable\n\n\ndef _l2normalize(v, eps=1e-12):\n    return v / (((v**2).sum())**0.5 + eps)\n\n\ndef sn_norm(weights, u):\n\n    weights = weights.view(weights.shape[0], -1)\n    v_final = _l2normalize(torch.matmul(u, weights))\n    u_final = _l2normalize(torch.matmul(v_final, torch.transpose(weights, 0, 1)))\n\n    return u_final, v_final\n\n\nclass SNConv2D(conv._ConvNd):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(SNConv2D, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n        self.mu = nn.Parameter(torch.cuda.FloatTensor(np.random.normal(0, 1, (1, out_channels))))\n        self.mu_new = nn.Parameter(torch.FloatTensor(np.random.normal(0, 1, (1, out_channels))))\n\n    def forward(self, input):\n        u_, v_ = sn_norm(self.weight.data, self.mu.data)\n        self.mu_new.data = u_\n        u_ = Variable(u_)\n        v_ = Variable(v_)\n        sigma = torch.matmul(torch.matmul(u_, self.weight.view(self.weight.shape[0], -1)), torch.transpose(v_, 0, 1))\n        weight_norm = self.weight / sigma\n        return func.conv2d(input, weight_norm, self.bias, self.stride,\n                           self.padding, self.dilation, self.groups)\n</code></pre>\n<p>I found <code>self.mu_new</code> turns into <code>Variable</code> in <code>forward()</code>(Maybe a copy of <code>self.mu_new</code>?) with multiple GPUs. And the code  <code>self.mu_new.data = u_</code> won't really change the data in <code>self.mu_new</code>. But it's OK with only one GPU, because <code>self.mu_new</code> is still <code>Parameter</code>(The original one?) in <code>forward()</code> with one GPU.<br>\nAny way to figure it out?<br>\nThanks!</p>", "body_text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nfrom torch.nn.modules import conv\nfrom torch.nn.modules.utils import _pair\nfrom torch.autograd import Variable\n\n\ndef _l2normalize(v, eps=1e-12):\n    return v / (((v**2).sum())**0.5 + eps)\n\n\ndef sn_norm(weights, u):\n\n    weights = weights.view(weights.shape[0], -1)\n    v_final = _l2normalize(torch.matmul(u, weights))\n    u_final = _l2normalize(torch.matmul(v_final, torch.transpose(weights, 0, 1)))\n\n    return u_final, v_final\n\n\nclass SNConv2D(conv._ConvNd):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(SNConv2D, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n        self.mu = nn.Parameter(torch.cuda.FloatTensor(np.random.normal(0, 1, (1, out_channels))))\n        self.mu_new = nn.Parameter(torch.FloatTensor(np.random.normal(0, 1, (1, out_channels))))\n\n    def forward(self, input):\n        u_, v_ = sn_norm(self.weight.data, self.mu.data)\n        self.mu_new.data = u_\n        u_ = Variable(u_)\n        v_ = Variable(v_)\n        sigma = torch.matmul(torch.matmul(u_, self.weight.view(self.weight.shape[0], -1)), torch.transpose(v_, 0, 1))\n        weight_norm = self.weight / sigma\n        return func.conv2d(input, weight_norm, self.bias, self.stride,\n                           self.padding, self.dilation, self.groups)\n\nI found self.mu_new turns into Variable in forward()(Maybe a copy of self.mu_new?) with multiple GPUs. And the code  self.mu_new.data = u_ won't really change the data in self.mu_new. But it's OK with only one GPU, because self.mu_new is still Parameter(The original one?) in forward() with one GPU.\nAny way to figure it out?\nThanks!", "body": "```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as func\r\nfrom torch.nn.modules import conv\r\nfrom torch.nn.modules.utils import _pair\r\nfrom torch.autograd import Variable\r\n\r\n\r\ndef _l2normalize(v, eps=1e-12):\r\n    return v / (((v**2).sum())**0.5 + eps)\r\n\r\n\r\ndef sn_norm(weights, u):\r\n\r\n    weights = weights.view(weights.shape[0], -1)\r\n    v_final = _l2normalize(torch.matmul(u, weights))\r\n    u_final = _l2normalize(torch.matmul(v_final, torch.transpose(weights, 0, 1)))\r\n\r\n    return u_final, v_final\r\n\r\n\r\nclass SNConv2D(conv._ConvNd):\r\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\r\n                 padding=0, dilation=1, groups=1, bias=True):\r\n        kernel_size = _pair(kernel_size)\r\n        stride = _pair(stride)\r\n        padding = _pair(padding)\r\n        dilation = _pair(dilation)\r\n        super(SNConv2D, self).__init__(\r\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\r\n            False, _pair(0), groups, bias)\r\n        self.mu = nn.Parameter(torch.cuda.FloatTensor(np.random.normal(0, 1, (1, out_channels))))\r\n        self.mu_new = nn.Parameter(torch.FloatTensor(np.random.normal(0, 1, (1, out_channels))))\r\n\r\n    def forward(self, input):\r\n        u_, v_ = sn_norm(self.weight.data, self.mu.data)\r\n        self.mu_new.data = u_\r\n        u_ = Variable(u_)\r\n        v_ = Variable(v_)\r\n        sigma = torch.matmul(torch.matmul(u_, self.weight.view(self.weight.shape[0], -1)), torch.transpose(v_, 0, 1))\r\n        weight_norm = self.weight / sigma\r\n        return func.conv2d(input, weight_norm, self.bias, self.stride,\r\n                           self.padding, self.dilation, self.groups)\r\n```\r\nI found `self.mu_new` turns into `Variable` in `forward()`(Maybe a copy of `self.mu_new`?) with multiple GPUs. And the code  `self.mu_new.data = u_` won't really change the data in `self.mu_new`. But it's OK with only one GPU, because `self.mu_new` is still `Parameter`(The original one?) in `forward()` with one GPU.\r\nAny way to figure it out?\r\nThanks!"}