{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12796", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12796/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12796/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12796/events", "html_url": "https://github.com/pytorch/pytorch/issues/12796", "id": 371305459, "node_id": "MDU6SXNzdWUzNzEzMDU0NTk=", "number": 12796, "title": "[Feature Request] torch.nn.Flatten(start_dim=1, end_dim=-1)", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-17T23:30:44Z", "updated_at": "2018-10-22T17:53:05Z", "closed_at": "2018-10-22T17:53:05Z", "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p><code>torch.nn.Flatten(start_dim=1, end_dim=-1)</code> basically just convert the input as <code>input.flatten(start_dim, end_dim)</code></p>\n<h2>Motivation</h2>\n<p>Let's think that we have code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">100</span>)\n\nmodel <span class=\"pl-k\">=</span> torch.nn.Sequential(\n    torch.nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">50</span>),\n    torch.nn.ReLU(),\n    torch.nn.Linear(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">1</span>),\n)\n\n<span class=\"pl-c1\">print</span>(model(a).shape)</pre></div>\n<p>The shape would be <code>torch.Size([1024, 1])</code>, but sometimes it would be more convenient to have a shape <code>torch.Size([1024])</code> instead. If we had <code>torch.nn.Flatten</code>, we can simply do:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">100</span>)\n\nmodel <span class=\"pl-k\">=</span> torch.nn.Sequential(\n    torch.nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">50</span>),\n    torch.nn.ReLU(),\n    torch.nn.Linear(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">1</span>),\n    torch.nn.Flatten(<span class=\"pl-v\">start_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n)\n\n<span class=\"pl-c1\">print</span>(model(a).shape)</pre></div>\n<p>Also, if we want to do a CNN that has fully connected layers following conv layers, <code>torch.nn.Flatten</code> would also be useful:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)\n\nmodel <span class=\"pl-k\">=</span> torch.nn.Sequential(\n    torch.nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>),\n    torch.nn.ReLU(),\n    torch.nn.Flatten(),\n    torch.nn.Linear(<span class=\"pl-c1\">46080</span>, <span class=\"pl-c1\">50</span>),\n    torch.nn.ReLU(),\n    torch.nn.Linear(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">1</span>),\n)\n\n<span class=\"pl-c1\">print</span>(model(a).shape)</pre></div>\n<h2>Additional context</h2>\n<p>If the pytorch team think this is a good feature, I will do the PR.</p>\n<p>See also: <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/Flatten\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/layers/Flatten</a></p>", "body_text": "\ud83d\ude80 Feature\ntorch.nn.Flatten(start_dim=1, end_dim=-1) basically just convert the input as input.flatten(start_dim, end_dim)\nMotivation\nLet's think that we have code\nimport torch\n\na = torch.rand(1024, 100)\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 1),\n)\n\nprint(model(a).shape)\nThe shape would be torch.Size([1024, 1]), but sometimes it would be more convenient to have a shape torch.Size([1024]) instead. If we had torch.nn.Flatten, we can simply do:\nimport torch\n\na = torch.rand(1024, 100)\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 1),\n    torch.nn.Flatten(start_dim=0)\n)\n\nprint(model(a).shape)\nAlso, if we want to do a CNN that has fully connected layers following conv layers, torch.nn.Flatten would also be useful:\nimport torch\n\na = torch.rand(1024, 3, 100, 100)\n\nmodel = torch.nn.Sequential(\n    torch.nn.Conv2d(3, 10, 3),\n    torch.nn.ReLU(),\n    torch.nn.Conv2d(10, 5, 3),\n    torch.nn.ReLU(),\n    torch.nn.Flatten(),\n    torch.nn.Linear(46080, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 1),\n)\n\nprint(model(a).shape)\nAdditional context\nIf the pytorch team think this is a good feature, I will do the PR.\nSee also: https://www.tensorflow.org/api_docs/python/tf/layers/Flatten", "body": "## \ud83d\ude80 Feature\r\n\r\n`torch.nn.Flatten(start_dim=1, end_dim=-1)` basically just convert the input as `input.flatten(start_dim, end_dim)`\r\n\r\n## Motivation\r\n\r\nLet's think that we have code\r\n\r\n```python\r\nimport torch\r\n\r\na = torch.rand(1024, 100)\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(100, 50),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Linear(50, 1),\r\n)\r\n\r\nprint(model(a).shape)\r\n```\r\nThe shape would be `torch.Size([1024, 1])`, but sometimes it would be more convenient to have a shape `torch.Size([1024])` instead. If we had `torch.nn.Flatten`, we can simply do:\r\n\r\n```python\r\nimport torch\r\n\r\na = torch.rand(1024, 100)\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(100, 50),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Linear(50, 1),\r\n    torch.nn.Flatten(start_dim=0)\r\n)\r\n\r\nprint(model(a).shape)\r\n```\r\n\r\nAlso, if we want to do a CNN that has fully connected layers following conv layers, `torch.nn.Flatten` would also be useful:\r\n\r\n```python\r\nimport torch\r\n\r\na = torch.rand(1024, 3, 100, 100)\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Conv2d(3, 10, 3),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Conv2d(10, 5, 3),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Flatten(),\r\n    torch.nn.Linear(46080, 50),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Linear(50, 1),\r\n)\r\n\r\nprint(model(a).shape)\r\n```\r\n\r\n## Additional context\r\n\r\nIf the pytorch team think this is a good feature, I will do the PR.\r\n\r\nSee also: https://www.tensorflow.org/api_docs/python/tf/layers/Flatten\r\n"}