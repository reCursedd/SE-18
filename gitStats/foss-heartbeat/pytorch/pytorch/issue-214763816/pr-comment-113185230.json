{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113185230", "pull_request_review_id": 34551680, "id": 113185230, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMzE4NTIzMA==", "diff_hunk": "@@ -113,134 +123,129 @@ auto Engine::thread_on_exception(FunctionTask& task, std::exception& e) -> void\n   }\n }\n \n-static variable_list call_pre_hooks(Function& fn, variable_list grad_output) {\n+static variable_list call_pre_hooks(Function& fn, variable_list inputs) {\n   for (auto& hook : fn.pre_hooks) {\n-    grad_output = (*hook)(grad_output);\n+    inputs = (*hook)(inputs);\n   }\n-  return grad_output;\n+  return inputs;\n }\n \n-static variable_list call_post_hooks(Function& fn, variable_list grad_input, variable_list grad_output) {\n+static variable_list call_post_hooks(Function& fn, variable_list outputs, variable_list inputs) {\n   for (auto& hook : fn.post_hooks) {\n-    grad_input = (*hook)(grad_input, grad_output);\n+    outputs = (*hook)(outputs, inputs);\n   }\n-  return grad_input;\n+  return outputs;\n }\n \n static variable_list call_function(FunctionTask& task) {\n-  auto grad_output = call_pre_hooks(*task.fn, GradBuffer::variables(std::move(task.grad)));\n-  auto grad_input = task.fn->apply(grad_output);\n-  return call_post_hooks(*task.fn, std::move(grad_input), std::move(grad_output));\n+  auto& fn = *task.fn;\n+  auto inputs = call_pre_hooks(fn, InputBuffer::variables(std::move(task.inputs)));\n+\n+  auto& function_callbacks = task.base->function_callbacks;\n+  auto callback_it = function_callbacks.find(&fn);\n+  if (callback_it != function_callbacks.end()) {\n+    auto& callback = callback_it->second;\n+    if (!callback(&fn, inputs)) return variable_list(fn.next_functions.size());\n+  }\n+\n+  auto fn_outputs = fn.apply(inputs);\n+  return call_post_hooks(fn, std::move(fn_outputs), std::move(inputs));\n }\n \n auto Engine::evaluate_function(FunctionTask& task) -> void {\n-  auto grad_inputs = call_function(task);\n+  auto outputs = call_function(task);\n \n   auto& fn = *task.fn;\n-  if (!task.base->retain_variables) {\n+  if (!task.base->keep_graph) {\n     fn.releaseVariables();\n   }\n \n-  if (grad_inputs.size() != fn.previous_functions.size()) {\n+  if (outputs.size() != fn.next_functions.size()) {\n     std::stringstream ss;\n-    ss << \"Function '\" << fn.name() << \"' returned an invalid number of gradients - expected \";\n-    ss << fn.previous_functions.size() << \", but got \" << grad_inputs.size();\n+    ss << \"Function '\" << fn.name() << \"' returned an invalid number of outputs - expected \";\n+    ss << fn.next_functions.size() << \", but got \" << outputs.size();\n     throw std::runtime_error(ss.str());\n   }\n \n-  int size = grad_inputs.size();\n-  for (int i = 0; i < size; ++i) {\n-    auto& grad_input = grad_inputs[i];\n-    auto& prev_fn = fn.previous_functions[i].first;\n-    int output_nr = fn.previous_functions[i].second;\n+  int num_outputs = outputs.size();\n+  for (int i = 0; i < num_outputs; ++i) {\n+    auto& output = outputs[i];\n+    auto& next_fn = fn.next_functions[i].first;\n+    int input_nr = fn.next_functions[i].second;\n \n-    // null inputs have no previous_function and we skip them here\n-    if (!prev_fn) {\n+    if (!next_fn) {\n       continue;\n     }\n \n     // Stochastic functions are placed in the ready queue by\n-    // compute_dependencies, so we can skip them here.\n-    if (prev_fn->is_stochastic || !prev_fn->requires_grad) {\n+    // compute_dependencies, so we have to skip them here.\n+    if (next_fn->is_stochastic || !next_fn->is_executable) {\n       continue;\n     }\n \n     std::lock_guard<std::mutex> lock(task.base->mutex);\n-    if (auto var = dynamic_cast<Variable*>(prev_fn.get())) {\n-      if (!grad_input) {\n-        // NOTE: grad_input can be NULL if the function returns None for a\n-        // non_differentiable input. We may need to track additional information\n-        // at the function level to determine if a NULL grad_input is an error.\n-        std::stringstream ss;\n-        ss << \"Function '\" << fn.name() << \"' missing gradient at \" << i;\n-        throw std::runtime_error(ss.str());\n-      }\n-      var->backward(grad_input);\n-      continue;\n-    }\n-\n-    // Check if the function is ready for backward\n+    // Check if the next function is ready to be computed\n     bool is_ready = false;\n     auto& dependencies = task.base->dependencies;\n-    auto it = dependencies.find(prev_fn.get());\n+    auto it = dependencies.find(next_fn.get());\n     if (it == dependencies.end()) {\n-      auto name = prev_fn->name();\n+      auto name = next_fn->name();\n       throw std::runtime_error(std::string(\"dependency not found for \") + name);\n     } else if (--it->second == 0) {\n       dependencies.erase(it);\n       is_ready = true;\n     }\n \n     auto& not_ready = task.base->not_ready;\n-    auto not_ready_it = not_ready.find(prev_fn.get());\n+    auto not_ready_it = not_ready.find(next_fn.get());\n     if (not_ready_it == not_ready.end()) {\n       // No buffers have been allocated for the function\n-      GradBuffer prev_buffer(prev_fn->num_outputs);\n-      prev_buffer.addGrad(output_nr, std::move(grad_input));\n+      InputBuffer input_buffer(next_fn->num_inputs);\n+      input_buffer.add(input_nr, std::move(output));", "path": "torch/csrc/autograd/engine.cpp", "position": 195, "original_position": 195, "commit_id": "fc48d2c1dd1d40fef3f8c727897eaac70d9bbd14", "original_commit_id": "5a98d4a69acba872b755d026412b777ebbf66a65", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "This should be protected by an `input_nr >= 0` or `next_fn->num_inputs > 0` because in the case for example of the `Error()` function, we try to write the output in an InputBuffer of size 0 (which segfauts).", "created_at": "2017-04-25T12:48:24Z", "updated_at": "2018-11-23T15:33:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/1016#discussion_r113185230", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1016", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113185230"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1016#discussion_r113185230"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1016"}}, "body_html": "<p>This should be protected by an <code>input_nr &gt;= 0</code> or <code>next_fn-&gt;num_inputs &gt; 0</code> because in the case for example of the <code>Error()</code> function, we try to write the output in an InputBuffer of size 0 (which segfauts).</p>", "body_text": "This should be protected by an input_nr >= 0 or next_fn->num_inputs > 0 because in the case for example of the Error() function, we try to write the output in an InputBuffer of size 0 (which segfauts)."}