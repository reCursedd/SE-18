{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110114460", "pull_request_review_id": 31261635, "id": 110114460, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDExNDQ2MA==", "diff_hunk": "@@ -73,522 +68,498 @@ def _do_forward(self, *args, **kwargs):\n \n class Transpose(Function):\n \n-    def __init__(self, *dims):\n-        super(Transpose, self).__init__()\n-        assert len(dims) == 2\n-        self.dims = dims\n-\n-    def forward(self, i):\n-        result = i.transpose(*self.dims)\n-        self.mark_shared_storage((i, result))\n+    @staticmethod\n+    def forward(ctx, i, dim1, dim2):\n+        result = i.transpose(dim1, dim2)\n+        ctx.dims = (dim1, dim2)\n+        ctx.mark_shared_storage((i, result))\n         return result\n \n-    def backward(self, grad_output):\n-        return grad_output.transpose(*self.dims)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.transpose(*ctx.dims), None, None\n \n \n class View(Function):\n \n-    def __init__(self, *sizes):\n-        super(View, self).__init__()\n-        self.sizes = sizes\n-\n-    def forward(self, i):\n-        self.input_size = i.size()\n-        result = i.view(*self.sizes)\n-        self.mark_shared_storage((i, result))\n+    @staticmethod\n+    def forward(ctx, i, sizes):\n+        ctx.new_sizes = sizes\n+        ctx.old_size = i.size()\n+        result = i.view(*sizes)\n+        ctx.mark_shared_storage((i, result))\n         return result\n \n-    def backward(self, grad_output):\n-        # TODO: not sure if this clone is necessary\n-        return grad_output.contiguous().view(self.input_size)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.contiguous().view(ctx.old_size), None\n \n \n class Expand(Function):\n \n-    def __init__(self, sizes):\n-        super(Expand, self).__init__()\n-        self.sizes = sizes\n-        self.expanded_dims = []\n-\n-    def forward(self, i):\n-        result = i.expand(*self.sizes)\n-        self.num_unsqueezed = len(self.sizes) - i.dim()\n-        self.expanded_dims = [dim for dim, (expanded, original)\n-                              in enumerate(zip(self.sizes[self.num_unsqueezed:], i.size()))\n-                              if expanded != original]\n-        self.mark_shared_storage((i, result))\n+    @staticmethod\n+    def forward(ctx, i, new_size):\n+        ctx.num_unsqueezed = len(new_size) - i.dim()\n+        ctx.expanded_dims = [dim for dim, (expanded, original)\n+                             in enumerate(zip(new_size[ctx.num_unsqueezed:], i.size()))\n+                             if expanded != original]\n+        result = i.expand(*new_size)\n+        ctx.mark_shared_storage((i, result))\n         return result\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    def backward(ctx, grad_output):\n         grad_input = grad_output\n-        for i in range(self.num_unsqueezed):\n+        for i in range(ctx.num_unsqueezed):\n             grad_input = grad_input.sum(0).squeeze(0)\n-        for dim in self.expanded_dims:\n+        for dim in ctx.expanded_dims:\n             grad_input = grad_input.sum(dim)\n-        return grad_input\n+        return grad_input, None\n \n \n class Type(Function):\n \n-    def __init__(self, dest_type):\n-        super(Type, self).__init__()\n-        self.dest_type = dest_type\n-\n-    def forward(self, i):\n-        assert self.dest_type != type(i)\n-        self.input_type = type(i)\n-        return i.type(self.dest_type)\n+    @staticmethod\n+    def forward(ctx, i, dest_type):\n+        ctx.input_type = type(i)\n+        return i.type(dest_type)\n \n-    def backward(self, grad_output):\n-        return grad_output.type(self.input_type)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.type(ctx.input_type), None\n \n \n class CudaTransfer(Function):\n \n-    def __init__(self, device_id=None, async=False):\n-        super(CudaTransfer, self).__init__()\n-        self.device_id = device_id\n-        self.async = async\n-\n-    def forward(self, i):\n-        self.source_device = -1 if not i.is_cuda else i.get_device()\n-        self.source_was_cuda = i.is_cuda\n-        if self.device_id:\n-            return i.cuda(self.device_id, async=self.async)\n+    @staticmethod\n+    def forward(ctx, i, device_id=None, async=False):\n+        ctx.source_device = -1 if not i.is_cuda else i.get_device()\n+        ctx.source_was_cuda = i.is_cuda\n+        if device_id:\n+            return i.cuda(device_id, async=async)\n         else:\n-            return i.cuda(async=self.async)\n-\n-    def backward(self, grad_output):\n-        if self.source_device != -1:\n-            return grad_output.cuda(self.source_device)\n-        elif self.source_was_cuda:\n-            return grad_output\n+            return i.cuda(async=async)\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        if ctx.source_device != -1:\n+            return grad_output.cuda(ctx.source_device), None, None\n+        elif ctx.source_was_cuda:\n+            return grad_output, None, None\n         else:\n-            return grad_output.cpu()\n+            return grad_output.cpu(), None, None\n \n \n class Permute(Function):\n \n-    def __init__(self, dim_indices):\n-        super(Permute, self).__init__()\n-        self.dim_indices = dim_indices\n-        self.rev_dim_indices = [None for _ in range(len(dim_indices))]\n-        for i, dim_idx in enumerate(self.dim_indices):\n-            self.rev_dim_indices[dim_idx] = i\n-\n-    def forward(self, i):\n-        result = i.permute(*self.dim_indices)\n-        self.mark_shared_storage((i, result))\n+    @staticmethod\n+    def forward(ctx, input, dim_indices):\n+        ctx.rev_dim_indices = [None for _ in range(len(dim_indices))]\n+        for i, dim_idx in enumerate(dim_indices):\n+            ctx.rev_dim_indices[dim_idx] = i\n+        result = input.permute(*dim_indices)\n+        ctx.mark_shared_storage((input, result))\n         return result\n \n-    def backward(self, grad_output):\n-        return grad_output.permute(*self.rev_dim_indices)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.permute(*ctx.rev_dim_indices), None\n \n \n class IndexAdd(InplaceFunction):\n \n-    def __init__(self, dim, inplace=False):\n-        super(IndexAdd, self).__init__(inplace)\n-        self.dim = dim\n-\n-    def forward(self, tensor1, index, tensor2):\n-        assert not self.needs_input_grad[1]\n-        if self.needs_input_grad[2]:\n-            self.save_for_backward(index)\n-        if not self.inplace:\n+    @staticmethod\n+    def forward(ctx, tensor1, dim, index, tensor2, inplace=False):\n+        assert not ctx.needs_input_grad[2]\n+        ctx.dim = dim\n+        if ctx.needs_input_grad[3]:\n+            ctx.save_for_backward(index)\n+        if not inplace:\n             tensor1 = tensor1.clone()\n         else:\n-            self.mark_dirty(tensor1)\n-        return tensor1.index_add_(self.dim, index, tensor2)\n+            ctx.mark_dirty(tensor1)\n+        return tensor1.index_add_(ctx.dim, index, tensor2)\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n         grad_tensor1 = grad_tensor2 = None\n \n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_tensor1 = grad_output\n \n-        if self.needs_input_grad[2]:\n-            index, = self.saved_tensors\n-            grad_tensor2 = grad_output.index_select(self.dim, index)\n+        if ctx.needs_input_grad[3]:\n+            index, = ctx.saved_tensors\n+            grad_tensor2 = grad_output.index_select(ctx.dim, index)\n \n-        return grad_tensor1, None, grad_tensor2\n+        return grad_tensor1, None, None, grad_tensor2, None\n \n \n class IndexCopy(InplaceFunction):\n \n-    def __init__(self, dim, inplace=False):\n-        super(IndexCopy, self).__init__(inplace)\n-        self.dim = dim\n-\n-    def forward(self, tensor1, index, tensor2):\n-        assert not self.needs_input_grad[1]\n-        if any(self.needs_input_grad):\n-            self.save_for_backward(index)\n-        if not self.inplace:\n+    @staticmethod\n+    def forward(ctx, tensor1, dim, index, tensor2, inplace=False):\n+        assert not ctx.needs_input_grad[2]\n+        ctx.dim = dim\n+        if any(ctx.needs_input_grad):\n+            ctx.save_for_backward(index)\n+        if not inplace:\n             tensor1 = tensor1.clone()\n         else:\n-            self.mark_dirty(tensor1)\n-        return tensor1.index_copy_(self.dim, index, tensor2)\n+            ctx.mark_dirty(tensor1)\n+        return tensor1.index_copy_(dim, index, tensor2)\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n         grad_tensor1 = grad_tensor2 = None\n \n-        if any(self.needs_input_grad):\n-            index, = self.saved_tensors\n+        if any(ctx.needs_input_grad):\n+            index, = ctx.saved_tensors\n \n-        if self.needs_input_grad[0]:\n-            grad_tensor1 = grad_output.clone().index_fill_(self.dim, index, 0)\n+        if ctx.needs_input_grad[0]:\n+            grad_tensor1 = grad_output.clone().index_fill_(ctx.dim, index, 0)\n \n-        if self.needs_input_grad[2]:\n-            grad_tensor2 = grad_output.index_select(self.dim, index)\n+        if ctx.needs_input_grad[2]:\n+            grad_tensor2 = grad_output.index_select(ctx.dim, index)\n \n-        return grad_tensor1, None, grad_tensor2\n+        return grad_tensor1, None, None, grad_tensor2, None\n \n \n class IndexFill(InplaceFunction):\n \n-    def __init__(self, dim, value, inplace=False):\n-        super(IndexFill, self).__init__(inplace)\n-        self.dim = dim\n-        self.value = value\n-\n-    def forward(self, tensor, index):\n-        assert not self.needs_input_grad[1]\n-        if self.needs_input_grad[0]:\n-            self.save_for_backward(index)\n-        if not self.inplace:\n+    @staticmethod\n+    def forward(ctx, tensor, dim, index, value, inplace=False):\n+        ctx.dim = dim\n+        assert not ctx.needs_input_grad[2]\n+        if ctx.needs_input_grad[0]:\n+            ctx.save_for_backward(index)\n+        if not inplace:\n             tensor = tensor.clone()\n         else:\n-            self.mark_dirty(tensor)\n-        return tensor.index_fill_(self.dim, index, self.value)\n+            ctx.mark_dirty(tensor)\n+        return tensor.index_fill_(dim, index, value)\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n         grad_tensor = None\n \n-        if self.needs_input_grad[0]:\n-            index, = self.saved_tensors\n-            grad_tensor = grad_output.clone().index_fill_(self.dim, index, 0)\n+        if ctx.needs_input_grad[0]:\n+            index, = ctx.saved_tensors\n+            grad_tensor = grad_output.clone().index_fill_(ctx.dim, index, 0)\n \n-        return grad_tensor, None\n+        return grad_tensor, None, None, None, None\n \n \n class IndexSelect(Function):\n \n-    def __init__(self, dim):\n-        super(IndexSelect, self).__init__()\n-        self.dim = dim\n-\n-    def forward(self, tensor, index):\n-        assert not self.needs_input_grad[1]\n+    @staticmethod\n+    def forward(ctx, tensor, dim, index):\n+        ctx.dim = dim\n+        assert not ctx.needs_input_grad[2]\n \n-        if self.needs_input_grad[0]:\n-            self.save_for_backward(index)\n-            self.input_size = tensor.size()\n+        if ctx.needs_input_grad[0]:\n+            ctx.save_for_backward(index)\n+            ctx.input_size = tensor.size()\n \n-        return tensor.index_select(self.dim, index)\n+        return tensor.index_select(dim, index)\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n         grad_tensor = None\n \n-        if self.needs_input_grad[0]:\n-            index, = self.saved_tensors\n-            grad_tensor = grad_output.new(*self.input_size).zero_()\n-            grad_tensor.index_add_(self.dim, index, grad_output)\n+        if ctx.needs_input_grad[0]:\n+            index, = ctx.saved_tensors\n+            grad_tensor = grad_output.new(*ctx.input_size).zero_()\n+            grad_tensor.index_add_(ctx.dim, index, grad_output)\n \n-        return grad_tensor, None\n+        return grad_tensor, None, None\n \n \n class Concat(Function):\n \n-    def __init__(self, dim):\n-        super(Concat, self).__init__()\n-        self.dim = dim\n-\n-    def forward(self, *inputs):\n-        self.input_sizes = [i.size(self.dim) for i in inputs]\n-        return torch.cat(inputs, self.dim)\n+    @staticmethod\n+    def forward(ctx, dim, *inputs):\n+        ctx.dim = dim\n+        ctx.input_sizes = [i.size(dim) for i in inputs]\n+        return torch.cat(inputs, dim)\n \n-    def backward(self, grad_output):\n-        return tuple(grad_output.narrow(self.dim, end - size, size) for size, end\n-                     in zip(self.input_sizes, _accumulate(self.input_sizes)))\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return (None,) + tuple(grad_output.narrow(ctx.dim, end - size, size) for size, end\n+                               in zip(ctx.input_sizes, _accumulate(ctx.input_sizes)))\n \n \n+# TODO: deprecate this\n class Resize(Function):\n \n-    def __init__(self, *sizes):\n-        super(Resize, self).__init__()\n-        self.sizes = sizes\n-        self.numel = reduce(lambda x, y: x * y, sizes, 1)\n-\n-    def forward(self, tensor):\n-        if tensor.numel() != self.numel:\n+    @staticmethod\n+    def forward(ctx, tensor, sizes):\n+        ctx.sizes = sizes\n+        ctx.numel = reduce(lambda x, y: x * y, sizes, 1)\n+        if tensor.numel() != ctx.numel:\n             raise RuntimeError((\"requested resize to {} ({} elements in total), \"\n                                 \"but the given tensor has a size of {} ({} elements). \"\n                                 \"autograd's resize can only change the shape of a given \"\n                                 \"tensor, while preserving the number of elements. \").format(\n-                'x'.join(map(str, self.sizes)), self.numel,\n+                'x'.join(map(str, sizes)), ctx.numel,\n                 'x'.join(map(str, tensor.size())), tensor.numel()))\n-        self.input_sizes = tensor.size()\n-        result = tensor.new(tensor).resize_(*self.sizes)\n-        self.mark_shared_storage((tensor, result))\n-        return result\n+        ctx.input_sizes = tensor.size()\n+        if tensor.is_contiguous():\n+            result = tensor.new(tensor).contiguous().view(*sizes)\n+            ctx.mark_shared_storage((tensor, result))\n+            return result\n+        else:\n+            return tensor.contiguous().view(*sizes)\n \n-    def backward(self, grad_output):\n-        assert grad_output.numel() == self.numel\n-        return grad_output.new(grad_output).resize_(self.input_sizes)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        assert grad_output.numel() == ctx.numel\n+        return grad_output.contiguous().view(ctx.input_sizes), None\n \n \n class Clone(Function):\n \n-    def forward(self, input):\n+    @staticmethod\n+    def forward(ctx, input):\n         return input.clone()\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    def backward(ctx, grad_output):\n         return grad_output\n \n \n class Squeeze(Function):\n \n-    def __init__(self, dim=None):\n-        super(Squeeze, self).__init__()\n-        self.dim = dim\n-\n-    def forward(self, input):\n-        self.input_size = input.size()\n-        self.numel = input.numel()\n-        if self.dim is not None:\n-            result = input.squeeze(self.dim)\n+    @staticmethod\n+    def forward(ctx, input, dim=None):\n+        ctx.dim = dim\n+        ctx.input_size = input.size()\n+        if dim is not None:\n+            result = input.squeeze(dim)\n         else:\n             result = input.squeeze()\n-        self.mark_shared_storage((input, result))\n+        ctx.mark_shared_storage((input, result))\n         return result\n \n-    def backward(self, grad_output):\n-        assert grad_output.numel() == self.numel\n-        return grad_output.contiguous().view(self.input_size)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.contiguous().view(ctx.input_size), None\n \n \n class Unsqueeze(Function):\n \n-    def __init__(self, dim):\n-        super(Unsqueeze, self).__init__()\n-        self.dim = dim\n-\n-    def forward(self, input):\n-        result = input.unsqueeze(self.dim)\n-        self.mark_shared_storage((input, result))\n+    @staticmethod\n+    def forward(ctx, input, dim):\n+        ctx.dim = dim\n+        result = input.unsqueeze(dim)\n+        ctx.mark_shared_storage((input, result))\n         return result\n \n-    def backward(self, grad_output):\n-        return grad_output.squeeze(self.dim)\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        return grad_output.squeeze(ctx.dim), None\n \n \n class MaskedCopy(InplaceFunction):\n \n-    def forward(self, tensor1, mask, tensor2):\n-        assert not self.needs_input_grad[1], \"MaskedCopy can't differentiate \" \\\n-            \"the mask\"\n-        if not self.inplace:\n+    @staticmethod\n+    def forward(ctx, tensor1, mask, tensor2, inplace=False):\n+        assert not ctx.needs_input_grad[1], \"MaskedCopy can't differentiate the mask\"\n+        if not inplace:\n             tensor1 = tensor1.clone()\n         else:\n-            self.mark_dirty(tensor1)\n-        self.save_for_backward(mask)\n+            ctx.mark_dirty(tensor1)\n+        ctx.save_for_backward(mask)\n         return tensor1.masked_copy_(mask, tensor2)\n \n-    def backward(self, grad_output):\n-        mask, = self.saved_tensors\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n+        mask, = ctx.saved_tensors\n         grad_tensor1 = grad_tensor2 = None\n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_tensor1 = grad_output.clone().masked_fill_(mask, 0)\n-        if self.needs_input_grad[2]:\n+        if ctx.needs_input_grad[2]:\n             grad_tensor2 = grad_output.masked_select(mask)\n-        return grad_tensor1, None, grad_tensor2\n+        return grad_tensor1, None, grad_tensor2, None\n \n \n class MaskedFill(InplaceFunction):\n \n-    def __init__(self, value, inplace=False):\n-        super(MaskedFill, self).__init__(inplace)\n-        self.value = value\n-\n-    def forward(self, tensor, mask):\n-        assert not self.needs_input_grad[1], \"MaskedFill can't differentiate \" \\\n-            \"the mask\"\n-        if not self.inplace:\n+    @staticmethod\n+    def forward(ctx, tensor, mask, value, inplace=False):\n+        assert not ctx.needs_input_grad[1], \"MaskedFill can't differentiate the mask\"\n+        if not inplace:\n             tensor = tensor.clone()\n         else:\n-            self.mark_dirty(tensor)\n-        self.save_for_backward(mask)\n-        return tensor.masked_fill_(mask, self.value)\n-\n-    def backward(self, grad_output):\n-        mask, = self.saved_tensors\n+            ctx.mark_dirty(tensor)\n+        ctx.save_for_backward(mask)\n+        return tensor.masked_fill_(mask, value)\n+\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n+        mask, = ctx.saved_tensors\n         grad_tensor = None\n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_tensor = grad_output.clone().masked_fill_(mask, 0)\n-        return grad_tensor, None\n+        return grad_tensor, None, None, None\n \n \n class MaskedSelect(Function):\n \n-    def forward(self, tensor, mask):\n-        assert not self.needs_input_grad[1], \"MaskedSelect can't differentiate \" \\\n-            \"the mask\"\n-        self.input_size = tensor.size()\n-        self.save_for_backward(mask)\n+    @staticmethod\n+    def forward(ctx, tensor, mask):\n+        assert not ctx.needs_input_grad[1], \"MaskedSelect can't differentiate the mask\"\n+        ctx.input_size = tensor.size()\n+        ctx.save_for_backward(mask)\n         return tensor.masked_select(mask)\n \n-    def backward(self, grad_output):\n-        mask, = self.saved_tensors\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n+        mask, = ctx.saved_tensors\n         grad_tensor = None\n-        if self.needs_input_grad[0]:\n-            # TODO: remove zero\n-            grad_tensor = grad_output.new(self.input_size).zero_()\n+        if ctx.needs_input_grad[0]:\n+            grad_tensor = grad_output.new(ctx.input_size).zero_()\n             grad_tensor.masked_copy_(mask, grad_output)\n         return grad_tensor, None\n \n \n class _MultiSelectionFunction(Function):\n \n-    def __init__(self, dim, return_indices):\n-        super(_MultiSelectionFunction, self).__init__()\n-        self.dim = dim\n-        self.return_indices = return_indices\n-\n-    def forward(self, input):\n-        fn = getattr(input, self.__class__.__name__.lower())\n-        self.input_size = input.size()\n-        output, indices = fn(*self.args)\n-        if self.return_indices:\n-            self.save_for_backward(indices)\n-            self.mark_non_differentiable(indices)\n+    @staticmethod\n+    def forward(ctx, input, dim, return_indices, args):\n+        fn = getattr(input, ctx._forward_cls.__name__.lower())\n+        ctx.return_indices = return_indices\n+        ctx.input_size = input.size()\n+        ctx.dim = dim\n+        output, indices = fn(*args)\n+        if return_indices:\n+            ctx.save_for_backward(indices)\n+            ctx.mark_non_differentiable(indices)\n             return output, indices\n         else:\n-            self.indices = indices\n+            ctx.indices = indices\n             return output\n \n-    def backward(self, grad_output, grad_indices=None):\n-        grad_input = grad_output.new(self.input_size).zero_()\n-        if self.return_indices:\n-            indices, = self.saved_tensors\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output, grad_indices=None):\n+        grad_input = grad_output.new(ctx.input_size).zero_()\n+        if ctx.return_indices:\n+            indices, = ctx.saved_tensors\n         else:\n-            indices = self.indices\n-        dim = self.dim if self.dim is not None else grad_output.dim() - 1\n-        return grad_input.scatter_(dim, indices, grad_output)\n+            indices = ctx.indices\n+        dim = ctx.dim if ctx.dim is not None else grad_output.dim() - 1\n+        return (grad_input.scatter_(dim, indices, grad_output),) + (None,) * ctx.num_flags\n \n \n class Sort(_MultiSelectionFunction):\n \n-    def __init__(self, dim=None, descending=False, return_indices=True):\n-        super(Sort, self).__init__(dim, return_indices)\n-        self.descending = descending\n-\n-    def forward(self, input):\n-        dim = self.dim if self.dim is not None else input.dim() - 1\n-        self.args = (dim, self.descending)\n-        return super(Sort, self).forward(input)\n+    @staticmethod\n+    def forward(ctx, input, dim=None, descending=False, return_indices=True):\n+        ctx.dim = dim if dim is not None else input.dim() - 1\n+        args = (ctx.dim, descending)\n+        ctx.num_flags = 3\n+        return _MultiSelectionFunction.forward(ctx, input, dim, return_indices, args)\n \n \n class Topk(_MultiSelectionFunction):\n \n-    def __init__(self, k, dim=None, largest=True, sort=True, return_indices=True):\n-        super(Topk, self).__init__(dim, return_indices)\n-        self.k = k\n-        self.largest = largest\n-        self.sort = sort\n-\n-    def forward(self, input):\n-        dim = self.dim if self.dim is not None else input.dim() - 1\n-        self.args = (self.k, dim, self.largest, self.sort)\n-        return super(Topk, self).forward(input)\n+    @staticmethod\n+    def forward(ctx, input, k, dim=None, largest=True, sort=True, return_indices=True):\n+        ctx.dim = dim if dim is not None else input.dim() - 1\n+        args = (k, ctx.dim, largest, sort)\n+        ctx.num_flags = 5\n+        return _MultiSelectionFunction.forward(ctx, input, dim, return_indices, args)\n \n \n class Chunk(Function):\n \n-    def __init__(self, num_chunks, dim=0):\n-        super(Chunk, self).__init__()\n-        self.num_chunks = num_chunks\n-        self.dim = dim\n-\n-    def forward(self, i):\n-        self.input_size = i.size()\n-        result = i.chunk(self.num_chunks, self.dim)\n-        self.mark_shared_storage(*((i, chunk) for chunk in result))\n+    @staticmethod\n+    def forward(ctx, i, num_chunks, dim=0):\n+        ctx.input_size = i.size()\n+        ctx.dim = dim\n+        result = i.chunk(num_chunks, dim)\n+        ctx.mark_shared_storage(*((i, chunk) for chunk in result))\n         return result\n \n-    def backward(self, *grad_output):\n-        grad_input = grad_output[0].new(self.input_size)\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, *grad_output):\n+        grad_input = grad_output[0].new(ctx.input_size)\n         offset = 0\n         for grad in grad_output:\n-            grad_size = grad.size(self.dim)\n-            grad_input.narrow(self.dim, offset, grad_size).copy_(grad)\n+            grad_size = grad.size(ctx.dim)\n+            grad_input.narrow(ctx.dim, offset, grad_size).copy_(grad)\n             offset += grad_size\n-        return grad_input\n+        return grad_input, None, None\n \n \n class Gather(Function):\n \n-    def __init__(self, dim):\n-        super(Gather, self).__init__()\n-        self.dim = dim\n-\n-    def forward(self, input, index):\n-        assert not self.needs_input_grad[1], \"Gather can't differentiate \" \\\n-            \"the index\"\n-        self.input_size = input.size()\n-        self.save_for_backward(index)\n-        return input.gather(self.dim, index)\n+    @staticmethod\n+    def forward(ctx, input, dim, index):\n+        assert not ctx.needs_input_grad[2], \"Gather can't differentiate the index\"\n+        ctx.input_size = input.size()\n+        ctx.save_for_backward(index)\n+        ctx.dim = dim\n+        return input.gather(dim, index)\n \n-    def backward(self, grad_output):\n-        index, = self.saved_tensors\n-        grad_input = grad_output.new(self.input_size).zero_()\n-        return grad_input.scatter_(self.dim, index, grad_output), None\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n+        index, = ctx.saved_tensors\n+        grad_input = grad_output.new(ctx.input_size).zero_()\n+        return grad_input.scatter_(ctx.dim, index, grad_output), None, None\n \n \n class Scatter(InplaceFunction):\n \n-    def __init__(self, dim, inplace=False):\n-        super(Scatter, self).__init__(inplace)\n-        self.dim = dim\n-\n-    def forward(self, input, index, source):\n-        assert not self.needs_input_grad[1], \"Scatter can't differentiate \" \\\n-            \"the index\"\n-        if self.inplace:\n-            self.mark_dirty(input)\n+    @staticmethod\n+    def forward(ctx, input, dim, index, source, inplace=False):\n+        assert not ctx.needs_input_grad[2], \"Scatter can't differentiate the index\"\n+        ctx.dim = dim\n+        if inplace:\n+            ctx.mark_dirty(input)\n         else:\n             input = input.clone()\n-        self.save_for_backward(index)\n-        return input.scatter_(self.dim, index, source)\n+        ctx.save_for_backward(index)\n+        return input.scatter_(ctx.dim, index, source)\n \n-    def backward(self, grad_output):\n-        index, = self.saved_tensors\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n+        index, = ctx.saved_tensors\n         grad_input = grad_source = None\n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_input = grad_output.clone()\n-            grad_input.scatter_(self.dim, index, 0)\n-        if self.needs_input_grad[2]:\n-            grad_source = grad_output.gather(self.dim, index)\n-        return grad_input, None, grad_source\n+            grad_input.scatter_(ctx.dim, index, 0)\n+        if ctx.needs_input_grad[3]:\n+            grad_source = grad_output.gather(ctx.dim, index)\n+        return grad_input, None, None, grad_source, None\n \n \n class Repeat(Function):\n \n-    def __init__(self, repeats):\n-        super(Repeat, self).__init__()\n-        self.repeats = repeats\n-\n-    def forward(self, input):\n-        return input.repeat(self.repeats)\n+    @staticmethod\n+    def forward(ctx, input, repeats):\n+        ctx.repeats = repeats\n+        return input.repeat(repeats)\n \n-    def backward(self, grad_output):\n+    @staticmethod\n+    @once_differentiable\n+    def backward(ctx, grad_output):\n         grad_input = grad_output\n-        for dim, repeat in enumerate(self.repeats):\n+        for dim, repeat in enumerate(ctx.repeats):\n             if repeat == 1:\n                 continue\n             grad_input = sum(grad_input.chunk(repeat, dim))\n-        return grad_input\n+        return grad_input, None\n \n \n class Cumsum(Function):", "path": "torch/autograd/_functions/tensor.py", "position": 911, "original_position": 911, "commit_id": "fc48d2c1dd1d40fef3f8c727897eaac70d9bbd14", "original_commit_id": "bede03dfafc40978334d5564eb2e2d48483d437a", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Because I missed it haha. Anyway, I won't be adding any more new style functions in this PR, I'll leave that for later so that others can contribute too.", "created_at": "2017-04-06T09:18:38Z", "updated_at": "2018-11-23T15:33:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/1016#discussion_r110114460", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1016", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110114460"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1016#discussion_r110114460"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1016"}}, "body_html": "<p>Because I missed it haha. Anyway, I won't be adding any more new style functions in this PR, I'll leave that for later so that others can contribute too.</p>", "body_text": "Because I missed it haha. Anyway, I won't be adding any more new style functions in this PR, I'll leave that for later so that others can contribute too.", "in_reply_to_id": 109777582}