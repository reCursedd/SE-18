{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109773242", "pull_request_review_id": 30897355, "id": 109773242, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTc3MzI0Mg==", "diff_hunk": "@@ -38,7 +38,46 @@ def backward(variables, grad_variables, retain_variables=False):\n             specify ``True`` if you want to differentiate some subgraph multiple\n             times.\n     \"\"\"\n+    grad_variables = tuple(var if isinstance(var, Variable) or var is None\n+                           else Variable(var, volatile=True)\n+                           for var in grad_variables)\n     Variable._execution_engine.run_backward(\n-        tuple(variables), tuple(grad_variables), retain_variables)\n+        tuple(variables), grad_variables, retain_variables)\n+\n+\n+def differentiate(outputs, grad_outputs, inputs, only_inputs=True, retain_variables=True):\n+    \"\"\"Computes and returns the sum of gradients of outputs w.r.t. the inputs.\n+\n+    ``grad_outputs`` should be a sequence of length matching ``output``\n+    containing the pre-computed gradients w.r.t. each of the outputs. If an\n+    output doesn't require_grad, then the gradient can be ``None``).\n+    Gradients can be given as Tensors when one doesn't need the graph of the\n+    derivative, or as Variables, in which case the graph will be created.\n+\n+    If ``only_inputs`` is True, the function will only return a list of gradients\n+    w.r.t the specified inputs. If it's False, then gradient w.r.t. all remaining\n+    leaves will still be computed, and will be accumulated into their ``.grad``\n+    attribute.\n+\n+    Arguments:\n+        variables (sequence of Variable): outputs of the differentiated function.", "path": "torch/autograd/__init__.py", "position": null, "original_position": 27, "commit_id": "fc48d2c1dd1d40fef3f8c727897eaac70d9bbd14", "original_commit_id": "bede03dfafc40978334d5564eb2e2d48483d437a", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "I think this should be `outputs` and `grad_outputs` below to be consistent with the naming in this function.", "created_at": "2017-04-04T20:47:37Z", "updated_at": "2018-11-23T15:33:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/1016#discussion_r109773242", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1016", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109773242"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1016#discussion_r109773242"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1016"}}, "body_html": "<p>I think this should be <code>outputs</code> and <code>grad_outputs</code> below to be consistent with the naming in this function.</p>", "body_text": "I think this should be outputs and grad_outputs below to be consistent with the naming in this function."}