{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/299905668", "html_url": "https://github.com/pytorch/pytorch/pull/1016#issuecomment-299905668", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1016", "id": 299905668, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTkwNTY2OA==", "user": {"login": "mjdietzx", "id": 7217256, "node_id": "MDQ6VXNlcjcyMTcyNTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7217256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjdietzx", "html_url": "https://github.com/mjdietzx", "followers_url": "https://api.github.com/users/mjdietzx/followers", "following_url": "https://api.github.com/users/mjdietzx/following{/other_user}", "gists_url": "https://api.github.com/users/mjdietzx/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjdietzx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjdietzx/subscriptions", "organizations_url": "https://api.github.com/users/mjdietzx/orgs", "repos_url": "https://api.github.com/users/mjdietzx/repos", "events_url": "https://api.github.com/users/mjdietzx/events{/privacy}", "received_events_url": "https://api.github.com/users/mjdietzx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T15:45:01Z", "updated_at": "2017-05-08T15:45:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm also trying to implemented wGAN with gradient penalty. I've taken a slightly different approach than <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7908951\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/HiiYL\">@HiiYL</a>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate `x_hat` where `x` is a batch of real images and `x_z` is a batch of generated images.</span>\nepsilon <span class=\"pl-k\">=</span> torch.randn(batch_size, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\nx_hat <span class=\"pl-k\">=</span> epsilon.expand_as(x.data) <span class=\"pl-k\">*</span> x.data <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">-</span> epsilon.expand_as(x_z.data)) <span class=\"pl-k\">*</span> x_z.data\nx_hat <span class=\"pl-k\">=</span> autograd.Variable(x_hat).detach()\n\no <span class=\"pl-k\">=</span> discriminator.forward(x_hat)\ngradients <span class=\"pl-k\">=</span> autograd.grad(o, x_hat)\ngradient_penalty <span class=\"pl-k\">=</span> autograd.Variable(<span class=\"pl-c1\">10.0</span> <span class=\"pl-k\">*</span> torch.pow(torch.norm(gradients, <span class=\"pl-v\">p</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>))\ngradient_penalty.backward()</pre></div>\n<p>and <code>gradients = autograd.grad(o, x_hat)</code> throws <code>RuntimeError: grad can be implicitly created only for scalar outputs</code></p>", "body_text": "I'm also trying to implemented wGAN with gradient penalty. I've taken a slightly different approach than @HiiYL.\n# calculate `x_hat` where `x` is a batch of real images and `x_z` is a batch of generated images.\nepsilon = torch.randn(batch_size, 1, 1, 1)\nx_hat = epsilon.expand_as(x.data) * x.data + (1.0 - epsilon.expand_as(x_z.data)) * x_z.data\nx_hat = autograd.Variable(x_hat).detach()\n\no = discriminator.forward(x_hat)\ngradients = autograd.grad(o, x_hat)\ngradient_penalty = autograd.Variable(10.0 * torch.pow(torch.norm(gradients, p=2) - 1.0, 2.0))\ngradient_penalty.backward()\nand gradients = autograd.grad(o, x_hat) throws RuntimeError: grad can be implicitly created only for scalar outputs", "body": "I'm also trying to implemented wGAN with gradient penalty. I've taken a slightly different approach than @HiiYL.\r\n\r\n```python\r\n# calculate `x_hat` where `x` is a batch of real images and `x_z` is a batch of generated images.\r\nepsilon = torch.randn(batch_size, 1, 1, 1)\r\nx_hat = epsilon.expand_as(x.data) * x.data + (1.0 - epsilon.expand_as(x_z.data)) * x_z.data\r\nx_hat = autograd.Variable(x_hat).detach()\r\n\r\no = discriminator.forward(x_hat)\r\ngradients = autograd.grad(o, x_hat)\r\ngradient_penalty = autograd.Variable(10.0 * torch.pow(torch.norm(gradients, p=2) - 1.0, 2.0))\r\ngradient_penalty.backward()\r\n```\r\n\r\nand `gradients = autograd.grad(o, x_hat)` throws `RuntimeError: grad can be implicitly created only for scalar outputs`"}