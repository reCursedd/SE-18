{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/299930097", "html_url": "https://github.com/pytorch/pytorch/pull/1016#issuecomment-299930097", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1016", "id": 299930097, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTkzMDA5Nw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T17:15:13Z", "updated_at": "2017-05-08T17:15:13Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7908951\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/HiiYL\">@HiiYL</a> the problem is that you're trying to compute the grad of grad of Convolution, but it's not implemented yet. This PR only added the machinery necessary to compute higher order derivatives, but didn't actually add support to the existing autograd functions.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7217256\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mjdietzx\">@mjdietzx</a> First, you should never call <code>.forward</code> directly on the model. Just do <code>discriminator(x_hat)</code>. The error should tell you exactly what's wrong - output of discriminator is not a scalar, so you need to specify a third argument to <code>autograd.grad</code>. You can find the description in the docs.</p>", "body_text": "@HiiYL the problem is that you're trying to compute the grad of grad of Convolution, but it's not implemented yet. This PR only added the machinery necessary to compute higher order derivatives, but didn't actually add support to the existing autograd functions.\n@mjdietzx First, you should never call .forward directly on the model. Just do discriminator(x_hat). The error should tell you exactly what's wrong - output of discriminator is not a scalar, so you need to specify a third argument to autograd.grad. You can find the description in the docs.", "body": "@HiiYL the problem is that you're trying to compute the grad of grad of Convolution, but it's not implemented yet. This PR only added the machinery necessary to compute higher order derivatives, but didn't actually add support to the existing autograd functions.\r\n@mjdietzx First, you should never call `.forward` directly on the model. Just do `discriminator(x_hat)`. The error should tell you exactly what's wrong - output of discriminator is not a scalar, so you need to specify a third argument to `autograd.grad`. You can find the description in the docs."}