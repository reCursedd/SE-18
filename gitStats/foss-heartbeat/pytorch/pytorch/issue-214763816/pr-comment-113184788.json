{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113184788", "pull_request_review_id": 34551680, "id": 113184788, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMzE4NDc4OA==", "diff_hunk": "@@ -411,140 +503,183 @@ static void _mark_non_differentiable(THPFunction *self, t2var_type &t2var)\n   self->non_differentiable = NULL;\n }\n \n-static bool _ensure_tuple(THPObjectPtr& obj)\n-{\n-  if (PyTuple_Check(obj.get()))\n-    return false;\n-\n-  PyObject *tuple = PyTuple_New(1);\n-  if (!tuple) throw python_error();\n-  PyTuple_SET_ITEM(tuple, 0, obj.release());\n-  obj = tuple;\n-  return true;\n-}\n+struct UnpackedInput {\n+  PyObject *raw_input;\n+  THPObjectPtr tensor_input;\n+  variable_list input_vars;\n+};\n \n-PyObject *THPFunction_do_forward(THPFunction *self, PyObject *inputs)\n-{\n-  try {\n-    Py_ssize_t num_inputs = inputs ? PyTuple_GET_SIZE(inputs) : 0;\n-\n-    // Unpack inputs and check if they require gradients or are volatile\n-    THPObjectPtr unpacked_inputs = PyTuple_New(num_inputs);\n-    self->needs_input_grad = PyTuple_New(num_inputs);\n-    self->cdata.requires_grad = false;\n-    bool is_volatile = false;\n-    for (int i = 0; i < num_inputs; i++) {\n-      PyObject *input = PyTuple_GET_ITEM(inputs, i);\n-      THPUtils_assert(THPVariable_Check(input), \"expected a Variable argument, \"\n-          \"but got %s\", THPUtils_typename(input));\n-      THPVariable *variable = (THPVariable*)input;\n-\n-      // Unpack the variable\n-      PyTuple_SET_ITEM(unpacked_inputs.get(), i, THPVariable_get_data(variable));\n-\n-      // We can't move this to C, because it's going to be accessed from user code.\n-      PyTuple_SET_ITEM(self->needs_input_grad, i, PyBool_FromLong(variable->cdata->requires_grad));\n-\n-      is_volatile = is_volatile || variable->cdata->is_volatile;\n-      self->cdata.requires_grad = self->cdata.requires_grad || variable->cdata->requires_grad;\n-    }\n+struct InputFlags {\n+  FunctionFlags flags;\n+  THPObjectPtr needs_input_grad;\n+  std::vector<bool> is_variable_input;\n+};\n \n-    // Now we're ready to call a forward (implemented in Python)\n-    THPObjectPtr forward_fn = PyObject_GetAttrString((PyObject*)self, \"forward\");\n-    THPUtils_assert(forward_fn.get(), \"function %s doesn't implement a required \"\n-        \"'forward' method\", THPUtils_typename((PyObject*)self));\n-    THPObjectPtr raw_output = PyObject_CallObject(forward_fn, unpacked_inputs);\n-    if (!raw_output) return NULL;\n-    // Wrap output in a tuple, if it's not one already\n-    bool unpack_output = _ensure_tuple(raw_output);\n-    int num_outputs = PyTuple_GET_SIZE(raw_output.get());\n-\n-\n-    THPObjectPtr outputs = PyTuple_New(num_outputs);\n-    if (!outputs) return NULL;\n-    if (is_volatile) {\n-      // If one of the inputs is volatile let's take a fast path - we want\n-      // minimize the overhead of inference\n-      for (int i = 0; i < num_outputs; i++) {\n-        PyObject *output = PyTuple_GET_ITEM(raw_output.get(), i);\n-        THPVariable *output_var = (THPVariable*)THPVariable_NewVolatile(output);\n-        if (!output_var) return NULL;\n-        output_var->cdata->output_nr = i;\n-        PyTuple_SET_ITEM(outputs.get(), i, (PyObject*)output_var);\n+template<bool enforce_variables>\n+std::pair<UnpackedInput, InputFlags> unpack_input(PyObject *args) {\n+  UnpackedInput unpacked;\n+  InputFlags flags;\n+\n+  auto num_args = PyTuple_GET_SIZE(args);\n+  unpacked.tensor_input = PyTuple_New(num_args);\n+  flags.needs_input_grad = PyTuple_New(num_args);\n+  for (int i = 0; i < num_args; i++) {\n+    PyObject *arg = PyTuple_GET_ITEM(args, i);\n+    PyObject *new_arg;\n+\n+    bool is_variable = THPVariable_Check(arg);\n+    flags.is_variable_input.push_back(is_variable);\n+    if (!is_variable) {\n+      if (enforce_variables) {\n+        THPUtils_setError(\"expected a Variable argument, but got %s\",\n+                          THPUtils_typename(arg));\n+        throw python_error();\n       }\n+      Py_INCREF(arg);\n+      new_arg = arg;\n+      Py_INCREF(Py_False);\n+      PyTuple_SET_ITEM(flags.needs_input_grad.get(), i, Py_False);\n     } else {\n-      // We're not volatile, so there's a lot of bookkeeping to do...\n-      self->num_inputs = num_inputs;\n-      self->cdata.num_outputs = num_outputs;\n-      t2var_type t2var;\n-\n-      // Save previous functions and initialize t2var map\n-      self->cdata.previous_functions.resize(num_inputs);\n-      for (int i = 0; i < num_inputs; i++) {\n-        THPVariable *input_var = (THPVariable*)PyTuple_GET_ITEM(inputs, i);\n-        PyObject *input_tensor = PyTuple_GET_ITEM(unpacked_inputs.get(), i);\n-        t2var.emplace(input_tensor, input_var);\n-\n-        // Save previous function\n-        std::shared_ptr<Function> prev_fn;\n-        if (input_var->cdata->creator) {\n-          prev_fn = input_var->cdata->creator;\n-        } else {\n-          prev_fn = input_var->cdata;\n-        }\n-        self->cdata.previous_functions[i] = std::make_pair<>(prev_fn, input_var->cdata->output_nr);\n-      }\n+      THPVariable* variable = (THPVariable*)arg;\n+      new_arg = THPVariable_get_data(variable);\n+      unpacked.input_vars.push_back(variable->cdata);\n+      PyObject* needs_grad = variable->cdata->requires_grad ? Py_True : Py_False;\n+      Py_INCREF(needs_grad);\n+      PyTuple_SET_ITEM(flags.needs_input_grad.get(), i, needs_grad);\n+    }\n+    PyTuple_SET_ITEM(unpacked.tensor_input.get(), i, new_arg);\n+  }\n \n-      std::unordered_set<PyObject *> dirty_inputs;\n-      _mark_dirty(self, t2var, dirty_inputs);\n-      _wrap_outputs(self, t2var, dirty_inputs, raw_output, outputs);\n-      _join_version_counters(self, t2var);\n-      if (self->cdata.requires_grad || self->cdata.is_stochastic) {\n-        _save_variables(self, t2var);\n-        _mark_non_differentiable(self, t2var);\n-      } else {\n-        // Remove unnecessary attributes\n-        Py_XDECREF(self->to_save);\n-        self->to_save = NULL;\n-        Py_XDECREF(self->non_differentiable);\n-        self->non_differentiable = NULL;\n-      }\n+  flags.flags = Function::flags(unpacked.input_vars);\n+  return std::make_pair(std::move(unpacked), std::move(flags));\n+}\n+\n+PyObject* process_outputs(THPFunction* grad_fn, const UnpackedInput& unpacked, THPObjectPtr raw_output) {\n+  bool unpack_output = _ensure_tuple(raw_output);\n+\n+  auto num_outputs = PyTuple_GET_SIZE(raw_output.get());\n+\n+  THPObjectPtr outputs = PyTuple_New(num_outputs);\n+  if (!outputs) throw python_error();\n+  if (!grad_fn) { // if volatile\n+    // If one of the inputs is volatile let's take a fast path - we want\n+    // minimize the overhead of inference\n+    for (int i = 0; i < num_outputs; i++) {\n+      PyObject *output = PyTuple_GET_ITEM(raw_output.get(), i);\n+      THPVariable *output_var = (THPVariable*)THPVariable_NewVolatile(output);\n+      if (!output_var) throw python_error();\n+      output_var->cdata->output_nr = i;\n+      PyTuple_SET_ITEM(outputs.get(), i, (PyObject*)output_var);\n+    }\n+  } else {\n+    grad_fn->cdata.num_inputs = num_outputs;\n+\n+    // Initialize t2var map\n+    t2var_type t2var;\n+    for (auto& c_var : unpacked.input_vars) {\n+      THPVariable* py_var = (THPVariable*)c_var->pyobj;\n+      t2var.emplace(py_var->data, py_var);\n     }\n \n-    // Unpack the output, unless .forward() returned a tuple\n-    if (unpack_output) {\n-      PyObject *output = PyTuple_GET_ITEM(outputs.get(), 0);\n-      Py_INCREF(output);\n-      return output;\n+    std::unordered_set<PyObject *> dirty_inputs;\n+    _mark_dirty(grad_fn, t2var, dirty_inputs);\n+    _wrap_outputs(grad_fn, t2var, dirty_inputs, raw_output, outputs);\n+    _join_version_counters(grad_fn, t2var);\n+    // TODO: is this stochastic check needed here?\n+    if (grad_fn->cdata.is_executable || grad_fn->cdata.is_stochastic) {\n+      _save_variables(grad_fn, t2var);", "path": "torch/csrc/autograd/python_function.cpp", "position": null, "original_position": 550, "commit_id": "fc48d2c1dd1d40fef3f8c727897eaac70d9bbd14", "original_commit_id": "5a98d4a69acba872b755d026412b777ebbf66a65", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "This should be performed after marking the variables as non differentiable (next line) otherwise the saved variables will always have `requires_grad=True` (trying to accumulate their gradient will fail as they don't have a grad accumulator and will crash).", "created_at": "2017-04-25T12:46:13Z", "updated_at": "2018-11-23T15:33:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/1016#discussion_r113184788", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1016", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113184788"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1016#discussion_r113184788"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1016"}}, "body_html": "<p>This should be performed after marking the variables as non differentiable (next line) otherwise the saved variables will always have <code>requires_grad=True</code> (trying to accumulate their gradient will fail as they don't have a grad accumulator and will crash).</p>", "body_text": "This should be performed after marking the variables as non differentiable (next line) otherwise the saved variables will always have requires_grad=True (trying to accumulate their gradient will fail as they don't have a grad accumulator and will crash)."}