{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/299970606", "html_url": "https://github.com/pytorch/pytorch/pull/1016#issuecomment-299970606", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1016", "id": 299970606, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTk3MDYwNg==", "user": {"login": "mjdietzx", "id": 7217256, "node_id": "MDQ6VXNlcjcyMTcyNTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7217256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjdietzx", "html_url": "https://github.com/mjdietzx", "followers_url": "https://api.github.com/users/mjdietzx/followers", "following_url": "https://api.github.com/users/mjdietzx/following{/other_user}", "gists_url": "https://api.github.com/users/mjdietzx/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjdietzx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjdietzx/subscriptions", "organizations_url": "https://api.github.com/users/mjdietzx/orgs", "repos_url": "https://api.github.com/users/mjdietzx/repos", "events_url": "https://api.github.com/users/mjdietzx/events{/privacy}", "received_events_url": "https://api.github.com/users/mjdietzx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T19:45:32Z", "updated_at": "2017-05-08T19:45:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>After fixing my first problem based on your suggestions I also now get the same error <code>RuntimeError: ConvBackward is not differentiable</code> when calling <code>backward()</code> on the gradients. so I guess gradient penalty in wGAN wont be possible until support for higher order derivatives is adding to existing autograd functions? is that on the roadmap <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> ?</p>", "body_text": "After fixing my first problem based on your suggestions I also now get the same error RuntimeError: ConvBackward is not differentiable when calling backward() on the gradients. so I guess gradient penalty in wGAN wont be possible until support for higher order derivatives is adding to existing autograd functions? is that on the roadmap @apaszke ?", "body": "After fixing my first problem based on your suggestions I also now get the same error `RuntimeError: ConvBackward is not differentiable` when calling `backward()` on the gradients. so I guess gradient penalty in wGAN wont be possible until support for higher order derivatives is adding to existing autograd functions? is that on the roadmap @apaszke ?"}