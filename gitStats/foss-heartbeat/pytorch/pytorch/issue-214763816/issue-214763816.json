{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1016", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1016/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1016/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1016/events", "html_url": "https://github.com/pytorch/pytorch/pull/1016", "id": 214763816, "node_id": "MDExOlB1bGxSZXF1ZXN0MTExMTEwNjEw", "number": 1016, "title": "Autograd refactor", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 34, "created_at": "2017-03-16T16:31:37Z", "updated_at": "2018-11-23T15:33:14Z", "closed_at": "2017-05-01T20:44:57Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1016", "html_url": "https://github.com/pytorch/pytorch/pull/1016", "diff_url": "https://github.com/pytorch/pytorch/pull/1016.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1016.patch"}, "body_html": "<p>Progress:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Change names in the backend to better match the new graph representation</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Remove Function superclass from Variable. Replace them with special nodes in the graph</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Implement the new function definition.\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Make sure it's compatible with legacy definitions.</li>\n</ul>\n</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Make sure Variables are never unpacked in the Engine (partially done in the first commit)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Implement some of the built in Functions using the new format (to see if it's actually usable)</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Add a function that returns a list of higher order grads</li>\n</ul>\n<hr>\n<p>The tasks below are left for future PRs. The first two can be easily paralellized and can be done by others too.</p>\n<ul>\n<li>Adapt all definitions of built in Functions (using <code>@differentiable_once</code> for now).</li>\n<li>Implement all <code>backward</code> functions using Variables (so they can be differentiated multiple times)</li>\n<li>Add deprecation warnings for old Function format</li>\n<li>Add a switch for saving .creator graph traces</li>\n</ul>\n<h2>Summary of changes</h2>\n<h3>New function definition format</h3>\n<p><strong>Note that the old declarations are still supported - most of the core implementations are still not converted.</strong></p>\n<p>New format allows to implement jacobian vector products (jvp, L-op) of functions depending on jvp's of other functions (aka. grad of grad, hessian-vector products).</p>\n<p>The new declarations look like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">MultiplyAdd</span>(<span class=\"pl-e\">Function</span>):\n                                                            <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1.</span>\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">input1</span>, <span class=\"pl-smi\">scalar</span>, <span class=\"pl-smi\">input2</span>):               <span class=\"pl-c\"><span class=\"pl-c\">#</span> 2.</span>\n        ctx.scalar <span class=\"pl-k\">=</span> scalar                                 <span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.</span>\n        <span class=\"pl-k\">return</span> input1 <span class=\"pl-k\">+</span> scalar <span class=\"pl-k\">*</span> input2\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):                         <span class=\"pl-c\"><span class=\"pl-c\">#</span> 4.</span>\n        <span class=\"pl-k\">return</span> grad_output, <span class=\"pl-c1\">None</span>, ctx.scalar <span class=\"pl-k\">*</span> grad_output  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 5.</span></pre></div>\n<h4>Adnotations:</h4>\n<ol>\n<li>Functions no longer can have an <code>__init__</code> method. Think of them as pairs of pure functions that are formulas specifying how to compute the function and its jvp (<code>Dfn * grad_output</code>).</li>\n<li>Beacuse of 1., <code>forward</code> can now accept arguments of arbitrary types (used to only accept Variables). Any Variables appearing in <code>args</code> will be unpacked into Tensors. <strong>Arguments are not recursively searched. For example, a list of Variables won't be unpacked into a list of Tensors, and they won't be registered as inputs in the graph</strong>. Keyword arguments are not supported (need arg ordering to construct the graph).</li>\n<li><code>forward</code> gets a <code>ctx</code> as a first argument - this is an object (of unspecified type - not an instance of this class) with an interface identical to <code>self</code> in old style definitions (<code>save_for_backward</code>, <code>mark_non_differentiable</code>, etc.) and is used to pass information to the <code>backward</code> call. For example, this function needs to save a <code>scalar</code> argument. Note that you shouldn't assign input or output tensors to it, however intermediate buffers are ok.</li>\n<li><code>grad_output</code> is now a Variable, and the whole backward method needs to be implemented in terms of Variables (they shouldn't be unpacked into tensors, or the derivative graph will be malformed, see notes on <code>@once_differentiable</code> below). <code>ctx</code> will be the same object that was passed to <code>forward</code>.</li>\n<li><code>backward </code> should return gradients for all arguments given to <code>forward</code> (even non-Variable arguments, but it should be <code>None</code> in such case). Unnecessary trailing <code>None</code>s are still accepted (useful when <code>forward</code> has optional arguments).</li>\n</ol>\n<p>For comparison, here's how a legacy definition of <code>MultiplyAdd</code> would look like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">MultiplyAdd</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">scalar</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.scalar <span class=\"pl-k\">=</span> scalar\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input1</span>, <span class=\"pl-smi\">scalar</span>, <span class=\"pl-smi\">input2</span>):\n        <span class=\"pl-k\">return</span> input1 <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.scalar <span class=\"pl-k\">*</span> input2\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        <span class=\"pl-k\">return</span> grad_output, <span class=\"pl-c1\">self</span>.scalar <span class=\"pl-k\">*</span> grad_output</pre></div>\n<h3><code>@once_differentiable</code></h3>\n<p>The fact that <code>backward</code> now takes Variables might unnecessarily complicate implementations of custom function that e.g. call into other libs. For that reason, this PR also introduces a <code>@once_differentiable</code> decorator, that can be used to wrap <code>backward</code>. After adding it, <code>backward</code> functions will get a tensor <code>grad_output</code> and will be expected to return a grad input tensor for each tensor argument given in <code>forward</code> (and <code>None</code> for all other args).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">SciPyFunction</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">input1</span>, <span class=\"pl-smi\">input2</span>):\n        <span class=\"pl-k\">return</span> scipy.my_function(input1.numpy(), input2.numpy())\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-en\">@once_differentiable</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):\n        <span class=\"pl-k\">return</span> scipy.my_function2(grad_output), scipy.my_function3(grad_output)</pre></div>\n<h3><code>torch.autograd.backward</code></h3>\n<p>Added <code>create_graph</code>. If True, the graph for vjp will be created (defaults to False), allowing to differentiate the grad computation. Defaults to True if <code>grad_variables</code> contains at least one non-volatile Variable, and False otherwise.<br>\nRenamed <code>retain_variables</code> to <code>retain_graph</code>. The old argument will remain supported until v0.3, but will print deprecation warnings. If unspecified, defaults to the value of <code>create_graph</code>.</p>\n<p>If <code>grad_variables</code> contains tensors, they are automatically promoted to Variables (volatile unless <code>create_graph</code> is True). Also, None entries in <code>grad_variables</code> are now accepted if their corresponding <code>variables</code> entries are scalar Variables (grad_output filled with 1 is allocated for them). Additionally, if all <code>grad_variables</code> could be <code>None</code> the argument is now optional.</p>\n<h3><code>torch.autograd.grad</code></h3>\n<p>While Chainer-style API is great for first order grads, it doesn't work nearly as well when computing higher order derivatives. See this example:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nx.mul(<span class=\"pl-c1\">2</span>).sum().backward(<span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> x.grad.mul(<span class=\"pl-c1\">2</span>).sum()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This accumulates grad of grad into x.grad, adding together results of both backward() calls</span>\ny.backward() </pre></div>\n<p>For that reason, this PR also implements <code>grad</code> - a functional-style function that computes the vjp, and instead of accumulating it into <code>.grad</code> of all leaves, it returns a list of grads w.r.t. given function inputs (parameters are considered inputs too).</p>\n<p>Example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> grad\n\nx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nx.mul(<span class=\"pl-c1\">2</span>).sum().backward(<span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> x.grad.mul(<span class=\"pl-c1\">2</span>).sum()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The line below **doesn't change x.grad**</span>\nx_hv <span class=\"pl-k\">=</span> grad(y, x) <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad of y w.r.t. x. This argument would be the grad_output, but y is scalar</span></pre></div>\n<p>Arguments <code>outputs</code>, <code>inputs</code>, <code>grad_outputs</code> arguments can be both sequences of Variables (or Tensors and Nones in case of <code>grad_outputs</code>), or single Variables.</p>\n<p>If one doesn't request the grad w.r.t. all leaf Variables, unneeded gradients are not computed, and won't be accumulated into them (by default <code>grad</code> has no side effects). If <code>only_inputs</code> argument is set to False, the whole graph will be differentiated, grads w.r.t. <code>inputs</code> will be returned in a list and not accumulated into <code>.grad</code>, grads w.r.t. all other leaves will be accumulated into their <code>.grad</code>.</p>\n<h3><code>.grad</code> semantics</h3>\n<p>By default the semantics are the same as right now. When not using any of the options implemented in this PR, <code>.grad</code> Variables will be volatile, and incoming grads will be accumulated in-place (both Variable and its <code>.data</code> will be the same objects - while we don't guarantee that, some people depend on that in their scripts, so it's best to support it unless there's no other way).<br>\nHowever, when using derivative graphs, these Variables will need to have their <code>.grad_fn</code> set correctly, and shouldn't be modified in-place (they might have been used in some functions!). For that reason in such cases the <code>.grad</code> attribute will point to a new Variable, with new <code>.data</code>, after each accumulation.</p>\n<p>To sum up:</p>\n<table>\n<thead>\n<tr>\n<th><code>.grad</code></th>\n<th>New grad</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>volatile</td>\n<td>volatile</td>\n<td>Accumulated in-place into <code>.grad</code></td>\n</tr>\n<tr>\n<td>volatile</td>\n<td>-</td>\n<td>Accumulated in-place into <code>.grad</code> which remains volatile</td>\n</tr>\n<tr>\n<td>-</td>\n<td>volatile</td>\n<td>New grad is converted to a Variable that doesn't require grad and added out-of-place. Result overwrites <code>.grad</code></td>\n</tr>\n<tr>\n<td>-</td>\n<td>-</td>\n<td>Added out-of-place. Result overwrites <code>.grad</code></td>\n</tr>\n</tbody>\n</table>\n<h3>Implementation details</h3>\n<ul>\n<li>Variables no longer subclass Function and therefore can no longer appear in the graph. After this PR graphs contain AccumulateGrad nodes instead of Variables.</li>\n<li>Engine now supports per-function callbacks that are called before evaluating the function, and if they return false the <code>apply</code> function won't be called (all gradients will default to null, which is an equivalent of 0), and its <code>next_functions</code> won't be added to the ready queue (unless they are already waiting for execution and this was their last dependency).</li>\n</ul>", "body_text": "Progress:\n\n Change names in the backend to better match the new graph representation\n Remove Function superclass from Variable. Replace them with special nodes in the graph\n Implement the new function definition.\n\n Make sure it's compatible with legacy definitions.\n\n\n Make sure Variables are never unpacked in the Engine (partially done in the first commit)\n Implement some of the built in Functions using the new format (to see if it's actually usable)\n Add a function that returns a list of higher order grads\n\n\nThe tasks below are left for future PRs. The first two can be easily paralellized and can be done by others too.\n\nAdapt all definitions of built in Functions (using @differentiable_once for now).\nImplement all backward functions using Variables (so they can be differentiated multiple times)\nAdd deprecation warnings for old Function format\nAdd a switch for saving .creator graph traces\n\nSummary of changes\nNew function definition format\nNote that the old declarations are still supported - most of the core implementations are still not converted.\nNew format allows to implement jacobian vector products (jvp, L-op) of functions depending on jvp's of other functions (aka. grad of grad, hessian-vector products).\nThe new declarations look like this:\nclass MultiplyAdd(Function):\n                                                            # 1.\n    @staticmethod\n    def forward(ctx, input1, scalar, input2):               # 2.\n        ctx.scalar = scalar                                 # 3.\n        return input1 + scalar * input2\n\n    @staticmethod\n    def backward(ctx, grad_output):                         # 4.\n        return grad_output, None, ctx.scalar * grad_output  # 5.\nAdnotations:\n\nFunctions no longer can have an __init__ method. Think of them as pairs of pure functions that are formulas specifying how to compute the function and its jvp (Dfn * grad_output).\nBeacuse of 1., forward can now accept arguments of arbitrary types (used to only accept Variables). Any Variables appearing in args will be unpacked into Tensors. Arguments are not recursively searched. For example, a list of Variables won't be unpacked into a list of Tensors, and they won't be registered as inputs in the graph. Keyword arguments are not supported (need arg ordering to construct the graph).\nforward gets a ctx as a first argument - this is an object (of unspecified type - not an instance of this class) with an interface identical to self in old style definitions (save_for_backward, mark_non_differentiable, etc.) and is used to pass information to the backward call. For example, this function needs to save a scalar argument. Note that you shouldn't assign input or output tensors to it, however intermediate buffers are ok.\ngrad_output is now a Variable, and the whole backward method needs to be implemented in terms of Variables (they shouldn't be unpacked into tensors, or the derivative graph will be malformed, see notes on @once_differentiable below). ctx will be the same object that was passed to forward.\nbackward  should return gradients for all arguments given to forward (even non-Variable arguments, but it should be None in such case). Unnecessary trailing Nones are still accepted (useful when forward has optional arguments).\n\nFor comparison, here's how a legacy definition of MultiplyAdd would look like:\nclass MultiplyAdd(Function):\n    def __init__(self, scalar):\n        super().__init__()\n        self.scalar = scalar\n\n    def forward(self, input1, scalar, input2):\n        return input1 + self.scalar * input2\n\n    def backward(self, grad_output):\n        return grad_output, self.scalar * grad_output\n@once_differentiable\nThe fact that backward now takes Variables might unnecessarily complicate implementations of custom function that e.g. call into other libs. For that reason, this PR also introduces a @once_differentiable decorator, that can be used to wrap backward. After adding it, backward functions will get a tensor grad_output and will be expected to return a grad input tensor for each tensor argument given in forward (and None for all other args).\nclass SciPyFunction(Function):\n    @staticmethod\n    def forward(ctx, input1, input2):\n        return scipy.my_function(input1.numpy(), input2.numpy())\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        return scipy.my_function2(grad_output), scipy.my_function3(grad_output)\ntorch.autograd.backward\nAdded create_graph. If True, the graph for vjp will be created (defaults to False), allowing to differentiate the grad computation. Defaults to True if grad_variables contains at least one non-volatile Variable, and False otherwise.\nRenamed retain_variables to retain_graph. The old argument will remain supported until v0.3, but will print deprecation warnings. If unspecified, defaults to the value of create_graph.\nIf grad_variables contains tensors, they are automatically promoted to Variables (volatile unless create_graph is True). Also, None entries in grad_variables are now accepted if their corresponding variables entries are scalar Variables (grad_output filled with 1 is allocated for them). Additionally, if all grad_variables could be None the argument is now optional.\ntorch.autograd.grad\nWhile Chainer-style API is great for first order grads, it doesn't work nearly as well when computing higher order derivatives. See this example:\nx = Variable(torch.randn(2, 2), requires_grad=True)\nx.mul(2).sum().backward(create_graph=True)\ny = x.grad.mul(2).sum()\n# This accumulates grad of grad into x.grad, adding together results of both backward() calls\ny.backward() \nFor that reason, this PR also implements grad - a functional-style function that computes the vjp, and instead of accumulating it into .grad of all leaves, it returns a list of grads w.r.t. given function inputs (parameters are considered inputs too).\nExample:\nfrom torch.autograd import grad\n\nx = Variable(torch.randn(2, 2), requires_grad=True)\nx.mul(2).sum().backward(create_graph=True)\ny = x.grad.mul(2).sum()\n# The line below **doesn't change x.grad**\nx_hv = grad(y, x) # grad of y w.r.t. x. This argument would be the grad_output, but y is scalar\nArguments outputs, inputs, grad_outputs arguments can be both sequences of Variables (or Tensors and Nones in case of grad_outputs), or single Variables.\nIf one doesn't request the grad w.r.t. all leaf Variables, unneeded gradients are not computed, and won't be accumulated into them (by default grad has no side effects). If only_inputs argument is set to False, the whole graph will be differentiated, grads w.r.t. inputs will be returned in a list and not accumulated into .grad, grads w.r.t. all other leaves will be accumulated into their .grad.\n.grad semantics\nBy default the semantics are the same as right now. When not using any of the options implemented in this PR, .grad Variables will be volatile, and incoming grads will be accumulated in-place (both Variable and its .data will be the same objects - while we don't guarantee that, some people depend on that in their scripts, so it's best to support it unless there's no other way).\nHowever, when using derivative graphs, these Variables will need to have their .grad_fn set correctly, and shouldn't be modified in-place (they might have been used in some functions!). For that reason in such cases the .grad attribute will point to a new Variable, with new .data, after each accumulation.\nTo sum up:\n\n\n\n.grad\nNew grad\nAction\n\n\n\n\nvolatile\nvolatile\nAccumulated in-place into .grad\n\n\nvolatile\n-\nAccumulated in-place into .grad which remains volatile\n\n\n-\nvolatile\nNew grad is converted to a Variable that doesn't require grad and added out-of-place. Result overwrites .grad\n\n\n-\n-\nAdded out-of-place. Result overwrites .grad\n\n\n\nImplementation details\n\nVariables no longer subclass Function and therefore can no longer appear in the graph. After this PR graphs contain AccumulateGrad nodes instead of Variables.\nEngine now supports per-function callbacks that are called before evaluating the function, and if they return false the apply function won't be called (all gradients will default to null, which is an equivalent of 0), and its next_functions won't be added to the ready queue (unless they are already waiting for execution and this was their last dependency).", "body": "Progress:\r\n- [x] Change names in the backend to better match the new graph representation\r\n- [x] Remove Function superclass from Variable. Replace them with special nodes in the graph\r\n- [x] Implement the new function definition.\r\n  - [x] Make sure it's compatible with legacy definitions.\r\n- [x] Make sure Variables are never unpacked in the Engine (partially done in the first commit)\r\n- [x] Implement some of the built in Functions using the new format (to see if it's actually usable)\r\n- [x] Add a function that returns a list of higher order grads\r\n\r\n---\r\n\r\nThe tasks below are left for future PRs. The first two can be easily paralellized and can be done by others too.\r\n\r\n- Adapt all definitions of built in Functions (using `@differentiable_once` for now).\r\n- Implement all `backward` functions using Variables (so they can be differentiated multiple times)\r\n- Add deprecation warnings for old Function format\r\n- Add a switch for saving .creator graph traces\r\n\r\n## Summary of changes\r\n\r\n### New function definition format\r\n\r\n**Note that the old declarations are still supported - most of the core implementations are still not converted.**\r\n\r\nNew format allows to implement jacobian vector products (jvp, L-op) of functions depending on jvp's of other functions (aka. grad of grad, hessian-vector products).\r\n\r\nThe new declarations look like this:\r\n```python\r\nclass MultiplyAdd(Function):\r\n                                                            # 1.\r\n    @staticmethod\r\n    def forward(ctx, input1, scalar, input2):               # 2.\r\n        ctx.scalar = scalar                                 # 3.\r\n        return input1 + scalar * input2\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):                         # 4.\r\n        return grad_output, None, ctx.scalar * grad_output  # 5.\r\n```\r\n\r\n#### Adnotations:\r\n1. Functions no longer can have an `__init__` method. Think of them as pairs of pure functions that are formulas specifying how to compute the function and its jvp (`Dfn * grad_output`).\r\n2. Beacuse of 1., `forward` can now accept arguments of arbitrary types (used to only accept Variables). Any Variables appearing in `args` will be unpacked into Tensors. **Arguments are not recursively searched. For example, a list of Variables won't be unpacked into a list of Tensors, and they won't be registered as inputs in the graph**. Keyword arguments are not supported (need arg ordering to construct the graph).\r\n3. `forward` gets a `ctx` as a first argument - this is an object (of unspecified type - not an instance of this class) with an interface identical to `self` in old style definitions (`save_for_backward`, `mark_non_differentiable`, etc.) and is used to pass information to the `backward` call. For example, this function needs to save a `scalar` argument. Note that you shouldn't assign input or output tensors to it, however intermediate buffers are ok.\r\n4. `grad_output` is now a Variable, and the whole backward method needs to be implemented in terms of Variables (they shouldn't be unpacked into tensors, or the derivative graph will be malformed, see notes on `@once_differentiable` below). `ctx` will be the same object that was passed to `forward`.\r\n5. `backward ` should return gradients for all arguments given to `forward` (even non-Variable arguments, but it should be `None` in such case). Unnecessary trailing `None`s are still accepted (useful when `forward` has optional arguments).\r\n\r\nFor comparison, here's how a legacy definition of `MultiplyAdd` would look like:\r\n\r\n```python\r\nclass MultiplyAdd(Function):\r\n    def __init__(self, scalar):\r\n        super().__init__()\r\n        self.scalar = scalar\r\n\r\n    def forward(self, input1, scalar, input2):\r\n        return input1 + self.scalar * input2\r\n\r\n    def backward(self, grad_output):\r\n        return grad_output, self.scalar * grad_output\r\n```\r\n\r\n### `@once_differentiable`\r\n\r\nThe fact that `backward` now takes Variables might unnecessarily complicate implementations of custom function that e.g. call into other libs. For that reason, this PR also introduces a `@once_differentiable` decorator, that can be used to wrap `backward`. After adding it, `backward` functions will get a tensor `grad_output` and will be expected to return a grad input tensor for each tensor argument given in `forward` (and `None` for all other args).\r\n\r\n```python\r\nclass SciPyFunction(Function):\r\n    @staticmethod\r\n    def forward(ctx, input1, input2):\r\n        return scipy.my_function(input1.numpy(), input2.numpy())\r\n\r\n    @staticmethod\r\n    @once_differentiable\r\n    def backward(ctx, grad_output):\r\n        return scipy.my_function2(grad_output), scipy.my_function3(grad_output)\r\n```\r\n\r\n### `torch.autograd.backward`\r\n\r\nAdded `create_graph`. If True, the graph for vjp will be created (defaults to False), allowing to differentiate the grad computation. Defaults to True if `grad_variables` contains at least one non-volatile Variable, and False otherwise.\r\nRenamed `retain_variables` to `retain_graph`. The old argument will remain supported until v0.3, but will print deprecation warnings. If unspecified, defaults to the value of `create_graph`.\r\n\r\nIf `grad_variables` contains tensors, they are automatically promoted to Variables (volatile unless `create_graph` is True). Also, None entries in `grad_variables` are now accepted if their corresponding `variables` entries are scalar Variables (grad_output filled with 1 is allocated for them). Additionally, if all `grad_variables` could be `None` the argument is now optional.\r\n\r\n### `torch.autograd.grad`\r\n\r\nWhile Chainer-style API is great for first order grads, it doesn't work nearly as well when computing higher order derivatives. See this example:\r\n\r\n```python\r\nx = Variable(torch.randn(2, 2), requires_grad=True)\r\nx.mul(2).sum().backward(create_graph=True)\r\ny = x.grad.mul(2).sum()\r\n# This accumulates grad of grad into x.grad, adding together results of both backward() calls\r\ny.backward() \r\n```\r\n\r\nFor that reason, this PR also implements `grad` - a functional-style function that computes the vjp, and instead of accumulating it into `.grad` of all leaves, it returns a list of grads w.r.t. given function inputs (parameters are considered inputs too).\r\n\r\nExample:\r\n```python\r\nfrom torch.autograd import grad\r\n\r\nx = Variable(torch.randn(2, 2), requires_grad=True)\r\nx.mul(2).sum().backward(create_graph=True)\r\ny = x.grad.mul(2).sum()\r\n# The line below **doesn't change x.grad**\r\nx_hv = grad(y, x) # grad of y w.r.t. x. This argument would be the grad_output, but y is scalar\r\n```\r\n\r\nArguments `outputs`, `inputs`, `grad_outputs` arguments can be both sequences of Variables (or Tensors and Nones in case of `grad_outputs`), or single Variables.\r\n\r\nIf one doesn't request the grad w.r.t. all leaf Variables, unneeded gradients are not computed, and won't be accumulated into them (by default `grad` has no side effects). If `only_inputs` argument is set to False, the whole graph will be differentiated, grads w.r.t. `inputs` will be returned in a list and not accumulated into `.grad`, grads w.r.t. all other leaves will be accumulated into their `.grad`.\r\n\r\n### `.grad` semantics\r\n\r\nBy default the semantics are the same as right now. When not using any of the options implemented in this PR, `.grad` Variables will be volatile, and incoming grads will be accumulated in-place (both Variable and its `.data` will be the same objects - while we don't guarantee that, some people depend on that in their scripts, so it's best to support it unless there's no other way).\r\nHowever, when using derivative graphs, these Variables will need to have their `.grad_fn` set correctly, and shouldn't be modified in-place (they might have been used in some functions!). For that reason in such cases the `.grad` attribute will point to a new Variable, with new `.data`, after each accumulation.\r\n\r\nTo sum up:\r\n\r\n| `.grad`  | New grad | Action                                                                                                          |\r\n|----------|----------|-----------------------------------------------------------------------------------------------------------------|\r\n| volatile | volatile | Accumulated in-place into `.grad`                                                                               |\r\n| volatile | -        | Accumulated in-place into `.grad` which remains volatile                                                        |\r\n| -        | volatile | New grad is converted to a Variable that doesn't require grad and added out-of-place. Result overwrites `.grad` |\r\n| -        | -        | Added out-of-place. Result overwrites `.grad`                                                                   |\r\n\r\n### Implementation details\r\n\r\n* Variables no longer subclass Function and therefore can no longer appear in the graph. After this PR graphs contain AccumulateGrad nodes instead of Variables.\r\n* Engine now supports per-function callbacks that are called before evaluating the function, and if they return false the `apply` function won't be called (all gradients will default to null, which is an equivalent of 0), and its `next_functions` won't be added to the ready queue (unless they are already waiting for execution and this was their last dependency)."}