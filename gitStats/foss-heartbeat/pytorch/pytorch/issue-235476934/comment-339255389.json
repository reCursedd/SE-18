{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/339255389", "html_url": "https://github.com/pytorch/pytorch/issues/1788#issuecomment-339255389", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1788", "id": 339255389, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTI1NTM4OQ==", "user": {"login": "dirkweissenborn", "id": 1391131, "node_id": "MDQ6VXNlcjEzOTExMzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1391131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkweissenborn", "html_url": "https://github.com/dirkweissenborn", "followers_url": "https://api.github.com/users/dirkweissenborn/followers", "following_url": "https://api.github.com/users/dirkweissenborn/following{/other_user}", "gists_url": "https://api.github.com/users/dirkweissenborn/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkweissenborn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkweissenborn/subscriptions", "organizations_url": "https://api.github.com/users/dirkweissenborn/orgs", "repos_url": "https://api.github.com/users/dirkweissenborn/repos", "events_url": "https://api.github.com/users/dirkweissenborn/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkweissenborn/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-25T08:31:18Z", "updated_at": "2017-10-25T08:31:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, I struggled with the same issue and found that the python code for packing and unpacking is highly unoptimized since it slices tensors for each timestep, which is really unecessary. Consider the case where a lengths in the batch are the same, a single slice and reshaping would be enough. Basically we only need to iterate over the different batch lengths we have and NOT over all timesteps which is really wasteful. Similarly, I think padding could be improved.</p>\n<p>I quick fix for improving packing would be the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">pack_padded_sequence</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">lengths</span>, <span class=\"pl-smi\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-k\">if</span> lengths[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">&lt;=</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>length of all samples has to be greater than 0, <span class=\"pl-pds\">\"</span></span>\n                         <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>but found an element in 'lengths' that is &lt;=0<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">if</span> batch_first:\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n\n    steps <span class=\"pl-k\">=</span> []\n    batch_sizes <span class=\"pl-k\">=</span> []\n    lengths_iter <span class=\"pl-k\">=</span> <span class=\"pl-c1\">reversed</span>(lengths)\n    batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(lengths) <span class=\"pl-k\">!=</span> batch_size:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lengths array has incorrect size<span class=\"pl-pds\">\"</span></span>)\n\n    prev_l <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">for</span> i, l <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(lengths_iter):\n        <span class=\"pl-k\">if</span> l <span class=\"pl-k\">&gt;</span> prev_l:\n            c_batch_size <span class=\"pl-k\">=</span> batch_size <span class=\"pl-k\">-</span> i\n            steps.append(<span class=\"pl-c1\">input</span>[prev_l:l, :c_batch_size].contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">2</span>)))\n            batch_sizes.extend([c_batch_size] <span class=\"pl-k\">*</span> (l <span class=\"pl-k\">-</span> prev_l))\n            prev_l <span class=\"pl-k\">=</span> l\n\n    <span class=\"pl-k\">return</span> nn.utils.rnn.PackedSequence(torch.cat(steps), batch_sizes)</pre></div>", "body_text": "Hi, I struggled with the same issue and found that the python code for packing and unpacking is highly unoptimized since it slices tensors for each timestep, which is really unecessary. Consider the case where a lengths in the batch are the same, a single slice and reshaping would be enough. Basically we only need to iterate over the different batch lengths we have and NOT over all timesteps which is really wasteful. Similarly, I think padding could be improved.\nI quick fix for improving packing would be the following:\ndef pack_padded_sequence(input, lengths, batch_first=False):\n    if lengths[-1] <= 0:\n        raise ValueError(\"length of all samples has to be greater than 0, \"\n                         \"but found an element in 'lengths' that is <=0\")\n    if batch_first:\n        input = input.transpose(0, 1)\n\n    steps = []\n    batch_sizes = []\n    lengths_iter = reversed(lengths)\n    batch_size = input.size(1)\n    if len(lengths) != batch_size:\n        raise ValueError(\"lengths array has incorrect size\")\n\n    prev_l = 0\n    for i, l in enumerate(lengths_iter):\n        if l > prev_l:\n            c_batch_size = batch_size - i\n            steps.append(input[prev_l:l, :c_batch_size].contiguous().view(-1, input.size(2)))\n            batch_sizes.extend([c_batch_size] * (l - prev_l))\n            prev_l = l\n\n    return nn.utils.rnn.PackedSequence(torch.cat(steps), batch_sizes)", "body": "Hi, I struggled with the same issue and found that the python code for packing and unpacking is highly unoptimized since it slices tensors for each timestep, which is really unecessary. Consider the case where a lengths in the batch are the same, a single slice and reshaping would be enough. Basically we only need to iterate over the different batch lengths we have and NOT over all timesteps which is really wasteful. Similarly, I think padding could be improved.\r\n\r\n I quick fix for improving packing would be the following:\r\n\r\n```python\r\ndef pack_padded_sequence(input, lengths, batch_first=False):\r\n    if lengths[-1] <= 0:\r\n        raise ValueError(\"length of all samples has to be greater than 0, \"\r\n                         \"but found an element in 'lengths' that is <=0\")\r\n    if batch_first:\r\n        input = input.transpose(0, 1)\r\n\r\n    steps = []\r\n    batch_sizes = []\r\n    lengths_iter = reversed(lengths)\r\n    batch_size = input.size(1)\r\n    if len(lengths) != batch_size:\r\n        raise ValueError(\"lengths array has incorrect size\")\r\n\r\n    prev_l = 0\r\n    for i, l in enumerate(lengths_iter):\r\n        if l > prev_l:\r\n            c_batch_size = batch_size - i\r\n            steps.append(input[prev_l:l, :c_batch_size].contiguous().view(-1, input.size(2)))\r\n            batch_sizes.extend([c_batch_size] * (l - prev_l))\r\n            prev_l = l\r\n\r\n    return nn.utils.rnn.PackedSequence(torch.cat(steps), batch_sizes)\r\n```"}