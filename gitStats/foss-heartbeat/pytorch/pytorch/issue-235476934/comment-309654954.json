{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/309654954", "html_url": "https://github.com/pytorch/pytorch/issues/1788#issuecomment-309654954", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1788", "id": 309654954, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTY1NDk1NA==", "user": {"login": "stefbraun", "id": 13469638, "node_id": "MDQ6VXNlcjEzNDY5NjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/13469638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefbraun", "html_url": "https://github.com/stefbraun", "followers_url": "https://api.github.com/users/stefbraun/followers", "following_url": "https://api.github.com/users/stefbraun/following{/other_user}", "gists_url": "https://api.github.com/users/stefbraun/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefbraun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefbraun/subscriptions", "organizations_url": "https://api.github.com/users/stefbraun/orgs", "repos_url": "https://api.github.com/users/stefbraun/repos", "events_url": "https://api.github.com/users/stefbraun/events{/privacy}", "received_events_url": "https://api.github.com/users/stefbraun/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-20T06:13:28Z", "updated_at": "2017-06-20T07:02:29Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Here is a script that provides two ways of using packed sequences: one way is fast, the other way is around 4 times slower. Both yield the same checksums at the final layer output. I am not able to explain why one is faster, could you please have a look at it? I also ran the code with nvprof and attached the output.</p>\n<p>Output of fast variant<br>\n<a href=\"https://github.com/pytorch/pytorch/files/1087245/fast_packed.txt\">nvprof_fast</a></p>\n<pre><code>fast_packed (\n  (gru): GRU(123, 320)\n  (linear): Linear (320 -&gt; 10)\n)\n::: # network parameters: 430410\n::: Checksum of final layer output from epoch 0:8.07597064972\n::: Checksum of final layer output from epoch 5:6.69699001312\n::: Checksum of final layer output from epoch 10:1.61638641357\n::: Checksum of final layer output from epoch 15:-6.10106754303\n::: Checksum of final layer output from epoch 20:-11.1396369934\n&gt;&gt;&gt; Median runtime per epoch [sec] 0.222762107849\n</code></pre>\n<p>Output of slow variant<br>\n<a href=\"https://github.com/pytorch/pytorch/files/1087241/slow_packed.txt\">nvprof_slow</a></p>\n<pre><code>slow_packed (\n  (gru): GRU(123, 320)\n  (linear): Linear (320 -&gt; 10)\n)\n::: # network parameters: 430410\n::: Checksum of final layer output from epoch 0:8.07597064972\n::: Checksum of final layer output from epoch 5:6.69699001312\n::: Checksum of final layer output from epoch 10:1.61638641357\n::: Checksum of final layer output from epoch 15:-6.10106754303\n::: Checksum of final layer output from epoch 20:-11.1396369934\n&gt;&gt;&gt; Median runtime per epoch [sec] 0.930104017258\n</code></pre>\n<p>Script</p>\n<pre><code>from timeit import default_timer as timer\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n\n# Set seeds\nnp.random.seed(11)\ntorch.manual_seed(11)\ntorch.cuda.manual_seed(11)\n\n\n# Provide default parameters\ndef default_params():\n    rnn_size = 320\n    learning_rate = 1e-3\n    epochs = 25\n    return rnn_size, learning_rate, epochs\n\n\n# Provide a toy batch\ndef toy_batch(seed=11, shape=(25, 1000, 123), classes=10):\n    batch_size, max_len, features = shape\n    np.random.seed(seed)\n\n    # Samples\n    bX = np.float32(np.random.uniform(-1, 1, (shape)))\n    b_lenX = np.int32(np.linspace(max_len, max_len, batch_size))\n    # print('::: Lengths of samples in batch: {}'.format(b_lenX))\n\n    # Targets\n    bY = np.int32(np.random.randint(low=0, high=classes - 1, size=batch_size))\n\n    return bX, b_lenX, bY, classes\n\n\n# Get data\nbX, b_lenX, bY, classes = toy_batch()\nbatch_size, max_len, features = bX.shape\nrnn_size, learning_rate, epochs = default_params()\n\n# PyTorch compatibility: time first, batch second\nbX = np.transpose(bX, (1, 0, 2))\n\n# Create symbolic vars\nbX = Variable(torch.from_numpy(bX).cuda())\nbX = pack_padded_sequence(bX, b_lenX[::-1])  # Pack those sequences for masking, plz\n\nbY = Variable(torch.from_numpy(bY).cuda())\n\n\n# Network 1: slow use of packed sequences\nclass slow_packed(nn.Module):\n    def __init__(self, features, rnn_size):\n        super(slow_packed, self).__init__()\n        self.gru = nn.GRU(input_size=features, hidden_size=rnn_size, bias=True)\n        self.linear = nn.Linear(rnn_size, classes, bias=True)\n\n    def forward(self, x):\n        h1p, state = self.gru(x)  # RNN\n        h1, lens = pad_packed_sequence(h1p)  # unpack\n        max_len, batch_size, features = h1.size()  # get sizes for reshape\n        h2 = h1.view(max_len * batch_size, -1)  # reshape\n        h3 = self.linear(h2)  # linear transform\n        h4 = h3.view(max_len, batch_size, -1)  # reshape\n        h5 = h4[-1, :, :]  # slice last element\n        return h5\n\n\n# Network 2: fast use of packed sequences\nclass fast_packed(nn.Module):\n    def __init__(self, features, rnn_size):\n        super(fast_packed, self).__init__()\n        self.gru = nn.GRU(input_size=features, hidden_size=rnn_size, bias=True)\n        self.linear = nn.Linear(rnn_size, classes, bias=True)\n\n    def forward(self, x):\n        h1p, state = self.gru(x)  # RNN\n        h3 = self.linear(h1p.data)  # linear transform\n        h3p = PackedSequence(h3, h1p.batch_sizes)  # create PackedSequence (pytorch docs: Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence().\n        h4, lens = pad_packed_sequence(h3p)  # unpack\n        h5 = h4[-1, :, :]  # slice last element\n        return h5\n\n\n########################################################\n# Switch between fast and slow variant by commenting\nnet = slow_packed(features=features, rnn_size=rnn_size)\n# net = fast_packed(features=features, rnn_size=rnn_size)\n########################################################\nnet.cuda()  # move network to GPU\nprint(net)\n\n# Print parameter count\nparams = 0\nfor param in list(net.parameters()):\n    sizes = 1\n    for el in param.size():\n        sizes = sizes * el\n    params += sizes\nprint('::: # network parameters: ' + str(params))\n\n# Create optimizer, criterion\noptimizer = optim.Adam(net.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()  # loss definition\n\n# Start training\ntime = []\nfor ep in range(epochs):\n\n    start = timer()\n    optimizer.zero_grad()\n    output = net(bX)\n    loss = criterion(output, bY.long())\n    loss.backward()\n    optimizer.step()\n\n    torch.cuda.synchronize()  # Synchronize for more precise timing\n    end = timer()\n    time.append(end - start)\n\n    if ep % 5 == 0:\n        print('::: Checksum of final layer output from epoch {}:{}'.format(ep, torch.sum(output).cpu().data.numpy()[0]))\n\nprint('&gt;&gt;&gt; Median runtime per epoch [sec] {}'.format(np.median(time)))\n</code></pre>", "body_text": "@apaszke Here is a script that provides two ways of using packed sequences: one way is fast, the other way is around 4 times slower. Both yield the same checksums at the final layer output. I am not able to explain why one is faster, could you please have a look at it? I also ran the code with nvprof and attached the output.\nOutput of fast variant\nnvprof_fast\nfast_packed (\n  (gru): GRU(123, 320)\n  (linear): Linear (320 -> 10)\n)\n::: # network parameters: 430410\n::: Checksum of final layer output from epoch 0:8.07597064972\n::: Checksum of final layer output from epoch 5:6.69699001312\n::: Checksum of final layer output from epoch 10:1.61638641357\n::: Checksum of final layer output from epoch 15:-6.10106754303\n::: Checksum of final layer output from epoch 20:-11.1396369934\n>>> Median runtime per epoch [sec] 0.222762107849\n\nOutput of slow variant\nnvprof_slow\nslow_packed (\n  (gru): GRU(123, 320)\n  (linear): Linear (320 -> 10)\n)\n::: # network parameters: 430410\n::: Checksum of final layer output from epoch 0:8.07597064972\n::: Checksum of final layer output from epoch 5:6.69699001312\n::: Checksum of final layer output from epoch 10:1.61638641357\n::: Checksum of final layer output from epoch 15:-6.10106754303\n::: Checksum of final layer output from epoch 20:-11.1396369934\n>>> Median runtime per epoch [sec] 0.930104017258\n\nScript\nfrom timeit import default_timer as timer\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n\n# Set seeds\nnp.random.seed(11)\ntorch.manual_seed(11)\ntorch.cuda.manual_seed(11)\n\n\n# Provide default parameters\ndef default_params():\n    rnn_size = 320\n    learning_rate = 1e-3\n    epochs = 25\n    return rnn_size, learning_rate, epochs\n\n\n# Provide a toy batch\ndef toy_batch(seed=11, shape=(25, 1000, 123), classes=10):\n    batch_size, max_len, features = shape\n    np.random.seed(seed)\n\n    # Samples\n    bX = np.float32(np.random.uniform(-1, 1, (shape)))\n    b_lenX = np.int32(np.linspace(max_len, max_len, batch_size))\n    # print('::: Lengths of samples in batch: {}'.format(b_lenX))\n\n    # Targets\n    bY = np.int32(np.random.randint(low=0, high=classes - 1, size=batch_size))\n\n    return bX, b_lenX, bY, classes\n\n\n# Get data\nbX, b_lenX, bY, classes = toy_batch()\nbatch_size, max_len, features = bX.shape\nrnn_size, learning_rate, epochs = default_params()\n\n# PyTorch compatibility: time first, batch second\nbX = np.transpose(bX, (1, 0, 2))\n\n# Create symbolic vars\nbX = Variable(torch.from_numpy(bX).cuda())\nbX = pack_padded_sequence(bX, b_lenX[::-1])  # Pack those sequences for masking, plz\n\nbY = Variable(torch.from_numpy(bY).cuda())\n\n\n# Network 1: slow use of packed sequences\nclass slow_packed(nn.Module):\n    def __init__(self, features, rnn_size):\n        super(slow_packed, self).__init__()\n        self.gru = nn.GRU(input_size=features, hidden_size=rnn_size, bias=True)\n        self.linear = nn.Linear(rnn_size, classes, bias=True)\n\n    def forward(self, x):\n        h1p, state = self.gru(x)  # RNN\n        h1, lens = pad_packed_sequence(h1p)  # unpack\n        max_len, batch_size, features = h1.size()  # get sizes for reshape\n        h2 = h1.view(max_len * batch_size, -1)  # reshape\n        h3 = self.linear(h2)  # linear transform\n        h4 = h3.view(max_len, batch_size, -1)  # reshape\n        h5 = h4[-1, :, :]  # slice last element\n        return h5\n\n\n# Network 2: fast use of packed sequences\nclass fast_packed(nn.Module):\n    def __init__(self, features, rnn_size):\n        super(fast_packed, self).__init__()\n        self.gru = nn.GRU(input_size=features, hidden_size=rnn_size, bias=True)\n        self.linear = nn.Linear(rnn_size, classes, bias=True)\n\n    def forward(self, x):\n        h1p, state = self.gru(x)  # RNN\n        h3 = self.linear(h1p.data)  # linear transform\n        h3p = PackedSequence(h3, h1p.batch_sizes)  # create PackedSequence (pytorch docs: Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence().\n        h4, lens = pad_packed_sequence(h3p)  # unpack\n        h5 = h4[-1, :, :]  # slice last element\n        return h5\n\n\n########################################################\n# Switch between fast and slow variant by commenting\nnet = slow_packed(features=features, rnn_size=rnn_size)\n# net = fast_packed(features=features, rnn_size=rnn_size)\n########################################################\nnet.cuda()  # move network to GPU\nprint(net)\n\n# Print parameter count\nparams = 0\nfor param in list(net.parameters()):\n    sizes = 1\n    for el in param.size():\n        sizes = sizes * el\n    params += sizes\nprint('::: # network parameters: ' + str(params))\n\n# Create optimizer, criterion\noptimizer = optim.Adam(net.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()  # loss definition\n\n# Start training\ntime = []\nfor ep in range(epochs):\n\n    start = timer()\n    optimizer.zero_grad()\n    output = net(bX)\n    loss = criterion(output, bY.long())\n    loss.backward()\n    optimizer.step()\n\n    torch.cuda.synchronize()  # Synchronize for more precise timing\n    end = timer()\n    time.append(end - start)\n\n    if ep % 5 == 0:\n        print('::: Checksum of final layer output from epoch {}:{}'.format(ep, torch.sum(output).cpu().data.numpy()[0]))\n\nprint('>>> Median runtime per epoch [sec] {}'.format(np.median(time)))", "body": "@apaszke Here is a script that provides two ways of using packed sequences: one way is fast, the other way is around 4 times slower. Both yield the same checksums at the final layer output. I am not able to explain why one is faster, could you please have a look at it? I also ran the code with nvprof and attached the output.\r\n\r\nOutput of fast variant\r\n[nvprof_fast](https://github.com/pytorch/pytorch/files/1087245/fast_packed.txt)\r\n```\r\nfast_packed (\r\n  (gru): GRU(123, 320)\r\n  (linear): Linear (320 -> 10)\r\n)\r\n::: # network parameters: 430410\r\n::: Checksum of final layer output from epoch 0:8.07597064972\r\n::: Checksum of final layer output from epoch 5:6.69699001312\r\n::: Checksum of final layer output from epoch 10:1.61638641357\r\n::: Checksum of final layer output from epoch 15:-6.10106754303\r\n::: Checksum of final layer output from epoch 20:-11.1396369934\r\n>>> Median runtime per epoch [sec] 0.222762107849\r\n```\r\nOutput of slow variant\r\n[nvprof_slow](https://github.com/pytorch/pytorch/files/1087241/slow_packed.txt)\r\n```\r\nslow_packed (\r\n  (gru): GRU(123, 320)\r\n  (linear): Linear (320 -> 10)\r\n)\r\n::: # network parameters: 430410\r\n::: Checksum of final layer output from epoch 0:8.07597064972\r\n::: Checksum of final layer output from epoch 5:6.69699001312\r\n::: Checksum of final layer output from epoch 10:1.61638641357\r\n::: Checksum of final layer output from epoch 15:-6.10106754303\r\n::: Checksum of final layer output from epoch 20:-11.1396369934\r\n>>> Median runtime per epoch [sec] 0.930104017258\r\n```\r\nScript\r\n```\r\nfrom timeit import default_timer as timer\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\r\n\r\n# Set seeds\r\nnp.random.seed(11)\r\ntorch.manual_seed(11)\r\ntorch.cuda.manual_seed(11)\r\n\r\n\r\n# Provide default parameters\r\ndef default_params():\r\n    rnn_size = 320\r\n    learning_rate = 1e-3\r\n    epochs = 25\r\n    return rnn_size, learning_rate, epochs\r\n\r\n\r\n# Provide a toy batch\r\ndef toy_batch(seed=11, shape=(25, 1000, 123), classes=10):\r\n    batch_size, max_len, features = shape\r\n    np.random.seed(seed)\r\n\r\n    # Samples\r\n    bX = np.float32(np.random.uniform(-1, 1, (shape)))\r\n    b_lenX = np.int32(np.linspace(max_len, max_len, batch_size))\r\n    # print('::: Lengths of samples in batch: {}'.format(b_lenX))\r\n\r\n    # Targets\r\n    bY = np.int32(np.random.randint(low=0, high=classes - 1, size=batch_size))\r\n\r\n    return bX, b_lenX, bY, classes\r\n\r\n\r\n# Get data\r\nbX, b_lenX, bY, classes = toy_batch()\r\nbatch_size, max_len, features = bX.shape\r\nrnn_size, learning_rate, epochs = default_params()\r\n\r\n# PyTorch compatibility: time first, batch second\r\nbX = np.transpose(bX, (1, 0, 2))\r\n\r\n# Create symbolic vars\r\nbX = Variable(torch.from_numpy(bX).cuda())\r\nbX = pack_padded_sequence(bX, b_lenX[::-1])  # Pack those sequences for masking, plz\r\n\r\nbY = Variable(torch.from_numpy(bY).cuda())\r\n\r\n\r\n# Network 1: slow use of packed sequences\r\nclass slow_packed(nn.Module):\r\n    def __init__(self, features, rnn_size):\r\n        super(slow_packed, self).__init__()\r\n        self.gru = nn.GRU(input_size=features, hidden_size=rnn_size, bias=True)\r\n        self.linear = nn.Linear(rnn_size, classes, bias=True)\r\n\r\n    def forward(self, x):\r\n        h1p, state = self.gru(x)  # RNN\r\n        h1, lens = pad_packed_sequence(h1p)  # unpack\r\n        max_len, batch_size, features = h1.size()  # get sizes for reshape\r\n        h2 = h1.view(max_len * batch_size, -1)  # reshape\r\n        h3 = self.linear(h2)  # linear transform\r\n        h4 = h3.view(max_len, batch_size, -1)  # reshape\r\n        h5 = h4[-1, :, :]  # slice last element\r\n        return h5\r\n\r\n\r\n# Network 2: fast use of packed sequences\r\nclass fast_packed(nn.Module):\r\n    def __init__(self, features, rnn_size):\r\n        super(fast_packed, self).__init__()\r\n        self.gru = nn.GRU(input_size=features, hidden_size=rnn_size, bias=True)\r\n        self.linear = nn.Linear(rnn_size, classes, bias=True)\r\n\r\n    def forward(self, x):\r\n        h1p, state = self.gru(x)  # RNN\r\n        h3 = self.linear(h1p.data)  # linear transform\r\n        h3p = PackedSequence(h3, h1p.batch_sizes)  # create PackedSequence (pytorch docs: Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence().\r\n        h4, lens = pad_packed_sequence(h3p)  # unpack\r\n        h5 = h4[-1, :, :]  # slice last element\r\n        return h5\r\n\r\n\r\n########################################################\r\n# Switch between fast and slow variant by commenting\r\nnet = slow_packed(features=features, rnn_size=rnn_size)\r\n# net = fast_packed(features=features, rnn_size=rnn_size)\r\n########################################################\r\nnet.cuda()  # move network to GPU\r\nprint(net)\r\n\r\n# Print parameter count\r\nparams = 0\r\nfor param in list(net.parameters()):\r\n    sizes = 1\r\n    for el in param.size():\r\n        sizes = sizes * el\r\n    params += sizes\r\nprint('::: # network parameters: ' + str(params))\r\n\r\n# Create optimizer, criterion\r\noptimizer = optim.Adam(net.parameters(), lr=learning_rate)\r\ncriterion = nn.CrossEntropyLoss()  # loss definition\r\n\r\n# Start training\r\ntime = []\r\nfor ep in range(epochs):\r\n\r\n    start = timer()\r\n    optimizer.zero_grad()\r\n    output = net(bX)\r\n    loss = criterion(output, bY.long())\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n    torch.cuda.synchronize()  # Synchronize for more precise timing\r\n    end = timer()\r\n    time.append(end - start)\r\n\r\n    if ep % 5 == 0:\r\n        print('::: Checksum of final layer output from epoch {}:{}'.format(ep, torch.sum(output).cpu().data.numpy()[0]))\r\n\r\nprint('>>> Median runtime per epoch [sec] {}'.format(np.median(time)))\r\n```"}