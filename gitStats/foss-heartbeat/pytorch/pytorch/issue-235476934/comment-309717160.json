{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/309717160", "html_url": "https://github.com/pytorch/pytorch/issues/1788#issuecomment-309717160", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1788", "id": 309717160, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTcxNzE2MA==", "user": {"login": "stefbraun", "id": 13469638, "node_id": "MDQ6VXNlcjEzNDY5NjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/13469638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefbraun", "html_url": "https://github.com/stefbraun", "followers_url": "https://api.github.com/users/stefbraun/followers", "following_url": "https://api.github.com/users/stefbraun/following{/other_user}", "gists_url": "https://api.github.com/users/stefbraun/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefbraun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefbraun/subscriptions", "organizations_url": "https://api.github.com/users/stefbraun/orgs", "repos_url": "https://api.github.com/users/stefbraun/repos", "events_url": "https://api.github.com/users/stefbraun/events{/privacy}", "received_events_url": "https://api.github.com/users/stefbraun/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-20T10:51:46Z", "updated_at": "2017-06-20T10:57:06Z", "author_association": "NONE", "body_html": "<p>thanks for your answer, but why do the networks give the same output? Perhaps some clarification is needed:</p>\n<ul>\n<li>every sample has the same sequence length here, in order to be able to compare the checksums of the final layers output. This makes slicing by -1 ( at max length = 1000) feasible.</li>\n<li>Both networks should do the same thing, as the checksum of the final layer is the same. It is common to put a dense layer on top of the RNN in speech recognition. This dense layer maps the output of each time step to classes in the case of CTC.</li>\n<li>As the dense layer operates on each time step, it needs an input of <code>(max_len*batch_size, rnn_size)</code>. I can either do that by viewing the unpacked RNN output (<code>h2</code> from the slow example) OR I can directly use the data from the packed sequence (<code>h1p.data</code> from the fast example).</li>\n<li>I commented to this issue because the fast way of using packed sequences comes with almost no speed penalty wrt not using packed sequences at all. If you wish, I can open a new issue.</li>\n</ul>", "body_text": "thanks for your answer, but why do the networks give the same output? Perhaps some clarification is needed:\n\nevery sample has the same sequence length here, in order to be able to compare the checksums of the final layers output. This makes slicing by -1 ( at max length = 1000) feasible.\nBoth networks should do the same thing, as the checksum of the final layer is the same. It is common to put a dense layer on top of the RNN in speech recognition. This dense layer maps the output of each time step to classes in the case of CTC.\nAs the dense layer operates on each time step, it needs an input of (max_len*batch_size, rnn_size). I can either do that by viewing the unpacked RNN output (h2 from the slow example) OR I can directly use the data from the packed sequence (h1p.data from the fast example).\nI commented to this issue because the fast way of using packed sequences comes with almost no speed penalty wrt not using packed sequences at all. If you wish, I can open a new issue.", "body": "thanks for your answer, but why do the networks give the same output? Perhaps some clarification is needed:\r\n- every sample has the same sequence length here, in order to be able to compare the checksums of the final layers output. This makes slicing by -1 ( at max length = 1000) feasible.\r\n- Both networks should do the same thing, as the checksum of the final layer is the same. It is common to put a dense layer on top of the RNN in speech recognition. This dense layer maps the output of each time step to classes in the case of CTC.\r\n- As the dense layer operates on each time step, it needs an input of ```(max_len*batch_size, rnn_size)```. I can either do that by viewing the unpacked RNN output (```h2``` from the slow example) OR I can directly use the data from the packed sequence (```h1p.data``` from the fast example).\r\n- I commented to this issue because the fast way of using packed sequences comes with almost no speed penalty wrt not using packed sequences at all. If you wish, I can open a new issue."}