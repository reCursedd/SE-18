{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/325826495", "html_url": "https://github.com/pytorch/pytorch/issues/1788#issuecomment-325826495", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1788", "id": 325826495, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNTgyNjQ5NQ==", "user": {"login": "elanmart", "id": 10772830, "node_id": "MDQ6VXNlcjEwNzcyODMw", "avatar_url": "https://avatars3.githubusercontent.com/u/10772830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elanmart", "html_url": "https://github.com/elanmart", "followers_url": "https://api.github.com/users/elanmart/followers", "following_url": "https://api.github.com/users/elanmart/following{/other_user}", "gists_url": "https://api.github.com/users/elanmart/gists{/gist_id}", "starred_url": "https://api.github.com/users/elanmart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elanmart/subscriptions", "organizations_url": "https://api.github.com/users/elanmart/orgs", "repos_url": "https://api.github.com/users/elanmart/repos", "events_url": "https://api.github.com/users/elanmart/events{/privacy}", "received_events_url": "https://api.github.com/users/elanmart/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-29T22:48:32Z", "updated_at": "2017-08-29T22:48:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> , apologies for being terrible at communication!</p>\n<p>We've decided to go with a different model after all, and I did not have time to further investigate the issue.</p>\n<p>But I wanted to follow-up on this on my own.</p>\n<p>Initially I thought the problem is due to the packing / unpacking being done in Python,<br>\nbut from some tests I did today it seems like it's not the case.</p>\n<p>For an epoch of training, packing &amp; unpacking takes <code>~3s</code>, and running LSTM <code>~10s</code>. But it seems like the biggest penalty is due to autograd -- with packing, calling <code>backward</code> takes <code>~50s</code>, while without it's around <code>~18</code>.</p>\n<p>I've tested this with sequences of equal length, and for ones sampled randomly from <code>(1, 128)</code>, and the results are similar.</p>\n<p>I'll try to follow-up with more details once I get the time to benchmark this with a better tool than <code>time.perf_counter()</code>. I'll post the script and more results when I double-check the script.</p>", "body_text": "Hi @apaszke , apologies for being terrible at communication!\nWe've decided to go with a different model after all, and I did not have time to further investigate the issue.\nBut I wanted to follow-up on this on my own.\nInitially I thought the problem is due to the packing / unpacking being done in Python,\nbut from some tests I did today it seems like it's not the case.\nFor an epoch of training, packing & unpacking takes ~3s, and running LSTM ~10s. But it seems like the biggest penalty is due to autograd -- with packing, calling backward takes ~50s, while without it's around ~18.\nI've tested this with sequences of equal length, and for ones sampled randomly from (1, 128), and the results are similar.\nI'll try to follow-up with more details once I get the time to benchmark this with a better tool than time.perf_counter(). I'll post the script and more results when I double-check the script.", "body": "Hi @apaszke , apologies for being terrible at communication!\r\n\r\nWe've decided to go with a different model after all, and I did not have time to further investigate the issue. \r\n\r\nBut I wanted to follow-up on this on my own. \r\n\r\nInitially I thought the problem is due to the packing / unpacking being done in Python,\r\nbut from some tests I did today it seems like it's not the case. \r\n\r\nFor an epoch of training, packing & unpacking takes `~3s`, and running LSTM `~10s`. But it seems like the biggest penalty is due to autograd -- with packing, calling `backward` takes `~50s`, while without it's around `~18`. \r\n\r\nI've tested this with sequences of equal length, and for ones sampled randomly from `(1, 128)`, and the results are similar.\r\n\r\nI'll try to follow-up with more details once I get the time to benchmark this with a better tool than `time.perf_counter()`. I'll post the script and more results when I double-check the script."}