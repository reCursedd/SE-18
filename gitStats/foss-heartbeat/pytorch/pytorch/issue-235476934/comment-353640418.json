{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/353640418", "html_url": "https://github.com/pytorch/pytorch/issues/1788#issuecomment-353640418", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1788", "id": 353640418, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzY0MDQxOA==", "user": {"login": "elanmart", "id": 10772830, "node_id": "MDQ6VXNlcjEwNzcyODMw", "avatar_url": "https://avatars3.githubusercontent.com/u/10772830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elanmart", "html_url": "https://github.com/elanmart", "followers_url": "https://api.github.com/users/elanmart/followers", "following_url": "https://api.github.com/users/elanmart/following{/other_user}", "gists_url": "https://api.github.com/users/elanmart/gists{/gist_id}", "starred_url": "https://api.github.com/users/elanmart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elanmart/subscriptions", "organizations_url": "https://api.github.com/users/elanmart/orgs", "repos_url": "https://api.github.com/users/elanmart/repos", "events_url": "https://api.github.com/users/elanmart/events{/privacy}", "received_events_url": "https://api.github.com/users/elanmart/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-22T17:18:10Z", "updated_at": "2017-12-22T17:18:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Wouldn't the backward for <code>pack_padded_sequence</code> be as simple the following? :</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_data</span>, <span class=\"pl-smi\">grad_batch_sizes</span>):\n\n    batch_sizes, <span class=\"pl-k\">=</span> ctx.saved_variables\n    input_size   <span class=\"pl-k\">=</span> ctx.input_size\n    \n    ret <span class=\"pl-k\">=</span> grad_steps.new(<span class=\"pl-k\">*</span>input_size).zero_()\n    idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">for</span> i, val <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(batch_sizes.data):\n        val <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(val)\n\n        ret[i, :val, <span class=\"pl-c1\">...</span>] <span class=\"pl-k\">=</span> grad_data[idx:idx<span class=\"pl-k\">+</span>item, <span class=\"pl-c1\">...</span>]\n        idx <span class=\"pl-k\">+=</span> val\n\n    <span class=\"pl-k\">return</span> ret, <span class=\"pl-c1\">None</span></pre></div>\n<p>I've needed the <code>packing</code> today again, and simply providing this <code>backward</code> reduces <code>CPU</code> backward from <code>600ms</code> to <code>14ms</code>, and it seems to return the same gradients as calling current implementation.</p>\n<p>Could I open a PR with this? Also, should this code go to <code>python</code> sources or to <code>ATen</code>? I've implemented it in <code>ATen</code> also but didn't find it to give any speedups (for the simple benchmarks I run)</p>", "body_text": "Wouldn't the backward for pack_padded_sequence be as simple the following? :\n@staticmethod\ndef backward(ctx, grad_data, grad_batch_sizes):\n\n    batch_sizes, = ctx.saved_variables\n    input_size   = ctx.input_size\n    \n    ret = grad_steps.new(*input_size).zero_()\n    idx = 0\n\n    for i, val in enumerate(batch_sizes.data):\n        val = int(val)\n\n        ret[i, :val, ...] = grad_data[idx:idx+item, ...]\n        idx += val\n\n    return ret, None\nI've needed the packing today again, and simply providing this backward reduces CPU backward from 600ms to 14ms, and it seems to return the same gradients as calling current implementation.\nCould I open a PR with this? Also, should this code go to python sources or to ATen? I've implemented it in ATen also but didn't find it to give any speedups (for the simple benchmarks I run)", "body": "Wouldn't the backward for `pack_padded_sequence` be as simple the following? :\r\n```python\r\n@staticmethod\r\ndef backward(ctx, grad_data, grad_batch_sizes):\r\n\r\n    batch_sizes, = ctx.saved_variables\r\n    input_size   = ctx.input_size\r\n    \r\n    ret = grad_steps.new(*input_size).zero_()\r\n    idx = 0\r\n\r\n    for i, val in enumerate(batch_sizes.data):\r\n        val = int(val)\r\n\r\n        ret[i, :val, ...] = grad_data[idx:idx+item, ...]\r\n        idx += val\r\n\r\n    return ret, None\r\n```\r\n\r\nI've needed the `packing` today again, and simply providing this `backward` reduces `CPU` backward from `600ms` to `14ms`, and it seems to return the same gradients as calling current implementation.\r\n\r\nCould I open a PR with this? Also, should this code go to `python` sources or to `ATen`? I've implemented it in `ATen` also but didn't find it to give any speedups (for the simple benchmarks I run)"}