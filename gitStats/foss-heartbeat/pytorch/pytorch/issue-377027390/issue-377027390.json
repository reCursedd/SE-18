{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13540", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13540/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13540/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13540/events", "html_url": "https://github.com/pytorch/pytorch/issues/13540", "id": 377027390, "node_id": "MDU6SXNzdWUzNzcwMjczOTA=", "number": 13540, "title": "torch.autograd.gradcheck doesn't work for a function with one argument", "user": {"login": "hayatoikoma", "id": 2889812, "node_id": "MDQ6VXNlcjI4ODk4MTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2889812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hayatoikoma", "html_url": "https://github.com/hayatoikoma", "followers_url": "https://api.github.com/users/hayatoikoma/followers", "following_url": "https://api.github.com/users/hayatoikoma/following{/other_user}", "gists_url": "https://api.github.com/users/hayatoikoma/gists{/gist_id}", "starred_url": "https://api.github.com/users/hayatoikoma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hayatoikoma/subscriptions", "organizations_url": "https://api.github.com/users/hayatoikoma/orgs", "repos_url": "https://api.github.com/users/hayatoikoma/repos", "events_url": "https://api.github.com/users/hayatoikoma/events{/privacy}", "received_events_url": "https://api.github.com/users/hayatoikoma/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-03T06:08:44Z", "updated_at": "2018-11-05T04:29:22Z", "closed_at": "2018-11-05T04:29:22Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p><code>torch.autograd.gradcheck</code> doesn't work for a function with one argument.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Function, gradcheck\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Sum1</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>):\n        ctx.save_for_backward(x)\n        <span class=\"pl-k\">return</span> x.sum()\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):\n        grad_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n        x <span class=\"pl-k\">=</span> ctx.saved_tensors\n\n        <span class=\"pl-k\">if</span> ctx.needs_input_grad[<span class=\"pl-c1\">0</span>]:\n            grad_x <span class=\"pl-k\">=</span> torch.ones_like(x) <span class=\"pl-k\">*</span> grad_output\n\n        <span class=\"pl-k\">return</span> grad_x\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Sum2</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>):\n        ctx.save_for_backward(x)\n        <span class=\"pl-k\">return</span> x.sum()\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):\n        grad_x <span class=\"pl-k\">=</span> grad_y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n        x <span class=\"pl-k\">=</span> ctx.saved_tensors\n\n        <span class=\"pl-k\">if</span> ctx.needs_input_grad[<span class=\"pl-c1\">0</span>]:\n            grad_x <span class=\"pl-k\">=</span> torch.ones_like(x) <span class=\"pl-k\">*</span> grad_output\n\n        <span class=\"pl-k\">return</span> grad_x, grad_y\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Sum3</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>):\n        ctx.save_for_backward(x)\n        <span class=\"pl-k\">return</span> x.sum()\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_output</span>):\n        grad_x <span class=\"pl-k\">=</span> grad_y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n        x <span class=\"pl-k\">=</span> ctx.saved_tensors[<span class=\"pl-c1\">0</span>]\n\n        <span class=\"pl-k\">if</span> ctx.needs_input_grad[<span class=\"pl-c1\">0</span>]:\n            grad_x <span class=\"pl-k\">=</span> torch.ones_like(x) <span class=\"pl-k\">*</span> grad_output\n\n        <span class=\"pl-k\">return</span> grad_x, grad_y\n\nsum1 <span class=\"pl-k\">=</span> Sum1.apply\nsum2 <span class=\"pl-k\">=</span> Sum2.apply\nsum3 <span class=\"pl-k\">=</span> Sum3.apply\n\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.double, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nb <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.double)\n\ngradcheck(sum3, (a,b))\ngradcheck(sum2, (a,b))\ngradcheck(sum1, a)\ngradcheck(sum1, (a))</pre></div>\n<h2>Expected behavior</h2>\n<ol>\n<li><code>gradcheck(sum3, (a,b))</code> works.</li>\n<li><code>gradcheck(sum2, (a,b))</code> fails because <code>ctx.saved_tensors</code> is a tuple, but I am not sure if it is an intended behavior.</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>miniconda3<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">-</span>nightly<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>autograd<span class=\"pl-k\">/</span>gradcheck.py <span class=\"pl-k\">in</span> gradcheck(func, inputs, eps, atol, rtol, raise_exception)\n    <span class=\"pl-c1\">193</span>             <span class=\"pl-k\">return</span> _as_tuple(func(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>))[i]\n    <span class=\"pl-c1\">194</span>\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">195</span>         analytical, reentrant, correct_grad_sizes <span class=\"pl-k\">=</span> get_analytical_jacobian(tupled_inputs, o)\n    <span class=\"pl-c1\">196</span>         numerical <span class=\"pl-k\">=</span> get_numerical_jacobian(fn, inputs, <span class=\"pl-v\">eps</span><span class=\"pl-k\">=</span>eps)\n    <span class=\"pl-c1\">197</span>\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>miniconda3<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">-</span>nightly<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>autograd<span class=\"pl-k\">/</span>gradcheck.py <span class=\"pl-k\">in</span> get_analytical_jacobian(<span class=\"pl-c1\">input</span>, output)\n     <span class=\"pl-c1\">94</span>         <span class=\"pl-k\">for</span> jacobian_c <span class=\"pl-k\">in</span> (jacobian, jacobian_reentrant):\n     <span class=\"pl-c1\">95</span>             grads_input <span class=\"pl-k\">=</span> torch.autograd.grad(output, diff_input_list, grad_output,\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">96</span>                                               <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">allow_unused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n     <span class=\"pl-c1\">97</span>             <span class=\"pl-k\">for</span> jacobian_x, d_x, x <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(jacobian_c, grads_input, diff_input_list):\n     <span class=\"pl-c1\">98</span>                 <span class=\"pl-k\">if</span> d_x <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> d_x.size() <span class=\"pl-k\">!=</span> x.size():\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>miniconda3<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">-</span>nightly<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>autograd<span class=\"pl-k\">/</span><span class=\"pl-c1\">__init__</span>.py <span class=\"pl-k\">in</span> grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\n    <span class=\"pl-c1\">143</span>     <span class=\"pl-k\">return</span> Variable._execution_engine.run_backward(\n    <span class=\"pl-c1\">144</span>         outputs, grad_outputs, retain_graph, create_graph,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">145</span>         inputs, allow_unused)\n    <span class=\"pl-c1\">146</span>\n    <span class=\"pl-c1\">147</span>\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>miniconda3<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">-</span>nightly<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>autograd<span class=\"pl-k\">/</span>function.py <span class=\"pl-k\">in</span> apply(<span class=\"pl-c1\">self</span>, <span class=\"pl-k\">*</span>args)\n     <span class=\"pl-c1\">74</span>\n     <span class=\"pl-c1\">75</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">apply</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">76</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._forward_cls.backward(<span class=\"pl-c1\">self</span>, <span class=\"pl-k\">*</span>args)\n     <span class=\"pl-c1\">77</span>\n     <span class=\"pl-c1\">78</span>\n\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">1f2fe1d5eb24</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> backward(ctx, grad_output)\n     <span class=\"pl-c1\">32</span>\n     <span class=\"pl-c1\">33</span>         <span class=\"pl-k\">if</span> ctx.needs_input_grad[<span class=\"pl-c1\">0</span>]:\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">34</span>             grad_x <span class=\"pl-k\">=</span> torch.ones_like(x) <span class=\"pl-k\">*</span> grad_output\n     <span class=\"pl-c1\">35</span>\n     <span class=\"pl-c1\">36</span>         <span class=\"pl-k\">return</span> grad_x, grad_y\n\n<span class=\"pl-c1\">TypeError</span>: ones_like() received an invalid combination of arguments <span class=\"pl-k\">-</span> got (<span class=\"pl-c1\">tuple</span>), but expected one of:\n <span class=\"pl-k\">*</span> (Tensor <span class=\"pl-c1\">input</span>, torch.dtype dtype, torch.layout layout, torch.device device, <span class=\"pl-c1\">bool</span> requires_grad)\n <span class=\"pl-k\">*</span> (Tensor <span class=\"pl-c1\">input</span>, <span class=\"pl-c1\">bool</span> requires_grad)</pre></div>\n<ol>\n<li><code>gradcheck(sum1, a)</code> and <code>gradcheck(sum1, (a))</code> fail because <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/autograd/gradcheck.py#L181\"><code>func(*inputs)</code></a> is expanded over the zeroth dimension of the tensor.</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>miniconda3<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">-</span>nightly<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>autograd<span class=\"pl-k\">/</span>gradcheck.py <span class=\"pl-k\">in</span> gradcheck(func, inputs, eps, atol, rtol, raise_exception)\n    <span class=\"pl-c1\">179</span>             <span class=\"pl-s\"><span class=\"pl-pds\">'</span>but none of the them have requires_grad=True.<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">180</span>\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">181</span>     output <span class=\"pl-k\">=</span> _differentiable_outputs(func(<span class=\"pl-k\">*</span>inputs))\n    <span class=\"pl-c1\">182</span>\n    <span class=\"pl-c1\">183</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">fail_test</span>(<span class=\"pl-smi\">msg</span>):\n\n<span class=\"pl-c1\">TypeError</span>: forward() takes <span class=\"pl-c1\">2</span> positional arguments but <span class=\"pl-c1\">11</span> were given</pre></div>\n\n\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version:  1.0.0.dev20181102</li>\n<li>OS: Ubuntu 16.04.5 LTS</li>\n<li>GCC version: (GCC) 4.8.5</li>\n<li>CMake version: version 3.11.1</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): <code>pip torch_nightly</code></li>\n<li>Python version: <code>3.6.6</code></li>\n</ul>", "body_text": "\ud83d\udc1b Bug\n\ntorch.autograd.gradcheck doesn't work for a function with one argument.\nTo Reproduce\nSteps to reproduce the behavior:\nimport torch\nfrom torch.autograd import Function, gradcheck\n\nclass Sum1(Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x.sum()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_x = None\n\n        x = ctx.saved_tensors\n\n        if ctx.needs_input_grad[0]:\n            grad_x = torch.ones_like(x) * grad_output\n\n        return grad_x\n\nclass Sum2(Function):\n    @staticmethod\n    def forward(ctx, x, y):\n        ctx.save_for_backward(x)\n        return x.sum()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_x = grad_y = None\n\n        x = ctx.saved_tensors\n\n        if ctx.needs_input_grad[0]:\n            grad_x = torch.ones_like(x) * grad_output\n\n        return grad_x, grad_y\n\nclass Sum3(Function):\n    @staticmethod\n    def forward(ctx, x, y):\n        ctx.save_for_backward(x)\n        return x.sum()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_x = grad_y = None\n\n        x = ctx.saved_tensors[0]\n\n        if ctx.needs_input_grad[0]:\n            grad_x = torch.ones_like(x) * grad_output\n\n        return grad_x, grad_y\n\nsum1 = Sum1.apply\nsum2 = Sum2.apply\nsum3 = Sum3.apply\n\na = torch.rand(10, dtype=torch.double, requires_grad=True)\nb = torch.rand(10, dtype=torch.double)\n\ngradcheck(sum3, (a,b))\ngradcheck(sum2, (a,b))\ngradcheck(sum1, a)\ngradcheck(sum1, (a))\nExpected behavior\n\ngradcheck(sum3, (a,b)) works.\ngradcheck(sum2, (a,b)) fails because ctx.saved_tensors is a tuple, but I am not sure if it is an intended behavior.\n\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/gradcheck.py in gradcheck(func, inputs, eps, atol, rtol, raise_exception)\n    193             return _as_tuple(func(*input))[i]\n    194\n--> 195         analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(tupled_inputs, o)\n    196         numerical = get_numerical_jacobian(fn, inputs, eps=eps)\n    197\n\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/gradcheck.py in get_analytical_jacobian(input, output)\n     94         for jacobian_c in (jacobian, jacobian_reentrant):\n     95             grads_input = torch.autograd.grad(output, diff_input_list, grad_output,\n---> 96                                               retain_graph=True, allow_unused=True)\n     97             for jacobian_x, d_x, x in zip(jacobian_c, grads_input, diff_input_list):\n     98                 if d_x is not None and d_x.size() != x.size():\n\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/__init__.py in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\n    143     return Variable._execution_engine.run_backward(\n    144         outputs, grad_outputs, retain_graph, create_graph,\n--> 145         inputs, allow_unused)\n    146\n    147\n\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/function.py in apply(self, *args)\n     74\n     75     def apply(self, *args):\n---> 76         return self._forward_cls.backward(self, *args)\n     77\n     78\n\n<ipython-input-1-1f2fe1d5eb24> in backward(ctx, grad_output)\n     32\n     33         if ctx.needs_input_grad[0]:\n---> 34             grad_x = torch.ones_like(x) * grad_output\n     35\n     36         return grad_x, grad_y\n\nTypeError: ones_like() received an invalid combination of arguments - got (tuple), but expected one of:\n * (Tensor input, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n * (Tensor input, bool requires_grad)\n\ngradcheck(sum1, a) and gradcheck(sum1, (a)) fail because func(*inputs) is expanded over the zeroth dimension of the tensor.\n\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/gradcheck.py in gradcheck(func, inputs, eps, atol, rtol, raise_exception)\n    179             'but none of the them have requires_grad=True.')\n    180\n--> 181     output = _differentiable_outputs(func(*inputs))\n    182\n    183     def fail_test(msg):\n\nTypeError: forward() takes 2 positional arguments but 11 were given\n\n\nEnvironment\n\nPyTorch Version:  1.0.0.dev20181102\nOS: Ubuntu 16.04.5 LTS\nGCC version: (GCC) 4.8.5\nCMake version: version 3.11.1\nHow you installed PyTorch (conda, pip, source): pip torch_nightly\nPython version: 3.6.6", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n`torch.autograd.gradcheck` doesn't work for a function with one argument.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```python\r\nimport torch\r\nfrom torch.autograd import Function, gradcheck\r\n\r\nclass Sum1(Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        ctx.save_for_backward(x)\r\n        return x.sum()\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        grad_x = None\r\n\r\n        x = ctx.saved_tensors\r\n\r\n        if ctx.needs_input_grad[0]:\r\n            grad_x = torch.ones_like(x) * grad_output\r\n\r\n        return grad_x\r\n\r\nclass Sum2(Function):\r\n    @staticmethod\r\n    def forward(ctx, x, y):\r\n        ctx.save_for_backward(x)\r\n        return x.sum()\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        grad_x = grad_y = None\r\n\r\n        x = ctx.saved_tensors\r\n\r\n        if ctx.needs_input_grad[0]:\r\n            grad_x = torch.ones_like(x) * grad_output\r\n\r\n        return grad_x, grad_y\r\n\r\nclass Sum3(Function):\r\n    @staticmethod\r\n    def forward(ctx, x, y):\r\n        ctx.save_for_backward(x)\r\n        return x.sum()\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        grad_x = grad_y = None\r\n\r\n        x = ctx.saved_tensors[0]\r\n\r\n        if ctx.needs_input_grad[0]:\r\n            grad_x = torch.ones_like(x) * grad_output\r\n\r\n        return grad_x, grad_y\r\n\r\nsum1 = Sum1.apply\r\nsum2 = Sum2.apply\r\nsum3 = Sum3.apply\r\n\r\na = torch.rand(10, dtype=torch.double, requires_grad=True)\r\nb = torch.rand(10, dtype=torch.double)\r\n\r\ngradcheck(sum3, (a,b))\r\ngradcheck(sum2, (a,b))\r\ngradcheck(sum1, a)\r\ngradcheck(sum1, (a))\r\n```\r\n\r\n## Expected behavior\r\n1. `gradcheck(sum3, (a,b))` works.\r\n1. `gradcheck(sum2, (a,b))` fails because `ctx.saved_tensors` is a tuple, but I am not sure if it is an intended behavior.\r\n```python\r\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/gradcheck.py in gradcheck(func, inputs, eps, atol, rtol, raise_exception)\r\n    193             return _as_tuple(func(*input))[i]\r\n    194\r\n--> 195         analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(tupled_inputs, o)\r\n    196         numerical = get_numerical_jacobian(fn, inputs, eps=eps)\r\n    197\r\n\r\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/gradcheck.py in get_analytical_jacobian(input, output)\r\n     94         for jacobian_c in (jacobian, jacobian_reentrant):\r\n     95             grads_input = torch.autograd.grad(output, diff_input_list, grad_output,\r\n---> 96                                               retain_graph=True, allow_unused=True)\r\n     97             for jacobian_x, d_x, x in zip(jacobian_c, grads_input, diff_input_list):\r\n     98                 if d_x is not None and d_x.size() != x.size():\r\n\r\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/__init__.py in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\r\n    143     return Variable._execution_engine.run_backward(\r\n    144         outputs, grad_outputs, retain_graph, create_graph,\r\n--> 145         inputs, allow_unused)\r\n    146\r\n    147\r\n\r\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/function.py in apply(self, *args)\r\n     74\r\n     75     def apply(self, *args):\r\n---> 76         return self._forward_cls.backward(self, *args)\r\n     77\r\n     78\r\n\r\n<ipython-input-1-1f2fe1d5eb24> in backward(ctx, grad_output)\r\n     32\r\n     33         if ctx.needs_input_grad[0]:\r\n---> 34             grad_x = torch.ones_like(x) * grad_output\r\n     35\r\n     36         return grad_x, grad_y\r\n\r\nTypeError: ones_like() received an invalid combination of arguments - got (tuple), but expected one of:\r\n * (Tensor input, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\r\n * (Tensor input, bool requires_grad)\r\n```\r\n1. `gradcheck(sum1, a)` and `gradcheck(sum1, (a))` fail because [`func(*inputs)`](https://github.com/pytorch/pytorch/blob/master/torch/autograd/gradcheck.py#L181) is expanded over the zeroth dimension of the tensor. \r\n```python\r\n~/miniconda3/envs/pytorch-nightly/lib/python3.6/site-packages/torch/autograd/gradcheck.py in gradcheck(func, inputs, eps, atol, rtol, raise_exception)\r\n    179             'but none of the them have requires_grad=True.')\r\n    180\r\n--> 181     output = _differentiable_outputs(func(*inputs))\r\n    182\r\n    183     def fail_test(msg):\r\n\r\nTypeError: forward() takes 2 positional arguments but 11 were given\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n - PyTorch Version:  1.0.0.dev20181102\r\n - OS: Ubuntu 16.04.5 LTS\r\n - GCC version: (GCC) 4.8.5\r\n - CMake version: version 3.11.1\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip torch_nightly`\r\n - Python version: `3.6.6`\r\n\r\n"}