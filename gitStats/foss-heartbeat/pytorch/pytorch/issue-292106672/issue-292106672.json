{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4893", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4893/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4893/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4893/events", "html_url": "https://github.com/pytorch/pytorch/issues/4893", "id": 292106672, "node_id": "MDU6SXNzdWUyOTIxMDY2NzI=", "number": 4893, "title": "GPU Softmax over last dimension of 3D tensor is slow", "user": {"login": "nikitakit", "id": 252225, "node_id": "MDQ6VXNlcjI1MjIyNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/252225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikitakit", "html_url": "https://github.com/nikitakit", "followers_url": "https://api.github.com/users/nikitakit/followers", "following_url": "https://api.github.com/users/nikitakit/following{/other_user}", "gists_url": "https://api.github.com/users/nikitakit/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikitakit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikitakit/subscriptions", "organizations_url": "https://api.github.com/users/nikitakit/orgs", "repos_url": "https://api.github.com/users/nikitakit/repos", "events_url": "https://api.github.com/users/nikitakit/events{/privacy}", "received_events_url": "https://api.github.com/users/nikitakit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-27T09:18:36Z", "updated_at": "2018-02-15T19:22:46Z", "closed_at": "2018-02-15T19:22:46Z", "author_association": "NONE", "body_html": "<p>Recently I profiled one of my models with <code>nvprof</code> and was surprised to find that the softmax layers for an attention mechanism were the most expensive entries in the <code>cuda_total_time</code> column.</p>\n<p>My original code looked something like this:</p>\n<pre><code>self.softmax = nn.Softmax(dim=-1)\n# ...\n# attn is a 3D tensor (batch_size x length x length). Length is in the 30-200 range\nattn = self.softmax(attn)\n</code></pre>\n<p>After some experimentation, I tried adding some transposes to the code:</p>\n<pre><code>self.softmax = nn.Softmax(dim=1)\nattn = self.softmax(attn.transpose(1, 2)).transpose(1, 2)\n</code></pre>\n<p>This increased my overall model speed by around 10%, and now matrix multiplication is at the top of <code>nvprof</code> (which is what I would expect).</p>\n<p>Could this be considered a performance bug? I wonder if there is some way for the softmax cuda code to have comparable speed regardless of the softmax dimension.</p>\n<p>(Sorry I don't have sample code at the moment; my actual code is deeply embedded in the current project I'm doing.)</p>\n<p>System info</p>\n<ul>\n<li>OS: Linux</li>\n<li>PyTorch version: 0.3.0</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: CUDA 9</li>\n<li>GPU models and configuration: K80</li>\n</ul>", "body_text": "Recently I profiled one of my models with nvprof and was surprised to find that the softmax layers for an attention mechanism were the most expensive entries in the cuda_total_time column.\nMy original code looked something like this:\nself.softmax = nn.Softmax(dim=-1)\n# ...\n# attn is a 3D tensor (batch_size x length x length). Length is in the 30-200 range\nattn = self.softmax(attn)\n\nAfter some experimentation, I tried adding some transposes to the code:\nself.softmax = nn.Softmax(dim=1)\nattn = self.softmax(attn.transpose(1, 2)).transpose(1, 2)\n\nThis increased my overall model speed by around 10%, and now matrix multiplication is at the top of nvprof (which is what I would expect).\nCould this be considered a performance bug? I wonder if there is some way for the softmax cuda code to have comparable speed regardless of the softmax dimension.\n(Sorry I don't have sample code at the moment; my actual code is deeply embedded in the current project I'm doing.)\nSystem info\n\nOS: Linux\nPyTorch version: 0.3.0\nHow you installed PyTorch (conda, pip, source): conda\nPython version: 3.6\nCUDA/cuDNN version: CUDA 9\nGPU models and configuration: K80", "body": "Recently I profiled one of my models with `nvprof` and was surprised to find that the softmax layers for an attention mechanism were the most expensive entries in the `cuda_total_time` column.\r\n\r\nMy original code looked something like this:\r\n```\r\nself.softmax = nn.Softmax(dim=-1)\r\n# ...\r\n# attn is a 3D tensor (batch_size x length x length). Length is in the 30-200 range\r\nattn = self.softmax(attn)\r\n```\r\n\r\nAfter some experimentation, I tried adding some transposes to the code:\r\n\r\n```\r\nself.softmax = nn.Softmax(dim=1)\r\nattn = self.softmax(attn.transpose(1, 2)).transpose(1, 2)\r\n```\r\n\r\nThis increased my overall model speed by around 10%, and now matrix multiplication is at the top of `nvprof` (which is what I would expect).\r\n\r\nCould this be considered a performance bug? I wonder if there is some way for the softmax cuda code to have comparable speed regardless of the softmax dimension.\r\n\r\n(Sorry I don't have sample code at the moment; my actual code is deeply embedded in the current project I'm doing.)\r\n\r\nSystem info\r\n- OS: Linux\r\n- PyTorch version: 0.3.0\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: CUDA 9\r\n- GPU models and configuration: K80"}