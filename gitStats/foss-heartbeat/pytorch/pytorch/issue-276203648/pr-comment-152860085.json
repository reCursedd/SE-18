{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152860085", "pull_request_review_id": 78787085, "id": 152860085, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mjg2MDA4NQ==", "diff_hunk": "@@ -426,58 +434,55 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned\n       }\n     }\n \n-    local_sum[threadIdx.y][threadIdx.x] =\n+    Accreal local_sum =\n         THCNumerics<Accreal>::mul(local_mean, ScalarConvert<int, Accreal>::to(count));\n     __syncthreads();\n \n-\n-    // Compute the true mean of each of the blockDim.x rows by reducing the sums\n-    for (unsigned s = 8; s > 1; s >>= 1) {\n-      if (row < num_rows && threadIdx.x < s) {\n-        local_sum[threadIdx.y][threadIdx.x] = THCNumerics<Accreal>::add(\n-            local_sum[threadIdx.y][threadIdx.x],\n-            local_sum[threadIdx.y][threadIdx.x + s]);\n-      }\n-      __syncthreads();\n+    /*\n+     * We are reducing across each row of 16 threads to find the true sum of the\n+     * entire input row.\n+     *\n+     * This is done in a warp shuffle: each warp has 2 rows of 16 threads for a\n+     * total of 32 threads. The warp will accumulate the sums of each row of \n+     * 16 threads in the first thread of the row (with threadIdx.x == 0)\n+     */\n+    for (unsigned s = 8; s >= 1; s >>= 1) {\n+      local_sum = THCNumerics<Accreal>::add(local_sum, \n+          WARP_SHFL_DOWN((row < num_rows) ? local_sum : acc_zero, s));\n     }\n-\n     if (row < num_rows && threadIdx.x == 0) {\n-      mean[threadIdx.y] = THCNumerics<Accreal>::div(\n-          THCNumerics<Accreal>::add(local_sum[threadIdx.y][0], local_sum[threadIdx.y][1]),\n-          ScalarConvert<int, Accreal>::to(row_size));\n+      // This is the true mean of the entire input row. There are 32 true means.\n+      mean[threadIdx.y] = THCNumerics<Accreal>::div(local_sum, ScalarConvert<int, Accreal>::to(row_size));", "path": "aten/src/THC/THCTensorMathReduce.cuh", "position": null, "original_position": 78, "commit_id": "3196e5ddaa826b9e19563999120c64aa6efb545e", "original_commit_id": "37bafd189fca4efae41f0a37000f5727ee9b02a3", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "If I understand correctly, `mean[threadIdx.y]` is computed by lane 0 and then broadcasted to all threads in the warp. Can this be `__shfl_sync` (`WARP_SHFL` in THC)?\r\n\r\nIt's probably not much faster than the shared-memory.", "created_at": "2017-11-23T18:51:53Z", "updated_at": "2018-11-23T15:36:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/3846#discussion_r152860085", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3846", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152860085"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3846#discussion_r152860085"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3846"}}, "body_html": "<p>If I understand correctly, <code>mean[threadIdx.y]</code> is computed by lane 0 and then broadcasted to all threads in the warp. Can this be <code>__shfl_sync</code> (<code>WARP_SHFL</code> in THC)?</p>\n<p>It's probably not much faster than the shared-memory.</p>", "body_text": "If I understand correctly, mean[threadIdx.y] is computed by lane 0 and then broadcasted to all threads in the warp. Can this be __shfl_sync (WARP_SHFL in THC)?\nIt's probably not much faster than the shared-memory."}