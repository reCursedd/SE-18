{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/347301880", "html_url": "https://github.com/pytorch/pytorch/pull/3846#issuecomment-347301880", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3846", "id": 347301880, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzMwMTg4MA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-27T19:39:39Z", "updated_at": "2017-11-27T19:39:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If all threads in warp need the result of reduction, reduction can be implemented using shfl_xor instead of shfl_down and shfl that you are using now, see section \"shuffle warp reduce\" here <a href=\"https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/\" rel=\"nofollow\">https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/</a>. But it's a purely theoretical exercise, performance likely won't be noticeably affected one way or another :-)</p>", "body_text": "If all threads in warp need the result of reduction, reduction can be implemented using shfl_xor instead of shfl_down and shfl that you are using now, see section \"shuffle warp reduce\" here https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/. But it's a purely theoretical exercise, performance likely won't be noticeably affected one way or another :-)", "body": "If all threads in warp need the result of reduction, reduction can be implemented using shfl_xor instead of shfl_down and shfl that you are using now, see section \"shuffle warp reduce\" here https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/. But it's a purely theoretical exercise, performance likely won't be noticeably affected one way or another :-)"}