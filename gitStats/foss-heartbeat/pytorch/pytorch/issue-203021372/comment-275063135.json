{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/275063135", "html_url": "https://github.com/pytorch/pytorch/issues/586#issuecomment-275063135", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/586", "id": 275063135, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTA2MzEzNQ==", "user": {"login": "tgeorgy", "id": 1074501, "node_id": "MDQ6VXNlcjEwNzQ1MDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1074501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgeorgy", "html_url": "https://github.com/tgeorgy", "followers_url": "https://api.github.com/users/tgeorgy/followers", "following_url": "https://api.github.com/users/tgeorgy/following{/other_user}", "gists_url": "https://api.github.com/users/tgeorgy/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgeorgy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgeorgy/subscriptions", "organizations_url": "https://api.github.com/users/tgeorgy/orgs", "repos_url": "https://api.github.com/users/tgeorgy/repos", "events_url": "https://api.github.com/users/tgeorgy/events{/privacy}", "received_events_url": "https://api.github.com/users/tgeorgy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-25T09:45:25Z", "updated_at": "2017-01-25T09:45:25Z", "author_association": "NONE", "body_html": "<p>Hi, thanks for the responses.</p>\n<p>What about another case when you run forward in evaluation mode and backward in training?<br>\nIt produces same issue, but seems to work fine in lua torch.<br>\nHere is the code with changes:</p>\n<div class=\"highlight highlight-source-python\"><pre>cudnn.enabled <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\nnet <span class=\"pl-k\">=</span> nn.BatchNorm1d(<span class=\"pl-c1\">3</span>).cuda()\n\nidat <span class=\"pl-k\">=</span> Variable(torch.rand(<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">3</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrad <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">3</span>).cuda()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 1st pass</span>\nnet.eval()\nres <span class=\"pl-k\">=</span> net(idat)\nnet.train()\nres.backward(grad)\ngrad0 <span class=\"pl-k\">=</span> idat.grad.data.cpu()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2nd pass</span>\nidat.grad.data.zero_()\n\nnet.eval()\nres <span class=\"pl-k\">=</span> net(idat)\nnet.train()\nres.backward(grad)\ngrad1 <span class=\"pl-k\">=</span> idat.grad.data.cpu()\n\n<span class=\"pl-k\">assert</span> (grad1 <span class=\"pl-k\">==</span> grad0).all(), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Grads have to be same<span class=\"pl-pds\">'</span></span></pre></div>\n<p>If there is another way to implement virtual batch normalization - please share.</p>", "body_text": "Hi, thanks for the responses.\nWhat about another case when you run forward in evaluation mode and backward in training?\nIt produces same issue, but seems to work fine in lua torch.\nHere is the code with changes:\ncudnn.enabled = True\n\nnet = nn.BatchNorm1d(3).cuda()\n\nidat = Variable(torch.rand(4,3).cuda(), requires_grad=True)\ngrad = torch.rand(4,3).cuda()\n\n# 1st pass\nnet.eval()\nres = net(idat)\nnet.train()\nres.backward(grad)\ngrad0 = idat.grad.data.cpu()\n\n# 2nd pass\nidat.grad.data.zero_()\n\nnet.eval()\nres = net(idat)\nnet.train()\nres.backward(grad)\ngrad1 = idat.grad.data.cpu()\n\nassert (grad1 == grad0).all(), 'Grads have to be same'\nIf there is another way to implement virtual batch normalization - please share.", "body": "Hi, thanks for the responses.\r\n\r\nWhat about another case when you run forward in evaluation mode and backward in training?\r\nIt produces same issue, but seems to work fine in lua torch.\r\nHere is the code with changes:\r\n```python\r\ncudnn.enabled = True\r\n\r\nnet = nn.BatchNorm1d(3).cuda()\r\n\r\nidat = Variable(torch.rand(4,3).cuda(), requires_grad=True)\r\ngrad = torch.rand(4,3).cuda()\r\n\r\n# 1st pass\r\nnet.eval()\r\nres = net(idat)\r\nnet.train()\r\nres.backward(grad)\r\ngrad0 = idat.grad.data.cpu()\r\n\r\n# 2nd pass\r\nidat.grad.data.zero_()\r\n\r\nnet.eval()\r\nres = net(idat)\r\nnet.train()\r\nres.backward(grad)\r\ngrad1 = idat.grad.data.cpu()\r\n\r\nassert (grad1 == grad0).all(), 'Grads have to be same'\r\n```\r\nIf there is another way to implement virtual batch normalization - please share."}