{"url": "https://api.github.com/repos/pytorch/pytorch/issues/586", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/586/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/586/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/586/events", "html_url": "https://github.com/pytorch/pytorch/issues/586", "id": 203021372, "node_id": "MDU6SXNzdWUyMDMwMjEzNzI=", "number": 586, "title": "CUDNN batchnorm backprop doesn't work properly in evaluation mode", "user": {"login": "tgeorgy", "id": 1074501, "node_id": "MDQ6VXNlcjEwNzQ1MDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1074501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgeorgy", "html_url": "https://github.com/tgeorgy", "followers_url": "https://api.github.com/users/tgeorgy/followers", "following_url": "https://api.github.com/users/tgeorgy/following{/other_user}", "gists_url": "https://api.github.com/users/tgeorgy/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgeorgy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgeorgy/subscriptions", "organizations_url": "https://api.github.com/users/tgeorgy/orgs", "repos_url": "https://api.github.com/users/tgeorgy/repos", "events_url": "https://api.github.com/users/tgeorgy/events{/privacy}", "received_events_url": "https://api.github.com/users/tgeorgy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-01-25T06:15:31Z", "updated_at": "2017-01-26T02:31:03Z", "closed_at": "2017-01-25T17:38:58Z", "author_association": "NONE", "body_html": "<p>Gradient changes, though inputs, outputs, and internal state stay same.</p>\n<div class=\"highlight highlight-source-python\"><pre>cudnn.enabled <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\nnet <span class=\"pl-k\">=</span> nn.BatchNorm1d(<span class=\"pl-c1\">3</span>).cuda()\nnet.eval()\n\nidat <span class=\"pl-k\">=</span> Variable(torch.rand(<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">3</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrad <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">3</span>).cuda()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 1st pass</span>\nres <span class=\"pl-k\">=</span> net(idat)\nres.backward(grad)\ngrad0 <span class=\"pl-k\">=</span> idat.grad.data.cpu()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2nd pass</span>\nidat.grad.data.zero_()\n\nres <span class=\"pl-k\">=</span> net(idat)\nres.backward(grad)\ngrad1 <span class=\"pl-k\">=</span> idat.grad.data.cpu()\n\n<span class=\"pl-c1\">print</span> (grad1 <span class=\"pl-k\">==</span> grad0).all()</pre></div>\n<p>Even when I run forward in evaluation mode and backward in training I still get the gradient changed.<br>\nIt works fine with cudnn disabled.</p>", "body_text": "Gradient changes, though inputs, outputs, and internal state stay same.\ncudnn.enabled = True\n\nnet = nn.BatchNorm1d(3).cuda()\nnet.eval()\n\nidat = Variable(torch.rand(4,3).cuda(), requires_grad=True)\ngrad = torch.rand(4,3).cuda()\n\n# 1st pass\nres = net(idat)\nres.backward(grad)\ngrad0 = idat.grad.data.cpu()\n\n# 2nd pass\nidat.grad.data.zero_()\n\nres = net(idat)\nres.backward(grad)\ngrad1 = idat.grad.data.cpu()\n\nprint (grad1 == grad0).all()\nEven when I run forward in evaluation mode and backward in training I still get the gradient changed.\nIt works fine with cudnn disabled.", "body": "Gradient changes, though inputs, outputs, and internal state stay same.\r\n```python\r\ncudnn.enabled = True\r\n\r\nnet = nn.BatchNorm1d(3).cuda()\r\nnet.eval()\r\n\r\nidat = Variable(torch.rand(4,3).cuda(), requires_grad=True)\r\ngrad = torch.rand(4,3).cuda()\r\n\r\n# 1st pass\r\nres = net(idat)\r\nres.backward(grad)\r\ngrad0 = idat.grad.data.cpu()\r\n\r\n# 2nd pass\r\nidat.grad.data.zero_()\r\n\r\nres = net(idat)\r\nres.backward(grad)\r\ngrad1 = idat.grad.data.cpu()\r\n\r\nprint (grad1 == grad0).all()\r\n```\r\nEven when I run forward in evaluation mode and backward in training I still get the gradient changed.\r\nIt works fine with cudnn disabled."}