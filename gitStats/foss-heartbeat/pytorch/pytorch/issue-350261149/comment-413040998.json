{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413040998", "html_url": "https://github.com/pytorch/pytorch/pull/10500#issuecomment-413040998", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10500", "id": 413040998, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzA0MDk5OA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-14T22:51:05Z", "updated_at": "2018-08-14T22:56:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't understand the stated justification for using fences here. You state: \"This is to preserve the original function schema of the op (otherwise, optimizations have to know how to emit world tokens).\" But why would optimizations have to know how to emit world tokens? In general, most optimizations won't be in the business of optimizing mutating operations like <code>aten::append</code> (by the way, I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>'s comment at <a href=\"https://github.com/pytorch/pytorch/pull/10500/files#r209985677\">https://github.com/pytorch/pytorch/pull/10500/files#r209985677</a>). So I don't see why they'd have to emit world tokens. To make an analogy, suppose there are some ATen functions that take and return special Tensor \"buffer\" arguments. If these functions are not the subject of optimization, why would an optimization pass need to know how to add/remove buffer arguments? (Furthermore, I WANT the function schema of mutating operations to say that they're mutating. Whether or not this is because the schema actually has the world token, or we have some special \"this has effects\" flag, I don't care; what I do want is for us to distinguish side effectful computations in some way in the schema.)</p>\n<p>In particular, you say, \"makes another kind of pass (all passes that want to emit or restructure ops on mutable values) more complex\". <em>Which</em> passes are you talking about? I can only think of one: the pass that is responsible for lowering Python IR into our core representation. But we will only ever have one such pass; but the number of optimization passes we may have that do DCE/code movement things are unlimited. That's why the original agreement was to do token passing.</p>", "body_text": "I don't understand the stated justification for using fences here. You state: \"This is to preserve the original function schema of the op (otherwise, optimizations have to know how to emit world tokens).\" But why would optimizations have to know how to emit world tokens? In general, most optimizations won't be in the business of optimizing mutating operations like aten::append (by the way, I agree with @apaszke's comment at https://github.com/pytorch/pytorch/pull/10500/files#r209985677). So I don't see why they'd have to emit world tokens. To make an analogy, suppose there are some ATen functions that take and return special Tensor \"buffer\" arguments. If these functions are not the subject of optimization, why would an optimization pass need to know how to add/remove buffer arguments? (Furthermore, I WANT the function schema of mutating operations to say that they're mutating. Whether or not this is because the schema actually has the world token, or we have some special \"this has effects\" flag, I don't care; what I do want is for us to distinguish side effectful computations in some way in the schema.)\nIn particular, you say, \"makes another kind of pass (all passes that want to emit or restructure ops on mutable values) more complex\". Which passes are you talking about? I can only think of one: the pass that is responsible for lowering Python IR into our core representation. But we will only ever have one such pass; but the number of optimization passes we may have that do DCE/code movement things are unlimited. That's why the original agreement was to do token passing.", "body": "I don't understand the stated justification for using fences here. You state: \"This is to preserve the original function schema of the op (otherwise, optimizations have to know how to emit world tokens).\" But why would optimizations have to know how to emit world tokens? In general, most optimizations won't be in the business of optimizing mutating operations like `aten::append` (by the way, I agree with @apaszke's comment at https://github.com/pytorch/pytorch/pull/10500/files#r209985677). So I don't see why they'd have to emit world tokens. To make an analogy, suppose there are some ATen functions that take and return special Tensor \"buffer\" arguments. If these functions are not the subject of optimization, why would an optimization pass need to know how to add/remove buffer arguments? (Furthermore, I WANT the function schema of mutating operations to say that they're mutating. Whether or not this is because the schema actually has the world token, or we have some special \"this has effects\" flag, I don't care; what I do want is for us to distinguish side effectful computations in some way in the schema.)\r\n\r\nIn particular, you say, \"makes another kind of pass (all passes that want to emit or restructure ops on mutable values) more complex\". *Which* passes are you talking about? I can only think of one: the pass that is responsible for lowering Python IR into our core representation. But we will only ever have one such pass; but the number of optimization passes we may have that do DCE/code movement things are unlimited. That's why the original agreement was to do token passing."}