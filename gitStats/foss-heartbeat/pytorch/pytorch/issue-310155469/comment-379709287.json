{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379709287", "html_url": "https://github.com/pytorch/pytorch/pull/6152#issuecomment-379709287", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6152", "id": 379709287, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTcwOTI4Nw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T10:38:00Z", "updated_at": "2018-04-09T10:40:22Z", "author_association": "MEMBER", "body_html": "<p>I'm pretty sure we'll never really want to use the order in which the dimensions were given, take this for an example:</p>\n<pre><code>In [1]: x = torch.randn(100, 100, 100, 100)\n\nIn [2]: %timeit x.sum(3).sum(2).sum(1).sum(0)\n9.44 ms \u00b1 8.49 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [3]: %timeit x.sum(0).sum(0).sum(0).sum(0)\n12 ms \u00b1 7.99 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [4]: %timeit x.view(-1).sum(0)\n9.23 ms \u00b1 15 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n<p>This makes sense, because reducing in order starting from the innermost dim is good for data locality, as earlier dimension will have lower strides once you get to them. (NB the difference only grows when I use fewer cores)</p>\n<p>An additional improvement would be to collapse the pseudo-contiguous pairs of dimensions (<code>stride[i+1] == size[i] * stride[i]</code>) into one. This is what happens in the last line, because such procedure gives you a 1D tensor if the input is contiguous.</p>\n<p>We can implement those later, but I feel like sorting dimensions would let us simplify the code a bit.</p>", "body_text": "I'm pretty sure we'll never really want to use the order in which the dimensions were given, take this for an example:\nIn [1]: x = torch.randn(100, 100, 100, 100)\n\nIn [2]: %timeit x.sum(3).sum(2).sum(1).sum(0)\n9.44 ms \u00b1 8.49 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [3]: %timeit x.sum(0).sum(0).sum(0).sum(0)\n12 ms \u00b1 7.99 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [4]: %timeit x.view(-1).sum(0)\n9.23 ms \u00b1 15 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nThis makes sense, because reducing in order starting from the innermost dim is good for data locality, as earlier dimension will have lower strides once you get to them. (NB the difference only grows when I use fewer cores)\nAn additional improvement would be to collapse the pseudo-contiguous pairs of dimensions (stride[i+1] == size[i] * stride[i]) into one. This is what happens in the last line, because such procedure gives you a 1D tensor if the input is contiguous.\nWe can implement those later, but I feel like sorting dimensions would let us simplify the code a bit.", "body": "I'm pretty sure we'll never really want to use the order in which the dimensions were given, take this for an example:\r\n```\r\nIn [1]: x = torch.randn(100, 100, 100, 100)\r\n\r\nIn [2]: %timeit x.sum(3).sum(2).sum(1).sum(0)\r\n9.44 ms \u00b1 8.49 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [3]: %timeit x.sum(0).sum(0).sum(0).sum(0)\r\n12 ms \u00b1 7.99 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [4]: %timeit x.view(-1).sum(0)\r\n9.23 ms \u00b1 15 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nThis makes sense, because reducing in order starting from the innermost dim is good for data locality, as earlier dimension will have lower strides once you get to them. (NB the difference only grows when I use fewer cores)\r\n\r\nAn additional improvement would be to collapse the pseudo-contiguous pairs of dimensions (`stride[i+1] == size[i] * stride[i]`) into one. This is what happens in the last line, because such procedure gives you a 1D tensor if the input is contiguous.\r\n\r\nWe can implement those later, but I feel like sorting dimensions would let us simplify the code a bit."}