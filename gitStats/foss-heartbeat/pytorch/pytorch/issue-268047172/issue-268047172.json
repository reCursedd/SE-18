{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3261", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3261/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3261/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3261/events", "html_url": "https://github.com/pytorch/pytorch/issues/3261", "id": 268047172, "node_id": "MDU6SXNzdWUyNjgwNDcxNzI=", "number": 3261, "title": "torch.max(value, dim=None) throws an error", "user": {"login": "jwvdm", "id": 1158561, "node_id": "MDQ6VXNlcjExNTg1NjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1158561?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwvdm", "html_url": "https://github.com/jwvdm", "followers_url": "https://api.github.com/users/jwvdm/followers", "following_url": "https://api.github.com/users/jwvdm/following{/other_user}", "gists_url": "https://api.github.com/users/jwvdm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwvdm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwvdm/subscriptions", "organizations_url": "https://api.github.com/users/jwvdm/orgs", "repos_url": "https://api.github.com/users/jwvdm/repos", "events_url": "https://api.github.com/users/jwvdm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwvdm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-24T14:09:21Z", "updated_at": "2017-10-24T18:28:16Z", "closed_at": "2017-10-24T18:28:16Z", "author_association": "NONE", "body_html": "<p>This may be expected/unavoidable behavior, but it turns out one cannot explicitly pass a <code>dim=None</code> argument to <code>torch.max</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>r <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>)\ntorch.max(r) <span class=\"pl-c\"><span class=\"pl-c\">#</span> this works</span>\ntorch.max(r, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>) </pre></div>\n<p>this returns the error</p>\n<blockquote>\n<p>TypeError: torch.max received an invalid combination of arguments - got (torch.FloatTensor, dim=NoneType), but expected one of:</p>\n<ul>\n<li>(torch.FloatTensor source)</li>\n<li>(torch.FloatTensor source, torch.FloatTensor other)<br>\ndidn't match because some of the keywords were incorrect: dim</li>\n<li>(torch.FloatTensor source, int dim)</li>\n<li>(torch.FloatTensor source, int dim, bool keepdim)</li>\n</ul>\n</blockquote>\n<p>This becomes an issue when writing functions that themselves accept a <code>dim</code> argument, e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">log_sum_exp</span>(<span class=\"pl-smi\">value</span>, <span class=\"pl-smi\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">keepdim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Numerically stable implementation of the operation</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    value.exp().sum(dim, keepdim).log()</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>: torch.max(value, dim=None) threw an error at time of writing</span>\n    <span class=\"pl-k\">if</span> dim <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        m, _ <span class=\"pl-k\">=</span> torch.max(value, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span>dim, <span class=\"pl-v\">keepdim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        value0 <span class=\"pl-k\">=</span> value <span class=\"pl-k\">-</span> m\n        <span class=\"pl-k\">if</span> keepdim <span class=\"pl-k\">is</span> <span class=\"pl-c1\">False</span>:\n            m <span class=\"pl-k\">=</span> m.squeeze(dim)\n        <span class=\"pl-k\">return</span> m <span class=\"pl-k\">+</span> torch.log(torch.sum(torch.exp(value0),\n                                       <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span>dim, <span class=\"pl-v\">keepdim</span><span class=\"pl-k\">=</span>keepdim))\n    <span class=\"pl-k\">else</span>:\n        m <span class=\"pl-k\">=</span> torch.max(value)\n        sum_exp <span class=\"pl-k\">=</span> torch.sum(torch.exp(value <span class=\"pl-k\">-</span> m))\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(sum_exp, Number):\n            <span class=\"pl-k\">return</span> m <span class=\"pl-k\">+</span> math.log(sum_exp)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">return</span> m <span class=\"pl-k\">+</span> torch.log(sum_exp)</pre></div>", "body_text": "This may be expected/unavoidable behavior, but it turns out one cannot explicitly pass a dim=None argument to torch.max:\nr = torch.rand(2, 3)\ntorch.max(r) # this works\ntorch.max(r, dim=None) \nthis returns the error\n\nTypeError: torch.max received an invalid combination of arguments - got (torch.FloatTensor, dim=NoneType), but expected one of:\n\n(torch.FloatTensor source)\n(torch.FloatTensor source, torch.FloatTensor other)\ndidn't match because some of the keywords were incorrect: dim\n(torch.FloatTensor source, int dim)\n(torch.FloatTensor source, int dim, bool keepdim)\n\n\nThis becomes an issue when writing functions that themselves accept a dim argument, e.g.\ndef log_sum_exp(value, dim=None, keepdim=False):\n    \"\"\"Numerically stable implementation of the operation\n\n    value.exp().sum(dim, keepdim).log()\n    \"\"\"\n    # TODO: torch.max(value, dim=None) threw an error at time of writing\n    if dim is not None:\n        m, _ = torch.max(value, dim=dim, keepdim=True)\n        value0 = value - m\n        if keepdim is False:\n            m = m.squeeze(dim)\n        return m + torch.log(torch.sum(torch.exp(value0),\n                                       dim=dim, keepdim=keepdim))\n    else:\n        m = torch.max(value)\n        sum_exp = torch.sum(torch.exp(value - m))\n        if isinstance(sum_exp, Number):\n            return m + math.log(sum_exp)\n        else:\n            return m + torch.log(sum_exp)", "body": "This may be expected/unavoidable behavior, but it turns out one cannot explicitly pass a `dim=None` argument to `torch.max`:\r\n```python\r\nr = torch.rand(2, 3)\r\ntorch.max(r) # this works\r\ntorch.max(r, dim=None) \r\n```\r\nthis returns the error\r\n> TypeError: torch.max received an invalid combination of arguments - got (torch.FloatTensor, dim=NoneType), but expected one of:\r\n> * (torch.FloatTensor source)\r\n> * (torch.FloatTensor source, torch.FloatTensor other)\r\n>      didn't match because some of the keywords were incorrect: dim\r\n> * (torch.FloatTensor source, int dim)\r\n> * (torch.FloatTensor source, int dim, bool keepdim)\r\n\r\nThis becomes an issue when writing functions that themselves accept a `dim` argument, e.g.\r\n```python\r\ndef log_sum_exp(value, dim=None, keepdim=False):\r\n    \"\"\"Numerically stable implementation of the operation\r\n\r\n    value.exp().sum(dim, keepdim).log()\r\n    \"\"\"\r\n    # TODO: torch.max(value, dim=None) threw an error at time of writing\r\n    if dim is not None:\r\n        m, _ = torch.max(value, dim=dim, keepdim=True)\r\n        value0 = value - m\r\n        if keepdim is False:\r\n            m = m.squeeze(dim)\r\n        return m + torch.log(torch.sum(torch.exp(value0),\r\n                                       dim=dim, keepdim=keepdim))\r\n    else:\r\n        m = torch.max(value)\r\n        sum_exp = torch.sum(torch.exp(value - m))\r\n        if isinstance(sum_exp, Number):\r\n            return m + math.log(sum_exp)\r\n        else:\r\n            return m + torch.log(sum_exp)\r\n```"}