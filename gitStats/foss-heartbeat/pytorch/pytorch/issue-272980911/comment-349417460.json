{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/349417460", "html_url": "https://github.com/pytorch/pytorch/issues/3624#issuecomment-349417460", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3624", "id": 349417460, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTQxNzQ2MA==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-05T19:40:03Z", "updated_at": "2017-12-05T19:40:20Z", "author_association": "MEMBER", "body_html": "<p>I'm not sure what \"matches NumPy behavior\" means here. There's no NumPy implementation of BatchNorm and even if there were, we would not match it exactly. Different implementations will produce slightly different results. You'll get different results for different ordering of additions not to mention <a href=\"http://christian-seiler.de/projekte/fpmath/\" rel=\"nofollow\">x87 vs SSE</a> fpmath (I think modern compilers default to SSE though).</p>\n<p>We never guarantee that float32 functions only use float32 computations. In many places we use double-precision floats on the CPU for reductions before converting back to single-precision.</p>", "body_text": "I'm not sure what \"matches NumPy behavior\" means here. There's no NumPy implementation of BatchNorm and even if there were, we would not match it exactly. Different implementations will produce slightly different results. You'll get different results for different ordering of additions not to mention x87 vs SSE fpmath (I think modern compilers default to SSE though).\nWe never guarantee that float32 functions only use float32 computations. In many places we use double-precision floats on the CPU for reductions before converting back to single-precision.", "body": "I'm not sure what \"matches NumPy behavior\" means here. There's no NumPy implementation of BatchNorm and even if there were, we would not match it exactly. Different implementations will produce slightly different results. You'll get different results for different ordering of additions not to mention [x87 vs SSE](http://christian-seiler.de/projekte/fpmath/) fpmath (I think modern compilers default to SSE though). \r\n\r\nWe never guarantee that float32 functions only use float32 computations. In many places we use double-precision floats on the CPU for reductions before converting back to single-precision."}