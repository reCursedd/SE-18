{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3624", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3624/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3624/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3624/events", "html_url": "https://github.com/pytorch/pytorch/issues/3624", "id": 272980911, "node_id": "MDU6SXNzdWUyNzI5ODA5MTE=", "number": 3624, "title": "Issue with type conversion for BatchNorm inference in THNN", "user": {"login": "libfun", "id": 301147, "node_id": "MDQ6VXNlcjMwMTE0Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/301147?v=4", "gravatar_id": "", "url": "https://api.github.com/users/libfun", "html_url": "https://github.com/libfun", "followers_url": "https://api.github.com/users/libfun/followers", "following_url": "https://api.github.com/users/libfun/following{/other_user}", "gists_url": "https://api.github.com/users/libfun/gists{/gist_id}", "starred_url": "https://api.github.com/users/libfun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/libfun/subscriptions", "organizations_url": "https://api.github.com/users/libfun/orgs", "repos_url": "https://api.github.com/users/libfun/repos", "events_url": "https://api.github.com/users/libfun/events{/privacy}", "received_events_url": "https://api.github.com/users/libfun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-11-10T16:07:19Z", "updated_at": "2017-12-06T08:26:41Z", "closed_at": "2017-12-06T08:26:41Z", "author_association": "NONE", "body_html": "<p>TL;DR:<br>\nTHNN/BatchNormalization.c, Line 53<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/5478d0154fca6859c2bd6c8147b6797074d1efcf/aten/src/THNN/generic/BatchNormalization.c#L53\">pytorch/aten/src/THNN/generic/BatchNormalization.c</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 53\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/5478d0154fca6859c2bd6c8147b6797074d1efcf\">5478d01</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L53\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"53\"></td>\n          <td id=\"LC53\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> invstd = <span class=\"pl-c1\">1</span> / <span class=\"pl-c1\">sqrt</span>(<span class=\"pl-c1\">THTensor_</span>(get1d)(running_var, f) + eps); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>The variable <code>eps</code> is double, so <code>THTensor_(get1d)(running_var, f)</code> converts to double, so whole <code>1 / sqrt</code> thing becomes double, and in the end converts to <code>real</code>, because <code>invstd</code> is <code>real</code>.</p>\n<p>If eps is converted to <code>float</code> before, the output should match the expected behavior.</p>\n<p>I was trying to convert Pytorch model to Caffe on CPU and all conv layers worked flawlessly and produced equal output tensors, but on BatchNorm layers something broke and the output was very different. After some investigation I found that Caffe implementation produced the same output as direct numpy equivalent in float32, but Pytorch didn't. After setting types to float64 for values described above I was able to reproduce Pytorch batchnorm output with numpy.</p>\n<p>This issue breaks direct conversion of Pytorch models to Caffe, and also seems like genuine unexpected behavior, since everything is expected be float32 during both training and inference.</p>", "body_text": "TL;DR:\nTHNN/BatchNormalization.c, Line 53\n\n  \n    \n      pytorch/aten/src/THNN/generic/BatchNormalization.c\n    \n    \n         Line 53\n      in\n      5478d01\n    \n    \n    \n    \n\n        \n          \n           invstd = 1 / sqrt(THTensor_(get1d)(running_var, f) + eps); \n        \n    \n  \n\n\nThe variable eps is double, so THTensor_(get1d)(running_var, f) converts to double, so whole 1 / sqrt thing becomes double, and in the end converts to real, because invstd is real.\nIf eps is converted to float before, the output should match the expected behavior.\nI was trying to convert Pytorch model to Caffe on CPU and all conv layers worked flawlessly and produced equal output tensors, but on BatchNorm layers something broke and the output was very different. After some investigation I found that Caffe implementation produced the same output as direct numpy equivalent in float32, but Pytorch didn't. After setting types to float64 for values described above I was able to reproduce Pytorch batchnorm output with numpy.\nThis issue breaks direct conversion of Pytorch models to Caffe, and also seems like genuine unexpected behavior, since everything is expected be float32 during both training and inference.", "body": "TL;DR:\r\nTHNN/BatchNormalization.c, Line 53\r\nhttps://github.com/pytorch/pytorch/blob/5478d0154fca6859c2bd6c8147b6797074d1efcf/aten/src/THNN/generic/BatchNormalization.c#L53\r\n\r\nThe variable `eps` is double, so `THTensor_(get1d)(running_var, f)` converts to double, so whole `1 / sqrt` thing becomes double, and in the end converts to `real`, because `invstd` is `real`.\r\n\r\nIf eps is converted to `float` before, the output should match the expected behavior.\r\n\r\nI was trying to convert Pytorch model to Caffe on CPU and all conv layers worked flawlessly and produced equal output tensors, but on BatchNorm layers something broke and the output was very different. After some investigation I found that Caffe implementation produced the same output as direct numpy equivalent in float32, but Pytorch didn't. After setting types to float64 for values described above I was able to reproduce Pytorch batchnorm output with numpy.\r\n\r\nThis issue breaks direct conversion of Pytorch models to Caffe, and also seems like genuine unexpected behavior, since everything is expected be float32 during both training and inference."}