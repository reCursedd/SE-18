{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385798019", "html_url": "https://github.com/pytorch/pytorch/issues/7008#issuecomment-385798019", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7008", "id": 385798019, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTc5ODAxOQ==", "user": {"login": "hwnam831", "id": 17243609, "node_id": "MDQ6VXNlcjE3MjQzNjA5", "avatar_url": "https://avatars1.githubusercontent.com/u/17243609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hwnam831", "html_url": "https://github.com/hwnam831", "followers_url": "https://api.github.com/users/hwnam831/followers", "following_url": "https://api.github.com/users/hwnam831/following{/other_user}", "gists_url": "https://api.github.com/users/hwnam831/gists{/gist_id}", "starred_url": "https://api.github.com/users/hwnam831/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hwnam831/subscriptions", "organizations_url": "https://api.github.com/users/hwnam831/orgs", "repos_url": "https://api.github.com/users/hwnam831/repos", "events_url": "https://api.github.com/users/hwnam831/events{/privacy}", "received_events_url": "https://api.github.com/users/hwnam831/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-01T21:35:40Z", "updated_at": "2018-05-01T21:37:30Z", "author_association": "NONE", "body_html": "<p>Did you try running the exact same script? I tried reproducing the same error on more than 3 machines and they show same errors. The error also occurs in cuda90 too. Pytorch 0.2 doesn't throw an error for the script but extremely slow because it calls hundreds of thousands of gemm kernels rather than calling a batched gemm kernel.</p>\n<p>I think there is an issue with batched Hgemm with large batch sizes. The first 3 lines execute batched Hgemm of 140000 batches and throw an error. Batched Sgemms have no problem with them and batched Hgemms with small batch sizes work fine too.</p>", "body_text": "Did you try running the exact same script? I tried reproducing the same error on more than 3 machines and they show same errors. The error also occurs in cuda90 too. Pytorch 0.2 doesn't throw an error for the script but extremely slow because it calls hundreds of thousands of gemm kernels rather than calling a batched gemm kernel.\nI think there is an issue with batched Hgemm with large batch sizes. The first 3 lines execute batched Hgemm of 140000 batches and throw an error. Batched Sgemms have no problem with them and batched Hgemms with small batch sizes work fine too.", "body": "Did you try running the exact same script? I tried reproducing the same error on more than 3 machines and they show same errors. The error also occurs in cuda90 too. Pytorch 0.2 doesn't throw an error for the script but extremely slow because it calls hundreds of thousands of gemm kernels rather than calling a batched gemm kernel.\r\n\r\nI think there is an issue with batched Hgemm with large batch sizes. The first 3 lines execute batched Hgemm of 140000 batches and throw an error. Batched Sgemms have no problem with them and batched Hgemms with small batch sizes work fine too."}