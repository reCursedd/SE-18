{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12206", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12206/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12206/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12206/events", "html_url": "https://github.com/pytorch/pytorch/issues/12206", "id": 365245042, "node_id": "MDU6SXNzdWUzNjUyNDUwNDI=", "number": 12206, "title": "[JIT] Operators defined in python are not supported well", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-30T17:46:16Z", "updated_at": "2018-10-05T15:34:53Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h1>The problem</h1>\n<p>There are many operators defined in python, such as <code>unique</code>, <code>clamp</code>, etc.</p>\n<p>These operators are not supported well, for example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@torch.jit.script</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> torch.unique(x)</pre></div>\n<p>gives</p>\n<pre><code>Traceback (most recent call last):\n  File \"quicktest.py\", line 3, in &lt;module&gt;\n    @torch.jit.script\n  File \"/data/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py\", line 639, in script\n    graph = _jit_script_compile(ast, rcb)\nRuntimeError: \nunknown builtin op:\n@torch.jit.script\ndef f(x):\n    return torch.unique(x)\n           ~~~~~~~~~~~~ &lt;--- HERE\n</code></pre>\n<p>and</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@torch.jit.script</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> torch.clamp(x, <span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)</pre></div>\n<p>gives</p>\n<pre><code>Traceback (most recent call last):\n  File \"quicktest.py\", line 7, in &lt;module&gt;\n    @torch.jit.script\n  File \"/data/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py\", line 639, in script\n    graph = _jit_script_compile(ast, rcb)\nRuntimeError: \narguments for call are not valid:\n  \n  for operator aten::clamp(Tensor self, Scalar min, Scalar max) -&gt; Tensor:\n  argument max not provided.\n  @torch.jit.script\n  def f(x):\n      return torch.clamp(x, min=0)\n             ~~~~~~~~~~~ &lt;--- HERE\nfor call at:\n@torch.jit.script\ndef f(x):\n    return torch.clamp(x, min=0)\n           ~~~~~~~~~~~ &lt;--- HERE\n</code></pre>\n<p>Many of these operators are just a wrapper of an ATen op, it is defined in python only because language difference such as C++ does not support kwargs, or C++ does not support a function to return different types.</p>\n<h1>What we should do</h1>\n<p>For these ops, I suggest to add a mechanism to transform the call of these operators to its ATen form.</p>\n<p>For example</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@torch.jit.script</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> torch.unique(x)</pre></div>\n<p>should be transformed to</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@torch.jit.script</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> torch._unique(x)[<span class=\"pl-c1\">0</span>]</pre></div>\n<p>which will be supported if <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"365227665\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12203\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12203/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12203\">#12203</a> get merged.</p>\n<p>and</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@torch.jit.script</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>):\n   <span class=\"pl-k\">return</span> torch.clamp(x, <span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)</pre></div>\n<p>should be transformed into</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@torch.jit.script</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">f</span>(<span class=\"pl-smi\">x</span>):\n   <span class=\"pl-k\">return</span> torch.clamp_min(x, <span class=\"pl-c1\">0</span>)</pre></div>\n<h1>Draft of ideas on implementation</h1>\n<p>In detail, the implementation might be something like:</p>\n<p>In <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/functional.py\">https://github.com/pytorch/pytorch/blob/master/torch/functional.py</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">unique</span>(...):\n    <span class=\"pl-c1\">...</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">unique_jit_transform</span>(<span class=\"pl-smi\">treeview</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> treeview is the part of AST constructed by `get_jit_ast` of this function call,</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> such as: Apply('torch.unique', {Ident('x')}, {})</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>in `treeview` dim is None and return_inverse is False<span class=\"pl-pds\">'</span></span>:\n        <span class=\"pl-k\">return</span> Subscript(Apply(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch._unique<span class=\"pl-pds\">'</span></span>, {Ident(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>)}, {}), Const(<span class=\"pl-c1\">0</span>))\n    <span class=\"pl-c1\">...</span>\n\nunique._jit_transform <span class=\"pl-k\">=</span> unique_jit_transform</pre></div>\n<p>And in <code>emitSugaredExpr</code> in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/script/compiler.cpp\">https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/script/compiler.cpp</a>, <code>case TK_APPLY</code>,  when we saw that the callee is such an operator, invode its <code>_jit_transform</code> and then continue IR generation.</p>", "body_text": "The problem\nThere are many operators defined in python, such as unique, clamp, etc.\nThese operators are not supported well, for example:\n@torch.jit.script\ndef f(x):\n    return torch.unique(x)\ngives\nTraceback (most recent call last):\n  File \"quicktest.py\", line 3, in <module>\n    @torch.jit.script\n  File \"/data/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py\", line 639, in script\n    graph = _jit_script_compile(ast, rcb)\nRuntimeError: \nunknown builtin op:\n@torch.jit.script\ndef f(x):\n    return torch.unique(x)\n           ~~~~~~~~~~~~ <--- HERE\n\nand\n@torch.jit.script\ndef f(x):\n    return torch.clamp(x, min=0)\ngives\nTraceback (most recent call last):\n  File \"quicktest.py\", line 7, in <module>\n    @torch.jit.script\n  File \"/data/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py\", line 639, in script\n    graph = _jit_script_compile(ast, rcb)\nRuntimeError: \narguments for call are not valid:\n  \n  for operator aten::clamp(Tensor self, Scalar min, Scalar max) -> Tensor:\n  argument max not provided.\n  @torch.jit.script\n  def f(x):\n      return torch.clamp(x, min=0)\n             ~~~~~~~~~~~ <--- HERE\nfor call at:\n@torch.jit.script\ndef f(x):\n    return torch.clamp(x, min=0)\n           ~~~~~~~~~~~ <--- HERE\n\nMany of these operators are just a wrapper of an ATen op, it is defined in python only because language difference such as C++ does not support kwargs, or C++ does not support a function to return different types.\nWhat we should do\nFor these ops, I suggest to add a mechanism to transform the call of these operators to its ATen form.\nFor example\n@torch.jit.script\ndef f(x):\n    return torch.unique(x)\nshould be transformed to\n@torch.jit.script\ndef f(x):\n    return torch._unique(x)[0]\nwhich will be supported if #12203 get merged.\nand\n@torch.jit.script\ndef f(x):\n   return torch.clamp(x, min=0)\nshould be transformed into\n@torch.jit.script\ndef f(x):\n   return torch.clamp_min(x, 0)\nDraft of ideas on implementation\nIn detail, the implementation might be something like:\nIn https://github.com/pytorch/pytorch/blob/master/torch/functional.py\ndef unique(...):\n    ...\n\ndef unique_jit_transform(treeview):\n    # treeview is the part of AST constructed by `get_jit_ast` of this function call,\n    # such as: Apply('torch.unique', {Ident('x')}, {})\n    if 'in `treeview` dim is None and return_inverse is False':\n        return Subscript(Apply('torch._unique', {Ident('x')}, {}), Const(0))\n    ...\n\nunique._jit_transform = unique_jit_transform\nAnd in emitSugaredExpr in https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/script/compiler.cpp, case TK_APPLY,  when we saw that the callee is such an operator, invode its _jit_transform and then continue IR generation.", "body": "# The problem\r\n\r\nThere are many operators defined in python, such as `unique`, `clamp`, etc.\r\n\r\nThese operators are not supported well, for example:\r\n\r\n```python\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.unique(x)\r\n```\r\ngives\r\n```\r\nTraceback (most recent call last):\r\n  File \"quicktest.py\", line 3, in <module>\r\n    @torch.jit.script\r\n  File \"/data/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py\", line 639, in script\r\n    graph = _jit_script_compile(ast, rcb)\r\nRuntimeError: \r\nunknown builtin op:\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.unique(x)\r\n           ~~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\nand \r\n\r\n```python\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.clamp(x, min=0)\r\n```\r\n\r\ngives\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"quicktest.py\", line 7, in <module>\r\n    @torch.jit.script\r\n  File \"/data/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py\", line 639, in script\r\n    graph = _jit_script_compile(ast, rcb)\r\nRuntimeError: \r\narguments for call are not valid:\r\n  \r\n  for operator aten::clamp(Tensor self, Scalar min, Scalar max) -> Tensor:\r\n  argument max not provided.\r\n  @torch.jit.script\r\n  def f(x):\r\n      return torch.clamp(x, min=0)\r\n             ~~~~~~~~~~~ <--- HERE\r\nfor call at:\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.clamp(x, min=0)\r\n           ~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\nMany of these operators are just a wrapper of an ATen op, it is defined in python only because language difference such as C++ does not support kwargs, or C++ does not support a function to return different types.\r\n\r\n# What we should do\r\n\r\nFor these ops, I suggest to add a mechanism to transform the call of these operators to its ATen form.\r\n\r\nFor example\r\n\r\n```python\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.unique(x)\r\n```\r\n\r\nshould be transformed to\r\n\r\n```python\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch._unique(x)[0]\r\n```\r\n\r\nwhich will be supported if https://github.com/pytorch/pytorch/pull/12203 get merged.\r\n\r\nand\r\n\r\n ```python\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.clamp(x, min=0)\r\n```\r\n\r\nshould be transformed into\r\n\r\n ```python\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.clamp_min(x, 0)\r\n```\r\n\r\n# Draft of ideas on implementation\r\n\r\nIn detail, the implementation might be something like:\r\n\r\nIn https://github.com/pytorch/pytorch/blob/master/torch/functional.py\r\n\r\n```python\r\ndef unique(...):\r\n    ...\r\n\r\ndef unique_jit_transform(treeview):\r\n    # treeview is the part of AST constructed by `get_jit_ast` of this function call,\r\n    # such as: Apply('torch.unique', {Ident('x')}, {})\r\n    if 'in `treeview` dim is None and return_inverse is False':\r\n        return Subscript(Apply('torch._unique', {Ident('x')}, {}), Const(0))\r\n    ...\r\n\r\nunique._jit_transform = unique_jit_transform\r\n```\r\n\r\nAnd in `emitSugaredExpr` in https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/script/compiler.cpp, `case TK_APPLY`,  when we saw that the callee is such an operator, invode its `_jit_transform` and then continue IR generation."}