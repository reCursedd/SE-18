{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398864305", "html_url": "https://github.com/pytorch/pytorch/issues/7961#issuecomment-398864305", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7961", "id": 398864305, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODg2NDMwNQ==", "user": {"login": "simon555", "id": 29953850, "node_id": "MDQ6VXNlcjI5OTUzODUw", "avatar_url": "https://avatars0.githubusercontent.com/u/29953850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simon555", "html_url": "https://github.com/simon555", "followers_url": "https://api.github.com/users/simon555/followers", "following_url": "https://api.github.com/users/simon555/following{/other_user}", "gists_url": "https://api.github.com/users/simon555/gists{/gist_id}", "starred_url": "https://api.github.com/users/simon555/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simon555/subscriptions", "organizations_url": "https://api.github.com/users/simon555/orgs", "repos_url": "https://api.github.com/users/simon555/repos", "events_url": "https://api.github.com/users/simon555/events{/privacy}", "received_events_url": "https://api.github.com/users/simon555/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-20T19:13:42Z", "updated_at": "2018-06-20T19:15:37Z", "author_association": "NONE", "body_html": "<p>Sure!</p>\n<p>I am working on Language Model, here is my model  :</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">LstmLm</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">args</span>, <span class=\"pl-smi\">vocab</span>, <span class=\"pl-smi\">padidx</span>):\n        <span class=\"pl-c1\">super</span>(LstmLm, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.args<span class=\"pl-k\">=</span>args\n        <span class=\"pl-c1\">self</span>.nhid<span class=\"pl-k\">=</span>args.nhid\n        <span class=\"pl-c1\">self</span>.nlayers<span class=\"pl-k\">=</span>args.nlayers\n        <span class=\"pl-c1\">self</span>.dropout<span class=\"pl-k\">=</span>args.dropout\n        <span class=\"pl-c1\">self</span>.tieweights<span class=\"pl-k\">=</span>args.tieweights\n        <span class=\"pl-c1\">self</span>.vsize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(vocab.itos)\n        <span class=\"pl-c1\">self</span>.padidx<span class=\"pl-k\">=</span>padidx\n        <span class=\"pl-k\">if</span> args.maxnorm<span class=\"pl-k\">==</span><span class=\"pl-c1\">False</span>:\n            maxNorm<span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>\n        <span class=\"pl-k\">else</span>:\n            maxNorm<span class=\"pl-k\">=</span>args.maxnorm\n        <span class=\"pl-c1\">self</span>.lut <span class=\"pl-k\">=</span> nn.Embedding(<span class=\"pl-c1\">self</span>.vsize, <span class=\"pl-c1\">self</span>.nhid, <span class=\"pl-v\">max_norm</span><span class=\"pl-k\">=</span>maxNorm)\n        <span class=\"pl-c1\">self</span>.rnn <span class=\"pl-k\">=</span> nn.LSTM(\n            <span class=\"pl-v\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.nhid,\n            <span class=\"pl-v\">hidden_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.nhid,\n            <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.nlayers,\n            <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dropout)\n        <span class=\"pl-c1\">self</span>.drop <span class=\"pl-k\">=</span> nn.Dropout(<span class=\"pl-c1\">self</span>.dropout)\n        <span class=\"pl-c1\">self</span>.proj <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">self</span>.nhid, <span class=\"pl-c1\">self</span>.vsize)\n        <span class=\"pl-c1\">self</span>.trainingEpochs<span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span>number of epochs already trained</span>\n        <span class=\"pl-c1\">self</span>.trainingBatches<span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>\n        <span class=\"pl-c1\">self</span>.temperature<span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>\n        <span class=\"pl-c1\">self</span>.generationIteratorBuilt<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>\n        \n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.tieweights:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> See https://arxiv.org/abs/1608.05859</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Seems to improve ppl by 13%.</span>\n            <span class=\"pl-c1\">self</span>.proj.weight <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lut.weight\n        \n        <span class=\"pl-k\">if</span> torch.cuda.is_available():\n            <span class=\"pl-c1\">self</span>.cuda()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">hid</span>):\n        emb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lut(<span class=\"pl-c1\">input</span>)\n        hids, hid <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.rnn(emb, hid)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Detach hiddens to truncate the computational graph for BPTT.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Recall that hid = (h,c).</span>\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.proj(<span class=\"pl-c1\">self</span>.drop(hids)), <span class=\"pl-c1\">tuple</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x.detach(), hid))\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">next_N_words</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">word</span>, <span class=\"pl-smi\">hid_</span>, <span class=\"pl-smi\">TEXT</span>, <span class=\"pl-smi\">length_to_predict</span>):\n        output<span class=\"pl-k\">=</span>[]\n        \n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(length_to_predict):\n            wordIndex<span class=\"pl-k\">=</span><span class=\"pl-c1\">TEXT</span>.vocab.stoi[word]\n            input_tensor<span class=\"pl-k\">=</span>Variable(torch.LongTensor([[wordIndex]]))\n            \n            <span class=\"pl-k\">if</span> torch.cuda.is_available():\n                input_tensor<span class=\"pl-k\">=</span>input_tensor.cuda()\n            \n            emb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lut(input_tensor)\n            \n            hids, hid_ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.rnn(emb, hid_)\n            \n            next_logits<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.proj((hids)).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n            next_distr<span class=\"pl-k\">=</span>torch.nn.Softmax(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)(torch.mul(next_logits, <span class=\"pl-c1\">1</span><span class=\"pl-k\">/</span><span class=\"pl-c1\">self</span>.temperature))\n            next_index<span class=\"pl-k\">=</span>torch.multinomial(next_distr,<span class=\"pl-c1\">1</span>)\n            \n            next_word<span class=\"pl-k\">=</span><span class=\"pl-c1\">TEXT</span>.vocab.itos[next_index.cpu().data[<span class=\"pl-c1\">0</span>]]\n            output<span class=\"pl-k\">+=</span>[next_word]\n            word<span class=\"pl-k\">=</span>next_word\n        <span class=\"pl-k\">return</span>(output)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">repackage_hidden</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">h</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Wraps hidden states in new Variables, to detach them from their history.<span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">type</span>(h) <span class=\"pl-k\">==</span> Variable:\n            <span class=\"pl-k\">return</span> Variable(h.data)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">tuple</span>(<span class=\"pl-c1\">self</span>.repackage_hidden(v) <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> h)\n    \n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">train_epoch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">iter</span>, <span class=\"pl-smi\">loss</span>, <span class=\"pl-smi\">optimizer</span>, <span class=\"pl-smi\">viz</span>, <span class=\"pl-smi\">win</span>, <span class=\"pl-smi\">TEXT</span>, <span class=\"pl-smi\">infoToPlot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>.train()\n        <span class=\"pl-c1\">self</span>.trainingBatches<span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>\n\n        train_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        nwords <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n        hid <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n        batch_id<span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> tqdm(<span class=\"pl-c1\">iter</span>):\n            <span class=\"pl-c1\">self</span>.trainingBatches<span class=\"pl-k\">+=</span><span class=\"pl-c1\">1</span>\n            \n            <span class=\"pl-k\">if</span> hid <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                hid[<span class=\"pl-c1\">0</span>].detach_()\n                hid[<span class=\"pl-c1\">1</span>].detach_()\n                \n            optimizer.zero_grad()\n            x <span class=\"pl-k\">=</span> batch.text\n            y <span class=\"pl-k\">=</span> batch.target\n            \n            <span class=\"pl-k\">if</span> torch.cuda.is_available():\n                x<span class=\"pl-k\">=</span>x.cuda()\n                y<span class=\"pl-k\">=</span>y.cuda()\n                \n            out, hid <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>(x, hid <span class=\"pl-k\">if</span> hid <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>)\n            bloss <span class=\"pl-k\">=</span> loss(out.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.vsize), y.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n            \n            bloss.backward()\n            train_loss <span class=\"pl-k\">+=</span> bloss\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> bytetensor.sum overflows, so cast to int</span>\n            local_nwords<span class=\"pl-k\">=</span> y.ne(<span class=\"pl-c1\">self</span>.padidx).int().sum()\n            nwords <span class=\"pl-k\">+=</span> local_nwords\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.args.clip <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n                clip_grad_norm_(<span class=\"pl-c1\">self</span>.parameters(), <span class=\"pl-c1\">self</span>.args.clip)\n\n            optimizer.step()\n            \n            \n            \n            <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> infoToPlot <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n                infoToPlot[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>trainPerp<span class=\"pl-pds\">'</span></span>]<span class=\"pl-k\">+=</span>[np.exp(bloss.item()<span class=\"pl-k\">/</span>local_nwords.item())]\n                \n            <span class=\"pl-k\">if</span> batch_id <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                sampled_sentences<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.generate_predictions(<span class=\"pl-c1\">TEXT</span>)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(sampled_sentences)</span>\n                infoToPlot[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>generated<span class=\"pl-pds\">'</span></span>]<span class=\"pl-k\">=</span>sampled_sentences\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(infoToPlot['generated'])</span>\n                win <span class=\"pl-k\">=</span> visdom_plot(viz, win, infoToPlot)\n\n                \n            \n            batch_id<span class=\"pl-k\">+=</span><span class=\"pl-c1\">1</span>\n        <span class=\"pl-c1\">self</span>.trainingEpochs<span class=\"pl-k\">+=</span><span class=\"pl-c1\">1</span>\n        <span class=\"pl-k\">return</span> train_loss.data[<span class=\"pl-c1\">0</span>], nwords.data[<span class=\"pl-c1\">0</span>]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">validate</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">iter</span>, <span class=\"pl-smi\">loss</span>, <span class=\"pl-smi\">viz</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">win</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">infoToPlot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>.eval()\n\n        valid_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        nwords <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n        hid <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n        <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> tqdm(<span class=\"pl-c1\">iter</span>):\n            x <span class=\"pl-k\">=</span> batch.text\n            y <span class=\"pl-k\">=</span> batch.target\n            out, hid <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>(x, hid <span class=\"pl-k\">if</span> hid <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>)\n            valid_loss <span class=\"pl-k\">+=</span> loss(out.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.vsize), y.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n            nwords <span class=\"pl-k\">+=</span> y.ne(<span class=\"pl-c1\">self</span>.padidx).int().sum()\n            \n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> infoToPlot <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            infoToPlot[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>validPerp<span class=\"pl-pds\">'</span></span>]<span class=\"pl-k\">+=</span>[np.exp(valid_loss.data[<span class=\"pl-c1\">0</span>]<span class=\"pl-k\">/</span>nwords.data[<span class=\"pl-c1\">0</span>])]\n            win <span class=\"pl-k\">=</span> visdom_plot(viz, win, infoToPlot, <span class=\"pl-v\">valid</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n            \n            \n        <span class=\"pl-k\">return</span> valid_loss.data[<span class=\"pl-c1\">0</span>], nwords.data[<span class=\"pl-c1\">0</span>]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">generate_predictions</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">TEXT</span>, <span class=\"pl-smi\">outputDirectory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">saveOutputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        <span class=\"pl-k\">if</span> outputDirectory<span class=\"pl-k\">==</span><span class=\"pl-c1\">None</span>:\n            outputDirectory<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.args.directoryData\n        <span class=\"pl-k\">if</span> epoch<span class=\"pl-k\">==</span><span class=\"pl-c1\">None</span>:\n            epoch<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.trainingEpochs\n        \n            \n        <span class=\"pl-c1\">self</span>.eval()\n        batch_number<span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>\n        outputs<span class=\"pl-k\">=</span>[<span class=\"pl-c1\">dict</span>({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_sentence<span class=\"pl-pds\">'</span></span>:<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>expected_sentence<span class=\"pl-pds\">'</span></span>:<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>output_sentence<span class=\"pl-pds\">'</span></span>:<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>}) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_number)]\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>prepare the data to generate on</span>\n        \n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">self</span>.generationIteratorBuilt:\n            data <span class=\"pl-k\">=</span> torchtext.datasets.LanguageModelingDataset(\n                <span class=\"pl-v\">path</span><span class=\"pl-k\">=</span>os.path.join(os.getcwd(),<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>data<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>splitted<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>smallData<span class=\"pl-pds\">\"</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gen.txt<span class=\"pl-pds\">\"</span></span>),\n                <span class=\"pl-v\">text_field</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">TEXT</span>)\n            data_iter <span class=\"pl-k\">=</span> torchtext.data.BPTTIterator(data, batch_number, <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.args.devid, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>print()</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(\"Generating the next 80 words, from the 20 first ones\")</span>\n            <span class=\"pl-c1\">self</span>.iterator<span class=\"pl-k\">=</span>itertools.cycle(<span class=\"pl-c1\">iter</span>(data_iter))\n            <span class=\"pl-c1\">self</span>.generationIteratorBuilt<span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>\n        sample<span class=\"pl-k\">=</span><span class=\"pl-c1\">next</span>(<span class=\"pl-c1\">self</span>.iterator)\n        input_sentence<span class=\"pl-k\">=</span>sample.text[:<span class=\"pl-c1\">20</span>,:]\n        <span class=\"pl-k\">if</span> torch.cuda.is_available:\n            input_sentence<span class=\"pl-k\">=</span>input_sentence.cuda()\n        expected_sentence<span class=\"pl-k\">=</span>sample.text\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>we will give the 20 first words of a sentence, and predict the 80 next characters</span>\n        input_words<span class=\"pl-k\">=</span>[[] <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_number)]        \n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_number):\n                input_words[i]<span class=\"pl-k\">=</span>[<span class=\"pl-c1\">TEXT</span>.vocab.itos[x] <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> input_sentence[:,i].data.tolist()]\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(input_words)</span>\n        \n        expected_output_words<span class=\"pl-k\">=</span>[[] <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_number)]        \n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_number):\n                expected_output_words[i]<span class=\"pl-k\">=</span>[<span class=\"pl-c1\">TEXT</span>.vocab.itos[x] <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> expected_sentence[:,i].data.tolist()]\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(output_words)          </span>\n      \n        \n       \n        <span class=\"pl-k\">if</span> saveOutputs:\n            <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(os.path.join(outputDirectory,<span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">__class__</span>.<span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.preds.txt<span class=\"pl-pds\">\"</span></span>), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>a<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> f:\n                f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">20</span>)\n                f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span> <span class=\"pl-cce\">\\n</span> NEW : EPOCH <span class=\"pl-c1\">{}</span> <span class=\"pl-cce\">\\n</span> <span class=\"pl-cce\">\\n</span> <span class=\"pl-pds\">'</span></span>.format(epoch))\n                f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">20</span>)\n                f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n            \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>first we run the model on the first 20 words, in order to give context to the hidden state</span>\n        output_sentence, hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>(input_sentence, <span class=\"pl-c1\">None</span>)\n            \n        <span class=\"pl-k\">for</span> batch_index <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_number):          \n            \n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>then we run the model using the  computed hidden states</span>\n            input_word<span class=\"pl-k\">=</span>expected_output_words[batch_index][<span class=\"pl-c1\">20</span>]\n            \n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>we select the correct hidden state for the batch</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>because next_N_words take a single word as input</span>\n            local_h<span class=\"pl-k\">=</span>hidden[<span class=\"pl-c1\">0</span>][:,batch_index:batch_index<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>,:]\n            local_c<span class=\"pl-k\">=</span>hidden[<span class=\"pl-c1\">1</span>][:,batch_index:batch_index<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>,:]\n            local_hidden<span class=\"pl-k\">=</span>(local_h, local_c)\n            \n            output<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.next_N_words(input_word, local_hidden, <span class=\"pl-c1\">TEXT</span>, <span class=\"pl-c1\">80</span>)\n\n            outputs[batch_index][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_sentence<span class=\"pl-pds\">'</span></span>]<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>.join([x<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> input_words[batch_index]])\n            outputs[batch_index][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>expected_sentence<span class=\"pl-pds\">'</span></span>]<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>.join([x<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> expected_output_words[batch_index]])\n            outputs[batch_index][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>output_sentence<span class=\"pl-pds\">'</span></span>]<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>.join([x<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> output])\n            \n            <span class=\"pl-k\">if</span> saveOutputs:\n                <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(os.path.join(outputDirectory,<span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">__class__</span>.<span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.preds.txt<span class=\"pl-pds\">\"</span></span>), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>a<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> f:\n    \n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input sentence : <span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n                    f.write( <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>.join([x<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> input_words[batch_index]]))\n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span> <span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n                    \n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>expected output sentence : <span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>.join([x<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> expected_output_words[batch_index]]))\n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span> <span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n                    \n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sampled output sentence : <span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>.join([x<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> output]))\n                    f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span> <span class=\"pl-cce\">\\n</span> <span class=\"pl-cce\">\\n</span> <span class=\"pl-pds\">'</span></span>)\n           \n        <span class=\"pl-k\">return</span>(outputs)<span class=\"pl-bu\">`</span></pre></div>", "body_text": "Sure!\nI am working on Language Model, here is my model  :\nclass LstmLm(nn.Module):\n    def __init__(self, args, vocab, padidx):\n        super(LstmLm, self).__init__()\n\n        self.args=args\n        self.nhid=args.nhid\n        self.nlayers=args.nlayers\n        self.dropout=args.dropout\n        self.tieweights=args.tieweights\n        self.vsize = len(vocab.itos)\n        self.padidx=padidx\n        if args.maxnorm==False:\n            maxNorm=None\n        else:\n            maxNorm=args.maxnorm\n        self.lut = nn.Embedding(self.vsize, self.nhid, max_norm=maxNorm)\n        self.rnn = nn.LSTM(\n            input_size=self.nhid,\n            hidden_size=self.nhid,\n            num_layers=self.nlayers,\n            dropout=self.dropout)\n        self.drop = nn.Dropout(self.dropout)\n        self.proj = nn.Linear(self.nhid, self.vsize)\n        self.trainingEpochs=0 #number of epochs already trained\n        self.trainingBatches=0\n        self.temperature=1\n        self.generationIteratorBuilt=False\n        \n        if self.tieweights:\n            # See https://arxiv.org/abs/1608.05859\n            # Seems to improve ppl by 13%.\n            self.proj.weight = self.lut.weight\n        \n        if torch.cuda.is_available():\n            self.cuda()\n\n    def forward(self, input, hid):\n        emb = self.lut(input)\n        hids, hid = self.rnn(emb, hid)\n        # Detach hiddens to truncate the computational graph for BPTT.\n        # Recall that hid = (h,c).\n        return self.proj(self.drop(hids)), tuple(map(lambda x: x.detach(), hid))\n    \n    def next_N_words(self, word, hid_, TEXT, length_to_predict):\n        output=[]\n        \n        for i in range(length_to_predict):\n            wordIndex=TEXT.vocab.stoi[word]\n            input_tensor=Variable(torch.LongTensor([[wordIndex]]))\n            \n            if torch.cuda.is_available():\n                input_tensor=input_tensor.cuda()\n            \n            emb = self.lut(input_tensor)\n            \n            hids, hid_ = self.rnn(emb, hid_)\n            \n            next_logits=self.proj((hids)).view(-1)\n            next_distr=torch.nn.Softmax(dim=0)(torch.mul(next_logits, 1/self.temperature))\n            next_index=torch.multinomial(next_distr,1)\n            \n            next_word=TEXT.vocab.itos[next_index.cpu().data[0]]\n            output+=[next_word]\n            word=next_word\n        return(output)\n\n    def repackage_hidden(self,h):\n        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n        if type(h) == Variable:\n            return Variable(h.data)\n        else:\n            return tuple(self.repackage_hidden(v) for v in h)\n    \n\n    def train_epoch(self, iter, loss, optimizer, viz, win, TEXT, infoToPlot=None):\n        self.train()\n        self.trainingBatches=0\n\n        train_loss = 0\n        nwords = 0\n\n        hid = None\n        batch_id=0\n        for batch in tqdm(iter):\n            self.trainingBatches+=1\n            \n            if hid is not None:\n                hid[0].detach_()\n                hid[1].detach_()\n                \n            optimizer.zero_grad()\n            x = batch.text\n            y = batch.target\n            \n            if torch.cuda.is_available():\n                x=x.cuda()\n                y=y.cuda()\n                \n            out, hid = self(x, hid if hid is not None else None)\n            bloss = loss(out.view(-1, self.vsize), y.view(-1))\n            \n            bloss.backward()\n            train_loss += bloss\n            # bytetensor.sum overflows, so cast to int\n            local_nwords= y.ne(self.padidx).int().sum()\n            nwords += local_nwords\n            if self.args.clip > 0:\n                clip_grad_norm_(self.parameters(), self.args.clip)\n\n            optimizer.step()\n            \n            \n            \n            if not infoToPlot is None:\n                infoToPlot['trainPerp']+=[np.exp(bloss.item()/local_nwords.item())]\n                \n            if batch_id % 100 == 0:\n                sampled_sentences=self.generate_predictions(TEXT)\n                #print(sampled_sentences)\n                infoToPlot['generated']=sampled_sentences\n                #print(infoToPlot['generated'])\n                win = visdom_plot(viz, win, infoToPlot)\n\n                \n            \n            batch_id+=1\n        self.trainingEpochs+=1\n        return train_loss.data[0], nwords.data[0]\n\n    def validate(self, iter, loss, viz=None, win=None, infoToPlot=None):\n        self.eval()\n\n        valid_loss = 0\n        nwords = 0\n\n        hid = None\n        for batch in tqdm(iter):\n            x = batch.text\n            y = batch.target\n            out, hid = self(x, hid if hid is not None else None)\n            valid_loss += loss(out.view(-1, self.vsize), y.view(-1))\n            nwords += y.ne(self.padidx).int().sum()\n            \n        if not infoToPlot is None:\n            infoToPlot['validPerp']+=[np.exp(valid_loss.data[0]/nwords.data[0])]\n            win = visdom_plot(viz, win, infoToPlot, valid=True)\n            \n            \n        return valid_loss.data[0], nwords.data[0]\n\n    def generate_predictions(self, TEXT, outputDirectory=None, epoch=None, saveOutputs=False):\n        if outputDirectory==None:\n            outputDirectory=self.args.directoryData\n        if epoch==None:\n            epoch=self.trainingEpochs\n        \n            \n        self.eval()\n        batch_number=10\n        outputs=[dict({'input_sentence':'','expected_sentence':'','output_sentence':''}) for _ in range(batch_number)]\n        #prepare the data to generate on\n        \n        if not self.generationIteratorBuilt:\n            data = torchtext.datasets.LanguageModelingDataset(\n                path=os.path.join(os.getcwd(),\"data\", \"splitted\", \"smallData\",\"gen.txt\"),\n                text_field=TEXT)\n            data_iter = torchtext.data.BPTTIterator(data, batch_number, 100, device=self.args.devid, train=False)\n            #print()\n            #print(\"Generating the next 80 words, from the 20 first ones\")\n            self.iterator=itertools.cycle(iter(data_iter))\n            self.generationIteratorBuilt=True\n        sample=next(self.iterator)\n        input_sentence=sample.text[:20,:]\n        if torch.cuda.is_available:\n            input_sentence=input_sentence.cuda()\n        expected_sentence=sample.text\n        \n        #we will give the 20 first words of a sentence, and predict the 80 next characters\n        input_words=[[] for _ in range(batch_number)]        \n        for i in range(batch_number):\n                input_words[i]=[TEXT.vocab.itos[x] for x in input_sentence[:,i].data.tolist()]\n                #print(input_words)\n        \n        expected_output_words=[[] for _ in range(batch_number)]        \n        for i in range(batch_number):\n                expected_output_words[i]=[TEXT.vocab.itos[x] for x in expected_sentence[:,i].data.tolist()]\n                #print(output_words)          \n      \n        \n       \n        if saveOutputs:\n            with open(os.path.join(outputDirectory,self.__class__.__name__ + \".preds.txt\"), \"a\") as f:\n                f.write('*' * 20)\n                f.write('\\n \\n NEW : EPOCH {} \\n \\n '.format(epoch))\n                f.write('*' * 20)\n                f.write('\\n')\n            \n        #first we run the model on the first 20 words, in order to give context to the hidden state\n        output_sentence, hidden = self(input_sentence, None)\n            \n        for batch_index in range(batch_number):          \n            \n            #then we run the model using the  computed hidden states\n            input_word=expected_output_words[batch_index][20]\n            \n            #we select the correct hidden state for the batch\n            #because next_N_words take a single word as input\n            local_h=hidden[0][:,batch_index:batch_index+1,:]\n            local_c=hidden[1][:,batch_index:batch_index+1,:]\n            local_hidden=(local_h, local_c)\n            \n            output=self.next_N_words(input_word, local_hidden, TEXT, 80)\n\n            outputs[batch_index]['input_sentence']=''.join([x+' ' for x in input_words[batch_index]])\n            outputs[batch_index]['expected_sentence']=''.join([x+' ' for x in expected_output_words[batch_index]])\n            outputs[batch_index]['output_sentence']=''.join([x+' ' for x in output])\n            \n            if saveOutputs:\n                with open(os.path.join(outputDirectory,self.__class__.__name__ + \".preds.txt\"), \"a\") as f:\n    \n                    f.write('input sentence : \\n')\n                    f.write( ''.join([x+' ' for x in input_words[batch_index]]))\n                    f.write('\\n \\n')\n                    \n                    f.write('expected output sentence : \\n')\n                    f.write(''.join([x+' ' for x in expected_output_words[batch_index]]))\n                    f.write('\\n \\n')\n                    \n                    f.write('sampled output sentence : \\n')\n                    f.write(''.join([x+' ' for x in output]))\n                    f.write('\\n \\n \\n ')\n           \n        return(outputs)`", "body": "Sure! \r\n\r\nI am working on Language Model, here is my model  :  \r\n\r\n```py\r\nclass LstmLm(nn.Module):\r\n    def __init__(self, args, vocab, padidx):\r\n        super(LstmLm, self).__init__()\r\n\r\n        self.args=args\r\n        self.nhid=args.nhid\r\n        self.nlayers=args.nlayers\r\n        self.dropout=args.dropout\r\n        self.tieweights=args.tieweights\r\n        self.vsize = len(vocab.itos)\r\n        self.padidx=padidx\r\n        if args.maxnorm==False:\r\n            maxNorm=None\r\n        else:\r\n            maxNorm=args.maxnorm\r\n        self.lut = nn.Embedding(self.vsize, self.nhid, max_norm=maxNorm)\r\n        self.rnn = nn.LSTM(\r\n            input_size=self.nhid,\r\n            hidden_size=self.nhid,\r\n            num_layers=self.nlayers,\r\n            dropout=self.dropout)\r\n        self.drop = nn.Dropout(self.dropout)\r\n        self.proj = nn.Linear(self.nhid, self.vsize)\r\n        self.trainingEpochs=0 #number of epochs already trained\r\n        self.trainingBatches=0\r\n        self.temperature=1\r\n        self.generationIteratorBuilt=False\r\n        \r\n        if self.tieweights:\r\n            # See https://arxiv.org/abs/1608.05859\r\n            # Seems to improve ppl by 13%.\r\n            self.proj.weight = self.lut.weight\r\n        \r\n        if torch.cuda.is_available():\r\n            self.cuda()\r\n\r\n    def forward(self, input, hid):\r\n        emb = self.lut(input)\r\n        hids, hid = self.rnn(emb, hid)\r\n        # Detach hiddens to truncate the computational graph for BPTT.\r\n        # Recall that hid = (h,c).\r\n        return self.proj(self.drop(hids)), tuple(map(lambda x: x.detach(), hid))\r\n    \r\n    def next_N_words(self, word, hid_, TEXT, length_to_predict):\r\n        output=[]\r\n        \r\n        for i in range(length_to_predict):\r\n            wordIndex=TEXT.vocab.stoi[word]\r\n            input_tensor=Variable(torch.LongTensor([[wordIndex]]))\r\n            \r\n            if torch.cuda.is_available():\r\n                input_tensor=input_tensor.cuda()\r\n            \r\n            emb = self.lut(input_tensor)\r\n            \r\n            hids, hid_ = self.rnn(emb, hid_)\r\n            \r\n            next_logits=self.proj((hids)).view(-1)\r\n            next_distr=torch.nn.Softmax(dim=0)(torch.mul(next_logits, 1/self.temperature))\r\n            next_index=torch.multinomial(next_distr,1)\r\n            \r\n            next_word=TEXT.vocab.itos[next_index.cpu().data[0]]\r\n            output+=[next_word]\r\n            word=next_word\r\n        return(output)\r\n\r\n    def repackage_hidden(self,h):\r\n        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\r\n        if type(h) == Variable:\r\n            return Variable(h.data)\r\n        else:\r\n            return tuple(self.repackage_hidden(v) for v in h)\r\n    \r\n\r\n    def train_epoch(self, iter, loss, optimizer, viz, win, TEXT, infoToPlot=None):\r\n        self.train()\r\n        self.trainingBatches=0\r\n\r\n        train_loss = 0\r\n        nwords = 0\r\n\r\n        hid = None\r\n        batch_id=0\r\n        for batch in tqdm(iter):\r\n            self.trainingBatches+=1\r\n            \r\n            if hid is not None:\r\n                hid[0].detach_()\r\n                hid[1].detach_()\r\n                \r\n            optimizer.zero_grad()\r\n            x = batch.text\r\n            y = batch.target\r\n            \r\n            if torch.cuda.is_available():\r\n                x=x.cuda()\r\n                y=y.cuda()\r\n                \r\n            out, hid = self(x, hid if hid is not None else None)\r\n            bloss = loss(out.view(-1, self.vsize), y.view(-1))\r\n            \r\n            bloss.backward()\r\n            train_loss += bloss\r\n            # bytetensor.sum overflows, so cast to int\r\n            local_nwords= y.ne(self.padidx).int().sum()\r\n            nwords += local_nwords\r\n            if self.args.clip > 0:\r\n                clip_grad_norm_(self.parameters(), self.args.clip)\r\n\r\n            optimizer.step()\r\n            \r\n            \r\n            \r\n            if not infoToPlot is None:\r\n                infoToPlot['trainPerp']+=[np.exp(bloss.item()/local_nwords.item())]\r\n                \r\n            if batch_id % 100 == 0:\r\n                sampled_sentences=self.generate_predictions(TEXT)\r\n                #print(sampled_sentences)\r\n                infoToPlot['generated']=sampled_sentences\r\n                #print(infoToPlot['generated'])\r\n                win = visdom_plot(viz, win, infoToPlot)\r\n\r\n                \r\n            \r\n            batch_id+=1\r\n        self.trainingEpochs+=1\r\n        return train_loss.data[0], nwords.data[0]\r\n\r\n    def validate(self, iter, loss, viz=None, win=None, infoToPlot=None):\r\n        self.eval()\r\n\r\n        valid_loss = 0\r\n        nwords = 0\r\n\r\n        hid = None\r\n        for batch in tqdm(iter):\r\n            x = batch.text\r\n            y = batch.target\r\n            out, hid = self(x, hid if hid is not None else None)\r\n            valid_loss += loss(out.view(-1, self.vsize), y.view(-1))\r\n            nwords += y.ne(self.padidx).int().sum()\r\n            \r\n        if not infoToPlot is None:\r\n            infoToPlot['validPerp']+=[np.exp(valid_loss.data[0]/nwords.data[0])]\r\n            win = visdom_plot(viz, win, infoToPlot, valid=True)\r\n            \r\n            \r\n        return valid_loss.data[0], nwords.data[0]\r\n\r\n    def generate_predictions(self, TEXT, outputDirectory=None, epoch=None, saveOutputs=False):\r\n        if outputDirectory==None:\r\n            outputDirectory=self.args.directoryData\r\n        if epoch==None:\r\n            epoch=self.trainingEpochs\r\n        \r\n            \r\n        self.eval()\r\n        batch_number=10\r\n        outputs=[dict({'input_sentence':'','expected_sentence':'','output_sentence':''}) for _ in range(batch_number)]\r\n        #prepare the data to generate on\r\n        \r\n        if not self.generationIteratorBuilt:\r\n            data = torchtext.datasets.LanguageModelingDataset(\r\n                path=os.path.join(os.getcwd(),\"data\", \"splitted\", \"smallData\",\"gen.txt\"),\r\n                text_field=TEXT)\r\n            data_iter = torchtext.data.BPTTIterator(data, batch_number, 100, device=self.args.devid, train=False)\r\n            #print()\r\n            #print(\"Generating the next 80 words, from the 20 first ones\")\r\n            self.iterator=itertools.cycle(iter(data_iter))\r\n            self.generationIteratorBuilt=True\r\n        sample=next(self.iterator)\r\n        input_sentence=sample.text[:20,:]\r\n        if torch.cuda.is_available:\r\n            input_sentence=input_sentence.cuda()\r\n        expected_sentence=sample.text\r\n        \r\n        #we will give the 20 first words of a sentence, and predict the 80 next characters\r\n        input_words=[[] for _ in range(batch_number)]        \r\n        for i in range(batch_number):\r\n                input_words[i]=[TEXT.vocab.itos[x] for x in input_sentence[:,i].data.tolist()]\r\n                #print(input_words)\r\n        \r\n        expected_output_words=[[] for _ in range(batch_number)]        \r\n        for i in range(batch_number):\r\n                expected_output_words[i]=[TEXT.vocab.itos[x] for x in expected_sentence[:,i].data.tolist()]\r\n                #print(output_words)          \r\n      \r\n        \r\n       \r\n        if saveOutputs:\r\n            with open(os.path.join(outputDirectory,self.__class__.__name__ + \".preds.txt\"), \"a\") as f:\r\n                f.write('*' * 20)\r\n                f.write('\\n \\n NEW : EPOCH {} \\n \\n '.format(epoch))\r\n                f.write('*' * 20)\r\n                f.write('\\n')\r\n            \r\n        #first we run the model on the first 20 words, in order to give context to the hidden state\r\n        output_sentence, hidden = self(input_sentence, None)\r\n            \r\n        for batch_index in range(batch_number):          \r\n            \r\n            #then we run the model using the  computed hidden states\r\n            input_word=expected_output_words[batch_index][20]\r\n            \r\n            #we select the correct hidden state for the batch\r\n            #because next_N_words take a single word as input\r\n            local_h=hidden[0][:,batch_index:batch_index+1,:]\r\n            local_c=hidden[1][:,batch_index:batch_index+1,:]\r\n            local_hidden=(local_h, local_c)\r\n            \r\n            output=self.next_N_words(input_word, local_hidden, TEXT, 80)\r\n\r\n            outputs[batch_index]['input_sentence']=''.join([x+' ' for x in input_words[batch_index]])\r\n            outputs[batch_index]['expected_sentence']=''.join([x+' ' for x in expected_output_words[batch_index]])\r\n            outputs[batch_index]['output_sentence']=''.join([x+' ' for x in output])\r\n            \r\n            if saveOutputs:\r\n                with open(os.path.join(outputDirectory,self.__class__.__name__ + \".preds.txt\"), \"a\") as f:\r\n    \r\n                    f.write('input sentence : \\n')\r\n                    f.write( ''.join([x+' ' for x in input_words[batch_index]]))\r\n                    f.write('\\n \\n')\r\n                    \r\n                    f.write('expected output sentence : \\n')\r\n                    f.write(''.join([x+' ' for x in expected_output_words[batch_index]]))\r\n                    f.write('\\n \\n')\r\n                    \r\n                    f.write('sampled output sentence : \\n')\r\n                    f.write(''.join([x+' ' for x in output]))\r\n                    f.write('\\n \\n \\n ')\r\n           \r\n        return(outputs)`"}