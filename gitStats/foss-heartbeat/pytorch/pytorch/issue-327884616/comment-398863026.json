{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398863026", "html_url": "https://github.com/pytorch/pytorch/issues/7961#issuecomment-398863026", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7961", "id": 398863026, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODg2MzAyNg==", "user": {"login": "simon555", "id": 29953850, "node_id": "MDQ6VXNlcjI5OTUzODUw", "avatar_url": "https://avatars0.githubusercontent.com/u/29953850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simon555", "html_url": "https://github.com/simon555", "followers_url": "https://api.github.com/users/simon555/followers", "following_url": "https://api.github.com/users/simon555/following{/other_user}", "gists_url": "https://api.github.com/users/simon555/gists{/gist_id}", "starred_url": "https://api.github.com/users/simon555/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simon555/subscriptions", "organizations_url": "https://api.github.com/users/simon555/orgs", "repos_url": "https://api.github.com/users/simon555/repos", "events_url": "https://api.github.com/users/simon555/events{/privacy}", "received_events_url": "https://api.github.com/users/simon555/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-20T19:08:52Z", "updated_at": "2018-06-20T19:10:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a><br>\nI get the same error when I use the train() mode :</p>\n<p>def train_epoch(self, iter, loss, optimizer, viz, win, TEXT, infoToPlot=None):<br>\nself.train()<br>\nself.trainingBatches=0</p>\n<pre><code>    train_loss = 0\n    nwords = 0\n\n    hid = None\n    batch_id=0\n    for batch in tqdm(iter):\n        self.trainingBatches+=1\n        \n        if hid is not None:\n            hid[0].detach_()\n            hid[1].detach_()\n            \n        optimizer.zero_grad()\n        x = batch.text\n        y = batch.target\n        \n        if torch.cuda.is_available():\n            x=x.cuda()\n            y=y.cuda()\n            \n        out, hid = self(x, hid if hid is not None else None)\n        bloss = loss(out.view(-1, self.vsize), y.view(-1))\n        \n        bloss.backward()\n        train_loss += bloss\n        # bytetensor.sum overflows, so cast to int\n        local_nwords= y.ne(self.padidx).int().sum()\n        nwords += local_nwords\n        if self.args.clip &gt; 0:\n            clip_grad_norm_(self.parameters(), self.args.clip)\n\n        optimizer.step()\n</code></pre>\n<p>error message :<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)</p>\n<p>File \"c:\\users\\simon\\anaconda3\\lib\\site-packages\\torch\\autograd_<em>init</em>_.py\", line 89, in backward<br>\nallow_unreachable=True)  # allow_unreachable flag</p>\n<p>RuntimeError: backward_input can only be called in training mode</p>\n<p>What is weird is that the code works on CPU but not on GPU...</p>", "body_text": "@SsnL\nI get the same error when I use the train() mode :\ndef train_epoch(self, iter, loss, optimizer, viz, win, TEXT, infoToPlot=None):\nself.train()\nself.trainingBatches=0\n    train_loss = 0\n    nwords = 0\n\n    hid = None\n    batch_id=0\n    for batch in tqdm(iter):\n        self.trainingBatches+=1\n        \n        if hid is not None:\n            hid[0].detach_()\n            hid[1].detach_()\n            \n        optimizer.zero_grad()\n        x = batch.text\n        y = batch.target\n        \n        if torch.cuda.is_available():\n            x=x.cuda()\n            y=y.cuda()\n            \n        out, hid = self(x, hid if hid is not None else None)\n        bloss = loss(out.view(-1, self.vsize), y.view(-1))\n        \n        bloss.backward()\n        train_loss += bloss\n        # bytetensor.sum overflows, so cast to int\n        local_nwords= y.ne(self.padidx).int().sum()\n        nwords += local_nwords\n        if self.args.clip > 0:\n            clip_grad_norm_(self.parameters(), self.args.clip)\n\n        optimizer.step()\n\nerror message :\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)\nFile \"c:\\users\\simon\\anaconda3\\lib\\site-packages\\torch\\autograd_init_.py\", line 89, in backward\nallow_unreachable=True)  # allow_unreachable flag\nRuntimeError: backward_input can only be called in training mode\nWhat is weird is that the code works on CPU but not on GPU...", "body": "@SsnL \r\nI get the same error when I use the train() mode : \r\n\r\ndef train_epoch(self, iter, loss, optimizer, viz, win, TEXT, infoToPlot=None):\r\n        self.train()\r\n        self.trainingBatches=0\r\n\r\n        train_loss = 0\r\n        nwords = 0\r\n\r\n        hid = None\r\n        batch_id=0\r\n        for batch in tqdm(iter):\r\n            self.trainingBatches+=1\r\n            \r\n            if hid is not None:\r\n                hid[0].detach_()\r\n                hid[1].detach_()\r\n                \r\n            optimizer.zero_grad()\r\n            x = batch.text\r\n            y = batch.target\r\n            \r\n            if torch.cuda.is_available():\r\n                x=x.cuda()\r\n                y=y.cuda()\r\n                \r\n            out, hid = self(x, hid if hid is not None else None)\r\n            bloss = loss(out.view(-1, self.vsize), y.view(-1))\r\n            \r\n            bloss.backward()\r\n            train_loss += bloss\r\n            # bytetensor.sum overflows, so cast to int\r\n            local_nwords= y.ne(self.padidx).int().sum()\r\n            nwords += local_nwords\r\n            if self.args.clip > 0:\r\n                clip_grad_norm_(self.parameters(), self.args.clip)\r\n\r\n            optimizer.step()\r\n\r\n\r\nerror message : \r\n torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n\r\n  File \"c:\\users\\simon\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 89, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n\r\nRuntimeError: backward_input can only be called in training mode\r\n\r\n\r\nWhat is weird is that the code works on CPU but not on GPU..."}