{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337549641", "html_url": "https://github.com/pytorch/pytorch/issues/3158#issuecomment-337549641", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3158", "id": 337549641, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzU0OTY0MQ==", "user": {"login": "chivee", "id": 2283057, "node_id": "MDQ6VXNlcjIyODMwNTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2283057?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chivee", "html_url": "https://github.com/chivee", "followers_url": "https://api.github.com/users/chivee/followers", "following_url": "https://api.github.com/users/chivee/following{/other_user}", "gists_url": "https://api.github.com/users/chivee/gists{/gist_id}", "starred_url": "https://api.github.com/users/chivee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chivee/subscriptions", "organizations_url": "https://api.github.com/users/chivee/orgs", "repos_url": "https://api.github.com/users/chivee/repos", "events_url": "https://api.github.com/users/chivee/events{/privacy}", "received_events_url": "https://api.github.com/users/chivee/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-18T10:48:43Z", "updated_at": "2017-10-18T10:48:43Z", "author_association": "NONE", "body_html": "<p>I'm looking for element-wise product. I'm misunderstood that  <a href=\"http://docs.nvidia.com/cuda/cusparse/index.html#cusparse-lt-t-gt-csrmm\" rel=\"nofollow\">this fuction</a> was describing an element-wise production.</p>\n<p>for <code>Dense[m,1] * Sparse[m,n] -&gt; Sparse[m,n]</code> and <code>Sparse[m,n] * Dense[1,n] -&gt; Sparse[m,n]</code> mainly have two issues:<br>\n1. SparseTensor doesn't offer cmul(DenseTensor), which can be solved by convert Dense to Sparse first.<br>\n2. Sparse * Sparse required the two tensor have <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THCS/generic/THCSTensorMath.cu#L411\">exactly same size </a>. To my understanding if two tensor was broadcastable, that should also work?</p>\n<p>and for <code>Sparse[m*n] * Dense[b,1,n] -&gt; Sparse[b,m,n]</code>. If cuSPARSE didn't offer something like <code>bspaddcmul </code>I'm guessing I can only using an for loop on Dense[b,1,n] right?</p>", "body_text": "I'm looking for element-wise product. I'm misunderstood that  this fuction was describing an element-wise production.\nfor Dense[m,1] * Sparse[m,n] -> Sparse[m,n] and Sparse[m,n] * Dense[1,n] -> Sparse[m,n] mainly have two issues:\n1. SparseTensor doesn't offer cmul(DenseTensor), which can be solved by convert Dense to Sparse first.\n2. Sparse * Sparse required the two tensor have exactly same size . To my understanding if two tensor was broadcastable, that should also work?\nand for Sparse[m*n] * Dense[b,1,n] -> Sparse[b,m,n]. If cuSPARSE didn't offer something like bspaddcmul I'm guessing I can only using an for loop on Dense[b,1,n] right?", "body": "I'm looking for element-wise product. I'm misunderstood that  [this fuction](http://docs.nvidia.com/cuda/cusparse/index.html#cusparse-lt-t-gt-csrmm) was describing an element-wise production. \r\n\r\nfor `Dense[m,1] * Sparse[m,n] -> Sparse[m,n]` and `Sparse[m,n] * Dense[1,n] -> Sparse[m,n]` mainly have two issues:\r\n         1. SparseTensor doesn't offer cmul(DenseTensor), which can be solved by convert Dense to Sparse first.\r\n         2. Sparse * Sparse required the two tensor have [exactly same size ](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCS/generic/THCSTensorMath.cu#L411). To my understanding if two tensor was broadcastable, that should also work?\r\n\r\nand for `Sparse[m*n] * Dense[b,1,n] -> Sparse[b,m,n]`. If cuSPARSE didn't offer something like `bspaddcmul `I'm guessing I can only using an for loop on Dense[b,1,n] right?"}