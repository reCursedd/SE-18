{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337557571", "html_url": "https://github.com/pytorch/pytorch/issues/3158#issuecomment-337557571", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3158", "id": 337557571, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzU1NzU3MQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-18T11:15:43Z", "updated_at": "2017-10-18T11:15:43Z", "author_association": "MEMBER", "body_html": "<ol>\n<li>Converting dense tensors to sparse is a bad idea. It will take a lot more memory than the original dense tensor and will be extremely slow. We should write specialized kernels for this.</li>\n<li>That's true, although I don't think our current broadcasting code supports sparse tensors. In general, broadcasting for sparse tensors is much harder and more expensive than for dense tensors, because you actually need to repeat all the values, making the tensor much larger</li>\n</ol>", "body_text": "Converting dense tensors to sparse is a bad idea. It will take a lot more memory than the original dense tensor and will be extremely slow. We should write specialized kernels for this.\nThat's true, although I don't think our current broadcasting code supports sparse tensors. In general, broadcasting for sparse tensors is much harder and more expensive than for dense tensors, because you actually need to repeat all the values, making the tensor much larger", "body": "1. Converting dense tensors to sparse is a bad idea. It will take a lot more memory than the original dense tensor and will be extremely slow. We should write specialized kernels for this.\r\n2. That's true, although I don't think our current broadcasting code supports sparse tensors. In general, broadcasting for sparse tensors is much harder and more expensive than for dense tensors, because you actually need to repeat all the values, making the tensor much larger"}