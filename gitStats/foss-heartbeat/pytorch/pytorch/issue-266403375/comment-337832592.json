{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337832592", "html_url": "https://github.com/pytorch/pytorch/issues/3158#issuecomment-337832592", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3158", "id": 337832592, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzgzMjU5Mg==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-19T08:10:49Z", "updated_at": "2017-10-19T08:10:49Z", "author_association": "MEMBER", "body_html": "<p>I don't think we'll be supporting other formats. They require new kernels for all math operations, and it's a lot of code to write and maintain.</p>\n<p>That's true, but only if you special case the \"broadcasted\" operators in the backend. There's no way to easily \"expand\" the sparse tensor outside of a math method. It's because all non-zero values have to be listed explicitly, and expanding actually adds more non-zero values.</p>\n<p>No, that's not true. We're using OpenMP in our code, which makes it very easy to paralellize math operations</p>", "body_text": "I don't think we'll be supporting other formats. They require new kernels for all math operations, and it's a lot of code to write and maintain.\nThat's true, but only if you special case the \"broadcasted\" operators in the backend. There's no way to easily \"expand\" the sparse tensor outside of a math method. It's because all non-zero values have to be listed explicitly, and expanding actually adds more non-zero values.\nNo, that's not true. We're using OpenMP in our code, which makes it very easy to paralellize math operations", "body": "I don't think we'll be supporting other formats. They require new kernels for all math operations, and it's a lot of code to write and maintain.\r\n\r\nThat's true, but only if you special case the \"broadcasted\" operators in the backend. There's no way to easily \"expand\" the sparse tensor outside of a math method. It's because all non-zero values have to be listed explicitly, and expanding actually adds more non-zero values.\r\n\r\nNo, that's not true. We're using OpenMP in our code, which makes it very easy to paralellize math operations"}