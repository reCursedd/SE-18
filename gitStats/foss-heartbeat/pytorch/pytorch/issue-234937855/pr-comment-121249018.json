{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121249018", "pull_request_review_id": 43289594, "id": 121249018, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMTI0OTAxOA==", "diff_hunk": "@@ -0,0 +1,82 @@\n+from copy import deepcopy\n+from . import CWrapPlugin\n+import yaml\n+\n+\n+class ProcessorSpecificPlugin(CWrapPlugin):\n+\n+    def process_declarations(self, declarations):\n+        # In order to move certain random functions into the same cwrap\n+        # declaration, we need to be able to handle the fact that on the CPU\n+        # these functions take a generator argument, while on the GPU, they\n+        # do not. As such, we would like to split those declarations at cwrap\n+        # runtime into two separate declarations, one for the CPU (unchanged),\n+        # and one for the GPU (with the generator argument removed).\n+\n+        def arg_contains_generator(arg):\n+            return (arg['type'] == 'THGenerator*' or (arg.get('default', None)\n+                    is not None and 'THPDefaultGenerator' in\n+                    str(arg.get('default', \"\"))))\n+\n+        def split_candidate(declaration):\n+            # First, check and see if it is a declaration for both CPU/GPU\n+            if all([proc in declaration['processors'] for\n+                    proc in ['CPU', 'CUDA']]):\n+                for option in declaration['options']:\n+                    for argument in option['arguments']:\n+                        if arg_contains_generator(argument):\n+                            return True\n+\n+            return False\n+\n+        def can_we_handle_the_split(declaration):\n+            # hook into here if the split cannot happen for some reason\n+            return True\n+\n+        def generator_split(declaration):\n+            # the split must make two changes: 1. remove the generator argument\n+            # for the GPU, and 2. assign the correct processors/types to the\n+            # split declaration\n+            dec_cpu = declaration\n+            dec_gpu = deepcopy(declaration)\n+\n+            # Remove GPU processor and types from dec_cpu\n+            dec_cpu['processors'].remove('CUDA')\n+            if dec_cpu.get('type_processor_pairs', False):\n+                dec_cpu['type_processor_pairs'] = (\n+                    [pair for pair in dec_cpu['type_processor_pairs'] if\n+                     pair[1] == 'CPU'])\n+            # also need to reach into options\n+            for option in dec_cpu['options']:\n+                option['processors'].remove('CUDA')\n+\n+            # Remove CPU processor and types from dec_gpu\n+            dec_gpu['processors'].remove('CPU')\n+            if dec_gpu.get('type_processor_pairs', False):\n+                dec_gpu['type_processor_pairs'] = (\n+                    [pair for pair in dec_gpu['type_processor_pairs'] if\n+                     pair[1] == 'CUDA'])\n+            # also need to reach into options\n+            for option in dec_gpu['options']:\n+                option['processors'].remove('CPU')\n+\n+            # Remove generator arguments from dec_gpu options\n+            for option in dec_gpu['options']:\n+                option['arguments'] = (\n+                    [arg for arg in option['arguments'] if\n+                     not arg_contains_generator(arg)])\n+\n+            return [dec_cpu, dec_gpu]\n+\n+        decs = []\n+        for declaration in declarations:\n+            if split_candidate(declaration):\n+                assert(can_we_handle_the_split(declaration))\n+                newdecs = generator_split(declaration)\n+                if 'geometric' in declaration['name']:\n+                    print(yaml.dump(newdecs))", "path": "tools/cwrap/plugins/ProcessorSpecificPlugin.py", "position": null, "original_position": 77, "commit_id": "dadf12e892b69bca8457a4be963fece8328f2393", "original_commit_id": "a5c86f848213f4877fae40e248a0b7b20d25a458", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "debug!", "created_at": "2017-06-10T04:02:25Z", "updated_at": "2018-11-23T15:33:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/1766#discussion_r121249018", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1766", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121249018"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1766#discussion_r121249018"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1766"}}, "body_html": "<p>debug!</p>", "body_text": "debug!"}