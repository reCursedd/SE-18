{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430479841", "html_url": "https://github.com/pytorch/pytorch/issues/12248#issuecomment-430479841", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12248", "id": 430479841, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDQ3OTg0MQ==", "user": {"login": "oleg-trott", "id": 2914939, "node_id": "MDQ6VXNlcjI5MTQ5Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2914939?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oleg-trott", "html_url": "https://github.com/oleg-trott", "followers_url": "https://api.github.com/users/oleg-trott/followers", "following_url": "https://api.github.com/users/oleg-trott/following{/other_user}", "gists_url": "https://api.github.com/users/oleg-trott/gists{/gist_id}", "starred_url": "https://api.github.com/users/oleg-trott/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oleg-trott/subscriptions", "organizations_url": "https://api.github.com/users/oleg-trott/orgs", "repos_url": "https://api.github.com/users/oleg-trott/repos", "events_url": "https://api.github.com/users/oleg-trott/events{/privacy}", "received_events_url": "https://api.github.com/users/oleg-trott/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T03:47:43Z", "updated_at": "2018-10-17T03:47:43Z", "author_association": "NONE", "body_html": "<p>Parameters are returned by nn.Module.parameters() to be given to the optimizer.</p>\n<p>On the other hand, buffers are updated by the users in an arbitrary fashion. Sometimes these updates don't need the gradient (<em>e.g.</em> <code>running_mean</code> in <code>BatchNorm</code>) Other times, they do. I can give a good example, but it's rather math-heavy.</p>", "body_text": "Parameters are returned by nn.Module.parameters() to be given to the optimizer.\nOn the other hand, buffers are updated by the users in an arbitrary fashion. Sometimes these updates don't need the gradient (e.g. running_mean in BatchNorm) Other times, they do. I can give a good example, but it's rather math-heavy.", "body": "Parameters are returned by nn.Module.parameters() to be given to the optimizer.\r\n\r\nOn the other hand, buffers are updated by the users in an arbitrary fashion. Sometimes these updates don't need the gradient (*e.g.* `running_mean` in `BatchNorm`) Other times, they do. I can give a good example, but it's rather math-heavy.\r\n\r\n\r\n"}