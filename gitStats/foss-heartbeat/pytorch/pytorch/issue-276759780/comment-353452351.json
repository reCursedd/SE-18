{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/353452351", "html_url": "https://github.com/pytorch/pytorch/pull/3875#issuecomment-353452351", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3875", "id": 353452351, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzQ1MjM1MQ==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-21T20:39:03Z", "updated_at": "2017-12-21T20:39:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Adam brings up a good point; I don't have a strong immediate feeling one way or another so let's try to look at different use cases:<br>\nFirst, the most common use of these functions will probably be in preparing data. There, code often (but not always) operates on sequences of token IDs (which have no feature dimension) rather than sequences of feature vectors. When that's the case existing code sometimes performs padding directly from a sequence of Python lists. So it would definitely be helpful to make sure these functions work on data with no feature dimension, and it could potentially also be worthwhile to make them work on inputs whose time dimension is encoded as a list.<br>\nOther than that, people writing dataloaders are likely to marginally prefer providing a sequence of <code>L x F</code> tensors to having to unsqueeze each of them (the tensors might be coming from numpy or a scikit-learn dataloader, where PyTorch's convention of using a batch-size-one tensor to denote a single example is somewhat foreign).<br>\nThe other use that I can think of is people either writing complicated manually-batched networks where flow switches back and forth between sections batched in different ways or people writing libraries that do the same thing automatically. While most of the time these people will actually want to use <code>pad_packed_sequence</code> and <code>pack_padded_sequence</code>, they'd want to use these functions if they have completely unbatched sections of code. And within those sections they'd want to use PyTorch-style unbatched tensors, i.e. tensors with batch size 1.<br>\nSo there are reasons for each choice but the reasons for stacking are probably less esoteric and more frequently encountered.</p>", "body_text": "Adam brings up a good point; I don't have a strong immediate feeling one way or another so let's try to look at different use cases:\nFirst, the most common use of these functions will probably be in preparing data. There, code often (but not always) operates on sequences of token IDs (which have no feature dimension) rather than sequences of feature vectors. When that's the case existing code sometimes performs padding directly from a sequence of Python lists. So it would definitely be helpful to make sure these functions work on data with no feature dimension, and it could potentially also be worthwhile to make them work on inputs whose time dimension is encoded as a list.\nOther than that, people writing dataloaders are likely to marginally prefer providing a sequence of L x F tensors to having to unsqueeze each of them (the tensors might be coming from numpy or a scikit-learn dataloader, where PyTorch's convention of using a batch-size-one tensor to denote a single example is somewhat foreign).\nThe other use that I can think of is people either writing complicated manually-batched networks where flow switches back and forth between sections batched in different ways or people writing libraries that do the same thing automatically. While most of the time these people will actually want to use pad_packed_sequence and pack_padded_sequence, they'd want to use these functions if they have completely unbatched sections of code. And within those sections they'd want to use PyTorch-style unbatched tensors, i.e. tensors with batch size 1.\nSo there are reasons for each choice but the reasons for stacking are probably less esoteric and more frequently encountered.", "body": "Adam brings up a good point; I don't have a strong immediate feeling one way or another so let's try to look at different use cases:\r\nFirst, the most common use of these functions will probably be in preparing data. There, code often (but not always) operates on sequences of token IDs (which have no feature dimension) rather than sequences of feature vectors. When that's the case existing code sometimes performs padding directly from a sequence of Python lists. So it would definitely be helpful to make sure these functions work on data with no feature dimension, and it could potentially also be worthwhile to make them work on inputs whose time dimension is encoded as a list.\r\nOther than that, people writing dataloaders are likely to marginally prefer providing a sequence of `L x F` tensors to having to unsqueeze each of them (the tensors might be coming from numpy or a scikit-learn dataloader, where PyTorch's convention of using a batch-size-one tensor to denote a single example is somewhat foreign).\r\nThe other use that I can think of is people either writing complicated manually-batched networks where flow switches back and forth between sections batched in different ways or people writing libraries that do the same thing automatically. While most of the time these people will actually want to use `pad_packed_sequence` and `pack_padded_sequence`, they'd want to use these functions if they have completely unbatched sections of code. And within those sections they'd want to use PyTorch-style unbatched tensors, i.e. tensors with batch size 1.\r\nSo there are reasons for each choice but the reasons for stacking are probably less esoteric and more frequently encountered."}