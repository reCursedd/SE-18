{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153145272", "pull_request_review_id": 79107367, "id": 153145272, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzE0NTI3Mg==", "diff_hunk": "@@ -123,3 +124,106 @@ def pad_packed_sequence(sequence, batch_first=False, padding_value=0.0):\n     if batch_first:\n         output = output.transpose(0, 1)\n     return output, lengths\n+\n+\n+def pad_sequence(sequences, lengths, batch_first=False):\n+    r\"\"\"Pad a list of variable length Variables with zero\n+\n+    The ``pad_sequence`` pads the list of Variables on zeroth dimension and\n+    stack all the sequences on zeroth dimension. For example, if the input is\n+    list of sequences with size `` Lx*`` and if batch_first is False, the\n+    output will be of size `` TxBx* `` and if batch_first is True,\n+    output will be of size ``BxTx* ``. The ``pad_sequence`` accepts list of\n+    sequences and its lengths which should be sorted in decreasing order\n+\n+    B is batch size\n+    T is length longest sequence\n+    L is length of the sequence\n+    * is any trailing dimension including zero\n+\n+    >>> from torch.nn.utils.rnn import pad_sequence\n+    >>> a = Variable(torch.ones(25, 300))\n+    >>> b = Variable(torch.ones(15, 300))\n+    >>> c = Variable(torch.ones(22, 300))\n+    >>> pad_sequence([a, b, c], [25, 15, 22]).size()\n+    torch.Size([25, 3, 300])\n+\n+    Note:\n+        This function returns a Variable of size LxBx* or BxLx* where L is the\n+        length of longest sequence (lengths[0])\n+\n+    Arguments:\n+        sequences (list(Variable)): list of variable length sequences.\n+        lengths (list[int]): list of sequences lengths of each batch element.\n+        batch_first (bool, optional): if True, the input is expected in Bx*x*\n+            format.\n+\n+    Returns:\n+        Variable of size ``seq_len x len(sequences) x * ``\n+            if batch_first = False\n+        Variable of size ``len(sequences) x seq_len x * `` otherwise\n+    \"\"\"\n+\n+    if len(lengths) != len(sequences):\n+        raise ValueError(\"number of elements in lengths and sequences didn't match\")\n+\n+    out_variable = []", "path": "torch/nn/utils/rnn.py", "position": null, "original_position": 53, "commit_id": "7bdd58e4ab2ef58e72c2d1b5fa62f3dc0f511cf6", "original_commit_id": "d42f5be718866b77f9b93940b60ecd7a75fb668c", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "It kind of will, but not completely - the tensor will be on a GPU, but on the \"current GPU\" set in the context, not the one of the inputs. You have to use `.new` if you want to retain the device.", "created_at": "2017-11-27T09:39:35Z", "updated_at": "2018-11-23T15:36:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/3875#discussion_r153145272", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3875", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153145272"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3875#discussion_r153145272"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3875"}}, "body_html": "<p>It kind of will, but not completely - the tensor will be on a GPU, but on the \"current GPU\" set in the context, not the one of the inputs. You have to use <code>.new</code> if you want to retain the device.</p>", "body_text": "It kind of will, but not completely - the tensor will be on a GPU, but on the \"current GPU\" set in the context, not the one of the inputs. You have to use .new if you want to retain the device.", "in_reply_to_id": 153080008}