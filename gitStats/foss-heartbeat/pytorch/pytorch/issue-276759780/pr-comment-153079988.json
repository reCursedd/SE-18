{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153079988", "pull_request_review_id": 79036021, "id": 153079988, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzA3OTk4OA==", "diff_hunk": "@@ -123,3 +124,106 @@ def pad_packed_sequence(sequence, batch_first=False, padding_value=0.0):\n     if batch_first:\n         output = output.transpose(0, 1)\n     return output, lengths\n+\n+\n+def pad_sequence(sequences, lengths, batch_first=False):\n+    r\"\"\"Pad a list of variable length Variables with zero\n+\n+    The ``pad_sequence`` pads the list of Variables on zeroth dimension and\n+    stack all the sequences on zeroth dimension. For example, if the input is\n+    list of sequences with size `` Lx*`` and if batch_first is False, the\n+    output will be of size `` TxBx* `` and if batch_first is True,\n+    output will be of size ``BxTx* ``. The ``pad_sequence`` accepts list of\n+    sequences and its lengths which should be sorted in decreasing order\n+\n+    B is batch size\n+    T is length longest sequence\n+    L is length of the sequence\n+    * is any trailing dimension including zero\n+\n+    >>> from torch.nn.utils.rnn import pad_sequence\n+    >>> a = Variable(torch.ones(25, 300))\n+    >>> b = Variable(torch.ones(15, 300))\n+    >>> c = Variable(torch.ones(22, 300))\n+    >>> pad_sequence([a, b, c], [25, 15, 22]).size()\n+    torch.Size([25, 3, 300])\n+\n+    Note:\n+        This function returns a Variable of size LxBx* or BxLx* where L is the\n+        length of longest sequence (lengths[0])\n+\n+    Arguments:\n+        sequences (list(Variable)): list of variable length sequences.\n+        lengths (list[int]): list of sequences lengths of each batch element.\n+        batch_first (bool, optional): if True, the input is expected in Bx*x*\n+            format.\n+\n+    Returns:\n+        Variable of size ``seq_len x len(sequences) x * ``\n+            if batch_first = False\n+        Variable of size ``len(sequences) x seq_len x * `` otherwise\n+    \"\"\"\n+\n+    if len(lengths) != len(sequences):\n+        raise ValueError(\"number of elements in lengths and sequences didn't match\")\n+\n+    out_variable = []\n+    max_len = lengths[0]\n+    prev_l = lengths[0]\n+    for variable, length in zip(sequences, lengths):\n+        # temperory sort check, will be removed when we handle sorting internally\n+        if prev_l < length:\n+                raise ValueError(\"lengths array has to be sorted in decreasing order\")\n+        prev_l = length\n+        if length < max_len:\n+            prev_l = length\n+            padding_shape = [lengths[0] - length] + list(variable.size()[1:])\n+            filler = Variable(torch.Tensor(*padding_shape).type_as(variable.data).zero_())\n+            out_variable.append(torch.cat((variable, filler)))\n+        else:\n+            out_variable.append(variable)\n+    if batch_first:\n+        return torch.stack(out_variable)\n+    else:\n+        return torch.stack(out_variable).transpose(0, 1)", "path": "torch/nn/utils/rnn.py", "position": null, "original_position": 71, "commit_id": "7bdd58e4ab2ef58e72c2d1b5fa62f3dc0f511cf6", "original_commit_id": "d42f5be718866b77f9b93940b60ecd7a75fb668c", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "this if can be replaced with `return torch.stack(out_variable, 0 if batch_first else 1)`", "created_at": "2017-11-26T19:46:51Z", "updated_at": "2018-11-23T15:36:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/3875#discussion_r153079988", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3875", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153079988"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3875#discussion_r153079988"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3875"}}, "body_html": "<p>this if can be replaced with <code>return torch.stack(out_variable, 0 if batch_first else 1)</code></p>", "body_text": "this if can be replaced with return torch.stack(out_variable, 0 if batch_first else 1)"}