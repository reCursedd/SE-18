{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7645", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7645/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7645/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7645/events", "html_url": "https://github.com/pytorch/pytorch/issues/7645", "id": 324073653, "node_id": "MDU6SXNzdWUzMjQwNzM2NTM=", "number": 7645, "title": "Max pooling behavior for nan values", "user": {"login": "ClementPinard", "id": 4380424, "node_id": "MDQ6VXNlcjQzODA0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4380424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ClementPinard", "html_url": "https://github.com/ClementPinard", "followers_url": "https://api.github.com/users/ClementPinard/followers", "following_url": "https://api.github.com/users/ClementPinard/following{/other_user}", "gists_url": "https://api.github.com/users/ClementPinard/gists{/gist_id}", "starred_url": "https://api.github.com/users/ClementPinard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ClementPinard/subscriptions", "organizations_url": "https://api.github.com/users/ClementPinard/orgs", "repos_url": "https://api.github.com/users/ClementPinard/repos", "events_url": "https://api.github.com/users/ClementPinard/events{/privacy}", "received_events_url": "https://api.github.com/users/ClementPinard/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-05-17T15:26:53Z", "updated_at": "2018-05-18T14:40:02Z", "closed_at": "2018-05-18T14:40:02Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>max pooling functions are not consistent with max functions.<br>\nBelow an example, every max pooling (be it 1d, 2d or 3d, adaptive or not) acts the same, on cpu or on cuda.</p>\n<p>Essentially, there are two fondamental differences :</p>\n<ul>\n<li>max pooling of all <code>nan</code> values is <code>-THINF</code> while for <code>max</code> it's <code>nan</code></li>\n<li>max pooling of nan and valid values is valid values, which means <code>nan</code>s get ignored, while for <code>max</code>, as soon as there is a <code>nan</code> value, the result is <code>nan</code>.</li>\n</ul>\n<p>More generally, choosing explicetely how to deal with <code>nan</code> as in numpy (<a href=\"https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.nanmax.html\" rel=\"nofollow\">e.g.</a>) could be a solution, but maybe this is related to CuDNN's max pooling ?</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre>a<span class=\"pl-k\">=</span> torch.full((<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>), <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nan<span class=\"pl-pds\">'</span></span>))\n<span class=\"pl-c1\">print</span>(a.max())  <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor(nan)</span>\n<span class=\"pl-c1\">print</span>(F.adaptive_max_pool1d(a, <span class=\"pl-c1\">1</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([[[-3.4028e+38]]])</span>\n\na.requires_grad<span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>\na.max().backward()\n<span class=\"pl-c1\">print</span>(a.grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([[[ 0., 0.]]])</span>\nF.adaptive_max_pool1d(a, <span class=\"pl-c1\">1</span>).backward()\n<span class=\"pl-c1\">print</span>(a.grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([[[ 0., 0.]]])</span>\n\n<span class=\"pl-k\">with</span> torch.no_grad():\n    a[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\na.max().backward()\n<span class=\"pl-c1\">print</span>(a.grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([[[ 0., 0.]]])</span>\nF.adaptive_max_pool1d(a, <span class=\"pl-c1\">1</span>).backward()\n<span class=\"pl-c1\">print</span>(a.grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([[[ 1., 0.]]])</span></pre></div>\n<h2>System Info</h2>\n<p>Built from latest sources (as of 05/17)<br>\nPyTorch version: 0.5.0a0+331a04d<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.1.85</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.9.4</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.1.85<br>\nGPU models and configuration: GPU 0: Quadro M1000M<br>\nNvidia driver version: 390.30<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a</p>\n<p>Versions of relevant libraries:<br>\n[conda] magma-cuda91              2.3.0                         1    pytorch<br>\n[conda] torch                     0.5.0a0+331a04d           <br>\n[conda] torch                     0.3.1b0+2b47480           <br>\n[conda] torch                     0.5.0a0+4251e38           </p>", "body_text": "Issue description\nmax pooling functions are not consistent with max functions.\nBelow an example, every max pooling (be it 1d, 2d or 3d, adaptive or not) acts the same, on cpu or on cuda.\nEssentially, there are two fondamental differences :\n\nmax pooling of all nan values is -THINF while for max it's nan\nmax pooling of nan and valid values is valid values, which means nans get ignored, while for max, as soon as there is a nan value, the result is nan.\n\nMore generally, choosing explicetely how to deal with nan as in numpy (e.g.) could be a solution, but maybe this is related to CuDNN's max pooling ?\nCode example\na= torch.full((1,1,2), float('nan'))\nprint(a.max())  # tensor(nan)\nprint(F.adaptive_max_pool1d(a, 1)) # tensor([[[-3.4028e+38]]])\n\na.requires_grad=True\na.max().backward()\nprint(a.grad) # tensor([[[ 0., 0.]]])\nF.adaptive_max_pool1d(a, 1).backward()\nprint(a.grad) # tensor([[[ 0., 0.]]])\n\nwith torch.no_grad():\n    a[0,0,0] = 0\n\na.max().backward()\nprint(a.grad) # tensor([[[ 0., 0.]]])\nF.adaptive_max_pool1d(a, 1).backward()\nprint(a.grad) # tensor([[[ 1., 0.]]])\nSystem Info\nBuilt from latest sources (as of 05/17)\nPyTorch version: 0.5.0a0+331a04d\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.9.4\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: GPU 0: Quadro M1000M\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\nVersions of relevant libraries:\n[conda] magma-cuda91              2.3.0                         1    pytorch\n[conda] torch                     0.5.0a0+331a04d           \n[conda] torch                     0.3.1b0+2b47480           \n[conda] torch                     0.5.0a0+4251e38", "body": "## Issue description\r\n\r\nmax pooling functions are not consistent with max functions.\r\nBelow an example, every max pooling (be it 1d, 2d or 3d, adaptive or not) acts the same, on cpu or on cuda.\r\n\r\nEssentially, there are two fondamental differences :\r\n - max pooling of all `nan` values is `-THINF` while for `max` it's `nan`\r\n - max pooling of nan and valid values is valid values, which means `nan`s get ignored, while for `max`, as soon as there is a `nan` value, the result is `nan`.\r\n\r\nMore generally, choosing explicetely how to deal with `nan` as in numpy ([e.g.](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.nanmax.html)) could be a solution, but maybe this is related to CuDNN's max pooling ?\r\n\r\n## Code example\r\n\r\n```python\r\na= torch.full((1,1,2), float('nan'))\r\nprint(a.max())  # tensor(nan)\r\nprint(F.adaptive_max_pool1d(a, 1)) # tensor([[[-3.4028e+38]]])\r\n\r\na.requires_grad=True\r\na.max().backward()\r\nprint(a.grad) # tensor([[[ 0., 0.]]])\r\nF.adaptive_max_pool1d(a, 1).backward()\r\nprint(a.grad) # tensor([[[ 0., 0.]]])\r\n\r\nwith torch.no_grad():\r\n    a[0,0,0] = 0\r\n\r\na.max().backward()\r\nprint(a.grad) # tensor([[[ 0., 0.]]])\r\nF.adaptive_max_pool1d(a, 1).backward()\r\nprint(a.grad) # tensor([[[ 1., 0.]]])\r\n```\r\n\r\n## System Info\r\n\r\nBuilt from latest sources (as of 05/17)\r\nPyTorch version: 0.5.0a0+331a04d\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: GPU 0: Quadro M1000M\r\nNvidia driver version: 390.30\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[conda] magma-cuda91              2.3.0                         1    pytorch\r\n[conda] torch                     0.5.0a0+331a04d           <pip>\r\n[conda] torch                     0.3.1b0+2b47480           <pip>\r\n[conda] torch                     0.5.0a0+4251e38           <pip>\r\n\r\n"}