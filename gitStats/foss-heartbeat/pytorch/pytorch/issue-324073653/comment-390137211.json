{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390137211", "html_url": "https://github.com/pytorch/pytorch/issues/7645#issuecomment-390137211", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7645", "id": 390137211, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDEzNzIxMQ==", "user": {"login": "ClementPinard", "id": 4380424, "node_id": "MDQ6VXNlcjQzODA0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4380424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ClementPinard", "html_url": "https://github.com/ClementPinard", "followers_url": "https://api.github.com/users/ClementPinard/followers", "following_url": "https://api.github.com/users/ClementPinard/following{/other_user}", "gists_url": "https://api.github.com/users/ClementPinard/gists{/gist_id}", "starred_url": "https://api.github.com/users/ClementPinard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ClementPinard/subscriptions", "organizations_url": "https://api.github.com/users/ClementPinard/orgs", "repos_url": "https://api.github.com/users/ClementPinard/repos", "events_url": "https://api.github.com/users/ClementPinard/events{/privacy}", "received_events_url": "https://api.github.com/users/ClementPinard/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-18T08:37:37Z", "updated_at": "2018-05-18T08:40:24Z", "author_association": "NONE", "body_html": "<p>My practical usecase is for KITTI groundtruth depthmaps and FlowMaps which are 2D sparse arrays.<br>\nFor an algorithm that use FlowNet-like architecture that outputs predictions at multiple scale levels, we can either compare predictions to downscaled GT or upscaled predictions to GT.</p>\n<p>The first being obviously less computationally expensive, the ignore NaN would help downscaling such sparse 2D maps.</p>\n<p>For the moment we do something I find ugly, you can see it  <a href=\"https://github.com/ClementPinard/FlowNetPytorch/blob/master/multiscaleloss.py#L19\">here</a><br>\nEssentially it zeroes the <code>nan</code>s, takes the map and construct two maps of positive and negative values which are then maxpooled and added back together.</p>\n<p>I am actually open for a strict \"no-nan\" policy on pooling functions, but in that case better enforce it before someone writes a code that tries to benefit from maxpooling ignore <code>nan</code>s feature/bug</p>\n<p>and if you have a clever way of pooling sparse 2D tensors, I'm open to it, but I guess it's a topic for pytorch forums ;)</p>", "body_text": "My practical usecase is for KITTI groundtruth depthmaps and FlowMaps which are 2D sparse arrays.\nFor an algorithm that use FlowNet-like architecture that outputs predictions at multiple scale levels, we can either compare predictions to downscaled GT or upscaled predictions to GT.\nThe first being obviously less computationally expensive, the ignore NaN would help downscaling such sparse 2D maps.\nFor the moment we do something I find ugly, you can see it  here\nEssentially it zeroes the nans, takes the map and construct two maps of positive and negative values which are then maxpooled and added back together.\nI am actually open for a strict \"no-nan\" policy on pooling functions, but in that case better enforce it before someone writes a code that tries to benefit from maxpooling ignore nans feature/bug\nand if you have a clever way of pooling sparse 2D tensors, I'm open to it, but I guess it's a topic for pytorch forums ;)", "body": "My practical usecase is for KITTI groundtruth depthmaps and FlowMaps which are 2D sparse arrays.\r\nFor an algorithm that use FlowNet-like architecture that outputs predictions at multiple scale levels, we can either compare predictions to downscaled GT or upscaled predictions to GT.\r\n\r\nThe first being obviously less computationally expensive, the ignore NaN would help downscaling such sparse 2D maps.\r\n\r\nFor the moment we do something I find ugly, you can see it  [here](https://github.com/ClementPinard/FlowNetPytorch/blob/master/multiscaleloss.py#L19)\r\nEssentially it zeroes the `nan`s, takes the map and construct two maps of positive and negative values which are then maxpooled and added back together.\r\n\r\nI am actually open for a strict \"no-nan\" policy on pooling functions, but in that case better enforce it before someone writes a code that tries to benefit from maxpooling ignore `nan`s feature/bug\r\n\r\nand if you have a clever way of pooling sparse 2D tensors, I'm open to it, but I guess it's a topic for pytorch forums ;)"}