{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13824", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13824/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13824/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13824/events", "html_url": "https://github.com/pytorch/pytorch/issues/13824", "id": 379520609, "node_id": "MDU6SXNzdWUzNzk1MjA2MDk=", "number": 13824, "title": "Support the seperated control of the replicating and the collecting of gradients of DataParallel to accelerate it", "user": {"login": "anoidgit", "id": 12478523, "node_id": "MDQ6VXNlcjEyNDc4NTIz", "avatar_url": "https://avatars0.githubusercontent.com/u/12478523?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anoidgit", "html_url": "https://github.com/anoidgit", "followers_url": "https://api.github.com/users/anoidgit/followers", "following_url": "https://api.github.com/users/anoidgit/following{/other_user}", "gists_url": "https://api.github.com/users/anoidgit/gists{/gist_id}", "starred_url": "https://api.github.com/users/anoidgit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anoidgit/subscriptions", "organizations_url": "https://api.github.com/users/anoidgit/orgs", "repos_url": "https://api.github.com/users/anoidgit/repos", "events_url": "https://api.github.com/users/anoidgit/events{/privacy}", "received_events_url": "https://api.github.com/users/anoidgit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-11T13:01:13Z", "updated_at": "2018-11-12T07:42:44Z", "closed_at": "2018-11-11T20:12:49Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature: Support seperated control of the replicating and the collecting of gradients of DataParallel to accelerate it</h2>\n\n<p>Add two function to <code>DataParallel</code>, one to replicate models to GPUs, the other to collect gradients.</p>\n<h2>Motivation</h2>\n<p>I update the parameters of the designed model after several iterative forward and backward calls, which means that the <code>DataParallel</code> does not need to replicate <code>self.module</code> in every forward call, and it only needs to accumulate the gradients on the other GPUs when I am going to update the parameters with a call to step() of the optimizer. Replicating models in every forward calls greatly hurts the performance in <a href=\"https://github.com/anoidgit/transformer\">my implementation</a>.</p>\n\n<h2>Pitch</h2>\n\n<p>I tried to alleviate this through the following code:</p>\n<pre><code>class DataParallelModel(DataParallel):\n    def __init__(self, module, device_ids=None, output_device=None, dim=0, host_control=False):\n        super(DataParallelModel, self).__init__(module, device_ids, output_device, dim)\n\n        if host_control:\n\t    self.update_replicates()\n        else:\n\t    self.nets = None\n\n    def forward(self, *inputs, **kwargs):\n        if not self.device_ids:\n            return self.module(*inputs, **kwargs)\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        if self.nets is None:\n            replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        else:\n\t    replicas = self.nets[:len(inputs)]\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n\n    def update_replicates(self):\n        self.nets = self.replicate(self.module, self.device_ids)\n</code></pre>\n<p>Since I want to make the training more efficient like this:</p>\n<pre><code>model = DataParallelModel(designed_model, host_control=True)\ncrit = get_some_criterion()\noptm = get_an_optimizer(model)\nupdate_batches = 16\ni = 1\nfor id, td in training_data():\n    loss = crit(model(id), td)\n    loss.backward()\n    if i % update_batches == 0:\n        optm.step()\n        model.zero_grad()\n        model.update_replicates()\n    i += 1\n</code></pre>\n<p>But an assertation failure was thrown by autograd, which is beyond my ability.<br>\nI hope it can support another function like <code>accumulate_gradients</code> which accumulates the gradients from those replicated models at the same time, thus I can avoid useless communications between GPUs. In this case, the training code could be like:</p>\n<pre><code>model = DataParallelModel(designed_model, host_control=True)\ncrit = get_some_criterion()\noptm = get_an_optimizer(model)\nupdate_batches = 16\ni = 1\nfor id, td in training_data():\n    loss = crit(model(id), td)\n    loss.backward()\n    if i % update_batches == 0:\n        model.accumulate_gradients()\n        optm.step()\n        model.zero_grad()\n        model.update_replicates()\n    i += 1\n</code></pre>\n<p>Accumulate the gradients of several small batches is very important in some cases where a large batch size is very important for the quality of the model while the GPU memory is not enough to support it, like the training of transformer(a Neural Machine Translation model).</p>", "body_text": "\ud83d\ude80 Feature: Support seperated control of the replicating and the collecting of gradients of DataParallel to accelerate it\n\nAdd two function to DataParallel, one to replicate models to GPUs, the other to collect gradients.\nMotivation\nI update the parameters of the designed model after several iterative forward and backward calls, which means that the DataParallel does not need to replicate self.module in every forward call, and it only needs to accumulate the gradients on the other GPUs when I am going to update the parameters with a call to step() of the optimizer. Replicating models in every forward calls greatly hurts the performance in my implementation.\n\nPitch\n\nI tried to alleviate this through the following code:\nclass DataParallelModel(DataParallel):\n    def __init__(self, module, device_ids=None, output_device=None, dim=0, host_control=False):\n        super(DataParallelModel, self).__init__(module, device_ids, output_device, dim)\n\n        if host_control:\n\t    self.update_replicates()\n        else:\n\t    self.nets = None\n\n    def forward(self, *inputs, **kwargs):\n        if not self.device_ids:\n            return self.module(*inputs, **kwargs)\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        if self.nets is None:\n            replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        else:\n\t    replicas = self.nets[:len(inputs)]\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n\n    def update_replicates(self):\n        self.nets = self.replicate(self.module, self.device_ids)\n\nSince I want to make the training more efficient like this:\nmodel = DataParallelModel(designed_model, host_control=True)\ncrit = get_some_criterion()\noptm = get_an_optimizer(model)\nupdate_batches = 16\ni = 1\nfor id, td in training_data():\n    loss = crit(model(id), td)\n    loss.backward()\n    if i % update_batches == 0:\n        optm.step()\n        model.zero_grad()\n        model.update_replicates()\n    i += 1\n\nBut an assertation failure was thrown by autograd, which is beyond my ability.\nI hope it can support another function like accumulate_gradients which accumulates the gradients from those replicated models at the same time, thus I can avoid useless communications between GPUs. In this case, the training code could be like:\nmodel = DataParallelModel(designed_model, host_control=True)\ncrit = get_some_criterion()\noptm = get_an_optimizer(model)\nupdate_batches = 16\ni = 1\nfor id, td in training_data():\n    loss = crit(model(id), td)\n    loss.backward()\n    if i % update_batches == 0:\n        model.accumulate_gradients()\n        optm.step()\n        model.zero_grad()\n        model.update_replicates()\n    i += 1\n\nAccumulate the gradients of several small batches is very important in some cases where a large batch size is very important for the quality of the model while the GPU memory is not enough to support it, like the training of transformer(a Neural Machine Translation model).", "body": "## \ud83d\ude80 Feature: Support seperated control of the replicating and the collecting of gradients of DataParallel to accelerate it\r\n<!-- A clear and concise description of the feature proposal -->\r\nAdd two function to `DataParallel`, one to replicate models to GPUs, the other to collect gradients.\r\n## Motivation\r\nI update the parameters of the designed model after several iterative forward and backward calls, which means that the `DataParallel` does not need to replicate `self.module` in every forward call, and it only needs to accumulate the gradients on the other GPUs when I am going to update the parameters with a call to step() of the optimizer. Replicating models in every forward calls greatly hurts the performance in [my implementation](https://github.com/anoidgit/transformer).\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI tried to alleviate this through the following code:\r\n```\r\nclass DataParallelModel(DataParallel):\r\n    def __init__(self, module, device_ids=None, output_device=None, dim=0, host_control=False):\r\n        super(DataParallelModel, self).__init__(module, device_ids, output_device, dim)\r\n\r\n        if host_control:\r\n\t    self.update_replicates()\r\n        else:\r\n\t    self.nets = None\r\n\r\n    def forward(self, *inputs, **kwargs):\r\n        if not self.device_ids:\r\n            return self.module(*inputs, **kwargs)\r\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n        if len(self.device_ids) == 1:\r\n            return self.module(*inputs[0], **kwargs[0])\r\n        if self.nets is None:\r\n            replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n        else:\r\n\t    replicas = self.nets[:len(inputs)]\r\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n        return self.gather(outputs, self.output_device)\r\n\r\n    def update_replicates(self):\r\n        self.nets = self.replicate(self.module, self.device_ids)\r\n```\r\nSince I want to make the training more efficient like this:\r\n```\r\nmodel = DataParallelModel(designed_model, host_control=True)\r\ncrit = get_some_criterion()\r\noptm = get_an_optimizer(model)\r\nupdate_batches = 16\r\ni = 1\r\nfor id, td in training_data():\r\n    loss = crit(model(id), td)\r\n    loss.backward()\r\n    if i % update_batches == 0:\r\n        optm.step()\r\n        model.zero_grad()\r\n        model.update_replicates()\r\n    i += 1\r\n```\r\nBut an assertation failure was thrown by autograd, which is beyond my ability.\r\nI hope it can support another function like `accumulate_gradients` which accumulates the gradients from those replicated models at the same time, thus I can avoid useless communications between GPUs. In this case, the training code could be like:\r\n```\r\nmodel = DataParallelModel(designed_model, host_control=True)\r\ncrit = get_some_criterion()\r\noptm = get_an_optimizer(model)\r\nupdate_batches = 16\r\ni = 1\r\nfor id, td in training_data():\r\n    loss = crit(model(id), td)\r\n    loss.backward()\r\n    if i % update_batches == 0:\r\n        model.accumulate_gradients()\r\n        optm.step()\r\n        model.zero_grad()\r\n        model.update_replicates()\r\n    i += 1\r\n```\r\nAccumulate the gradients of several small batches is very important in some cases where a large batch size is very important for the quality of the model while the GPU memory is not enough to support it, like the training of transformer(a Neural Machine Translation model)."}