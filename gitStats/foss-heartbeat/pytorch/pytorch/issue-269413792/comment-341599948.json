{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341599948", "html_url": "https://github.com/pytorch/pytorch/pull/3355#issuecomment-341599948", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3355", "id": 341599948, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTU5OTk0OA==", "user": {"login": "EntilZha", "id": 1382460, "node_id": "MDQ6VXNlcjEzODI0NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1382460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EntilZha", "html_url": "https://github.com/EntilZha", "followers_url": "https://api.github.com/users/EntilZha/followers", "following_url": "https://api.github.com/users/EntilZha/following{/other_user}", "gists_url": "https://api.github.com/users/EntilZha/gists{/gist_id}", "starred_url": "https://api.github.com/users/EntilZha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EntilZha/subscriptions", "organizations_url": "https://api.github.com/users/EntilZha/orgs", "repos_url": "https://api.github.com/users/EntilZha/repos", "events_url": "https://api.github.com/users/EntilZha/events{/privacy}", "received_events_url": "https://api.github.com/users/EntilZha/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T00:53:22Z", "updated_at": "2017-11-03T00:58:10Z", "author_association": "NONE", "body_html": "<p>I can make a separate issue if it would be better, but this might be a good place to start since this change seems like it might have caused my current issue. With a recent version of master including this change (commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/66d24c50671fa1886fed65128950240d809b4866/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/66d24c50671fa1886fed65128950240d809b4866\"><tt>66d24c5</tt></a>) I am getting a regression in my code as demonstrated below:</p>\n<p>PyTorch 0.2</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> torch\nIn [<span class=\"pl-c1\">2</span>]: <span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nIn [<span class=\"pl-c1\">3</span>]: np.array([torch.LongTensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]), torch.LongTensor([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])])\nOut[<span class=\"pl-c1\">4</span>]: \narray([\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">2</span>\n[torch.LongTensor of size <span class=\"pl-c1\">2</span>]\n,\n       \n <span class=\"pl-c1\">3</span>\n <span class=\"pl-c1\">4</span>\n[torch.LongTensor of size <span class=\"pl-c1\">2</span>]\n], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">object</span>)\n</pre></div>\n<p>PyTorch master</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: np.array([torch.LongTensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]), torch.LongTensor([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])])\nIn [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> torch\n\nIn [<span class=\"pl-c1\">2</span>]: <span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nIn [<span class=\"pl-c1\">3</span>]: np.array([torch.LongTensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]), torch.LongTensor([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])])\n<span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span><span class=\"pl-k\">-</span>c15fb94d2d75<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> np.array([torch.LongTensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]), torch.LongTensor([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])])\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda3<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>tensor.py <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__int__</span>(<span class=\"pl-c1\">self</span>)\n    <span class=\"pl-c1\">375</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.numel() <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n    <span class=\"pl-c1\">376</span>             <span class=\"pl-k\">return</span> <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">self</span>[(<span class=\"pl-c1\">0</span>,) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.ndimension()])\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">377</span>         <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">TypeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>only 1-element tensors can be converted <span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">378</span>                         <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>to Python scalars<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">379</span> \n\n<span class=\"pl-c1\">TypeError</span>: only <span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>element tensors can be converted to Python scalars</pre></div>\n<p>The reason to do this is so that I can say take three numpy arrays of torch tensors where the order lines up and:</p>\n<ol>\n<li>Create a random ordering of batches with <code>batch_order = np.random.permutation(n_batches)</code></li>\n<li>During each epoch index all three numpy arrays like <code>x_array[batch_order]</code>, <code>other_input_array[batch_order]</code>, <code>y_array[batch_order]</code>. Each torch tensor represents one batch</li>\n</ol>\n<p>To be fair, if one passes <code>np.array(tensor_list, dtype=np.object)</code> the error goes away so it may just be something on the numpy side attempting to cast the tensor to a int/float</p>\n<p>Thanks!</p>", "body_text": "I can make a separate issue if it would be better, but this might be a good place to start since this change seems like it might have caused my current issue. With a recent version of master including this change (commit 66d24c5) I am getting a regression in my code as demonstrated below:\nPyTorch 0.2\nIn [1]: import torch\nIn [2]: import numpy as np\n\nIn [3]: np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\nOut[4]: \narray([\n 1\n 2\n[torch.LongTensor of size 2]\n,\n       \n 3\n 4\n[torch.LongTensor of size 2]\n], dtype=object)\n\nPyTorch master\nIn [1]: np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\nIn [1]: import torch\n\nIn [2]: import numpy as np\n\nIn [3]: np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-c15fb94d2d75> in <module>()\n----> 1 np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\n\n~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in __int__(self)\n    375         if self.numel() == 1:\n    376             return int(self[(0,) * self.ndimension()])\n--> 377         raise TypeError(\"only 1-element tensors can be converted \"\n    378                         \"to Python scalars\")\n    379 \n\nTypeError: only 1-element tensors can be converted to Python scalars\nThe reason to do this is so that I can say take three numpy arrays of torch tensors where the order lines up and:\n\nCreate a random ordering of batches with batch_order = np.random.permutation(n_batches)\nDuring each epoch index all three numpy arrays like x_array[batch_order], other_input_array[batch_order], y_array[batch_order]. Each torch tensor represents one batch\n\nTo be fair, if one passes np.array(tensor_list, dtype=np.object) the error goes away so it may just be something on the numpy side attempting to cast the tensor to a int/float\nThanks!", "body": "I can make a separate issue if it would be better, but this might be a good place to start since this change seems like it might have caused my current issue. With a recent version of master including this change (commit 66d24c5) I am getting a regression in my code as demonstrated below:\r\n\r\nPyTorch 0.2\r\n```python\r\nIn [1]: import torch\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\r\nOut[4]: \r\narray([\r\n 1\r\n 2\r\n[torch.LongTensor of size 2]\r\n,\r\n       \r\n 3\r\n 4\r\n[torch.LongTensor of size 2]\r\n], dtype=object)\r\n\r\n```\r\nPyTorch master\r\n```python\r\nIn [1]: np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\r\nIn [1]: import torch\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-c15fb94d2d75> in <module>()\r\n----> 1 np.array([torch.LongTensor([1, 2]), torch.LongTensor([3, 4])])\r\n\r\n~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in __int__(self)\r\n    375         if self.numel() == 1:\r\n    376             return int(self[(0,) * self.ndimension()])\r\n--> 377         raise TypeError(\"only 1-element tensors can be converted \"\r\n    378                         \"to Python scalars\")\r\n    379 \r\n\r\nTypeError: only 1-element tensors can be converted to Python scalars\r\n```\r\n\r\nThe reason to do this is so that I can say take three numpy arrays of torch tensors where the order lines up and:\r\n1. Create a random ordering of batches with `batch_order = np.random.permutation(n_batches)`\r\n2. During each epoch index all three numpy arrays like `x_array[batch_order]`, `other_input_array[batch_order]`, `y_array[batch_order]`. Each torch tensor represents one batch\r\n\r\nTo be fair, if one passes `np.array(tensor_list, dtype=np.object)` the error goes away so it may just be something on the numpy side attempting to cast the tensor to a int/float\r\n\r\nThanks!"}