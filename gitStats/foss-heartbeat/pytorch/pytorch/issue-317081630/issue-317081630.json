{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6892", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6892/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6892/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6892/events", "html_url": "https://github.com/pytorch/pytorch/issues/6892", "id": 317081630, "node_id": "MDU6SXNzdWUzMTcwODE2MzA=", "number": 6892, "title": "[Bug] TensorFromBlob on data created from CUDA Driver API Fails", "user": {"login": "Wapaul1", "id": 14142309, "node_id": "MDQ6VXNlcjE0MTQyMzA5", "avatar_url": "https://avatars2.githubusercontent.com/u/14142309?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wapaul1", "html_url": "https://github.com/Wapaul1", "followers_url": "https://api.github.com/users/Wapaul1/followers", "following_url": "https://api.github.com/users/Wapaul1/following{/other_user}", "gists_url": "https://api.github.com/users/Wapaul1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wapaul1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wapaul1/subscriptions", "organizations_url": "https://api.github.com/users/Wapaul1/orgs", "repos_url": "https://api.github.com/users/Wapaul1/repos", "events_url": "https://api.github.com/users/Wapaul1/events{/privacy}", "received_events_url": "https://api.github.com/users/Wapaul1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-24T06:05:28Z", "updated_at": "2018-04-26T16:24:52Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>If you allocate memory on the GPU using <code>cuMemAlloc</code>, then <code>tensorFromBlob</code> will fail on it. If you use <code>cudaMalloc</code>, it will succeed.</p>\n<p>More generally, is the CUDA Driver API actually supported in Pytorch, or is it only the Runtime API? The actual error appears to occur when <code>cudaPointerGetAttributes</code> is called on the data pointer.</p>\n<h2>Code example</h2>\n<p>Using <code>cpp_extension</code>, if you have the following C++ code:</p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>torch/torch.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>cuda.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>cuda_runtime.h<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">at</span><span class=\"pl-k\">;</span>\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">std</span><span class=\"pl-k\">;</span>\n\nTensor <span class=\"pl-en\">load</span>() {\n  CUdeviceptr data_done;\n  CUdevice handle;\n  <span class=\"pl-c1\">cuDeviceGet</span>(&amp;handle, <span class=\"pl-c1\">0</span>);\n  CUcontext context_;\n  <span class=\"pl-c1\">cuCtxCreate</span>(&amp;context_, <span class=\"pl-c1\">0</span>, handle);\n  CUresult err = <span class=\"pl-c1\">cuMemAlloc</span>(&amp;data_done, <span class=\"pl-c1\">16</span>*<span class=\"pl-k\">sizeof</span>(<span class=\"pl-k\">float</span>));\n  <span class=\"pl-k\">auto</span> f = <span class=\"pl-c1\">torch::CUDA</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">tensorFromBlob</span>(<span class=\"pl-k\">reinterpret_cast</span>&lt;<span class=\"pl-k\">void</span>*&gt;(data_done), {<span class=\"pl-c1\">16</span>});\n  <span class=\"pl-k\">return</span> f;\n}\n\n<span class=\"pl-en\">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) {\n  m.<span class=\"pl-c1\">def</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>load<span class=\"pl-pds\">\"</span></span>, &amp;load, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>load data<span class=\"pl-pds\">\"</span></span>);\n}\n</pre></div>\n<p>Then when <code>load</code> is called in python, it will fail with:</p>\n<div class=\"highlight highlight-source-python\"><pre>THCudaCheck <span class=\"pl-c1\">FAIL</span> <span class=\"pl-v\">file</span><span class=\"pl-k\">=</span><span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>devbox<span class=\"pl-k\">/</span>projects<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>aten<span class=\"pl-k\">/</span>src<span class=\"pl-k\">/</span><span class=\"pl-c1\">THC</span><span class=\"pl-k\">/</span>generic<span class=\"pl-k\">/</span>THCStorage.c line<span class=\"pl-k\">=</span><span class=\"pl-c1\">150</span> error<span class=\"pl-k\">=</span><span class=\"pl-c1\">49</span> : incompatible driver context\n<span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span><span class=\"pl-k\">-</span>e8b40809576d<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> pyt.load()\n\n<span class=\"pl-c1\">RuntimeError</span>: cuda runtime error (<span class=\"pl-c1\">49</span>) : incompatible driver context at <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>devbox<span class=\"pl-k\">/</span>projects<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>aten<span class=\"pl-k\">/</span>src<span class=\"pl-k\">/</span><span class=\"pl-c1\">THC</span><span class=\"pl-k\">/</span>generic<span class=\"pl-k\">/</span>THCStorage.c:<span class=\"pl-c1\">150</span></pre></div>\n<p>However, if you use <code>cudaMalloc</code> as below, then it will succeed:</p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>torch/torch.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>cuda.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>cuda_runtime.h<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">at</span><span class=\"pl-k\">;</span>\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">std</span><span class=\"pl-k\">;</span>\n\nTensor <span class=\"pl-en\">load</span>() {\n  CUdeviceptr data_done;\n  cudaError_t err2 = <span class=\"pl-c1\">cudaMalloc</span>(<span class=\"pl-k\">reinterpret_cast</span>&lt;<span class=\"pl-k\">void</span>**&gt;(&amp;data_done), <span class=\"pl-c1\">16</span>*<span class=\"pl-k\">sizeof</span>(<span class=\"pl-k\">float</span>));\n  <span class=\"pl-k\">auto</span> f = <span class=\"pl-c1\">torch::CUDA</span>(<span class=\"pl-c1\">kFloat</span>).<span class=\"pl-c1\">tensorFromBlob</span>(<span class=\"pl-k\">reinterpret_cast</span>&lt;<span class=\"pl-k\">void</span>*&gt;(data_done), {<span class=\"pl-c1\">16</span>});\n  <span class=\"pl-k\">return</span> f;\n}\n\n<span class=\"pl-en\">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) {\n  m.<span class=\"pl-c1\">def</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>load<span class=\"pl-pds\">\"</span></span>, &amp;load, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>load data<span class=\"pl-pds\">\"</span></span>);\n}\n</pre></div>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Build command you used (if compiling from source): python setup.py install</li>\n<li>OS: Linux Mint 18.2 Sonya</li>\n<li>PyTorch version: 0.4.0a0+c43c911</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 9.1.85, no cuDNN</li>\n<li>GPU models and configuration: GTX 970</li>\n<li>GCC version (if compiling from source): 5.4.0 20160609</li>\n<li>CMake version: 3.5.1</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nIf you allocate memory on the GPU using cuMemAlloc, then tensorFromBlob will fail on it. If you use cudaMalloc, it will succeed.\nMore generally, is the CUDA Driver API actually supported in Pytorch, or is it only the Runtime API? The actual error appears to occur when cudaPointerGetAttributes is called on the data pointer.\nCode example\nUsing cpp_extension, if you have the following C++ code:\n#include <torch/torch.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\nusing namespace at;\nusing namespace std;\n\nTensor load() {\n  CUdeviceptr data_done;\n  CUdevice handle;\n  cuDeviceGet(&handle, 0);\n  CUcontext context_;\n  cuCtxCreate(&context_, 0, handle);\n  CUresult err = cuMemAlloc(&data_done, 16*sizeof(float));\n  auto f = torch::CUDA(kFloat).tensorFromBlob(reinterpret_cast<void*>(data_done), {16});\n  return f;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"load\", &load, \"load data\");\n}\n\nThen when load is called in python, it will fail with:\nTHCudaCheck FAIL file=/home/devbox/projects/pytorch/aten/src/THC/generic/THCStorage.c line=150 error=49 : incompatible driver context\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-3-e8b40809576d> in <module>()\n----> 1 pyt.load()\n\nRuntimeError: cuda runtime error (49) : incompatible driver context at /home/devbox/projects/pytorch/aten/src/THC/generic/THCStorage.c:150\nHowever, if you use cudaMalloc as below, then it will succeed:\n#include <torch/torch.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\nusing namespace at;\nusing namespace std;\n\nTensor load() {\n  CUdeviceptr data_done;\n  cudaError_t err2 = cudaMalloc(reinterpret_cast<void**>(&data_done), 16*sizeof(float));\n  auto f = torch::CUDA(kFloat).tensorFromBlob(reinterpret_cast<void*>(data_done), {16});\n  return f;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"load\", &load, \"load data\");\n}\n\nSystem Info\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): source\nBuild command you used (if compiling from source): python setup.py install\nOS: Linux Mint 18.2 Sonya\nPyTorch version: 0.4.0a0+c43c911\nPython version: 3.6\nCUDA/cuDNN version: 9.1.85, no cuDNN\nGPU models and configuration: GTX 970\nGCC version (if compiling from source): 5.4.0 20160609\nCMake version: 3.5.1\nVersions of any other relevant libraries:", "body": "## Issue description\r\n\r\nIf you allocate memory on the GPU using `cuMemAlloc`, then `tensorFromBlob` will fail on it. If you use `cudaMalloc`, it will succeed. \r\n\r\nMore generally, is the CUDA Driver API actually supported in Pytorch, or is it only the Runtime API? The actual error appears to occur when `cudaPointerGetAttributes` is called on the data pointer. \r\n\r\n## Code example\r\n\r\nUsing `cpp_extension`, if you have the following C++ code:\r\n\r\n```c++\r\n#include <torch/torch.h>\r\n#include <cuda.h>\r\n#include <cuda_runtime.h>\r\n\r\nusing namespace at;\r\nusing namespace std;\r\n\r\nTensor load() {\r\n  CUdeviceptr data_done;\r\n  CUdevice handle;\r\n  cuDeviceGet(&handle, 0);\r\n  CUcontext context_;\r\n  cuCtxCreate(&context_, 0, handle);\r\n  CUresult err = cuMemAlloc(&data_done, 16*sizeof(float));\r\n  auto f = torch::CUDA(kFloat).tensorFromBlob(reinterpret_cast<void*>(data_done), {16});\r\n  return f;\r\n}\r\n\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n  m.def(\"load\", &load, \"load data\");\r\n}\r\n\r\n```\r\nThen when `load` is called in python, it will fail with:\r\n```python\r\nTHCudaCheck FAIL file=/home/devbox/projects/pytorch/aten/src/THC/generic/THCStorage.c line=150 error=49 : incompatible driver context\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-e8b40809576d> in <module>()\r\n----> 1 pyt.load()\r\n\r\nRuntimeError: cuda runtime error (49) : incompatible driver context at /home/devbox/projects/pytorch/aten/src/THC/generic/THCStorage.c:150\r\n```\r\nHowever, if you use `cudaMalloc` as below, then it will succeed:\r\n```c++\r\n#include <torch/torch.h>\r\n#include <cuda.h>\r\n#include <cuda_runtime.h>\r\n\r\nusing namespace at;\r\nusing namespace std;\r\n\r\nTensor load() {\r\n  CUdeviceptr data_done;\r\n  cudaError_t err2 = cudaMalloc(reinterpret_cast<void**>(&data_done), 16*sizeof(float));\r\n  auto f = torch::CUDA(kFloat).tensorFromBlob(reinterpret_cast<void*>(data_done), {16});\r\n  return f;\r\n}\r\n\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n  m.def(\"load\", &load, \"load data\");\r\n}\r\n\r\n```\r\n## System Info\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Build command you used (if compiling from source): python setup.py install\r\n- OS: Linux Mint 18.2 Sonya\r\n- PyTorch version: 0.4.0a0+c43c911\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1.85, no cuDNN\r\n- GPU models and configuration: GTX 970\r\n- GCC version (if compiling from source): 5.4.0 20160609\r\n- CMake version: 3.5.1\r\n- Versions of any other relevant libraries:\r\n"}