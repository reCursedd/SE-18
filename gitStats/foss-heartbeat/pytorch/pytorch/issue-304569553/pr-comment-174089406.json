{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174089406", "pull_request_review_id": 103380719, "id": 174089406, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NDA4OTQwNg==", "diff_hunk": "@@ -15,122 +15,103 @@\n namespace at {\n namespace native {\n \n-using sum_type = void(Tensor &, const Tensor &, size_t, bool);\n-sum_type *sumImpl = &DispatchStub<sum_type>::init<sumImplC, &sumImpl>;\n-\n-Tensor _sum_cpu(const Tensor &self) {\n-  if (self.is_contiguous()) {\n-    Tensor result = self.type().tensor({});\n-    sumImpl(result, self, 0, true);\n-    return result;\n-  }\n-  return self._sumall();\n-}\n-\n-Tensor _sum_cuda(const Tensor &self_) { return self_._sumall(); }\n-\n-Tensor sum(const Tensor &self, int64_t dim_, bool keepdim) {\n+static Tensor _dimreduce_setup(const Tensor &self, int64_t dim_,\n+                               int64_t ident) {\n   int64_t dim = maybe_wrap_dim(dim_, self.dim());\n   Tensor result = self.type().tensor();\n   if (self.numel() == 1 && self.ndimension() == 0) {\n     result.resize_({});\n     result.fill_(self);\n-    return result;\n+    return std::move(result);\n   }\n   // Return identity\n   if (self.numel() == 0 && self.ndimension() == 1) {\n     result.resize_({0});\n-    result.fill_(0);\n-    return result;\n+    result.fill_(ident);\n+    return std::move(result);\n   }\n-  at::sum_out(result, self, dim, keepdim);\n-  return result;\n+  IntList self_sizes = self.sizes();\n+  std::vector<int64_t> result_sizes;\n+  result_sizes.insert(result_sizes.end(), self_sizes.begin(), self_sizes.end());\n+  result_sizes[dim] = 1;\n+  result.resize_(result_sizes);\n+  return std::move(result);\n }\n \n-Tensor &_sum_out_cpu(Tensor &result, const Tensor &self, int64_t dim,\n-                     bool keepdim) {\n-  if (self.is_contiguous() && result.is_contiguous()) {\n-    IntList self_sizes = self.sizes();\n-    std::vector<int64_t> result_sizes;\n-    result_sizes.insert(result_sizes.end(), self_sizes.begin(),\n-                        self_sizes.end());\n-\n-    result_sizes[dim] = 1;\n-    result.resize_(result_sizes);\n-\n-    sumImpl(result, self, dim, false);\n+using reduce_type = void(Tensor &, const Tensor &, size_t, bool);\n+reduce_type *sumImpl = &DispatchStub<reduce_type>::init<sumImplC, &sumImpl>;\n+reduce_type *prodImpl = &DispatchStub<reduce_type>::init<prodImplC, &prodImpl>;\n \n-    if (!keepdim) {\n-      result.squeeze_(dim);\n-    }\n-    return result;\n-  } else {\n-    return at::_sum_out(result, self, dim, keepdim);\n-  }\n+Tensor sum(const Tensor &self, int64_t dim_, bool keepdim) {\n+  int64_t dim = maybe_wrap_dim(dim_, self.dim());\n+  Tensor result = _dimreduce_setup(self, dim_, 0);\n+  at::sum_out(result, self, dim, keepdim);\n+  return result;\n }\n \n-Tensor &_sum_out_cuda(Tensor &result, const Tensor &self, int64_t dim,\n-                      bool keepdim) {\n-  return at::_sum_out(result, self, dim, keepdim);\n+Tensor prod(const Tensor &self, int64_t dim_, bool keepdim) {\n+  int64_t dim = maybe_wrap_dim(dim_, self.dim());\n+  Tensor result = _dimreduce_setup(self, dim_, 1);\n+  at::prod_out(result, self, dim, keepdim);\n+  return result;\n }\n \n-using prod_type = void(Tensor &, const Tensor &, size_t, bool);\n-prod_type *prodImpl = &DispatchStub<prod_type>::init<prodImplC, &prodImpl>;\n-\n-Tensor _prod_cpu(const Tensor &self) {\n+Tensor _reduce_cpu(reduce_type * f, const Tensor &self) {\n   if (self.is_contiguous()) {\n     Tensor result = self.type().tensor({});\n-    prodImpl(result, self, 0, true);\n+    f(result, self, 0, true);\n     return result;\n   }\n-  return self._prodall();\n+  return self._sumall();\n }\n \n-Tensor _prod_cuda(const Tensor &self_) { return self_._prodall(); }\n+Tensor _sum_cpu(const Tensor &self) {\n+  return _reduce_cpu(sumImpl, self);\n+}\n \n-Tensor prod(const Tensor &self, int64_t dim_, bool keepdim) {\n-  int64_t dim = maybe_wrap_dim(dim_, self.dim());\n-  Tensor result = self.type().tensor();\n-  if (self.numel() == 1 && self.ndimension() == 0) {\n-    result.resize_({});\n-    result.fill_(self);\n-    return result;\n-  }\n-  // Return identity\n-  if (self.numel() == 0 && self.ndimension() == 1) {\n-    result.resize_({0});\n-    result.fill_(1);\n+Tensor _prod_cpu(const Tensor &self) {\n+  return _reduce_cpu(prodImpl, self);", "path": "aten/src/ATen/native/ReduceOps.cpp", "position": null, "original_position": 120, "commit_id": "f811874db6271f4f1ca3eb7340a62cec88afa0cc", "original_commit_id": "012f2b0c7ee524a376d84f16382e18cee8c30af2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This will fall back to `self._sumall` if the `if` fails", "created_at": "2018-03-13T11:02:01Z", "updated_at": "2018-11-23T15:40:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/5723#discussion_r174089406", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5723", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/174089406"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5723#discussion_r174089406"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5723"}}, "body_html": "<p>This will fall back to <code>self._sumall</code> if the <code>if</code> fails</p>", "body_text": "This will fall back to self._sumall if the if fails"}