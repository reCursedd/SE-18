{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5275", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5275/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5275/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5275/events", "html_url": "https://github.com/pytorch/pytorch/pull/5275", "id": 297841796, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY5NjY2OTI2", "number": 5275, "title": "Fixes UB when using legacy python functions and mark_non_differentiable", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-02-16T16:25:03Z", "updated_at": "2018-11-23T15:39:41Z", "closed_at": "2018-02-18T03:06:41Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5275", "html_url": "https://github.com/pytorch/pytorch/pull/5275", "diff_url": "https://github.com/pytorch/pytorch/pull/5275.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5275.patch"}, "body_html": "<p>This should fix the flaky segfaults I was seeing in <code>test_autograd/test_dep_nograd</code>.</p>\n<p>If an output of a python Function is marked as non_differentiable,<br>\nautograd won't save a gradfn for that output. During the backward<br>\npass, this translates to autograd passing an undefined tensor to the<br>\nbackward of the Function. The legacy python Function path checks<br>\nif <em>any</em> of the inputs to backward requires_grad, here:<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/utils.cpp#L16\">https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/utils.cpp#L16</a></p>\n<p>This requires_grad check uses Variable::get(), which casts the<br>\nundefined tensor to a VariableImpl and then accesses the _requires_grad<br>\nmember. This is UB because the undefined tensor is NOT a VariableImpl.</p>\n<p>The fix I did here is to add a check for if the variable/tensor is defined<br>\nin the legacy python Function code path.</p>\n<p>The other that that I could do is to reevaluate how autograd treats outputs marked as non_differentiable. The inputs to the legacy python function are already zero-filled, but they don't get zero-filled early enough to avoid <code>requires_grad</code> being called on them right now.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>\n<h3>Test Plan</h3>\n<p>I should probably figure out how to use ASAN and run my code on an ASAN build... TBD in progress.</p>", "body_text": "This should fix the flaky segfaults I was seeing in test_autograd/test_dep_nograd.\nIf an output of a python Function is marked as non_differentiable,\nautograd won't save a gradfn for that output. During the backward\npass, this translates to autograd passing an undefined tensor to the\nbackward of the Function. The legacy python Function path checks\nif any of the inputs to backward requires_grad, here:\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/utils.cpp#L16\nThis requires_grad check uses Variable::get(), which casts the\nundefined tensor to a VariableImpl and then accesses the _requires_grad\nmember. This is UB because the undefined tensor is NOT a VariableImpl.\nThe fix I did here is to add a check for if the variable/tensor is defined\nin the legacy python Function code path.\nThe other that that I could do is to reevaluate how autograd treats outputs marked as non_differentiable. The inputs to the legacy python function are already zero-filled, but they don't get zero-filled early enough to avoid requires_grad being called on them right now.\ncc @ezyang\nTest Plan\nI should probably figure out how to use ASAN and run my code on an ASAN build... TBD in progress.", "body": "This should fix the flaky segfaults I was seeing in `test_autograd/test_dep_nograd`.\r\n\r\nIf an output of a python Function is marked as non_differentiable,\r\nautograd won't save a gradfn for that output. During the backward\r\npass, this translates to autograd passing an undefined tensor to the\r\nbackward of the Function. The legacy python Function path checks\r\nif *any* of the inputs to backward requires_grad, here:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/utils.cpp#L16\r\n\r\nThis requires_grad check uses Variable::get(), which casts the\r\nundefined tensor to a VariableImpl and then accesses the _requires_grad\r\nmember. This is UB because the undefined tensor is NOT a VariableImpl.\r\n\r\nThe fix I did here is to add a check for if the variable/tensor is defined\r\nin the legacy python Function code path.\r\n\r\nThe other that that I could do is to reevaluate how autograd treats outputs marked as non_differentiable. The inputs to the legacy python function are already zero-filled, but they don't get zero-filled early enough to avoid `requires_grad` being called on them right now.\r\n\r\ncc @ezyang \r\n\r\n### Test Plan\r\nI should probably figure out how to use ASAN and run my code on an ASAN build... TBD in progress."}