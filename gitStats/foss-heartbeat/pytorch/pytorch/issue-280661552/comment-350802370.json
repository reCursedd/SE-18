{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/350802370", "html_url": "https://github.com/pytorch/pytorch/pull/4096#issuecomment-350802370", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4096", "id": 350802370, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDgwMjM3MA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-11T17:50:01Z", "updated_at": "2017-12-11T17:50:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Luca and I chatted about this on Slack, here's some notes:</p>\n<pre><code>I noticed a bug where the nn special case lays out the output arguments in the wrong order, if I do something like \"foo, bar: derivative1 \\n baz: derivative2\"\n\n\n[11:41] \nI'm going to work around this for now but it might be worth fixing\n\n\nLuca Antiga [11:47 AM] \nI\u2019m not sure I get it, sorry. Can you write a snippet?\n\nEdward Yang [11:47 AM] \nyep\n\n\nslackbot [11:47 AM] Only visible to you\nOK \u2014 I will automatically import GitHub Gists that you link to.\n\nEdward Yang [11:47 AM] \nshared this YAML snippet: derivatives.yaml\n- name: conv_depthwise2d(Tensor self, Tensor weight, IntList kernel_size, Tensor bias, IntList stride, IntList padding, IntList dilation)\n self, weight: conv_depthwise2d_backward(grad, self, weight, kernel_size, stride, padding, dilation, grad_input_mask)\n bias: grad.contiguous().view({grad.size(0), grad.size(1), -1}).sum(0).sum(1)\nAdd Comment\n\nEdward Yang [11:48 AM] \noops, there's a type\n\n[11:48] \n*typo\n\nEdward Yang [11:48 AM] \nshared this YAML snippet: derivatives.yaml\n- name: conv_depthwise2d(Tensor self, Tensor weight, IntList kernel_size, Tensor bias, IntList stride, IntList padding, IntList dilation)\n self, weight: conv_depthwise2d_backward(grad, self, weight, kernel_size, stride, padding, dilation, grad_input_mask)\n bias: grad.contiguous().view({grad.size(0), grad.size(1), -1}).sum(0).sum(1)\nAdd Comment\n\n\n\nEdward Yang [11:49 AM] \nOK, fixed\n\n\n[11:49] \nSo, the intended semantics of this yaml is that conv_depthwise2d_backward is used to compute gradients for self and weight, and we have a separate calculation for bias\n\n\n[11:49] \nBut there are two problems.\n\n\n[11:50] \nFirst, the generation code always assigns output indices starting from 0\n\n[11:50] \nso you end up with self = 0, weight = 1, bias = 0. Bias and self clobber each other's indices.\n\nLuca Antiga [11:50 AM] \nI see. Ok, it shouldn\u2019t be hard to fix\n\n\nEdward Yang [11:50 AM] \nThat's not too difficult to fix https://gist.github.com/ezyang/55d9727b5494d08f01d42a85ed926aba\n\n\nEdward Yang [11:50 AM] \nshared this Diff snippet: index.patch\ndiff --git a/tools/autograd/gen_variable_type.py b/tools/autograd/gen_variable_type.py\nindex 7cf53f1..965ba08 100644\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n--- a/tools/autograd/gen_variable_type.py\n+++ b/tools/autograd/gen_variable_type.py\n@@ -387,14 +387,17 @@ def load_derivatives(path, declarations_by_signature, declarations_by_name):\u00b7\u00b7\u00b7\n     fwd = declarations_by_name[fwd_name][0]\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     derivatives = []\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n+    num_outputs = 0\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     for raw_names, formula in defn.items():\n       var_names = split_names(raw_names)\n-      output_indices = list(range(len(var_names)))\n+      old_num_outputs = num_outputs\n+      num_outputs += len(var_names)\n+      output_indices = list(range(old_num_outputs, num_outputs))\n       derivatives.append(create_derivative(fwd, formula, output_indices, var_names))\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     buffers = declaration['buffers']\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n-    func = create_autograd_function(defn_name, derivatives, len(output_indices), buffers)\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n+    func = create_autograd_function(defn_name, derivatives, num_outputs, buffers)\n     declaration['derivative'] = func\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     return func\nAdd Comment Click to expand inline 24 lines\n\nEdward Yang [11:50 AM] \nThe second problem is more annoying though\n\n[11:51] \nthe output indices for self, weight and bias are actually fixed\n\n[11:51] \ni.e., you *must* assign self = 0, weight = 1, bias = 2\n\n[11:51] \nbut notice that the index allocation loop is iterating over a dictionary. So the order is nondeterministic\n\n[11:51] \nso, there's a bit of logic in the main path of this function for computing the index layout\n\n\n[11:52] \nand making sure everybody gets the right index. This is logic you don't want to duplicate.\n\n\nLuca Antiga [11:53 AM] \nYep. We need to parse the outputs and iterate over those outputs in the same order. Plus we need to offset indices so that they increment for every line.\n\n\nEdward Yang [11:53 AM] \nyep. My point is, we already implemented this below\n\n\nLuca Antiga [11:54 AM] \nCorrect\n</code></pre>", "body_text": "Luca and I chatted about this on Slack, here's some notes:\nI noticed a bug where the nn special case lays out the output arguments in the wrong order, if I do something like \"foo, bar: derivative1 \\n baz: derivative2\"\n\n\n[11:41] \nI'm going to work around this for now but it might be worth fixing\n\n\nLuca Antiga [11:47 AM] \nI\u2019m not sure I get it, sorry. Can you write a snippet?\n\nEdward Yang [11:47 AM] \nyep\n\n\nslackbot [11:47 AM] Only visible to you\nOK \u2014 I will automatically import GitHub Gists that you link to.\n\nEdward Yang [11:47 AM] \nshared this YAML snippet: derivatives.yaml\n- name: conv_depthwise2d(Tensor self, Tensor weight, IntList kernel_size, Tensor bias, IntList stride, IntList padding, IntList dilation)\n self, weight: conv_depthwise2d_backward(grad, self, weight, kernel_size, stride, padding, dilation, grad_input_mask)\n bias: grad.contiguous().view({grad.size(0), grad.size(1), -1}).sum(0).sum(1)\nAdd Comment\n\nEdward Yang [11:48 AM] \noops, there's a type\n\n[11:48] \n*typo\n\nEdward Yang [11:48 AM] \nshared this YAML snippet: derivatives.yaml\n- name: conv_depthwise2d(Tensor self, Tensor weight, IntList kernel_size, Tensor bias, IntList stride, IntList padding, IntList dilation)\n self, weight: conv_depthwise2d_backward(grad, self, weight, kernel_size, stride, padding, dilation, grad_input_mask)\n bias: grad.contiguous().view({grad.size(0), grad.size(1), -1}).sum(0).sum(1)\nAdd Comment\n\n\n\nEdward Yang [11:49 AM] \nOK, fixed\n\n\n[11:49] \nSo, the intended semantics of this yaml is that conv_depthwise2d_backward is used to compute gradients for self and weight, and we have a separate calculation for bias\n\n\n[11:49] \nBut there are two problems.\n\n\n[11:50] \nFirst, the generation code always assigns output indices starting from 0\n\n[11:50] \nso you end up with self = 0, weight = 1, bias = 0. Bias and self clobber each other's indices.\n\nLuca Antiga [11:50 AM] \nI see. Ok, it shouldn\u2019t be hard to fix\n\n\nEdward Yang [11:50 AM] \nThat's not too difficult to fix https://gist.github.com/ezyang/55d9727b5494d08f01d42a85ed926aba\n\n\nEdward Yang [11:50 AM] \nshared this Diff snippet: index.patch\ndiff --git a/tools/autograd/gen_variable_type.py b/tools/autograd/gen_variable_type.py\nindex 7cf53f1..965ba08 100644\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n--- a/tools/autograd/gen_variable_type.py\n+++ b/tools/autograd/gen_variable_type.py\n@@ -387,14 +387,17 @@ def load_derivatives(path, declarations_by_signature, declarations_by_name):\u00b7\u00b7\u00b7\n     fwd = declarations_by_name[fwd_name][0]\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     derivatives = []\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n+    num_outputs = 0\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     for raw_names, formula in defn.items():\n       var_names = split_names(raw_names)\n-      output_indices = list(range(len(var_names)))\n+      old_num_outputs = num_outputs\n+      num_outputs += len(var_names)\n+      output_indices = list(range(old_num_outputs, num_outputs))\n       derivatives.append(create_derivative(fwd, formula, output_indices, var_names))\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     buffers = declaration['buffers']\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n-    func = create_autograd_function(defn_name, derivatives, len(output_indices), buffers)\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n+    func = create_autograd_function(defn_name, derivatives, num_outputs, buffers)\n     declaration['derivative'] = func\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n     return func\nAdd Comment Click to expand inline 24 lines\n\nEdward Yang [11:50 AM] \nThe second problem is more annoying though\n\n[11:51] \nthe output indices for self, weight and bias are actually fixed\n\n[11:51] \ni.e., you *must* assign self = 0, weight = 1, bias = 2\n\n[11:51] \nbut notice that the index allocation loop is iterating over a dictionary. So the order is nondeterministic\n\n[11:51] \nso, there's a bit of logic in the main path of this function for computing the index layout\n\n\n[11:52] \nand making sure everybody gets the right index. This is logic you don't want to duplicate.\n\n\nLuca Antiga [11:53 AM] \nYep. We need to parse the outputs and iterate over those outputs in the same order. Plus we need to offset indices so that they increment for every line.\n\n\nEdward Yang [11:53 AM] \nyep. My point is, we already implemented this below\n\n\nLuca Antiga [11:54 AM] \nCorrect", "body": "Luca and I chatted about this on Slack, here's some notes:\r\n\r\n```\r\nI noticed a bug where the nn special case lays out the output arguments in the wrong order, if I do something like \"foo, bar: derivative1 \\n baz: derivative2\"\r\n\r\n\r\n[11:41] \r\nI'm going to work around this for now but it might be worth fixing\r\n\r\n\r\nLuca Antiga [11:47 AM] \r\nI\u2019m not sure I get it, sorry. Can you write a snippet?\r\n\r\nEdward Yang [11:47 AM] \r\nyep\r\n\r\n\r\nslackbot [11:47 AM] Only visible to you\r\nOK \u2014 I will automatically import GitHub Gists that you link to.\r\n\r\nEdward Yang [11:47 AM] \r\nshared this YAML snippet: derivatives.yaml\r\n- name: conv_depthwise2d(Tensor self, Tensor weight, IntList kernel_size, Tensor bias, IntList stride, IntList padding, IntList dilation)\r\n self, weight: conv_depthwise2d_backward(grad, self, weight, kernel_size, stride, padding, dilation, grad_input_mask)\r\n bias: grad.contiguous().view({grad.size(0), grad.size(1), -1}).sum(0).sum(1)\r\nAdd Comment\r\n\r\nEdward Yang [11:48 AM] \r\noops, there's a type\r\n\r\n[11:48] \r\n*typo\r\n\r\nEdward Yang [11:48 AM] \r\nshared this YAML snippet: derivatives.yaml\r\n- name: conv_depthwise2d(Tensor self, Tensor weight, IntList kernel_size, Tensor bias, IntList stride, IntList padding, IntList dilation)\r\n self, weight: conv_depthwise2d_backward(grad, self, weight, kernel_size, stride, padding, dilation, grad_input_mask)\r\n bias: grad.contiguous().view({grad.size(0), grad.size(1), -1}).sum(0).sum(1)\r\nAdd Comment\r\n\r\n\r\n\r\nEdward Yang [11:49 AM] \r\nOK, fixed\r\n\r\n\r\n[11:49] \r\nSo, the intended semantics of this yaml is that conv_depthwise2d_backward is used to compute gradients for self and weight, and we have a separate calculation for bias\r\n\r\n\r\n[11:49] \r\nBut there are two problems.\r\n\r\n\r\n[11:50] \r\nFirst, the generation code always assigns output indices starting from 0\r\n\r\n[11:50] \r\nso you end up with self = 0, weight = 1, bias = 0. Bias and self clobber each other's indices.\r\n\r\nLuca Antiga [11:50 AM] \r\nI see. Ok, it shouldn\u2019t be hard to fix\r\n\r\n\r\nEdward Yang [11:50 AM] \r\nThat's not too difficult to fix https://gist.github.com/ezyang/55d9727b5494d08f01d42a85ed926aba\r\n\r\n\r\nEdward Yang [11:50 AM] \r\nshared this Diff snippet: index.patch\r\ndiff --git a/tools/autograd/gen_variable_type.py b/tools/autograd/gen_variable_type.py\r\nindex 7cf53f1..965ba08 100644\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n--- a/tools/autograd/gen_variable_type.py\r\n+++ b/tools/autograd/gen_variable_type.py\r\n@@ -387,14 +387,17 @@ def load_derivatives(path, declarations_by_signature, declarations_by_name):\u00b7\u00b7\u00b7\r\n     fwd = declarations_by_name[fwd_name][0]\r\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n     derivatives = []\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n+    num_outputs = 0\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n     for raw_names, formula in defn.items():\r\n       var_names = split_names(raw_names)\r\n-      output_indices = list(range(len(var_names)))\r\n+      old_num_outputs = num_outputs\r\n+      num_outputs += len(var_names)\r\n+      output_indices = list(range(old_num_outputs, num_outputs))\r\n       derivatives.append(create_derivative(fwd, formula, output_indices, var_names))\r\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n     buffers = declaration['buffers']\r\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n-    func = create_autograd_function(defn_name, derivatives, len(output_indices), buffers)\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n+    func = create_autograd_function(defn_name, derivatives, num_outputs, buffers)\r\n     declaration['derivative'] = func\r\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n     return func\r\nAdd Comment Click to expand inline 24 lines\r\n\r\nEdward Yang [11:50 AM] \r\nThe second problem is more annoying though\r\n\r\n[11:51] \r\nthe output indices for self, weight and bias are actually fixed\r\n\r\n[11:51] \r\ni.e., you *must* assign self = 0, weight = 1, bias = 2\r\n\r\n[11:51] \r\nbut notice that the index allocation loop is iterating over a dictionary. So the order is nondeterministic\r\n\r\n[11:51] \r\nso, there's a bit of logic in the main path of this function for computing the index layout\r\n\r\n\r\n[11:52] \r\nand making sure everybody gets the right index. This is logic you don't want to duplicate.\r\n\r\n\r\nLuca Antiga [11:53 AM] \r\nYep. We need to parse the outputs and iterate over those outputs in the same order. Plus we need to offset indices so that they increment for every line.\r\n\r\n\r\nEdward Yang [11:53 AM] \r\nyep. My point is, we already implemented this below\r\n\r\n\r\nLuca Antiga [11:54 AM] \r\nCorrect\r\n```"}