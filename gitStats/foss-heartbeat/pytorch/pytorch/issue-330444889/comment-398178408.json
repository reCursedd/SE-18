{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398178408", "html_url": "https://github.com/pytorch/pytorch/pull/8255#issuecomment-398178408", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8255", "id": 398178408, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODE3ODQwOA==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-18T20:06:00Z", "updated_at": "2018-06-18T20:06:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As discussed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"327137574\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7903\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7903/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7903\">#7903</a> OpenMP and TBB don't play nicely together when called in sequence. This becomes apparent when run on smaller (in terms of model size) pipelines and even causes a performance penalty for larger ones. While we consider TBB to be more user friendly and it is to similar or exceeds OpenMP in microbenchmarks, there is a clear indication that this conflict makes it worth removing TBB until resolved.</p>\n<p>I'm using two benchmarks to determine three. One on <a href=\"https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/python/benchmarks/cpu_lstm_benchmark.py\">LSTMs</a>, one using <a href=\"https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/python/benchmarks/cpu_convnet_benchmark.py\">convnets</a> and one on <a href=\"https://github.com/pytorch/benchmark/blob/master/timing/python/benchmarks/cpu_reduce_benchmark.py\">reductions of Tensors</a>.</p>\n<p>lscpu</p>\n<pre><code>Architecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                80\nOn-line CPU(s) list:   0-79\nThread(s) per core:    2\nCore(s) per socket:    20\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 79\nModel name:            Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\nStepping:              1\nCPU MHz:               2200.515\nCPU max MHz:           3600.0000\nCPU min MHz:           1200.0000\nBogoMIPS:              4400.92\nVirtualization:        VT-x\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              51200K\nNUMA node0 CPU(s):     0-19,40-59\nNUMA node1 CPU(s):     20-39,60-79\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\n</code></pre>\n<p>Everything is run using these commands</p>\n<pre><code>master\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPULSTMBench --benchmark-min-time 5 --benchmark-out lstm_tbb.csv\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPUConvnets --benchmark-min-time 5 --benchmark-out convnet_tbb.csv\n\nthis branch\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPULSTMBench --benchmark-min-time 5 --benchmark-out lstm_no_tbb.csv\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPUConvnets --benchmark-min-time 5 --benchmark-out convnet_no_tbb.csv\n</code></pre>\n<p>Here is a summary of how the pipelines behave</p>\n<p>Convnets</p>\n<pre><code>                                                                   time_mean_omp  time_mean_tbb     ratio\n('arch', 'size')                      single_batch_size inference\n('alexnet', (128, 3, 224, 224))       False             False       1.588810e+06   1.587562e+06  0.999214\n                                                        True        5.226531e+05   5.215516e+05  0.997892\n                                      True              False       1.147931e+05   1.137034e+05  0.990507\n                                                        True        1.673087e+04   1.675282e+04  1.001312\n('densenet121', (32, 3, 224, 224))    False             False       5.495539e+06   5.332294e+06  0.970295\n                                                        True        1.724098e+06   1.701506e+06  0.986896\n                                      True              False       5.462541e+05   5.523636e+05  1.011184\n                                                        True        1.509744e+05   1.507845e+05  0.998742\n('inception_v3', (128, 3, 299, 299))  False             False       6.270218e+07   5.970539e+07  0.952206\n                                                        True        4.006578e+07   4.028711e+07  1.005524\n                                      True              False       5.188417e+05   5.175509e+05  0.997512\n                                                        True        1.345969e+05   1.352717e+05  1.005013\n('resnet50', (128, 3, 224, 224))      False             False       1.812910e+07   1.817211e+07  1.002372\n                                                        True        5.770941e+06   5.529934e+06  0.958238\n                                      True              False       3.405237e+05   3.506097e+05  1.029619\n                                                        True        9.377948e+04   9.620484e+04  1.025862\n('squeezenet1_0', (128, 3, 224, 224)) False             False       7.158607e+06   8.241708e+06  1.151300\n                                                        True        2.599580e+06   2.723237e+06  1.047568\n                                      True              False       2.025395e+05   1.874034e+05  0.925268\n                                                        True        6.221556e+04   6.163652e+04  0.990693\n('vgg11', (64, 3, 224, 224))          False             False       9.835194e+06   9.588080e+06  0.974874\n                                                        True        3.320521e+06   3.324367e+06  1.001158\n                                      True              False       4.675848e+05   4.765923e+05  1.019264\n                                                        True        1.115608e+05   1.126198e+05  1.009493\n</code></pre>\n<p>LSTM</p>\n<pre><code>                             time_mean_omp  time_mean_tbb     ratio\nsize                  train\n[128, 25, 1024, 1024] False   1.007327e+05   1.085608e+05  1.077712\n                      True    4.368012e+05   6.503151e+05  1.488813\n[128, 25, 2048, 2048] False   3.576995e+05   3.801865e+05  1.062866\n                      True    1.402720e+06   1.523486e+06  1.086094\n[128, 25, 4096, 4096] False   1.383664e+06   1.416808e+06  1.023954\n                      True    4.810787e+06   4.993124e+06  1.037902\n[128, 25, 512, 512]   False   4.098869e+04   5.075058e+04  1.238161\n                      True    1.725077e+05   3.652667e+05  2.117394\n[16, 25, 1024, 1024]  False   2.246846e+04   2.584505e+04  1.150281\n                      True    1.895742e+05   3.561756e+05  1.878819\n[16, 25, 2048, 2048]  False   1.006856e+05   1.031588e+05  1.024564\n                      True    7.307728e+05   8.858235e+05  1.212174\n[16, 25, 4096, 4096]  False   4.035820e+05   4.489742e+05  1.112473\n                      True    2.895958e+06   2.955301e+06  1.020492\n[16, 25, 512, 512]    False   1.150059e+04   1.438014e+04  1.250383\n                      True    5.547399e+04   1.260169e+05  2.271640\n[32, 25, 1024, 1024]  False   3.263373e+04   3.661207e+04  1.121909\n                      True    2.202593e+05   3.881302e+05  1.762151\n[32, 25, 2048, 2048]  False   1.332801e+05   1.498380e+05  1.124234\n                      True    8.251015e+05   9.707169e+05  1.176482\n[32, 25, 4096, 4096]  False   4.926930e+05   5.203439e+05  1.056122\n                      True    3.023547e+06   3.175212e+06  1.050161\n[32, 25, 512, 512]    False   1.611146e+04   1.857857e+04  1.153128\n                      True    6.992925e+04   2.074067e+05  2.965950\n[64, 15, 500, 500]    False   1.414319e+04   1.684935e+04  1.191340\n                      True    6.251698e+04   1.919383e+05  3.070179\n[64, 20, 500, 500]    False   1.852781e+04   2.199515e+04  1.187143\n                      True    8.112935e+04   2.446003e+05  3.014943\n[64, 25, 1024, 1024]  False   5.869704e+04   6.597632e+04  1.124014\n                      True    3.170311e+05   5.013353e+05  1.581344\n[64, 25, 2048, 2048]  False   2.067647e+05   2.057209e+05  0.994952\n                      True    9.700367e+05   1.100064e+06  1.134044\n[64, 25, 4096, 4096]  False   7.416225e+05   7.759166e+05  1.046242\n                      True    3.550565e+06   3.632379e+06  1.023042\n[64, 25, 500, 500]    False   2.332017e+04   2.697028e+04  1.156522\n                      True    9.533814e+04   2.693729e+05  2.825447\n[64, 25, 512, 512]    False   2.386821e+04   2.814705e+04  1.179269\n                      True    1.011653e+05   2.738765e+05  2.707217\n[64, 30, 500, 500]    False   2.806576e+04   3.235880e+04  1.152964\n                      True    1.160014e+05   2.983147e+05  2.571646\n[64, 35, 500, 500]    False   3.239543e+04   3.830646e+04  1.182465\n                      True    1.367728e+05   3.383658e+05  2.473926\n[64, 40, 500, 500]    False   3.661284e+04   4.324057e+04  1.181022\n                      True    1.545467e+05   3.804455e+05  2.461686\n[64, 45, 500, 500]    False   4.166001e+04   4.947403e+04  1.187567\n                      True    1.680951e+05   3.899506e+05  2.319821\n[64, 50, 500, 500]    False   4.640269e+04   5.331036e+04  1.148863\n                      True    1.891509e+05   4.188073e+05  2.214144\n</code></pre>\n<p>Reductions</p>\n<pre><code>framework                                         Torch  Torch Master     Ratio  Better\ncont  trans function         dims      mag\nFalse False ('mean', 'mean') (3, 0)    6     912.122876    941.700329  1.032427    True\n                             (3, 1)    6     740.484658    777.944006  1.050588    True\n                             (3, 2)    6     865.601647    895.333579  1.034348    True\n                             (3, None) 6     871.369710    917.613672  1.053070    True\n            ('prod', 'prod') (3, 0)    6     914.061697   1536.255665  1.680691    True\n                             (3, 1)    6     908.402899    961.876213  1.058865    True\n                             (3, 2)    6     893.260250    927.102735  1.037886    True\n                             (3, None) 6     935.746461    946.417881  1.011404    True\n            ('sum', 'sum')   (3, 0)    6     909.629612   1090.266941  1.198583    True\n                             (3, 1)    6     968.961737    723.008869  0.746169   False\n                             (3, 2)    6     837.462308    896.964276  1.071050    True\n                             (3, None) 6     883.105125   3642.472254  4.124619    True\n      True  ('mean', 'mean') (3, 0)    6     893.509792   1020.316406  1.141920    True\n                             (3, 1)    6     663.651006    843.271851  1.270656    True\n                             (3, 2)    6     830.082199    909.492339  1.095665    True\n                             (3, None) 6     925.906566    905.371910  0.977822   False\n            ('prod', 'prod') (3, 0)    6     874.321803    796.725109  0.911249   False\n                             (3, 1)    6     998.074155    970.187973  0.972060   False\n                             (3, 2)    6     892.377073   5037.521025  5.645059    True\n                             (3, None) 6     864.195713    873.382280  1.010630    True\n            ('sum', 'sum')   (3, 0)    6     883.150532    883.328756  1.000202    True\n                             (3, 1)    6     706.692991    813.904740  1.151709    True\n                             (3, 2)    6     830.393846    871.377515  1.049354    True\n                             (3, None) 6     927.566764    891.658756  0.961288   False\nTrue  False ('mean', 'mean') (3, 0)    6     101.413808    100.846970  0.994411   False\n                             (3, 1)    6     134.550141    134.304050  0.998171   False\n                             (3, 2)    6     120.839348    123.271828  1.020130    True\n                             (3, None) 6     102.028968    100.964635  0.989568   False\n            ('prod', 'prod') (3, 0)    6      37.397496     50.595512  1.352912    True\n                             (3, 1)    6    2309.906156   2260.965736  0.978813   False\n                             (3, 2)    6     538.915190    233.449269  0.433184   False\n                             (3, None) 6      39.095202     51.140199  1.308094    True\n            ('sum', 'sum')   (3, 0)    6      25.424164     26.853998  1.056239    True\n                             (3, 1)    6     228.845478    211.093272  0.922427   False\n                             (3, 2)    6     351.833678     61.770088  0.175566   False\n                             (3, None) 6      26.628582     27.351721  1.027156    True\n      True  ('mean', 'mean') (3, 0)    6     135.988884    134.877603  0.991828   False\n                             (3, 1)    6     166.726860    164.797547  0.988428   False\n                             (3, 2)    6     127.108735    127.582341  1.003726    True\n                             (3, None) 6     136.350892    135.341018  0.992594   False\n            ('prod', 'prod') (3, 0)    6     141.157679    139.109549  0.985490   False\n                             (3, 1)    6     526.907283    516.999232  0.981196   False\n                             (3, 2)    6     491.112516    492.466914  1.002758    True\n                             (3, None) 6     139.793561    138.295213  0.989282   False\n            ('sum', 'sum')   (3, 0)    6     140.407802    139.640030  0.994532   False\n                             (3, 1)    6     160.127446    159.641335  0.996964   False\n                             (3, 2)    6     122.370890    121.908376  0.996220   False\n                             (3, None) 6     136.217024    135.191196  0.992469   False\n</code></pre>\n<p>In conclusion this resolves the issues with the LSTM pipeline, the convnets stay roughly the same. However for vectorized reductions (sum, prod), the performance penalty is harsh when reducing over dimension 3, for a 3 dimensional tensor of size (99, 99, 99).</p>\n<p>This requires further investigation.</p>\n<p>I'll now remove the tags. We'll land this until after the issues have been resolved.</p>", "body_text": "As discussed in #7903 OpenMP and TBB don't play nicely together when called in sequence. This becomes apparent when run on smaller (in terms of model size) pipelines and even causes a performance penalty for larger ones. While we consider TBB to be more user friendly and it is to similar or exceeds OpenMP in microbenchmarks, there is a clear indication that this conflict makes it worth removing TBB until resolved.\nI'm using two benchmarks to determine three. One on LSTMs, one using convnets and one on reductions of Tensors.\nlscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                80\nOn-line CPU(s) list:   0-79\nThread(s) per core:    2\nCore(s) per socket:    20\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 79\nModel name:            Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\nStepping:              1\nCPU MHz:               2200.515\nCPU max MHz:           3600.0000\nCPU min MHz:           1200.0000\nBogoMIPS:              4400.92\nVirtualization:        VT-x\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              51200K\nNUMA node0 CPU(s):     0-19,40-59\nNUMA node1 CPU(s):     20-39,60-79\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\n\nEverything is run using these commands\nmaster\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPULSTMBench --benchmark-min-time 5 --benchmark-out lstm_tbb.csv\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPUConvnets --benchmark-min-time 5 --benchmark-out convnet_tbb.csv\n\nthis branch\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPULSTMBench --benchmark-min-time 5 --benchmark-out lstm_no_tbb.csv\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPUConvnets --benchmark-min-time 5 --benchmark-out convnet_no_tbb.csv\n\nHere is a summary of how the pipelines behave\nConvnets\n                                                                   time_mean_omp  time_mean_tbb     ratio\n('arch', 'size')                      single_batch_size inference\n('alexnet', (128, 3, 224, 224))       False             False       1.588810e+06   1.587562e+06  0.999214\n                                                        True        5.226531e+05   5.215516e+05  0.997892\n                                      True              False       1.147931e+05   1.137034e+05  0.990507\n                                                        True        1.673087e+04   1.675282e+04  1.001312\n('densenet121', (32, 3, 224, 224))    False             False       5.495539e+06   5.332294e+06  0.970295\n                                                        True        1.724098e+06   1.701506e+06  0.986896\n                                      True              False       5.462541e+05   5.523636e+05  1.011184\n                                                        True        1.509744e+05   1.507845e+05  0.998742\n('inception_v3', (128, 3, 299, 299))  False             False       6.270218e+07   5.970539e+07  0.952206\n                                                        True        4.006578e+07   4.028711e+07  1.005524\n                                      True              False       5.188417e+05   5.175509e+05  0.997512\n                                                        True        1.345969e+05   1.352717e+05  1.005013\n('resnet50', (128, 3, 224, 224))      False             False       1.812910e+07   1.817211e+07  1.002372\n                                                        True        5.770941e+06   5.529934e+06  0.958238\n                                      True              False       3.405237e+05   3.506097e+05  1.029619\n                                                        True        9.377948e+04   9.620484e+04  1.025862\n('squeezenet1_0', (128, 3, 224, 224)) False             False       7.158607e+06   8.241708e+06  1.151300\n                                                        True        2.599580e+06   2.723237e+06  1.047568\n                                      True              False       2.025395e+05   1.874034e+05  0.925268\n                                                        True        6.221556e+04   6.163652e+04  0.990693\n('vgg11', (64, 3, 224, 224))          False             False       9.835194e+06   9.588080e+06  0.974874\n                                                        True        3.320521e+06   3.324367e+06  1.001158\n                                      True              False       4.675848e+05   4.765923e+05  1.019264\n                                                        True        1.115608e+05   1.126198e+05  1.009493\n\nLSTM\n                             time_mean_omp  time_mean_tbb     ratio\nsize                  train\n[128, 25, 1024, 1024] False   1.007327e+05   1.085608e+05  1.077712\n                      True    4.368012e+05   6.503151e+05  1.488813\n[128, 25, 2048, 2048] False   3.576995e+05   3.801865e+05  1.062866\n                      True    1.402720e+06   1.523486e+06  1.086094\n[128, 25, 4096, 4096] False   1.383664e+06   1.416808e+06  1.023954\n                      True    4.810787e+06   4.993124e+06  1.037902\n[128, 25, 512, 512]   False   4.098869e+04   5.075058e+04  1.238161\n                      True    1.725077e+05   3.652667e+05  2.117394\n[16, 25, 1024, 1024]  False   2.246846e+04   2.584505e+04  1.150281\n                      True    1.895742e+05   3.561756e+05  1.878819\n[16, 25, 2048, 2048]  False   1.006856e+05   1.031588e+05  1.024564\n                      True    7.307728e+05   8.858235e+05  1.212174\n[16, 25, 4096, 4096]  False   4.035820e+05   4.489742e+05  1.112473\n                      True    2.895958e+06   2.955301e+06  1.020492\n[16, 25, 512, 512]    False   1.150059e+04   1.438014e+04  1.250383\n                      True    5.547399e+04   1.260169e+05  2.271640\n[32, 25, 1024, 1024]  False   3.263373e+04   3.661207e+04  1.121909\n                      True    2.202593e+05   3.881302e+05  1.762151\n[32, 25, 2048, 2048]  False   1.332801e+05   1.498380e+05  1.124234\n                      True    8.251015e+05   9.707169e+05  1.176482\n[32, 25, 4096, 4096]  False   4.926930e+05   5.203439e+05  1.056122\n                      True    3.023547e+06   3.175212e+06  1.050161\n[32, 25, 512, 512]    False   1.611146e+04   1.857857e+04  1.153128\n                      True    6.992925e+04   2.074067e+05  2.965950\n[64, 15, 500, 500]    False   1.414319e+04   1.684935e+04  1.191340\n                      True    6.251698e+04   1.919383e+05  3.070179\n[64, 20, 500, 500]    False   1.852781e+04   2.199515e+04  1.187143\n                      True    8.112935e+04   2.446003e+05  3.014943\n[64, 25, 1024, 1024]  False   5.869704e+04   6.597632e+04  1.124014\n                      True    3.170311e+05   5.013353e+05  1.581344\n[64, 25, 2048, 2048]  False   2.067647e+05   2.057209e+05  0.994952\n                      True    9.700367e+05   1.100064e+06  1.134044\n[64, 25, 4096, 4096]  False   7.416225e+05   7.759166e+05  1.046242\n                      True    3.550565e+06   3.632379e+06  1.023042\n[64, 25, 500, 500]    False   2.332017e+04   2.697028e+04  1.156522\n                      True    9.533814e+04   2.693729e+05  2.825447\n[64, 25, 512, 512]    False   2.386821e+04   2.814705e+04  1.179269\n                      True    1.011653e+05   2.738765e+05  2.707217\n[64, 30, 500, 500]    False   2.806576e+04   3.235880e+04  1.152964\n                      True    1.160014e+05   2.983147e+05  2.571646\n[64, 35, 500, 500]    False   3.239543e+04   3.830646e+04  1.182465\n                      True    1.367728e+05   3.383658e+05  2.473926\n[64, 40, 500, 500]    False   3.661284e+04   4.324057e+04  1.181022\n                      True    1.545467e+05   3.804455e+05  2.461686\n[64, 45, 500, 500]    False   4.166001e+04   4.947403e+04  1.187567\n                      True    1.680951e+05   3.899506e+05  2.319821\n[64, 50, 500, 500]    False   4.640269e+04   5.331036e+04  1.148863\n                      True    1.891509e+05   4.188073e+05  2.214144\n\nReductions\nframework                                         Torch  Torch Master     Ratio  Better\ncont  trans function         dims      mag\nFalse False ('mean', 'mean') (3, 0)    6     912.122876    941.700329  1.032427    True\n                             (3, 1)    6     740.484658    777.944006  1.050588    True\n                             (3, 2)    6     865.601647    895.333579  1.034348    True\n                             (3, None) 6     871.369710    917.613672  1.053070    True\n            ('prod', 'prod') (3, 0)    6     914.061697   1536.255665  1.680691    True\n                             (3, 1)    6     908.402899    961.876213  1.058865    True\n                             (3, 2)    6     893.260250    927.102735  1.037886    True\n                             (3, None) 6     935.746461    946.417881  1.011404    True\n            ('sum', 'sum')   (3, 0)    6     909.629612   1090.266941  1.198583    True\n                             (3, 1)    6     968.961737    723.008869  0.746169   False\n                             (3, 2)    6     837.462308    896.964276  1.071050    True\n                             (3, None) 6     883.105125   3642.472254  4.124619    True\n      True  ('mean', 'mean') (3, 0)    6     893.509792   1020.316406  1.141920    True\n                             (3, 1)    6     663.651006    843.271851  1.270656    True\n                             (3, 2)    6     830.082199    909.492339  1.095665    True\n                             (3, None) 6     925.906566    905.371910  0.977822   False\n            ('prod', 'prod') (3, 0)    6     874.321803    796.725109  0.911249   False\n                             (3, 1)    6     998.074155    970.187973  0.972060   False\n                             (3, 2)    6     892.377073   5037.521025  5.645059    True\n                             (3, None) 6     864.195713    873.382280  1.010630    True\n            ('sum', 'sum')   (3, 0)    6     883.150532    883.328756  1.000202    True\n                             (3, 1)    6     706.692991    813.904740  1.151709    True\n                             (3, 2)    6     830.393846    871.377515  1.049354    True\n                             (3, None) 6     927.566764    891.658756  0.961288   False\nTrue  False ('mean', 'mean') (3, 0)    6     101.413808    100.846970  0.994411   False\n                             (3, 1)    6     134.550141    134.304050  0.998171   False\n                             (3, 2)    6     120.839348    123.271828  1.020130    True\n                             (3, None) 6     102.028968    100.964635  0.989568   False\n            ('prod', 'prod') (3, 0)    6      37.397496     50.595512  1.352912    True\n                             (3, 1)    6    2309.906156   2260.965736  0.978813   False\n                             (3, 2)    6     538.915190    233.449269  0.433184   False\n                             (3, None) 6      39.095202     51.140199  1.308094    True\n            ('sum', 'sum')   (3, 0)    6      25.424164     26.853998  1.056239    True\n                             (3, 1)    6     228.845478    211.093272  0.922427   False\n                             (3, 2)    6     351.833678     61.770088  0.175566   False\n                             (3, None) 6      26.628582     27.351721  1.027156    True\n      True  ('mean', 'mean') (3, 0)    6     135.988884    134.877603  0.991828   False\n                             (3, 1)    6     166.726860    164.797547  0.988428   False\n                             (3, 2)    6     127.108735    127.582341  1.003726    True\n                             (3, None) 6     136.350892    135.341018  0.992594   False\n            ('prod', 'prod') (3, 0)    6     141.157679    139.109549  0.985490   False\n                             (3, 1)    6     526.907283    516.999232  0.981196   False\n                             (3, 2)    6     491.112516    492.466914  1.002758    True\n                             (3, None) 6     139.793561    138.295213  0.989282   False\n            ('sum', 'sum')   (3, 0)    6     140.407802    139.640030  0.994532   False\n                             (3, 1)    6     160.127446    159.641335  0.996964   False\n                             (3, 2)    6     122.370890    121.908376  0.996220   False\n                             (3, None) 6     136.217024    135.191196  0.992469   False\n\nIn conclusion this resolves the issues with the LSTM pipeline, the convnets stay roughly the same. However for vectorized reductions (sum, prod), the performance penalty is harsh when reducing over dimension 3, for a 3 dimensional tensor of size (99, 99, 99).\nThis requires further investigation.\nI'll now remove the tags. We'll land this until after the issues have been resolved.", "body": "As discussed in https://github.com/pytorch/pytorch/issues/7903 OpenMP and TBB don't play nicely together when called in sequence. This becomes apparent when run on smaller (in terms of model size) pipelines and even causes a performance penalty for larger ones. While we consider TBB to be more user friendly and it is to similar or exceeds OpenMP in microbenchmarks, there is a clear indication that this conflict makes it worth removing TBB until resolved.\r\n\r\nI'm using two benchmarks to determine three. One on [LSTMs](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/python/benchmarks/cpu_lstm_benchmark.py), one using [convnets](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/python/benchmarks/cpu_convnet_benchmark.py) and one on [reductions of Tensors](https://github.com/pytorch/benchmark/blob/master/timing/python/benchmarks/cpu_reduce_benchmark.py). \r\n\r\nlscpu\r\n\r\n```\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                80\r\nOn-line CPU(s) list:   0-79\r\nThread(s) per core:    2\r\nCore(s) per socket:    20\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 79\r\nModel name:            Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\r\nStepping:              1\r\nCPU MHz:               2200.515\r\nCPU max MHz:           3600.0000\r\nCPU min MHz:           1200.0000\r\nBogoMIPS:              4400.92\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              51200K\r\nNUMA node0 CPU(s):     0-19,40-59\r\nNUMA node1 CPU(s):     20-39,60-79\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\r\n```\r\n\r\nEverything is run using these commands\r\n\r\n```\r\nmaster\r\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPULSTMBench --benchmark-min-time 5 --benchmark-out lstm_tbb.csv\r\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPUConvnets --benchmark-min-time 5 --benchmark-out convnet_tbb.csv\r\n\r\nthis branch\r\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPULSTMBench --benchmark-min-time 5 --benchmark-out lstm_no_tbb.csv\r\nOMP_NUM_THREADS=20 numactl --membind=0 --cpubind=0 python run.py --include CPUConvnets --benchmark-min-time 5 --benchmark-out convnet_no_tbb.csv\r\n```\r\n\r\nHere is a summary of how the pipelines behave\r\n\r\nConvnets\r\n\r\n```\r\n                                                                   time_mean_omp  time_mean_tbb     ratio\r\n('arch', 'size')                      single_batch_size inference\r\n('alexnet', (128, 3, 224, 224))       False             False       1.588810e+06   1.587562e+06  0.999214\r\n                                                        True        5.226531e+05   5.215516e+05  0.997892\r\n                                      True              False       1.147931e+05   1.137034e+05  0.990507\r\n                                                        True        1.673087e+04   1.675282e+04  1.001312\r\n('densenet121', (32, 3, 224, 224))    False             False       5.495539e+06   5.332294e+06  0.970295\r\n                                                        True        1.724098e+06   1.701506e+06  0.986896\r\n                                      True              False       5.462541e+05   5.523636e+05  1.011184\r\n                                                        True        1.509744e+05   1.507845e+05  0.998742\r\n('inception_v3', (128, 3, 299, 299))  False             False       6.270218e+07   5.970539e+07  0.952206\r\n                                                        True        4.006578e+07   4.028711e+07  1.005524\r\n                                      True              False       5.188417e+05   5.175509e+05  0.997512\r\n                                                        True        1.345969e+05   1.352717e+05  1.005013\r\n('resnet50', (128, 3, 224, 224))      False             False       1.812910e+07   1.817211e+07  1.002372\r\n                                                        True        5.770941e+06   5.529934e+06  0.958238\r\n                                      True              False       3.405237e+05   3.506097e+05  1.029619\r\n                                                        True        9.377948e+04   9.620484e+04  1.025862\r\n('squeezenet1_0', (128, 3, 224, 224)) False             False       7.158607e+06   8.241708e+06  1.151300\r\n                                                        True        2.599580e+06   2.723237e+06  1.047568\r\n                                      True              False       2.025395e+05   1.874034e+05  0.925268\r\n                                                        True        6.221556e+04   6.163652e+04  0.990693\r\n('vgg11', (64, 3, 224, 224))          False             False       9.835194e+06   9.588080e+06  0.974874\r\n                                                        True        3.320521e+06   3.324367e+06  1.001158\r\n                                      True              False       4.675848e+05   4.765923e+05  1.019264\r\n                                                        True        1.115608e+05   1.126198e+05  1.009493\r\n```\r\n\r\nLSTM\r\n\r\n```\r\n                             time_mean_omp  time_mean_tbb     ratio\r\nsize                  train\r\n[128, 25, 1024, 1024] False   1.007327e+05   1.085608e+05  1.077712\r\n                      True    4.368012e+05   6.503151e+05  1.488813\r\n[128, 25, 2048, 2048] False   3.576995e+05   3.801865e+05  1.062866\r\n                      True    1.402720e+06   1.523486e+06  1.086094\r\n[128, 25, 4096, 4096] False   1.383664e+06   1.416808e+06  1.023954\r\n                      True    4.810787e+06   4.993124e+06  1.037902\r\n[128, 25, 512, 512]   False   4.098869e+04   5.075058e+04  1.238161\r\n                      True    1.725077e+05   3.652667e+05  2.117394\r\n[16, 25, 1024, 1024]  False   2.246846e+04   2.584505e+04  1.150281\r\n                      True    1.895742e+05   3.561756e+05  1.878819\r\n[16, 25, 2048, 2048]  False   1.006856e+05   1.031588e+05  1.024564\r\n                      True    7.307728e+05   8.858235e+05  1.212174\r\n[16, 25, 4096, 4096]  False   4.035820e+05   4.489742e+05  1.112473\r\n                      True    2.895958e+06   2.955301e+06  1.020492\r\n[16, 25, 512, 512]    False   1.150059e+04   1.438014e+04  1.250383\r\n                      True    5.547399e+04   1.260169e+05  2.271640\r\n[32, 25, 1024, 1024]  False   3.263373e+04   3.661207e+04  1.121909\r\n                      True    2.202593e+05   3.881302e+05  1.762151\r\n[32, 25, 2048, 2048]  False   1.332801e+05   1.498380e+05  1.124234\r\n                      True    8.251015e+05   9.707169e+05  1.176482\r\n[32, 25, 4096, 4096]  False   4.926930e+05   5.203439e+05  1.056122\r\n                      True    3.023547e+06   3.175212e+06  1.050161\r\n[32, 25, 512, 512]    False   1.611146e+04   1.857857e+04  1.153128\r\n                      True    6.992925e+04   2.074067e+05  2.965950\r\n[64, 15, 500, 500]    False   1.414319e+04   1.684935e+04  1.191340\r\n                      True    6.251698e+04   1.919383e+05  3.070179\r\n[64, 20, 500, 500]    False   1.852781e+04   2.199515e+04  1.187143\r\n                      True    8.112935e+04   2.446003e+05  3.014943\r\n[64, 25, 1024, 1024]  False   5.869704e+04   6.597632e+04  1.124014\r\n                      True    3.170311e+05   5.013353e+05  1.581344\r\n[64, 25, 2048, 2048]  False   2.067647e+05   2.057209e+05  0.994952\r\n                      True    9.700367e+05   1.100064e+06  1.134044\r\n[64, 25, 4096, 4096]  False   7.416225e+05   7.759166e+05  1.046242\r\n                      True    3.550565e+06   3.632379e+06  1.023042\r\n[64, 25, 500, 500]    False   2.332017e+04   2.697028e+04  1.156522\r\n                      True    9.533814e+04   2.693729e+05  2.825447\r\n[64, 25, 512, 512]    False   2.386821e+04   2.814705e+04  1.179269\r\n                      True    1.011653e+05   2.738765e+05  2.707217\r\n[64, 30, 500, 500]    False   2.806576e+04   3.235880e+04  1.152964\r\n                      True    1.160014e+05   2.983147e+05  2.571646\r\n[64, 35, 500, 500]    False   3.239543e+04   3.830646e+04  1.182465\r\n                      True    1.367728e+05   3.383658e+05  2.473926\r\n[64, 40, 500, 500]    False   3.661284e+04   4.324057e+04  1.181022\r\n                      True    1.545467e+05   3.804455e+05  2.461686\r\n[64, 45, 500, 500]    False   4.166001e+04   4.947403e+04  1.187567\r\n                      True    1.680951e+05   3.899506e+05  2.319821\r\n[64, 50, 500, 500]    False   4.640269e+04   5.331036e+04  1.148863\r\n                      True    1.891509e+05   4.188073e+05  2.214144\r\n```\r\n\r\nReductions\r\n\r\n```\r\nframework                                         Torch  Torch Master     Ratio  Better\r\ncont  trans function         dims      mag\r\nFalse False ('mean', 'mean') (3, 0)    6     912.122876    941.700329  1.032427    True\r\n                             (3, 1)    6     740.484658    777.944006  1.050588    True\r\n                             (3, 2)    6     865.601647    895.333579  1.034348    True\r\n                             (3, None) 6     871.369710    917.613672  1.053070    True\r\n            ('prod', 'prod') (3, 0)    6     914.061697   1536.255665  1.680691    True\r\n                             (3, 1)    6     908.402899    961.876213  1.058865    True\r\n                             (3, 2)    6     893.260250    927.102735  1.037886    True\r\n                             (3, None) 6     935.746461    946.417881  1.011404    True\r\n            ('sum', 'sum')   (3, 0)    6     909.629612   1090.266941  1.198583    True\r\n                             (3, 1)    6     968.961737    723.008869  0.746169   False\r\n                             (3, 2)    6     837.462308    896.964276  1.071050    True\r\n                             (3, None) 6     883.105125   3642.472254  4.124619    True\r\n      True  ('mean', 'mean') (3, 0)    6     893.509792   1020.316406  1.141920    True\r\n                             (3, 1)    6     663.651006    843.271851  1.270656    True\r\n                             (3, 2)    6     830.082199    909.492339  1.095665    True\r\n                             (3, None) 6     925.906566    905.371910  0.977822   False\r\n            ('prod', 'prod') (3, 0)    6     874.321803    796.725109  0.911249   False\r\n                             (3, 1)    6     998.074155    970.187973  0.972060   False\r\n                             (3, 2)    6     892.377073   5037.521025  5.645059    True\r\n                             (3, None) 6     864.195713    873.382280  1.010630    True\r\n            ('sum', 'sum')   (3, 0)    6     883.150532    883.328756  1.000202    True\r\n                             (3, 1)    6     706.692991    813.904740  1.151709    True\r\n                             (3, 2)    6     830.393846    871.377515  1.049354    True\r\n                             (3, None) 6     927.566764    891.658756  0.961288   False\r\nTrue  False ('mean', 'mean') (3, 0)    6     101.413808    100.846970  0.994411   False\r\n                             (3, 1)    6     134.550141    134.304050  0.998171   False\r\n                             (3, 2)    6     120.839348    123.271828  1.020130    True\r\n                             (3, None) 6     102.028968    100.964635  0.989568   False\r\n            ('prod', 'prod') (3, 0)    6      37.397496     50.595512  1.352912    True\r\n                             (3, 1)    6    2309.906156   2260.965736  0.978813   False\r\n                             (3, 2)    6     538.915190    233.449269  0.433184   False\r\n                             (3, None) 6      39.095202     51.140199  1.308094    True\r\n            ('sum', 'sum')   (3, 0)    6      25.424164     26.853998  1.056239    True\r\n                             (3, 1)    6     228.845478    211.093272  0.922427   False\r\n                             (3, 2)    6     351.833678     61.770088  0.175566   False\r\n                             (3, None) 6      26.628582     27.351721  1.027156    True\r\n      True  ('mean', 'mean') (3, 0)    6     135.988884    134.877603  0.991828   False\r\n                             (3, 1)    6     166.726860    164.797547  0.988428   False\r\n                             (3, 2)    6     127.108735    127.582341  1.003726    True\r\n                             (3, None) 6     136.350892    135.341018  0.992594   False\r\n            ('prod', 'prod') (3, 0)    6     141.157679    139.109549  0.985490   False\r\n                             (3, 1)    6     526.907283    516.999232  0.981196   False\r\n                             (3, 2)    6     491.112516    492.466914  1.002758    True\r\n                             (3, None) 6     139.793561    138.295213  0.989282   False\r\n            ('sum', 'sum')   (3, 0)    6     140.407802    139.640030  0.994532   False\r\n                             (3, 1)    6     160.127446    159.641335  0.996964   False\r\n                             (3, 2)    6     122.370890    121.908376  0.996220   False\r\n                             (3, None) 6     136.217024    135.191196  0.992469   False\r\n```\r\n\r\nIn conclusion this resolves the issues with the LSTM pipeline, the convnets stay roughly the same. However for vectorized reductions (sum, prod), the performance penalty is harsh when reducing over dimension 3, for a 3 dimensional tensor of size (99, 99, 99). \r\n\r\nThis requires further investigation.\r\n\r\nI'll now remove the tags. We'll land this until after the issues have been resolved.\r\n"}