{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/438118764", "html_url": "https://github.com/pytorch/pytorch/pull/2811#issuecomment-438118764", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2811", "id": 438118764, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODExODc2NA==", "user": {"login": "Amir-Arsalan", "id": 6061721, "node_id": "MDQ6VXNlcjYwNjE3MjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/6061721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Amir-Arsalan", "html_url": "https://github.com/Amir-Arsalan", "followers_url": "https://api.github.com/users/Amir-Arsalan/followers", "following_url": "https://api.github.com/users/Amir-Arsalan/following{/other_user}", "gists_url": "https://api.github.com/users/Amir-Arsalan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Amir-Arsalan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Amir-Arsalan/subscriptions", "organizations_url": "https://api.github.com/users/Amir-Arsalan/orgs", "repos_url": "https://api.github.com/users/Amir-Arsalan/repos", "events_url": "https://api.github.com/users/Amir-Arsalan/events{/privacy}", "received_events_url": "https://api.github.com/users/Amir-Arsalan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-13T03:23:46Z", "updated_at": "2018-11-13T04:26:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> Has there been any updates on this? I want to be able to use <code>fork</code> instead of <code>spawn</code> for running my processes as my work depends heavily on the benefits that <code>fork</code>ing provides.</p>\n<p>I just noticed that I cannot transfer my tensors to GPU when in a <code>fork</code>ed process from the <code>multiprocessing.Process</code> module. I do not even do <code>import torch</code> before I call <code>multiprocessing.Process</code> process via the <code>.start()</code> function but I get the following error:</p>\n<pre><code>File \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 162, in _lazy_init\n   torch._C._cuda_init()\nRuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:51\n</code></pre>\n<p>So basically: 1) I run the process 2) do <code>import torch</code> inside of the <code>fork</code>ed process 3) call <code>torch.manual_seed()</code> and <code>torch.cuda.manual_seed()</code> functions inside the process 4) convert a Numpy tensor to a PyTorch tensor 5) transfer the PyTorch tensor to GPU via the <code>.cuda()</code> function. 6) The above error is thrown</p>\n<p>Update:<br>\nI noticed that some of the other packages that I have to import earlier on call the CUDA drivers and that's potentially why I am facing this issue. I really hope there is gonna be a workaround for this.</p>\n<p>I opened an issue for this here <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"380054310\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13883\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13883/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13883\">#13883</a></p>", "body_text": "@ezyang Has there been any updates on this? I want to be able to use fork instead of spawn for running my processes as my work depends heavily on the benefits that forking provides.\nI just noticed that I cannot transfer my tensors to GPU when in a forked process from the multiprocessing.Process module. I do not even do import torch before I call multiprocessing.Process process via the .start() function but I get the following error:\nFile \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 162, in _lazy_init\n   torch._C._cuda_init()\nRuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:51\n\nSo basically: 1) I run the process 2) do import torch inside of the forked process 3) call torch.manual_seed() and torch.cuda.manual_seed() functions inside the process 4) convert a Numpy tensor to a PyTorch tensor 5) transfer the PyTorch tensor to GPU via the .cuda() function. 6) The above error is thrown\nUpdate:\nI noticed that some of the other packages that I have to import earlier on call the CUDA drivers and that's potentially why I am facing this issue. I really hope there is gonna be a workaround for this.\nI opened an issue for this here #13883", "body": "@ezyang Has there been any updates on this? I want to be able to use `fork` instead of `spawn` for running my processes as my work depends heavily on the benefits that `fork`ing provides.  \r\n\r\nI just noticed that I cannot transfer my tensors to GPU when in a `fork`ed process from the `multiprocessing.Process` module. I do not even do `import torch` before I call `multiprocessing.Process` process via the `.start()` function but I get the following error:\r\n\r\n ```\r\nFile \"/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\", line 162, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: cuda runtime error (3) : initialization error at /pytorch/aten/src/THC/THCGeneral.cpp:51\r\n```\r\n\r\nSo basically: 1) I run the process 2) do `import torch` inside of the `fork`ed process 3) call `torch.manual_seed()` and `torch.cuda.manual_seed()` functions inside the process 4) convert a Numpy tensor to a PyTorch tensor 5) transfer the PyTorch tensor to GPU via the `.cuda()` function. 6) The above error is thrown\r\n\r\nUpdate: \r\nI noticed that some of the other packages that I have to import earlier on call the CUDA drivers and that's potentially why I am facing this issue. I really hope there is gonna be a workaround for this.\r\n\r\nI opened an issue for this here #13883"}