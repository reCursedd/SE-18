{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/372953680", "html_url": "https://github.com/pytorch/pytorch/issues/5212#issuecomment-372953680", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5212", "id": 372953680, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Mjk1MzY4MA==", "user": {"login": "ptrblck", "id": 11662379, "node_id": "MDQ6VXNlcjExNjYyMzc5", "avatar_url": "https://avatars3.githubusercontent.com/u/11662379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ptrblck", "html_url": "https://github.com/ptrblck", "followers_url": "https://api.github.com/users/ptrblck/followers", "following_url": "https://api.github.com/users/ptrblck/following{/other_user}", "gists_url": "https://api.github.com/users/ptrblck/gists{/gist_id}", "starred_url": "https://api.github.com/users/ptrblck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ptrblck/subscriptions", "organizations_url": "https://api.github.com/users/ptrblck/orgs", "repos_url": "https://api.github.com/users/ptrblck/repos", "events_url": "https://api.github.com/users/ptrblck/events{/privacy}", "received_events_url": "https://api.github.com/users/ptrblck/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T09:17:26Z", "updated_at": "2018-03-14T09:17:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1092464\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Diego999\">@Diego999</a> I already have the code on my machine, but was talking to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> about the right syntax.<br>\nSummary:</p>\n<p>One issue is that numpy's <code>.split</code> function behaves differently than the current Pytorch implementation. I'm no sure how to deal with this, since I like the PyTorch behavior, but maybe the core devs would like to make the API as numpy-like as possible.</p>\n<ul>\n<li><code>np.split</code> takes an integer or a list. The integer defines the number of <code>sections</code>, while the list defines <code>split_indices</code>.</li>\n<li><code>torch.split</code> currently also takes an integer or a list. The integer defines the <code>split_size</code>, while the list defines <code>split_sizes</code> for each section.</li>\n</ul>\n<p><strong>numpy integer example</strong>:</p>\n<pre><code>a = np.random.rand(10, 20)\ntmp0 = np.split(a, indices_or_sections=5, axis=0) # split into 5 sections\nfor t in tmp0:\n    print(t.shape)\n</code></pre>\n<p>This will return 5 sections, each of a size of <code>[2, 20]</code>.</p>\n<pre><code>np.split(a, indices_or_sections=7, axis=0) # error, since no equal division\n</code></pre>\n<p>Won't work, since <code>axis=0</code> is not divisible by 7.</p>\n<p><strong>PyTorch integer example</strong>:</p>\n<pre><code>x = torch.randn(10, 20)\ntmp2 = torch.split(x, split_size_or_sections=4, dim=0) # use size 4\nfor t in tmp2:\n    print(t.shape) # last split might be smaller\n</code></pre>\n<p>Will return 3 splits with dims: <code>[4, 20], [4, 20], [2, 20]</code>. <code>dim=0</code> does not have to be divisible by <code>split_size_or_sections</code>. In my opinion it's more user-friendly than numpy's approach.</p>\n<p><strong>numpy list example</strong>:</p>\n<pre><code>tmp1 = np.split(a, [5, 7], 0) # use indices ([:5], [5:7], [7:])\nfor t in tmp1:\nprint(t.shape)\n</code></pre>\n<p>Will split <code>a</code> into 3 sections using the list entries as indices: <code>[:5], [5:7], [7:]</code>.</p>\n<p><strong>PyTorch list example</strong>:</p>\n<pre><code>tmp3 = torch.split(x, split_size_or_sections=[5, 2, 3], dim=0)\nfor t in tmp3:\n    print(t.shape)\n</code></pre>\n<p>Will return 3 splits with the defined dimensions: <code>[5, 20], [2, 20], [3, 20]</code>. Currently the sum of the list entries has to be equal to the specified dimension size of the <code>Tensor</code>.</p>\n<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1092464\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Diego999\">@Diego999</a> suggested we could modify the code easily to be less strict about the sizes:</p>\n<pre><code>torch.split(x, split_size_or_sections=[5, 4], dim=0) # currently error, since 5+4 != dim(0)\n# Could return Tensors of size [5, 20], [4, 20] and [1, 20]\n</code></pre>\n<p>What do the others think about this issue?<br>\nAre we good to go with relaxing the size constraints of <code>torch.split</code>?<br>\nHow should we deal with numpy's approach of splitting using indices instead of sizes?</p>\n<ul>\n<li>\n<p>Add another split function / refactor the current <code>torch.split</code></p>\n</li>\n<li>\n<p>Specify a flag to use the list as sizes or indices, e.g. <code>use_as_index=False</code></p>\n</li>\n<li>\n<p>Just keep the PyTorch approach and hope numpy users won't be confused</p>\n</li>\n</ul>\n<p><a href=\"https://gist.github.com/ptrblck/a04b4ba8d641267746c2e85b3da6c899\">Here</a> is a complete gist for reproduction.</p>", "body_text": "@Diego999 I already have the code on my machine, but was talking to @apaszke about the right syntax.\nSummary:\nOne issue is that numpy's .split function behaves differently than the current Pytorch implementation. I'm no sure how to deal with this, since I like the PyTorch behavior, but maybe the core devs would like to make the API as numpy-like as possible.\n\nnp.split takes an integer or a list. The integer defines the number of sections, while the list defines split_indices.\ntorch.split currently also takes an integer or a list. The integer defines the split_size, while the list defines split_sizes for each section.\n\nnumpy integer example:\na = np.random.rand(10, 20)\ntmp0 = np.split(a, indices_or_sections=5, axis=0) # split into 5 sections\nfor t in tmp0:\n    print(t.shape)\n\nThis will return 5 sections, each of a size of [2, 20].\nnp.split(a, indices_or_sections=7, axis=0) # error, since no equal division\n\nWon't work, since axis=0 is not divisible by 7.\nPyTorch integer example:\nx = torch.randn(10, 20)\ntmp2 = torch.split(x, split_size_or_sections=4, dim=0) # use size 4\nfor t in tmp2:\n    print(t.shape) # last split might be smaller\n\nWill return 3 splits with dims: [4, 20], [4, 20], [2, 20]. dim=0 does not have to be divisible by split_size_or_sections. In my opinion it's more user-friendly than numpy's approach.\nnumpy list example:\ntmp1 = np.split(a, [5, 7], 0) # use indices ([:5], [5:7], [7:])\nfor t in tmp1:\nprint(t.shape)\n\nWill split a into 3 sections using the list entries as indices: [:5], [5:7], [7:].\nPyTorch list example:\ntmp3 = torch.split(x, split_size_or_sections=[5, 2, 3], dim=0)\nfor t in tmp3:\n    print(t.shape)\n\nWill return 3 splits with the defined dimensions: [5, 20], [2, 20], [3, 20]. Currently the sum of the list entries has to be equal to the specified dimension size of the Tensor.\nAs @Diego999 suggested we could modify the code easily to be less strict about the sizes:\ntorch.split(x, split_size_or_sections=[5, 4], dim=0) # currently error, since 5+4 != dim(0)\n# Could return Tensors of size [5, 20], [4, 20] and [1, 20]\n\nWhat do the others think about this issue?\nAre we good to go with relaxing the size constraints of torch.split?\nHow should we deal with numpy's approach of splitting using indices instead of sizes?\n\n\nAdd another split function / refactor the current torch.split\n\n\nSpecify a flag to use the list as sizes or indices, e.g. use_as_index=False\n\n\nJust keep the PyTorch approach and hope numpy users won't be confused\n\n\nHere is a complete gist for reproduction.", "body": "@Diego999 I already have the code on my machine, but was talking to @apaszke about the right syntax.\r\nSummary:\r\n\r\nOne issue is that numpy's `.split` function behaves differently than the current Pytorch implementation. I'm no sure how to deal with this, since I like the PyTorch behavior, but maybe the core devs would like to make the API as numpy-like as possible.\r\n\r\n* `np.split` takes an integer or a list. The integer defines the number of `sections`, while the list defines `split_indices`. \r\n* `torch.split` currently also takes an integer or a list. The integer defines the `split_size`, while the list defines `split_sizes` for each section.\r\n\r\n**numpy integer example**:\r\n```\r\na = np.random.rand(10, 20)\r\ntmp0 = np.split(a, indices_or_sections=5, axis=0) # split into 5 sections\r\nfor t in tmp0:\r\n    print(t.shape)\r\n```\r\nThis will return 5 sections, each of a size of `[2, 20]`.\r\n\r\n```\r\nnp.split(a, indices_or_sections=7, axis=0) # error, since no equal division\r\n```\r\nWon't work, since `axis=0` is not divisible by 7.\r\n\r\n**PyTorch integer example**:\r\n```\r\nx = torch.randn(10, 20)\r\ntmp2 = torch.split(x, split_size_or_sections=4, dim=0) # use size 4\r\nfor t in tmp2:\r\n    print(t.shape) # last split might be smaller\r\n```\r\nWill return 3 splits with dims: `[4, 20], [4, 20], [2, 20]`. `dim=0` does not have to be divisible by `split_size_or_sections`. In my opinion it's more user-friendly than numpy's approach.\r\n\r\n**numpy list example**:\r\n```\r\ntmp1 = np.split(a, [5, 7], 0) # use indices ([:5], [5:7], [7:])\r\nfor t in tmp1:\r\nprint(t.shape)\r\n```\r\nWill split `a` into 3 sections using the list entries as indices: `[:5], [5:7], [7:]`.\r\n\r\n**PyTorch list example**:\r\n```\r\ntmp3 = torch.split(x, split_size_or_sections=[5, 2, 3], dim=0)\r\nfor t in tmp3:\r\n    print(t.shape)\r\n```\r\nWill return 3 splits with the defined dimensions: `[5, 20], [2, 20], [3, 20]`. Currently the sum of the list entries has to be equal to the specified dimension size of the `Tensor`.\r\n\r\nAs @Diego999 suggested we could modify the code easily to be less strict about the sizes:\r\n```\r\ntorch.split(x, split_size_or_sections=[5, 4], dim=0) # currently error, since 5+4 != dim(0)\r\n# Could return Tensors of size [5, 20], [4, 20] and [1, 20]\r\n```\r\n\r\nWhat do the others think about this issue?\r\nAre we good to go with relaxing the size constraints of `torch.split`?\r\nHow should we deal with numpy's approach of splitting using indices instead of sizes?\r\n\r\n- Add another split function / refactor the current `torch.split`\r\n\r\n- Specify a flag to use the list as sizes or indices, e.g. `use_as_index=False`\r\n\r\n- Just keep the PyTorch approach and hope numpy users won't be confused\r\n\r\n[Here](https://gist.github.com/ptrblck/a04b4ba8d641267746c2e85b3da6c899) is a complete gist for reproduction.\r\n\r\n"}