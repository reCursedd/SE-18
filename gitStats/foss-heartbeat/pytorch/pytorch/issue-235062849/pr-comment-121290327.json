{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121290327", "pull_request_review_id": 43327604, "id": 121290327, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMTI5MDMyNw==", "diff_hunk": "@@ -153,3 +153,54 @@ def __repr__(self):\n         return self.__class__.__name__ + ' (' \\\n             + 'p=' + str(self.p) \\\n             + inplace_str + ')'\n+\n+\n+class AlphaDropout(Module):\n+    r\"\"\"Applies Alpha Dropout over the input.\n+\n+    Alpha Dropout is a type of Dropout that maintains the self-normalizing\n+    property.\n+    For an input with zero mean and unit standard deviation, the output of\n+    Alpha Dropout maintains the original mean and standard deviation of the\n+    input.\n+    Alpha Dropout goes hand-in-hand with SELU activation function, which ensures\n+    that the outputs have zero mean and unit standard deviation.\n+\n+    During training, it randomly masks some of the elements of the input\n+    tensor with probability *p* using samples from a bernoulli distribution.\n+    The elements to masked are randomized on every forward call, and scaled\n+    and shifted to maintain zero mean and unit standard deviation.\n+\n+    During evaluation the module simply computes an identity function.\n+\n+    More details can be found in the paper `Self-Normalizing Neural Networks`_ .\n+\n+    Args:\n+        p (float): probability of an element to be dropped. Default: 0.5\n+\n+    Shape:\n+        - Input: `Any`. Input can be of any shape\n+        - Output: `Same`. Output is of the same shape as input\n+\n+    Examples::\n+\n+        >>> m = nn.AlphaDropout(p=0.2)\n+        >>> input = autograd.Variable(torch.randn(20, 16))\n+        >>> output = m(input)\n+\n+    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515\n+    \"\"\"\n+\n+    def __init__(self, p=0.5):\n+        super(AlphaDropout, self).__init__()\n+        if p < 0 or p > 1:", "path": "torch/nn/modules/dropout.py", "position": 44, "original_position": 44, "commit_id": "c06c7699168a15291b7454b8771ea9960456fe06", "original_commit_id": "7ab4b5f6a1d4c763b9af4bf622c96e1a7e4bd4d2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "These checks should be consistent between module and a function.", "created_at": "2017-06-11T20:06:40Z", "updated_at": "2018-11-23T15:33:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/1775#discussion_r121290327", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1775", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121290327"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1775#discussion_r121290327"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1775"}}, "body_html": "<p>These checks should be consistent between module and a function.</p>", "body_text": "These checks should be consistent between module and a function."}