{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/329738356", "html_url": "https://github.com/pytorch/pytorch/pull/2657#issuecomment-329738356", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2657", "id": 329738356, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTczODM1Ng==", "user": {"login": "dhpollack", "id": 368699, "node_id": "MDQ6VXNlcjM2ODY5OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/368699?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dhpollack", "html_url": "https://github.com/dhpollack", "followers_url": "https://api.github.com/users/dhpollack/followers", "following_url": "https://api.github.com/users/dhpollack/following{/other_user}", "gists_url": "https://api.github.com/users/dhpollack/gists{/gist_id}", "starred_url": "https://api.github.com/users/dhpollack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dhpollack/subscriptions", "organizations_url": "https://api.github.com/users/dhpollack/orgs", "repos_url": "https://api.github.com/users/dhpollack/repos", "events_url": "https://api.github.com/users/dhpollack/events{/privacy}", "received_events_url": "https://api.github.com/users/dhpollack/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-15T09:59:03Z", "updated_at": "2017-09-15T09:59:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yea, I believe this is \"backwards compatible\".  Unless you had code that didn't run before, i.e. a 4D tensor trying to pad one dimension.  So if you had a tensor of size = (2, 3, 4, 5) and of padding=(1, 1) it would fail before, but now it would return a tensor of size (2, 3, 4, 7).  But the behavior of F.pad(size(2, 3, 4, 5), (1, 1, 1, 1)) will return the same tensor as before.</p>\n<p>Ok, I'll work on the documentation. It is a little confusing to explain because the padding tuple goes in reverse order of the dimensions.  In the nn/modules/padding.py file, I used the following explanation.  Would you prefer something like this or more like what you've done with using the 4D tensor as an example and then letting the user intuitively figuring out what happens in N-dimensions.  I sided on being more mathy here, but actually find it sort of difficult to understand myself.</p>\n<pre><code>r\"\"\"Pads the input tensor boundaries with a constant value.\n\nArgs:\n    padding (int, tuple): the size of the padding. If is int, uses the same\n        padding in both boundaries. If a m-tuple, where m is divisible by\n        2, then padding is(begin_dim_pad, end_dim_pad, ...) for each pair\n        paddings starting from the last dimension of the input moving\n        forward.\n    value (int): value inserted into tensor as padding\n\nShape:\n    - Input: :math:`(N_i, D_j_{in})`\n    - Output: :math:`(N_i, D_j_{out})` where\n      :math:`J = len(padding) // 2`\n      :math:`j = (0, 1, ..., J)`\n      :math:`~j = (J, J-1, ..., 0)`\n      :math:`D_j_{out} = D_j_{in} + padding[~j*2] + padding[~j*2 + 1]`\n\n\"\"\"\n</code></pre>", "body_text": "Yea, I believe this is \"backwards compatible\".  Unless you had code that didn't run before, i.e. a 4D tensor trying to pad one dimension.  So if you had a tensor of size = (2, 3, 4, 5) and of padding=(1, 1) it would fail before, but now it would return a tensor of size (2, 3, 4, 7).  But the behavior of F.pad(size(2, 3, 4, 5), (1, 1, 1, 1)) will return the same tensor as before.\nOk, I'll work on the documentation. It is a little confusing to explain because the padding tuple goes in reverse order of the dimensions.  In the nn/modules/padding.py file, I used the following explanation.  Would you prefer something like this or more like what you've done with using the 4D tensor as an example and then letting the user intuitively figuring out what happens in N-dimensions.  I sided on being more mathy here, but actually find it sort of difficult to understand myself.\nr\"\"\"Pads the input tensor boundaries with a constant value.\n\nArgs:\n    padding (int, tuple): the size of the padding. If is int, uses the same\n        padding in both boundaries. If a m-tuple, where m is divisible by\n        2, then padding is(begin_dim_pad, end_dim_pad, ...) for each pair\n        paddings starting from the last dimension of the input moving\n        forward.\n    value (int): value inserted into tensor as padding\n\nShape:\n    - Input: :math:`(N_i, D_j_{in})`\n    - Output: :math:`(N_i, D_j_{out})` where\n      :math:`J = len(padding) // 2`\n      :math:`j = (0, 1, ..., J)`\n      :math:`~j = (J, J-1, ..., 0)`\n      :math:`D_j_{out} = D_j_{in} + padding[~j*2] + padding[~j*2 + 1]`\n\n\"\"\"", "body": "Yea, I believe this is \"backwards compatible\".  Unless you had code that didn't run before, i.e. a 4D tensor trying to pad one dimension.  So if you had a tensor of size = (2, 3, 4, 5) and of padding=(1, 1) it would fail before, but now it would return a tensor of size (2, 3, 4, 7).  But the behavior of F.pad(size(2, 3, 4, 5), (1, 1, 1, 1)) will return the same tensor as before. \r\n\r\nOk, I'll work on the documentation. It is a little confusing to explain because the padding tuple goes in reverse order of the dimensions.  In the nn/modules/padding.py file, I used the following explanation.  Would you prefer something like this or more like what you've done with using the 4D tensor as an example and then letting the user intuitively figuring out what happens in N-dimensions.  I sided on being more mathy here, but actually find it sort of difficult to understand myself.\r\n\r\n    r\"\"\"Pads the input tensor boundaries with a constant value.\r\n\r\n    Args:\r\n        padding (int, tuple): the size of the padding. If is int, uses the same\r\n            padding in both boundaries. If a m-tuple, where m is divisible by\r\n            2, then padding is(begin_dim_pad, end_dim_pad, ...) for each pair\r\n            paddings starting from the last dimension of the input moving\r\n            forward.\r\n        value (int): value inserted into tensor as padding\r\n\r\n    Shape:\r\n        - Input: :math:`(N_i, D_j_{in})`\r\n        - Output: :math:`(N_i, D_j_{out})` where\r\n          :math:`J = len(padding) // 2`\r\n          :math:`j = (0, 1, ..., J)`\r\n          :math:`~j = (J, J-1, ..., 0)`\r\n          :math:`D_j_{out} = D_j_{in} + padding[~j*2] + padding[~j*2 + 1]`\r\n\r\n    \"\"\""}