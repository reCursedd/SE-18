{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3412", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3412/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3412/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3412/events", "html_url": "https://github.com/pytorch/pytorch/issues/3412", "id": 270138240, "node_id": "MDU6SXNzdWUyNzAxMzgyNDA=", "number": 3412, "title": "Segfault in neg, introduced in 3e6e81d", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-31T22:24:39Z", "updated_at": "2017-11-02T21:25:01Z", "closed_at": "2017-11-02T21:25:01Z", "author_association": "NONE", "body_html": "<p>Received a segfault in backward of <code>neg</code> at commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/3c00c0169df850b97d098c0cbd17b63ec47162b5/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/3c00c0169df850b97d098c0cbd17b63ec47162b5\"><tt>3c00c01</tt></a> (at <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/8cd0df020ca6a251ab566e1c5a84627cbc70d483/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/8cd0df020ca6a251ab566e1c5a84627cbc70d483\"><tt>8cd0df0</tt></a> there were no problems).</p>\n<p>Confirmed, it is <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/3e6e81da460f6f90d65c0def58a3538cfe106cda/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/3e6e81da460f6f90d65c0def58a3538cfe106cda\"><tt>3e6e81d</tt></a> that introduced this regression.</p>\n<pre><code>  read 39 \"python\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7ffeff7fe700 (LWP 3823)]\n0x00007fffc1523cd4 in at::Tensor::neg (this=0x7ffef00009f0)\n    at ...torch/lib/tmp_install/include/ATen/TensorMethods.h:514\n514         return type().neg(*this);\n(gdb) bt\n#0  0x00007fffc1523cd4 in at::Tensor::neg (this=0x7ffef00009f0)\n    at .../torch/lib/tmp_install/include/ATen/TensorMethods.h:514\n#1  torch::autograd::generated::NegBackward::apply (this=0x464afc00,\n    grads=std::vector of length 1, capacity 1 = {...}) at torch/csrc/autograd/generated/Functions.cpp:1271\n#2  0x00007fffc1472e6b in torch::autograd::Function::operator() (\n    inputs=std::vector of length 1, capacity 1 = {...}, this=&lt;optimized out&gt;)\n    at ...torch/csrc/autograd/function.h:89\n#3  torch::autograd::call_function (task=...) at torch/csrc/autograd/engine.cpp:208\n#4  torch::autograd::Engine::evaluate_function (this=this@entry=0x7fffc27764c0 &lt;engine&gt;, task=...)\n    at torch/csrc/autograd/engine.cpp:220\n#5  0x00007fffc1474fee in torch::autograd::Engine::thread_main (this=0x7fffc27764c0 &lt;engine&gt;, graph_task=0x0)\n    at torch/csrc/autograd/engine.cpp:144\n#6  0x00007fffc1471dd2 in torch::autograd::Engine::thread_init (this=this@entry=0x7fffc27764c0 &lt;engine&gt;,\n    device=device@entry=-1) at torch/csrc/autograd/engine.cpp:121\n#7  0x00007fffc149443a in torch::autograd::python::PythonEngine::thread_init (this=0x7fffc27764c0 &lt;engine&gt;,\n    device=-1) at torch/csrc/autograd/python_engine.cpp:28\n#8  0x00007fffae5ddc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#9  0x00007ffff7bc16ba in start_thread (arg=0x7ffeff7fe700) at pthread_create.c:333\n#10 0x00007ffff78f73dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n\n(gdb) print *this\n$5 = {&lt;at::detail::TensorBase&gt; = {pImpl = 0x0}, &lt;No data fields&gt;}\n\n(gdb) frame 1\n#1  torch::autograd::generated::NegBackward::apply (this=0x464ab9d0, grads=std::vector of length 1, capacity 1 = {...})\n    at torch/csrc/autograd/generated/Functions.cpp:1264\n1264        grad_inputs[0] = grad.neg();\n(gdb) print grad\n$6 = &lt;optimized out&gt;\n</code></pre>\n<p>Is the fact that <code>grad</code> is optimized out causing the problem?</p>", "body_text": "Received a segfault in backward of neg at commit 3c00c01 (at 8cd0df0 there were no problems).\nConfirmed, it is 3e6e81d that introduced this regression.\n  read 39 \"python\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7ffeff7fe700 (LWP 3823)]\n0x00007fffc1523cd4 in at::Tensor::neg (this=0x7ffef00009f0)\n    at ...torch/lib/tmp_install/include/ATen/TensorMethods.h:514\n514         return type().neg(*this);\n(gdb) bt\n#0  0x00007fffc1523cd4 in at::Tensor::neg (this=0x7ffef00009f0)\n    at .../torch/lib/tmp_install/include/ATen/TensorMethods.h:514\n#1  torch::autograd::generated::NegBackward::apply (this=0x464afc00,\n    grads=std::vector of length 1, capacity 1 = {...}) at torch/csrc/autograd/generated/Functions.cpp:1271\n#2  0x00007fffc1472e6b in torch::autograd::Function::operator() (\n    inputs=std::vector of length 1, capacity 1 = {...}, this=<optimized out>)\n    at ...torch/csrc/autograd/function.h:89\n#3  torch::autograd::call_function (task=...) at torch/csrc/autograd/engine.cpp:208\n#4  torch::autograd::Engine::evaluate_function (this=this@entry=0x7fffc27764c0 <engine>, task=...)\n    at torch/csrc/autograd/engine.cpp:220\n#5  0x00007fffc1474fee in torch::autograd::Engine::thread_main (this=0x7fffc27764c0 <engine>, graph_task=0x0)\n    at torch/csrc/autograd/engine.cpp:144\n#6  0x00007fffc1471dd2 in torch::autograd::Engine::thread_init (this=this@entry=0x7fffc27764c0 <engine>,\n    device=device@entry=-1) at torch/csrc/autograd/engine.cpp:121\n#7  0x00007fffc149443a in torch::autograd::python::PythonEngine::thread_init (this=0x7fffc27764c0 <engine>,\n    device=-1) at torch/csrc/autograd/python_engine.cpp:28\n#8  0x00007fffae5ddc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#9  0x00007ffff7bc16ba in start_thread (arg=0x7ffeff7fe700) at pthread_create.c:333\n#10 0x00007ffff78f73dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n\n(gdb) print *this\n$5 = {<at::detail::TensorBase> = {pImpl = 0x0}, <No data fields>}\n\n(gdb) frame 1\n#1  torch::autograd::generated::NegBackward::apply (this=0x464ab9d0, grads=std::vector of length 1, capacity 1 = {...})\n    at torch/csrc/autograd/generated/Functions.cpp:1264\n1264        grad_inputs[0] = grad.neg();\n(gdb) print grad\n$6 = <optimized out>\n\nIs the fact that grad is optimized out causing the problem?", "body": "Received a segfault in backward of `neg` at commit 3c00c01 (at 8cd0df020ca6a251ab566e1c5a84627cbc70d483 there were no problems).\r\n\r\nConfirmed, it is 3e6e81da460f6f90d65c0def58a3538cfe106cda that introduced this regression.\r\n\r\n```\r\n  read 39 \"python\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7ffeff7fe700 (LWP 3823)]\r\n0x00007fffc1523cd4 in at::Tensor::neg (this=0x7ffef00009f0)\r\n    at ...torch/lib/tmp_install/include/ATen/TensorMethods.h:514\r\n514         return type().neg(*this);\r\n(gdb) bt\r\n#0  0x00007fffc1523cd4 in at::Tensor::neg (this=0x7ffef00009f0)\r\n    at .../torch/lib/tmp_install/include/ATen/TensorMethods.h:514\r\n#1  torch::autograd::generated::NegBackward::apply (this=0x464afc00,\r\n    grads=std::vector of length 1, capacity 1 = {...}) at torch/csrc/autograd/generated/Functions.cpp:1271\r\n#2  0x00007fffc1472e6b in torch::autograd::Function::operator() (\r\n    inputs=std::vector of length 1, capacity 1 = {...}, this=<optimized out>)\r\n    at ...torch/csrc/autograd/function.h:89\r\n#3  torch::autograd::call_function (task=...) at torch/csrc/autograd/engine.cpp:208\r\n#4  torch::autograd::Engine::evaluate_function (this=this@entry=0x7fffc27764c0 <engine>, task=...)\r\n    at torch/csrc/autograd/engine.cpp:220\r\n#5  0x00007fffc1474fee in torch::autograd::Engine::thread_main (this=0x7fffc27764c0 <engine>, graph_task=0x0)\r\n    at torch/csrc/autograd/engine.cpp:144\r\n#6  0x00007fffc1471dd2 in torch::autograd::Engine::thread_init (this=this@entry=0x7fffc27764c0 <engine>,\r\n    device=device@entry=-1) at torch/csrc/autograd/engine.cpp:121\r\n#7  0x00007fffc149443a in torch::autograd::python::PythonEngine::thread_init (this=0x7fffc27764c0 <engine>,\r\n    device=-1) at torch/csrc/autograd/python_engine.cpp:28\r\n#8  0x00007fffae5ddc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#9  0x00007ffff7bc16ba in start_thread (arg=0x7ffeff7fe700) at pthread_create.c:333\r\n#10 0x00007ffff78f73dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n\r\n(gdb) print *this\r\n$5 = {<at::detail::TensorBase> = {pImpl = 0x0}, <No data fields>}\r\n\r\n(gdb) frame 1\r\n#1  torch::autograd::generated::NegBackward::apply (this=0x464ab9d0, grads=std::vector of length 1, capacity 1 = {...})\r\n    at torch/csrc/autograd/generated/Functions.cpp:1264\r\n1264        grad_inputs[0] = grad.neg();\r\n(gdb) print grad\r\n$6 = <optimized out>\r\n```\r\n\r\nIs the fact that `grad` is optimized out causing the problem?"}