{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392097778", "html_url": "https://github.com/pytorch/pytorch/issues/711#issuecomment-392097778", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/711", "id": 392097778, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjA5Nzc3OA==", "user": {"login": "guillaume-chevalier", "id": 11862328, "node_id": "MDQ6VXNlcjExODYyMzI4", "avatar_url": "https://avatars3.githubusercontent.com/u/11862328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guillaume-chevalier", "html_url": "https://github.com/guillaume-chevalier", "followers_url": "https://api.github.com/users/guillaume-chevalier/followers", "following_url": "https://api.github.com/users/guillaume-chevalier/following{/other_user}", "gists_url": "https://api.github.com/users/guillaume-chevalier/gists{/gist_id}", "starred_url": "https://api.github.com/users/guillaume-chevalier/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guillaume-chevalier/subscriptions", "organizations_url": "https://api.github.com/users/guillaume-chevalier/orgs", "repos_url": "https://api.github.com/users/guillaume-chevalier/repos", "events_url": "https://api.github.com/users/guillaume-chevalier/events{/privacy}", "received_events_url": "https://api.github.com/users/guillaume-chevalier/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-25T15:39:21Z", "updated_at": "2018-05-25T15:46:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=544269\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/icoxfog417\">@icoxfog417</a> Yes, that's the point. And not only that, the fact there are 3 or more classes just to implement the RNN and RNN Cells functionnalities is a bit puzzling.</p>\n<p>In my jargon, requiring to add entries to such \"if elif else\" everywhere for new implementations is called poor polymorphism <g-emoji class=\"g-emoji\" alias=\"stuck_out_tongue\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f61b.png\">\ud83d\ude1b</g-emoji>. There is then the lack of new kind of classes to represent those \"GRU\" Cells, \"LSTM\" Cells, and other types of cells such as the default one. I'd suggest a base class named \"RNNCell\" or something like that. And then it would be possible to loop on those cells from the outside classes. There could also be some State classes. For example, see my implementation here of such diverse classes. See:</p>\n<ul>\n<li>LARNN (RNN) class: <a href=\"https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L20\">https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L20</a></li>\n<li>LARNN Cell class: <a href=\"https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L150\">https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L150</a></li>\n<li>LARNN Cell State class: <a href=\"https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L108\">https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L108</a></li>\n</ul>\n<p>Note: after reflexion on my own code, I'd rename the \"LARNN\" class (first in list above) to simply \"RNN\", such that it gets passed an instanciated cell class as argument to loop on. This would decouple the RNN concept from the LARNN Cell and the LARNN State and would make the RNN class suitable to receive any type of Cell and any type of State. You could probably also have a \"StackedRNN\" class in which we could inject an RNN class with its cell, something like that. Note that in my case here, the State is a queue (first in, last out) tensor on the previous states, on which I add a multi-head attention mechanism <a href=\"https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/master/inkscape_drawings/png_exported/larnn-cell.png\">see visualization</a>.</p>\n<p>It would therefore be interesting to have a \"State\" abstract pure class, because states are often quite different: the RNN has one inner state to pass, the LSTM has a tuple of inner states(cell state and hidden state), and my LARNN has a queue of cell states and one hidden state (like the LSTM, but the cell state is a queue).</p>\n<p>In the end, I needed to code this from scratch and loop myself over my own cell class, becaues I couldn't derive things from the PyTorch classes because of the poor polymorphism (and also the lack of clarity or proper OO architecture). However, it's not bad to derive such State classes and Cell classes and RNN base class, because from that people could write quite flexible and dynamic code.</p>\n<p>To sum up, here are the classes I suggest to have:</p>\n<ul>\n<li>RNN (abstract): base class to loop on a cell state. We must indicate which type of cell to use by passing the cell class (or instanciated cell class) as an argument somewhere.</li>\n<li>Cell (abstract): the inside of an RNN</li>\n<li>State (abstract): state implementations which can vary according to the type of cell.</li>\n<li>StackedRNN: pass an RNN class to stack it and clone the weights of its cell.</li>\n<li>RNNCell(Cell): instanciates a VanillaState</li>\n<li>RNNCellState(State): holds the state...</li>\n<li>LSTMCell(Cell): the LSTM Cell</li>\n<li>LSTMCellState(State): the tuple state of the LSTM, wrapped as a class.</li>\n<li>GRUCell(Cell): ...</li>\n<li>GRUCellState(State): ...</li>\n</ul>\n<p>I hope this comment helps.</p>", "body_text": "@icoxfog417 Yes, that's the point. And not only that, the fact there are 3 or more classes just to implement the RNN and RNN Cells functionnalities is a bit puzzling.\nIn my jargon, requiring to add entries to such \"if elif else\" everywhere for new implementations is called poor polymorphism \ud83d\ude1b. There is then the lack of new kind of classes to represent those \"GRU\" Cells, \"LSTM\" Cells, and other types of cells such as the default one. I'd suggest a base class named \"RNNCell\" or something like that. And then it would be possible to loop on those cells from the outside classes. There could also be some State classes. For example, see my implementation here of such diverse classes. See:\n\nLARNN (RNN) class: https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L20\nLARNN Cell class: https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L150\nLARNN Cell State class: https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L108\n\nNote: after reflexion on my own code, I'd rename the \"LARNN\" class (first in list above) to simply \"RNN\", such that it gets passed an instanciated cell class as argument to loop on. This would decouple the RNN concept from the LARNN Cell and the LARNN State and would make the RNN class suitable to receive any type of Cell and any type of State. You could probably also have a \"StackedRNN\" class in which we could inject an RNN class with its cell, something like that. Note that in my case here, the State is a queue (first in, last out) tensor on the previous states, on which I add a multi-head attention mechanism see visualization.\nIt would therefore be interesting to have a \"State\" abstract pure class, because states are often quite different: the RNN has one inner state to pass, the LSTM has a tuple of inner states(cell state and hidden state), and my LARNN has a queue of cell states and one hidden state (like the LSTM, but the cell state is a queue).\nIn the end, I needed to code this from scratch and loop myself over my own cell class, becaues I couldn't derive things from the PyTorch classes because of the poor polymorphism (and also the lack of clarity or proper OO architecture). However, it's not bad to derive such State classes and Cell classes and RNN base class, because from that people could write quite flexible and dynamic code.\nTo sum up, here are the classes I suggest to have:\n\nRNN (abstract): base class to loop on a cell state. We must indicate which type of cell to use by passing the cell class (or instanciated cell class) as an argument somewhere.\nCell (abstract): the inside of an RNN\nState (abstract): state implementations which can vary according to the type of cell.\nStackedRNN: pass an RNN class to stack it and clone the weights of its cell.\nRNNCell(Cell): instanciates a VanillaState\nRNNCellState(State): holds the state...\nLSTMCell(Cell): the LSTM Cell\nLSTMCellState(State): the tuple state of the LSTM, wrapped as a class.\nGRUCell(Cell): ...\nGRUCellState(State): ...\n\nI hope this comment helps.", "body": "@icoxfog417 Yes, that's the point. And not only that, the fact there are 3 or more classes just to implement the RNN and RNN Cells functionnalities is a bit puzzling. \r\n\r\nIn my jargon, requiring to add entries to such \"if elif else\" everywhere for new implementations is called poor polymorphism :stuck_out_tongue:. There is then the lack of new kind of classes to represent those \"GRU\" Cells, \"LSTM\" Cells, and other types of cells such as the default one. I'd suggest a base class named \"RNNCell\" or something like that. And then it would be possible to loop on those cells from the outside classes. There could also be some State classes. For example, see my implementation here of such diverse classes. See: \r\n- LARNN (RNN) class: https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L20\r\n- LARNN Cell class: https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L150\r\n- LARNN Cell State class: https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/df490bec6acc7949102daa38ca744a10a97a4ea8/larnn.py#L108\r\n\r\nNote: after reflexion on my own code, I'd rename the \"LARNN\" class (first in list above) to simply \"RNN\", such that it gets passed an instanciated cell class as argument to loop on. This would decouple the RNN concept from the LARNN Cell and the LARNN State and would make the RNN class suitable to receive any type of Cell and any type of State. You could probably also have a \"StackedRNN\" class in which we could inject an RNN class with its cell, something like that. Note that in my case here, the State is a queue (first in, last out) tensor on the previous states, on which I add a multi-head attention mechanism [see visualization](https://github.com/guillaume-chevalier/Linear-Attention-Recurrent-Neural-Network/blob/master/inkscape_drawings/png_exported/larnn-cell.png). \r\n\r\nIt would therefore be interesting to have a \"State\" abstract pure class, because states are often quite different: the RNN has one inner state to pass, the LSTM has a tuple of inner states(cell state and hidden state), and my LARNN has a queue of cell states and one hidden state (like the LSTM, but the cell state is a queue). \r\n\r\nIn the end, I needed to code this from scratch and loop myself over my own cell class, becaues I couldn't derive things from the PyTorch classes because of the poor polymorphism (and also the lack of clarity or proper OO architecture). However, it's not bad to derive such State classes and Cell classes and RNN base class, because from that people could write quite flexible and dynamic code. \r\n\r\nTo sum up, here are the classes I suggest to have: \r\n- RNN (abstract): base class to loop on a cell state. We must indicate which type of cell to use by passing the cell class (or instanciated cell class) as an argument somewhere. \r\n- Cell (abstract): the inside of an RNN\r\n- State (abstract): state implementations which can vary according to the type of cell. \r\n- StackedRNN: pass an RNN class to stack it and clone the weights of its cell. \r\n- RNNCell(Cell): instanciates a VanillaState\r\n- RNNCellState(State): holds the state...\r\n- LSTMCell(Cell): the LSTM Cell\r\n- LSTMCellState(State): the tuple state of the LSTM, wrapped as a class. \r\n- GRUCell(Cell): ...\r\n- GRUCellState(State): ...\r\n\r\nI hope this comment helps. "}