{"url": "https://api.github.com/repos/pytorch/pytorch/issues/711", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/711/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/711/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/711/events", "html_url": "https://github.com/pytorch/pytorch/issues/711", "id": 206652810, "node_id": "MDU6SXNzdWUyMDY2NTI4MTA=", "number": 711, "title": "Feature Request: Easier to extend base RNN implementation", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 586699385, "node_id": "MDU6TGFiZWw1ODY2OTkzODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/24hr+", "name": "24hr+", "color": "d4a5d9", "default": false}, {"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2017-02-09T22:45:17Z", "updated_at": "2018-10-16T10:28:37Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Currently base RNN class/functions are hard to extend. If someone would like to extend LSTM with new features to pytorch they would have to modify:<br>\nAutogradRNN (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L192-L234\">nn/_functions/rnn.py</a>)<br>\nStackedRNN (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L50-L88\">nn/_functions/rnn.py</a>)<br>\nRNNBase (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L9-L54\">nn/modules/rnn.py</a>)<br>\nFurthermore, the default RNN implementation is restrictive, enforcing every stacked RNN layer to be exactly the same.</p>\n<p>I was thinking it may be worthwhile to instead have an RNN driver abstract RNN base class.</p>\n<p>RNN Driver would be similar in function to recurrent, stackedRNN, and AutogradRNN functions but would be dependent on abstract RNN class but independent of specific RNN definitions.</p>\n<p>The code would look something like...</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">RNNDriver</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):        \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>constructor could either take an RNN or list of RNN layers</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputRNN</span>, <span class=\"pl-smi\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(inputRNN, <span class=\"pl-c1\">list</span>):\n            <span class=\"pl-c1\">self</span>.rnns <span class=\"pl-k\">=</span> [inputRNN]\n            <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_layers<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>):\n                <span class=\"pl-c1\">self</span>.rnns.append(inputRNN.clone())\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(inputRNN) <span class=\"pl-k\">==</span> num_layers, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RNN list length must be equal to num_layers<span class=\"pl-pds\">\"</span></span>\n            <span class=\"pl-c1\">self</span>.rnns<span class=\"pl-k\">=</span>inputRNN\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Parameters call to group parameters of all layers</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">parameters</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        memo <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>()\n        <span class=\"pl-k\">for</span> rnn <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.rnns:\n            <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> rnn.parameters(memo):\n                <span class=\"pl-k\">yield</span> p\n                \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-smi\">bidirectional</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        <span class=\"pl-c1\">...</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">initHidden</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">bsz</span>):\n        <span class=\"pl-k\">for</span> rnn <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.rnns:\n            rnn.initHidden(bsz)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">resetHidden</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">bsz</span>):\n        <span class=\"pl-k\">for</span> rnn <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.rnns:\n            rnn.resetHidden(bsz)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">initInference</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">bsz</span>):    \n        <span class=\"pl-k\">for</span> rnn <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.rnns:\n            rnn.initInference(bsz)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RNNBase</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Base initialization could be for a simple RNN layer or could be empty</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>):\n        <span class=\"pl-c1\">super</span>(RNNBase, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.hidden_size <span class=\"pl-k\">=</span> hidden_size\n        <span class=\"pl-c1\">self</span>.input_size <span class=\"pl-k\">=</span> input_size\n\n        <span class=\"pl-c1\">self</span>.w_ih <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(hidden_size, input_size))\n        <span class=\"pl-c1\">self</span>.b_ih <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(hidden_size))\n        <span class=\"pl-c1\">self</span>.w_hh <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n\n        <span class=\"pl-c1\">self</span>.hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n        <span class=\"pl-c1\">self</span>.reset_parameters()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">reset_parameters</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">feature_size</span>):\n            stdv <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">/</span> math.sqrt(feature_size)\n            <span class=\"pl-k\">for</span> weight <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.parameters():\n                weight.data.uniform_(<span class=\"pl-k\">-</span>stdv, stdv)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">initHidden</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">bsz</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Create hidden variable(s)</span>\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">NotImplementedError</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">resetHidden</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">bsz</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Re-wrap hidden variables</span>\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">NotImplementedError</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">initInference</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">bsz</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Re-wrap hidden in a variable with volatile=True</span>\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">NotImplementedError</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Implement RNN layer and return output</span>\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">NotImplementedError</span></pre></div>", "body_text": "Currently base RNN class/functions are hard to extend. If someone would like to extend LSTM with new features to pytorch they would have to modify:\nAutogradRNN (nn/_functions/rnn.py)\nStackedRNN (nn/_functions/rnn.py)\nRNNBase (nn/modules/rnn.py)\nFurthermore, the default RNN implementation is restrictive, enforcing every stacked RNN layer to be exactly the same.\nI was thinking it may be worthwhile to instead have an RNN driver abstract RNN base class.\nRNN Driver would be similar in function to recurrent, stackedRNN, and AutogradRNN functions but would be dependent on abstract RNN class but independent of specific RNN definitions.\nThe code would look something like...\nclass RNNDriver(nn.Module):        \n    #constructor could either take an RNN or list of RNN layers\n    def __init__(self, inputRNN, num_layers=1):\n        if not isinstance(inputRNN, list):\n            self.rnns = [inputRNN]\n            for i in range(num_layers-1):\n                self.rnns.append(inputRNN.clone())\n        else:\n            assert len(inputRNN) == num_layers, \"RNN list length must be equal to num_layers\"\n            self.rnns=inputRNN\n    #Parameters call to group parameters of all layers\n    def parameters(self):\n        memo = set()\n        for rnn in self.rnns:\n            for p in rnn.parameters(memo):\n                yield p\n                \n    def forward(self, input, train=True, batch_first=False, dropout=0, bidirectional=False):\n        ...\n\n    def initHidden(self, bsz):\n        for rnn in self.rnns:\n            rnn.initHidden(bsz)\n\n    def resetHidden(self, bsz):\n        for rnn in self.rnns:\n            rnn.resetHidden(bsz)\n\n    def initInference(self, bsz):    \n        for rnn in self.rnns:\n            rnn.initInference(bsz)\n\nclass RNNBase(nn.Module):\n#Base initialization could be for a simple RNN layer or could be empty\n    def __init__(self, input_size, hidden_size):\n        super(RNNBase, self).__init__()\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n\n        self.w_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\n        self.b_ih = nn.Parameter(torch.Tensor(hidden_size))\n        self.w_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n\n        self.hidden = None\n        self.reset_parameters()\n\n    def reset_parameters(self, feature_size):\n            stdv = 1.0 / math.sqrt(feature_size)\n            for weight in self.parameters():\n                weight.data.uniform_(-stdv, stdv)\n\n    def initHidden(self, bsz):\n        #Create hidden variable(s)\n        raise NotImplementedError\n\n    def resetHidden(self, bsz):\n        #Re-wrap hidden variables\n        raise NotImplementedError\n\n    def initInference(self, bsz):\n        #Re-wrap hidden in a variable with volatile=True\n        raise NotImplementedError\n\n    def forward(self, input):\n        #Implement RNN layer and return output\n        raise NotImplementedError", "body": "Currently base RNN class/functions are hard to extend. If someone would like to extend LSTM with new features to pytorch they would have to modify:\r\nAutogradRNN ([nn/_functions/rnn.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L192-L234))\r\nStackedRNN ([nn/_functions/rnn.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L50-L88))\r\nRNNBase ([nn/modules/rnn.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L9-L54))\r\nFurthermore, the default RNN implementation is restrictive, enforcing every stacked RNN layer to be exactly the same. \r\n\r\nI was thinking it may be worthwhile to instead have an RNN driver abstract RNN base class.\r\n\r\nRNN Driver would be similar in function to recurrent, stackedRNN, and AutogradRNN functions but would be dependent on abstract RNN class but independent of specific RNN definitions.\r\n\r\nThe code would look something like...\r\n\r\n```py\r\nclass RNNDriver(nn.Module):        \r\n    #constructor could either take an RNN or list of RNN layers\r\n    def __init__(self, inputRNN, num_layers=1):\r\n        if not isinstance(inputRNN, list):\r\n            self.rnns = [inputRNN]\r\n            for i in range(num_layers-1):\r\n                self.rnns.append(inputRNN.clone())\r\n        else:\r\n            assert len(inputRNN) == num_layers, \"RNN list length must be equal to num_layers\"\r\n            self.rnns=inputRNN\r\n    #Parameters call to group parameters of all layers\r\n    def parameters(self):\r\n        memo = set()\r\n        for rnn in self.rnns:\r\n            for p in rnn.parameters(memo):\r\n                yield p\r\n                \r\n    def forward(self, input, train=True, batch_first=False, dropout=0, bidirectional=False):\r\n        ...\r\n\r\n    def initHidden(self, bsz):\r\n        for rnn in self.rnns:\r\n            rnn.initHidden(bsz)\r\n\r\n    def resetHidden(self, bsz):\r\n        for rnn in self.rnns:\r\n            rnn.resetHidden(bsz)\r\n\r\n    def initInference(self, bsz):    \r\n        for rnn in self.rnns:\r\n            rnn.initInference(bsz)\r\n\r\nclass RNNBase(nn.Module):\r\n#Base initialization could be for a simple RNN layer or could be empty\r\n    def __init__(self, input_size, hidden_size):\r\n        super(RNNBase, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.input_size = input_size\r\n\r\n        self.w_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\r\n        self.b_ih = nn.Parameter(torch.Tensor(hidden_size))\r\n        self.w_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\r\n\r\n        self.hidden = None\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self, feature_size):\r\n            stdv = 1.0 / math.sqrt(feature_size)\r\n            for weight in self.parameters():\r\n                weight.data.uniform_(-stdv, stdv)\r\n\r\n    def initHidden(self, bsz):\r\n        #Create hidden variable(s)\r\n        raise NotImplementedError\r\n\r\n    def resetHidden(self, bsz):\r\n        #Re-wrap hidden variables\r\n        raise NotImplementedError\r\n\r\n    def initInference(self, bsz):\r\n        #Re-wrap hidden in a variable with volatile=True\r\n        raise NotImplementedError\r\n\r\n    def forward(self, input):\r\n        #Implement RNN layer and return output\r\n        raise NotImplementedError\r\n```"}