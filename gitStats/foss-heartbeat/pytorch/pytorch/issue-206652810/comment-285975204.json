{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/285975204", "html_url": "https://github.com/pytorch/pytorch/issues/711#issuecomment-285975204", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/711", "id": 285975204, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTk3NTIwNA==", "user": {"login": "hashbangCoder", "id": 16652882, "node_id": "MDQ6VXNlcjE2NjUyODgy", "avatar_url": "https://avatars3.githubusercontent.com/u/16652882?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hashbangCoder", "html_url": "https://github.com/hashbangCoder", "followers_url": "https://api.github.com/users/hashbangCoder/followers", "following_url": "https://api.github.com/users/hashbangCoder/following{/other_user}", "gists_url": "https://api.github.com/users/hashbangCoder/gists{/gist_id}", "starred_url": "https://api.github.com/users/hashbangCoder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hashbangCoder/subscriptions", "organizations_url": "https://api.github.com/users/hashbangCoder/orgs", "repos_url": "https://api.github.com/users/hashbangCoder/repos", "events_url": "https://api.github.com/users/hashbangCoder/events{/privacy}", "received_events_url": "https://api.github.com/users/hashbangCoder/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-12T20:47:07Z", "updated_at": "2017-03-12T20:47:50Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Currently base RNN class/functions are hard to extend. If someone would like to extend LSTM with new features to pytorch they would have to modify:<br>\nAutogradRNN (nn/_functions/rnn.py)<br>\nStackedRNN (nn/_functions/rnn.py)<br>\nRNNBase (nn/modules/rnn.py)</p>\n</blockquote>\n<p>Could you explain this a bit? Likely I'm missing something, but my understanding was that it was trivial to modify LSTM cells as given <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L20\">here</a>. With the new cell, shouldn't it be simple to loop through the sequence and execute the forward call each time and let autograd take care of the backward call?</p>\n<p>I'm planning to implement a simple variation the LSTM cell with a third input alongside input(t) and hidden(t-1).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22205833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/csarofeen\">@csarofeen</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>", "body_text": "Currently base RNN class/functions are hard to extend. If someone would like to extend LSTM with new features to pytorch they would have to modify:\nAutogradRNN (nn/_functions/rnn.py)\nStackedRNN (nn/_functions/rnn.py)\nRNNBase (nn/modules/rnn.py)\n\nCould you explain this a bit? Likely I'm missing something, but my understanding was that it was trivial to modify LSTM cells as given here. With the new cell, shouldn't it be simple to loop through the sequence and execute the forward call each time and let autograd take care of the backward call?\nI'm planning to implement a simple variation the LSTM cell with a third input alongside input(t) and hidden(t-1).\n@csarofeen @apaszke", "body": "> Currently base RNN class/functions are hard to extend. If someone would like to extend LSTM with new features to pytorch they would have to modify:\r\nAutogradRNN (nn/_functions/rnn.py)\r\nStackedRNN (nn/_functions/rnn.py)\r\nRNNBase (nn/modules/rnn.py)\r\n\r\n Could you explain this a bit? Likely I'm missing something, but my understanding was that it was trivial to modify LSTM cells as given [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L20). With the new cell, shouldn't it be simple to loop through the sequence and execute the forward call each time and let autograd take care of the backward call? \r\n\r\nI'm planning to implement a simple variation the LSTM cell with a third input alongside input(t) and hidden(t-1).\r\n\r\n@csarofeen @apaszke "}