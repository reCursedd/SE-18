{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145700177", "pull_request_review_id": 70537224, "id": 145700177, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTcwMDE3Nw==", "diff_hunk": "@@ -36,12 +39,81 @@ def eval(self):\n         return self\n \n \n+class LayerNorm(Module):\n+    r\"\"\"Applies Layer Normalization over a 2D input that is seen\n+    as a mini-batch of 1D inputs.\n+\n+    .. math::\n+\n+        y = \\gamma * \\frac{x - \\mu_x}{\\sigma_x + \\epsilon} + \\beta\n+\n+    The mean and standard deviation are calculated for each object in a\n+    mini-batch (over `num_features`). Gamma and beta are\n+    optional learnable parameter vectors of size C (where C is the input size).\n+\n+    Args:\n+        num_features: num_features from an expected input of size\n+            `batch_size x num_features`. Specified only if learnable parameters\n+            are desired. Default: None\n+        eps: a value added to the denominator for numerical stability.\n+            Default: 1e-5\n+\n+    Shape:\n+        - Input: :math:`(N, C)`\n+        - Output: :math:`(N, C)` (same shape as input)\n+\n+    Examples:\n+        >>> # Without Learnable Parameters\n+        >>> m = nn.LayerNorm()\n+        >>> # With Learnable Parameters\n+        >>> m = nn.LayerNorm(100)\n+        >>> input = autograd.Variable(torch.randn(20, 100))\n+        >>> output = m(input)\n+    \"\"\"\n+\n+    def __init__(self, num_features=None, eps=1e-5):\n+        super(LayerNorm, self).__init__()\n+        self.num_features = num_features\n+        self.affine = num_features is not None\n+        self.eps = eps\n+        if self.affine:\n+            self.weight = Parameter(torch.ones(num_features))\n+            self.bias = Parameter(torch.zeros(num_features))\n+        else:\n+            self.register_parameter('weight', None)\n+            self.register_parameter('bias', None)\n+        self.reset_parameters()", "path": "torch/nn/modules/instancenorm.py", "position": null, "original_position": 56, "commit_id": "9182562a19855ec294d297118c01d0a3f04df0dc", "original_commit_id": "00f97a87246a6a7a144b1c3ffb3ebbdcfbe9dd5c", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "body": "Good point - going to just create them as Tensors and then keep this in to a) separate concerns of weight initialisation into one place b) be consistent with `batchnorm.py`.", "created_at": "2017-10-19T13:32:28Z", "updated_at": "2018-11-23T15:35:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/2019#discussion_r145700177", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2019", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145700177"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2019#discussion_r145700177"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2019"}}, "body_html": "<p>Good point - going to just create them as Tensors and then keep this in to a) separate concerns of weight initialisation into one place b) be consistent with <code>batchnorm.py</code>.</p>", "body_text": "Good point - going to just create them as Tensors and then keep this in to a) separate concerns of weight initialisation into one place b) be consistent with batchnorm.py.", "in_reply_to_id": 145656834}