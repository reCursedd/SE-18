{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145658945", "pull_request_review_id": 70486478, "id": 145658945, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTY1ODk0NQ==", "diff_hunk": "@@ -36,12 +39,81 @@ def eval(self):\n         return self\n \n \n+class LayerNorm(Module):\n+    r\"\"\"Applies Layer Normalization over a 2D input that is seen\n+    as a mini-batch of 1D inputs.\n+\n+    .. math::\n+\n+        y = \\gamma * \\frac{x - \\mu_x}{\\sigma_x + \\epsilon} + \\beta\n+\n+    The mean and standard deviation are calculated for each object in a\n+    mini-batch (over `num_features`). Gamma and beta are\n+    optional learnable parameter vectors of size C (where C is the input size).\n+\n+    Args:\n+        num_features: num_features from an expected input of size\n+            `batch_size x num_features`. Specified only if learnable parameters\n+            are desired. Default: None\n+        eps: a value added to the denominator for numerical stability.\n+            Default: 1e-5\n+\n+    Shape:\n+        - Input: :math:`(N, C)`\n+        - Output: :math:`(N, C)` (same shape as input)\n+\n+    Examples:\n+        >>> # Without Learnable Parameters\n+        >>> m = nn.LayerNorm()\n+        >>> # With Learnable Parameters\n+        >>> m = nn.LayerNorm(100)\n+        >>> input = autograd.Variable(torch.randn(20, 100))\n+        >>> output = m(input)\n+    \"\"\"\n+\n+    def __init__(self, num_features=None, eps=1e-5):\n+        super(LayerNorm, self).__init__()\n+        self.num_features = num_features\n+        self.affine = num_features is not None\n+        self.eps = eps\n+        if self.affine:\n+            self.weight = Parameter(torch.ones(num_features))\n+            self.bias = Parameter(torch.zeros(num_features))\n+        else:\n+            self.register_parameter('weight', None)\n+            self.register_parameter('bias', None)\n+        self.reset_parameters()\n+\n+    def reset_parameters(self):\n+        if self.affine:\n+            self.weight.data.fill_(1)\n+            self.bias.data.zero_()\n+\n+    def forward(self, input):\n+        return F.layer_norm(input, weight=self.weight, bias=self.bias,\n+                            eps=self.eps)\n+\n+    def __repr__(self):\n+        if self.affine:\n+            return ('{name}({num_features}, eps={eps})'\n+                    .format(name=self.__class__.__name__, **self.__dict__))\n+        else:\n+            return ('{name}(eps={eps})'\n+                    .format(name=self.__class__.__name__, **self.__dict__))\n+\n+    def _check_input_dim(self, input):", "path": "torch/nn/modules/instancenorm.py", "position": null, "original_position": 75, "commit_id": "9182562a19855ec294d297118c01d0a3f04df0dc", "original_commit_id": "00f97a87246a6a7a144b1c3ffb3ebbdcfbe9dd5c", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "body": "where is this called? Perhaps this logic should be moved into the `layer_norm` function?", "created_at": "2017-10-19T10:20:19Z", "updated_at": "2018-11-23T15:35:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/2019#discussion_r145658945", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2019", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145658945"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2019#discussion_r145658945"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2019"}}, "body_html": "<p>where is this called? Perhaps this logic should be moved into the <code>layer_norm</code> function?</p>", "body_text": "where is this called? Perhaps this logic should be moved into the layer_norm function?"}