{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145702993", "pull_request_review_id": 70540636, "id": 145702993, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTcwMjk5Mw==", "diff_hunk": "@@ -36,12 +39,81 @@ def eval(self):\n         return self\n \n \n+class LayerNorm(Module):\n+    r\"\"\"Applies Layer Normalization over a 2D input that is seen\n+    as a mini-batch of 1D inputs.\n+\n+    .. math::\n+\n+        y = \\gamma * \\frac{x - \\mu_x}{\\sigma_x + \\epsilon} + \\beta\n+\n+    The mean and standard deviation are calculated for each object in a\n+    mini-batch (over `num_features`). Gamma and beta are\n+    optional learnable parameter vectors of size C (where C is the input size).\n+\n+    Args:\n+        num_features: num_features from an expected input of size\n+            `batch_size x num_features`. Specified only if learnable parameters\n+            are desired. Default: None\n+        eps: a value added to the denominator for numerical stability.\n+            Default: 1e-5\n+\n+    Shape:\n+        - Input: :math:`(N, C)`\n+        - Output: :math:`(N, C)` (same shape as input)\n+\n+    Examples:\n+        >>> # Without Learnable Parameters\n+        >>> m = nn.LayerNorm()\n+        >>> # With Learnable Parameters\n+        >>> m = nn.LayerNorm(100)\n+        >>> input = autograd.Variable(torch.randn(20, 100))\n+        >>> output = m(input)\n+    \"\"\"\n+\n+    def __init__(self, num_features=None, eps=1e-5):\n+        super(LayerNorm, self).__init__()\n+        self.num_features = num_features\n+        self.affine = num_features is not None\n+        self.eps = eps\n+        if self.affine:\n+            self.weight = Parameter(torch.ones(num_features))\n+            self.bias = Parameter(torch.zeros(num_features))\n+        else:\n+            self.register_parameter('weight', None)\n+            self.register_parameter('bias', None)\n+        self.reset_parameters()\n+\n+    def reset_parameters(self):\n+        if self.affine:\n+            self.weight.data.fill_(1)\n+            self.bias.data.zero_()\n+\n+    def forward(self, input):\n+        return F.layer_norm(input, weight=self.weight, bias=self.bias,\n+                            eps=self.eps)\n+\n+    def __repr__(self):\n+        if self.affine:\n+            return ('{name}({num_features}, eps={eps})'\n+                    .format(name=self.__class__.__name__, **self.__dict__))\n+        else:\n+            return ('{name}(eps={eps})'\n+                    .format(name=self.__class__.__name__, **self.__dict__))\n+\n+    def _check_input_dim(self, input):\n+        if input.dim() != 2:\n+            raise ValueError('expected 2D input (got {}D input)'\n+                             .format(input.dim()))\n+        super(LayerNorm, self)._check_input_dim(input)", "path": "torch/nn/modules/instancenorm.py", "position": null, "original_position": 79, "commit_id": "9182562a19855ec294d297118c01d0a3f04df0dc", "original_commit_id": "00f97a87246a6a7a144b1c3ffb3ebbdcfbe9dd5c", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "body": "Ah I see these functions have been removed from BatchNorm and InstanceNorm from when I copied them. It's not needed here, correct.", "created_at": "2017-10-19T13:42:20Z", "updated_at": "2018-11-23T15:35:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/2019#discussion_r145702993", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2019", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145702993"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2019#discussion_r145702993"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2019"}}, "body_html": "<p>Ah I see these functions have been removed from BatchNorm and InstanceNorm from when I copied them. It's not needed here, correct.</p>", "body_text": "Ah I see these functions have been removed from BatchNorm and InstanceNorm from when I copied them. It's not needed here, correct.", "in_reply_to_id": 145657648}