{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/315534362", "html_url": "https://github.com/pytorch/pytorch/pull/2019#issuecomment-315534362", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2019", "id": 315534362, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTUzNDM2Mg==", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-15T13:36:06Z", "updated_at": "2017-07-15T13:36:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6727524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/DmitryUlyanov\">@DmitryUlyanov</a> Thanks for the extra info. I found <a href=\"https://arxiv.org/abs/1611.04520\" rel=\"nofollow\">Normalizing the Normalizers</a>, which seems to back up the interpretation that the entire layer is normalised over in layer norm/as opposed to instance norm. Whilst I am using the text for <code>InstanceNorm{1,2,3}d</code> for <code>LayerNorm</code>, I am adjusting the text for <code>InstanceNorm{1,2,3}d</code> to have:</p>\n<blockquote>\n<p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. Gamma and beta are learnable parameter vectors of size C (where C is the input size). This can be seen as an extension of layer normalization where statistics are only calculated over <code>num_features</code> and NOT all non-batch dimensions.</p>\n</blockquote>\n<p>Is this both descriptive and fair? I would invite you to have a look at the new PR that I've opened to see the exact changes.</p>", "body_text": "@DmitryUlyanov Thanks for the extra info. I found Normalizing the Normalizers, which seems to back up the interpretation that the entire layer is normalised over in layer norm/as opposed to instance norm. Whilst I am using the text for InstanceNorm{1,2,3}d for LayerNorm, I am adjusting the text for InstanceNorm{1,2,3}d to have:\n\nThe mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. Gamma and beta are learnable parameter vectors of size C (where C is the input size). This can be seen as an extension of layer normalization where statistics are only calculated over num_features and NOT all non-batch dimensions.\n\nIs this both descriptive and fair? I would invite you to have a look at the new PR that I've opened to see the exact changes.", "body": "@DmitryUlyanov Thanks for the extra info. I found [Normalizing the Normalizers](https://arxiv.org/abs/1611.04520), which seems to back up the interpretation that the entire layer is normalised over in layer norm/as opposed to instance norm. Whilst I am using the text for `InstanceNorm{1,2,3}d` for `LayerNorm`, I am adjusting the text for `InstanceNorm{1,2,3}d` to have:\r\n\r\n> The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. Gamma and beta are learnable parameter vectors of size C (where C is the input size). This can be seen as an extension of layer normalization where statistics are only calculated over `num_features` and NOT all non-batch dimensions.\r\n\r\nIs this both descriptive and fair? I would invite you to have a look at the new PR that I've opened to see the exact changes."}