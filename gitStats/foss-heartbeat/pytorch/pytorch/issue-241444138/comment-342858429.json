{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342858429", "html_url": "https://github.com/pytorch/pytorch/pull/2019#issuecomment-342858429", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2019", "id": 342858429, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mjg1ODQyOQ==", "user": {"login": "mys007", "id": 5921083, "node_id": "MDQ6VXNlcjU5MjEwODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5921083?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mys007", "html_url": "https://github.com/mys007", "followers_url": "https://api.github.com/users/mys007/followers", "following_url": "https://api.github.com/users/mys007/following{/other_user}", "gists_url": "https://api.github.com/users/mys007/gists{/gist_id}", "starred_url": "https://api.github.com/users/mys007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mys007/subscriptions", "organizations_url": "https://api.github.com/users/mys007/orgs", "repos_url": "https://api.github.com/users/mys007/repos", "events_url": "https://api.github.com/users/mys007/events{/privacy}", "received_events_url": "https://api.github.com/users/mys007/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-08T15:46:55Z", "updated_at": "2017-11-08T15:47:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=991891\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Kaixhin\">@Kaixhin</a> Cool. Just a detail: to be equivalent to Batch/InstanceNorm implementation (<a href=\"https://github.com/zdevito/ATen/blob/master/src/THNN/generic/BatchNormalization.c\">https://github.com/zdevito/ATen/blob/master/src/THNN/generic/BatchNormalization.c</a>), one should use biased stddev computation. But even with that, <code>LN(t)</code> and <code>IN(t.unsqueeze(1)).squeeze(1)</code> for <code>LN=LayerNorm()</code> and <code>IN=nn.InstanceNorm1d(1)</code> have some subtle differences, I assume they are numerical issues.</p>\n<p>I haven't found any plans for <code>std()</code> in issues. IMHO it should be also done with subgradient, instead of just adding eps. I'll try to look at it later.</p>", "body_text": "@Kaixhin Cool. Just a detail: to be equivalent to Batch/InstanceNorm implementation (https://github.com/zdevito/ATen/blob/master/src/THNN/generic/BatchNormalization.c), one should use biased stddev computation. But even with that, LN(t) and IN(t.unsqueeze(1)).squeeze(1) for LN=LayerNorm() and IN=nn.InstanceNorm1d(1) have some subtle differences, I assume they are numerical issues.\nI haven't found any plans for std() in issues. IMHO it should be also done with subgradient, instead of just adding eps. I'll try to look at it later.", "body": "@Kaixhin Cool. Just a detail: to be equivalent to Batch/InstanceNorm implementation (https://github.com/zdevito/ATen/blob/master/src/THNN/generic/BatchNormalization.c), one should use biased stddev computation. But even with that, `LN(t)` and `IN(t.unsqueeze(1)).squeeze(1)` for `LN=LayerNorm()` and `IN=nn.InstanceNorm1d(1)` have some subtle differences, I assume they are numerical issues.\r\n\r\nI haven't found any plans for `std()` in issues. IMHO it should be also done with subgradient, instead of just adding eps. I'll try to look at it later."}