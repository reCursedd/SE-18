{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9302", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9302/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9302/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9302/events", "html_url": "https://github.com/pytorch/pytorch/pull/9302", "id": 339811275, "node_id": "MDExOlB1bGxSZXF1ZXN0MjAwMzcyMDgw", "number": 9302, "title": "Grad clip for parameters on different devices", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-10T12:00:02Z", "updated_at": "2018-07-10T14:58:00Z", "closed_at": "2018-07-10T14:58:00Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9302", "html_url": "https://github.com/pytorch/pytorch/pull/9302", "diff_url": "https://github.com/pytorch/pytorch/pull/9302.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9302.patch"}, "body_html": "<p>I'm trying to write a multi-gpu network by pipelining some layers onto different GPUs. However, the current gradient clip requires all the parameters to locate in the same device.</p>\n<p>The overhead of CUDA launch is reduced since the scalar calculation is performed on CPU, but it introduces extra data transfers.</p>\n<p>No performance regression is observed by running the following snippet:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">import</span> torch\n\nmodule <span class=\"pl-k\">=</span> torch.nn.Sequential(\n    torch.nn.LSTM(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">1024</span>),\n    torch.nn.LSTM(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>),\n    torch.nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10000</span>),\n).cuda()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> warming-up</span>\ntorch.nn.utils.clip_grad_norm_(module.parameters(), <span class=\"pl-c1\">1</span>)\ntorch.cuda.synchronize()\nstart <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n    torch.nn.utils.clip_grad_norm_(module.parameters(), <span class=\"pl-c1\">1</span>)\ntorch.cuda.synchronize()\ntime_elapse <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> start\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span> ms per clip<span class=\"pl-pds\">'</span></span>.format(time_elapse))</pre></div>", "body_text": "I'm trying to write a multi-gpu network by pipelining some layers onto different GPUs. However, the current gradient clip requires all the parameters to locate in the same device.\nThe overhead of CUDA launch is reduced since the scalar calculation is performed on CPU, but it introduces extra data transfers.\nNo performance regression is observed by running the following snippet:\nimport time\n\nimport torch\n\nmodule = torch.nn.Sequential(\n    torch.nn.LSTM(1024, 1024),\n    torch.nn.LSTM(256, 256),\n    torch.nn.Linear(100, 10000),\n).cuda()\n\n# warming-up\ntorch.nn.utils.clip_grad_norm_(module.parameters(), 1)\ntorch.cuda.synchronize()\nstart = time.time()\nfor _ in range(1000):\n    torch.nn.utils.clip_grad_norm_(module.parameters(), 1)\ntorch.cuda.synchronize()\ntime_elapse = time.time() - start\nprint('{} ms per clip'.format(time_elapse))", "body": "I'm trying to write a multi-gpu network by pipelining some layers onto different GPUs. However, the current gradient clip requires all the parameters to locate in the same device.\r\n\r\nThe overhead of CUDA launch is reduced since the scalar calculation is performed on CPU, but it introduces extra data transfers.\r\n\r\nNo performance regression is observed by running the following snippet:\r\n```python\r\nimport time\r\n\r\nimport torch\r\n\r\nmodule = torch.nn.Sequential(\r\n    torch.nn.LSTM(1024, 1024),\r\n    torch.nn.LSTM(256, 256),\r\n    torch.nn.Linear(100, 10000),\r\n).cuda()\r\n\r\n# warming-up\r\ntorch.nn.utils.clip_grad_norm_(module.parameters(), 1)\r\ntorch.cuda.synchronize()\r\nstart = time.time()\r\nfor _ in range(1000):\r\n    torch.nn.utils.clip_grad_norm_(module.parameters(), 1)\r\ntorch.cuda.synchronize()\r\ntime_elapse = time.time() - start\r\nprint('{} ms per clip'.format(time_elapse))\r\n```\r\n"}