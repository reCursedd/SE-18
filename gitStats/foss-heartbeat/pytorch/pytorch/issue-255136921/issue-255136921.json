{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2618", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2618/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2618/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2618/events", "html_url": "https://github.com/pytorch/pytorch/issues/2618", "id": 255136921, "node_id": "MDU6SXNzdWUyNTUxMzY5MjE=", "number": 2618, "title": "Lack of Square Function", "user": {"login": "jhirshman", "id": 876992, "node_id": "MDQ6VXNlcjg3Njk5Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/876992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhirshman", "html_url": "https://github.com/jhirshman", "followers_url": "https://api.github.com/users/jhirshman/followers", "following_url": "https://api.github.com/users/jhirshman/following{/other_user}", "gists_url": "https://api.github.com/users/jhirshman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhirshman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhirshman/subscriptions", "organizations_url": "https://api.github.com/users/jhirshman/orgs", "repos_url": "https://api.github.com/users/jhirshman/repos", "events_url": "https://api.github.com/users/jhirshman/events{/privacy}", "received_events_url": "https://api.github.com/users/jhirshman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-09-05T01:23:40Z", "updated_at": "2017-09-10T17:53:10Z", "closed_at": "2017-09-10T17:53:05Z", "author_association": "NONE", "body_html": "<p>Unless, I'm mistaken pytorch lacks a function to square tensors (unlike numpy which has np.square).  There are of course many ways of getting around this.  For a while, I chose to do torch.pow(a, 2).  However, I noticed that this function is really slow.  Upon investigating, I found that it is 30x slower than a * a or torch.mul(a, a).</p>\n<p>I'm opening this issue to encourage the inclusion of a square function so that others don't make a similar mistake.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/876992/30041327-1ca25c30-919e-11e7-88ec-f8944132ea77.png\"><img src=\"https://user-images.githubusercontent.com/876992/30041327-1ca25c30-919e-11e7-88ec-f8944132ea77.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>", "body_text": "Unless, I'm mistaken pytorch lacks a function to square tensors (unlike numpy which has np.square).  There are of course many ways of getting around this.  For a while, I chose to do torch.pow(a, 2).  However, I noticed that this function is really slow.  Upon investigating, I found that it is 30x slower than a * a or torch.mul(a, a).\nI'm opening this issue to encourage the inclusion of a square function so that others don't make a similar mistake.", "body": "Unless, I'm mistaken pytorch lacks a function to square tensors (unlike numpy which has np.square).  There are of course many ways of getting around this.  For a while, I chose to do torch.pow(a, 2).  However, I noticed that this function is really slow.  Upon investigating, I found that it is 30x slower than a * a or torch.mul(a, a).\r\n\r\nI'm opening this issue to encourage the inclusion of a square function so that others don't make a similar mistake.\r\n\r\n![image](https://user-images.githubusercontent.com/876992/30041327-1ca25c30-919e-11e7-88ec-f8944132ea77.png)\r\n\r\n\r\n"}