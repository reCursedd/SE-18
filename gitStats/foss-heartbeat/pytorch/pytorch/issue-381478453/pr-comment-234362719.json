{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234362719", "pull_request_review_id": 175999937, "id": 234362719, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNDM2MjcxOQ==", "diff_hunk": "@@ -1,219 +1,94 @@\n #ifndef CAFFE2_OPERATORS_INT8_ADD_OP_H_\n #define CAFFE2_OPERATORS_INT8_ADD_OP_H_\n \n+#include <qnnpack.h>\n+\n #include \"caffe2/core/context.h\"\n #include \"caffe2/core/operator.h\"\n #include \"caffe2/core/tensor_int8.h\"\n-#include \"caffe2/operators/quantized/int8_simd.h\"\n #include \"caffe2/operators/quantized/int8_utils.h\"\n \n namespace caffe2 {\n \n namespace int8 {\n \n-namespace {\n-\n-/*\n- * Implementation based on TensorFlow Lite kernels:\n- * - Repo: https://github.com/tensorflow/tensorflow\n- * - Path: tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h\n- * - Hash: d4ad9c73969c45d1a224ebfc43eb645b9860216b\n- */\n-\n-/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-    http://www.apache.org/licenses/LICENSE-2.0\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-constexpr size_t kAddLeftShift = 20;\n-\n-void Int8Add(\n-    const uint8_t* X0_data,\n-    size_t N,\n-    size_t D,\n-    int32_t X0_offset,\n-    int32_t X0_multiplier,\n-    int X0_shift,\n-    const uint8_t* X1_data,\n-    int32_t X1_offset,\n-    int32_t X1_multiplier,\n-    int X1_shift,\n-    int32_t Y_offset,\n-    int32_t Y_multiplier,\n-    int Y_shift,\n-    uint8_t* Y_data,\n-    uint8_t Y_activation_min,\n-    uint8_t Y_activation_max,\n-    ThreadPool* threadPool) {\n-  CHECK_GT(X0_offset, -256);\n-  CHECK_GT(X1_offset, -256);\n-  CHECK_LT(X0_offset, 256);\n-  CHECK_LT(X1_offset, 256);\n-  static_assert(kAddLeftShift > 5, \"\");\n-  static_assert(kAddLeftShift <= 20, \"\");\n-\n-  auto f = [&](int, size_t n) {\n-    size_t d = 0;\n-\n-#ifdef INT8_NEON_SIMD\n-    constexpr size_t kIntermediateAddLeftShift = 4;\n-    const auto X0_offset_val =\n-        vshlq_n_s16(vdupq_n_s16(X0_offset), kIntermediateAddLeftShift);\n-    const auto X1_offset_val =\n-        vshlq_n_s16(vdupq_n_s16(X1_offset), kIntermediateAddLeftShift);\n-    const auto X0_shift_dup = vdupq_n_s32(-X0_shift);\n-    const auto X1_shift_dup = vdupq_n_s32(-X1_shift);\n-    const auto DUnroll = (D / 8) * 8;\n-\n-    for (; d < DUnroll; d += 8) {\n-      const auto X0_val_original = vld1_u8(X0_data + n * D + d);\n-      const auto X1_val_original = vld1_u8(X1_data + n * D + d);\n-\n-      // Load input\n-      // Widen to int16\n-      // Add int16 offset.\n-      // Widen to int32\n-      // Shift right by 20.\n-      // Alternatively, we can widening shift X by 4, shifty X0_offset by 4,\n-      // add, then shift by 16. Safe as X << 5 + X_offset << 5 can't overflow\n-      // uint16, as X ~ 8 bit, X_offset ~ 10 bit, so 15 bits total from X +\n-      // X_offset\n-      const auto X0_val_s16 = vreinterpretq_s16_u16(\n-          vshll_n_u8(X0_val_original, kIntermediateAddLeftShift));\n-      const auto X1_val_s16 = vreinterpretq_s16_u16(\n-          vshll_n_u8(X1_val_original, kIntermediateAddLeftShift));\n-      const auto X0_val = vaddq_s16(X0_val_s16, X0_offset_val);\n-      const auto X1_val = vaddq_s16(X1_val_s16, X1_offset_val);\n-      const auto X0_val_high = vget_high_s16(X0_val);\n-      const auto X0_val_low = vget_low_s16(X0_val);\n-      const auto X1_val_high = vget_high_s16(X1_val);\n-      const auto X1_val_low = vget_low_s16(X1_val);\n-      auto x11 =\n-          vshll_n_s16(X0_val_low, kAddLeftShift - kIntermediateAddLeftShift);\n-      auto x12 =\n-          vshll_n_s16(X0_val_high, kAddLeftShift - kIntermediateAddLeftShift);\n-      auto x21 =\n-          vshll_n_s16(X1_val_low, kAddLeftShift - kIntermediateAddLeftShift);\n-      auto x22 =\n-          vshll_n_s16(X1_val_high, kAddLeftShift - kIntermediateAddLeftShift);\n-      x11 = vqrdmulhq_n_s32(x11, X0_multiplier);\n-      x12 = vqrdmulhq_n_s32(x12, X0_multiplier);\n-      x21 = vqrdmulhq_n_s32(x21, X1_multiplier);\n-      x22 = vqrdmulhq_n_s32(x22, X1_multiplier);\n-      x11 = vshlq_s32(x11, X0_shift_dup);\n-      x12 = vshlq_s32(x12, X0_shift_dup);\n-      x21 = vshlq_s32(x21, X1_shift_dup);\n-      x22 = vshlq_s32(x22, X1_shift_dup);\n-      auto s1 = vaddq_s32(x11, x21);\n-      auto s2 = vaddq_s32(x12, x22);\n-      s1 = vqrdmulhq_n_s32(s1, Y_multiplier);\n-      s2 = vqrdmulhq_n_s32(s2, Y_multiplier);\n-      using gemmlowp::RoundingDivideByPOT;\n-      s1 = RoundingDivideByPOT(s1, Y_shift);\n-      s2 = RoundingDivideByPOT(s2, Y_shift);\n-      const auto s1_narrowed = vmovn_s32(s1);\n-      const auto s2_narrowed = vmovn_s32(s2);\n-      const auto s = vaddq_s16(\n-          vcombine_s16(s1_narrowed, s2_narrowed), vdupq_n_s16(Y_offset));\n-      auto ss = vqmovun_s16(s);\n-      ss = vmin_u8(ss, vdup_n_u8(Y_activation_max));\n-      ss = vmax_u8(ss, vdup_n_u8(Y_activation_min));\n-      vst1_u8(Y_data + n * D + d, ss);\n-    }\n-#endif // NEON\n-\n-    for (; d < D; d++) {\n-      const int32_t X0_val = X0_offset + X0_data[n * D + d];\n-      const int32_t X1_val = X1_offset + X1_data[n * D + d];\n-      const int32_t shifted_X0_val = X0_val * (1 << kAddLeftShift);\n-      const int32_t shifted_X1_val = X1_val * (1 << kAddLeftShift);\n-      const int32_t scaled_X0_val = MultiplyByQuantizedMultiplierSmallerThanOne(\n-          shifted_X0_val, X0_multiplier, X0_shift);\n-      const int32_t scaled_X1_val = MultiplyByQuantizedMultiplierSmallerThanOne(\n-          shifted_X1_val, X1_multiplier, X1_shift);\n-      const int32_t raw_sum = scaled_X0_val + scaled_X1_val;\n-      const int32_t raw_Y = MultiplyByQuantizedMultiplierSmallerThanOne(\n-                                raw_sum, Y_multiplier, Y_shift) +\n-          Y_offset;\n-      const int32_t clamped_Y = std::min<int32_t>(\n-          Y_activation_max, std::max<int32_t>(Y_activation_min, raw_Y));\n-      Y_data[n * D + d] = static_cast<uint8_t>(clamped_Y);\n-    }\n-  };\n-  threadPool->run(f, N);\n-}\n-\n-} // namespace\n-\n template <Activation Ac>\n class Int8AddOp final : public Operator<CPUContext> {\n  public:\n   Int8AddOp(const OperatorDef& operator_def, Workspace* ws)\n       : Operator<CPUContext>(operator_def, ws),\n         ws_(ws) {}\n \n+  ~Int8AddOp() {\n+    if (this->qnnpackOperator_ != nullptr) {\n+      qnnp_delete_operator(this->qnnpackOperator_);\n+      this->qnnpackOperator_ = nullptr;\n+    }\n+  }\n+\n   bool RunOnDevice() override {\n     CAFFE_ENFORCE_EQ(Inputs().size(), 2);\n-    const auto& X0 = Inputs()[0]->template Get<Int8TensorCPU>();\n-    const auto& X1 = Inputs()[1]->template Get<Int8TensorCPU>();\n+    const auto& A = Inputs()[0]->template Get<Int8TensorCPU>();\n+    const auto& B = Inputs()[1]->template Get<Int8TensorCPU>();\n     auto* Y = Outputs()[0]->template GetMutable<Int8TensorCPU>();\n-    auto X0_offset = -X0.zero_point;\n-    auto X1_offset = -X1.zero_point;\n-    int32_t Y_offset = this->template GetSingleArgument<int>(\"Y_zero_point\", 0);\n-    auto Y_scale = this->template GetSingleArgument<float>(\"Y_scale\", 1);\n-    const double twice_max_input_scale = 2 * std::max(X0.scale, X1.scale);\n-    const double real_X0_multiplier = X0.scale / twice_max_input_scale;\n-    const double real_X1_multiplier = X1.scale / twice_max_input_scale;\n-    const double real_Y_multiplier =\n-        twice_max_input_scale / ((1 << kAddLeftShift) * Y_scale);\n-\n-    Y->t.ResizeLike(X0.t);\n-    Y->zero_point = Y_offset;\n+\n+    const int32_t Y_zero_point = this->template GetSingleArgument<int>(\"Y_zero_point\", 0);\n+    const float Y_scale = this->template GetSingleArgument<float>(\"Y_scale\", 1);\n+    Y->t.ResizeLike(A.t);\n+    Y->zero_point = Y_zero_point;\n     Y->scale = Y_scale;\n \n-    int32_t X0_multiplier;\n-    int X0_shift;\n-    QuantizeMultiplierSmallerThanOne(\n-        real_X0_multiplier, &X0_multiplier, &X0_shift);\n-    int32_t X1_multiplier;\n-    int X1_shift;\n-    QuantizeMultiplierSmallerThanOne(\n-        real_X1_multiplier, &X1_multiplier, &X1_shift);\n-    int32_t Y_multiplier;\n-    int Y_shift;\n-    QuantizeMultiplierSmallerThanOne(\n-        real_Y_multiplier, &Y_multiplier, &Y_shift);\n-\n-    Int8Add(\n-        X0.t.template data<uint8_t>(),\n-        X0.t.numel() / X0.t.size(X0.t.dim() - 1),\n-        X0.t.size(X0.t.dim() - 1),\n-        X0_offset,\n-        X0_multiplier,\n-        X0_shift,\n-        X1.t.template data<uint8_t>(),\n-        X1_offset,\n-        X1_multiplier,\n-        X1_shift,\n-        Y_offset,\n-        Y_multiplier,\n-        Y_shift,\n+    initQNNPACK();\n+\n+    pthreadpool_t threadpool =\n+        reinterpret_cast<pthreadpool_t>(ws_->GetThreadPool());\n+\n+    if (this->qnnpackOperator_ == nullptr) {\n+      const qnnp_status createStatus = qnnp_create_add_nc_q8(\n+        1 /* channels */,\n+        A.zero_point, A.scale,\n+        B.zero_point, B.scale,\n+        static_cast<uint8_t>(Y_zero_point), Y_scale,\n+        activationLimits(Y_scale, Y_zero_point, Ac).first,\n+        activationLimits(Y_scale, Y_zero_point, Ac).second,\n+        &qnnpackOperator_);\n+      CAFFE_ENFORCE(\n+          createStatus == qnnp_status_success,\n+          \"failed to create QNNPACK add operator\");\n+      CAFFE_ENFORCE(this->qnnpackOperator_ != nullptr);\n+    }\n+\n+    const qnnp_status setupStatus = qnnp_setup_add_nc_q8(\n+        this->qnnpackOperator_,\n+        A.t.numel() /* batch size */,\n+        A.t.template data<uint8_t>(),\n+        1 /* A stride */,\n+        B.t.template data<uint8_t>(),\n+        1 /* B stride */,\n         Y->t.template mutable_data<uint8_t>(),\n-        activationLimits(Y->scale, Y->zero_point, Ac).first,\n-        activationLimits(Y->scale, Y->zero_point, Ac).second,\n-        ws_->GetThreadPool());\n+        1 /* Y stride */);\n+    CAFFE_ENFORCE(\n+        setupStatus == qnnp_status_success,\n+        \"failed to setup QNNPACK add operator\");\n+\n+#ifdef FBCODE_CAFFE2", "path": "caffe2/operators/quantized/int8_add_op.h", "position": 272, "original_position": 260, "commit_id": "09cc1201f2d30bd5ccc1910c5147ab64430a518b", "original_commit_id": "41f1cf38c8a88a2aeb2acf37679ecc138a20e7b0", "user": {"login": "ajtulloch", "id": 1121581, "node_id": "MDQ6VXNlcjExMjE1ODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1121581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajtulloch", "html_url": "https://github.com/ajtulloch", "followers_url": "https://api.github.com/users/ajtulloch/followers", "following_url": "https://api.github.com/users/ajtulloch/following{/other_user}", "gists_url": "https://api.github.com/users/ajtulloch/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajtulloch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajtulloch/subscriptions", "organizations_url": "https://api.github.com/users/ajtulloch/orgs", "repos_url": "https://api.github.com/users/ajtulloch/repos", "events_url": "https://api.github.com/users/ajtulloch/events{/privacy}", "received_events_url": "https://api.github.com/users/ajtulloch/received_events", "type": "User", "site_admin": false}, "body": "Why is this branch needed?", "created_at": "2018-11-16T22:09:27Z", "updated_at": "2018-11-23T15:55:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/14089#discussion_r234362719", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/14089", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234362719"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/14089#discussion_r234362719"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/14089"}}, "body_html": "<p>Why is this branch needed?</p>", "body_text": "Why is this branch needed?"}