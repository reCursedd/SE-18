{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157279972", "pull_request_review_id": 83902253, "id": 157279972, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzI3OTk3Mg==", "diff_hunk": "@@ -279,34 +280,63 @@ static PyObject * THPVariable_invert(PyObject* self, PyObject* args) {\n   END_HANDLE_TH_ERRORS\n }\n \n-static Tensor dispatch_to_backend(const Tensor & self, Backend backend) {\n+static void lazy_init_cuda() {\n+  static std::once_flag once;\n+  std::call_once(once, []() {\n+    auto module = THPObjectPtr(PyImport_ImportModule(\"torch.cuda\"));\n+    if (!module) throw python_error();\n+    auto res = THPObjectPtr(PyObject_CallMethod(module.get(), \"_lazy_init\", \"\"));\n+    if (!res) throw python_error();\n+  });\n+}\n+\n+static Tensor dispatch_type(const Tensor & self, const at::Type & type, int device=-1, bool async=false) {\n+  if (type.is_cuda()) {\n+    lazy_init_cuda();\n+  }\n   AutoNoGIL no_gil;\n-  AutoGPU auto_gpu(self);\n-  return self.toBackend(backend);\n+  AutoGPU auto_gpu(device != -1 ? device : self.type().is_cuda() ? self.get_device() : -1);", "path": "tools/autograd/templates/python_variable_methods.cpp", "position": null, "original_position": 69, "commit_id": "ddae27e99e428d3a90a94f9f4c170412bc2e0089", "original_commit_id": "9d37eb7f448cf29ec288c8f8ffa9b3fec223b733", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Good catch -- I'll fix the code. It's a little weird because `tensor.cuda()` defaults to the current device, while every other method on tensor (including `.byte()`, `.float()`, etc.) defaults to the tensor's device.", "created_at": "2017-12-15T19:23:20Z", "updated_at": "2018-11-23T15:37:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/4139#discussion_r157279972", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4139", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157279972"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4139#discussion_r157279972"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4139"}}, "body_html": "<p>Good catch -- I'll fix the code. It's a little weird because <code>tensor.cuda()</code> defaults to the current device, while every other method on tensor (including <code>.byte()</code>, <code>.float()</code>, etc.) defaults to the tensor's device.</p>", "body_text": "Good catch -- I'll fix the code. It's a little weird because tensor.cuda() defaults to the current device, while every other method on tensor (including .byte(), .float(), etc.) defaults to the tensor's device.", "in_reply_to_id": 157272371}