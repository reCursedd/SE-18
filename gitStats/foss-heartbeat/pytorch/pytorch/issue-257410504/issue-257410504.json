{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2721", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2721/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2721/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2721/events", "html_url": "https://github.com/pytorch/pytorch/issues/2721", "id": 257410504, "node_id": "MDU6SXNzdWUyNTc0MTA1MDQ=", "number": 2721, "title": "CPU/GPU inconsistency for gradient of x.norm() when x.norm() == 0", "user": {"login": "fdraxler", "id": 15550824, "node_id": "MDQ6VXNlcjE1NTUwODI0", "avatar_url": "https://avatars2.githubusercontent.com/u/15550824?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fdraxler", "html_url": "https://github.com/fdraxler", "followers_url": "https://api.github.com/users/fdraxler/followers", "following_url": "https://api.github.com/users/fdraxler/following{/other_user}", "gists_url": "https://api.github.com/users/fdraxler/gists{/gist_id}", "starred_url": "https://api.github.com/users/fdraxler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fdraxler/subscriptions", "organizations_url": "https://api.github.com/users/fdraxler/orgs", "repos_url": "https://api.github.com/users/fdraxler/repos", "events_url": "https://api.github.com/users/fdraxler/events{/privacy}", "received_events_url": "https://api.github.com/users/fdraxler/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-13T14:50:40Z", "updated_at": "2017-09-27T03:55:17Z", "closed_at": "2017-09-27T03:55:17Z", "author_association": "NONE", "body_html": "<p>Running backward() on the norm of a tensor with zero norm on the GPU is inconsistent with the behaviour on the CPU. The GPU raises a RuntimeError, the CPU sets the gradient to nan.</p>\n<p>This code reproduces the inconsistency:</p>\n<pre><code>import torch\na = torch.FloatTensor((5,5))\na.zero_()\n\n# Calculate norm  on CPU\nv = torch.autograd.Variable(a, requires_grad=True)\nn = v.norm()\nn.backward()\nprint(v.grad) # Variable containing: nan nan [torch.FloatTensor of size 2]\n\n# Calculate norm with CUDA\nvc = torch.autograd.Variable(a, requires_grad=True).cuda()\nnc = vc.norm()\nnc.backward() # Raises RuntimeError\nprint(vc.grad) # never reached\n</code></pre>\n<p>Check out the full stack trace to identify the relevant code parts:</p>\n<pre><code>Traceback (most recent call last):\n  File \"&lt;ipython-input&gt;\", line 1, in &lt;module&gt;\n    nc.backward()\n  File \"/.../site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"/.../site-packages/torch/autograd/function.py\", line 91, in apply\n    return self._forward_cls.backward(self, *args)\n  File \"/.../site-packages/torch/autograd/_functions/reduce.py\", line 244, in backward\n    scale_v = (grad_output / ctx.norm).expand_as(input)\n  File \"/.../site-packages/torch/autograd/variable.py\", line 841, in __div__\n    return self.div(other)\n  File \"/.../site-packages/torch/autograd/variable.py\", line 354, in div\n    return DivConstant.apply(self, other)\n  File \"/.../site-packages/torch/autograd/_functions/basic_ops.py\", line 171, in forward\n    return tensor.div(ctx.constant)\nRuntimeError: invalid argument 3: divide by zero at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCTensorMathPairwise.cu:66\n</code></pre>\n<p>My torch version is 0.2.0 (py36h53baedd_4cu80)</p>", "body_text": "Running backward() on the norm of a tensor with zero norm on the GPU is inconsistent with the behaviour on the CPU. The GPU raises a RuntimeError, the CPU sets the gradient to nan.\nThis code reproduces the inconsistency:\nimport torch\na = torch.FloatTensor((5,5))\na.zero_()\n\n# Calculate norm  on CPU\nv = torch.autograd.Variable(a, requires_grad=True)\nn = v.norm()\nn.backward()\nprint(v.grad) # Variable containing: nan nan [torch.FloatTensor of size 2]\n\n# Calculate norm with CUDA\nvc = torch.autograd.Variable(a, requires_grad=True).cuda()\nnc = vc.norm()\nnc.backward() # Raises RuntimeError\nprint(vc.grad) # never reached\n\nCheck out the full stack trace to identify the relevant code parts:\nTraceback (most recent call last):\n  File \"<ipython-input>\", line 1, in <module>\n    nc.backward()\n  File \"/.../site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"/.../site-packages/torch/autograd/function.py\", line 91, in apply\n    return self._forward_cls.backward(self, *args)\n  File \"/.../site-packages/torch/autograd/_functions/reduce.py\", line 244, in backward\n    scale_v = (grad_output / ctx.norm).expand_as(input)\n  File \"/.../site-packages/torch/autograd/variable.py\", line 841, in __div__\n    return self.div(other)\n  File \"/.../site-packages/torch/autograd/variable.py\", line 354, in div\n    return DivConstant.apply(self, other)\n  File \"/.../site-packages/torch/autograd/_functions/basic_ops.py\", line 171, in forward\n    return tensor.div(ctx.constant)\nRuntimeError: invalid argument 3: divide by zero at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCTensorMathPairwise.cu:66\n\nMy torch version is 0.2.0 (py36h53baedd_4cu80)", "body": "Running backward() on the norm of a tensor with zero norm on the GPU is inconsistent with the behaviour on the CPU. The GPU raises a RuntimeError, the CPU sets the gradient to nan.\r\n\r\nThis code reproduces the inconsistency:\r\n\r\n```\r\nimport torch\r\na = torch.FloatTensor((5,5))\r\na.zero_()\r\n\r\n# Calculate norm  on CPU\r\nv = torch.autograd.Variable(a, requires_grad=True)\r\nn = v.norm()\r\nn.backward()\r\nprint(v.grad) # Variable containing: nan nan [torch.FloatTensor of size 2]\r\n\r\n# Calculate norm with CUDA\r\nvc = torch.autograd.Variable(a, requires_grad=True).cuda()\r\nnc = vc.norm()\r\nnc.backward() # Raises RuntimeError\r\nprint(vc.grad) # never reached\r\n```\r\n\r\nCheck out the full stack trace to identify the relevant code parts:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<ipython-input>\", line 1, in <module>\r\n    nc.backward()\r\n  File \"/.../site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\n  File \"/.../site-packages/torch/autograd/function.py\", line 91, in apply\r\n    return self._forward_cls.backward(self, *args)\r\n  File \"/.../site-packages/torch/autograd/_functions/reduce.py\", line 244, in backward\r\n    scale_v = (grad_output / ctx.norm).expand_as(input)\r\n  File \"/.../site-packages/torch/autograd/variable.py\", line 841, in __div__\r\n    return self.div(other)\r\n  File \"/.../site-packages/torch/autograd/variable.py\", line 354, in div\r\n    return DivConstant.apply(self, other)\r\n  File \"/.../site-packages/torch/autograd/_functions/basic_ops.py\", line 171, in forward\r\n    return tensor.div(ctx.constant)\r\nRuntimeError: invalid argument 3: divide by zero at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCTensorMathPairwise.cu:66\r\n```\r\n\r\nMy torch version is 0.2.0 (py36h53baedd_4cu80)\r\n"}