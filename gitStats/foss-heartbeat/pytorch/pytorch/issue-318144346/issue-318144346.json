{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7003", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7003/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7003/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7003/events", "html_url": "https://github.com/pytorch/pytorch/issues/7003", "id": 318144346, "node_id": "MDU6SXNzdWUzMTgxNDQzNDY=", "number": 7003, "title": "SELU layer regression in 0.4.0", "user": {"login": "ctlaltdefeat", "id": 4173549, "node_id": "MDQ6VXNlcjQxNzM1NDk=", "avatar_url": "https://avatars1.githubusercontent.com/u/4173549?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ctlaltdefeat", "html_url": "https://github.com/ctlaltdefeat", "followers_url": "https://api.github.com/users/ctlaltdefeat/followers", "following_url": "https://api.github.com/users/ctlaltdefeat/following{/other_user}", "gists_url": "https://api.github.com/users/ctlaltdefeat/gists{/gist_id}", "starred_url": "https://api.github.com/users/ctlaltdefeat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ctlaltdefeat/subscriptions", "organizations_url": "https://api.github.com/users/ctlaltdefeat/orgs", "repos_url": "https://api.github.com/users/ctlaltdefeat/repos", "events_url": "https://api.github.com/users/ctlaltdefeat/events{/privacy}", "received_events_url": "https://api.github.com/users/ctlaltdefeat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-26T18:22:10Z", "updated_at": "2018-04-26T21:23:46Z", "closed_at": "2018-04-26T21:23:46Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>The <code>nn.SELU</code> activation layer seems to have suffered some weird regression in version 0.4.0 when compared to 0.3.1, in that in a fairly straightforward sequential network with linear layers followed by SELU activations that was training well in 0.3.1 now seems unable to train (error stays very high). The same network with ReLU activations trains just fine, although not as well as the SELU network in 0.3.1 (which is be to expected, as my usecase is one where SELU has theoretical improvements).</p>\n<h2>Code example</h2>\n<p>Unfortunately it's not quite possible for me to reinstall 0.3.1 at the moment to compare and provide a code example, but I was wondering if there is something in the code for the SELU layer for which functionality changed in 0.4.0 and could indicate the problem.</p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.1.85</p>\n<p>OS: Arch Linux<br>\nGCC version: (GCC) 7.3.0<br>\nCMake version: version 3.10.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.1.85<br>\nGPU models and configuration: GPU 0: GeForce GTX 1080<br>\nNvidia driver version: 390.25<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.2)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchaudio (0.1)<br>\n[pip3] torchfile (0.1.0)<br>\n[pip3] torchtext (0.2.1)<br>\n[pip3] torchvision (0.2.0)<br>\n[pip3] warpctc-pytorch (0.1)<br>\n[conda] cuda90                    1.0                  h6433d27_0    pytorch<br>\n[conda] cuda91                    1.0                  h4c16780_0    pytorch<br>\n[conda] pytorch                   0.3.1           py36_cuda9.1.85_cudnn7.0.5_2  [cuda91]  pytorch<br>\n[conda] torch                     0.4.0                     <br>\n[conda] torchaudio                0.1                       <br>\n[conda] torchfile                 0.1.0                     <br>\n[conda] torchtext                 0.2.1                     <br>\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch<br>\n[conda] warpctc_pytorch           0.1                       </p>", "body_text": "Issue description\nThe nn.SELU activation layer seems to have suffered some weird regression in version 0.4.0 when compared to 0.3.1, in that in a fairly straightforward sequential network with linear layers followed by SELU activations that was training well in 0.3.1 now seems unable to train (error stays very high). The same network with ReLU activations trains just fine, although not as well as the SELU network in 0.3.1 (which is be to expected, as my usecase is one where SELU has theoretical improvements).\nCode example\nUnfortunately it's not quite possible for me to reinstall 0.3.1 at the moment to compare and provide a code example, but I was wondering if there is something in the code for the SELU layer for which functionality changed in 0.4.0 and could indicate the problem.\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\nOS: Arch Linux\nGCC version: (GCC) 7.3.0\nCMake version: version 3.10.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: GPU 0: GeForce GTX 1080\nNvidia driver version: 390.25\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip3] numpy (1.14.2)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.4.0)\n[pip3] torchaudio (0.1)\n[pip3] torchfile (0.1.0)\n[pip3] torchtext (0.2.1)\n[pip3] torchvision (0.2.0)\n[pip3] warpctc-pytorch (0.1)\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\n[conda] pytorch                   0.3.1           py36_cuda9.1.85_cudnn7.0.5_2  [cuda91]  pytorch\n[conda] torch                     0.4.0                     \n[conda] torchaudio                0.1                       \n[conda] torchfile                 0.1.0                     \n[conda] torchtext                 0.2.1                     \n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch\n[conda] warpctc_pytorch           0.1", "body": "## Issue description\r\n\r\nThe ```nn.SELU``` activation layer seems to have suffered some weird regression in version 0.4.0 when compared to 0.3.1, in that in a fairly straightforward sequential network with linear layers followed by SELU activations that was training well in 0.3.1 now seems unable to train (error stays very high). The same network with ReLU activations trains just fine, although not as well as the SELU network in 0.3.1 (which is be to expected, as my usecase is one where SELU has theoretical improvements).\r\n\r\n## Code example\r\n\r\nUnfortunately it's not quite possible for me to reinstall 0.3.1 at the moment to compare and provide a code example, but I was wondering if there is something in the code for the SELU layer for which functionality changed in 0.4.0 and could indicate the problem.\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 390.25\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchaudio (0.1)\r\n[pip3] torchfile (0.1.0)\r\n[pip3] torchtext (0.2.1)\r\n[pip3] torchvision (0.2.0)\r\n[pip3] warpctc-pytorch (0.1)\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] pytorch                   0.3.1           py36_cuda9.1.85_cudnn7.0.5_2  [cuda91]  pytorch\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchaudio                0.1                       <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchtext                 0.2.1                     <pip>\r\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch\r\n[conda] warpctc_pytorch           0.1                       <pip>\r\n\r\n"}