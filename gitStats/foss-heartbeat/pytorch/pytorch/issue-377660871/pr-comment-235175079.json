{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/235175079", "pull_request_review_id": 176982069, "id": 235175079, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNTE3NTA3OQ==", "diff_hunk": "@@ -33,5 +41,106 @@ Tensor& _copy__cpu(Tensor& self, const Tensor& src) {\n   return self;\n }\n \n+// special case copy where tensor is contiguous and src is a transposed matrix\n+// This can be generalized to most copies, but it's tricker\n+void _copy_same_type_transpose_(Tensor& self, const Tensor& src) {\n+  const int64_t BLOCK_SZ = 60;\n+  Tensor buf = empty({BLOCK_SZ, BLOCK_SZ}, self.options());\n+\n+  AT_DISPATCH_ALL_TYPES_AND_HALF(\n+      self.type(), \"_copy_same_type_transpose_\", [&]() {\n+        scalar_t* sp = src.data<scalar_t>();\n+        scalar_t* rp = self.data<scalar_t>();\n+        scalar_t* bp = buf.data<scalar_t>();\n+\n+        int64_t NR = src.size(0);\n+        int64_t NC = src.size(1);\n+        for (int64_t R = 0; R < NR; R += BLOCK_SZ) {\n+          for (int64_t C = 0; C < NC; C += BLOCK_SZ) {\n+            scalar_t* spo = sp + R + C * NR;\n+            scalar_t* rpo = rp + C + R * NC;\n+\n+            int nr = std::min(NR - R, BLOCK_SZ);\n+            int nc = std::min(NC - C, BLOCK_SZ);\n+\n+            // 1. copy columns from src to buf\n+            for (int c = 0; c < nc; c++) {\n+              memcpy(bp + c * BLOCK_SZ, spo + c * NR, nr * sizeof(scalar_t));\n+            }\n+\n+            // 2. transpose buf in place\n+            int rc_max = std::max(nr, nc);\n+            int rc_min = std::min(nr, nc);\n+            for (int r = 0; r < rc_max; r++) {\n+              int end = std::min(r, rc_min);\n+              for (int c = 0; c < end; c++) {\n+                scalar_t tmp = bp[r + BLOCK_SZ * c];\n+                bp[r + BLOCK_SZ * c] = bp[r * BLOCK_SZ + c];\n+                bp[r * BLOCK_SZ + c] = tmp;\n+              }\n+            }\n+\n+            // 3. copy rows from buf to dst\n+            for (int r = 0; r < nr; r++) {\n+              memcpy(rpo + r * NC, bp + r * BLOCK_SZ, nc * sizeof(scalar_t));\n+            }\n+          }\n+        }\n+      });\n+}\n+\n+void _copy_same_type_(Tensor& self, const Tensor& src) {\n+  if (self.is_same(src)) {\n+    return;\n+  }\n+\n+  bool serial_path = false;\n+  if (self.numel() == src.numel()) {\n+    if (self.is_contiguous() && src.is_contiguous()) {\n+      AT_DISPATCH_ALL_TYPES_AND_HALF(self.type(), \"_copy_same_type_\", [&]() {\n+        scalar_t* self_ptr = self.data<scalar_t>();\n+        scalar_t* src_ptr = src.data<scalar_t>();\n+\n+        auto sample = [&](int64_t begin, int64_t end) {\n+          int64_t len = end - begin;\n+          scalar_t* self_seg = self_ptr + begin;\n+          scalar_t* src_seg = src_ptr + begin;\n+          at::vec256::convert<scalar_t, scalar_t>(src_seg, self_seg, len);\n+        };\n+\n+        parallel_for(0, self.numel(), /* grain_size= */ 800, sample);\n+      });\n+    } else if (copy_transpose_valid(self, src)) {\n+      _copy_same_type_transpose_(self, src);\n+    } else {\n+#ifdef _OPENMP\n+      if (in_parallel_region()) {", "path": "aten/src/ATen/native/Copy.cpp", "position": 99, "original_position": 99, "commit_id": "33ca80c566afa23b32ab629b42a0b16f99fb1bce", "original_commit_id": "33ca80c566afa23b32ab629b42a0b16f99fb1bce", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "This if statement looks flipped. `in_parallel_region == omp_in_parallel`; but when this is true, we should run the serial path. But you are running serial path when we are NOT in a parallel region.", "created_at": "2018-11-20T21:27:28Z", "updated_at": "2018-11-23T15:55:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/13603#discussion_r235175079", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13603", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/235175079"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13603#discussion_r235175079"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13603"}}, "body_html": "<p>This if statement looks flipped. <code>in_parallel_region == omp_in_parallel</code>; but when this is true, we should run the serial path. But you are running serial path when we are NOT in a parallel region.</p>", "body_text": "This if statement looks flipped. in_parallel_region == omp_in_parallel; but when this is true, we should run the serial path. But you are running serial path when we are NOT in a parallel region."}