{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6852", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6852/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6852/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6852/events", "html_url": "https://github.com/pytorch/pytorch/issues/6852", "id": 316658027, "node_id": "MDU6SXNzdWUzMTY2NTgwMjc=", "number": 6852, "title": "RuntimeError: size mismatch in matmul", "user": {"login": "monajalal", "id": 1892917, "node_id": "MDQ6VXNlcjE4OTI5MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1892917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/monajalal", "html_url": "https://github.com/monajalal", "followers_url": "https://api.github.com/users/monajalal/followers", "following_url": "https://api.github.com/users/monajalal/following{/other_user}", "gists_url": "https://api.github.com/users/monajalal/gists{/gist_id}", "starred_url": "https://api.github.com/users/monajalal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/monajalal/subscriptions", "organizations_url": "https://api.github.com/users/monajalal/orgs", "repos_url": "https://api.github.com/users/monajalal/repos", "events_url": "https://api.github.com/users/monajalal/events{/privacy}", "received_events_url": "https://api.github.com/users/monajalal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-23T04:28:32Z", "updated_at": "2018-04-26T04:17:28Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I am using this git repo. I had no problem running it on CPU without F1 measure. Now that I want to run it on GPU with F1 measure I get this error:</p>\n<p>Do you know how to fix the below error?</p>\n<pre><code>[jalal@goku AttentionTargetSentiment]$ python main.py -cuda -getF1 -which-model contextualized -which-embedding 200dt -device 0 \n\nLoading data...\nall 23095\nnotfound: 9849\nratio: 0.4264559428447716\n\nParameters:\n\tATTENTION_SIZE=150\n\tBATCH_SIZE=16\n\tCLIP_NORM=None\n\tCUDA=True\n\tDEVICE=0\n\tDROPOUT_EMBED=0.2\n\tDROPOUT_RNN=0.4\n\tEMBED_DIM=200\n\tEMBED_NUM=23095\n\tEPOCHS=30\n\tGETF1=True\n\tGRAYSCALE=None\n\tHIDDEN_SIZE=150\n\tIF_RE=False\n\tLABEL_NUM=3\n\tLOG_INTERVAL=1\n\tLR=0.01\n\tLR_SCHEDULER=None\n\tMAX_NORM=None\n\tMESSAGE=tt\n\tNEED_SMALLEMBED=False\n\tSAVE_DIR=snapshot/tt\n\tSAVE_INTERVAL=100\n\tSHUFFLE=True\n\tSNAPSHOT=None\n\tTEST=False\n\tTEST_INTERVAL=100\n\tUSE_EMBEDDING=True\n\tWEIGHT_DECAY=1e-06\n\tWHICH_DATA=Z\n\tWHICH_EMBEDDING=200dt\n\tWHICH_INIT=xavier\n\tWHICH_MODEL=contextualized\n\tWHICH_OPTIM=Adagrad\nTraceback (most recent call last):\n  File \"main.py\", line 113, in &lt;module&gt;\n    train.getF1(args, m_model, test_data.iterator, train_data.vocabulary_label)\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/train.py\", line 152, in getF1\n    logit = model(feature, batch.target_start, batch.target_end)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/model/contextualized.py\", line 90, in forward\n    s = self.attention(x, average_target)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/model/attention.py\", line 22, in forward\n    m_combine = F.tanh(self.linear(m_combine))\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 55, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\", line 837, in linear\n    output = input.matmul(weight.t())\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\", line 386, in matmul\n    **return torch.matmul(self, other)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/functional.py\", line 192, in matmul\n    output = torch.mm(tensor1, tensor2)\nRuntimeError: size mismatch at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:247**\n</code></pre>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"316657686\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/vipzgy/AttentionTargetSentiment/issues/4\" data-hovercard-type=\"issue\" data-hovercard-url=\"/vipzgy/AttentionTargetSentiment/issues/4/hovercard\" href=\"https://github.com/vipzgy/AttentionTargetSentiment/issues/4\">vipzgy/AttentionTargetSentiment#4</a></p>\n<pre><code>[jalal@goku AttentionTargetSentiment]$ conda list torch\n# packages in environment at /scratch/sjn/anaconda:\n#\n# Name                    Version                   Build  Channel\ntorch                     0.3.1                     &lt;pip&gt;\n[jalal@goku AttentionTargetSentiment]$ python -V\nPython 3.6.3 :: Anaconda custom (64-bit)\n[jalal@goku AttentionTargetSentiment]$ lsb_release -a\nLSB Version:\t:core-4.1-amd64:core-4.1-noarch\nDistributor ID:\tCentOS\nDescription:\tCentOS Linux release 7.4.1708 (Core) \nRelease:\t7.4.1708\nCodename:\tCore\n\n</code></pre>", "body_text": "I am using this git repo. I had no problem running it on CPU without F1 measure. Now that I want to run it on GPU with F1 measure I get this error:\nDo you know how to fix the below error?\n[jalal@goku AttentionTargetSentiment]$ python main.py -cuda -getF1 -which-model contextualized -which-embedding 200dt -device 0 \n\nLoading data...\nall 23095\nnotfound: 9849\nratio: 0.4264559428447716\n\nParameters:\n\tATTENTION_SIZE=150\n\tBATCH_SIZE=16\n\tCLIP_NORM=None\n\tCUDA=True\n\tDEVICE=0\n\tDROPOUT_EMBED=0.2\n\tDROPOUT_RNN=0.4\n\tEMBED_DIM=200\n\tEMBED_NUM=23095\n\tEPOCHS=30\n\tGETF1=True\n\tGRAYSCALE=None\n\tHIDDEN_SIZE=150\n\tIF_RE=False\n\tLABEL_NUM=3\n\tLOG_INTERVAL=1\n\tLR=0.01\n\tLR_SCHEDULER=None\n\tMAX_NORM=None\n\tMESSAGE=tt\n\tNEED_SMALLEMBED=False\n\tSAVE_DIR=snapshot/tt\n\tSAVE_INTERVAL=100\n\tSHUFFLE=True\n\tSNAPSHOT=None\n\tTEST=False\n\tTEST_INTERVAL=100\n\tUSE_EMBEDDING=True\n\tWEIGHT_DECAY=1e-06\n\tWHICH_DATA=Z\n\tWHICH_EMBEDDING=200dt\n\tWHICH_INIT=xavier\n\tWHICH_MODEL=contextualized\n\tWHICH_OPTIM=Adagrad\nTraceback (most recent call last):\n  File \"main.py\", line 113, in <module>\n    train.getF1(args, m_model, test_data.iterator, train_data.vocabulary_label)\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/train.py\", line 152, in getF1\n    logit = model(feature, batch.target_start, batch.target_end)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/model/contextualized.py\", line 90, in forward\n    s = self.attention(x, average_target)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/model/attention.py\", line 22, in forward\n    m_combine = F.tanh(self.linear(m_combine))\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 55, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\", line 837, in linear\n    output = input.matmul(weight.t())\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\", line 386, in matmul\n    **return torch.matmul(self, other)\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/functional.py\", line 192, in matmul\n    output = torch.mm(tensor1, tensor2)\nRuntimeError: size mismatch at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:247**\n\nvipzgy/AttentionTargetSentiment#4\n[jalal@goku AttentionTargetSentiment]$ conda list torch\n# packages in environment at /scratch/sjn/anaconda:\n#\n# Name                    Version                   Build  Channel\ntorch                     0.3.1                     <pip>\n[jalal@goku AttentionTargetSentiment]$ python -V\nPython 3.6.3 :: Anaconda custom (64-bit)\n[jalal@goku AttentionTargetSentiment]$ lsb_release -a\nLSB Version:\t:core-4.1-amd64:core-4.1-noarch\nDistributor ID:\tCentOS\nDescription:\tCentOS Linux release 7.4.1708 (Core) \nRelease:\t7.4.1708\nCodename:\tCore", "body": "I am using this git repo. I had no problem running it on CPU without F1 measure. Now that I want to run it on GPU with F1 measure I get this error:\r\n\r\nDo you know how to fix the below error?\r\n\r\n```\r\n[jalal@goku AttentionTargetSentiment]$ python main.py -cuda -getF1 -which-model contextualized -which-embedding 200dt -device 0 \r\n\r\nLoading data...\r\nall 23095\r\nnotfound: 9849\r\nratio: 0.4264559428447716\r\n\r\nParameters:\r\n\tATTENTION_SIZE=150\r\n\tBATCH_SIZE=16\r\n\tCLIP_NORM=None\r\n\tCUDA=True\r\n\tDEVICE=0\r\n\tDROPOUT_EMBED=0.2\r\n\tDROPOUT_RNN=0.4\r\n\tEMBED_DIM=200\r\n\tEMBED_NUM=23095\r\n\tEPOCHS=30\r\n\tGETF1=True\r\n\tGRAYSCALE=None\r\n\tHIDDEN_SIZE=150\r\n\tIF_RE=False\r\n\tLABEL_NUM=3\r\n\tLOG_INTERVAL=1\r\n\tLR=0.01\r\n\tLR_SCHEDULER=None\r\n\tMAX_NORM=None\r\n\tMESSAGE=tt\r\n\tNEED_SMALLEMBED=False\r\n\tSAVE_DIR=snapshot/tt\r\n\tSAVE_INTERVAL=100\r\n\tSHUFFLE=True\r\n\tSNAPSHOT=None\r\n\tTEST=False\r\n\tTEST_INTERVAL=100\r\n\tUSE_EMBEDDING=True\r\n\tWEIGHT_DECAY=1e-06\r\n\tWHICH_DATA=Z\r\n\tWHICH_EMBEDDING=200dt\r\n\tWHICH_INIT=xavier\r\n\tWHICH_MODEL=contextualized\r\n\tWHICH_OPTIM=Adagrad\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 113, in <module>\r\n    train.getF1(args, m_model, test_data.iterator, train_data.vocabulary_label)\r\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/train.py\", line 152, in getF1\r\n    logit = model(feature, batch.target_start, batch.target_end)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/model/contextualized.py\", line 90, in forward\r\n    s = self.attention(x, average_target)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/scratch2/debate_tweets/sentiment/AttentionTargetSentiment/model/attention.py\", line 22, in forward\r\n    m_combine = F.tanh(self.linear(m_combine))\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 55, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\", line 837, in linear\r\n    output = input.matmul(weight.t())\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\", line 386, in matmul\r\n    **return torch.matmul(self, other)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/torch/functional.py\", line 192, in matmul\r\n    output = torch.mm(tensor1, tensor2)\r\nRuntimeError: size mismatch at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:247**\r\n```\r\n\r\nhttps://github.com/vipzgy/AttentionTargetSentiment/issues/4\r\n\r\n```\r\n[jalal@goku AttentionTargetSentiment]$ conda list torch\r\n# packages in environment at /scratch/sjn/anaconda:\r\n#\r\n# Name                    Version                   Build  Channel\r\ntorch                     0.3.1                     <pip>\r\n[jalal@goku AttentionTargetSentiment]$ python -V\r\nPython 3.6.3 :: Anaconda custom (64-bit)\r\n[jalal@goku AttentionTargetSentiment]$ lsb_release -a\r\nLSB Version:\t:core-4.1-amd64:core-4.1-noarch\r\nDistributor ID:\tCentOS\r\nDescription:\tCentOS Linux release 7.4.1708 (Core) \r\nRelease:\t7.4.1708\r\nCodename:\tCore\r\n\r\n```"}