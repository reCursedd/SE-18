{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210109590", "pull_request_review_id": 146259038, "id": 210109590, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDEwOTU5MA==", "diff_hunk": "@@ -0,0 +1,700 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+template<typename T>\n+using pair_of = std::pair<T, T>;\n+\n+template<typename T>\n+using tpair_of = std::tuple<T, T>;\n+\n+// Those could have been function pointers, but MSVC chokes on function pointers as template parameters\n+struct tanh_f {\n+  Tensor operator()(const Tensor& t) const { return at::tanh(t); }\n+};\n+\n+struct relu_f {\n+  Tensor operator()(const Tensor& t) const { return at::relu(t); }\n+};\n+\n+struct PackedSequence {\n+  PackedSequence() = default;\n+  PackedSequence(Tensor _data, Tensor _batch_sizes)\n+    : data(std::move(_data)), batch_sizes(std::move(_batch_sizes)) {}\n+\n+  Tensor data;\n+  Tensor batch_sizes;\n+};\n+\n+// Pretty much all cells we support take the same set of arguments, but threading those\n+// 4 arguments manually is really annoying. Their lifetime is externally managed, so we only\n+// pass this struct of references around.\n+struct CellParams {\n+  CellParams(const Tensor& _w_ih, const Tensor& _w_hh, const Tensor& _b_ih, const Tensor& _b_hh)\n+    : w_ih(_w_ih), w_hh(_w_hh), b_ih(_b_ih), b_hh(_b_hh) {};\n+\n+  const Tensor& w_ih;\n+  const Tensor& w_hh;\n+  const Tensor& b_ih; /* optional */\n+  const Tensor& b_hh; /* optional */\n+};\n+\n+// Gathers every two elements of a vector in a vector of pairs\n+template<typename T>\n+static std::vector<pair_of<T>> pair_vec(const std::vector<T>& vals) {\n+  AT_CHECK(vals.size() % 2 == 0, \"Odd number of params or hiddens given to a bidirectional RNN\");\n+  std::vector<pair_of<T>> result;\n+  result.reserve(vals.size() / 2);\n+  for (int64_t i = 0; i < vals.size(); i += 2) {\n+    result.emplace_back(vals[i], vals[i + 1]);\n+  }\n+  return result;\n+}\n+\n+// Flattens a vector of pairs\n+template<typename T>\n+static std::vector<T> unpair_vec(std::vector<pair_of<T>>&& vals) {\n+  std::vector<T> result;\n+  result.reserve(vals.size() * 2);\n+  for (int64_t i = 0; i < vals.size(); i++) {\n+    result.push_back(std::move(vals[i].first));\n+    result.push_back(std::move(vals[i].second));\n+  }\n+  return result;\n+}\n+\n+// Parses a flat list of parameter tensors into a list of CellParams\n+static std::vector<CellParams> gather_params(TensorList params, bool has_biases) {\n+  static at::Tensor undefined;\n+  std::vector<CellParams> result;\n+  if (has_biases) {\n+    AT_CHECK(params.size() % 4 == 0, \"got an incorrect number of RNN parameters\");\n+    for (size_t i = 0; i < params.size(); i += 4) {\n+      result.emplace_back(params[i], params[i + 1], params[i + 2], params[i + 3]);\n+    }\n+  } else {\n+    AT_CHECK(params.size() % 2 == 0, \"got an incorrect number of RNN parameters\");\n+    for (size_t i = 0; i < params.size(); i += 2) {\n+      result.emplace_back(params[i], params[i + 1], undefined, undefined);\n+    }\n+  }\n+  return result;\n+}\n+\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// HIDDEN STATE FUNCTIONS\n+//\n+// Functions implemented below are implemented as templates based on hidden type,\n+// because they need to work both with simple RNNs and GRU (which use a single Tensor),\n+// as well as with LSTM (or possibly more complicated architectures in the future).\n+// Still, there are some operations that need to be performed on the hidden states\n+// alone, and for this purpose we provide an overloaded set of functions below.\n+\n+Tensor hidden_as_output(const Tensor& t) { return t; }\n+Tensor hidden_as_output(const tpair_of<Tensor>& t) { return std::get<0>(t); }\n+\n+template<size_t index>\n+std::vector<Tensor> project(at::ArrayRef<tpair_of<Tensor>> tuples) {\n+  std::vector<Tensor> result;\n+  result.reserve(tuples.size());\n+  for (auto & t : tuples) {\n+    result.push_back(std::get<index>(t));\n+  }\n+  return result;\n+}\n+\n+Tensor hidden_concat(at::ArrayRef<Tensor> hiddens) { return at::cat(hiddens, 0); }\n+tpair_of<Tensor> hidden_concat(at::ArrayRef<tpair_of<Tensor>> hiddens) {\n+  return std::make_tuple(hidden_concat(project<0>(hiddens)), hidden_concat(project<1>(hiddens)));\n+}\n+\n+Tensor hidden_slice(const Tensor& t, int64_t start, int64_t end) {\n+  return t.narrow(0, start, end - start);\n+}\n+tpair_of<Tensor> hidden_slice(const tpair_of<Tensor>& t, int64_t start, int64_t end) {\n+  return std::make_tuple(hidden_slice(std::get<0>(t), start, end),\n+                         hidden_slice(std::get<1>(t), start, end));\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// CELL IMPLEMENTATIONS\n+//\n+// Cell is a basic component of an RNN, representing a single application of the\n+// recurrent function. You can think of it as a function of signature\n+//\n+// (Tensor input, hidden_type hidden, CellParams) -> hidden_type\n+//\n+// which means that it consumes an input tensor, and updates the previous hidden state.\n+// It's a struct only because functional programming in C++ is a pain, and it's easier\n+// to pass around \"vtable pointers\" than actual function pointers.\n+\n+Tensor linear(const Tensor& input, const Tensor& weight, /* optional */ const Tensor& bias={}) {\n+  if (input.dim() == 2 && bias.defined()) {\n+    // fused op is marginally faster\n+    return at::addmm(bias, input, weight.t());\n+  }\n+\n+  auto output = at::matmul(input, weight.t());\n+  if (bias.defined()) {\n+    output.add_(bias);\n+  }\n+  return output;\n+}\n+\n+template<typename hidden_type_tmpl>\n+struct Cell {\n+  using hidden_type = hidden_type_tmpl;\n+  virtual hidden_type operator()(const Tensor& input, const hidden_type& hidden, const CellParams& params) const = 0;\n+};\n+\n+template<typename nonlinearity>\n+struct SimpleCell : Cell<Tensor> {\n+  hidden_type operator()(const Tensor& input, const hidden_type& hidden, const CellParams& params) const override {\n+    return nonlinearity{}(linear(input, params.w_ih, params.b_ih) + linear(hidden, params.w_hh, params.b_hh));\n+  }\n+};\n+\n+// TODO: can use inplace ops?\n+struct LSTMCell : Cell<std::tuple<Tensor, Tensor>> {\n+  hidden_type operator()(const Tensor& input, const hidden_type& hidden, const CellParams& params) const override {\n+    auto hx = std::get<0>(hidden);\n+    auto cx = std::get<1>(hidden);\n+\n+    if (input.is_cuda()) {\n+      auto igates = at::matmul(input, params.w_ih.t());\n+      auto hgates = at::matmul(hx, params.w_hh.t());\n+      auto result = at::_thnn_fused_lstm_cell(igates, hgates, cx, params.b_ih, params.b_hh);\n+      // Slice off the workspace argument (it's needed only for AD).\n+      return std::make_tuple(std::get<0>(result), std::get<1>(result));\n+    }\n+\n+    auto gates = linear(input, params.w_ih, params.b_ih) + linear(hx, params.w_hh, params.b_hh);\n+    auto chunked_gates = gates.chunk(4, 1);\n+\n+    auto ingate = chunked_gates[0].sigmoid();\n+    auto forgetgate = chunked_gates[1].sigmoid();\n+    auto cellgate = chunked_gates[2].tanh();\n+    auto outgate = chunked_gates[3].sigmoid();\n+\n+    auto cy = (forgetgate * cx) + (ingate * cellgate);\n+    auto hy = outgate * cy.tanh();\n+\n+    return std::make_tuple(hy, cy);\n+  }\n+};\n+\n+struct GRUCell : Cell<Tensor> {\n+  hidden_type operator()(const Tensor& input, const hidden_type& hidden, const CellParams& params) const override {\n+    if (input.is_cuda()) {\n+      auto igates = at::matmul(input, params.w_ih.t());\n+      auto hgates = at::matmul(hidden, params.w_hh.t());\n+      auto result = at::_thnn_fused_gru_cell(igates, hgates, hidden, params.b_ih, params.b_hh);\n+      // Slice off the workspace argument (it's needed only for AD).\n+      return std::get<0>(result);\n+    }\n+\n+    auto igates = linear(input, params.w_ih, params.b_ih);\n+    auto hgates = linear(hidden, params.w_hh, params.b_hh);\n+    auto chunked_igates = igates.chunk(3, 1);\n+    auto chunked_hgates = hgates.chunk(3, 1);\n+\n+    auto reset_gate = at::sigmoid(chunked_igates[0] + chunked_hgates[0]);\n+    auto input_gate = at::sigmoid(chunked_igates[1] + chunked_hgates[1]);\n+    auto new_gate = at::tanh(chunked_igates[2] + reset_gate * chunked_hgates[2]);\n+\n+    return new_gate + input_gate * (hidden - new_gate);\n+  }\n+};\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// LAYER IMPLEMENTATIONS\n+//\n+// Layers are scan-like higher-order functions, which take in cells, and\n+// transform them to fuctions of signature\n+//\n+// (io_type input, hidden_type hidden, param_type params) -> (io_type, hidden_type)\n+//\n+// which can apply the cell over a sequence of inputs, and produce both a new set\n+// of hidden states, as well as a concatenated output of each step.\n+\n+template<typename output_type, typename hidden_type>\n+struct LayerOutput {\n+  output_type outputs;\n+  hidden_type final_hidden;\n+};\n+\n+template<typename io_type, typename hidden_type, typename param_type>\n+struct Layer {\n+  using output_type = LayerOutput<io_type, hidden_type>;\n+  virtual output_type operator()(const io_type& input, const hidden_type& input_hidden, const param_type& params) const = 0;\n+};\n+\n+template<typename hidden_type>\n+struct FullLayer : Layer<Tensor, hidden_type, CellParams> {\n+  using output_type = typename Layer<Tensor, hidden_type, CellParams>::output_type;\n+  using unstacked_output_type = LayerOutput<std::vector<Tensor>, hidden_type>;\n+\n+  FullLayer(Cell<hidden_type>& cell)\n+    : cell_(cell) {};\n+\n+  unstacked_output_type operator()(std::vector<Tensor> step_inputs, const hidden_type& input_hidden, const CellParams& params) const {\n+    std::vector<Tensor> step_outputs;\n+    auto hidden = input_hidden;\n+    for (size_t i = 0; i < step_inputs.size(); i++) {\n+      hidden = cell_(step_inputs[i], hidden, params);\n+      step_outputs.push_back(hidden_as_output(hidden));\n+    }\n+    return {step_outputs, hidden};\n+  }\n+\n+  output_type operator()(const Tensor& inputs, const hidden_type& input_hidden, const CellParams& params) const override {\n+    auto unstacked_output = (*this)(inputs.unbind(0), input_hidden, params);\n+    return {at::stack(unstacked_output.outputs, 0), unstacked_output.final_hidden};\n+  }\n+\n+  Cell<hidden_type>& cell_;\n+};\n+\n+template<typename dir_hidden_type>\n+struct FullBidirectionalLayer : Layer<Tensor, pair_of<dir_hidden_type>, pair_of<CellParams>> {\n+  using hidden_type = pair_of<dir_hidden_type>;\n+  using param_type = pair_of<CellParams>;\n+  using output_type = typename Layer<Tensor, hidden_type, param_type>::output_type;\n+\n+  FullBidirectionalLayer(Cell<dir_hidden_type>& cell)\n+    : layer_(cell) {};\n+\n+  output_type operator()(const Tensor& input, const hidden_type& input_hidden, const param_type& params) const override {\n+    auto step_inputs = input.unbind(0);\n+    auto fw_result = layer_(step_inputs, input_hidden.first, params.first);\n+    auto fw_output = at::stack(fw_result.outputs, 0);\n+\n+    auto rev_step_inputs = reverse(std::move(step_inputs));\n+    auto rev_result = layer_(rev_step_inputs, input_hidden.second, params.second);\n+    std::reverse(rev_result.outputs.begin(), rev_result.outputs.end());\n+    auto rev_output = at::stack(rev_result.outputs, 0);\n+\n+    return {at::cat({fw_output, rev_output}, fw_output.dim() - 1),\n+            std::make_pair(fw_result.final_hidden, rev_result.final_hidden)};\n+  }\n+\n+  std::vector<Tensor> reverse(std::vector<Tensor>&& x) const {\n+    std::reverse(x.begin(), x.end());\n+    return x;\n+  }\n+\n+  FullLayer<dir_hidden_type> layer_;\n+};\n+\n+template<typename hidden_type>\n+struct PackedLayer : Layer<PackedSequence, hidden_type, CellParams> {\n+  using output_type = typename Layer<PackedSequence, hidden_type, CellParams>::output_type;\n+\n+  PackedLayer(Cell<hidden_type>& cell)\n+    : cell_(cell) {};\n+\n+  output_type operator()(const PackedSequence& input, const hidden_type& input_hidden, const CellParams& params) const override {\n+    std::vector<at::Tensor> step_outputs;\n+    std::vector<hidden_type> hiddens;\n+    int64_t input_offset = 0;\n+    int64_t num_steps = input.batch_sizes.size(0);\n+    int64_t* batch_sizes = input.batch_sizes.data<int64_t>();\n+    int64_t last_batch_size = batch_sizes[0];\n+\n+    // TODO: use split with sizes?\n+    auto hidden = input_hidden;\n+    for (int64_t i = 0; i < num_steps; ++i) {\n+      int64_t batch_size = batch_sizes[i];\n+      auto step_input = input.data.narrow(0, input_offset, batch_size);\n+      input_offset += batch_size;\n+\n+      int64_t dec = last_batch_size - batch_size;\n+      if (dec > 0) {\n+        hiddens.push_back(hidden_slice(hidden, last_batch_size - dec, last_batch_size));\n+        hidden = hidden_slice(hidden, 0, last_batch_size - dec);\n+      }\n+\n+      last_batch_size = batch_size;\n+      hidden = cell_(step_input, hidden, params);\n+      step_outputs.push_back(hidden_as_output(hidden));\n+    }\n+    hiddens.push_back(hidden);\n+    std::reverse(hiddens.begin(), hiddens.end());\n+\n+    return { PackedSequence{ at::cat(step_outputs, 0), input.batch_sizes }, hidden_concat(hiddens) };\n+  }\n+\n+  Cell<hidden_type>& cell_;\n+};\n+\n+template<typename hidden_type>\n+struct ReversedPackedLayer : Layer<PackedSequence, hidden_type, CellParams> {\n+  using output_type = typename Layer<PackedSequence, hidden_type, CellParams>::output_type;\n+\n+  ReversedPackedLayer(Cell<hidden_type>& cell)\n+    : cell_(cell) {};\n+\n+  output_type operator()(const PackedSequence& input, const hidden_type& input_hidden, const CellParams& params) const override {\n+    std::vector<at::Tensor> step_outputs;\n+    int64_t input_offset = input.data.size(0);\n+    int64_t num_steps = input.batch_sizes.size(0);\n+    int64_t* batch_sizes = input.batch_sizes.data<int64_t>();\n+    int64_t last_batch_size = batch_sizes[num_steps - 1];\n+\n+    auto hidden = hidden_slice(input_hidden, 0, batch_sizes[num_steps - 1]);\n+    for (int64_t i = num_steps - 1; i >= 0; --i) {\n+      int64_t batch_size = batch_sizes[i];\n+      int64_t inc = batch_size - last_batch_size;\n+      if (inc > 0) {\n+        hidden = hidden_concat(ArrayRef<hidden_type>{hidden, hidden_slice(input_hidden, last_batch_size, batch_size)});\n+      }\n+\n+      auto step_input = input.data.narrow(0, input_offset - batch_size, batch_size);\n+      input_offset -= batch_size;\n+\n+      last_batch_size = batch_size;\n+      hidden = cell_(step_input, hidden, params);\n+      step_outputs.push_back(hidden_as_output(hidden));\n+    }\n+    std::reverse(step_outputs.begin(), step_outputs.end());\n+    return { PackedSequence{ at::cat(step_outputs, 0), input.batch_sizes }, hidden };\n+  }\n+\n+  Cell<hidden_type>& cell_;\n+};\n+\n+template<typename dir_hidden_type>\n+struct PackedBidirectionalLayer : Layer<PackedSequence, pair_of<dir_hidden_type>, pair_of<CellParams>> {\n+  using hidden_type = pair_of<dir_hidden_type>;\n+  using param_type = pair_of<CellParams>;\n+  using output_type = typename Layer<PackedSequence, hidden_type, param_type>::output_type;\n+\n+  PackedBidirectionalLayer(Cell<dir_hidden_type>& cell)\n+    : layer_(cell), rev_layer_(cell) {};\n+\n+  output_type operator()(const PackedSequence& input, const hidden_type& input_hidden, const param_type& params) const override {\n+    auto fw_result = layer_(input, input_hidden.first, params.first);\n+    auto rev_result = rev_layer_(input, input_hidden.second, params.second);\n+    PackedSequence output { at::cat({fw_result.outputs.data, rev_result.outputs.data}, -1), input.batch_sizes };\n+    return { output, std::make_pair(fw_result.final_hidden, rev_result.final_hidden) };\n+  }\n+\n+  PackedLayer<dir_hidden_type> layer_;\n+  ReversedPackedLayer<dir_hidden_type> rev_layer_;\n+};\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// apply_layer_stack\n+//\n+// layers are convenient, but in reality we often want to stack them. this little\n+// helper manages slicing of all inputs and parameters, and repeatedly feeds them\n+// into the given layer. returns the last layer's outputs, and a vector of final\n+// hidden states produced at each level.\n+\n+Tensor dropout(const Tensor& input, double p) {\n+  return at::dropout(input, p, /*train=*/true);", "path": "aten/src/ATen/native/RNN.cpp", "position": 410, "original_position": 399, "commit_id": "bb5fc94a73109344648bd957b2b82664b1cf9d64", "original_commit_id": "6e28112ad6c358c1240e170485f9a4913aa29607", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Oh I see, you don't bother calling dropout if it is not `train`", "created_at": "2018-08-14T21:27:24Z", "updated_at": "2018-11-23T15:49:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/10481#discussion_r210109590", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10481", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/210109590"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10481#discussion_r210109590"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10481"}}, "body_html": "<p>Oh I see, you don't bother calling dropout if it is not <code>train</code></p>", "body_text": "Oh I see, you don't bother calling dropout if it is not train", "in_reply_to_id": 210109248}