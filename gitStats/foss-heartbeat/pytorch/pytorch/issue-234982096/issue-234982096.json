{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1767", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1767/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1767/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1767/events", "html_url": "https://github.com/pytorch/pytorch/issues/1767", "id": 234982096, "node_id": "MDU6SXNzdWUyMzQ5ODIwOTY=", "number": 1767, "title": "numerical instability for Adam and Adadelta optimizer", "user": {"login": "xuancong84", "id": 10172392, "node_id": "MDQ6VXNlcjEwMTcyMzky", "avatar_url": "https://avatars0.githubusercontent.com/u/10172392?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuancong84", "html_url": "https://github.com/xuancong84", "followers_url": "https://api.github.com/users/xuancong84/followers", "following_url": "https://api.github.com/users/xuancong84/following{/other_user}", "gists_url": "https://api.github.com/users/xuancong84/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuancong84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuancong84/subscriptions", "organizations_url": "https://api.github.com/users/xuancong84/orgs", "repos_url": "https://api.github.com/users/xuancong84/repos", "events_url": "https://api.github.com/users/xuancong84/events{/privacy}", "received_events_url": "https://api.github.com/users/xuancong84/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-06-10T05:37:36Z", "updated_at": "2018-05-20T07:09:58Z", "closed_at": "2017-07-15T15:25:49Z", "author_association": "NONE", "body_html": "<p>For Adam and Adadelta optimizer, when the model is close to convergence, the accuracy often suddenly drops to 0 with perplexity going to NAN, as shown below:</p>\n<p>Epoch  3, 251750/348124; acc:  70.47; ppl:    3.77;  3911 tok/s; lr: 0.0010000; 717152.5 s elapsed<br>\nEpoch  3, 251800/348124; acc:  71.91; ppl:    3.53;  3796 tok/s; lr: 0.0010000; 717190.5 s elapsed<br>\nEpoch  3, 251850/348124; acc:  71.03; ppl:    3.58;  3752 tok/s; lr: 0.0010000; 717227.2 s elapsed<br>\nEpoch  3, 251900/348124; acc:  69.85; ppl:    3.86;  3830 tok/s; lr: 0.0010000; 717266.6 s elapsed<br>\nEpoch  3, 251950/348124; acc:  70.55; ppl:    3.73;  3930 tok/s; lr: 0.0010000; 717302.3 s elapsed<br>\nEpoch  3, 252000/348124; acc:  69.78; ppl:    4.03;  3912 tok/s; lr: 0.0010000; 717340.9 s elapsed<br>\nEpoch  3, 252050/348124; acc:  69.01; ppl:    4.18;  2699 tok/s; lr: 0.0010000; 717392.5 s elapsed<br>\nEpoch  3, 252100/348124; acc:  70.09; ppl:    3.90;  3935 tok/s; lr: 0.0010000; 717429.4 s elapsed<br>\nEpoch  3, 252150/348124; acc:  69.48; ppl:    4.18;  3758 tok/s; lr: 0.0010000; 717463.5 s elapsed<br>\nEpoch  3, 252200/348124; acc:  26.95; ppl:     nan;  3753 tok/s; lr: 0.0010000; 717506.3 s elapsed<br>\nEpoch  3, 252250/348124; acc:   0.00; ppl:     nan;  3925 tok/s; lr: 0.0010000; 717546.5 s elapsed<br>\nEpoch  3, 252300/348124; acc:   0.00; ppl:     nan;  3822 tok/s; lr: 0.0010000; 717584.6 s elapsed<br>\nEpoch  3, 252350/348124; acc:   0.00; ppl:     nan;  3813 tok/s; lr: 0.0010000; 717622.8 s elapsed<br>\nEpoch  3, 252400/348124; acc:   0.00; ppl:     nan;  3677 tok/s; lr: 0.0010000; 717661.0 s elapsed<br>\nEpoch  3, 252450/348124; acc:   0.00; ppl:     nan;  3999 tok/s; lr: 0.0010000; 717699.2 s elapsed<br>\nEpoch  3, 252500/348124; acc:   0.00; ppl:     nan;  3939 tok/s; lr: 0.0010000; 717738.1 s elapsed<br>\nEpoch  3, 252550/348124; acc:   0.00; ppl:     nan;  3872 tok/s; lr: 0.0010000; 717771.3 s elapsed</p>\n<p>The code I have run is OpenNMT-py on a large dataset with 16M parallel sentences (Unite Nation Parallel Corpus v1.0), this phenomenon is observed on Adam and Adadelta which involves division, so far not seen on SGD. I suggest developers to check for divide by zero in Adam and Adadelta optimizers, and probably others.</p>", "body_text": "For Adam and Adadelta optimizer, when the model is close to convergence, the accuracy often suddenly drops to 0 with perplexity going to NAN, as shown below:\nEpoch  3, 251750/348124; acc:  70.47; ppl:    3.77;  3911 tok/s; lr: 0.0010000; 717152.5 s elapsed\nEpoch  3, 251800/348124; acc:  71.91; ppl:    3.53;  3796 tok/s; lr: 0.0010000; 717190.5 s elapsed\nEpoch  3, 251850/348124; acc:  71.03; ppl:    3.58;  3752 tok/s; lr: 0.0010000; 717227.2 s elapsed\nEpoch  3, 251900/348124; acc:  69.85; ppl:    3.86;  3830 tok/s; lr: 0.0010000; 717266.6 s elapsed\nEpoch  3, 251950/348124; acc:  70.55; ppl:    3.73;  3930 tok/s; lr: 0.0010000; 717302.3 s elapsed\nEpoch  3, 252000/348124; acc:  69.78; ppl:    4.03;  3912 tok/s; lr: 0.0010000; 717340.9 s elapsed\nEpoch  3, 252050/348124; acc:  69.01; ppl:    4.18;  2699 tok/s; lr: 0.0010000; 717392.5 s elapsed\nEpoch  3, 252100/348124; acc:  70.09; ppl:    3.90;  3935 tok/s; lr: 0.0010000; 717429.4 s elapsed\nEpoch  3, 252150/348124; acc:  69.48; ppl:    4.18;  3758 tok/s; lr: 0.0010000; 717463.5 s elapsed\nEpoch  3, 252200/348124; acc:  26.95; ppl:     nan;  3753 tok/s; lr: 0.0010000; 717506.3 s elapsed\nEpoch  3, 252250/348124; acc:   0.00; ppl:     nan;  3925 tok/s; lr: 0.0010000; 717546.5 s elapsed\nEpoch  3, 252300/348124; acc:   0.00; ppl:     nan;  3822 tok/s; lr: 0.0010000; 717584.6 s elapsed\nEpoch  3, 252350/348124; acc:   0.00; ppl:     nan;  3813 tok/s; lr: 0.0010000; 717622.8 s elapsed\nEpoch  3, 252400/348124; acc:   0.00; ppl:     nan;  3677 tok/s; lr: 0.0010000; 717661.0 s elapsed\nEpoch  3, 252450/348124; acc:   0.00; ppl:     nan;  3999 tok/s; lr: 0.0010000; 717699.2 s elapsed\nEpoch  3, 252500/348124; acc:   0.00; ppl:     nan;  3939 tok/s; lr: 0.0010000; 717738.1 s elapsed\nEpoch  3, 252550/348124; acc:   0.00; ppl:     nan;  3872 tok/s; lr: 0.0010000; 717771.3 s elapsed\nThe code I have run is OpenNMT-py on a large dataset with 16M parallel sentences (Unite Nation Parallel Corpus v1.0), this phenomenon is observed on Adam and Adadelta which involves division, so far not seen on SGD. I suggest developers to check for divide by zero in Adam and Adadelta optimizers, and probably others.", "body": "For Adam and Adadelta optimizer, when the model is close to convergence, the accuracy often suddenly drops to 0 with perplexity going to NAN, as shown below:\r\n\r\nEpoch  3, 251750/348124; acc:  70.47; ppl:    3.77;  3911 tok/s; lr: 0.0010000; 717152.5 s elapsed\r\nEpoch  3, 251800/348124; acc:  71.91; ppl:    3.53;  3796 tok/s; lr: 0.0010000; 717190.5 s elapsed\r\nEpoch  3, 251850/348124; acc:  71.03; ppl:    3.58;  3752 tok/s; lr: 0.0010000; 717227.2 s elapsed\r\nEpoch  3, 251900/348124; acc:  69.85; ppl:    3.86;  3830 tok/s; lr: 0.0010000; 717266.6 s elapsed\r\nEpoch  3, 251950/348124; acc:  70.55; ppl:    3.73;  3930 tok/s; lr: 0.0010000; 717302.3 s elapsed\r\nEpoch  3, 252000/348124; acc:  69.78; ppl:    4.03;  3912 tok/s; lr: 0.0010000; 717340.9 s elapsed\r\nEpoch  3, 252050/348124; acc:  69.01; ppl:    4.18;  2699 tok/s; lr: 0.0010000; 717392.5 s elapsed\r\nEpoch  3, 252100/348124; acc:  70.09; ppl:    3.90;  3935 tok/s; lr: 0.0010000; 717429.4 s elapsed\r\nEpoch  3, 252150/348124; acc:  69.48; ppl:    4.18;  3758 tok/s; lr: 0.0010000; 717463.5 s elapsed\r\nEpoch  3, 252200/348124; acc:  26.95; ppl:     nan;  3753 tok/s; lr: 0.0010000; 717506.3 s elapsed\r\nEpoch  3, 252250/348124; acc:   0.00; ppl:     nan;  3925 tok/s; lr: 0.0010000; 717546.5 s elapsed\r\nEpoch  3, 252300/348124; acc:   0.00; ppl:     nan;  3822 tok/s; lr: 0.0010000; 717584.6 s elapsed\r\nEpoch  3, 252350/348124; acc:   0.00; ppl:     nan;  3813 tok/s; lr: 0.0010000; 717622.8 s elapsed\r\nEpoch  3, 252400/348124; acc:   0.00; ppl:     nan;  3677 tok/s; lr: 0.0010000; 717661.0 s elapsed\r\nEpoch  3, 252450/348124; acc:   0.00; ppl:     nan;  3999 tok/s; lr: 0.0010000; 717699.2 s elapsed\r\nEpoch  3, 252500/348124; acc:   0.00; ppl:     nan;  3939 tok/s; lr: 0.0010000; 717738.1 s elapsed\r\nEpoch  3, 252550/348124; acc:   0.00; ppl:     nan;  3872 tok/s; lr: 0.0010000; 717771.3 s elapsed\r\n\r\nThe code I have run is OpenNMT-py on a large dataset with 16M parallel sentences (Unite Nation Parallel Corpus v1.0), this phenomenon is observed on Adam and Adadelta which involves division, so far not seen on SGD. I suggest developers to check for divide by zero in Adam and Adadelta optimizers, and probably others."}