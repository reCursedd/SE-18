{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/327038479", "html_url": "https://github.com/pytorch/pytorch/issues/2601#issuecomment-327038479", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2601", "id": 327038479, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzAzODQ3OQ==", "user": {"login": "yuandong-tian", "id": 2973937, "node_id": "MDQ6VXNlcjI5NzM5Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2973937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuandong-tian", "html_url": "https://github.com/yuandong-tian", "followers_url": "https://api.github.com/users/yuandong-tian/followers", "following_url": "https://api.github.com/users/yuandong-tian/following{/other_user}", "gists_url": "https://api.github.com/users/yuandong-tian/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuandong-tian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuandong-tian/subscriptions", "organizations_url": "https://api.github.com/users/yuandong-tian/orgs", "repos_url": "https://api.github.com/users/yuandong-tian/repos", "events_url": "https://api.github.com/users/yuandong-tian/events{/privacy}", "received_events_url": "https://api.github.com/users/yuandong-tian/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-04T23:23:05Z", "updated_at": "2017-09-04T23:24:12Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Then how to clone a Variable as if it is recomputed by its precedents again, but does not involve extra computation? I still want the gradients back-propagated. For example:</p>\n<pre><code>c = a + b\nd = a + b\ne = 2 * c + d * d\ne.backward()\n</code></pre>\n<p>will give two Variables <code>c</code> and <code>d</code>. I can add a hook on <code>c</code> to change its gradient, but not on <code>d</code>.  I wonder is there any way to avoid computing <code>a + b</code> twice?</p>", "body_text": "@apaszke Then how to clone a Variable as if it is recomputed by its precedents again, but does not involve extra computation? I still want the gradients back-propagated. For example:\nc = a + b\nd = a + b\ne = 2 * c + d * d\ne.backward()\n\nwill give two Variables c and d. I can add a hook on c to change its gradient, but not on d.  I wonder is there any way to avoid computing a + b twice?", "body": "@apaszke Then how to clone a Variable as if it is recomputed by its precedents again, but does not involve extra computation? I still want the gradients back-propagated. For example:\r\n\r\n```\r\nc = a + b\r\nd = a + b\r\ne = 2 * c + d * d\r\ne.backward()\r\n```\r\nwill give two Variables `c` and `d`. I can add a hook on `c` to change its gradient, but not on `d`.  I wonder is there any way to avoid computing `a + b` twice?"}