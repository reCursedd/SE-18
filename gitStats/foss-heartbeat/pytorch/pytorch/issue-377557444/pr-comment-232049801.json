{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232049801", "pull_request_review_id": 173157893, "id": 232049801, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjA0OTgwMQ==", "diff_hunk": "@@ -0,0 +1,36 @@\n+#pragma once\n+\n+#include <c10/util/Exception.h>\n+#include <torch/data/datasets/base.h>\n+#include <torch/data/example.h>\n+\n+namespace torch {\n+namespace data {\n+namespace datasets {\n+/// A dataset that supports loading an entire chunk of data.\n+///\n+/// A chunk could be an entire file, such as an audio data file or an image,\n+/// or part of a file in the case of a large text file split based on seek\n+/// positions. ChunkDataSet extends the DataSet functionality to read an\n+/// entire chunk at once.\n+template <\n+    typename Self,\n+    typename Batch = std::vector<Example<>>,\n+    typename BatchRequest = ArrayRef<size_t>>\n+class ChunkDataSet : public BatchDataset<Self, Batch, BatchRequest> {", "path": "torch/csrc/api/include/torch/data/datasets/chunk.h", "position": 20, "original_position": 20, "commit_id": "41208e442f929e8883fd0359e14f0d98466e0c85", "original_commit_id": "880aeebda0f9be2118eb95f99e50d9d620e7b43b", "user": {"login": "jaliyae", "id": 12703337, "node_id": "MDQ6VXNlcjEyNzAzMzM3", "avatar_url": "https://avatars3.githubusercontent.com/u/12703337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jaliyae", "html_url": "https://github.com/jaliyae", "followers_url": "https://api.github.com/users/jaliyae/followers", "following_url": "https://api.github.com/users/jaliyae/following{/other_user}", "gists_url": "https://api.github.com/users/jaliyae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jaliyae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jaliyae/subscriptions", "organizations_url": "https://api.github.com/users/jaliyae/orgs", "repos_url": "https://api.github.com/users/jaliyae/repos", "events_url": "https://api.github.com/users/jaliyae/events{/privacy}", "received_events_url": "https://api.github.com/users/jaliyae/received_events", "type": "User", "site_admin": false}, "body": "I think the confusion is mainly because you are assuming chunk==batch. Typically, a chunk is a much larger amount of data or Examples. Say 1000 Examples and a batch is set to 100. We want to pre-load an entire chunk when we read data from the underneath storage, but then divide it to 10 batches for training. \r\n\r\nSo now, as you have mentioned above, we can use the BatchDataSet to load the chunk, but we cannot return the whole chunk to the user or apply collate() or any other map() operation on the whole chunk. We still need to do those at the individual small batch level. This means we need two APIs, one to load the whole chunk and one to load the batch. \r\n\r\nThe main reason for having two APIs is to support the map() operation. To use map()/collate() on a dataset, the data must come out from the get_batch() API, which is run on the dataloader threads. We want preloading of chunks(bigger blocks) and let user only worry about that, and hence the second API. Hope this clarifies. \r\n", "created_at": "2018-11-08T20:29:10Z", "updated_at": "2018-11-23T15:54:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232049801", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13585", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232049801"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232049801"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13585"}}, "body_html": "<p>I think the confusion is mainly because you are assuming chunk==batch. Typically, a chunk is a much larger amount of data or Examples. Say 1000 Examples and a batch is set to 100. We want to pre-load an entire chunk when we read data from the underneath storage, but then divide it to 10 batches for training.</p>\n<p>So now, as you have mentioned above, we can use the BatchDataSet to load the chunk, but we cannot return the whole chunk to the user or apply collate() or any other map() operation on the whole chunk. We still need to do those at the individual small batch level. This means we need two APIs, one to load the whole chunk and one to load the batch.</p>\n<p>The main reason for having two APIs is to support the map() operation. To use map()/collate() on a dataset, the data must come out from the get_batch() API, which is run on the dataloader threads. We want preloading of chunks(bigger blocks) and let user only worry about that, and hence the second API. Hope this clarifies.</p>", "body_text": "I think the confusion is mainly because you are assuming chunk==batch. Typically, a chunk is a much larger amount of data or Examples. Say 1000 Examples and a batch is set to 100. We want to pre-load an entire chunk when we read data from the underneath storage, but then divide it to 10 batches for training.\nSo now, as you have mentioned above, we can use the BatchDataSet to load the chunk, but we cannot return the whole chunk to the user or apply collate() or any other map() operation on the whole chunk. We still need to do those at the individual small batch level. This means we need two APIs, one to load the whole chunk and one to load the batch.\nThe main reason for having two APIs is to support the map() operation. To use map()/collate() on a dataset, the data must come out from the get_batch() API, which is run on the dataloader threads. We want preloading of chunks(bigger blocks) and let user only worry about that, and hence the second API. Hope this clarifies.", "in_reply_to_id": 232002171}