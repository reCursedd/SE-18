{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232340331", "pull_request_review_id": 173523441, "id": 232340331, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjM0MDMzMQ==", "diff_hunk": "@@ -0,0 +1,36 @@\n+#pragma once\n+\n+#include <c10/util/Exception.h>\n+#include <torch/data/datasets/base.h>\n+#include <torch/data/example.h>\n+\n+namespace torch {\n+namespace data {\n+namespace datasets {\n+/// A dataset that supports loading an entire chunk of data.\n+///\n+/// A chunk could be an entire file, such as an audio data file or an image,\n+/// or part of a file in the case of a large text file split based on seek\n+/// positions. ChunkDataSet extends the DataSet functionality to read an\n+/// entire chunk at once.\n+template <\n+    typename Self,\n+    typename Batch = std::vector<Example<>>,\n+    typename BatchRequest = ArrayRef<size_t>>\n+class ChunkDataSet : public BatchDataset<Self, Batch, BatchRequest> {", "path": "torch/csrc/api/include/torch/data/datasets/chunk.h", "position": 20, "original_position": 20, "commit_id": "41208e442f929e8883fd0359e14f0d98466e0c85", "original_commit_id": "880aeebda0f9be2118eb95f99e50d9d620e7b43b", "user": {"login": "jaliyae", "id": 12703337, "node_id": "MDQ6VXNlcjEyNzAzMzM3", "avatar_url": "https://avatars3.githubusercontent.com/u/12703337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jaliyae", "html_url": "https://github.com/jaliyae", "followers_url": "https://api.github.com/users/jaliyae/followers", "following_url": "https://api.github.com/users/jaliyae/following{/other_user}", "gists_url": "https://api.github.com/users/jaliyae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jaliyae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jaliyae/subscriptions", "organizations_url": "https://api.github.com/users/jaliyae/orgs", "repos_url": "https://api.github.com/users/jaliyae/repos", "events_url": "https://api.github.com/users/jaliyae/events{/privacy}", "received_events_url": "https://api.github.com/users/jaliyae/received_events", "type": "User", "site_admin": false}, "body": "@apaszke This is exactly what we try to achieve and hence the minimum change to the dataloader. The two options we discussed was whether to change the dataloader or dataset and decided to move the preloading to the dataset and keep the dataloader as close to what it is today. To get this with the existing APIs, we need the changes in the PR. For example, Dataset has a single key API, get_batch() we will implement all the preloading under this API, but then we would like user's to implement the data parsing in read_chunk() API. We don't want the dataset to be copied to threads or someone specify a size to a sampler because those should come after reading data. This PR is will get us the flexibility to do those using the existing dataloader. Please let me know if you have specific questions.", "created_at": "2018-11-09T17:57:51Z", "updated_at": "2018-11-23T15:54:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232340331", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13585", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232340331"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232340331"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13585"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> This is exactly what we try to achieve and hence the minimum change to the dataloader. The two options we discussed was whether to change the dataloader or dataset and decided to move the preloading to the dataset and keep the dataloader as close to what it is today. To get this with the existing APIs, we need the changes in the PR. For example, Dataset has a single key API, get_batch() we will implement all the preloading under this API, but then we would like user's to implement the data parsing in read_chunk() API. We don't want the dataset to be copied to threads or someone specify a size to a sampler because those should come after reading data. This PR is will get us the flexibility to do those using the existing dataloader. Please let me know if you have specific questions.</p>", "body_text": "@apaszke This is exactly what we try to achieve and hence the minimum change to the dataloader. The two options we discussed was whether to change the dataloader or dataset and decided to move the preloading to the dataset and keep the dataloader as close to what it is today. To get this with the existing APIs, we need the changes in the PR. For example, Dataset has a single key API, get_batch() we will implement all the preloading under this API, but then we would like user's to implement the data parsing in read_chunk() API. We don't want the dataset to be copied to threads or someone specify a size to a sampler because those should come after reading data. This PR is will get us the flexibility to do those using the existing dataloader. Please let me know if you have specific questions.", "in_reply_to_id": 232002171}