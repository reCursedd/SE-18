{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232432575", "pull_request_review_id": 173639857, "id": 232432575, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjQzMjU3NQ==", "diff_hunk": "@@ -0,0 +1,36 @@\n+#pragma once\n+\n+#include <c10/util/Exception.h>\n+#include <torch/data/datasets/base.h>\n+#include <torch/data/example.h>\n+\n+namespace torch {\n+namespace data {\n+namespace datasets {\n+/// A dataset that supports loading an entire chunk of data.\n+///\n+/// A chunk could be an entire file, such as an audio data file or an image,\n+/// or part of a file in the case of a large text file split based on seek\n+/// positions. ChunkDataSet extends the DataSet functionality to read an\n+/// entire chunk at once.\n+template <\n+    typename Self,\n+    typename Batch = std::vector<Example<>>,\n+    typename BatchRequest = ArrayRef<size_t>>\n+class ChunkDataSet : public BatchDataset<Self, Batch, BatchRequest> {", "path": "torch/csrc/api/include/torch/data/datasets/chunk.h", "position": 20, "original_position": 20, "commit_id": "41208e442f929e8883fd0359e14f0d98466e0c85", "original_commit_id": "880aeebda0f9be2118eb95f99e50d9d620e7b43b", "user": {"login": "jaliyae", "id": 12703337, "node_id": "MDQ6VXNlcjEyNzAzMzM3", "avatar_url": "https://avatars3.githubusercontent.com/u/12703337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jaliyae", "html_url": "https://github.com/jaliyae", "followers_url": "https://api.github.com/users/jaliyae/followers", "following_url": "https://api.github.com/users/jaliyae/following{/other_user}", "gists_url": "https://api.github.com/users/jaliyae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jaliyae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jaliyae/subscriptions", "organizations_url": "https://api.github.com/users/jaliyae/orgs", "repos_url": "https://api.github.com/users/jaliyae/repos", "events_url": "https://api.github.com/users/jaliyae/events{/privacy}", "received_events_url": "https://api.github.com/users/jaliyae/received_events", "type": "User", "site_admin": false}, "body": "We had an offline discussion with Peter and discussed our proposal in detail. We ended up with two main tasks, (I) for the dataloader constructor overload, Peter is trying to see if we can move the dataset without copying via a wrapper.  (ii) Peter also pointed us that the current BatchSizeSampler will not stop the dataloder even after the dataset is exhausted, so we are looking into a fix for that. Once we have those, we will provide an update to the PR.", "created_at": "2018-11-10T00:53:35Z", "updated_at": "2018-11-23T15:54:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232432575", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13585", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232432575"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232432575"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13585"}}, "body_html": "<p>We had an offline discussion with Peter and discussed our proposal in detail. We ended up with two main tasks, (I) for the dataloader constructor overload, Peter is trying to see if we can move the dataset without copying via a wrapper.  (ii) Peter also pointed us that the current BatchSizeSampler will not stop the dataloder even after the dataset is exhausted, so we are looking into a fix for that. Once we have those, we will provide an update to the PR.</p>", "body_text": "We had an offline discussion with Peter and discussed our proposal in detail. We ended up with two main tasks, (I) for the dataloader constructor overload, Peter is trying to see if we can move the dataset without copying via a wrapper.  (ii) Peter also pointed us that the current BatchSizeSampler will not stop the dataloder even after the dataset is exhausted, so we are looking into a fix for that. Once we have those, we will provide an update to the PR.", "in_reply_to_id": 232002171}