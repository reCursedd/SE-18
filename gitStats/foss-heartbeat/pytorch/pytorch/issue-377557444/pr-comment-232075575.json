{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232075575", "pull_request_review_id": 173191171, "id": 232075575, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjA3NTU3NQ==", "diff_hunk": "@@ -49,6 +50,37 @@ class DataLoader {\n     }\n   }\n \n+  /// Constructs a new `DataLoader` from a `dataset`, `options`, and `Sampler`.", "path": "torch/csrc/api/include/torch/data/dataloader.h", "position": 21, "original_position": 21, "commit_id": "41208e442f929e8883fd0359e14f0d98466e0c85", "original_commit_id": "880aeebda0f9be2118eb95f99e50d9d620e7b43b", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "But in that case we don't need a dataset in the main thread anymore I think? You'd just have the workers reference the wrapper over the expensive dataset, and behind the scenes they would be accessing the shared dataset just like in your proposal. It would also work very nicely with transforms, since you would just apply the transforms on top of the wrapper dataset, such that they would be happening in the worker threads and not inside the shared dataset. What do you think? We could code up how that would look like, but I think it sounds doable", "created_at": "2018-11-08T21:56:12Z", "updated_at": "2018-11-23T15:54:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232075575", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13585", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232075575"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13585#discussion_r232075575"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13585"}}, "body_html": "<p>But in that case we don't need a dataset in the main thread anymore I think? You'd just have the workers reference the wrapper over the expensive dataset, and behind the scenes they would be accessing the shared dataset just like in your proposal. It would also work very nicely with transforms, since you would just apply the transforms on top of the wrapper dataset, such that they would be happening in the worker threads and not inside the shared dataset. What do you think? We could code up how that would look like, but I think it sounds doable</p>", "body_text": "But in that case we don't need a dataset in the main thread anymore I think? You'd just have the workers reference the wrapper over the expensive dataset, and behind the scenes they would be accessing the shared dataset just like in your proposal. It would also work very nicely with transforms, since you would just apply the transforms on top of the wrapper dataset, such that they would be happening in the worker threads and not inside the shared dataset. What do you think? We could code up how that would look like, but I think it sounds doable", "in_reply_to_id": 232003002}