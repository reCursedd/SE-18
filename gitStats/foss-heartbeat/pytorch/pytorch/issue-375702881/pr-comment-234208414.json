{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234208414", "pull_request_review_id": 175806824, "id": 234208414, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNDIwODQxNA==", "diff_hunk": "@@ -1003,72 +1003,53 @@ def softmax(input, dim=None, _stacklevel=3, dtype=None):\n         return input.softmax(dim, dtype=dtype)\n \n \n-def _sample_gumbel(shape, eps=1e-10, out=None):\n-    \"\"\"\n-    Sample from Gumbel(0, 1)\n-\n-    based on\n-    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n-    (MIT license)\n-    \"\"\"\n-    U = out.resize_(shape).uniform_() if out is not None else torch.rand(shape)\n-    return - torch.log(eps - torch.log(U + eps))\n-\n-\n-def _gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n-    \"\"\"\n-    Draw a sample from the Gumbel-Softmax distribution\n-\n-    based on\n-    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb\n-    (MIT license)\n-    \"\"\"\n-    dims = logits.dim()\n-    gumbel_noise = _sample_gumbel(logits.size(), eps=eps, out=logits.data.new())\n-    y = logits + gumbel_noise\n-    return softmax(y / tau, dims - 1)\n-\n-\n-def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):\n+def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n     r\"\"\"\n-    Sample from the Gumbel-Softmax distribution and optionally discretize.\n+    Samples from the `Gumbel-Softmax distribution`_ and optionally discretizes.\n \n     Args:\n-      logits: `[batch_size, num_features]` unnormalized log probabilities\n+      logits: `[..., num_features]` unnormalized log probabilities\n       tau: non-negative scalar temperature\n       hard: if ``True``, the returned samples will be discretized as one-hot vectors,\n             but will be differentiated as if it is the soft sample in autograd\n+      eps: Deprecated parameter to avoid log(0). Now handled elsewhere.\n+      dim (int): A dimension along which softmax will be computed.\n \n     Returns:\n-      Sampled tensor of shape ``batch_size x num_features`` from the Gumbel-Softmax distribution.\n+      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n       If ``hard=True``, the returned samples will be one-hot, otherwise they will\n-      be probability distributions that sum to 1 across features\n+      be probability distributions that sum to 1 across `dim`.\n \n-    Constraints:\n+    .. note::\n+      This function is here for legacy reasons, may be removed from nn.Functional in the future.\n \n-    - Currently only work on 2D input :attr:`logits` tensor of shape ``batch_size x num_features``\n+    Examples::\n+        >>> logits = torch.randn(20, 32)\n+        >>> # Sample soft categorical using reparametrization trick:\n+        >>> F.gumbel_softmax(logits, tau=1, hard=False)\n+        >>> # Sample hard categorical using \"Straight-through\" trick:\n+        >>> F.gumbel_softmax(logits, tau=1, hard=True)\n \n-    Based on\n-    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n-    (MIT license)\n+    .. _Gumbel-Softmax distribution:\n+        https://arxiv.org/abs/1611.00712\n+        https://arxiv.org/abs/1611.01144\n     \"\"\"\n-    shape = logits.size()\n-    assert len(shape) == 2\n-    y_soft = _gumbel_softmax_sample(logits, tau=tau, eps=eps)\n+\n+    if eps != 1e-10:\n+        warnings.warn(\"`eps` parameter is deprecated.\", DeprecationWarning)\n+\n+    y_soft = logits.new(logits.shape)\n+    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise\n+    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise\n+\n     if hard:\n-        _, k = y_soft.max(-1)\n-        # this bit is based on\n-        # https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5\n-        y_hard = logits.new_zeros(*shape).scatter_(-1, k.view(-1, 1), 1.0)\n-        # this cool bit of code achieves two things:\n-        # - makes the output value exactly one-hot (since we add then\n-        #   subtract y_soft value)\n-        # - makes the gradient equal to y_soft gradient (since we strip\n-        #   all other gradients)\n-        y = y_hard - y_soft.detach() + y_soft\n+        # Straight through.\n+        index = y_soft.max(dim, keepdim=True)[1]\n+        y_hard = logits.new_zeros(logits.shape).scatter_(dim, index, 1.0)\n+        return y_hard - y_soft.detach() + y_soft", "path": "torch/nn/functional.py", "position": 113, "original_position": 96, "commit_id": "87d85b5147da20e0fc72d7442b4b2e62b1b647aa", "original_commit_id": "96a29e1b9c41d9b0387e339af11e4b9f29e7192e", "user": {"login": "ragulpr", "id": 10998266, "node_id": "MDQ6VXNlcjEwOTk4MjY2", "avatar_url": "https://avatars3.githubusercontent.com/u/10998266?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ragulpr", "html_url": "https://github.com/ragulpr", "followers_url": "https://api.github.com/users/ragulpr/followers", "following_url": "https://api.github.com/users/ragulpr/following{/other_user}", "gists_url": "https://api.github.com/users/ragulpr/gists{/gist_id}", "starred_url": "https://api.github.com/users/ragulpr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ragulpr/subscriptions", "organizations_url": "https://api.github.com/users/ragulpr/orgs", "repos_url": "https://api.github.com/users/ragulpr/repos", "events_url": "https://api.github.com/users/ragulpr/events{/privacy}", "received_events_url": "https://api.github.com/users/ragulpr/received_events", "type": "User", "site_admin": false}, "body": "I remember first seeing that trick and my head immediately exploded in aw(e) \ud83d\ude04 I reasoned that users of gumbel softmax trick would already know. I added it to docstring with ea1b998", "created_at": "2018-11-16T13:48:16Z", "updated_at": "2018-11-23T15:55:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/13339#discussion_r234208414", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13339", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234208414"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13339#discussion_r234208414"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13339"}}, "body_html": "<p>I remember first seeing that trick and my head immediately exploded in aw(e) <g-emoji class=\"g-emoji\" alias=\"smile\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f604.png\">\ud83d\ude04</g-emoji> I reasoned that users of gumbel softmax trick would already know. I added it to docstring with <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/ea1b998d99890656f077f8017cc4c6ca59ecbf43/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/ea1b998d99890656f077f8017cc4c6ca59ecbf43\"><tt>ea1b998</tt></a></p>", "body_text": "I remember first seeing that trick and my head immediately exploded in aw(e) \ud83d\ude04 I reasoned that users of gumbel softmax trick would already know. I added it to docstring with ea1b998", "in_reply_to_id": 234054769}