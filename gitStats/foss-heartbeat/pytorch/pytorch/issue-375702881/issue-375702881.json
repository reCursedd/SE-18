{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13339", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13339/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13339/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13339/events", "html_url": "https://github.com/pytorch/pytorch/pull/13339", "id": 375702881, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI3MDg4OTY2", "number": 13339, "title": "Cleanup gumbel_softmax", "user": {"login": "ragulpr", "id": 10998266, "node_id": "MDQ6VXNlcjEwOTk4MjY2", "avatar_url": "https://avatars3.githubusercontent.com/u/10998266?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ragulpr", "html_url": "https://github.com/ragulpr", "followers_url": "https://api.github.com/users/ragulpr/followers", "following_url": "https://api.github.com/users/ragulpr/following{/other_user}", "gists_url": "https://api.github.com/users/ragulpr/gists{/gist_id}", "starred_url": "https://api.github.com/users/ragulpr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ragulpr/subscriptions", "organizations_url": "https://api.github.com/users/ragulpr/orgs", "repos_url": "https://api.github.com/users/ragulpr/repos", "events_url": "https://api.github.com/users/ragulpr/events{/privacy}", "received_events_url": "https://api.github.com/users/ragulpr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-10-30T21:32:54Z", "updated_at": "2018-11-23T15:55:09Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13339", "html_url": "https://github.com/pytorch/pytorch/pull/13339", "diff_url": "https://github.com/pytorch/pytorch/pull/13339.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13339.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #12643.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"370059964\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12643\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12643/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12643\">#12643</a>, amends to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"269329705\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3341\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3341/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3341\">#3341</a>.</p>\n<ul>\n<li>Allow multidimensional input <del>(but apply softmax over <code>dim=-1</code>)</del> with <code>dim</code> argument</li>\n<li>Cleaner: Less lines of code</li>\n<li>Faster (1.32x speedup vs original, 2x speedup vs using <code>torch.Distributions</code>)</li>\n<li>Small fixes in docstring</li>\n<li>Remove some references in docstring. Was the linked (excellent) ipynb the first to do the straight-through trick? Instead, I propose changing to reference to the two papers most known for it.</li>\n<li>Add deprecationwarning for <code>eps</code>. It's not needed anymore.</li>\n<li>Initial commit keeps some code alternatives commented to exploit CI</li>\n</ul>\n<h2>Controversies:</h2>\n<ul>\n<li>As of discussion when <code>gumbel_softmax</code> was added (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"269329705\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3341\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3341/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3341\">#3341</a>), this was merged into <code>torch.nn.functional</code> before all the work with <code>Distributions</code> and <code>Pyro</code>, and there will probably be multiple other best practices for this in the future.<br>\nI've tested building using the <code>Distributions</code>-api, but it was too slow, see below.</li>\n</ul>\n<p>I therefore propose not using <code>Distributions</code> to keep it fast and simple, but adding a comment in docstring that <code>gumbel_softmax</code> may be deprecated in the future.</p>\n<h3>Build on <code>torch.distributions.RelaxedOneHotCategorical</code>?</h3>\n<pre><code>dist = torch.distributions.RelaxedOneHotCategorical(temperature=tau, logits=logits, validate_args=False)\ny_soft = dist.rsample()\n</code></pre>\n<p>Pros:</p>\n<ul>\n<li>Built using tricks like <code>logsumexp</code> etc</li>\n<li>Explicitly uses <code>torch.distributions.utils._finfo</code> to avoid overflow (old implementation had an <code>eps</code> flag)</li>\n<li>Maintained for this exact purpose.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>Very slow. Construction of distribution adds overhead see timings below. May be solved in future with speedups of <code>TransformedDistribution</code> and <code>Distribution</code>.</li>\n<li>Assumes which <code>dim</code> to apply softmax over.</li>\n</ul>\n<h3>Build on <code>torch.exponential_()</code> (as proposed)?</h3>\n<pre><code>    y_soft = logits.new(logits.shape)\n    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise\n    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise\n</code></pre>\n<p>Pros:</p>\n<ul>\n<li>Faster</li>\n</ul>\n<h2>Timings:</h2>\n<pre><code>    import time\n    start = time.time()\n    num_draws = 1000000\n    logits = torch.randn(1,3)\n\n    for draw in range(num_draws):\n        y_draw = gumbel_softmax(logits, hard=True)\n        counts = counts + y_draw\n    print(end - start)        \n\n## torch.nn.functional.gumbel_softmax()\n&gt;&gt; 12.995795965194702\n\n## Using exponential_() as this commit\n&gt;&gt; 7.658372640609741\n\n## Using RelaxedOneHotCategorical\n&gt;&gt; 20.3382670879364\n</code></pre>\n<h2>TODO</h2>\n<p>Decide on which path to chose. I'll commit in changes to the unit tests in a while to show that it passes both old tests and new tests. I'll also remove the commented code about <code>RelaxedOneHotCategorical</code></p>", "body_text": "Fixes #12643, amends to #3341.\n\nAllow multidimensional input (but apply softmax over dim=-1) with dim argument\nCleaner: Less lines of code\nFaster (1.32x speedup vs original, 2x speedup vs using torch.Distributions)\nSmall fixes in docstring\nRemove some references in docstring. Was the linked (excellent) ipynb the first to do the straight-through trick? Instead, I propose changing to reference to the two papers most known for it.\nAdd deprecationwarning for eps. It's not needed anymore.\nInitial commit keeps some code alternatives commented to exploit CI\n\nControversies:\n\nAs of discussion when gumbel_softmax was added (#3341), this was merged into torch.nn.functional before all the work with Distributions and Pyro, and there will probably be multiple other best practices for this in the future.\nI've tested building using the Distributions-api, but it was too slow, see below.\n\nI therefore propose not using Distributions to keep it fast and simple, but adding a comment in docstring that gumbel_softmax may be deprecated in the future.\nBuild on torch.distributions.RelaxedOneHotCategorical?\ndist = torch.distributions.RelaxedOneHotCategorical(temperature=tau, logits=logits, validate_args=False)\ny_soft = dist.rsample()\n\nPros:\n\nBuilt using tricks like logsumexp etc\nExplicitly uses torch.distributions.utils._finfo to avoid overflow (old implementation had an eps flag)\nMaintained for this exact purpose.\n\nCons:\n\nVery slow. Construction of distribution adds overhead see timings below. May be solved in future with speedups of TransformedDistribution and Distribution.\nAssumes which dim to apply softmax over.\n\nBuild on torch.exponential_() (as proposed)?\n    y_soft = logits.new(logits.shape)\n    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise\n    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise\n\nPros:\n\nFaster\n\nTimings:\n    import time\n    start = time.time()\n    num_draws = 1000000\n    logits = torch.randn(1,3)\n\n    for draw in range(num_draws):\n        y_draw = gumbel_softmax(logits, hard=True)\n        counts = counts + y_draw\n    print(end - start)        \n\n## torch.nn.functional.gumbel_softmax()\n>> 12.995795965194702\n\n## Using exponential_() as this commit\n>> 7.658372640609741\n\n## Using RelaxedOneHotCategorical\n>> 20.3382670879364\n\nTODO\nDecide on which path to chose. I'll commit in changes to the unit tests in a while to show that it passes both old tests and new tests. I'll also remove the commented code about RelaxedOneHotCategorical", "body": "Fixes #12643, amends to #3341.\r\n\r\n- Allow multidimensional input ~~(but apply softmax over `dim=-1`)~~ with `dim` argument\r\n- Cleaner: Less lines of code\r\n- Faster (1.32x speedup vs original, 2x speedup vs using `torch.Distributions`)\r\n- Small fixes in docstring\r\n- Remove some references in docstring. Was the linked (excellent) ipynb the first to do the straight-through trick? Instead, I propose changing to reference to the two papers most known for it.\r\n- Add deprecationwarning for `eps`. It's not needed anymore.\r\n- Initial commit keeps some code alternatives commented to exploit CI\r\n\r\n## Controversies:\r\n- As of discussion when `gumbel_softmax` was added (#3341), this was merged into `torch.nn.functional` before all the work with `Distributions` and `Pyro`, and there will probably be multiple other best practices for this in the future. \r\nI've tested building using the `Distributions`-api, but it was too slow, see below.\r\n\r\nI therefore propose not using `Distributions` to keep it fast and simple, but adding a comment in docstring that `gumbel_softmax` may be deprecated in the future. \r\n\r\n### Build on `torch.distributions.RelaxedOneHotCategorical`?\r\n```\r\ndist = torch.distributions.RelaxedOneHotCategorical(temperature=tau, logits=logits, validate_args=False)\r\ny_soft = dist.rsample()\r\n```\r\n\r\nPros:\r\n* Built using tricks like `logsumexp` etc\r\n* Explicitly uses `torch.distributions.utils._finfo` to avoid overflow (old implementation had an `eps` flag)\r\n* Maintained for this exact purpose.\r\n\r\nCons:\r\n* Very slow. Construction of distribution adds overhead see timings below. May be solved in future with speedups of `TransformedDistribution` and `Distribution`.\r\n* Assumes which `dim` to apply softmax over.\r\n\r\n### Build on `torch.exponential_()` (as proposed)?\r\n```\r\n    y_soft = logits.new(logits.shape)\r\n    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise\r\n    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise\r\n```\r\nPros:\r\n* Faster\r\n\r\n## Timings:\r\n```\r\n    import time\r\n    start = time.time()\r\n    num_draws = 1000000\r\n    logits = torch.randn(1,3)\r\n\r\n    for draw in range(num_draws):\r\n        y_draw = gumbel_softmax(logits, hard=True)\r\n        counts = counts + y_draw\r\n    print(end - start)        \r\n\r\n## torch.nn.functional.gumbel_softmax()\r\n>> 12.995795965194702\r\n\r\n## Using exponential_() as this commit\r\n>> 7.658372640609741\r\n\r\n## Using RelaxedOneHotCategorical\r\n>> 20.3382670879364\r\n````\r\n\r\n## TODO\r\nDecide on which path to chose. I'll commit in changes to the unit tests in a while to show that it passes both old tests and new tests. I'll also remove the commented code about `RelaxedOneHotCategorical`"}