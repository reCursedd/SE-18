{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435325130", "html_url": "https://github.com/pytorch/pytorch/pull/13339#issuecomment-435325130", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13339", "id": 435325130, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTMyNTEzMA==", "user": {"login": "ragulpr", "id": 10998266, "node_id": "MDQ6VXNlcjEwOTk4MjY2", "avatar_url": "https://avatars3.githubusercontent.com/u/10998266?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ragulpr", "html_url": "https://github.com/ragulpr", "followers_url": "https://api.github.com/users/ragulpr/followers", "following_url": "https://api.github.com/users/ragulpr/following{/other_user}", "gists_url": "https://api.github.com/users/ragulpr/gists{/gist_id}", "starred_url": "https://api.github.com/users/ragulpr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ragulpr/subscriptions", "organizations_url": "https://api.github.com/users/ragulpr/orgs", "repos_url": "https://api.github.com/users/ragulpr/repos", "events_url": "https://api.github.com/users/ragulpr/events{/privacy}", "received_events_url": "https://api.github.com/users/ragulpr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-02T09:46:01Z", "updated_at": "2018-11-02T09:46:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1093846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alicanb\">@alicanb</a> I've been looking into how that would be possible. I think it's two things; consistent in values or consistent in style. Long story short, the line-by-line translation using <a href=\"https://github.com/pytorch/pytorch/blob/99ce499bfecd4ed0e87cf60d87ee483c6383d95c/torch/distributions/relaxed_categorical.py#L68\">reference implementation</a></p>\n<h4>Alt a)</h4>\n<pre><code>    uniforms = clamp_probs(torch.rand(logits.shape, dtype=logits.dtype, device=logits.device))\n    gumbels = -((-(uniforms.log())).log()) # -log(exponential) = gumbel(0,1)\n    scores = (logits + gumbels) / tau # gumbel(logits,tau)\n    y_soft = scores - scores.logsumexp(dim=dim, keepdim=True)\n    # need to apply transform(x) as it's a TransformedDistribution\n    # https://github.com/pytorch/pytorch/blob/99ce499bfecd4ed0e87cf60d87ee483c6383d95c/torch/distributions/transformed_distribution.py#L99\n    y_soft = y_soft.exp() \n\n14.4 s \u00b1 86 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p>But this is going to be slow and <code>logsumexp</code> unnecessary. To speed this up and leave a smaller <a href=\"https://gist.github.com/ragulpr/f35513dda57b6f312e0eef107f1f4b2d\">memory footprint</a>  by reusing <code>gumbels</code> variable we get below. This is still consistent in values (generates same thing)</p>\n<h4>Alt b)</h4>\n<pre><code>    # merge logsumexp and exp into softmax, reuse variable names\n    uniforms = clamp_probs(torch.rand(logits.shape, dtype=logits.dtype, device=logits.device))\n    gumbels = -((-(uniforms.log())).log()) # -log(~Exp(1)) = ~Gumbel(0,1)\n    gumbels = (logits + gumbels) / tau # ~Gumbel(logits,tau)\n    y_soft = gumbels.softmax(dim)\n\n11.5 s \u00b1 130 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p>Third alternative is to impose the more legible 'style' of relaxed_categorical rsample implementation but use <code>exponential_()</code> and <code>.new()</code>. This is slightly different in values as it doesn't <code>clamp_probs</code> as aggressively.</p>\n<h4>Alt c)</h4>\n<pre><code>    gumbels = -logits.new(logits.shape).exponential_().log() # ~Gumbel(0,1)\n    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n    y_soft = gumbels.softmax(dim)  # Gumbel softmax noise\n\n11.1 s \u00b1 58.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p>For reference, this was what I committed;</p>\n<h4>Alt d)</h4>\n<pre><code>    y_soft = logits.new(logits.shape)\n    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise\n    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise\n\n10.6 s \u00b1 59 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p>And using finally using <code>RelaxedOneHotCategorical</code> without inlining</p>\n<pre><code>    dist = torch.distributions.RelaxedOneHotCategorical(\n        temperature=tau, logits=logits, validate_args=False)\n    y_soft = dist.rsample()\n\n21.2 s \u00b1 326 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p>In the end, if we want to mimic in values I propose going with <em>alt b)</em> otherwise <em>alt c)</em></p>", "body_text": "Thanks @alicanb I've been looking into how that would be possible. I think it's two things; consistent in values or consistent in style. Long story short, the line-by-line translation using reference implementation\nAlt a)\n    uniforms = clamp_probs(torch.rand(logits.shape, dtype=logits.dtype, device=logits.device))\n    gumbels = -((-(uniforms.log())).log()) # -log(exponential) = gumbel(0,1)\n    scores = (logits + gumbels) / tau # gumbel(logits,tau)\n    y_soft = scores - scores.logsumexp(dim=dim, keepdim=True)\n    # need to apply transform(x) as it's a TransformedDistribution\n    # https://github.com/pytorch/pytorch/blob/99ce499bfecd4ed0e87cf60d87ee483c6383d95c/torch/distributions/transformed_distribution.py#L99\n    y_soft = y_soft.exp() \n\n14.4 s \u00b1 86 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nBut this is going to be slow and logsumexp unnecessary. To speed this up and leave a smaller memory footprint  by reusing gumbels variable we get below. This is still consistent in values (generates same thing)\nAlt b)\n    # merge logsumexp and exp into softmax, reuse variable names\n    uniforms = clamp_probs(torch.rand(logits.shape, dtype=logits.dtype, device=logits.device))\n    gumbels = -((-(uniforms.log())).log()) # -log(~Exp(1)) = ~Gumbel(0,1)\n    gumbels = (logits + gumbels) / tau # ~Gumbel(logits,tau)\n    y_soft = gumbels.softmax(dim)\n\n11.5 s \u00b1 130 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nThird alternative is to impose the more legible 'style' of relaxed_categorical rsample implementation but use exponential_() and .new(). This is slightly different in values as it doesn't clamp_probs as aggressively.\nAlt c)\n    gumbels = -logits.new(logits.shape).exponential_().log() # ~Gumbel(0,1)\n    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n    y_soft = gumbels.softmax(dim)  # Gumbel softmax noise\n\n11.1 s \u00b1 58.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nFor reference, this was what I committed;\nAlt d)\n    y_soft = logits.new(logits.shape)\n    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise\n    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise\n\n10.6 s \u00b1 59 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nAnd using finally using RelaxedOneHotCategorical without inlining\n    dist = torch.distributions.RelaxedOneHotCategorical(\n        temperature=tau, logits=logits, validate_args=False)\n    y_soft = dist.rsample()\n\n21.2 s \u00b1 326 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn the end, if we want to mimic in values I propose going with alt b) otherwise alt c)", "body": "Thanks @alicanb I've been looking into how that would be possible. I think it's two things; consistent in values or consistent in style. Long story short, the line-by-line translation using [reference implementation](https://github.com/pytorch/pytorch/blob/99ce499bfecd4ed0e87cf60d87ee483c6383d95c/torch/distributions/relaxed_categorical.py#L68)\r\n\r\n#### Alt a)\r\n```\r\n    uniforms = clamp_probs(torch.rand(logits.shape, dtype=logits.dtype, device=logits.device))\r\n    gumbels = -((-(uniforms.log())).log()) # -log(exponential) = gumbel(0,1)\r\n    scores = (logits + gumbels) / tau # gumbel(logits,tau)\r\n    y_soft = scores - scores.logsumexp(dim=dim, keepdim=True)\r\n    # need to apply transform(x) as it's a TransformedDistribution\r\n    # https://github.com/pytorch/pytorch/blob/99ce499bfecd4ed0e87cf60d87ee483c6383d95c/torch/distributions/transformed_distribution.py#L99\r\n    y_soft = y_soft.exp() \r\n\r\n14.4 s \u00b1 86 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nBut this is going to be slow and `logsumexp` unnecessary. To speed this up and leave a smaller [memory footprint](https://gist.github.com/ragulpr/f35513dda57b6f312e0eef107f1f4b2d)  by reusing `gumbels` variable we get below. This is still consistent in values (generates same thing)\r\n\r\n#### Alt b)\r\n```\r\n    # merge logsumexp and exp into softmax, reuse variable names\r\n    uniforms = clamp_probs(torch.rand(logits.shape, dtype=logits.dtype, device=logits.device))\r\n    gumbels = -((-(uniforms.log())).log()) # -log(~Exp(1)) = ~Gumbel(0,1)\r\n    gumbels = (logits + gumbels) / tau # ~Gumbel(logits,tau)\r\n    y_soft = gumbels.softmax(dim)\r\n\r\n11.5 s \u00b1 130 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nThird alternative is to impose the more legible 'style' of relaxed_categorical rsample implementation but use `exponential_()` and `.new()`. This is slightly different in values as it doesn't `clamp_probs` as aggressively.\r\n\r\n#### Alt c)\r\n```\r\n    gumbels = -logits.new(logits.shape).exponential_().log() # ~Gumbel(0,1)\r\n    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\r\n    y_soft = gumbels.softmax(dim)  # Gumbel softmax noise\r\n\r\n11.1 s \u00b1 58.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nFor reference, this was what I committed;\r\n\r\n#### Alt d)\r\n```\r\n    y_soft = logits.new(logits.shape)\r\n    y_soft = (logits - y_soft.exponential_().log()) / tau  # Gumbel noise\r\n    y_soft = y_soft.softmax(dim)  # Gumbel softmax noise\r\n\r\n10.6 s \u00b1 59 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nAnd using finally using `RelaxedOneHotCategorical` without inlining\r\n```\r\n    dist = torch.distributions.RelaxedOneHotCategorical(\r\n        temperature=tau, logits=logits, validate_args=False)\r\n    y_soft = dist.rsample()\r\n\r\n21.2 s \u00b1 326 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nIn the end, if we want to mimic in values I propose going with *alt b)* otherwise *alt c)*"}