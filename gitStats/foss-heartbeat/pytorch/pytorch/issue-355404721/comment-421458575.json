{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/421458575", "html_url": "https://github.com/pytorch/pytorch/pull/11061#issuecomment-421458575", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11061", "id": 421458575, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTQ1ODU3NQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-14T19:18:26Z", "updated_at": "2018-09-14T19:18:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>To figure out a good solution to this problem, we must ask ourselves:</p>\n<ol>\n<li>How is <code>tensor()</code> with a <code>Tensor</code> argument implemented today, and</li>\n<li>Why does this implementation \"do the wrong thing\" with respect to <code>requires_grad</code>?</li>\n</ol>\n<p>We can see that <code>tensor()</code> dispatches to <code>tensor_ctor</code>, which dispatches to <code>internal_new_from_data</code>. Here is the important lines in the function:</p>\n<pre><code>  if (THPVariable_Check(data)) {\n      auto var = reinterpret_cast&lt;THPVariable*&gt;(data)-&gt;cdata;\n      auto type_inference_device_type = device_opt.has_value() ? device_opt-&gt;type()\n                                                               : torch::getDeviceType(var.type());\n      // infer the scalar type and device type; it's not expected to infer the layout since these constructors\n      // are defined per-layout-type (e.g. tensor vs sparse_coo_tensor).\n      const auto&amp; type_inference_type = torch::getVariableType(var.type().scalarType(),\n                                                       *torch::getLayout(type.backend()),\n                                                       type_inference_device_type);\n      const auto&amp; type_to_use = type_inference ? type_inference_type : type;\n      return copy_variables ? new_with_tensor_copy(type_to_use, var, device_index) :\n                              new_with_type_conversion(type_to_use, var, device_index);\n  }\n</code></pre>\n<p>In the bodies of <code>new_with_tensor_copy</code> and  <code>new_with_type_conversion</code> you can see this calls <code>copy</code> or <code>toType</code> on <strong>Variable</strong>, which means you get the normal gradient behavior as if you had called <code>copy()</code> or <code>toType()</code> on them, which is why you incorrectly get <code>requires_grad</code> at the end.</p>\n<p>Based on this, this suggests two solutions:</p>\n<ol>\n<li>Keep calling <code>copy</code>/<code>toType</code>, but then <code>detach</code> the tensor afterwards, so that you set <code>requires_grad = false</code> and delete the <code>grad_fn</code>. Effectively, you are saying that <code>tensor(t)</code> is equivalent to <code>t.clone().detach()</code>. This is the \"do the wrong thing first, and then fixup after yourself\" strategy.</li>\n<li>Create a new operator on VariableType, <code>new_with_tensor</code>, which does the right thing from the get go and doesn't actually ever create a grad fn or set requires grad on the output. This is the \"do the right thing from the beginning\" strategy, but it is more complicated to implement.</li>\n</ol>\n<p>I think both approaches are reasonable. If you want to do (2), you'll need to write a custom <code>VariableType</code> method implementation, based on the fact that detach is hardcoded, like this:</p>\n<pre><code>Tensor VariableType::detach(const Tensor &amp; self) const {\n  profiler::RecordFunction profiler(\"detach\");\n  torch::jit::Node* node = nullptr;\n  if (jit::tracer::isTracing()) {\n    auto&amp; graph = jit::tracer::getTracingState()-&gt;graph;\n    node = graph-&gt;create(jit::aten::detach, /*outputs=*/0);\n    jit::tracer::recordSourceLocation(node);\n    jit::tracer::addInputs(node, \"self\", self);\n    graph-&gt;appendNode(node);\n\n  }\n  // &lt;NON_GENERATED_CODE&gt;\n  auto result = as_variable_ref(const_cast&lt;Tensor&amp;&gt;(self)).detach();\n  // &lt;/NON_GENERATED_CODE&gt;\n  if (jit::tracer::isTracing()) {\n    jit::tracer::addOutput(node, result);\n  }\n  return result;\n}\n</code></pre>\n<p>You'll have something similar, except that it calls <code>copy</code>/<code>toType</code> as necessary. But I think detach after the fact is also a very reasonable strategy, and much simpler.</p>", "body_text": "To figure out a good solution to this problem, we must ask ourselves:\n\nHow is tensor() with a Tensor argument implemented today, and\nWhy does this implementation \"do the wrong thing\" with respect to requires_grad?\n\nWe can see that tensor() dispatches to tensor_ctor, which dispatches to internal_new_from_data. Here is the important lines in the function:\n  if (THPVariable_Check(data)) {\n      auto var = reinterpret_cast<THPVariable*>(data)->cdata;\n      auto type_inference_device_type = device_opt.has_value() ? device_opt->type()\n                                                               : torch::getDeviceType(var.type());\n      // infer the scalar type and device type; it's not expected to infer the layout since these constructors\n      // are defined per-layout-type (e.g. tensor vs sparse_coo_tensor).\n      const auto& type_inference_type = torch::getVariableType(var.type().scalarType(),\n                                                       *torch::getLayout(type.backend()),\n                                                       type_inference_device_type);\n      const auto& type_to_use = type_inference ? type_inference_type : type;\n      return copy_variables ? new_with_tensor_copy(type_to_use, var, device_index) :\n                              new_with_type_conversion(type_to_use, var, device_index);\n  }\n\nIn the bodies of new_with_tensor_copy and  new_with_type_conversion you can see this calls copy or toType on Variable, which means you get the normal gradient behavior as if you had called copy() or toType() on them, which is why you incorrectly get requires_grad at the end.\nBased on this, this suggests two solutions:\n\nKeep calling copy/toType, but then detach the tensor afterwards, so that you set requires_grad = false and delete the grad_fn. Effectively, you are saying that tensor(t) is equivalent to t.clone().detach(). This is the \"do the wrong thing first, and then fixup after yourself\" strategy.\nCreate a new operator on VariableType, new_with_tensor, which does the right thing from the get go and doesn't actually ever create a grad fn or set requires grad on the output. This is the \"do the right thing from the beginning\" strategy, but it is more complicated to implement.\n\nI think both approaches are reasonable. If you want to do (2), you'll need to write a custom VariableType method implementation, based on the fact that detach is hardcoded, like this:\nTensor VariableType::detach(const Tensor & self) const {\n  profiler::RecordFunction profiler(\"detach\");\n  torch::jit::Node* node = nullptr;\n  if (jit::tracer::isTracing()) {\n    auto& graph = jit::tracer::getTracingState()->graph;\n    node = graph->create(jit::aten::detach, /*outputs=*/0);\n    jit::tracer::recordSourceLocation(node);\n    jit::tracer::addInputs(node, \"self\", self);\n    graph->appendNode(node);\n\n  }\n  // <NON_GENERATED_CODE>\n  auto result = as_variable_ref(const_cast<Tensor&>(self)).detach();\n  // </NON_GENERATED_CODE>\n  if (jit::tracer::isTracing()) {\n    jit::tracer::addOutput(node, result);\n  }\n  return result;\n}\n\nYou'll have something similar, except that it calls copy/toType as necessary. But I think detach after the fact is also a very reasonable strategy, and much simpler.", "body": "To figure out a good solution to this problem, we must ask ourselves:\r\n\r\n1. How is `tensor()` with a `Tensor` argument implemented today, and\r\n2. Why does this implementation \"do the wrong thing\" with respect to `requires_grad`?\r\n\r\nWe can see that `tensor()` dispatches to `tensor_ctor`, which dispatches to `internal_new_from_data`. Here is the important lines in the function:\r\n\r\n```\r\n  if (THPVariable_Check(data)) {\r\n      auto var = reinterpret_cast<THPVariable*>(data)->cdata;\r\n      auto type_inference_device_type = device_opt.has_value() ? device_opt->type()\r\n                                                               : torch::getDeviceType(var.type());\r\n      // infer the scalar type and device type; it's not expected to infer the layout since these constructors\r\n      // are defined per-layout-type (e.g. tensor vs sparse_coo_tensor).\r\n      const auto& type_inference_type = torch::getVariableType(var.type().scalarType(),\r\n                                                       *torch::getLayout(type.backend()),\r\n                                                       type_inference_device_type);\r\n      const auto& type_to_use = type_inference ? type_inference_type : type;\r\n      return copy_variables ? new_with_tensor_copy(type_to_use, var, device_index) :\r\n                              new_with_type_conversion(type_to_use, var, device_index);\r\n  }\r\n```\r\n\r\nIn the bodies of `new_with_tensor_copy` and  `new_with_type_conversion` you can see this calls `copy` or `toType` on **Variable**, which means you get the normal gradient behavior as if you had called `copy()` or `toType()` on them, which is why you incorrectly get `requires_grad` at the end.\r\n\r\nBased on this, this suggests two solutions:\r\n\r\n1. Keep calling `copy`/`toType`, but then `detach` the tensor afterwards, so that you set `requires_grad = false` and delete the `grad_fn`. Effectively, you are saying that `tensor(t)` is equivalent to `t.clone().detach()`. This is the \"do the wrong thing first, and then fixup after yourself\" strategy.\r\n2. Create a new operator on VariableType, `new_with_tensor`, which does the right thing from the get go and doesn't actually ever create a grad fn or set requires grad on the output. This is the \"do the right thing from the beginning\" strategy, but it is more complicated to implement.\r\n\r\nI think both approaches are reasonable. If you want to do (2), you'll need to write a custom `VariableType` method implementation, based on the fact that detach is hardcoded, like this:\r\n\r\n```\r\nTensor VariableType::detach(const Tensor & self) const {\r\n  profiler::RecordFunction profiler(\"detach\");\r\n  torch::jit::Node* node = nullptr;\r\n  if (jit::tracer::isTracing()) {\r\n    auto& graph = jit::tracer::getTracingState()->graph;\r\n    node = graph->create(jit::aten::detach, /*outputs=*/0);\r\n    jit::tracer::recordSourceLocation(node);\r\n    jit::tracer::addInputs(node, \"self\", self);\r\n    graph->appendNode(node);\r\n\r\n  }\r\n  // <NON_GENERATED_CODE>\r\n  auto result = as_variable_ref(const_cast<Tensor&>(self)).detach();\r\n  // </NON_GENERATED_CODE>\r\n  if (jit::tracer::isTracing()) {\r\n    jit::tracer::addOutput(node, result);\r\n  }\r\n  return result;\r\n}\r\n```\r\n\r\nYou'll have something similar, except that it calls `copy`/`toType` as necessary. But I think detach after the fact is also a very reasonable strategy, and much simpler."}