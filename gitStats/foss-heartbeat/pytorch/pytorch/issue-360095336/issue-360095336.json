{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11672", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11672/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11672/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11672/events", "html_url": "https://github.com/pytorch/pytorch/issues/11672", "id": 360095336, "node_id": "MDU6SXNzdWUzNjAwOTUzMzY=", "number": 11672, "title": "Distributed(c10d) occasionally locks up for mixed precision training", "user": {"login": "bearpelican", "id": 980342, "node_id": "MDQ6VXNlcjk4MDM0Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/980342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bearpelican", "html_url": "https://github.com/bearpelican", "followers_url": "https://api.github.com/users/bearpelican/followers", "following_url": "https://api.github.com/users/bearpelican/following{/other_user}", "gists_url": "https://api.github.com/users/bearpelican/gists{/gist_id}", "starred_url": "https://api.github.com/users/bearpelican/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bearpelican/subscriptions", "organizations_url": "https://api.github.com/users/bearpelican/orgs", "repos_url": "https://api.github.com/users/bearpelican/repos", "events_url": "https://api.github.com/users/bearpelican/events{/privacy}", "received_events_url": "https://api.github.com/users/bearpelican/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2018-09-13T22:53:10Z", "updated_at": "2018-11-21T08:11:00Z", "closed_at": "2018-11-21T08:10:30Z", "author_association": "NONE", "body_html": "<p>After creating DistributedDataParallel model, pytorch freezes when trying to create float tensors.<br>\nThis happens 50% of the time.<br>\nFreeze happens on this <a href=\"https://gist.github.com/bearpelican/4adcadce7f0687681bc9be2324ebafc8#file-train_c10d_deadlock-py-L73\">line</a></p>\n<p>Strangely, it only happens when I set batch norm layers to full precision. Uncommenting out this <a href=\"https://gist.github.com/bearpelican/4adcadce7f0687681bc9be2324ebafc8#file-train_c10d_deadlock-py-L66\">line</a> works fine.</p>\n<p>Hardware - p3.16xlarge (8 V100 GPUs)<br>\nVersion: Pytorch 0.5.0a0+57f149a<br>\nNCCL 2.2.13, Cuda 9.2, Cudnn 7.2.1</p>\n<p>Distributed command run:<br>\nNCCL_DEBUG=VERSION python -m torch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=[machineip] --master_port=[port] train_minimal.py --dist-url env://</p>", "body_text": "After creating DistributedDataParallel model, pytorch freezes when trying to create float tensors.\nThis happens 50% of the time.\nFreeze happens on this line\nStrangely, it only happens when I set batch norm layers to full precision. Uncommenting out this line works fine.\nHardware - p3.16xlarge (8 V100 GPUs)\nVersion: Pytorch 0.5.0a0+57f149a\nNCCL 2.2.13, Cuda 9.2, Cudnn 7.2.1\nDistributed command run:\nNCCL_DEBUG=VERSION python -m torch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=[machineip] --master_port=[port] train_minimal.py --dist-url env://", "body": "After creating DistributedDataParallel model, pytorch freezes when trying to create float tensors.\r\nThis happens 50% of the time.\r\nFreeze happens on this [line](https://gist.github.com/bearpelican/4adcadce7f0687681bc9be2324ebafc8#file-train_c10d_deadlock-py-L73)\r\n\r\nStrangely, it only happens when I set batch norm layers to full precision. Uncommenting out this [line](https://gist.github.com/bearpelican/4adcadce7f0687681bc9be2324ebafc8#file-train_c10d_deadlock-py-L66\r\n) works fine.\r\n\r\nHardware - p3.16xlarge (8 V100 GPUs)\r\nVersion: Pytorch 0.5.0a0+57f149a\r\nNCCL 2.2.13, Cuda 9.2, Cudnn 7.2.1\r\n\r\nDistributed command run:\r\nNCCL_DEBUG=VERSION python -m torch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=[machineip] --master_port=[port] train_minimal.py --dist-url env://"}