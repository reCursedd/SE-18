{"url": "https://api.github.com/repos/pytorch/pytorch/issues/166", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/166/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/166/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/166/events", "html_url": "https://github.com/pytorch/pytorch/issues/166", "id": 184995643, "node_id": "MDU6SXNzdWUxODQ5OTU2NDM=", "number": 166, "title": "can't change requires_grad for non leaf variables", "user": {"login": "glample", "id": 8885556, "node_id": "MDQ6VXNlcjg4ODU1NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8885556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glample", "html_url": "https://github.com/glample", "followers_url": "https://api.github.com/users/glample/followers", "following_url": "https://api.github.com/users/glample/following{/other_user}", "gists_url": "https://api.github.com/users/glample/gists{/gist_id}", "starred_url": "https://api.github.com/users/glample/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glample/subscriptions", "organizations_url": "https://api.github.com/users/glample/orgs", "repos_url": "https://api.github.com/users/glample/repos", "events_url": "https://api.github.com/users/glample/events{/privacy}", "received_events_url": "https://api.github.com/users/glample/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-10-25T01:23:41Z", "updated_at": "2016-10-31T21:47:52Z", "closed_at": "2016-10-31T21:47:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In my code I have a target <code>y</code> which is computed using some Linear and Conv2d modules, and I can't do <code>y.requires_grad = False</code> before I pass it to a loss module.</p>\n<p>Problem comes from:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/93d02e4686ef4732062964df01b8698e457897a7/torch/autograd/variable.py#L39-L41\">pytorch/torch/autograd/variable.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 39 to 41\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/93d02e4686ef4732062964df01b8698e457897a7\">93d02e4</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L39\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"39\"></td>\n          <td id=\"LC39\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.creator <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L40\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"40\"></td>\n          <td id=\"LC40\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>you can only change requires_grad flags of <span class=\"pl-pds\">\"</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L41\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"41\"></td>\n          <td id=\"LC41\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">             <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>leaf variables<span class=\"pl-pds\">\"</span></span>) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>Is there any particular reason why this restriction was implemented?</p>\n<p>Also I was wondering what would happen if we have something like:<br>\nx -&gt; var1 -&gt; var2 -&gt; var3 -&gt; var_loss</p>\n<p>and we call loss.backward() when every variable requires a gradient, apart from var2, for instance? Would it be like if var1.requires_grad is also set to False?</p>", "body_text": "In my code I have a target y which is computed using some Linear and Conv2d modules, and I can't do y.requires_grad = False before I pass it to a loss module.\nProblem comes from:\n\n  \n    \n      pytorch/torch/autograd/variable.py\n    \n    \n        Lines 39 to 41\n      in\n      93d02e4\n    \n    \n    \n    \n\n        \n          \n           if self.creator is not None: \n        \n\n        \n          \n               raise RuntimeError(\"you can only change requires_grad flags of \" \n        \n\n        \n          \n                       \"leaf variables\") \n        \n    \n  \n\n\nIs there any particular reason why this restriction was implemented?\nAlso I was wondering what would happen if we have something like:\nx -> var1 -> var2 -> var3 -> var_loss\nand we call loss.backward() when every variable requires a gradient, apart from var2, for instance? Would it be like if var1.requires_grad is also set to False?", "body": "In my code I have a target `y` which is computed using some Linear and Conv2d modules, and I can't do `y.requires_grad = False` before I pass it to a loss module.\n\nProblem comes from:\nhttps://github.com/pytorch/pytorch/blob/93d02e4686ef4732062964df01b8698e457897a7/torch/autograd/variable.py#L39-L41\n\nIs there any particular reason why this restriction was implemented?\n\nAlso I was wondering what would happen if we have something like:\nx -> var1 -> var2 -> var3 -> var_loss\n\nand we call loss.backward() when every variable requires a gradient, apart from var2, for instance? Would it be like if var1.requires_grad is also set to False?\n"}