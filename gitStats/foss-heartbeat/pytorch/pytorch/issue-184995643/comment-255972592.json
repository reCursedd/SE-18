{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/255972592", "html_url": "https://github.com/pytorch/pytorch/issues/166#issuecomment-255972592", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/166", "id": 255972592, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTk3MjU5Mg==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-25T08:35:59Z", "updated_at": "2016-10-25T08:36:14Z", "author_association": "MEMBER", "body_html": "<p>Yeah, so the reason why I locked changing requires_grad for non-leaf variables is that it's quite tricky to solve properly. There are two ways I though of, and both have problems:</p>\n<ol>\n<li>If you want to disable the gradient flow in the backward, it's not enough to do this. You also have to modify the <code>requires_grad</code> flag of the function that created the variable. In your example it would effectively block var1 and x from receiving gradients, which seems to be an error. If you didn't care about their gradients in the first place, you could have marked them so.</li>\n<li>If you only want to mark that you don't care about the gradient of a variable (a single graph branch), without changing the flag of the creator function, it will likely throw an error in the backward phase, as the following function will see that its input didn't require gradient and will simply return <code>None</code>.</li>\n</ol>\n<p>We might be able to get around 2. by converting <code>None</code>s to correctly sized zero-filled gradient tensors in the backward.</p>", "body_text": "Yeah, so the reason why I locked changing requires_grad for non-leaf variables is that it's quite tricky to solve properly. There are two ways I though of, and both have problems:\n\nIf you want to disable the gradient flow in the backward, it's not enough to do this. You also have to modify the requires_grad flag of the function that created the variable. In your example it would effectively block var1 and x from receiving gradients, which seems to be an error. If you didn't care about their gradients in the first place, you could have marked them so.\nIf you only want to mark that you don't care about the gradient of a variable (a single graph branch), without changing the flag of the creator function, it will likely throw an error in the backward phase, as the following function will see that its input didn't require gradient and will simply return None.\n\nWe might be able to get around 2. by converting Nones to correctly sized zero-filled gradient tensors in the backward.", "body": "Yeah, so the reason why I locked changing requires_grad for non-leaf variables is that it's quite tricky to solve properly. There are two ways I though of, and both have problems:\n1. If you want to disable the gradient flow in the backward, it's not enough to do this. You also have to modify the `requires_grad` flag of the function that created the variable. In your example it would effectively block var1 and x from receiving gradients, which seems to be an error. If you didn't care about their gradients in the first place, you could have marked them so.\n2. If you only want to mark that you don't care about the gradient of a variable (a single graph branch), without changing the flag of the creator function, it will likely throw an error in the backward phase, as the following function will see that its input didn't require gradient and will simply return `None`.\n\nWe might be able to get around 2. by converting `None`s to correctly sized zero-filled gradient tensors in the backward.\n"}