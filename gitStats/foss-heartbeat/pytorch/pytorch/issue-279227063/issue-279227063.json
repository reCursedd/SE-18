{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4023", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4023/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4023/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4023/events", "html_url": "https://github.com/pytorch/pytorch/issues/4023", "id": 279227063, "node_id": "MDU6SXNzdWUyNzkyMjcwNjM=", "number": 4023, "title": "b'CUDNN_STATUS_EXECUTION_FAILED", "user": {"login": "ElijahLai", "id": 25605935, "node_id": "MDQ6VXNlcjI1NjA1OTM1", "avatar_url": "https://avatars3.githubusercontent.com/u/25605935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ElijahLai", "html_url": "https://github.com/ElijahLai", "followers_url": "https://api.github.com/users/ElijahLai/followers", "following_url": "https://api.github.com/users/ElijahLai/following{/other_user}", "gists_url": "https://api.github.com/users/ElijahLai/gists{/gist_id}", "starred_url": "https://api.github.com/users/ElijahLai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ElijahLai/subscriptions", "organizations_url": "https://api.github.com/users/ElijahLai/orgs", "repos_url": "https://api.github.com/users/ElijahLai/repos", "events_url": "https://api.github.com/users/ElijahLai/events{/privacy}", "received_events_url": "https://api.github.com/users/ElijahLai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 806617721, "node_id": "MDU6TGFiZWw4MDY2MTc3MjE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cudnn", "name": "cudnn", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-12-05T02:50:23Z", "updated_at": "2018-01-24T04:10:05Z", "closed_at": null, "author_association": "NONE", "body_html": "<pre lang=\"ERROR\" data-meta=\"INFOR\"><code>~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in backward_extended(self, grad_output, grad_hy)\n    332                 output,\n    333                 weight,\n--&gt; 334                 grad_weight)\n    335         else:\n    336             grad_weight = [(None,) * len(layer_weight) for layer_weight in weight]\n\n~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py in backward_weight(fn, input, hx, output, weight, grad_weight)\n    462             ctypes.c_void_p(workspace.data_ptr()), workspace.size(0),\n    463             fn.w_desc, ctypes.c_void_p(dw.data_ptr()),\n--&gt; 464             ctypes.c_void_p(fn.reserve.data_ptr()), fn.reserve.size(0)\n    465         ))\n    466 \n\n~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py in check_error(status)\n    253 def check_error(status):\n    254     if status is not 0:\n--&gt; 255         raise CuDNNError(status)\n    256 \n    257 \n\nCuDNNError: 8: b'CUDNN_STATUS_EXECUTION_FAILED'\n</code></pre>\n<pre lang=\"MY\" data-meta=\"CODE\"><code>class DecoderRNN(nn.Module):\n    def __init__(self,embedding_size,hidden_size,vervab_size,n_layers=1,dropout_p=0.2):\n        \"\"\"\n        :param embedding_size: word embedding size\n        :param hidden_size: encoder hidden size\n        :param vervab_size: output size(all the vercabulary)\n        :param n_layers: layers of cells(lstm/gru)\n        :param dropout_p: drop out persentage\n        \"\"\"\n        super(DecoderRNN,self).__init__()\n        # define data size\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.dropout_p = dropout_p\n        \n        # define operation\n        self.embedding = nn.Embedding(vervab_size,embedding_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.lstm = nn.LSTM(embedding_size,hidden_size)\n        self.out = nn.Linear(hidden_size,vervab_size)\n        self.softmax = nn.LogSoftmax()\n        \n    def forward(self,input,hidden):\n        embedded = self.embedding(input)\n        output = self.dropout(embedded)\n        for i in range(self.n_layers):\n            output, hidden = self.lstm(output,hidden)\n        output = self.softmax(self.out(output[0]))\n        return output,hidden\n</code></pre>\n<p>I'm very comfused what happen to my code, I can use GPU to run another case but I always meet this error information during runing Attention-based seq2seq and now vanilla seq2seq.</p>\n<p>Could anyone help me ?</p>", "body_text": "~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in backward_extended(self, grad_output, grad_hy)\n    332                 output,\n    333                 weight,\n--> 334                 grad_weight)\n    335         else:\n    336             grad_weight = [(None,) * len(layer_weight) for layer_weight in weight]\n\n~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py in backward_weight(fn, input, hx, output, weight, grad_weight)\n    462             ctypes.c_void_p(workspace.data_ptr()), workspace.size(0),\n    463             fn.w_desc, ctypes.c_void_p(dw.data_ptr()),\n--> 464             ctypes.c_void_p(fn.reserve.data_ptr()), fn.reserve.size(0)\n    465         ))\n    466 \n\n~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py in check_error(status)\n    253 def check_error(status):\n    254     if status is not 0:\n--> 255         raise CuDNNError(status)\n    256 \n    257 \n\nCuDNNError: 8: b'CUDNN_STATUS_EXECUTION_FAILED'\n\nclass DecoderRNN(nn.Module):\n    def __init__(self,embedding_size,hidden_size,vervab_size,n_layers=1,dropout_p=0.2):\n        \"\"\"\n        :param embedding_size: word embedding size\n        :param hidden_size: encoder hidden size\n        :param vervab_size: output size(all the vercabulary)\n        :param n_layers: layers of cells(lstm/gru)\n        :param dropout_p: drop out persentage\n        \"\"\"\n        super(DecoderRNN,self).__init__()\n        # define data size\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.dropout_p = dropout_p\n        \n        # define operation\n        self.embedding = nn.Embedding(vervab_size,embedding_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.lstm = nn.LSTM(embedding_size,hidden_size)\n        self.out = nn.Linear(hidden_size,vervab_size)\n        self.softmax = nn.LogSoftmax()\n        \n    def forward(self,input,hidden):\n        embedded = self.embedding(input)\n        output = self.dropout(embedded)\n        for i in range(self.n_layers):\n            output, hidden = self.lstm(output,hidden)\n        output = self.softmax(self.out(output[0]))\n        return output,hidden\n\nI'm very comfused what happen to my code, I can use GPU to run another case but I always meet this error information during runing Attention-based seq2seq and now vanilla seq2seq.\nCould anyone help me ?", "body": "~~~~~~~~~~~~~~~~~~~~~~~~~~  ERROR INFOR\r\n~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py in backward_extended(self, grad_output, grad_hy)\r\n    332                 output,\r\n    333                 weight,\r\n--> 334                 grad_weight)\r\n    335         else:\r\n    336             grad_weight = [(None,) * len(layer_weight) for layer_weight in weight]\r\n\r\n~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py in backward_weight(fn, input, hx, output, weight, grad_weight)\r\n    462             ctypes.c_void_p(workspace.data_ptr()), workspace.size(0),\r\n    463             fn.w_desc, ctypes.c_void_p(dw.data_ptr()),\r\n--> 464             ctypes.c_void_p(fn.reserve.data_ptr()), fn.reserve.size(0)\r\n    465         ))\r\n    466 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py in check_error(status)\r\n    253 def check_error(status):\r\n    254     if status is not 0:\r\n--> 255         raise CuDNNError(status)\r\n    256 \r\n    257 \r\n\r\nCuDNNError: 8: b'CUDNN_STATUS_EXECUTION_FAILED'\r\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n~~~~~~~~~~~~~~~~~~~~~~~  MY CODE\r\nclass DecoderRNN(nn.Module):\r\n    def __init__(self,embedding_size,hidden_size,vervab_size,n_layers=1,dropout_p=0.2):\r\n        \"\"\"\r\n        :param embedding_size: word embedding size\r\n        :param hidden_size: encoder hidden size\r\n        :param vervab_size: output size(all the vercabulary)\r\n        :param n_layers: layers of cells(lstm/gru)\r\n        :param dropout_p: drop out persentage\r\n        \"\"\"\r\n        super(DecoderRNN,self).__init__()\r\n        # define data size\r\n        self.n_layers = n_layers\r\n        self.hidden_size = hidden_size\r\n        self.dropout_p = dropout_p\r\n        \r\n        # define operation\r\n        self.embedding = nn.Embedding(vervab_size,embedding_size)\r\n        self.dropout = nn.Dropout(self.dropout_p)\r\n        self.lstm = nn.LSTM(embedding_size,hidden_size)\r\n        self.out = nn.Linear(hidden_size,vervab_size)\r\n        self.softmax = nn.LogSoftmax()\r\n        \r\n    def forward(self,input,hidden):\r\n        embedded = self.embedding(input)\r\n        output = self.dropout(embedded)\r\n        for i in range(self.n_layers):\r\n            output, hidden = self.lstm(output,hidden)\r\n        output = self.softmax(self.out(output[0]))\r\n        return output,hidden\r\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nI'm very comfused what happen to my code, I can use GPU to run another case but I always meet this error information during runing Attention-based seq2seq and now vanilla seq2seq.\r\n\r\nCould anyone help me ?"}