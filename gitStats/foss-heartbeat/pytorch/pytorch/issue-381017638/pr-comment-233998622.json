{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233998622", "pull_request_review_id": 175544201, "id": 233998622, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzk5ODYyMg==", "diff_hunk": "@@ -284,4 +301,67 @@ Tensor& potrs_out(Tensor& result, const Tensor& self, const Tensor& A, bool uppe\n   return at::_th_potrs_single_out(result, self, A, upper);\n }\n \n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ cholesky ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+template<typename scalar_t>\n+static void apply_cholesky(Tensor& self, bool upper, std::vector<int64_t>& infos) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"cholesky: LAPACK library not found in compilation\");\n+#endif\n+\n+  char uplo = upper ? 'U' : 'L';\n+\n+  auto self_data = self.data<scalar_t>();\n+  auto self_matrix_stride = matrixStride(self);\n+\n+  auto batch_size = batchCount(self);\n+  auto n = self.size(-2);\n+\n+  int info;\n+  for (int64_t i = 0; i < batch_size; i++) {\n+    scalar_t* self_working_ptr = &self_data[i * self_matrix_stride];\n+    lapackCholesky<scalar_t>(uplo, n, self_working_ptr, n, &info);\n+    infos[i] = info;\n+    if (info != 0) {\n+      return;\n+    }\n+  }\n+}\n+\n+Tensor _cholesky_helper_cpu(const Tensor& self, bool upper) {\n+  std::vector<int64_t> infos(batchCount(self), 0);\n+  auto self_working_copy = cloneBatchedColumnMajor(self);\n+  AT_DISPATCH_FLOATING_TYPES(self.type(), \"cholesky\", [&]{\n+    apply_cholesky<scalar_t>(self_working_copy, upper, infos);\n+  });\n+  batchCheckErrors(infos, \"cholesky\");\n+  return self_working_copy;\n+}\n+\n+Tensor cholesky(const Tensor &self, bool upper) {\n+  if (self.size(-1) == 0) {\n+    return at::empty_like(self);\n+  }\n+  if (self.dim() == 2) {\n+    return at::_th_potrf_single(self, upper);", "path": "aten/src/ATen/native/BatchLinearAlgebra.cpp", "position": 154, "original_position": 103, "commit_id": "1906ead76a20bb6dfc1d7714b7756545f68b1115", "original_commit_id": "49e4efde354abe5b1b71dd97f51a5e2e9131d552", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "So, at the end of the day, `_th_potrf_single` is calling the same LAPACK function, right? Can we just get rid of that codepath entirely and kill all of the TH bindings too? :)", "created_at": "2018-11-15T20:35:37Z", "updated_at": "2018-11-23T15:54:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/14017#discussion_r233998622", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/14017", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233998622"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/14017#discussion_r233998622"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/14017"}}, "body_html": "<p>So, at the end of the day, <code>_th_potrf_single</code> is calling the same LAPACK function, right? Can we just get rid of that codepath entirely and kill all of the TH bindings too? :)</p>", "body_text": "So, at the end of the day, _th_potrf_single is calling the same LAPACK function, right? Can we just get rid of that codepath entirely and kill all of the TH bindings too? :)"}