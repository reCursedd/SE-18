{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5426", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5426/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5426/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5426/events", "html_url": "https://github.com/pytorch/pytorch/issues/5426", "id": 300501936, "node_id": "MDU6SXNzdWUzMDA1MDE5MzY=", "number": 5426, "title": "torch.from_numpy is slow for mmap'ped numpy arrays", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-02-27T05:50:29Z", "updated_at": "2018-07-19T00:30:24Z", "closed_at": "2018-02-27T18:09:41Z", "author_association": "NONE", "body_html": "<p>I'm looking at feasibility of using Ray as parameter server combined with single-machine NN implementation.</p>\n<p>The bottleneck for using it with PyTorch is that result of Ray calls come as numpy array created on top of <a href=\"https://groups.google.com/forum/#!msg/ray-dev/pNsxWI-iSyI/vhWnjgAfAwAJ\" rel=\"nofollow\">mmaped memory</a>, and PyTorch <code>from_numpy</code> on these arrays is slow, working at about 2.5 GB/sec, which is the speed of single-threaded memcpy.</p>\n<p>Regular numpy array can be turned into PyTorch GPU tensors at 8-11 GB/sec, depending on whether memory has been page locked.</p>\n<p>Is this memcpy necessary? Can it be done in multi-threaded way?<br>\nRelated issue\u00a0on Ray side: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"300449020\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/ray-project/ray/issues/1614\" data-hovercard-type=\"issue\" data-hovercard-url=\"/ray-project/ray/issues/1614/hovercard\" href=\"https://github.com/ray-project/ray/issues/1614\">ray-project/ray#1614</a></p>\n<p><a href=\"https://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py\">https://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py</a></p>\n<p>Pytorch 0.3.0.post4<br>\nRepro instructions like in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"300323456\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5412\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5412/hovercard?comment_id=368658892&amp;comment_type=issue_comment\" href=\"https://github.com/pytorch/pytorch/issues/5412#issuecomment-368658892\">#5412 (comment)</a></p>\n<pre><code># torch.from_numpy(ray.get(f.remote()))\npython tf_numpy_benchmark.py --benchmark=pytorch_from_numpy_noclone --allocator=ray --num-iters=51\npytorch_from_numpy            :   2.5 GB/sec, min: 39.86, median: 41.38, mean: 63.55\n\n# torch.from_numpy(page_locked_numpy).cuda()\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=tfgpu --num-iters=51\npytorchgpu_from_numpy         :  11.0 GB/sec, min:  9.05, median:  9.07, mean:  9.07\n\n# torch.from_numpy(regular_numpy).cuda()\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=numpy --num-iters=51\npytorchgpu_from_numpy         :   8.2 GB/sec, min: 12.24, median: 12.42, mean: 12.45\n</code></pre>", "body_text": "I'm looking at feasibility of using Ray as parameter server combined with single-machine NN implementation.\nThe bottleneck for using it with PyTorch is that result of Ray calls come as numpy array created on top of mmaped memory, and PyTorch from_numpy on these arrays is slow, working at about 2.5 GB/sec, which is the speed of single-threaded memcpy.\nRegular numpy array can be turned into PyTorch GPU tensors at 8-11 GB/sec, depending on whether memory has been page locked.\nIs this memcpy necessary? Can it be done in multi-threaded way?\nRelated issue\u00a0on Ray side: ray-project/ray#1614\nhttps://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py\nPytorch 0.3.0.post4\nRepro instructions like in #5412 (comment)\n# torch.from_numpy(ray.get(f.remote()))\npython tf_numpy_benchmark.py --benchmark=pytorch_from_numpy_noclone --allocator=ray --num-iters=51\npytorch_from_numpy            :   2.5 GB/sec, min: 39.86, median: 41.38, mean: 63.55\n\n# torch.from_numpy(page_locked_numpy).cuda()\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=tfgpu --num-iters=51\npytorchgpu_from_numpy         :  11.0 GB/sec, min:  9.05, median:  9.07, mean:  9.07\n\n# torch.from_numpy(regular_numpy).cuda()\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=numpy --num-iters=51\npytorchgpu_from_numpy         :   8.2 GB/sec, min: 12.24, median: 12.42, mean: 12.45", "body": "I'm looking at feasibility of using Ray as parameter server combined with single-machine NN implementation. \r\n\r\nThe bottleneck for using it with PyTorch is that result of Ray calls come as numpy array created on top of [mmaped memory](https://groups.google.com/forum/#!msg/ray-dev/pNsxWI-iSyI/vhWnjgAfAwAJ), and PyTorch `from_numpy` on these arrays is slow, working at about 2.5 GB/sec, which is the speed of single-threaded memcpy.\r\n\r\nRegular numpy array can be turned into PyTorch GPU tensors at 8-11 GB/sec, depending on whether memory has been page locked.\r\n\r\nIs this memcpy necessary? Can it be done in multi-threaded way?\r\nRelated issue\u00a0on Ray side: https://github.com/ray-project/ray/issues/1614\r\n\r\nhttps://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py\r\n\r\nPytorch 0.3.0.post4\r\nRepro instructions like in https://github.com/pytorch/pytorch/issues/5412#issuecomment-368658892\r\n```\r\n# torch.from_numpy(ray.get(f.remote()))\r\npython tf_numpy_benchmark.py --benchmark=pytorch_from_numpy_noclone --allocator=ray --num-iters=51\r\npytorch_from_numpy            :   2.5 GB/sec, min: 39.86, median: 41.38, mean: 63.55\r\n\r\n# torch.from_numpy(page_locked_numpy).cuda()\r\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=tfgpu --num-iters=51\r\npytorchgpu_from_numpy         :  11.0 GB/sec, min:  9.05, median:  9.07, mean:  9.07\r\n\r\n# torch.from_numpy(regular_numpy).cuda()\r\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=numpy --num-iters=51\r\npytorchgpu_from_numpy         :   8.2 GB/sec, min: 12.24, median: 12.42, mean: 12.45\r\n```"}