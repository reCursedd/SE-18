{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4355", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4355/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4355/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4355/events", "html_url": "https://github.com/pytorch/pytorch/issues/4355", "id": 284620960, "node_id": "MDU6SXNzdWUyODQ2MjA5NjA=", "number": 4355, "title": "gradcheck fails with cryptic error", "user": {"login": "jpeg729", "id": 3158606, "node_id": "MDQ6VXNlcjMxNTg2MDY=", "avatar_url": "https://avatars0.githubusercontent.com/u/3158606?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpeg729", "html_url": "https://github.com/jpeg729", "followers_url": "https://api.github.com/users/jpeg729/followers", "following_url": "https://api.github.com/users/jpeg729/following{/other_user}", "gists_url": "https://api.github.com/users/jpeg729/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpeg729/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpeg729/subscriptions", "organizations_url": "https://api.github.com/users/jpeg729/orgs", "repos_url": "https://api.github.com/users/jpeg729/repos", "events_url": "https://api.github.com/users/jpeg729/events{/privacy}", "received_events_url": "https://api.github.com/users/jpeg729/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-26T23:10:28Z", "updated_at": "2017-12-28T10:46:06Z", "closed_at": "2017-12-27T21:05:29Z", "author_association": "NONE", "body_html": "<p>I tried implementing the <a href=\"https://arxiv.org/pdf/1710.09967.pdf\" rel=\"nofollow\">ISRLU activation</a> but the <code>gradcheck</code> fails. I have pytorch 0.3.0 installed via conda.</p>\n<p>Here is the code.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Function\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">ISRLU</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">tensor</span>, <span class=\"pl-smi\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        negatives <span class=\"pl-k\">=</span> torch.min(tensor, <span class=\"pl-c1\">0</span> <span class=\"pl-k\">*</span> tensor)\n        ctx.nisr <span class=\"pl-k\">=</span> torch.rsqrt(<span class=\"pl-c1\">1</span>. <span class=\"pl-k\">+</span> alpha <span class=\"pl-k\">*</span> (negatives <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> ctx.nisr == 1 where tensor elements are positive</span>\n        <span class=\"pl-k\">return</span> tensor <span class=\"pl-k\">*</span> ctx.nisr\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_out</span>):\n        <span class=\"pl-k\">return</span> ctx.nisr <span class=\"pl-k\">**</span> <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">None</span>\n\n<span class=\"pl-c1\">ISRLU</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">ISRLU</span>.apply\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable, gradcheck\n    inputs <span class=\"pl-k\">=</span> (Variable(torch.randn(<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">20</span>).double(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(torch.randn(<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">20</span>).double(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),)\n    test <span class=\"pl-k\">=</span> gradcheck(<span class=\"pl-c1\">ISRLU</span>, inputs, <span class=\"pl-v\">eps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-6</span>, <span class=\"pl-v\">atol</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n    <span class=\"pl-c1\">print</span>(test)</pre></div>\n<p>And here is the error.</p>\n<pre><code>Traceback (most recent call last):\n  File \"activations/isrlu.py\", line 50, in &lt;module&gt;\n    test = gradcheck(ISRLU, inputs, eps=1e-6, atol=1e-4)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 176, in gradcheck\n    analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(_as_tuple(inputs), o)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 110, in get_analytical_jacobian\n    output.backward(grad_output, create_graph=True)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 167, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 99, in backward\n    variables, grad_variables, retain_graph)\nRuntimeError: expected Variable or None (got torch.DoubleTensor)\n</code></pre>\n<p>Any ideas?</p>\n<p>Interestingly, <code>gradcheck</code> isn't documented on the pytorch website. It is mentioned on the \"Extending pytorch\" page, but you won't find it on the <code>torch.autograd</code> page.</p>", "body_text": "I tried implementing the ISRLU activation but the gradcheck fails. I have pytorch 0.3.0 installed via conda.\nHere is the code.\nimport torch\nfrom torch.autograd import Function\n\nclass ISRLU(Function):\n    @staticmethod\n    def forward(ctx, tensor, alpha=1):\n        negatives = torch.min(tensor, 0 * tensor)\n        ctx.nisr = torch.rsqrt(1. + alpha * (negatives ** 2))\n        # ctx.nisr == 1 where tensor elements are positive\n        return tensor * ctx.nisr\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        return ctx.nisr ** 3, None\n\nISRLU = ISRLU.apply\n\nif __name__ == \"__main__\":\n    from torch.autograd import Variable, gradcheck\n    inputs = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(20,20).double(), requires_grad=True),)\n    test = gradcheck(ISRLU, inputs, eps=1e-6, atol=1e-4)\n    print(test)\nAnd here is the error.\nTraceback (most recent call last):\n  File \"activations/isrlu.py\", line 50, in <module>\n    test = gradcheck(ISRLU, inputs, eps=1e-6, atol=1e-4)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 176, in gradcheck\n    analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(_as_tuple(inputs), o)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 110, in get_analytical_jacobian\n    output.backward(grad_output, create_graph=True)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 167, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 99, in backward\n    variables, grad_variables, retain_graph)\nRuntimeError: expected Variable or None (got torch.DoubleTensor)\n\nAny ideas?\nInterestingly, gradcheck isn't documented on the pytorch website. It is mentioned on the \"Extending pytorch\" page, but you won't find it on the torch.autograd page.", "body": "I tried implementing the [ISRLU activation](https://arxiv.org/pdf/1710.09967.pdf) but the `gradcheck` fails. I have pytorch 0.3.0 installed via conda.\r\n\r\nHere is the code.\r\n```python\r\nimport torch\r\nfrom torch.autograd import Function\r\n\r\nclass ISRLU(Function):\r\n    @staticmethod\r\n    def forward(ctx, tensor, alpha=1):\r\n        negatives = torch.min(tensor, 0 * tensor)\r\n        ctx.nisr = torch.rsqrt(1. + alpha * (negatives ** 2))\r\n        # ctx.nisr == 1 where tensor elements are positive\r\n        return tensor * ctx.nisr\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        return ctx.nisr ** 3, None\r\n\r\nISRLU = ISRLU.apply\r\n\r\nif __name__ == \"__main__\":\r\n    from torch.autograd import Variable, gradcheck\r\n    inputs = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(20,20).double(), requires_grad=True),)\r\n    test = gradcheck(ISRLU, inputs, eps=1e-6, atol=1e-4)\r\n    print(test)\r\n```\r\nAnd here is the error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"activations/isrlu.py\", line 50, in <module>\r\n    test = gradcheck(ISRLU, inputs, eps=1e-6, atol=1e-4)\r\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 176, in gradcheck\r\n    analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(_as_tuple(inputs), o)\r\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 110, in get_analytical_jacobian\r\n    output.backward(grad_output, create_graph=True)\r\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 167, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/home/mpage/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 99, in backward\r\n    variables, grad_variables, retain_graph)\r\nRuntimeError: expected Variable or None (got torch.DoubleTensor)\r\n```\r\nAny ideas?\r\n\r\nInterestingly, `gradcheck` isn't documented on the pytorch website. It is mentioned on the \"Extending pytorch\" page, but you won't find it on the `torch.autograd` page."}