{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2460", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2460/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2460/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2460/events", "html_url": "https://github.com/pytorch/pytorch/issues/2460", "id": 250664081, "node_id": "MDU6SXNzdWUyNTA2NjQwODE=", "number": 2460, "title": "flatten_parameters() in RNNs for contiguous memory", "user": {"login": "aconneau", "id": 10132730, "node_id": "MDQ6VXNlcjEwMTMyNzMw", "avatar_url": "https://avatars1.githubusercontent.com/u/10132730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aconneau", "html_url": "https://github.com/aconneau", "followers_url": "https://api.github.com/users/aconneau/followers", "following_url": "https://api.github.com/users/aconneau/following{/other_user}", "gists_url": "https://api.github.com/users/aconneau/gists{/gist_id}", "starred_url": "https://api.github.com/users/aconneau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aconneau/subscriptions", "organizations_url": "https://api.github.com/users/aconneau/orgs", "repos_url": "https://api.github.com/users/aconneau/repos", "events_url": "https://api.github.com/users/aconneau/events{/privacy}", "received_events_url": "https://api.github.com/users/aconneau/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-08-16T15:15:35Z", "updated_at": "2017-12-04T16:17:05Z", "closed_at": "2017-08-25T15:08:55Z", "author_association": "NONE", "body_html": "<p>Since the new pytorch version, I have a warning when I <a href=\"https://github.com/facebookresearch/InferSent/blob/master/encoder/models.py#L49\">encode sentence with a BiLSTM-max</a> that says:</p>\n<pre><code>models.py:53: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\nsent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n</code></pre>\n<p>When I do <code>model.enc_lstm.flatten_parameters()</code> as suggested, and then try to encode again, I get the following error though:</p>\n<pre><code>/home/aconneau/python/InferSent/encoder/models.py in forward(self, sent_tuple)\n     51         # Handling padding in Recurrent Networks\n     52         sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\n---&gt; 53         sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n     54         sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n     55 \n\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    222         for hook in self._forward_pre_hooks.values():\n    223             hook(self, input)\n--&gt; 224         result = self.forward(*input, **kwargs)\n    225         for hook in self._forward_hooks.values():\n    226             hook_result = hook(self, input, result)\n\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc in forward(self, input, hx)\n    143         if has_flat_weights:\n    144             first_data = next(self.parameters()).data\n--&gt; 145             assert first_data.storage().size() == self._param_buf_size\n    146             flat_weight = first_data.new().set_(first_data.storage(), 0, torch.Size([self._param_buf_size]))\n    147         else:\n\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __getattr__(self, name)\n    260                 return modules[name]\n    261         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n--&gt; 262             type(self).__name__, name))\n    263 \n    264     def __setattr__(self, name, value):\n\nAttributeError: 'LSTM' object has no attribute '_param_buf_size'\n</code></pre>\n<p>Any idea?</p>", "body_text": "Since the new pytorch version, I have a warning when I encode sentence with a BiLSTM-max that says:\nmodels.py:53: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\nsent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n\nWhen I do model.enc_lstm.flatten_parameters() as suggested, and then try to encode again, I get the following error though:\n/home/aconneau/python/InferSent/encoder/models.py in forward(self, sent_tuple)\n     51         # Handling padding in Recurrent Networks\n     52         sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\n---> 53         sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n     54         sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n     55 \n\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    222         for hook in self._forward_pre_hooks.values():\n    223             hook(self, input)\n--> 224         result = self.forward(*input, **kwargs)\n    225         for hook in self._forward_hooks.values():\n    226             hook_result = hook(self, input, result)\n\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc in forward(self, input, hx)\n    143         if has_flat_weights:\n    144             first_data = next(self.parameters()).data\n--> 145             assert first_data.storage().size() == self._param_buf_size\n    146             flat_weight = first_data.new().set_(first_data.storage(), 0, torch.Size([self._param_buf_size]))\n    147         else:\n\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __getattr__(self, name)\n    260                 return modules[name]\n    261         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n--> 262             type(self).__name__, name))\n    263 \n    264     def __setattr__(self, name, value):\n\nAttributeError: 'LSTM' object has no attribute '_param_buf_size'\n\nAny idea?", "body": "Since the new pytorch version, I have a warning when I [encode sentence with a BiLSTM-max](https://github.com/facebookresearch/InferSent/blob/master/encoder/models.py#L49) that says:\r\n\r\n\r\n```\r\nmodels.py:53: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\r\nsent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\r\n```\r\n\r\n\r\nWhen I do `model.enc_lstm.flatten_parameters()` as suggested, and then try to encode again, I get the following error though:\r\n\r\n\r\n```\r\n/home/aconneau/python/InferSent/encoder/models.py in forward(self, sent_tuple)\r\n     51         # Handling padding in Recurrent Networks\r\n     52         sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\r\n---> 53         sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\r\n     54         sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\r\n     55 \r\n\r\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    222         for hook in self._forward_pre_hooks.values():\r\n    223             hook(self, input)\r\n--> 224         result = self.forward(*input, **kwargs)\r\n    225         for hook in self._forward_hooks.values():\r\n    226             hook_result = hook(self, input, result)\r\n\r\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/rnn.pyc in forward(self, input, hx)\r\n    143         if has_flat_weights:\r\n    144             first_data = next(self.parameters()).data\r\n--> 145             assert first_data.storage().size() == self._param_buf_size\r\n    146             flat_weight = first_data.new().set_(first_data.storage(), 0, torch.Size([self._param_buf_size]))\r\n    147         else:\r\n\r\n/home/aconneau/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc in __getattr__(self, name)\r\n    260                 return modules[name]\r\n    261         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\n--> 262             type(self).__name__, name))\r\n    263 \r\n    264     def __setattr__(self, name, value):\r\n\r\nAttributeError: 'LSTM' object has no attribute '_param_buf_size'\r\n```\r\n\r\nAny idea?"}