{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/441137502", "html_url": "https://github.com/pytorch/pytorch/issues/11430#issuecomment-441137502", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11430", "id": 441137502, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MTEzNzUwMg==", "user": {"login": "gg4u", "id": 284214, "node_id": "MDQ6VXNlcjI4NDIxNA==", "avatar_url": "https://avatars0.githubusercontent.com/u/284214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gg4u", "html_url": "https://github.com/gg4u", "followers_url": "https://api.github.com/users/gg4u/followers", "following_url": "https://api.github.com/users/gg4u/following{/other_user}", "gists_url": "https://api.github.com/users/gg4u/gists{/gist_id}", "starred_url": "https://api.github.com/users/gg4u/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gg4u/subscriptions", "organizations_url": "https://api.github.com/users/gg4u/orgs", "repos_url": "https://api.github.com/users/gg4u/repos", "events_url": "https://api.github.com/users/gg4u/events{/privacy}", "received_events_url": "https://api.github.com/users/gg4u/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-23T00:31:44Z", "updated_at": "2018-11-23T00:31:44Z", "author_association": "NONE", "body_html": "<p>Hi, met same error on Google Colab, trying to use CPU runtime.</p>\n<p>Possible to allow pytorch use CPU ?</p>\n<pre><code>\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-17-a1274325e1cb&gt; in &lt;module&gt;()\n      7 #This tends to be the case with the oldest photos.\n      8 render_factor=42\n----&gt; 9 filters = [Colorizer(gpu=0, weights_path=weights_path)]\n     10 vis = ModelImageVisualizer(filters, render_factor=render_factor, results_dir=results_dir)\n\n/content/DeOldify/fasterai/filters.py in __init__(self, gpu, weights_path)\n     79     def __init__(self, gpu:int, weights_path:Path):\n     80         super().__init__(tfms=[BlackAndWhiteTransform()])\n---&gt; 81         self.model = Unet34(nf_factor=2).cuda(gpu)\n     82         self._init_model(self.model, weights_path)\n     83         self.render_base=16\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in cuda(self, device)\n    256             Module: self\n    257         \"\"\"\n--&gt; 258         return self._apply(lambda t: t.cuda(device))\n    259 \n    260     def cpu(self):\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--&gt; 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--&gt; 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\n    189                 # Tensors stored in modules are graph leaves, and we don't\n    190                 # want to create copy nodes, so we have to unpack the data.\n--&gt; 191                 param.data = fn(param.data)\n    192                 if param._grad is not None:\n    193                     param._grad.data = fn(param._grad.data)\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in &lt;lambda&gt;(t)\n    256             Module: self\n    257         \"\"\"\n--&gt; 258         return self._apply(lambda t: t.cuda(device))\n    259 \n    260     def cpu(self):\n\nRuntimeError: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.\n</code></pre>", "body_text": "Hi, met same error on Google Colab, trying to use CPU runtime.\nPossible to allow pytorch use CPU ?\n\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-17-a1274325e1cb> in <module>()\n      7 #This tends to be the case with the oldest photos.\n      8 render_factor=42\n----> 9 filters = [Colorizer(gpu=0, weights_path=weights_path)]\n     10 vis = ModelImageVisualizer(filters, render_factor=render_factor, results_dir=results_dir)\n\n/content/DeOldify/fasterai/filters.py in __init__(self, gpu, weights_path)\n     79     def __init__(self, gpu:int, weights_path:Path):\n     80         super().__init__(tfms=[BlackAndWhiteTransform()])\n---> 81         self.model = Unet34(nf_factor=2).cuda(gpu)\n     82         self._init_model(self.model, weights_path)\n     83         self.render_base=16\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in cuda(self, device)\n    256             Module: self\n    257         \"\"\"\n--> 258         return self._apply(lambda t: t.cuda(device))\n    259 \n    260     def cpu(self):\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--> 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--> 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\n    189                 # Tensors stored in modules are graph leaves, and we don't\n    190                 # want to create copy nodes, so we have to unpack the data.\n--> 191                 param.data = fn(param.data)\n    192                 if param._grad is not None:\n    193                     param._grad.data = fn(param._grad.data)\n\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in <lambda>(t)\n    256             Module: self\n    257         \"\"\"\n--> 258         return self._apply(lambda t: t.cuda(device))\n    259 \n    260     def cpu(self):\n\nRuntimeError: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.", "body": "Hi, met same error on Google Colab, trying to use CPU runtime.\r\n\r\nPossible to allow pytorch use CPU ?\r\n\r\n```\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-17-a1274325e1cb> in <module>()\r\n      7 #This tends to be the case with the oldest photos.\r\n      8 render_factor=42\r\n----> 9 filters = [Colorizer(gpu=0, weights_path=weights_path)]\r\n     10 vis = ModelImageVisualizer(filters, render_factor=render_factor, results_dir=results_dir)\r\n\r\n/content/DeOldify/fasterai/filters.py in __init__(self, gpu, weights_path)\r\n     79     def __init__(self, gpu:int, weights_path:Path):\r\n     80         super().__init__(tfms=[BlackAndWhiteTransform()])\r\n---> 81         self.model = Unet34(nf_factor=2).cuda(gpu)\r\n     82         self._init_model(self.model, weights_path)\r\n     83         self.render_base=16\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in cuda(self, device)\r\n    256             Module: self\r\n    257         \"\"\"\r\n--> 258         return self._apply(lambda t: t.cuda(device))\r\n    259 \r\n    260     def cpu(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    183     def _apply(self, fn):\r\n    184         for module in self.children():\r\n--> 185             module._apply(fn)\r\n    186 \r\n    187         for param in self._parameters.values():\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    183     def _apply(self, fn):\r\n    184         for module in self.children():\r\n--> 185             module._apply(fn)\r\n    186 \r\n    187         for param in self._parameters.values():\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    189                 # Tensors stored in modules are graph leaves, and we don't\r\n    190                 # want to create copy nodes, so we have to unpack the data.\r\n--> 191                 param.data = fn(param.data)\r\n    192                 if param._grad is not None:\r\n    193                     param._grad.data = fn(param._grad.data)\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in <lambda>(t)\r\n    256             Module: self\r\n    257         \"\"\"\r\n--> 258         return self._apply(lambda t: t.cuda(device))\r\n    259 \r\n    260     def cpu(self):\r\n\r\nRuntimeError: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.\r\n```"}