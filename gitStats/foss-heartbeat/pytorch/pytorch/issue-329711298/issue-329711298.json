{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8186", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8186/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8186/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8186/events", "html_url": "https://github.com/pytorch/pytorch/issues/8186", "id": 329711298, "node_id": "MDU6SXNzdWUzMjk3MTEyOTg=", "number": 8186, "title": "[Caffe2] Successive in-place operators cause RuntimeError of gradient operator versions", "user": {"login": "II-Matto", "id": 2261229, "node_id": "MDQ6VXNlcjIyNjEyMjk=", "avatar_url": "https://avatars0.githubusercontent.com/u/2261229?v=4", "gravatar_id": "", "url": "https://api.github.com/users/II-Matto", "html_url": "https://github.com/II-Matto", "followers_url": "https://api.github.com/users/II-Matto/followers", "following_url": "https://api.github.com/users/II-Matto/following{/other_user}", "gists_url": "https://api.github.com/users/II-Matto/gists{/gist_id}", "starred_url": "https://api.github.com/users/II-Matto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/II-Matto/subscriptions", "organizations_url": "https://api.github.com/users/II-Matto/orgs", "repos_url": "https://api.github.com/users/II-Matto/repos", "events_url": "https://api.github.com/users/II-Matto/events{/privacy}", "received_events_url": "https://api.github.com/users/II-Matto/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-06T05:02:17Z", "updated_at": "2018-06-11T14:09:27Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Errors are reported when running a network with two successive in-place operators, e.g.</p>\n<pre><code>FC(x, y)--&gt;ReLU(y, y)--&gt;Dropout(y, y)\nConv(x, y)--&gt; ReLU(y, y)--&gt;UserDefinedInplaceOp(y, y)\n</code></pre>\n<p>The error (from the <code>FC-ReLU-Dropout</code> case) is as follows:</p>\n<pre><code>...\n  File \"/home/&lt;user_name&gt;/repo/pytorch/build/caffe2/python/model_helper.py\", line 335, in AddGradientOperators\n    self.grad_map = self.net.AddGradientOperators(*args, **kwargs)\n  File \"/home/&lt;user_name&gt;/repo/pytorch/build/caffe2/python/core.py\", line 1864, in AddGradientOperators\n    self._net.op[skip:], ys)\n  File \"/home/&lt;user_name&gt;/repo/pytorch/build/caffe2/python/core.py\", line 1126, in GetBackwardPass\n    return ir.GetBackwardPass(ys)\n  File \"/home/&lt;user_name&gt;/repo/pytorch/build/caffe2/python/core.py\", line 1001, in GetBackwardPass\n    forward_op_idx, all_input_to_grad)\n  File \"/home/&lt;user_name&gt;/repo/pytorch/build/caffe2/python/core.py\", line 955, in _GenerateGradientsForForwardOp\n    forward_op_idx, gradient_ops, g_output, g_input)\n  File \"/home/&lt;user_name&gt;/repo/pytorch/build/caffe2/python/core.py\", line 615, in BuildGradientGenerators\n    s, g_output, fwd_op_idx, locally_generated_blobs)\n  File \"/home/&lt;user_name&gt;/repo/pytorch/build/caffe2/python/core.py\", line 558, in CheckGradientOperatorInput\n    ) + versionMismatchInfoOut(grad_op_input)\nRuntimeError: Gradient operator needs output \"gpu_0/fc6\" at version 1, but currently we have version 2.\n...\n</code></pre>\n<p>The problem can be solved if using a different output name for the last operator, e.g. changing from <code>y</code> to <code>z</code> as follows:</p>\n<pre><code>FC(x, y)--&gt;ReLU(y, y)--&gt;Dropout(y, z)\nConv(x, y)--&gt; ReLU(y, y)--&gt;UserDefinedInplaceOp(y, z)\n</code></pre>\n<p>But this seems a little weird. Note that in Caffe, it is OK to allow ReLU and Dropout to be both in-place operations.</p>\n<p>I noticed that the gradient operator of Dropout does not allow in-place computation due to the trick used in generating a random mask. But for my <code>UserDefinedInplaceOp</code>, the gradient operator does support in-place gradient computation, which is also declared with <code>AllowInplace({{0, 0}})</code> in <code>OPERATOR_SCHEMA</code> for both the operator and the gradient operator. <strong>So why does this problem happen? Does this mean one cannot use two consecutive in-place operators with Caffe2? If so, why? If not, what operators are allowed to use in such cases?</strong></p>\n<p>The problem is also encountered in issue: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"247930770\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/caffe2/caffe2/issues/1017\" data-hovercard-type=\"issue\" data-hovercard-url=\"/caffe2/caffe2/issues/1017/hovercard\" href=\"https://github.com/caffe2/caffe2/issues/1017\">caffe2/caffe2#1017</a>.</p>\n<p>BTW, what is the parameter of <code>AllowInplace</code> exactly? I see explanations nowhere.</p>\n<h2>Code example</h2>\n<p>One can try to add an in-place <code>Dropout</code> operator after the <code>FC-ReLU</code> in VGG-16 network and run it to reproduce the error.</p>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: <strong>Caffe2</strong></li>\n<li>How you installed PyTorch (conda, pip, source): <strong>source</strong></li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS: <strong>Ubuntu 16.04</strong></li>\n<li>PyTorch version: <strong>(latest clone)</strong></li>\n<li>Python version: <strong>Python 2.7.15 :: Anaconda, Inc.</strong></li>\n<li>CUDA/cuDNN version: <strong>8.0/cudnn-8.0-linux-x64-v7</strong></li>\n<li>GPU models and configuration: <strong>(Titan Xp)</strong></li>\n<li>GCC version (if compiling from source): <strong>5.4.0</strong></li>\n<li>CMake version: <strong>3.11.3</strong></li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nErrors are reported when running a network with two successive in-place operators, e.g.\nFC(x, y)-->ReLU(y, y)-->Dropout(y, y)\nConv(x, y)--> ReLU(y, y)-->UserDefinedInplaceOp(y, y)\n\nThe error (from the FC-ReLU-Dropout case) is as follows:\n...\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/model_helper.py\", line 335, in AddGradientOperators\n    self.grad_map = self.net.AddGradientOperators(*args, **kwargs)\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 1864, in AddGradientOperators\n    self._net.op[skip:], ys)\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 1126, in GetBackwardPass\n    return ir.GetBackwardPass(ys)\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 1001, in GetBackwardPass\n    forward_op_idx, all_input_to_grad)\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 955, in _GenerateGradientsForForwardOp\n    forward_op_idx, gradient_ops, g_output, g_input)\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 615, in BuildGradientGenerators\n    s, g_output, fwd_op_idx, locally_generated_blobs)\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 558, in CheckGradientOperatorInput\n    ) + versionMismatchInfoOut(grad_op_input)\nRuntimeError: Gradient operator needs output \"gpu_0/fc6\" at version 1, but currently we have version 2.\n...\n\nThe problem can be solved if using a different output name for the last operator, e.g. changing from y to z as follows:\nFC(x, y)-->ReLU(y, y)-->Dropout(y, z)\nConv(x, y)--> ReLU(y, y)-->UserDefinedInplaceOp(y, z)\n\nBut this seems a little weird. Note that in Caffe, it is OK to allow ReLU and Dropout to be both in-place operations.\nI noticed that the gradient operator of Dropout does not allow in-place computation due to the trick used in generating a random mask. But for my UserDefinedInplaceOp, the gradient operator does support in-place gradient computation, which is also declared with AllowInplace({{0, 0}}) in OPERATOR_SCHEMA for both the operator and the gradient operator. So why does this problem happen? Does this mean one cannot use two consecutive in-place operators with Caffe2? If so, why? If not, what operators are allowed to use in such cases?\nThe problem is also encountered in issue: caffe2/caffe2#1017.\nBTW, what is the parameter of AllowInplace exactly? I see explanations nowhere.\nCode example\nOne can try to add an in-place Dropout operator after the FC-ReLU in VGG-16 network and run it to reproduce the error.\nSystem Info\n\nPyTorch or Caffe2: Caffe2\nHow you installed PyTorch (conda, pip, source): source\nBuild command you used (if compiling from source):\nOS: Ubuntu 16.04\nPyTorch version: (latest clone)\nPython version: Python 2.7.15 :: Anaconda, Inc.\nCUDA/cuDNN version: 8.0/cudnn-8.0-linux-x64-v7\nGPU models and configuration: (Titan Xp)\nGCC version (if compiling from source): 5.4.0\nCMake version: 3.11.3\nVersions of any other relevant libraries:", "body": "## Issue description\r\n\r\nErrors are reported when running a network with two successive in-place operators, e.g.\r\n```\r\nFC(x, y)-->ReLU(y, y)-->Dropout(y, y)\r\nConv(x, y)--> ReLU(y, y)-->UserDefinedInplaceOp(y, y)\r\n```\r\nThe error (from the `FC-ReLU-Dropout` case) is as follows:\r\n```\r\n...\r\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/model_helper.py\", line 335, in AddGradientOperators\r\n    self.grad_map = self.net.AddGradientOperators(*args, **kwargs)\r\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 1864, in AddGradientOperators\r\n    self._net.op[skip:], ys)\r\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 1126, in GetBackwardPass\r\n    return ir.GetBackwardPass(ys)\r\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 1001, in GetBackwardPass\r\n    forward_op_idx, all_input_to_grad)\r\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 955, in _GenerateGradientsForForwardOp\r\n    forward_op_idx, gradient_ops, g_output, g_input)\r\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 615, in BuildGradientGenerators\r\n    s, g_output, fwd_op_idx, locally_generated_blobs)\r\n  File \"/home/<user_name>/repo/pytorch/build/caffe2/python/core.py\", line 558, in CheckGradientOperatorInput\r\n    ) + versionMismatchInfoOut(grad_op_input)\r\nRuntimeError: Gradient operator needs output \"gpu_0/fc6\" at version 1, but currently we have version 2.\r\n...\r\n```\r\nThe problem can be solved if using a different output name for the last operator, e.g. changing from `y` to `z` as follows:\r\n```\r\nFC(x, y)-->ReLU(y, y)-->Dropout(y, z)\r\nConv(x, y)--> ReLU(y, y)-->UserDefinedInplaceOp(y, z)\r\n```\r\nBut this seems a little weird. Note that in Caffe, it is OK to allow ReLU and Dropout to be both in-place operations.\r\n\r\nI noticed that the gradient operator of Dropout does not allow in-place computation due to the trick used in generating a random mask. But for my `UserDefinedInplaceOp`, the gradient operator does support in-place gradient computation, which is also declared with `AllowInplace({{0, 0}})` in `OPERATOR_SCHEMA` for both the operator and the gradient operator. **So why does this problem happen? Does this mean one cannot use two consecutive in-place operators with Caffe2? If so, why? If not, what operators are allowed to use in such cases?**\r\n\r\nThe problem is also encountered in issue: https://github.com/caffe2/caffe2/issues/1017.\r\n\r\nBTW, what is the parameter of `AllowInplace` exactly? I see explanations nowhere.\r\n\r\n## Code example\r\n\r\nOne can try to add an in-place `Dropout` operator after the `FC-ReLU` in VGG-16 network and run it to reproduce the error.\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: **Caffe2**\r\n- How you installed PyTorch (conda, pip, source): **source**\r\n- Build command you used (if compiling from source):\r\n- OS: **Ubuntu 16.04**\r\n- PyTorch version: **(latest clone)**\r\n- Python version: **Python 2.7.15 :: Anaconda, Inc.**\r\n- CUDA/cuDNN version: **8.0/cudnn-8.0-linux-x64-v7**\r\n- GPU models and configuration: **(Titan Xp)**\r\n- GCC version (if compiling from source): **5.4.0**\r\n- CMake version: **3.11.3**\r\n- Versions of any other relevant libraries:\r\n"}