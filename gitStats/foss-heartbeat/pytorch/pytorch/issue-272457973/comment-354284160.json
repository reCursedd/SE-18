{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/354284160", "html_url": "https://github.com/pytorch/pytorch/issues/3587#issuecomment-354284160", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3587", "id": 354284160, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDI4NDE2MA==", "user": {"login": "cyxtj", "id": 2942204, "node_id": "MDQ6VXNlcjI5NDIyMDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2942204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cyxtj", "html_url": "https://github.com/cyxtj", "followers_url": "https://api.github.com/users/cyxtj/followers", "following_url": "https://api.github.com/users/cyxtj/following{/other_user}", "gists_url": "https://api.github.com/users/cyxtj/gists{/gist_id}", "starred_url": "https://api.github.com/users/cyxtj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cyxtj/subscriptions", "organizations_url": "https://api.github.com/users/cyxtj/orgs", "repos_url": "https://api.github.com/users/cyxtj/repos", "events_url": "https://api.github.com/users/cyxtj/events{/privacy}", "received_events_url": "https://api.github.com/users/cyxtj/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-28T12:54:44Z", "updated_at": "2017-12-28T12:54:44Z", "author_association": "NONE", "body_html": "<pre><code>ipt = Variable(torch.from_numpy(np.asarray([0., 0, 1, 1.]).reshape(-1, 1, 1).astype(np.float32)))\nh0 = Variable(torch.zeros(2, 1,1 ))\nrnn = nn.RNN(1,1, 1, bidirectional=True, bias=False, nonlinearity='relu')\nfor k, v  in rnn.named_parameters():\n   setattr(rnn, k, torch.nn.Parameter(torch.ones_like(v.data)))\nopt, hn = rnn(ipt, h0)\nprint list(rnn.named_parameters())\nprint opt\nprint ipt\n</code></pre>\n<p>the output I got:</p>\n<pre><code>[('weight_ih_l0', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n), ('weight_hh_l0', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n), ('weight_ih_l0_reverse', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n), ('weight_hh_l0_reverse', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n)]\nVariable containing:\n(0 ,.,.) =\n  0  2\n\n(1 ,.,.) =\n  0  2\n\n(2 ,.,.) =\n  1  2\n\n(3 ,.,.) =\n  2  1\n[torch.FloatTensor of size 4x1x2]\nVariable containing:\n(0 ,.,.) =\n  0\n\n(1 ,.,.) =\n  0\n\n(2 ,.,.) =\n  1\n\n(3 ,.,.) =\n  1\n[torch.FloatTensor of size 4x1x1]\n</code></pre>\n<p>to be more intuitive, I arrange the result:</p>\n<pre><code>--forward     0   -&gt;    0   -&gt;   1   -&gt;     2\nbackward      2   &lt;-     2   &lt;-   2    &lt;-  1\n----input      0   --    0    --   1     --  1\nwordindex    0   --    1    --   2    --  3\nhn:              [forward 2, backward 2]\n</code></pre>\n<p>After this experiment:</p>\n<ol>\n<li>forward and backward are computed independently</li>\n<li>after running through f/b ward, it returns something like <code>torch.cat([forward, backward], dim=-1)</code></li>\n<li>the hidden it return is actually  hidden of  word 3 in forward pass, and hidden of word 0 in backward pass.</li>\n<li>therefore it is not a good idea to use rnn cell and run a for loop by yourself if you set bidirectional=True.</li>\n</ol>\n<p>I think these behavior should be clarify somewhere in the official doc, which unfortunately I did not find yet.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> can you consider about adding it? Or this is something beyond pytorch but go depth to CuDNN?</p>", "body_text": "ipt = Variable(torch.from_numpy(np.asarray([0., 0, 1, 1.]).reshape(-1, 1, 1).astype(np.float32)))\nh0 = Variable(torch.zeros(2, 1,1 ))\nrnn = nn.RNN(1,1, 1, bidirectional=True, bias=False, nonlinearity='relu')\nfor k, v  in rnn.named_parameters():\n   setattr(rnn, k, torch.nn.Parameter(torch.ones_like(v.data)))\nopt, hn = rnn(ipt, h0)\nprint list(rnn.named_parameters())\nprint opt\nprint ipt\n\nthe output I got:\n[('weight_ih_l0', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n), ('weight_hh_l0', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n), ('weight_ih_l0_reverse', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n), ('weight_hh_l0_reverse', Parameter containing:\n 1\n[torch.FloatTensor of size 1x1]\n)]\nVariable containing:\n(0 ,.,.) =\n  0  2\n\n(1 ,.,.) =\n  0  2\n\n(2 ,.,.) =\n  1  2\n\n(3 ,.,.) =\n  2  1\n[torch.FloatTensor of size 4x1x2]\nVariable containing:\n(0 ,.,.) =\n  0\n\n(1 ,.,.) =\n  0\n\n(2 ,.,.) =\n  1\n\n(3 ,.,.) =\n  1\n[torch.FloatTensor of size 4x1x1]\n\nto be more intuitive, I arrange the result:\n--forward     0   ->    0   ->   1   ->     2\nbackward      2   <-     2   <-   2    <-  1\n----input      0   --    0    --   1     --  1\nwordindex    0   --    1    --   2    --  3\nhn:              [forward 2, backward 2]\n\nAfter this experiment:\n\nforward and backward are computed independently\nafter running through f/b ward, it returns something like torch.cat([forward, backward], dim=-1)\nthe hidden it return is actually  hidden of  word 3 in forward pass, and hidden of word 0 in backward pass.\ntherefore it is not a good idea to use rnn cell and run a for loop by yourself if you set bidirectional=True.\n\nI think these behavior should be clarify somewhere in the official doc, which unfortunately I did not find yet.\n@apaszke can you consider about adding it? Or this is something beyond pytorch but go depth to CuDNN?", "body": "```\r\nipt = Variable(torch.from_numpy(np.asarray([0., 0, 1, 1.]).reshape(-1, 1, 1).astype(np.float32)))\r\nh0 = Variable(torch.zeros(2, 1,1 ))\r\nrnn = nn.RNN(1,1, 1, bidirectional=True, bias=False, nonlinearity='relu')\r\nfor k, v  in rnn.named_parameters():\r\n   setattr(rnn, k, torch.nn.Parameter(torch.ones_like(v.data)))\r\nopt, hn = rnn(ipt, h0)\r\nprint list(rnn.named_parameters())\r\nprint opt\r\nprint ipt\r\n```\r\nthe output I got: \r\n```\r\n[('weight_ih_l0', Parameter containing:\r\n 1\r\n[torch.FloatTensor of size 1x1]\r\n), ('weight_hh_l0', Parameter containing:\r\n 1\r\n[torch.FloatTensor of size 1x1]\r\n), ('weight_ih_l0_reverse', Parameter containing:\r\n 1\r\n[torch.FloatTensor of size 1x1]\r\n), ('weight_hh_l0_reverse', Parameter containing:\r\n 1\r\n[torch.FloatTensor of size 1x1]\r\n)]\r\nVariable containing:\r\n(0 ,.,.) =\r\n  0  2\r\n\r\n(1 ,.,.) =\r\n  0  2\r\n\r\n(2 ,.,.) =\r\n  1  2\r\n\r\n(3 ,.,.) =\r\n  2  1\r\n[torch.FloatTensor of size 4x1x2]\r\nVariable containing:\r\n(0 ,.,.) =\r\n  0\r\n\r\n(1 ,.,.) =\r\n  0\r\n\r\n(2 ,.,.) =\r\n  1\r\n\r\n(3 ,.,.) =\r\n  1\r\n[torch.FloatTensor of size 4x1x1]\r\n```\r\nto be more intuitive, I arrange the result:\r\n```\r\n--forward     0   ->    0   ->   1   ->     2\r\nbackward      2   <-     2   <-   2    <-  1\r\n----input      0   --    0    --   1     --  1\r\nwordindex    0   --    1    --   2    --  3\r\nhn:              [forward 2, backward 2]\r\n```\r\nAfter this experiment:\r\n1. forward and backward are computed independently\r\n2. after running through f/b ward, it returns something like `torch.cat([forward, backward], dim=-1)`\r\n3. the hidden it return is actually  hidden of  word 3 in forward pass, and hidden of word 0 in backward pass.\r\n4. therefore it is not a good idea to use rnn cell and run a for loop by yourself if you set bidirectional=True.\r\n\r\nI think these behavior should be clarify somewhere in the official doc, which unfortunately I did not find yet.\r\n@apaszke can you consider about adding it? Or this is something beyond pytorch but go depth to CuDNN?"}