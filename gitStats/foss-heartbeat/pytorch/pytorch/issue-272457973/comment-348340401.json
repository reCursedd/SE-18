{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348340401", "html_url": "https://github.com/pytorch/pytorch/issues/3587#issuecomment-348340401", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3587", "id": 348340401, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODM0MDQwMQ==", "user": {"login": "ozancaglayan", "id": 330946, "node_id": "MDQ6VXNlcjMzMDk0Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/330946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ozancaglayan", "html_url": "https://github.com/ozancaglayan", "followers_url": "https://api.github.com/users/ozancaglayan/followers", "following_url": "https://api.github.com/users/ozancaglayan/following{/other_user}", "gists_url": "https://api.github.com/users/ozancaglayan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ozancaglayan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ozancaglayan/subscriptions", "organizations_url": "https://api.github.com/users/ozancaglayan/orgs", "repos_url": "https://api.github.com/users/ozancaglayan/repos", "events_url": "https://api.github.com/users/ozancaglayan/events{/privacy}", "received_events_url": "https://api.github.com/users/ozancaglayan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-30T22:23:58Z", "updated_at": "2017-11-30T22:25:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>From what I've understand so far about non <code>Cell</code> suffixed CuDNN variants:</p>\n<ol>\n<li>You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. <code>pack_padded_sequence()</code></li>\n<li>After calling the RNN, you receive a tuple of 2 items: <strong>packed</strong> <code>Variable</code> of all hidden states (<code>hs</code>) and a normal <code>Variable</code> containing the last hidden states (<code>ht</code>).</li>\n<li>For <code>hs</code> you unpack it using <code>pad_packed_sequence()</code> to get a normal <code>Variable</code>.</li>\n<li><code>ht</code> contains the <strong>correct</strong> forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc <strong>but</strong> this tensor does not concatenate the forward-backward states although <code>hs</code> returns them in a concatenated fashion.</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> hidden_dim = 3, bidirectional=True, num_layers=1</span>\nIn [<span class=\"pl-c1\">525</span>]: input_\nOut[<span class=\"pl-c1\">525</span>]:\n\n <span class=\"pl-c1\">1</span>  <span class=\"pl-c1\">3</span>\n <span class=\"pl-c1\">3</span>  <span class=\"pl-c1\">5</span>\n <span class=\"pl-c1\">3</span>  <span class=\"pl-c1\">2</span>\n <span class=\"pl-c1\">2</span>  <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">1</span>  <span class=\"pl-c1\">0</span>\n[torch.LongTensor of size <span class=\"pl-ii\">5x2</span>]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> hs and ht are the return values of GRU here (for LSTM you'll also have c_t)</span>\nIn [<span class=\"pl-c1\">526</span>]: <span class=\"pl-c1\">print</span>(hs[:, <span class=\"pl-c1\">1</span>], ht[:, <span class=\"pl-c1\">1</span>])\n\nVariable containing:\n   (( forward states ))                      (( backward states ))\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.0982</span>  <span class=\"pl-c1\">0.0275</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.3005</span>            <span class=\"pl-c1\">0.3609</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.4958</span>  <span class=\"pl-c1\">0.3408</span>\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1710</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.0576</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.3759</span>            <span class=\"pl-c1\">0.2550</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.3478</span>  <span class=\"pl-c1\">0.2796</span>\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1935</span>  <span class=\"pl-c1\">0.0484</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.4111</span>            <span class=\"pl-c1\">0.2088</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.2813</span>  <span class=\"pl-c1\">0.1440</span>\n <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>            <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>\n <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>            <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>\n[torch.FloatTensor of size <span class=\"pl-ii\">5x6</span>]\n\n Variable containing:\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1935</span>  <span class=\"pl-c1\">0.0484</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.4111</span>\n <span class=\"pl-c1\">0.3609</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.4958</span>  <span class=\"pl-c1\">0.3408</span>\n[torch.FloatTensor of size <span class=\"pl-ii\">2x3</span>]</pre></div>\n<p>Here you can see that the last state for the forward sequence (3-&gt;5-&gt;2) is the third row's first 3 elements <code>-0.1935  0.0484 -0.4111</code> that you also find in the <code>ht</code> variable in the first row.</p>\n<p>The last state for the backward sequence (2-&gt;5-&gt;3) is the first row's second part <code>0.3609 -0.4958  0.3408</code> that you also find in the <code>ht</code>variable in the second row.</p>\n<p>So if you want to apply attention the first tensor is the one that you'll need. If you want to just take the last states, second tensor is at your help.</p>\n<p>But if <code>num_layers &gt; 1</code>, the second tensor becomes a mess :) Overall, I think this part of the PyTorch API really needs more intuitive handling.</p>", "body_text": "From what I've understand so far about non Cell suffixed CuDNN variants:\n\nYou need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. pack_padded_sequence()\nAfter calling the RNN, you receive a tuple of 2 items: packed Variable of all hidden states (hs) and a normal Variable containing the last hidden states (ht).\nFor hs you unpack it using pad_packed_sequence() to get a normal Variable.\nht contains the correct forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc but this tensor does not concatenate the forward-backward states although hs returns them in a concatenated fashion.\n\n# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.\n# hidden_dim = 3, bidirectional=True, num_layers=1\nIn [525]: input_\nOut[525]:\n\n 1  3\n 3  5\n 3  2\n 2  0\n 1  0\n[torch.LongTensor of size 5x2]\n\n# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)\nIn [526]: print(hs[:, 1], ht[:, 1])\n\nVariable containing:\n   (( forward states ))                      (( backward states ))\n-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408\n-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796\n-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n[torch.FloatTensor of size 5x6]\n\n Variable containing:\n-0.1935  0.0484 -0.4111\n 0.3609 -0.4958  0.3408\n[torch.FloatTensor of size 2x3]\nHere you can see that the last state for the forward sequence (3->5->2) is the third row's first 3 elements -0.1935  0.0484 -0.4111 that you also find in the ht variable in the first row.\nThe last state for the backward sequence (2->5->3) is the first row's second part 0.3609 -0.4958  0.3408 that you also find in the htvariable in the second row.\nSo if you want to apply attention the first tensor is the one that you'll need. If you want to just take the last states, second tensor is at your help.\nBut if num_layers > 1, the second tensor becomes a mess :) Overall, I think this part of the PyTorch API really needs more intuitive handling.", "body": "From what I've understand so far about non `Cell` suffixed CuDNN variants:\r\n\r\n1. You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. `pack_padded_sequence()`\r\n2. After calling the RNN, you receive a tuple of 2 items: **packed** `Variable` of all hidden states (`hs`) and a normal `Variable` containing the last hidden states (`ht`).\r\n3. For `hs` you unpack it using `pad_packed_sequence()` to get a normal `Variable`.\r\n4. `ht` contains the **correct** forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc **but** this tensor does not concatenate the forward-backward states although `hs` returns them in a concatenated fashion.\r\n\r\n```python\r\n# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.\r\n# hidden_dim = 3, bidirectional=True, num_layers=1\r\nIn [525]: input_\r\nOut[525]:\r\n\r\n 1  3\r\n 3  5\r\n 3  2\r\n 2  0\r\n 1  0\r\n[torch.LongTensor of size 5x2]\r\n\r\n# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)\r\nIn [526]: print(hs[:, 1], ht[:, 1])\r\n\r\nVariable containing:\r\n   (( forward states ))                      (( backward states ))\r\n-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408\r\n-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796\r\n-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440\r\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\r\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\r\n[torch.FloatTensor of size 5x6]\r\n\r\n Variable containing:\r\n-0.1935  0.0484 -0.4111\r\n 0.3609 -0.4958  0.3408\r\n[torch.FloatTensor of size 2x3]\r\n```\r\nHere you can see that the last state for the forward sequence (3->5->2) is the third row's first 3 elements `-0.1935  0.0484 -0.4111` that you also find in the `ht` variable in the first row.\r\n\r\nThe last state for the backward sequence (2->5->3) is the first row's second part `0.3609 -0.4958  0.3408` that you also find in the `ht`variable in the second row.\r\n\r\nSo if you want to apply attention the first tensor is the one that you'll need. If you want to just take the last states, second tensor is at your help.\r\n\r\nBut if `num_layers > 1`, the second tensor becomes a mess :) Overall, I think this part of the PyTorch API really needs more intuitive handling. \r\n"}