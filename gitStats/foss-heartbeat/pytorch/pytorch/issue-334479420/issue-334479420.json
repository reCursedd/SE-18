{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8742", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8742/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8742/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8742/events", "html_url": "https://github.com/pytorch/pytorch/pull/8742", "id": 334479420, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk2NDQxNDg2", "number": 8742, "title": "[TESTING ONLY] Test Hypothesis 3.60.1", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-21T12:51:56Z", "updated_at": "2018-07-25T21:40:44Z", "closed_at": "2018-07-25T21:40:44Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8742", "html_url": "https://github.com/pytorch/pytorch/pull/8742", "diff_url": "https://github.com/pytorch/pytorch/pull/8742.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8742.patch"}, "body_html": "<p>I believe that Hypothesis 3.59.0 --&gt; 3.60.1 has caused a regression</p>\n<pre><code>05:09:34 =================================== FAILURES ===================================\n05:09:34 ____________ TestBooleanMaskOp.test_sequence_mask_batching_lengths _____________\n05:09:34 \n05:09:34 self = &lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;\n05:09:34 \n05:09:34     @given(x=hu.tensor(min_dim=2,\n05:09:34 &gt;                      max_dim=5,\n05:09:34                        elements=st.floats(min_value=0.5, max_value=1.0)),\n05:09:34            dtype=st.sampled_from([np.float32, np.float16]),\n05:09:34            **hu.gcs)\n05:09:34     def test_sequence_mask_batching_lengths(self, x, dtype, gc, dc):\n05:09:34 \n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: \n05:09:34 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:587: in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/executors.py:58: in default_new_style_executor\n05:09:34     return function(data)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:578: in run\n05:09:34     return test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:525: in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:227: in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py:395: in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34 E   AssertionError: Gradient check failed for input data\n05:09:34 ----------------------------- Captured stdout call -----------------------------\n05:09:34 {u'lengths': , u'data': }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[126.           0.57714844   0.        ]]\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[6.         0.50292969 0.        ]]\n05:09:34 ---------------------------------- Hypothesis ----------------------------------\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[0.5]], dtype=float32), dtype=float32, gc=, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.7377987 ],\n05:09:34           [0.60064876]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7033071 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.60725623],\n05:09:34           [0.7377987 ]]]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Traceback (most recent call last):\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 672, in evaluate_test_data\n05:09:34     result = self.execute(data, collect=True)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 587, in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\n05:09:34     return function(data)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 583, in run\n05:09:34     return test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 180, in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 525, in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 227, in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py\", line 395, in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34   File \"/usr/lib/python2.7/unittest/case.py\", line 422, in assertTrue\n05:09:34     raise self.failureException(msg)\n05:09:34 AssertionError: Gradient check failed for input data\n05:09:34 \n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Falsifying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34  generated xml file: /var/lib/jenkins/workspace/caffe2_tests/python/result.xml -\n05:09:35 ---------- onnx coverage: ----------\n05:09:35 Operators (passed/loaded/total): 78/78/115\n05:09:35 ------------------------------------\n05:09:35 Operator               Attributes\n05:09:35                        (name: #values)\n05:09:35 Asin                   No attributes\n05:09:35 Slice                  axes: 3\n05:09:35                        ends: 8\n05:09:35                        starts: 7\n05:09:35 Log                    No attributes\n05:09:35 Sub                    No attributes\n05:09:35 Min                    No attributes\n05:09:35 Softsign               No attributes\n05:09:35 Reshape                No attributes\n05:09:35 Sum                    No attributes\n05:09:35 Xor                    No attributes\n05:09:35 Relu                   No attributes\n05:09:35 Upsample               mode: 1\n05:09:35                        scales: 1\n05:09:35 Add                    axis: 2\n05:09:35                        broadcast: 1\n05:09:35 Abs                    No attributes\n05:09:35 Pad                    mode: 3\n05:09:35                        pads: 4\n05:09:35                        value: 3\n05:09:35 Split                  axis: 3\n05:09:35                        split: 2\n05:09:35 TopK                   k: 1\n05:09:35                        axis: 0\n05:09:35 Selu                   alpha: 1\n05:09:35                        gamma: 1\n05:09:35 Mul                    axis: 1\n05:09:35                        broadcast: 1\n05:09:35 LSTM                   hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        activations: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35                        input_forget: 0\n05:09:35 ReduceMean             axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Acos                   No attributes\n05:09:35 Gemm                   alpha: 2\n05:09:35                        beta: 3\n05:09:35                        broadcast: 1\n05:09:35                        transA: 1\n05:09:35                        transB: 1\n05:09:35 Elu                    alpha: 1\n05:09:35 Sqrt                   No attributes\n05:09:35 Greater                No attributes\n05:09:35 Atan                   No attributes\n05:09:35 Sin                    No attributes\n05:09:35 BatchNormalization     epsilon: 4\n05:09:35                        is_test: 1\n05:09:35                        momentum: 3\n05:09:35                        spatial: 0\n05:09:35 Dropout                is_test: 1\n05:09:35                        ratio: 3\n05:09:35 Floor                  No attributes\n05:09:35 Transpose              perm: 9\n05:09:35 LeakyRelu              alpha: 3\n05:09:35 Ceil                   No attributes\n05:09:35 ReduceSum              axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Identity               No attributes\n05:09:35 AveragePool            kernel_shape: 8\n05:09:35                        pads: 5\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35                        count_include_pad: 0\n05:09:35 Reciprocal             No attributes\n05:09:35 Neg                    No attributes\n05:09:35 Not                    No attributes\n05:09:35 Squeeze                axes: 3\n05:09:35 GlobalMaxPool          No attributes\n05:09:35 Concat                 axis: 3\n05:09:35 And                    No attributes\n05:09:35 Cos                    No attributes\n05:09:35 RNN                    activations: 2\n05:09:35                        hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35 Mean                   No attributes\n05:09:35 Tanh                   No attributes\n05:09:35 Less                   No attributes\n05:09:35 Gather                 axis: 2\n05:09:35 LogSoftmax             axis: 5\n05:09:35 Equal                  No attributes\n05:09:35 Softplus               No attributes\n05:09:35 Softmax                axis: 4\n05:09:35 Exp                    No attributes\n05:09:35 Div                    No attributes\n05:09:35 GlobalAveragePool      No attributes\n05:09:35 Or                     No attributes\n05:09:35 PRelu                  No attributes\n05:09:35 ConstantFill           input_as_shape: 1\n05:09:35                        dtype: 0\n05:09:35                        extra_shape: 0\n05:09:35                        shape: 0\n05:09:35                        value: 0\n05:09:35 InstanceNormalization  epsilon: 2\n05:09:35 Unsqueeze              axes: 2\n05:09:35 Constant               value: 9\n05:09:35 Clip                   max: 3\n05:09:35                        min: 3\n05:09:35 Conv                   dilations: 6\n05:09:35                        group: 7\n05:09:35                        kernel_shape: 11\n05:09:35                        pads: 10\n05:09:35                        strides: 7\n05:09:35                        auto_pad: 0\n05:09:35 Sigmoid                No attributes\n05:09:35 Pow                    No attributes\n05:09:35 ConvTranspose          dilations: 1\n05:09:35                        group: 1\n05:09:35                        kernel_shape: 1\n05:09:35                        output_padding: 2\n05:09:35                        pads: 1\n05:09:35                        strides: 3\n05:09:35                        auto_pad: 0\n05:09:35                        output_shape: 0\n05:09:35 MaxPool                kernel_shape: 7\n05:09:35                        pads: 7\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35 LRN                    alpha: 3\n05:09:35                        beta: 2\n05:09:35                        bias: 2\n05:09:35                        size: 2\n05:09:35 ReduceMin              axes: 1\n05:09:35                        keepdims: 2\n05:09:35 MatMul                 No attributes\n05:09:35 ThresholdedRelu        alpha: 1\n05:09:35 Flatten                axis: 4\n05:09:35 Max                    No attributes\n05:09:35 Shape                  No attributes\n05:09:35 Tan                    No attributes\n05:09:35 ReduceMax              axes: 1\n05:09:35                        keepdims: 2\n</code></pre>\n<p>Testing if this is true. Full regression log: <a href=\"https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console</a></p>\n<p>Signed-off-by: Edward Z. Yang <a href=\"mailto:ezyang@fb.com\">ezyang@fb.com</a></p>", "body_text": "I believe that Hypothesis 3.59.0 --> 3.60.1 has caused a regression\n05:09:34 =================================== FAILURES ===================================\n05:09:34 ____________ TestBooleanMaskOp.test_sequence_mask_batching_lengths _____________\n05:09:34 \n05:09:34 self = <caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>\n05:09:34 \n05:09:34     @given(x=hu.tensor(min_dim=2,\n05:09:34 >                      max_dim=5,\n05:09:34                        elements=st.floats(min_value=0.5, max_value=1.0)),\n05:09:34            dtype=st.sampled_from([np.float32, np.float16]),\n05:09:34            **hu.gcs)\n05:09:34     def test_sequence_mask_batching_lengths(self, x, dtype, gc, dc):\n05:09:34 \n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: \n05:09:34 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:587: in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/executors.py:58: in default_new_style_executor\n05:09:34     return function(data)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:578: in run\n05:09:34     return test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:525: in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:227: in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py:395: in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34 E   AssertionError: Gradient check failed for input data\n05:09:34 ----------------------------- Captured stdout call -----------------------------\n05:09:34 {u'lengths': , u'data': }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[126.           0.57714844   0.        ]]\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[6.         0.50292969 0.        ]]\n05:09:34 ---------------------------------- Hypothesis ----------------------------------\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[0.5]], dtype=float32), dtype=float32, gc=, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.7377987 ],\n05:09:34           [0.60064876]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7033071 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.60725623],\n05:09:34           [0.7377987 ]]]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Traceback (most recent call last):\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 672, in evaluate_test_data\n05:09:34     result = self.execute(data, collect=True)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 587, in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\n05:09:34     return function(data)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 583, in run\n05:09:34     return test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 180, in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 525, in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 227, in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py\", line 395, in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34   File \"/usr/lib/python2.7/unittest/case.py\", line 422, in assertTrue\n05:09:34     raise self.failureException(msg)\n05:09:34 AssertionError: Gradient check failed for input data\n05:09:34 \n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Falsifying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34  generated xml file: /var/lib/jenkins/workspace/caffe2_tests/python/result.xml -\n05:09:35 ---------- onnx coverage: ----------\n05:09:35 Operators (passed/loaded/total): 78/78/115\n05:09:35 ------------------------------------\n05:09:35 Operator               Attributes\n05:09:35                        (name: #values)\n05:09:35 Asin                   No attributes\n05:09:35 Slice                  axes: 3\n05:09:35                        ends: 8\n05:09:35                        starts: 7\n05:09:35 Log                    No attributes\n05:09:35 Sub                    No attributes\n05:09:35 Min                    No attributes\n05:09:35 Softsign               No attributes\n05:09:35 Reshape                No attributes\n05:09:35 Sum                    No attributes\n05:09:35 Xor                    No attributes\n05:09:35 Relu                   No attributes\n05:09:35 Upsample               mode: 1\n05:09:35                        scales: 1\n05:09:35 Add                    axis: 2\n05:09:35                        broadcast: 1\n05:09:35 Abs                    No attributes\n05:09:35 Pad                    mode: 3\n05:09:35                        pads: 4\n05:09:35                        value: 3\n05:09:35 Split                  axis: 3\n05:09:35                        split: 2\n05:09:35 TopK                   k: 1\n05:09:35                        axis: 0\n05:09:35 Selu                   alpha: 1\n05:09:35                        gamma: 1\n05:09:35 Mul                    axis: 1\n05:09:35                        broadcast: 1\n05:09:35 LSTM                   hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        activations: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35                        input_forget: 0\n05:09:35 ReduceMean             axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Acos                   No attributes\n05:09:35 Gemm                   alpha: 2\n05:09:35                        beta: 3\n05:09:35                        broadcast: 1\n05:09:35                        transA: 1\n05:09:35                        transB: 1\n05:09:35 Elu                    alpha: 1\n05:09:35 Sqrt                   No attributes\n05:09:35 Greater                No attributes\n05:09:35 Atan                   No attributes\n05:09:35 Sin                    No attributes\n05:09:35 BatchNormalization     epsilon: 4\n05:09:35                        is_test: 1\n05:09:35                        momentum: 3\n05:09:35                        spatial: 0\n05:09:35 Dropout                is_test: 1\n05:09:35                        ratio: 3\n05:09:35 Floor                  No attributes\n05:09:35 Transpose              perm: 9\n05:09:35 LeakyRelu              alpha: 3\n05:09:35 Ceil                   No attributes\n05:09:35 ReduceSum              axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Identity               No attributes\n05:09:35 AveragePool            kernel_shape: 8\n05:09:35                        pads: 5\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35                        count_include_pad: 0\n05:09:35 Reciprocal             No attributes\n05:09:35 Neg                    No attributes\n05:09:35 Not                    No attributes\n05:09:35 Squeeze                axes: 3\n05:09:35 GlobalMaxPool          No attributes\n05:09:35 Concat                 axis: 3\n05:09:35 And                    No attributes\n05:09:35 Cos                    No attributes\n05:09:35 RNN                    activations: 2\n05:09:35                        hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35 Mean                   No attributes\n05:09:35 Tanh                   No attributes\n05:09:35 Less                   No attributes\n05:09:35 Gather                 axis: 2\n05:09:35 LogSoftmax             axis: 5\n05:09:35 Equal                  No attributes\n05:09:35 Softplus               No attributes\n05:09:35 Softmax                axis: 4\n05:09:35 Exp                    No attributes\n05:09:35 Div                    No attributes\n05:09:35 GlobalAveragePool      No attributes\n05:09:35 Or                     No attributes\n05:09:35 PRelu                  No attributes\n05:09:35 ConstantFill           input_as_shape: 1\n05:09:35                        dtype: 0\n05:09:35                        extra_shape: 0\n05:09:35                        shape: 0\n05:09:35                        value: 0\n05:09:35 InstanceNormalization  epsilon: 2\n05:09:35 Unsqueeze              axes: 2\n05:09:35 Constant               value: 9\n05:09:35 Clip                   max: 3\n05:09:35                        min: 3\n05:09:35 Conv                   dilations: 6\n05:09:35                        group: 7\n05:09:35                        kernel_shape: 11\n05:09:35                        pads: 10\n05:09:35                        strides: 7\n05:09:35                        auto_pad: 0\n05:09:35 Sigmoid                No attributes\n05:09:35 Pow                    No attributes\n05:09:35 ConvTranspose          dilations: 1\n05:09:35                        group: 1\n05:09:35                        kernel_shape: 1\n05:09:35                        output_padding: 2\n05:09:35                        pads: 1\n05:09:35                        strides: 3\n05:09:35                        auto_pad: 0\n05:09:35                        output_shape: 0\n05:09:35 MaxPool                kernel_shape: 7\n05:09:35                        pads: 7\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35 LRN                    alpha: 3\n05:09:35                        beta: 2\n05:09:35                        bias: 2\n05:09:35                        size: 2\n05:09:35 ReduceMin              axes: 1\n05:09:35                        keepdims: 2\n05:09:35 MatMul                 No attributes\n05:09:35 ThresholdedRelu        alpha: 1\n05:09:35 Flatten                axis: 4\n05:09:35 Max                    No attributes\n05:09:35 Shape                  No attributes\n05:09:35 Tan                    No attributes\n05:09:35 ReduceMax              axes: 1\n05:09:35                        keepdims: 2\n\nTesting if this is true. Full regression log: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console\nSigned-off-by: Edward Z. Yang ezyang@fb.com", "body": "I believe that Hypothesis 3.59.0 --> 3.60.1 has caused a regression\r\n\r\n```\r\n05:09:34 =================================== FAILURES ===================================\r\n05:09:34 ____________ TestBooleanMaskOp.test_sequence_mask_batching_lengths _____________\r\n05:09:34 \r\n05:09:34 self = <caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>\r\n05:09:34 \r\n05:09:34     @given(x=hu.tensor(min_dim=2,\r\n05:09:34 >                      max_dim=5,\r\n05:09:34                        elements=st.floats(min_value=0.5, max_value=1.0)),\r\n05:09:34            dtype=st.sampled_from([np.float32, np.float16]),\r\n05:09:34            **hu.gcs)\r\n05:09:34     def test_sequence_mask_batching_lengths(self, x, dtype, gc, dc):\r\n05:09:34 \r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: \r\n05:09:34 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:587: in execute\r\n05:09:34     result = self.test_runner(data, run)\r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/executors.py:58: in default_new_style_executor\r\n05:09:34     return function(data)\r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:578: in run\r\n05:09:34     return test(*args, **kwargs)\r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: in test_sequence_mask_batching_lengths\r\n05:09:34     max_dim=5,\r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:525: in test\r\n05:09:34     result = self.test(*args, **kwargs)\r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:227: in test_sequence_mask_batching_lengths\r\n05:09:34     threshold=threshold)\r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py:395: in assertGradientChecks\r\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\r\n05:09:34 E   AssertionError: Gradient check failed for input data\r\n05:09:34 ----------------------------- Captured stdout call -----------------------------\r\n05:09:34 {u'lengths': , u'data': }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 Failed. [idx, grad, grad_estimate] are:\r\n05:09:34 [[126.           0.57714844   0.        ]]\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 Failed. [idx, grad, grad_estimate] are:\r\n05:09:34 [[6.         0.50292969 0.        ]]\r\n05:09:34 ---------------------------------- Hypothesis ----------------------------------\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[0.5]], dtype=float32), dtype=float32, gc=, dc=[, device_type: 1])\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.7377987 ],\r\n05:09:34           [0.60064876]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.7377987 ],\r\n05:09:34           [0.7377987 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.7377987 ],\r\n05:09:34           [0.7033071 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.7377987 ],\r\n05:09:34           [0.7377987 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.60725623],\r\n05:09:34           [0.7377987 ]]]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\r\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\r\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Traceback (most recent call last):\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 672, in evaluate_test_data\r\n05:09:34     result = self.execute(data, collect=True)\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 587, in execute\r\n05:09:34     result = self.test_runner(data, run)\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\r\n05:09:34     return function(data)\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 583, in run\r\n05:09:34     return test(*args, **kwargs)\r\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 180, in test_sequence_mask_batching_lengths\r\n05:09:34     max_dim=5,\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 525, in test\r\n05:09:34     result = self.test(*args, **kwargs)\r\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 227, in test_sequence_mask_batching_lengths\r\n05:09:34     threshold=threshold)\r\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py\", line 395, in assertGradientChecks\r\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\r\n05:09:34   File \"/usr/lib/python2.7/unittest/case.py\", line 422, in assertTrue\r\n05:09:34     raise self.failureException(msg)\r\n05:09:34 AssertionError: Gradient check failed for input data\r\n05:09:34 \r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\r\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\r\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Falsifying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\r\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\r\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34  generated xml file: /var/lib/jenkins/workspace/caffe2_tests/python/result.xml -\r\n05:09:35 ---------- onnx coverage: ----------\r\n05:09:35 Operators (passed/loaded/total): 78/78/115\r\n05:09:35 ------------------------------------\r\n05:09:35 Operator               Attributes\r\n05:09:35                        (name: #values)\r\n05:09:35 Asin                   No attributes\r\n05:09:35 Slice                  axes: 3\r\n05:09:35                        ends: 8\r\n05:09:35                        starts: 7\r\n05:09:35 Log                    No attributes\r\n05:09:35 Sub                    No attributes\r\n05:09:35 Min                    No attributes\r\n05:09:35 Softsign               No attributes\r\n05:09:35 Reshape                No attributes\r\n05:09:35 Sum                    No attributes\r\n05:09:35 Xor                    No attributes\r\n05:09:35 Relu                   No attributes\r\n05:09:35 Upsample               mode: 1\r\n05:09:35                        scales: 1\r\n05:09:35 Add                    axis: 2\r\n05:09:35                        broadcast: 1\r\n05:09:35 Abs                    No attributes\r\n05:09:35 Pad                    mode: 3\r\n05:09:35                        pads: 4\r\n05:09:35                        value: 3\r\n05:09:35 Split                  axis: 3\r\n05:09:35                        split: 2\r\n05:09:35 TopK                   k: 1\r\n05:09:35                        axis: 0\r\n05:09:35 Selu                   alpha: 1\r\n05:09:35                        gamma: 1\r\n05:09:35 Mul                    axis: 1\r\n05:09:35                        broadcast: 1\r\n05:09:35 LSTM                   hidden_size: 1\r\n05:09:35                        activation_alpha: 0\r\n05:09:35                        activation_beta: 0\r\n05:09:35                        activations: 0\r\n05:09:35                        clip: 0\r\n05:09:35                        direction: 0\r\n05:09:35                        input_forget: 0\r\n05:09:35 ReduceMean             axes: 2\r\n05:09:35                        keepdims: 2\r\n05:09:35 Acos                   No attributes\r\n05:09:35 Gemm                   alpha: 2\r\n05:09:35                        beta: 3\r\n05:09:35                        broadcast: 1\r\n05:09:35                        transA: 1\r\n05:09:35                        transB: 1\r\n05:09:35 Elu                    alpha: 1\r\n05:09:35 Sqrt                   No attributes\r\n05:09:35 Greater                No attributes\r\n05:09:35 Atan                   No attributes\r\n05:09:35 Sin                    No attributes\r\n05:09:35 BatchNormalization     epsilon: 4\r\n05:09:35                        is_test: 1\r\n05:09:35                        momentum: 3\r\n05:09:35                        spatial: 0\r\n05:09:35 Dropout                is_test: 1\r\n05:09:35                        ratio: 3\r\n05:09:35 Floor                  No attributes\r\n05:09:35 Transpose              perm: 9\r\n05:09:35 LeakyRelu              alpha: 3\r\n05:09:35 Ceil                   No attributes\r\n05:09:35 ReduceSum              axes: 2\r\n05:09:35                        keepdims: 2\r\n05:09:35 Identity               No attributes\r\n05:09:35 AveragePool            kernel_shape: 8\r\n05:09:35                        pads: 5\r\n05:09:35                        strides: 6\r\n05:09:35                        auto_pad: 0\r\n05:09:35                        count_include_pad: 0\r\n05:09:35 Reciprocal             No attributes\r\n05:09:35 Neg                    No attributes\r\n05:09:35 Not                    No attributes\r\n05:09:35 Squeeze                axes: 3\r\n05:09:35 GlobalMaxPool          No attributes\r\n05:09:35 Concat                 axis: 3\r\n05:09:35 And                    No attributes\r\n05:09:35 Cos                    No attributes\r\n05:09:35 RNN                    activations: 2\r\n05:09:35                        hidden_size: 1\r\n05:09:35                        activation_alpha: 0\r\n05:09:35                        activation_beta: 0\r\n05:09:35                        clip: 0\r\n05:09:35                        direction: 0\r\n05:09:35 Mean                   No attributes\r\n05:09:35 Tanh                   No attributes\r\n05:09:35 Less                   No attributes\r\n05:09:35 Gather                 axis: 2\r\n05:09:35 LogSoftmax             axis: 5\r\n05:09:35 Equal                  No attributes\r\n05:09:35 Softplus               No attributes\r\n05:09:35 Softmax                axis: 4\r\n05:09:35 Exp                    No attributes\r\n05:09:35 Div                    No attributes\r\n05:09:35 GlobalAveragePool      No attributes\r\n05:09:35 Or                     No attributes\r\n05:09:35 PRelu                  No attributes\r\n05:09:35 ConstantFill           input_as_shape: 1\r\n05:09:35                        dtype: 0\r\n05:09:35                        extra_shape: 0\r\n05:09:35                        shape: 0\r\n05:09:35                        value: 0\r\n05:09:35 InstanceNormalization  epsilon: 2\r\n05:09:35 Unsqueeze              axes: 2\r\n05:09:35 Constant               value: 9\r\n05:09:35 Clip                   max: 3\r\n05:09:35                        min: 3\r\n05:09:35 Conv                   dilations: 6\r\n05:09:35                        group: 7\r\n05:09:35                        kernel_shape: 11\r\n05:09:35                        pads: 10\r\n05:09:35                        strides: 7\r\n05:09:35                        auto_pad: 0\r\n05:09:35 Sigmoid                No attributes\r\n05:09:35 Pow                    No attributes\r\n05:09:35 ConvTranspose          dilations: 1\r\n05:09:35                        group: 1\r\n05:09:35                        kernel_shape: 1\r\n05:09:35                        output_padding: 2\r\n05:09:35                        pads: 1\r\n05:09:35                        strides: 3\r\n05:09:35                        auto_pad: 0\r\n05:09:35                        output_shape: 0\r\n05:09:35 MaxPool                kernel_shape: 7\r\n05:09:35                        pads: 7\r\n05:09:35                        strides: 6\r\n05:09:35                        auto_pad: 0\r\n05:09:35 LRN                    alpha: 3\r\n05:09:35                        beta: 2\r\n05:09:35                        bias: 2\r\n05:09:35                        size: 2\r\n05:09:35 ReduceMin              axes: 1\r\n05:09:35                        keepdims: 2\r\n05:09:35 MatMul                 No attributes\r\n05:09:35 ThresholdedRelu        alpha: 1\r\n05:09:35 Flatten                axis: 4\r\n05:09:35 Max                    No attributes\r\n05:09:35 Shape                  No attributes\r\n05:09:35 Tan                    No attributes\r\n05:09:35 ReduceMax              axes: 1\r\n05:09:35                        keepdims: 2\r\n```\r\n\r\nTesting if this is true. Full regression log: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\n"}