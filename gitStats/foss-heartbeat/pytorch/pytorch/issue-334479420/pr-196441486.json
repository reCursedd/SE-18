{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8742", "id": 196441486, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk2NDQxNDg2", "html_url": "https://github.com/pytorch/pytorch/pull/8742", "diff_url": "https://github.com/pytorch/pytorch/pull/8742.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8742.patch", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8742", "number": 8742, "state": "closed", "locked": false, "title": "[TESTING ONLY] Test Hypothesis 3.60.1", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "I believe that Hypothesis 3.59.0 --> 3.60.1 has caused a regression\r\n\r\n```\r\n05:09:34 =================================== FAILURES ===================================\r\n05:09:34 ____________ TestBooleanMaskOp.test_sequence_mask_batching_lengths _____________\r\n05:09:34 \r\n05:09:34 self = <caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>\r\n05:09:34 \r\n05:09:34     @given(x=hu.tensor(min_dim=2,\r\n05:09:34 >                      max_dim=5,\r\n05:09:34                        elements=st.floats(min_value=0.5, max_value=1.0)),\r\n05:09:34            dtype=st.sampled_from([np.float32, np.float16]),\r\n05:09:34            **hu.gcs)\r\n05:09:34     def test_sequence_mask_batching_lengths(self, x, dtype, gc, dc):\r\n05:09:34 \r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: \r\n05:09:34 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:587: in execute\r\n05:09:34     result = self.test_runner(data, run)\r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/executors.py:58: in default_new_style_executor\r\n05:09:34     return function(data)\r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:578: in run\r\n05:09:34     return test(*args, **kwargs)\r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: in test_sequence_mask_batching_lengths\r\n05:09:34     max_dim=5,\r\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:525: in test\r\n05:09:34     result = self.test(*args, **kwargs)\r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:227: in test_sequence_mask_batching_lengths\r\n05:09:34     threshold=threshold)\r\n05:09:34 lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py:395: in assertGradientChecks\r\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\r\n05:09:34 E   AssertionError: Gradient check failed for input data\r\n05:09:34 ----------------------------- Captured stdout call -----------------------------\r\n05:09:34 {u'lengths': , u'data': }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 Failed. [idx, grad, grad_estimate] are:\r\n05:09:34 [[126.           0.57714844   0.        ]]\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 {u'lengths': device_type: 1\r\n05:09:34 , u'data': device_type: 1\r\n05:09:34 }\r\n05:09:34 Failed. [idx, grad, grad_estimate] are:\r\n05:09:34 [[6.         0.50292969 0.        ]]\r\n05:09:34 ---------------------------------- Hypothesis ----------------------------------\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[0.5]], dtype=float32), dtype=float32, gc=, dc=[, device_type: 1])\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.7377987 ],\r\n05:09:34           [0.60064876]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.7377987 ],\r\n05:09:34           [0.7377987 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.7377987 ],\r\n05:09:34           [0.7033071 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.7377987 ],\r\n05:09:34           [0.7377987 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.60725623],\r\n05:09:34           [0.7377987 ]]]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\r\n05:09:34 \r\n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\r\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\r\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\r\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Traceback (most recent call last):\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 672, in evaluate_test_data\r\n05:09:34     result = self.execute(data, collect=True)\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 587, in execute\r\n05:09:34     result = self.test_runner(data, run)\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\r\n05:09:34     return function(data)\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 583, in run\r\n05:09:34     return test(*args, **kwargs)\r\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 180, in test_sequence_mask_batching_lengths\r\n05:09:34     max_dim=5,\r\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 525, in test\r\n05:09:34     result = self.test(*args, **kwargs)\r\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 227, in test_sequence_mask_batching_lengths\r\n05:09:34     threshold=threshold)\r\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py\", line 395, in assertGradientChecks\r\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\r\n05:09:34   File \"/usr/lib/python2.7/unittest/case.py\", line 422, in assertTrue\r\n05:09:34     raise self.failureException(msg)\r\n05:09:34 AssertionError: Gradient check failed for input data\r\n05:09:34 \r\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\r\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\r\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34 Falsifying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\r\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34 \r\n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\r\n05:09:34 \r\n05:09:34 \r\n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\r\n05:09:34 \r\n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\r\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\r\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\r\n05:09:34  generated xml file: /var/lib/jenkins/workspace/caffe2_tests/python/result.xml -\r\n05:09:35 ---------- onnx coverage: ----------\r\n05:09:35 Operators (passed/loaded/total): 78/78/115\r\n05:09:35 ------------------------------------\r\n05:09:35 Operator               Attributes\r\n05:09:35                        (name: #values)\r\n05:09:35 Asin                   No attributes\r\n05:09:35 Slice                  axes: 3\r\n05:09:35                        ends: 8\r\n05:09:35                        starts: 7\r\n05:09:35 Log                    No attributes\r\n05:09:35 Sub                    No attributes\r\n05:09:35 Min                    No attributes\r\n05:09:35 Softsign               No attributes\r\n05:09:35 Reshape                No attributes\r\n05:09:35 Sum                    No attributes\r\n05:09:35 Xor                    No attributes\r\n05:09:35 Relu                   No attributes\r\n05:09:35 Upsample               mode: 1\r\n05:09:35                        scales: 1\r\n05:09:35 Add                    axis: 2\r\n05:09:35                        broadcast: 1\r\n05:09:35 Abs                    No attributes\r\n05:09:35 Pad                    mode: 3\r\n05:09:35                        pads: 4\r\n05:09:35                        value: 3\r\n05:09:35 Split                  axis: 3\r\n05:09:35                        split: 2\r\n05:09:35 TopK                   k: 1\r\n05:09:35                        axis: 0\r\n05:09:35 Selu                   alpha: 1\r\n05:09:35                        gamma: 1\r\n05:09:35 Mul                    axis: 1\r\n05:09:35                        broadcast: 1\r\n05:09:35 LSTM                   hidden_size: 1\r\n05:09:35                        activation_alpha: 0\r\n05:09:35                        activation_beta: 0\r\n05:09:35                        activations: 0\r\n05:09:35                        clip: 0\r\n05:09:35                        direction: 0\r\n05:09:35                        input_forget: 0\r\n05:09:35 ReduceMean             axes: 2\r\n05:09:35                        keepdims: 2\r\n05:09:35 Acos                   No attributes\r\n05:09:35 Gemm                   alpha: 2\r\n05:09:35                        beta: 3\r\n05:09:35                        broadcast: 1\r\n05:09:35                        transA: 1\r\n05:09:35                        transB: 1\r\n05:09:35 Elu                    alpha: 1\r\n05:09:35 Sqrt                   No attributes\r\n05:09:35 Greater                No attributes\r\n05:09:35 Atan                   No attributes\r\n05:09:35 Sin                    No attributes\r\n05:09:35 BatchNormalization     epsilon: 4\r\n05:09:35                        is_test: 1\r\n05:09:35                        momentum: 3\r\n05:09:35                        spatial: 0\r\n05:09:35 Dropout                is_test: 1\r\n05:09:35                        ratio: 3\r\n05:09:35 Floor                  No attributes\r\n05:09:35 Transpose              perm: 9\r\n05:09:35 LeakyRelu              alpha: 3\r\n05:09:35 Ceil                   No attributes\r\n05:09:35 ReduceSum              axes: 2\r\n05:09:35                        keepdims: 2\r\n05:09:35 Identity               No attributes\r\n05:09:35 AveragePool            kernel_shape: 8\r\n05:09:35                        pads: 5\r\n05:09:35                        strides: 6\r\n05:09:35                        auto_pad: 0\r\n05:09:35                        count_include_pad: 0\r\n05:09:35 Reciprocal             No attributes\r\n05:09:35 Neg                    No attributes\r\n05:09:35 Not                    No attributes\r\n05:09:35 Squeeze                axes: 3\r\n05:09:35 GlobalMaxPool          No attributes\r\n05:09:35 Concat                 axis: 3\r\n05:09:35 And                    No attributes\r\n05:09:35 Cos                    No attributes\r\n05:09:35 RNN                    activations: 2\r\n05:09:35                        hidden_size: 1\r\n05:09:35                        activation_alpha: 0\r\n05:09:35                        activation_beta: 0\r\n05:09:35                        clip: 0\r\n05:09:35                        direction: 0\r\n05:09:35 Mean                   No attributes\r\n05:09:35 Tanh                   No attributes\r\n05:09:35 Less                   No attributes\r\n05:09:35 Gather                 axis: 2\r\n05:09:35 LogSoftmax             axis: 5\r\n05:09:35 Equal                  No attributes\r\n05:09:35 Softplus               No attributes\r\n05:09:35 Softmax                axis: 4\r\n05:09:35 Exp                    No attributes\r\n05:09:35 Div                    No attributes\r\n05:09:35 GlobalAveragePool      No attributes\r\n05:09:35 Or                     No attributes\r\n05:09:35 PRelu                  No attributes\r\n05:09:35 ConstantFill           input_as_shape: 1\r\n05:09:35                        dtype: 0\r\n05:09:35                        extra_shape: 0\r\n05:09:35                        shape: 0\r\n05:09:35                        value: 0\r\n05:09:35 InstanceNormalization  epsilon: 2\r\n05:09:35 Unsqueeze              axes: 2\r\n05:09:35 Constant               value: 9\r\n05:09:35 Clip                   max: 3\r\n05:09:35                        min: 3\r\n05:09:35 Conv                   dilations: 6\r\n05:09:35                        group: 7\r\n05:09:35                        kernel_shape: 11\r\n05:09:35                        pads: 10\r\n05:09:35                        strides: 7\r\n05:09:35                        auto_pad: 0\r\n05:09:35 Sigmoid                No attributes\r\n05:09:35 Pow                    No attributes\r\n05:09:35 ConvTranspose          dilations: 1\r\n05:09:35                        group: 1\r\n05:09:35                        kernel_shape: 1\r\n05:09:35                        output_padding: 2\r\n05:09:35                        pads: 1\r\n05:09:35                        strides: 3\r\n05:09:35                        auto_pad: 0\r\n05:09:35                        output_shape: 0\r\n05:09:35 MaxPool                kernel_shape: 7\r\n05:09:35                        pads: 7\r\n05:09:35                        strides: 6\r\n05:09:35                        auto_pad: 0\r\n05:09:35 LRN                    alpha: 3\r\n05:09:35                        beta: 2\r\n05:09:35                        bias: 2\r\n05:09:35                        size: 2\r\n05:09:35 ReduceMin              axes: 1\r\n05:09:35                        keepdims: 2\r\n05:09:35 MatMul                 No attributes\r\n05:09:35 ThresholdedRelu        alpha: 1\r\n05:09:35 Flatten                axis: 4\r\n05:09:35 Max                    No attributes\r\n05:09:35 Shape                  No attributes\r\n05:09:35 Tan                    No attributes\r\n05:09:35 ReduceMax              axes: 1\r\n05:09:35                        keepdims: 2\r\n```\r\n\r\nTesting if this is true. Full regression log: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\n", "created_at": "2018-06-21T12:51:56Z", "updated_at": "2018-07-25T21:40:44Z", "closed_at": "2018-07-25T21:40:44Z", "merged_at": null, "merge_commit_sha": "47ff3a3fec7ac6f5e57ba6c86131788707314b22", "assignee": null, "assignees": [], "requested_reviewers": [], "requested_teams": [], "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "milestone": null, "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8742/commits", "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8742/comments", "review_comment_url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8742/comments", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/a904e3abb735d3dcf61bbfea79f24d388cee2265", "head": {"label": "ezyang:pr/test-hypothesis-3.60.1", "ref": "pr/test-hypothesis-3.60.1", "sha": "a904e3abb735d3dcf61bbfea79f24d388cee2265", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "repo": {"id": 101798885, "node_id": "MDEwOlJlcG9zaXRvcnkxMDE3OTg4ODU=", "name": "pytorch", "full_name": "ezyang/pytorch", "private": false, "owner": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "html_url": "https://github.com/ezyang/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": true, "url": "https://api.github.com/repos/ezyang/pytorch", "forks_url": "https://api.github.com/repos/ezyang/pytorch/forks", "keys_url": "https://api.github.com/repos/ezyang/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/ezyang/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/ezyang/pytorch/teams", "hooks_url": "https://api.github.com/repos/ezyang/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/ezyang/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/ezyang/pytorch/events", "assignees_url": "https://api.github.com/repos/ezyang/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/ezyang/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/ezyang/pytorch/tags", "blobs_url": "https://api.github.com/repos/ezyang/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/ezyang/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/ezyang/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/ezyang/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/ezyang/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/ezyang/pytorch/languages", "stargazers_url": "https://api.github.com/repos/ezyang/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/ezyang/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/ezyang/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/ezyang/pytorch/subscription", "commits_url": "https://api.github.com/repos/ezyang/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/ezyang/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/ezyang/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/ezyang/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/ezyang/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/ezyang/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/ezyang/pytorch/merges", "archive_url": "https://api.github.com/repos/ezyang/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/ezyang/pytorch/downloads", "issues_url": "https://api.github.com/repos/ezyang/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/ezyang/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/ezyang/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/ezyang/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/ezyang/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/ezyang/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/ezyang/pytorch/deployments", "created_at": "2017-08-29T19:28:39Z", "updated_at": "2018-10-29T15:06:40Z", "pushed_at": "2018-11-21T22:30:09Z", "git_url": "git://github.com/ezyang/pytorch.git", "ssh_url": "git@github.com:ezyang/pytorch.git", "clone_url": "https://github.com/ezyang/pytorch.git", "svn_url": "https://github.com/ezyang/pytorch", "homepage": "http://pytorch.org", "size": 88254, "stargazers_count": 1, "watchers_count": 1, "language": "C++", "has_issues": false, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 0, "mirror_url": null, "archived": false, "open_issues_count": 2, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 0, "open_issues": 2, "watchers": 1, "default_branch": "master"}}, "base": {"label": "pytorch:master", "ref": "master", "sha": "9dffaf593e8c58a6d02583079162f4a88cb1bc66", "user": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "repo": {"id": 65600975, "node_id": "MDEwOlJlcG9zaXRvcnk2NTYwMDk3NQ==", "name": "pytorch", "full_name": "pytorch/pytorch", "private": false, "owner": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "html_url": "https://github.com/pytorch/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": false, "url": "https://api.github.com/repos/pytorch/pytorch", "forks_url": "https://api.github.com/repos/pytorch/pytorch/forks", "keys_url": "https://api.github.com/repos/pytorch/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/pytorch/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/pytorch/pytorch/teams", "hooks_url": "https://api.github.com/repos/pytorch/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/pytorch/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/pytorch/pytorch/events", "assignees_url": "https://api.github.com/repos/pytorch/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/pytorch/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/pytorch/pytorch/tags", "blobs_url": "https://api.github.com/repos/pytorch/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/pytorch/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/pytorch/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/pytorch/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/pytorch/pytorch/languages", "stargazers_url": "https://api.github.com/repos/pytorch/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/pytorch/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/pytorch/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/pytorch/pytorch/subscription", "commits_url": "https://api.github.com/repos/pytorch/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/pytorch/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/pytorch/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/pytorch/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/pytorch/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/pytorch/pytorch/merges", "archive_url": "https://api.github.com/repos/pytorch/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/pytorch/pytorch/downloads", "issues_url": "https://api.github.com/repos/pytorch/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/pytorch/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/pytorch/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/pytorch/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/pytorch/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/pytorch/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/pytorch/pytorch/deployments", "created_at": "2016-08-13T05:26:41Z", "updated_at": "2018-11-24T15:34:47Z", "pushed_at": "2018-11-24T12:42:01Z", "git_url": "git://github.com/pytorch/pytorch.git", "ssh_url": "git@github.com:pytorch/pytorch.git", "clone_url": "https://github.com/pytorch/pytorch.git", "svn_url": "https://github.com/pytorch/pytorch", "homepage": "http://pytorch.org", "size": 89656, "stargazers_count": 21592, "watchers_count": 21592, "language": "C++", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 5154, "mirror_url": null, "archived": false, "open_issues_count": 2196, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 5154, "open_issues": 2196, "watchers": 21592, "default_branch": "master"}}, "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8742"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8742"}, "issue": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/8742"}, "comments": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/8742/comments"}, "review_comments": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8742/comments"}, "review_comment": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}"}, "commits": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8742/commits"}, "statuses": {"href": "https://api.github.com/repos/pytorch/pytorch/statuses/a904e3abb735d3dcf61bbfea79f24d388cee2265"}}, "author_association": "CONTRIBUTOR", "body_html": "<p>I believe that Hypothesis 3.59.0 --&gt; 3.60.1 has caused a regression</p>\n<pre><code>05:09:34 =================================== FAILURES ===================================\n05:09:34 ____________ TestBooleanMaskOp.test_sequence_mask_batching_lengths _____________\n05:09:34 \n05:09:34 self = &lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;\n05:09:34 \n05:09:34     @given(x=hu.tensor(min_dim=2,\n05:09:34 &gt;                      max_dim=5,\n05:09:34                        elements=st.floats(min_value=0.5, max_value=1.0)),\n05:09:34            dtype=st.sampled_from([np.float32, np.float16]),\n05:09:34            **hu.gcs)\n05:09:34     def test_sequence_mask_batching_lengths(self, x, dtype, gc, dc):\n05:09:34 \n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: \n05:09:34 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:587: in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/executors.py:58: in default_new_style_executor\n05:09:34     return function(data)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:578: in run\n05:09:34     return test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:525: in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:227: in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py:395: in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34 E   AssertionError: Gradient check failed for input data\n05:09:34 ----------------------------- Captured stdout call -----------------------------\n05:09:34 {u'lengths': , u'data': }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[126.           0.57714844   0.        ]]\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[6.         0.50292969 0.        ]]\n05:09:34 ---------------------------------- Hypothesis ----------------------------------\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[0.5]], dtype=float32), dtype=float32, gc=, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.7377987 ],\n05:09:34           [0.60064876]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7033071 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.60725623],\n05:09:34           [0.7377987 ]]]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Traceback (most recent call last):\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 672, in evaluate_test_data\n05:09:34     result = self.execute(data, collect=True)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 587, in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\n05:09:34     return function(data)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 583, in run\n05:09:34     return test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 180, in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 525, in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 227, in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py\", line 395, in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34   File \"/usr/lib/python2.7/unittest/case.py\", line 422, in assertTrue\n05:09:34     raise self.failureException(msg)\n05:09:34 AssertionError: Gradient check failed for input data\n05:09:34 \n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Falsifying example: test_sequence_mask_batching_lengths(self=&lt;caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths&gt;, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34  generated xml file: /var/lib/jenkins/workspace/caffe2_tests/python/result.xml -\n05:09:35 ---------- onnx coverage: ----------\n05:09:35 Operators (passed/loaded/total): 78/78/115\n05:09:35 ------------------------------------\n05:09:35 Operator               Attributes\n05:09:35                        (name: #values)\n05:09:35 Asin                   No attributes\n05:09:35 Slice                  axes: 3\n05:09:35                        ends: 8\n05:09:35                        starts: 7\n05:09:35 Log                    No attributes\n05:09:35 Sub                    No attributes\n05:09:35 Min                    No attributes\n05:09:35 Softsign               No attributes\n05:09:35 Reshape                No attributes\n05:09:35 Sum                    No attributes\n05:09:35 Xor                    No attributes\n05:09:35 Relu                   No attributes\n05:09:35 Upsample               mode: 1\n05:09:35                        scales: 1\n05:09:35 Add                    axis: 2\n05:09:35                        broadcast: 1\n05:09:35 Abs                    No attributes\n05:09:35 Pad                    mode: 3\n05:09:35                        pads: 4\n05:09:35                        value: 3\n05:09:35 Split                  axis: 3\n05:09:35                        split: 2\n05:09:35 TopK                   k: 1\n05:09:35                        axis: 0\n05:09:35 Selu                   alpha: 1\n05:09:35                        gamma: 1\n05:09:35 Mul                    axis: 1\n05:09:35                        broadcast: 1\n05:09:35 LSTM                   hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        activations: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35                        input_forget: 0\n05:09:35 ReduceMean             axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Acos                   No attributes\n05:09:35 Gemm                   alpha: 2\n05:09:35                        beta: 3\n05:09:35                        broadcast: 1\n05:09:35                        transA: 1\n05:09:35                        transB: 1\n05:09:35 Elu                    alpha: 1\n05:09:35 Sqrt                   No attributes\n05:09:35 Greater                No attributes\n05:09:35 Atan                   No attributes\n05:09:35 Sin                    No attributes\n05:09:35 BatchNormalization     epsilon: 4\n05:09:35                        is_test: 1\n05:09:35                        momentum: 3\n05:09:35                        spatial: 0\n05:09:35 Dropout                is_test: 1\n05:09:35                        ratio: 3\n05:09:35 Floor                  No attributes\n05:09:35 Transpose              perm: 9\n05:09:35 LeakyRelu              alpha: 3\n05:09:35 Ceil                   No attributes\n05:09:35 ReduceSum              axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Identity               No attributes\n05:09:35 AveragePool            kernel_shape: 8\n05:09:35                        pads: 5\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35                        count_include_pad: 0\n05:09:35 Reciprocal             No attributes\n05:09:35 Neg                    No attributes\n05:09:35 Not                    No attributes\n05:09:35 Squeeze                axes: 3\n05:09:35 GlobalMaxPool          No attributes\n05:09:35 Concat                 axis: 3\n05:09:35 And                    No attributes\n05:09:35 Cos                    No attributes\n05:09:35 RNN                    activations: 2\n05:09:35                        hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35 Mean                   No attributes\n05:09:35 Tanh                   No attributes\n05:09:35 Less                   No attributes\n05:09:35 Gather                 axis: 2\n05:09:35 LogSoftmax             axis: 5\n05:09:35 Equal                  No attributes\n05:09:35 Softplus               No attributes\n05:09:35 Softmax                axis: 4\n05:09:35 Exp                    No attributes\n05:09:35 Div                    No attributes\n05:09:35 GlobalAveragePool      No attributes\n05:09:35 Or                     No attributes\n05:09:35 PRelu                  No attributes\n05:09:35 ConstantFill           input_as_shape: 1\n05:09:35                        dtype: 0\n05:09:35                        extra_shape: 0\n05:09:35                        shape: 0\n05:09:35                        value: 0\n05:09:35 InstanceNormalization  epsilon: 2\n05:09:35 Unsqueeze              axes: 2\n05:09:35 Constant               value: 9\n05:09:35 Clip                   max: 3\n05:09:35                        min: 3\n05:09:35 Conv                   dilations: 6\n05:09:35                        group: 7\n05:09:35                        kernel_shape: 11\n05:09:35                        pads: 10\n05:09:35                        strides: 7\n05:09:35                        auto_pad: 0\n05:09:35 Sigmoid                No attributes\n05:09:35 Pow                    No attributes\n05:09:35 ConvTranspose          dilations: 1\n05:09:35                        group: 1\n05:09:35                        kernel_shape: 1\n05:09:35                        output_padding: 2\n05:09:35                        pads: 1\n05:09:35                        strides: 3\n05:09:35                        auto_pad: 0\n05:09:35                        output_shape: 0\n05:09:35 MaxPool                kernel_shape: 7\n05:09:35                        pads: 7\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35 LRN                    alpha: 3\n05:09:35                        beta: 2\n05:09:35                        bias: 2\n05:09:35                        size: 2\n05:09:35 ReduceMin              axes: 1\n05:09:35                        keepdims: 2\n05:09:35 MatMul                 No attributes\n05:09:35 ThresholdedRelu        alpha: 1\n05:09:35 Flatten                axis: 4\n05:09:35 Max                    No attributes\n05:09:35 Shape                  No attributes\n05:09:35 Tan                    No attributes\n05:09:35 ReduceMax              axes: 1\n05:09:35                        keepdims: 2\n</code></pre>\n<p>Testing if this is true. Full regression log: <a href=\"https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console</a></p>\n<p>Signed-off-by: Edward Z. Yang <a href=\"mailto:ezyang@fb.com\">ezyang@fb.com</a></p>", "body_text": "I believe that Hypothesis 3.59.0 --> 3.60.1 has caused a regression\n05:09:34 =================================== FAILURES ===================================\n05:09:34 ____________ TestBooleanMaskOp.test_sequence_mask_batching_lengths _____________\n05:09:34 \n05:09:34 self = <caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>\n05:09:34 \n05:09:34     @given(x=hu.tensor(min_dim=2,\n05:09:34 >                      max_dim=5,\n05:09:34                        elements=st.floats(min_value=0.5, max_value=1.0)),\n05:09:34            dtype=st.sampled_from([np.float32, np.float16]),\n05:09:34            **hu.gcs)\n05:09:34     def test_sequence_mask_batching_lengths(self, x, dtype, gc, dc):\n05:09:34 \n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: \n05:09:34 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:587: in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/executors.py:58: in default_new_style_executor\n05:09:34     return function(data)\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:578: in run\n05:09:34     return test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:180: in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34 ../lib/python2.7/dist-packages/hypothesis/core.py:525: in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py:227: in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34 lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py:395: in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34 E   AssertionError: Gradient check failed for input data\n05:09:34 ----------------------------- Captured stdout call -----------------------------\n05:09:34 {u'lengths': , u'data': }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[126.           0.57714844   0.        ]]\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 {u'lengths': device_type: 1\n05:09:34 , u'data': device_type: 1\n05:09:34 }\n05:09:34 Failed. [idx, grad, grad_estimate] are:\n05:09:34 [[6.         0.50292969 0.        ]]\n05:09:34 ---------------------------------- Hypothesis ----------------------------------\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[0.5]], dtype=float32), dtype=float32, gc=, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.7377987 ],\n05:09:34           [0.60064876]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7033071 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.7377987 ],\n05:09:34           [0.7377987 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.60725623],\n05:09:34           [0.7377987 ]]]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]],\n05:09:34 \n05:09:34        [[0.8309552, 0.8309552, 0.8309552, 0.8309552],\n05:09:34         [0.8309552, 0.8309552, 0.8309552, 0.8309552]]], dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Traceback (most recent call last):\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 672, in evaluate_test_data\n05:09:34     result = self.execute(data, collect=True)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 587, in execute\n05:09:34     result = self.test_runner(data, run)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\n05:09:34     return function(data)\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 583, in run\n05:09:34     return test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 180, in test_sequence_mask_batching_lengths\n05:09:34     max_dim=5,\n05:09:34   File \"/usr/local/lib/python2.7/dist-packages/hypothesis/core.py\", line 525, in test\n05:09:34     result = self.test(*args, **kwargs)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/boolean_mask_test.py\", line 227, in test_sequence_mask_batching_lengths\n05:09:34     threshold=threshold)\n05:09:34   File \"/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py\", line 395, in assertGradientChecks\n05:09:34     \"Gradient check failed for input \" + str(op.input[outputs_to_check])\n05:09:34   File \"/usr/lib/python2.7/unittest/case.py\", line 422, in assertTrue\n05:09:34     raise self.failureException(msg)\n05:09:34 AssertionError: Gradient check failed for input data\n05:09:34 \n05:09:34 Trying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34 Falsifying example: test_sequence_mask_batching_lengths(self=<caffe2.python.operator_test.boolean_mask_test.TestBooleanMaskOp testMethod=test_sequence_mask_batching_lengths>, x=array([[[[[0.6944938 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5027218 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.52371424, 0.74308896],\n05:09:34           [0.56708384, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6563938 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.79903495, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.67317814, 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6202377 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.77230674, 0.6549941 , 0.6549941 , 0.8260463 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.7526704 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.5771142 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.59608275]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.58727473],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.76952314]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.58501023, 0.6549941 , 0.7542836 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.8475317 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.7596469 , 0.91022944, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6713312 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6293779 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9366175 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.88177323, 0.6549941 , 0.6549941 ],\n05:09:34           [0.84528357, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.5511952 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.5360435 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.7013548 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.86907005, 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.96185774, 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.8696888 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.9163116 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.504911  , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]],\n05:09:34 \n05:09:34 \n05:09:34 \n05:09:34        [[[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.70324486, 0.6549941 ]]],\n05:09:34 \n05:09:34 \n05:09:34         [[[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.7148637 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.9026597 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.9786567 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]],\n05:09:34 \n05:09:34          [[0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ],\n05:09:34           [0.95630765, 0.6549941 , 0.6549941 , 0.6549941 , 0.6549941 ]]]]],\n05:09:34       dtype=float32), dtype=float16, gc=device_type: 1, dc=[, device_type: 1])\n05:09:34  generated xml file: /var/lib/jenkins/workspace/caffe2_tests/python/result.xml -\n05:09:35 ---------- onnx coverage: ----------\n05:09:35 Operators (passed/loaded/total): 78/78/115\n05:09:35 ------------------------------------\n05:09:35 Operator               Attributes\n05:09:35                        (name: #values)\n05:09:35 Asin                   No attributes\n05:09:35 Slice                  axes: 3\n05:09:35                        ends: 8\n05:09:35                        starts: 7\n05:09:35 Log                    No attributes\n05:09:35 Sub                    No attributes\n05:09:35 Min                    No attributes\n05:09:35 Softsign               No attributes\n05:09:35 Reshape                No attributes\n05:09:35 Sum                    No attributes\n05:09:35 Xor                    No attributes\n05:09:35 Relu                   No attributes\n05:09:35 Upsample               mode: 1\n05:09:35                        scales: 1\n05:09:35 Add                    axis: 2\n05:09:35                        broadcast: 1\n05:09:35 Abs                    No attributes\n05:09:35 Pad                    mode: 3\n05:09:35                        pads: 4\n05:09:35                        value: 3\n05:09:35 Split                  axis: 3\n05:09:35                        split: 2\n05:09:35 TopK                   k: 1\n05:09:35                        axis: 0\n05:09:35 Selu                   alpha: 1\n05:09:35                        gamma: 1\n05:09:35 Mul                    axis: 1\n05:09:35                        broadcast: 1\n05:09:35 LSTM                   hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        activations: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35                        input_forget: 0\n05:09:35 ReduceMean             axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Acos                   No attributes\n05:09:35 Gemm                   alpha: 2\n05:09:35                        beta: 3\n05:09:35                        broadcast: 1\n05:09:35                        transA: 1\n05:09:35                        transB: 1\n05:09:35 Elu                    alpha: 1\n05:09:35 Sqrt                   No attributes\n05:09:35 Greater                No attributes\n05:09:35 Atan                   No attributes\n05:09:35 Sin                    No attributes\n05:09:35 BatchNormalization     epsilon: 4\n05:09:35                        is_test: 1\n05:09:35                        momentum: 3\n05:09:35                        spatial: 0\n05:09:35 Dropout                is_test: 1\n05:09:35                        ratio: 3\n05:09:35 Floor                  No attributes\n05:09:35 Transpose              perm: 9\n05:09:35 LeakyRelu              alpha: 3\n05:09:35 Ceil                   No attributes\n05:09:35 ReduceSum              axes: 2\n05:09:35                        keepdims: 2\n05:09:35 Identity               No attributes\n05:09:35 AveragePool            kernel_shape: 8\n05:09:35                        pads: 5\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35                        count_include_pad: 0\n05:09:35 Reciprocal             No attributes\n05:09:35 Neg                    No attributes\n05:09:35 Not                    No attributes\n05:09:35 Squeeze                axes: 3\n05:09:35 GlobalMaxPool          No attributes\n05:09:35 Concat                 axis: 3\n05:09:35 And                    No attributes\n05:09:35 Cos                    No attributes\n05:09:35 RNN                    activations: 2\n05:09:35                        hidden_size: 1\n05:09:35                        activation_alpha: 0\n05:09:35                        activation_beta: 0\n05:09:35                        clip: 0\n05:09:35                        direction: 0\n05:09:35 Mean                   No attributes\n05:09:35 Tanh                   No attributes\n05:09:35 Less                   No attributes\n05:09:35 Gather                 axis: 2\n05:09:35 LogSoftmax             axis: 5\n05:09:35 Equal                  No attributes\n05:09:35 Softplus               No attributes\n05:09:35 Softmax                axis: 4\n05:09:35 Exp                    No attributes\n05:09:35 Div                    No attributes\n05:09:35 GlobalAveragePool      No attributes\n05:09:35 Or                     No attributes\n05:09:35 PRelu                  No attributes\n05:09:35 ConstantFill           input_as_shape: 1\n05:09:35                        dtype: 0\n05:09:35                        extra_shape: 0\n05:09:35                        shape: 0\n05:09:35                        value: 0\n05:09:35 InstanceNormalization  epsilon: 2\n05:09:35 Unsqueeze              axes: 2\n05:09:35 Constant               value: 9\n05:09:35 Clip                   max: 3\n05:09:35                        min: 3\n05:09:35 Conv                   dilations: 6\n05:09:35                        group: 7\n05:09:35                        kernel_shape: 11\n05:09:35                        pads: 10\n05:09:35                        strides: 7\n05:09:35                        auto_pad: 0\n05:09:35 Sigmoid                No attributes\n05:09:35 Pow                    No attributes\n05:09:35 ConvTranspose          dilations: 1\n05:09:35                        group: 1\n05:09:35                        kernel_shape: 1\n05:09:35                        output_padding: 2\n05:09:35                        pads: 1\n05:09:35                        strides: 3\n05:09:35                        auto_pad: 0\n05:09:35                        output_shape: 0\n05:09:35 MaxPool                kernel_shape: 7\n05:09:35                        pads: 7\n05:09:35                        strides: 6\n05:09:35                        auto_pad: 0\n05:09:35 LRN                    alpha: 3\n05:09:35                        beta: 2\n05:09:35                        bias: 2\n05:09:35                        size: 2\n05:09:35 ReduceMin              axes: 1\n05:09:35                        keepdims: 2\n05:09:35 MatMul                 No attributes\n05:09:35 ThresholdedRelu        alpha: 1\n05:09:35 Flatten                axis: 4\n05:09:35 Max                    No attributes\n05:09:35 Shape                  No attributes\n05:09:35 Tan                    No attributes\n05:09:35 ReduceMax              axes: 1\n05:09:35                        keepdims: 2\n\nTesting if this is true. Full regression log: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda8.0-cudnn6-ubuntu16.04-test/6136//console\nSigned-off-by: Edward Z. Yang ezyang@fb.com", "merged": false, "mergeable": null, "rebaseable": null, "mergeable_state": "unknown", "merged_by": null, "comments": 3, "review_comments": 0, "maintainer_can_modify": false, "commits": 4, "additions": 3, "deletions": 0, "changed_files": 1}