{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164270001", "pull_request_review_id": 92023702, "id": 164270001, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDI3MDAwMQ==", "diff_hunk": "@@ -401,6 +401,33 @@ Tensor renorm_backward(const Tensor & grad, const Tensor & self, Scalar p, int64\n   return at::where(mask, grad, grad_norm);\n }\n \n+Tensor sum_tensorlist(TensorList tl) {\n+  if (tl.size() == 0) {\n+    throw std::runtime_error(\"Can't sum tensorlist of size 0\");\n+  }\n+  Tensor sum = tl[0];\n+  for(size_t i = 1; i < tl.size(); ++i) {\n+    sum = sum + tl[i];\n+  }\n+  return sum;\n+}\n+\n+Tensor repeat_backward(Tensor grad, int64_t input_dims, IntList repeats) {\n+  int64_t num_unsqueezed = grad.dim() - input_dims;\n+  for (int64_t i = 0; i < num_unsqueezed; ++i) {\n+    grad = grad.sum(0, false);\n+  }\n+  for (size_t j = num_unsqueezed; j < repeats.size(); ++j) {\n+    int64_t repeat = repeats[j];\n+    if (repeat == 1) {\n+      continue;\n+    }\n+    int64_t dim = j - num_unsqueezed;\n+    grad = sum_tensorlist(grad.chunk(repeat, dim));", "path": "tools/autograd/templates/Functions.cpp", "position": 26, "original_position": 26, "commit_id": "6b7bc03b95ab54ff2f08bdda31fa6268cf48b1bf", "original_commit_id": "6b7bc03b95ab54ff2f08bdda31fa6268cf48b1bf", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Actually couldn't we implement this as a single `sum(dim=...)` call? I think that if you have a 4D tensor with sizes `(s1, s2, s3, s4)` and repeat it like this: `(2, 3, 4, 5)`, then you effectively could take a 4D result you got, view it like this: `(2, s1, 3, s2, 4, s3, 5, s4)` and then call `.sum(dim=i)` for odd `i`.", "created_at": "2018-01-27T11:35:50Z", "updated_at": "2018-11-23T15:38:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/4885#discussion_r164270001", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4885", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164270001"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4885#discussion_r164270001"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4885"}}, "body_html": "<p>Actually couldn't we implement this as a single <code>sum(dim=...)</code> call? I think that if you have a 4D tensor with sizes <code>(s1, s2, s3, s4)</code> and repeat it like this: <code>(2, 3, 4, 5)</code>, then you effectively could take a 4D result you got, view it like this: <code>(2, s1, 3, s2, 4, s3, 5, s4)</code> and then call <code>.sum(dim=i)</code> for odd <code>i</code>.</p>", "body_text": "Actually couldn't we implement this as a single sum(dim=...) call? I think that if you have a 4D tensor with sizes (s1, s2, s3, s4) and repeat it like this: (2, 3, 4, 5), then you effectively could take a 4D result you got, view it like this: (2, s1, 3, s2, 4, s3, 5, s4) and then call .sum(dim=i) for odd i."}