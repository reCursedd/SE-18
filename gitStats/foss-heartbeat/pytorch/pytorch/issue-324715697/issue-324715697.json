{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7714", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7714/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7714/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7714/events", "html_url": "https://github.com/pytorch/pytorch/issues/7714", "id": 324715697, "node_id": "MDU6SXNzdWUzMjQ3MTU2OTc=", "number": 7714, "title": "Very slow for gradient penalty!", "user": {"login": "zzd1992", "id": 11853283, "node_id": "MDQ6VXNlcjExODUzMjgz", "avatar_url": "https://avatars3.githubusercontent.com/u/11853283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zzd1992", "html_url": "https://github.com/zzd1992", "followers_url": "https://api.github.com/users/zzd1992/followers", "following_url": "https://api.github.com/users/zzd1992/following{/other_user}", "gists_url": "https://api.github.com/users/zzd1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/zzd1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zzd1992/subscriptions", "organizations_url": "https://api.github.com/users/zzd1992/orgs", "repos_url": "https://api.github.com/users/zzd1992/repos", "events_url": "https://api.github.com/users/zzd1992/events{/privacy}", "received_events_url": "https://api.github.com/users/zzd1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-20T14:59:55Z", "updated_at": "2018-05-20T19:30:42Z", "closed_at": "2018-05-20T19:30:42Z", "author_association": "NONE", "body_html": "<p>Gradient penalty (GP) here means we minimize L2 norm of the gradient w.r.t input images.</p>\n<p>I work on Ubuntu14.04, Python2.7, CUDA8.0 and CUDNN.<br>\nThe version of Pytorch is 0.4.0.<br>\nI find it is very slow when I apply gradient penalty (GP) for training CIFAR10 with Resnet18.<br>\nI test the average running time of each step with and without GP:</p>\n<pre><code>without : 0.065s\nwith GP: 0.330s\n</code></pre>\n<p>4 times slower compared with the standard training!<br>\nHere is my code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn, autograd\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> models <span class=\"pl-k\">import</span> PreActResNet18\n<span class=\"pl-k\">import</span> time\n\nnet <span class=\"pl-k\">=</span> PreActResNet18()\nnet.cuda()\nopt <span class=\"pl-k\">=</span> torch.optim.SGD(net.parameters(), <span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">weight_decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\nbatchsize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n\n<span class=\"pl-c1\">GP</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nstart <span class=\"pl-k\">=</span> time.time()\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n    x <span class=\"pl-k\">=</span> torch.rand((batchsize, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)).cuda()\n    y <span class=\"pl-k\">=</span> torch.randint(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">10</span>, (batchsize,)).cuda().long()\n    x, y <span class=\"pl-k\">=</span> Variable(x, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(y)\n    opt.zero_grad()\n    preds <span class=\"pl-k\">=</span> net(x)\n    loss_c <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()(preds, y)\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">GP</span>:\n        grad <span class=\"pl-k\">=</span> autograd.grad(loss_c, x, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                             <span class=\"pl-v\">only_inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\n        loss_g <span class=\"pl-k\">=</span> (grad <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>).mean() <span class=\"pl-k\">*</span> batchsize\n        (loss_c <span class=\"pl-k\">+</span> loss_g).backward()\n    <span class=\"pl-k\">else</span>:\n        loss_c.backward()\n    opt.step()\n\nend <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{<span class=\"pl-k\">:.3f</span>}</span>s for each step<span class=\"pl-pds\">\"</span></span>.format((end<span class=\"pl-k\">-</span>start)<span class=\"pl-k\">/</span><span class=\"pl-c1\">100</span>))</pre></div>\n<p>Is there better implementation?<br>\nOr just as slow as it is?</p>", "body_text": "Gradient penalty (GP) here means we minimize L2 norm of the gradient w.r.t input images.\nI work on Ubuntu14.04, Python2.7, CUDA8.0 and CUDNN.\nThe version of Pytorch is 0.4.0.\nI find it is very slow when I apply gradient penalty (GP) for training CIFAR10 with Resnet18.\nI test the average running time of each step with and without GP:\nwithout : 0.065s\nwith GP: 0.330s\n\n4 times slower compared with the standard training!\nHere is my code:\nimport torch\nfrom torch import nn, autograd\nfrom torch.autograd import Variable\nfrom models import PreActResNet18\nimport time\n\nnet = PreActResNet18()\nnet.cuda()\nopt = torch.optim.SGD(net.parameters(), 0.01, momentum=0.9, weight_decay=1e-4)\nbatchsize = 128\n\nGP = True\nstart = time.time()\n\nfor i in range(100):\n    x = torch.rand((batchsize, 3, 32, 32)).cuda()\n    y = torch.randint(0, 10, (batchsize,)).cuda().long()\n    x, y = Variable(x, requires_grad=True), Variable(y)\n    opt.zero_grad()\n    preds = net(x)\n    loss_c = nn.CrossEntropyLoss()(preds, y)\n\n    if GP:\n        grad = autograd.grad(loss_c, x, create_graph=True, retain_graph=True,\n                             only_inputs=True)[0]\n        loss_g = (grad ** 2).mean() * batchsize\n        (loss_c + loss_g).backward()\n    else:\n        loss_c.backward()\n    opt.step()\n\nend = time.time()\nprint(\"{:.3f}s for each step\".format((end-start)/100))\nIs there better implementation?\nOr just as slow as it is?", "body": "Gradient penalty (GP) here means we minimize L2 norm of the gradient w.r.t input images.\r\n\r\nI work on Ubuntu14.04, Python2.7, CUDA8.0 and CUDNN.\r\nThe version of Pytorch is 0.4.0.\r\nI find it is very slow when I apply gradient penalty (GP) for training CIFAR10 with Resnet18.\r\nI test the average running time of each step with and without GP:\r\n```\r\nwithout : 0.065s\r\nwith GP: 0.330s\r\n```\r\n4 times slower compared with the standard training!\r\nHere is my code:\r\n```python\r\nimport torch\r\nfrom torch import nn, autograd\r\nfrom torch.autograd import Variable\r\nfrom models import PreActResNet18\r\nimport time\r\n\r\nnet = PreActResNet18()\r\nnet.cuda()\r\nopt = torch.optim.SGD(net.parameters(), 0.01, momentum=0.9, weight_decay=1e-4)\r\nbatchsize = 128\r\n\r\nGP = True\r\nstart = time.time()\r\n\r\nfor i in range(100):\r\n    x = torch.rand((batchsize, 3, 32, 32)).cuda()\r\n    y = torch.randint(0, 10, (batchsize,)).cuda().long()\r\n    x, y = Variable(x, requires_grad=True), Variable(y)\r\n    opt.zero_grad()\r\n    preds = net(x)\r\n    loss_c = nn.CrossEntropyLoss()(preds, y)\r\n\r\n    if GP:\r\n        grad = autograd.grad(loss_c, x, create_graph=True, retain_graph=True,\r\n                             only_inputs=True)[0]\r\n        loss_g = (grad ** 2).mean() * batchsize\r\n        (loss_c + loss_g).backward()\r\n    else:\r\n        loss_c.backward()\r\n    opt.step()\r\n\r\nend = time.time()\r\nprint(\"{:.3f}s for each step\".format((end-start)/100))\r\n```\r\nIs there better implementation?\r\nOr just as slow as it is? "}