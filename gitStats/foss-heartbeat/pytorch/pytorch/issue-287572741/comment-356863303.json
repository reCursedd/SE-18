{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/356863303", "html_url": "https://github.com/pytorch/pytorch/issues/4588#issuecomment-356863303", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4588", "id": 356863303, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Njg2MzMwMw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T08:36:32Z", "updated_at": "2018-01-11T08:36:32Z", "author_association": "MEMBER", "body_html": "<p>The default is like this, because a lot (most?) of our tests involve comparing to approximations/computing finite differences and that requires higher precision than float. Also, it doesn't really hurt perf/mem usage, because most tests should be very small and double math is as fast as float math on the CPU. For CUDA, we're using <code>torch.cuda.FloatTensor</code> in most places IIRC</p>", "body_text": "The default is like this, because a lot (most?) of our tests involve comparing to approximations/computing finite differences and that requires higher precision than float. Also, it doesn't really hurt perf/mem usage, because most tests should be very small and double math is as fast as float math on the CPU. For CUDA, we're using torch.cuda.FloatTensor in most places IIRC", "body": "The default is like this, because a lot (most?) of our tests involve comparing to approximations/computing finite differences and that requires higher precision than float. Also, it doesn't really hurt perf/mem usage, because most tests should be very small and double math is as fast as float math on the CPU. For CUDA, we're using `torch.cuda.FloatTensor` in most places IIRC"}