{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9596", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9596/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9596/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9596/events", "html_url": "https://github.com/pytorch/pytorch/pull/9596", "id": 342850742, "node_id": "MDExOlB1bGxSZXF1ZXN0MjAyNjUzODUx", "number": 9596, "title": "[JIT] Fix the clamp special case and gradient problem on None, add None to JIT", "user": {"login": "wanchaol", "id": 9443650, "node_id": "MDQ6VXNlcjk0NDM2NTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9443650?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wanchaol", "html_url": "https://github.com/wanchaol", "followers_url": "https://api.github.com/users/wanchaol/followers", "following_url": "https://api.github.com/users/wanchaol/following{/other_user}", "gists_url": "https://api.github.com/users/wanchaol/gists{/gist_id}", "starred_url": "https://api.github.com/users/wanchaol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wanchaol/subscriptions", "organizations_url": "https://api.github.com/users/wanchaol/orgs", "repos_url": "https://api.github.com/users/wanchaol/repos", "events_url": "https://api.github.com/users/wanchaol/events{/privacy}", "received_events_url": "https://api.github.com/users/wanchaol/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-19T19:00:50Z", "updated_at": "2018-11-23T15:48:19Z", "closed_at": "2018-07-28T05:55:35Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9596", "html_url": "https://github.com/pytorch/pytorch/pull/9596", "diff_url": "https://github.com/pytorch/pytorch/pull/9596.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9596.patch"}, "body_html": "<p>Supersedes <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"336036935\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8925\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8925/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8925\">#8925</a></p>\n<p>This PR <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #8502.\">fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"332537196\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8502\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8502/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/8502\">#8502</a>, it fixes the gradients problem for clamp when passing None to the function, and add support for the NoneLiteral and NoneType in script to enable clamp tests. Now we could have corner cases like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@torch.jit.script</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">func</span>():\n    x <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    y <span class=\"pl-k\">=</span> torch.clamp(x, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> max = 0</span>\n    y <span class=\"pl-k\">=</span> torch.clamp(x, <span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">max</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)</pre></div>\n<p>In both JIT and Aten, we use Scalar(NAN) as a sentinel value when passing None type to function clamp, this is the current way we used to support None type in JIT and to solve the gradient problem when user explicitly passing None into clamp.</p>\n<p>In JIT side, we create a tensor(NAN) and undefinedTensor if we encounter None when matching the function schema, and later in the interpreter, it will translate to Scalar(NAN) if needed.</p>\n<p>Ideally we don't need clamp_min and clamp_max in ATenNative/Autograd and could only support clamp after this change, but since bunch of other operators (e.g. Activation.cpp, Loss.cpp) is using clamp_min in several places, we will still have the functions available, but all python invocations will only call clamp instead of clamp_min/max (with calling underlying th_max/th_min in clamp).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4685384\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jamesr66a\">@jamesr66a</a></p>", "body_text": "Supersedes #8925\nThis PR fixes #8502, it fixes the gradients problem for clamp when passing None to the function, and add support for the NoneLiteral and NoneType in script to enable clamp tests. Now we could have corner cases like:\n@torch.jit.script\ndef func():\n    x = torch.randn(3, 3, requires_grad=True)\n    y = torch.clamp(x, None, 0) # max = 0\n    y = torch.clamp(x, min=None, max=0)\nIn both JIT and Aten, we use Scalar(NAN) as a sentinel value when passing None type to function clamp, this is the current way we used to support None type in JIT and to solve the gradient problem when user explicitly passing None into clamp.\nIn JIT side, we create a tensor(NAN) and undefinedTensor if we encounter None when matching the function schema, and later in the interpreter, it will translate to Scalar(NAN) if needed.\nIdeally we don't need clamp_min and clamp_max in ATenNative/Autograd and could only support clamp after this change, but since bunch of other operators (e.g. Activation.cpp, Loss.cpp) is using clamp_min in several places, we will still have the functions available, but all python invocations will only call clamp instead of clamp_min/max (with calling underlying th_max/th_min in clamp).\n@zdevito @jamesr66a", "body": "Supersedes #8925 \r\n\r\nThis PR fixes #8502, it fixes the gradients problem for clamp when passing None to the function, and add support for the NoneLiteral and NoneType in script to enable clamp tests. Now we could have corner cases like:\r\n\r\n```python\r\n@torch.jit.script\r\ndef func():\r\n    x = torch.randn(3, 3, requires_grad=True)\r\n    y = torch.clamp(x, None, 0) # max = 0\r\n    y = torch.clamp(x, min=None, max=0)\r\n```\r\n\r\nIn both JIT and Aten, we use Scalar(NAN) as a sentinel value when passing None type to function clamp, this is the current way we used to support None type in JIT and to solve the gradient problem when user explicitly passing None into clamp. \r\n\r\nIn JIT side, we create a tensor(NAN) and undefinedTensor if we encounter None when matching the function schema, and later in the interpreter, it will translate to Scalar(NAN) if needed. \r\n\r\nIdeally we don't need clamp_min and clamp_max in ATenNative/Autograd and could only support clamp after this change, but since bunch of other operators (e.g. Activation.cpp, Loss.cpp) is using clamp_min in several places, we will still have the functions available, but all python invocations will only call clamp instead of clamp_min/max (with calling underlying th_max/th_min in clamp). \r\n\r\n@zdevito @jamesr66a "}