{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10742", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10742/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10742/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10742/events", "html_url": "https://github.com/pytorch/pytorch/issues/10742", "id": 352661979, "node_id": "MDU6SXNzdWUzNTI2NjE5Nzk=", "number": 10742, "title": "[feature request][pytorch] finfo as in numpy and finfo for default dtype", "user": {"login": "sytrus-in-github", "id": 12224616, "node_id": "MDQ6VXNlcjEyMjI0NjE2", "avatar_url": "https://avatars0.githubusercontent.com/u/12224616?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytrus-in-github", "html_url": "https://github.com/sytrus-in-github", "followers_url": "https://api.github.com/users/sytrus-in-github/followers", "following_url": "https://api.github.com/users/sytrus-in-github/following{/other_user}", "gists_url": "https://api.github.com/users/sytrus-in-github/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytrus-in-github/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytrus-in-github/subscriptions", "organizations_url": "https://api.github.com/users/sytrus-in-github/orgs", "repos_url": "https://api.github.com/users/sytrus-in-github/repos", "events_url": "https://api.github.com/users/sytrus-in-github/events{/privacy}", "received_events_url": "https://api.github.com/users/sytrus-in-github/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2018-08-21T18:40:44Z", "updated_at": "2018-10-08T23:17:11Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>It is nice that pytorch now has <code>dtype</code> attribute for tensors and a default dtype so that we can easily switch between different precisions. However, as I understand it:</p>\n<ol>\n<li>if we want to define e.g. a small value <code>eps</code> that is the smallest representable positive value of double, right now we have to switch to numpy and use sth like <code>np.finfo(np.float64).tiny</code>.</li>\n<li>if we want this value to depend on the current default dtype, we have to call <code>torch.get_default_dtype()</code>, convert it somehow to numpy dtype, then use <code>np.finfo</code>. This must be performed every time we need this number unless we are sure the default dtype is not changed in between.</li>\n</ol>\n<p>It would thus be very nice if we have sth like <code>torch.finfo</code> similar to <code>np.finfo</code> that additionally takes the current default pytorch dtype as the default argument so as to have a hassle-free and efficient way of solving the above problems.</p>", "body_text": "Issue description\nIt is nice that pytorch now has dtype attribute for tensors and a default dtype so that we can easily switch between different precisions. However, as I understand it:\n\nif we want to define e.g. a small value eps that is the smallest representable positive value of double, right now we have to switch to numpy and use sth like np.finfo(np.float64).tiny.\nif we want this value to depend on the current default dtype, we have to call torch.get_default_dtype(), convert it somehow to numpy dtype, then use np.finfo. This must be performed every time we need this number unless we are sure the default dtype is not changed in between.\n\nIt would thus be very nice if we have sth like torch.finfo similar to np.finfo that additionally takes the current default pytorch dtype as the default argument so as to have a hassle-free and efficient way of solving the above problems.", "body": "## Issue description\r\n\r\nIt is nice that pytorch now has `dtype` attribute for tensors and a default dtype so that we can easily switch between different precisions. However, as I understand it:\r\n\r\n1. if we want to define e.g. a small value `eps` that is the smallest representable positive value of double, right now we have to switch to numpy and use sth like `np.finfo(np.float64).tiny`.\r\n2. if we want this value to depend on the current default dtype, we have to call `torch.get_default_dtype()`, convert it somehow to numpy dtype, then use `np.finfo`. This must be performed every time we need this number unless we are sure the default dtype is not changed in between.\r\n\r\nIt would thus be very nice if we have sth like `torch.finfo` similar to `np.finfo` that additionally takes the current default pytorch dtype as the default argument so as to have a hassle-free and efficient way of solving the above problems.\r\n"}