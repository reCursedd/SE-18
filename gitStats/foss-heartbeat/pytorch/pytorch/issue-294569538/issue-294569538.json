{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5064", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5064/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5064/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5064/events", "html_url": "https://github.com/pytorch/pytorch/pull/5064", "id": 294569538, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY3Mjc1Mjg5", "number": 5064, "title": "DDP: 10% of NCCL backend perf improvements with mixed-prec support", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2018-02-05T22:45:43Z", "updated_at": "2018-11-23T15:39:48Z", "closed_at": "2018-02-21T22:59:53Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5064", "html_url": "https://github.com/pytorch/pytorch/pull/5064", "diff_url": "https://github.com/pytorch/pytorch/pull/5064.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5064.patch"}, "body_html": "<p>This PR lets NCCL backend to directly enqueue the NCCL reduction kernels in the same thread since it's an async call by nature, and everything now goes to the default stream. This essentially gets rid of the overheads of python thread synchronization, stream synchronization, as well as bucketing map lookup overheads.</p>\n<p>For DDP with multiple GPU (the default use case), instead of doing a two step reduction, we use the new NCCL backend API to all-reduce all GPU at one time, this is basically 4 times all-reduce throughput on DGX1s compared to the all-reduce(single GPU version) plus the intra-node reduce overheads.</p>\n<p>As a results, we have the following perf improvements.</p>\n<p>For DDP with 8 GPU (default use case):</p>\n<p>On two DGX1s with 8 V100s, 256 batch size / process, single process / node<br>\nResNet50<br>\n<strong>0.139 sec / iter</strong> reduced from <strong>0.154 sec / iter</strong> (about 10 percent improvements)<br>\nResNet101<br>\n<strong>0.247 sec / iter</strong> reduced from <strong>0.272 sec / iter</strong> (about 10 percent improvements)</p>\n<p>For DDP with 1 GPU (multi-process use case)</p>\n<p>On singe DGX1s with 8 V100s, ResNet50, 32 batch size per GPU and process, 8 processes distributed training<br>\n<strong>0.109 sec / iter</strong> reduced from <strong>0.116 sec / iter</strong> (about 6 percent improvement)</p>\n<p>In addition,  added bucketing to limit the memory usage, added mixed precision support.</p>", "body_text": "This PR lets NCCL backend to directly enqueue the NCCL reduction kernels in the same thread since it's an async call by nature, and everything now goes to the default stream. This essentially gets rid of the overheads of python thread synchronization, stream synchronization, as well as bucketing map lookup overheads.\nFor DDP with multiple GPU (the default use case), instead of doing a two step reduction, we use the new NCCL backend API to all-reduce all GPU at one time, this is basically 4 times all-reduce throughput on DGX1s compared to the all-reduce(single GPU version) plus the intra-node reduce overheads.\nAs a results, we have the following perf improvements.\nFor DDP with 8 GPU (default use case):\nOn two DGX1s with 8 V100s, 256 batch size / process, single process / node\nResNet50\n0.139 sec / iter reduced from 0.154 sec / iter (about 10 percent improvements)\nResNet101\n0.247 sec / iter reduced from 0.272 sec / iter (about 10 percent improvements)\nFor DDP with 1 GPU (multi-process use case)\nOn singe DGX1s with 8 V100s, ResNet50, 32 batch size per GPU and process, 8 processes distributed training\n0.109 sec / iter reduced from 0.116 sec / iter (about 6 percent improvement)\nIn addition,  added bucketing to limit the memory usage, added mixed precision support.", "body": "This PR lets NCCL backend to directly enqueue the NCCL reduction kernels in the same thread since it's an async call by nature, and everything now goes to the default stream. This essentially gets rid of the overheads of python thread synchronization, stream synchronization, as well as bucketing map lookup overheads.  \r\n\r\nFor DDP with multiple GPU (the default use case), instead of doing a two step reduction, we use the new NCCL backend API to all-reduce all GPU at one time, this is basically 4 times all-reduce throughput on DGX1s compared to the all-reduce(single GPU version) plus the intra-node reduce overheads.\r\n\r\nAs a results, we have the following perf improvements.\r\n\r\nFor DDP with 8 GPU (default use case):\r\n\r\nOn two DGX1s with 8 V100s, 256 batch size / process, single process / node\r\nResNet50\r\n**0.139 sec / iter** reduced from **0.154 sec / iter** (about 10 percent improvements)\r\nResNet101\r\n**0.247 sec / iter** reduced from **0.272 sec / iter** (about 10 percent improvements)\r\n\r\nFor DDP with 1 GPU (multi-process use case)\r\n\r\nOn singe DGX1s with 8 V100s, ResNet50, 32 batch size per GPU and process, 8 processes distributed training \r\n**0.109 sec / iter** reduced from **0.116 sec / iter** (about 6 percent improvement)\r\n\r\nIn addition,  added bucketing to limit the memory usage, added mixed precision support.\r\n\r\n"}