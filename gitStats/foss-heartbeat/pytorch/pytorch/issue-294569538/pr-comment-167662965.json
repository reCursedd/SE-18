{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167662965", "pull_request_review_id": 95834652, "id": 167662965, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzY2Mjk2NQ==", "diff_hunk": "@@ -222,7 +243,85 @@ def _register_grad_hooks(self):\n                     grad_acc.register_hook(self._make_param_hook(p, device_idx))\n                     self._grad_accs.append(grad_acc)\n \n+    def _register_nccl_grad_hook(self):\n+        \"\"\"\n+        This function registers the callback all-reduction function for the\n+        NCCL backend. All gradients will be all reduced in one single step.\n+        The NCCL reduction will directly be enqueued into the\n+        default CUDA stream. Therefore, no synchronization is needed.\n+        \"\"\"\n+        # Creating a new group\n+        self.nccl_reduction_group_id = dist.new_group()\n+\n+        def reduction_fn_nccl():\n+            # This function only needs to be called once\n+            if not self.need_reduction:\n+                return\n+            self.need_reduction = False\n+            all_grads = [[] for _ in range(len(self._module_copies))]\n+            all_grads_coalesced = []\n+            # The gradient bucket for the first device of self.device_ids\n+            mst_dev_grads_buckets = []\n+\n+            # Coalesce all the gradients\n+            for dev_idx, module in enumerate(self._module_copies):\n+                for param in module.parameters():\n+                    if not param.requires_grad or param.grad is None:\n+                        continue\n+                    if param.grad.requires_grad:\n+                        raise RuntimeError(\"DistributedDataParallel only works \"\n+                                           \"with gradients that don't require \"\n+                                           \"grad\")\n+                    # Adding the gradients for reduction\n+                    all_grads[dev_idx].append(param.grad.data)\n+\n+                # Now bucketing the parameters\n+                dev_grads_buckets = _take_tensors(all_grads[dev_idx],\n+                                                  self.nccl_reduce_bucket_size)\n+                if dev_idx == 0:\n+                    mst_dev_grads_buckets = list(dev_grads_buckets)\n+                    all_grads_coalesced = \\\n+                        [[] for _ in range(len(mst_dev_grads_buckets))]\n+\n+                for bkt_idx, dev_grads in enumerate(dev_grads_buckets):", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 124, "commit_id": "97e14897a9a7407c1e3aed1ab6d9ff0a9e1f9ed8", "original_commit_id": "5855d66caba8af65cff9926561bdf8198cb429e8", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Is it expected that this loop will execute if and only if `dev_idx != 0`? `take_tensors` returns an iterator and doing `list(dev_grads_buckets)` will deplete it, meaning that this will terminate immediately. If this is on purpose, please add this loop in an `else` branch, to make it clear that it was your intent.", "created_at": "2018-02-12T19:34:57Z", "updated_at": "2018-11-23T15:39:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/5064#discussion_r167662965", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5064", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167662965"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5064#discussion_r167662965"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5064"}}, "body_html": "<p>Is it expected that this loop will execute if and only if <code>dev_idx != 0</code>? <code>take_tensors</code> returns an iterator and doing <code>list(dev_grads_buckets)</code> will deplete it, meaning that this will terminate immediately. If this is on purpose, please add this loop in an <code>else</code> branch, to make it clear that it was your intent.</p>", "body_text": "Is it expected that this loop will execute if and only if dev_idx != 0? take_tensors returns an iterator and doing list(dev_grads_buckets) will deplete it, meaning that this will terminate immediately. If this is on purpose, please add this loop in an else branch, to make it clear that it was your intent."}