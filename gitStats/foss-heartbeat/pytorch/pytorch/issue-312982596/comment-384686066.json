{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/384686066", "html_url": "https://github.com/pytorch/pytorch/issues/6469#issuecomment-384686066", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6469", "id": 384686066, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDY4NjA2Ng==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-26T15:34:05Z", "updated_at": "2018-04-27T08:52:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19503980\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kshitij12345\">@kshitij12345</a> so here are some ideas how I might approach developping (as in take it with a grain of salt and only do what you think is useful to you. Also I would appreciate if you or the others pointed out if this is not helping or so.)</p>\n<ul>\n<li>The derivatives are governed by <code>torch/autograd</code> (rather than ATen itelf). Most of the magic happens in <code>tools/autograd</code>.<br>\nLet's look into that directory: In <code>derivatives.yaml</code> are the derivatives for functions that aren't themselves built from differentiable bits. Embedding is in  <a href=\"https://github.com/pytorch/pytorch/blob/24d05662ead139883ac0d9545accc862196c3c75/tools/autograd/derivatives.yaml#L697\">derivatives.yaml</a><br>\nso we are looking for embedding_backward and we see that it <code>embedding</code> can be differentiated w.r.t. weight (only).</li>\n<li>Many of the backwards are defined in <code>tools/autograd/templates/Functions.cpp</code>. However, this is not the case here. So we look in the <code>aten</code> directory (I personally like to use <code>rgrep &lt;function_name&gt; aten/src/</code> a lot). Indeed, it is defined in <a href=\"https://github.com/pytorch/pytorch/blob/24d05662ead139883ac0d9545accc862196c3c75/aten/src/ATen/native/native_functions.yaml#L291-L298\"><code>aten/src/ATen/native/native_functions.yaml</code></a> and the definitions are in the <code>Embedding.cpp</code> and <code>cuda/Embedding.cu</code> in that directory.</li>\n<li>Get a handle on what you expect to see and what the current state is.<br>\nMy repro would look something like this:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.tensor([[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">5</span>],[<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">9</span>]])\nembedding_matrix <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nemb <span class=\"pl-k\">=</span> torch.nn.functional.embedding(<span class=\"pl-c1\">input</span>, embedding_matrix) <span class=\"pl-c\"><span class=\"pl-c\">#</span> optionally have this weight in backward</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>emb = embedding_matrix[input] # alternative for unweighted backward</span>\nweights <span class=\"pl-k\">=</span> torch.randn_like(emb)\nl <span class=\"pl-k\">=</span> (emb<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>weights).sum()\nd_embedding_matrix, d_emb, <span class=\"pl-k\">=</span> torch.autograd.grad(l, [embedding_matrix,emb], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nd_embedding_matrix.sum().backward()</pre></div>\n<p>I usually define python functions and checking they do the same as the existing or desired derivative.</p>\n<ul>\n<li>The forward of embedding is basically torch.index_select with a bit of <code>view</code> for non-flat indices.</li>\n<li>The backward is a fusion of <code>_index_add</code> and (indeed, look at index_select in <code>derivatives.yaml</code>) an optional multiplication by the inverse frequencies. (Indeed, one might be tempted benchmark in python much slower <code>index_add_</code> and <code>mul_</code> are - if that is the case, defining a partial derivative for <code>mul_</code> and using that would solve the problem. Possibly double checking whether embedding's OPENMP ifdef might be incorporated into index_add_.)</li>\n<li>If you were to define the double backward, it would be the backward of index_add_ (which is index_select_ again) and mul (which is the frequency weighting on the grad). You can check this by using defining a derivative with <code>index_add_</code>and <code>mul_</code>. Note that for double backwards you also need the derivative by <code>grad</code> (the gradient w.r.t. the outputs).</li>\n</ul>\n<p>Then you need an implementation strategy. This might also be something where the core devs and the people implementing the current code have an opinion. (eg <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> ?)</p>\n<ul>\n<li>If we leave the backward as is, one could just implement embedding_double_backward in <code>tools/autograd/templates/Functions.cpp</code> that implements the necessary double backwards. However it might be nicer to avoid this overly specific code, and if you start before getting the input of the people that review your patch, you might end up coding for experience only. (Guess how I know...)</li>\n<li>Introduce an optional weight to <code>index_add_</code> and define embedding_backward with that. This would be my favourite  as it would seem the cleanest way.</li>\n<li>Possibly see whether <code>index_add_</code> and <code>mul_</code> separately is an option.</li>\n</ul>\n<p>I hope this helps to get you started. You can also ping me directly if I can be of help.</p>", "body_text": "@kshitij12345 so here are some ideas how I might approach developping (as in take it with a grain of salt and only do what you think is useful to you. Also I would appreciate if you or the others pointed out if this is not helping or so.)\n\nThe derivatives are governed by torch/autograd (rather than ATen itelf). Most of the magic happens in tools/autograd.\nLet's look into that directory: In derivatives.yaml are the derivatives for functions that aren't themselves built from differentiable bits. Embedding is in  derivatives.yaml\nso we are looking for embedding_backward and we see that it embedding can be differentiated w.r.t. weight (only).\nMany of the backwards are defined in tools/autograd/templates/Functions.cpp. However, this is not the case here. So we look in the aten directory (I personally like to use rgrep <function_name> aten/src/ a lot). Indeed, it is defined in aten/src/ATen/native/native_functions.yaml and the definitions are in the Embedding.cpp and cuda/Embedding.cu in that directory.\nGet a handle on what you expect to see and what the current state is.\nMy repro would look something like this:\n\ninput = torch.tensor([[1,2,4,5],[4,3,2,9]])\nembedding_matrix = torch.rand(10, 3, requires_grad=True)\nemb = torch.nn.functional.embedding(input, embedding_matrix) # optionally have this weight in backward\n#emb = embedding_matrix[input] # alternative for unweighted backward\nweights = torch.randn_like(emb)\nl = (emb**2*weights).sum()\nd_embedding_matrix, d_emb, = torch.autograd.grad(l, [embedding_matrix,emb], create_graph=True, retain_graph=True)\nd_embedding_matrix.sum().backward()\nI usually define python functions and checking they do the same as the existing or desired derivative.\n\nThe forward of embedding is basically torch.index_select with a bit of view for non-flat indices.\nThe backward is a fusion of _index_add and (indeed, look at index_select in derivatives.yaml) an optional multiplication by the inverse frequencies. (Indeed, one might be tempted benchmark in python much slower index_add_ and mul_ are - if that is the case, defining a partial derivative for mul_ and using that would solve the problem. Possibly double checking whether embedding's OPENMP ifdef might be incorporated into index_add_.)\nIf you were to define the double backward, it would be the backward of index_add_ (which is index_select_ again) and mul (which is the frequency weighting on the grad). You can check this by using defining a derivative with index_add_and mul_. Note that for double backwards you also need the derivative by grad (the gradient w.r.t. the outputs).\n\nThen you need an implementation strategy. This might also be something where the core devs and the people implementing the current code have an opinion. (eg @apaszke, @colesbury ?)\n\nIf we leave the backward as is, one could just implement embedding_double_backward in tools/autograd/templates/Functions.cpp that implements the necessary double backwards. However it might be nicer to avoid this overly specific code, and if you start before getting the input of the people that review your patch, you might end up coding for experience only. (Guess how I know...)\nIntroduce an optional weight to index_add_ and define embedding_backward with that. This would be my favourite  as it would seem the cleanest way.\nPossibly see whether index_add_ and mul_ separately is an option.\n\nI hope this helps to get you started. You can also ping me directly if I can be of help.", "body": "@kshitij12345 so here are some ideas how I might approach developping (as in take it with a grain of salt and only do what you think is useful to you. Also I would appreciate if you or the others pointed out if this is not helping or so.)\r\n\r\n- The derivatives are governed by `torch/autograd` (rather than ATen itelf). Most of the magic happens in `tools/autograd`.\r\n  Let's look into that directory: In `derivatives.yaml` are the derivatives for functions that aren't themselves built from differentiable bits. Embedding is in  [derivatives.yaml](https://github.com/pytorch/pytorch/blob/24d05662ead139883ac0d9545accc862196c3c75/tools/autograd/derivatives.yaml#L697)\r\n  so we are looking for embedding_backward and we see that it `embedding` can be differentiated w.r.t. weight (only).\r\n- Many of the backwards are defined in `tools/autograd/templates/Functions.cpp`. However, this is not the case here. So we look in the `aten` directory (I personally like to use `rgrep <function_name> aten/src/` a lot). Indeed, it is defined in [`aten/src/ATen/native/native_functions.yaml`](https://github.com/pytorch/pytorch/blob/24d05662ead139883ac0d9545accc862196c3c75/aten/src/ATen/native/native_functions.yaml#L291-L298) and the definitions are in the `Embedding.cpp` and `cuda/Embedding.cu` in that directory.\r\n- Get a handle on what you expect to see and what the current state is.\r\n  My repro would look something like this:\r\n```python\r\ninput = torch.tensor([[1,2,4,5],[4,3,2,9]])\r\nembedding_matrix = torch.rand(10, 3, requires_grad=True)\r\nemb = torch.nn.functional.embedding(input, embedding_matrix) # optionally have this weight in backward\r\n#emb = embedding_matrix[input] # alternative for unweighted backward\r\nweights = torch.randn_like(emb)\r\nl = (emb**2*weights).sum()\r\nd_embedding_matrix, d_emb, = torch.autograd.grad(l, [embedding_matrix,emb], create_graph=True, retain_graph=True)\r\nd_embedding_matrix.sum().backward()\r\n```\r\nI usually define python functions and checking they do the same as the existing or desired derivative.\r\n\r\n  - The forward of embedding is basically torch.index_select with a bit of `view` for non-flat indices.\r\n  - The backward is a fusion of `_index_add` and (indeed, look at index_select in `derivatives.yaml`) an optional multiplication by the inverse frequencies. (Indeed, one might be tempted benchmark in python much slower `index_add_` and `mul_` are - if that is the case, defining a partial derivative for `mul_` and using that would solve the problem. Possibly double checking whether embedding's OPENMP ifdef might be incorporated into index_add_.) \r\n  - If you were to define the double backward, it would be the backward of index_add_ (which is index_select_ again) and mul (which is the frequency weighting on the grad). You can check this by using defining a derivative with `index_add_`and `mul_`. Note that for double backwards you also need the derivative by `grad` (the gradient w.r.t. the outputs).\r\n\r\nThen you need an implementation strategy. This might also be something where the core devs and the people implementing the current code have an opinion. (eg @apaszke, @colesbury ?)\r\n- If we leave the backward as is, one could just implement embedding_double_backward in `tools/autograd/templates/Functions.cpp` that implements the necessary double backwards. However it might be nicer to avoid this overly specific code, and if you start before getting the input of the people that review your patch, you might end up coding for experience only. (Guess how I know...)\r\n- Introduce an optional weight to `index_add_` and define embedding_backward with that. This would be my favourite  as it would seem the cleanest way.\r\n- Possibly see whether `index_add_` and `mul_` separately is an option.\r\n\r\nI hope this helps to get you started. You can also ping me directly if I can be of help."}