{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6469", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6469/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6469/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6469/events", "html_url": "https://github.com/pytorch/pytorch/issues/6469", "id": 312982596, "node_id": "MDU6SXNzdWUzMTI5ODI1OTY=", "number": 6469, "title": "[PyTorch] Dense embedding doesn't work with double backward", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131850, "node_id": "MDU6TGFiZWw0MjQxMzE4NTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true}, {"id": 622622153, "node_id": "MDU6TGFiZWw2MjI2MjIxNTM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/newcomer", "name": "newcomer", "color": "0052cc", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2018-04-10T15:46:30Z", "updated_at": "2018-07-13T22:04:46Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Sparse version works, but dense doesn't.</p>\n<p>code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Test</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.embd <span class=\"pl-k\">=</span> torch.nn.Embedding(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">100</span>)\n        <span class=\"pl-c1\">self</span>.dense <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1</span>)\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inp</span>):\n        inp <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embd(inp)\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.dense(inp)\n\ntest <span class=\"pl-k\">=</span> Test()\ntest.cuda()\ninp <span class=\"pl-k\">=</span> Variable(torch.ones(<span class=\"pl-c1\">10</span>).long().cuda())\nout <span class=\"pl-k\">=</span> test(inp)\nraw_loss <span class=\"pl-k\">=</span> out.mean(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\nloss_grad <span class=\"pl-k\">=</span> torch.autograd.grad(<span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>raw_loss,\n                         <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">list</span>(test.parameters()),\n                         <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">only_inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nnorm <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sum</span>([param.norm()<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span> <span class=\"pl-k\">for</span> param <span class=\"pl-k\">in</span> loss_grad])\nloss <span class=\"pl-k\">=</span> raw_loss <span class=\"pl-k\">+</span> norm\n\nloss.backward(<span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p><a href=\"https://discuss.pytorch.org/t/nn-embedding-doesnt-seem-to-support-double-backward/16165\" rel=\"nofollow\">https://discuss.pytorch.org/t/nn-embedding-doesnt-seem-to-support-double-backward/16165</a></p>", "body_text": "Sparse version works, but dense doesn't.\ncode:\nclass Test(torch.nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.embd = torch.nn.Embedding(1000, 100)\n        self.dense = torch.nn.Linear(100, 1)\n    \n    def forward(self, inp):\n        inp = self.embd(inp)\n        return self.dense(inp)\n\ntest = Test()\ntest.cuda()\ninp = Variable(torch.ones(10).long().cuda())\nout = test(inp)\nraw_loss = out.mean(dim=0)\n\nloss_grad = torch.autograd.grad(outputs=raw_loss,\n                         inputs=list(test.parameters()),\n                         retain_graph=True, create_graph=True, only_inputs=True)\nnorm = sum([param.norm()**2 for param in loss_grad])\nloss = raw_loss + norm\n\nloss.backward(retain_graph=True)\nhttps://discuss.pytorch.org/t/nn-embedding-doesnt-seem-to-support-double-backward/16165", "body": "Sparse version works, but dense doesn't.\r\n\r\ncode:\r\n```python\r\nclass Test(torch.nn.Module):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        self.embd = torch.nn.Embedding(1000, 100)\r\n        self.dense = torch.nn.Linear(100, 1)\r\n    \r\n    def forward(self, inp):\r\n        inp = self.embd(inp)\r\n        return self.dense(inp)\r\n\r\ntest = Test()\r\ntest.cuda()\r\ninp = Variable(torch.ones(10).long().cuda())\r\nout = test(inp)\r\nraw_loss = out.mean(dim=0)\r\n\r\nloss_grad = torch.autograd.grad(outputs=raw_loss,\r\n                         inputs=list(test.parameters()),\r\n                         retain_graph=True, create_graph=True, only_inputs=True)\r\nnorm = sum([param.norm()**2 for param in loss_grad])\r\nloss = raw_loss + norm\r\n\r\nloss.backward(retain_graph=True)\r\n```\r\n\r\nhttps://discuss.pytorch.org/t/nn-embedding-doesnt-seem-to-support-double-backward/16165"}