{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4659", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4659/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4659/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4659/events", "html_url": "https://github.com/pytorch/pytorch/issues/4659", "id": 288371538, "node_id": "MDU6SXNzdWUyODgzNzE1Mzg=", "number": 4659, "title": "[feature proposal] adaptive softmax", "user": {"login": "elanmart", "id": 10772830, "node_id": "MDQ6VXNlcjEwNzcyODMw", "avatar_url": "https://avatars3.githubusercontent.com/u/10772830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elanmart", "html_url": "https://github.com/elanmart", "followers_url": "https://api.github.com/users/elanmart/followers", "following_url": "https://api.github.com/users/elanmart/following{/other_user}", "gists_url": "https://api.github.com/users/elanmart/gists{/gist_id}", "starred_url": "https://api.github.com/users/elanmart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elanmart/subscriptions", "organizations_url": "https://api.github.com/users/elanmart/orgs", "repos_url": "https://api.github.com/users/elanmart/repos", "events_url": "https://api.github.com/users/elanmart/events{/privacy}", "received_events_url": "https://api.github.com/users/elanmart/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-14T00:32:36Z", "updated_at": "2018-01-14T22:00:07Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, some time ago Adam mentioned that you were considering adding the <code>adaptive softmax</code>  from [1] to pytorch.</p>\n<p>I wanted to ask if you would be interested in having an <code>nn.Module</code> implementation of that paper.<br>\nFrom my benchmarks on <code>text-8</code> dataset (<code>44371</code> tokens), batch size of <code>128</code>, bptt of <code>20</code>, I get the following timings per batch.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">standard</th>\n<th align=\"center\">adaptive</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">forward [ms]</td>\n<td align=\"center\"><code>58.1</code></td>\n<td align=\"center\"><code>13.5</code></td>\n</tr>\n<tr>\n<td align=\"center\">backward [ms]</td>\n<td align=\"center\"><code>90.74</code></td>\n<td align=\"center\"><code>24.03</code></td>\n</tr>\n</tbody>\n</table>\n<p>I think implementing <code>backward</code> by hand would require moving this to <code>C++</code> since the computation requires calling <code>softmax_backward</code>?</p>\n<p>Please find below the API that I was thinking about.</p>\n<p>[1] <a href=\"https://arxiv.org/abs/1609.04309\" rel=\"nofollow\">Efficient softmax approximation for GPUs</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">AdaptiveLogSoftmax</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_features</span>, <span class=\"pl-smi\">n_classes</span>, <span class=\"pl-smi\">cutoffs</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Parameters</span>\n<span class=\"pl-s\">        ----------</span>\n<span class=\"pl-s\">        in_features : int</span>\n<span class=\"pl-s\">            dimensionality of input data</span>\n<span class=\"pl-s\">        n_classes : int</span>\n<span class=\"pl-s\">            number of classes</span>\n<span class=\"pl-s\">        cutoffs : List[int]</span>\n<span class=\"pl-s\">            cutoff values for clusters. </span>\n<span class=\"pl-s\">            e.g. cutoffs = [100, 1000] means that 100 most frequent labels will</span>\n<span class=\"pl-s\">            end up in main cluster, labels between 101 and 1000</span>\n<span class=\"pl-s\">             will end up in the first sub-cluster, and labels </span>\n<span class=\"pl-s\">            between 1000 and n_classes will end up in second sub-cluster</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n\n        <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">reset_parameters</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">pass</span>        \n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Parameters</span>\n<span class=\"pl-s\">        ----------</span>\n<span class=\"pl-s\">        input : torch.FloatTensor of size (batch_size x self.in_features)</span>\n<span class=\"pl-s\">        target : torch.LongTensor of size (batch_size)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Returns</span>\n<span class=\"pl-s\">        -------</span>\n<span class=\"pl-s\">        output : torch.FloatTensor of size (batch_size)</span>\n<span class=\"pl-s\">            each entry is a log probability of a corresponding `target` value</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n\n        <span class=\"pl-k\">pass</span>\n        \n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_log_proba</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Computes log probabilities for all `self.n_claseess` classes <span class=\"pl-pds\">\"\"\"</span></span>\n\n        <span class=\"pl-k\">pass</span></pre></div>", "body_text": "Hi, some time ago Adam mentioned that you were considering adding the adaptive softmax  from [1] to pytorch.\nI wanted to ask if you would be interested in having an nn.Module implementation of that paper.\nFrom my benchmarks on text-8 dataset (44371 tokens), batch size of 128, bptt of 20, I get the following timings per batch.\n\n\n\n\nstandard\nadaptive\n\n\n\n\nforward [ms]\n58.1\n13.5\n\n\nbackward [ms]\n90.74\n24.03\n\n\n\nI think implementing backward by hand would require moving this to C++ since the computation requires calling softmax_backward?\nPlease find below the API that I was thinking about.\n[1] Efficient softmax approximation for GPUs\nclass AdaptiveLogSoftmax(nn.Module):\n    def __init__(self, in_features, n_classes, cutoffs):\n        \"\"\"\n        Parameters\n        ----------\n        in_features : int\n            dimensionality of input data\n        n_classes : int\n            number of classes\n        cutoffs : List[int]\n            cutoff values for clusters. \n            e.g. cutoffs = [100, 1000] means that 100 most frequent labels will\n            end up in main cluster, labels between 101 and 1000\n             will end up in the first sub-cluster, and labels \n            between 1000 and n_classes will end up in second sub-cluster\n        \"\"\"\n\n        pass\n\n    def reset_parameters(self):\n        pass        \n\n    def forward(self, input, target):\n        \"\"\"\n        Parameters\n        ----------\n        input : torch.FloatTensor of size (batch_size x self.in_features)\n        target : torch.LongTensor of size (batch_size)\n\n        Returns\n        -------\n        output : torch.FloatTensor of size (batch_size)\n            each entry is a log probability of a corresponding `target` value\n        \"\"\"\n\n        pass\n        \n\n    def get_log_proba(self, input):\n        \"\"\" Computes log probabilities for all `self.n_claseess` classes \"\"\"\n\n        pass", "body": "Hi, some time ago Adam mentioned that you were considering adding the `adaptive softmax`  from [1] to pytorch.\r\n\r\nI wanted to ask if you would be interested in having an `nn.Module` implementation of that paper. \r\nFrom my benchmarks on `text-8` dataset (`44371` tokens), batch size of `128`, bptt of `20`, I get the following timings per batch.\r\n\r\n|                | standard   | adaptive |\r\n|:------------:           |:-------------:|:-----:|\r\n| forward [ms]      | `58.1`       |  `13.5` |\r\n| backward [ms]   | `90.74`     | `24.03` |\r\n\r\nI think implementing `backward` by hand would require moving this to `C++` since the computation requires calling `softmax_backward`?\r\n\r\nPlease find below the API that I was thinking about.\r\n\r\n[1] [Efficient softmax approximation for GPUs](https://arxiv.org/abs/1609.04309)\r\n```python\r\n\r\nclass AdaptiveLogSoftmax(nn.Module):\r\n    def __init__(self, in_features, n_classes, cutoffs):\r\n        \"\"\"\r\n        Parameters\r\n        ----------\r\n        in_features : int\r\n            dimensionality of input data\r\n        n_classes : int\r\n            number of classes\r\n        cutoffs : List[int]\r\n            cutoff values for clusters. \r\n            e.g. cutoffs = [100, 1000] means that 100 most frequent labels will\r\n            end up in main cluster, labels between 101 and 1000\r\n             will end up in the first sub-cluster, and labels \r\n            between 1000 and n_classes will end up in second sub-cluster\r\n        \"\"\"\r\n\r\n        pass\r\n\r\n    def reset_parameters(self):\r\n        pass        \r\n\r\n    def forward(self, input, target):\r\n        \"\"\"\r\n        Parameters\r\n        ----------\r\n        input : torch.FloatTensor of size (batch_size x self.in_features)\r\n        target : torch.LongTensor of size (batch_size)\r\n\r\n        Returns\r\n        -------\r\n        output : torch.FloatTensor of size (batch_size)\r\n            each entry is a log probability of a corresponding `target` value\r\n        \"\"\"\r\n\r\n        pass\r\n        \r\n\r\n    def get_log_proba(self, input):\r\n        \"\"\" Computes log probabilities for all `self.n_claseess` classes \"\"\"\r\n\r\n        pass\r\n```"}