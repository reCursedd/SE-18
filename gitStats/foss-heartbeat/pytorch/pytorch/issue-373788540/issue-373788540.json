{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13110", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13110/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13110/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13110/events", "html_url": "https://github.com/pytorch/pytorch/issues/13110", "id": 373788540, "node_id": "MDU6SXNzdWUzNzM3ODg1NDA=", "number": 13110, "title": "UserWarning: ONNX export failed on RNN/GRU/LSTM because batch_first not supported warnings.warn(\"ONNX export failed on \" + op + \" because \" + msg + \" not supported\")", "user": {"login": "wangyunxiaa", "id": 41035013, "node_id": "MDQ6VXNlcjQxMDM1MDEz", "avatar_url": "https://avatars3.githubusercontent.com/u/41035013?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangyunxiaa", "html_url": "https://github.com/wangyunxiaa", "followers_url": "https://api.github.com/users/wangyunxiaa/followers", "following_url": "https://api.github.com/users/wangyunxiaa/following{/other_user}", "gists_url": "https://api.github.com/users/wangyunxiaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangyunxiaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangyunxiaa/subscriptions", "organizations_url": "https://api.github.com/users/wangyunxiaa/orgs", "repos_url": "https://api.github.com/users/wangyunxiaa/repos", "events_url": "https://api.github.com/users/wangyunxiaa/events{/privacy}", "received_events_url": "https://api.github.com/users/wangyunxiaa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-25T06:11:55Z", "updated_at": "2018-11-17T13:28:01Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hello,<br>\nI want to convert a pre-trained  pytorch model to onnx,but when i do the convert,<br>\n<code>torch.onnx.export(lang_model, (dummy_input1, dummy_input2), \"lmmodel.onnx\", verbose=True, input_names=input_names, output_names=output_names)</code><br>\nI have an error about LSTM layer:<br>\n<code>Traceback (most recent call last): File \"torch2onnx.py\", line 82,  in &lt;module&gt; torch.onnx.export(lang_model, (dummy_input1,  dummy_input2), \"lmmodel.onnx\", verbose=True, input_names=input_names,  output_names=output_names) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/__init__.py\",  line 27, in export return utils.export(*args, **kwargs) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/utils.py\",  line 104, in export operator_export_type=operator_export_type) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/utils.py\",  line 286, in _export proto, export_map = graph.export(params,  _onnx_opset_version, defer_weight_export, operator_export_type)  RuntimeError: ONNX export failed: Couldn't export operator aten::lstm</code><br>\nThe LSTM layer is set batch_first=True when do training stage, does onnx  support the layer of LSTM, what should i do to support it?<br>\nThanks~</p>", "body_text": "Hello,\nI want to convert a pre-trained  pytorch model to onnx,but when i do the convert,\ntorch.onnx.export(lang_model, (dummy_input1, dummy_input2), \"lmmodel.onnx\", verbose=True, input_names=input_names, output_names=output_names)\nI have an error about LSTM layer:\nTraceback (most recent call last): File \"torch2onnx.py\", line 82,  in <module> torch.onnx.export(lang_model, (dummy_input1,  dummy_input2), \"lmmodel.onnx\", verbose=True, input_names=input_names,  output_names=output_names) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/__init__.py\",  line 27, in export return utils.export(*args, **kwargs) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/utils.py\",  line 104, in export operator_export_type=operator_export_type) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/utils.py\",  line 286, in _export proto, export_map = graph.export(params,  _onnx_opset_version, defer_weight_export, operator_export_type)  RuntimeError: ONNX export failed: Couldn't export operator aten::lstm\nThe LSTM layer is set batch_first=True when do training stage, does onnx  support the layer of LSTM, what should i do to support it?\nThanks~", "body": "\r\nHello,\r\n I want to convert a pre-trained  pytorch model to onnx,but when i do the convert,\r\n`torch.onnx.export(lang_model, (dummy_input1, dummy_input2), \"lmmodel.onnx\", verbose=True, input_names=input_names, output_names=output_names)`\r\nI have an error about LSTM layer:\r\n`Traceback (most recent call last): File \"torch2onnx.py\", line 82,  in <module> torch.onnx.export(lang_model, (dummy_input1,  dummy_input2), \"lmmodel.onnx\", verbose=True, input_names=input_names,  output_names=output_names) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/__init__.py\",  line 27, in export return utils.export(*args, **kwargs) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/utils.py\",  line 104, in export operator_export_type=operator_export_type) File  \"/home/yunxiawang/anaconda2/envs/py36_torch_onnx_source/lib/python3.6/site-packages/torch/onnx/utils.py\",  line 286, in _export proto, export_map = graph.export(params,  _onnx_opset_version, defer_weight_export, operator_export_type)  RuntimeError: ONNX export failed: Couldn't export operator aten::lstm`\r\nThe LSTM layer is set batch_first=True when do training stage, does onnx  support the layer of LSTM, what should i do to support it? \r\nThanks~\r\n\r\n\r\n\r\n"}