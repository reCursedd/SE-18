{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12426", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12426/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12426/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12426/events", "html_url": "https://github.com/pytorch/pytorch/issues/12426", "id": 367492710, "node_id": "MDU6SXNzdWUzNjc0OTI3MTA=", "number": 12426, "title": "the derivative for 'pow' is not implemented", "user": {"login": "DriesSmit", "id": 30599100, "node_id": "MDQ6VXNlcjMwNTk5MTAw", "avatar_url": "https://avatars0.githubusercontent.com/u/30599100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DriesSmit", "html_url": "https://github.com/DriesSmit", "followers_url": "https://api.github.com/users/DriesSmit/followers", "following_url": "https://api.github.com/users/DriesSmit/following{/other_user}", "gists_url": "https://api.github.com/users/DriesSmit/gists{/gist_id}", "starred_url": "https://api.github.com/users/DriesSmit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DriesSmit/subscriptions", "organizations_url": "https://api.github.com/users/DriesSmit/orgs", "repos_url": "https://api.github.com/users/DriesSmit/repos", "events_url": "https://api.github.com/users/DriesSmit/events{/privacy}", "received_events_url": "https://api.github.com/users/DriesSmit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-10-06T20:23:34Z", "updated_at": "2018-10-09T18:40:16Z", "closed_at": "2018-10-09T18:40:16Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>When I try to use autograd through the torch.pow function, I get the following error: RuntimeError: the derivative for pow is not implemented. I am using 'e' as the base of the power.</p>\n<p>Would It be possible to implement this functionality for 'e' or maybe direct me to an existing way to do this for the special case when the base is the natural number 'e'? I am using pow to implement my own version of the logsumexp function so that I don't need to create a 3D matrix first and then use the current torch.logsumexp function to collapse it back to a 2D matrix.</p>\n<p>Thank you.</p>", "body_text": "\ud83d\ude80 Feature\nWhen I try to use autograd through the torch.pow function, I get the following error: RuntimeError: the derivative for pow is not implemented. I am using 'e' as the base of the power.\nWould It be possible to implement this functionality for 'e' or maybe direct me to an existing way to do this for the special case when the base is the natural number 'e'? I am using pow to implement my own version of the logsumexp function so that I don't need to create a 3D matrix first and then use the current torch.logsumexp function to collapse it back to a 2D matrix.\nThank you.", "body": "## \ud83d\ude80 Feature\r\nWhen I try to use autograd through the torch.pow function, I get the following error: RuntimeError: the derivative for pow is not implemented. I am using 'e' as the base of the power. \r\n\r\nWould It be possible to implement this functionality for 'e' or maybe direct me to an existing way to do this for the special case when the base is the natural number 'e'? I am using pow to implement my own version of the logsumexp function so that I don't need to create a 3D matrix first and then use the current torch.logsumexp function to collapse it back to a 2D matrix.\r\n\r\nThank you."}