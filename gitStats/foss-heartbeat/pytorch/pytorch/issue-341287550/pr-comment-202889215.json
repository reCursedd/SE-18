{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202889215", "pull_request_review_id": 137670612, "id": 202889215, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjg4OTIxNQ==", "diff_hunk": "@@ -0,0 +1,50 @@\n+from numbers import Number\n+import math\n+import torch\n+from torch.distributions import constraints\n+from torch.distributions.uniform import Uniform\n+from torch.distributions.transformed_distribution import TransformedDistribution\n+from torch.distributions.transforms import AffineTransform, ExpTransform, PowerTransform\n+from torch.distributions.utils import _finfo, broadcast_all\n+from torch.distributions.gumbel import euler_constant\n+\n+\n+class Weibull(TransformedDistribution):\n+    r\"\"\"\n+    Samples from a two-parameter Weibull distribution.\n+\n+    Example:\n+\n+        >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n+        >>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\n+        tensor([ 0.4784])\n+\n+    Args:\n+        scale (float or Tensor): Scale parameter of distribution (lambda).\n+        concentration (float or Tensor): Concentration parameter of distribution (k/shape).\n+    \"\"\"\n+    arg_constraints = {'scale': constraints.positive, 'concentration': constraints.positive}\n+    support = constraints.positive\n+\n+    def __init__(self, scale, concentration, validate_args=None):\n+        self.scale, self.concentration = broadcast_all(scale, concentration)\n+        self.concentration_reciprocal = self.concentration.reciprocal()\n+        finfo = _finfo(self.scale)\n+        base_dist = Uniform(self.scale.new(self.scale.size()).fill_(finfo.tiny), 1 - finfo.eps)", "path": "torch/distributions/weibull.py", "position": null, "original_position": 33, "commit_id": "b85dd4803e7f166e1f2d368c18f5fb941c1e9694", "original_commit_id": "3ea69d6f6432584e722415a8dfb8c31d8a9b3b72", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "Is there a reason you're using `log(Uniform)` rather than `Exponential`? It might be cheaper to use `Exponential` which calls the PyTorch exponential generator under the hood. This would also reduce the number of transforms from 4 to 2.\r\n```py\r\nbase_dist = Exponential(self.scale.new_tensor(self.scale.size()).fill_(1.0))\r\ntransforms = [PowerTransform(exponent=self.concentration_reciprocal),\r\n              AffineTransform(loc=0, scale=self.scale)]\r\n```", "created_at": "2018-07-17T04:08:38Z", "updated_at": "2018-11-23T15:47:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/9454#discussion_r202889215", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9454", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202889215"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9454#discussion_r202889215"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9454"}}, "body_html": "<p>Is there a reason you're using <code>log(Uniform)</code> rather than <code>Exponential</code>? It might be cheaper to use <code>Exponential</code> which calls the PyTorch exponential generator under the hood. This would also reduce the number of transforms from 4 to 2.</p>\n<div class=\"highlight highlight-source-python\"><pre>base_dist <span class=\"pl-k\">=</span> Exponential(<span class=\"pl-c1\">self</span>.scale.new_tensor(<span class=\"pl-c1\">self</span>.scale.size()).fill_(<span class=\"pl-c1\">1.0</span>))\ntransforms <span class=\"pl-k\">=</span> [PowerTransform(<span class=\"pl-v\">exponent</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.concentration_reciprocal),\n              AffineTransform(<span class=\"pl-v\">loc</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.scale)]</pre></div>", "body_text": "Is there a reason you're using log(Uniform) rather than Exponential? It might be cheaper to use Exponential which calls the PyTorch exponential generator under the hood. This would also reduce the number of transforms from 4 to 2.\nbase_dist = Exponential(self.scale.new_tensor(self.scale.size()).fill_(1.0))\ntransforms = [PowerTransform(exponent=self.concentration_reciprocal),\n              AffineTransform(loc=0, scale=self.scale)]"}