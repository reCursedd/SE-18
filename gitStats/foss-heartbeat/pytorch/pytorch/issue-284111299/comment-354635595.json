{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/354635595", "html_url": "https://github.com/pytorch/pytorch/pull/4312#issuecomment-354635595", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4312", "id": 354635595, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDYzNTU5NQ==", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-01T04:01:37Z", "updated_at": "2018-01-02T04:17:53Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Changes</h2>\n<p>Latest 2 commits make the following changes:</p>\n<ul>\n<li>Actually use the mean and standard deviation (was not using the variables before and thus generating unit Gaussian samples),</li>\n<li>Added a scalar <code>normal_fill</code> function that interleaves values just like the vectorized code, so that there is no difference in generated samples between AVX and non-AVX platforms. This function is also around 1.5x faster than the old version. Thus also non-AVX versions get a speedup (for contiguous tensors with at least 16 values).</li>\n<li>Implemented all the vector dispatch stuff</li>\n<li>Using <code>int64_t</code> for size</li>\n<li>Using <code>THAssert</code> instead of <code>assert</code></li>\n<li>AVX code uses <code>_m256_loadu_ps</code> instead of <code>_m256_load_ps</code>, i.e. misaligned loads so that it works also for misaligned data,</li>\n<li>Using explicit FMA <code>_m256_fmadd_ps</code> instruction instead of multiply + add (clang doesn't do this automatically, GCC does ... it's less code anyway. Also FMA is available wherever AVX is on Intel and AMD chips, so should be fine). Marginally faster.</li>\n</ul>\n<h2>Benchmarks</h2>\n<p>This time compiling with GCC 7.</p>\n<h3>Microbenchmarks</h3>\n<p>10,000 x 10,000 (float/AVX): 3.3s -&gt; 0.48s (6.875x speedup)<br>\n1,000 x 1,000 (float/AVX): 35ms -&gt; 4.9ms (7.1x speedup)<br>\n10,000 x 10,000 (double/scalar): 3.2s -&gt; 2.1s (1.5x speedup)</p>\n<p>float/AVX here means it's using the vectorized version, double/scalar means it's the interleaved scalar function that I added, since the vectorized version is only called for floats.</p>\n<h3>Imagenet Startup Times</h3>\n<p>VGG19: 5.61s -&gt; 1.62s (3.5x speedup)<br>\nResNet101: 2.4s -&gt; 0.65s (3.7x speedup)<br>\nResNet50: 1.21s -&gt; 0.46s (2.6x speedup)</p>\n<p>(Please re-review the code <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> )</p>\n<p>Happy new year <g-emoji class=\"g-emoji\" alias=\"fireworks\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f386.png\">\ud83c\udf86</g-emoji> <g-emoji class=\"g-emoji\" alias=\"tada\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f389.png\">\ud83c\udf89</g-emoji></p>", "body_text": "Changes\nLatest 2 commits make the following changes:\n\nActually use the mean and standard deviation (was not using the variables before and thus generating unit Gaussian samples),\nAdded a scalar normal_fill function that interleaves values just like the vectorized code, so that there is no difference in generated samples between AVX and non-AVX platforms. This function is also around 1.5x faster than the old version. Thus also non-AVX versions get a speedup (for contiguous tensors with at least 16 values).\nImplemented all the vector dispatch stuff\nUsing int64_t for size\nUsing THAssert instead of assert\nAVX code uses _m256_loadu_ps instead of _m256_load_ps, i.e. misaligned loads so that it works also for misaligned data,\nUsing explicit FMA _m256_fmadd_ps instruction instead of multiply + add (clang doesn't do this automatically, GCC does ... it's less code anyway. Also FMA is available wherever AVX is on Intel and AMD chips, so should be fine). Marginally faster.\n\nBenchmarks\nThis time compiling with GCC 7.\nMicrobenchmarks\n10,000 x 10,000 (float/AVX): 3.3s -> 0.48s (6.875x speedup)\n1,000 x 1,000 (float/AVX): 35ms -> 4.9ms (7.1x speedup)\n10,000 x 10,000 (double/scalar): 3.2s -> 2.1s (1.5x speedup)\nfloat/AVX here means it's using the vectorized version, double/scalar means it's the interleaved scalar function that I added, since the vectorized version is only called for floats.\nImagenet Startup Times\nVGG19: 5.61s -> 1.62s (3.5x speedup)\nResNet101: 2.4s -> 0.65s (3.7x speedup)\nResNet50: 1.21s -> 0.46s (2.6x speedup)\n(Please re-review the code @colesbury @zdevito @soumith )\nHappy new year \ud83c\udf86 \ud83c\udf89", "body": "## Changes\r\n\r\nLatest 2 commits make the following changes:\r\n\r\n- Actually use the mean and standard deviation (was not using the variables before and thus generating unit Gaussian samples),\r\n- Added a scalar `normal_fill` function that interleaves values just like the vectorized code, so that there is no difference in generated samples between AVX and non-AVX platforms. This function is also around 1.5x faster than the old version. Thus also non-AVX versions get a speedup (for contiguous tensors with at least 16 values).\r\n- Implemented all the vector dispatch stuff\r\n- Using `int64_t` for size\r\n- Using `THAssert` instead of `assert`\r\n- AVX code uses `_m256_loadu_ps` instead of `_m256_load_ps`, i.e. misaligned loads so that it works also for misaligned data,\r\n- Using explicit FMA `_m256_fmadd_ps` instruction instead of multiply + add (clang doesn't do this automatically, GCC does ... it's less code anyway. Also FMA is available wherever AVX is on Intel and AMD chips, so should be fine). Marginally faster.\r\n\r\n## Benchmarks\r\n\r\nThis time compiling with GCC 7.\r\n\r\n### Microbenchmarks\r\n\r\n10,000 x 10,000 (float/AVX): 3.3s -> 0.48s (6.875x speedup)\r\n1,000 x 1,000 (float/AVX): 35ms -> 4.9ms (7.1x speedup)\r\n10,000 x 10,000 (double/scalar): 3.2s -> 2.1s (1.5x speedup)\r\n\r\nfloat/AVX here means it's using the vectorized version, double/scalar means it's the interleaved scalar function that I added, since the vectorized version is only called for floats.\r\n\r\n### Imagenet Startup Times\r\n\r\nVGG19: 5.61s -> 1.62s (3.5x speedup)\r\nResNet101: 2.4s -> 0.65s (3.7x speedup)\r\nResNet50: 1.21s -> 0.46s (2.6x speedup)\r\n\r\n(Please re-review the code @colesbury @zdevito @soumith )\r\n\r\nHappy new year \ud83c\udf86 :tada:"}