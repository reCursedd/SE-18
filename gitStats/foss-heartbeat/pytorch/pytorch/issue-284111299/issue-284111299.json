{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4312", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4312/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4312/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4312/events", "html_url": "https://github.com/pytorch/pytorch/pull/4312", "id": 284111299, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU5ODI3ODg0", "number": 4312, "title": "Vectorize normal_", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-12-22T09:12:31Z", "updated_at": "2018-11-23T15:37:46Z", "closed_at": "2018-01-03T21:30:55Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4312", "html_url": "https://github.com/pytorch/pytorch/pull/4312", "diff_url": "https://github.com/pytorch/pytorch/pull/4312.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4312.patch"}, "body_html": "<p>PyTorch is known to have high start-up times, to a large part due to slow tensor initialization/filling when calling <code>normal_</code>. <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THRandom.c#L253\">Torch's <code>normal</code> function</a> uses the Box-Mueller transform to produce Gaussian distributed floats from uniform floats. I did some benchmarks and found that generating normal numbers took around 5 times longer than generating only uniform floats, suggesting that the current normal sampling code was the bottleneck.</p>\n<p>This PR addresses this by introducing a vectorized version of the Box-Mueller transform that essentially does the same thing, but for 8 values at a time. This version is called only for floats, only if we have AVX2, only if there are more than 16 values (due to implementation) and only if the tensor is contiguous. However, this should cover like 90%-95% of real-world cases where it's currently slow.</p>\n<p>My initial, small-scale benchmarks show a 5x-6x speed-up:</p>\n<p>Before:</p>\n<pre><code>In [1]: import torch\nIn [2]: x = torch.Tensor(10000, 10000)\nIn [3]: %time _ = x.normal_(0, 1)\nCPU times: user 3.45 s, sys: 111 ms, total: 3.57 s\nWall time: 3.57 s\n</code></pre>\n<p>After:</p>\n<pre><code>In [1]: import torch\nIn [2]: x = torch.Tensor(10000, 10000)\nIn [3]: %time _ = x.normal_(0, 1)\nCPU times: user 611 ms, sys: 1.07 ms, total: 612 ms\nWall time: 613 ms\n</code></pre>\n<p>Which looks pretty good. I will see how this affects loading a model like imagenet or similar.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a></p>", "body_text": "PyTorch is known to have high start-up times, to a large part due to slow tensor initialization/filling when calling normal_. Torch's normal function uses the Box-Mueller transform to produce Gaussian distributed floats from uniform floats. I did some benchmarks and found that generating normal numbers took around 5 times longer than generating only uniform floats, suggesting that the current normal sampling code was the bottleneck.\nThis PR addresses this by introducing a vectorized version of the Box-Mueller transform that essentially does the same thing, but for 8 values at a time. This version is called only for floats, only if we have AVX2, only if there are more than 16 values (due to implementation) and only if the tensor is contiguous. However, this should cover like 90%-95% of real-world cases where it's currently slow.\nMy initial, small-scale benchmarks show a 5x-6x speed-up:\nBefore:\nIn [1]: import torch\nIn [2]: x = torch.Tensor(10000, 10000)\nIn [3]: %time _ = x.normal_(0, 1)\nCPU times: user 3.45 s, sys: 111 ms, total: 3.57 s\nWall time: 3.57 s\n\nAfter:\nIn [1]: import torch\nIn [2]: x = torch.Tensor(10000, 10000)\nIn [3]: %time _ = x.normal_(0, 1)\nCPU times: user 611 ms, sys: 1.07 ms, total: 612 ms\nWall time: 613 ms\n\nWhich looks pretty good. I will see how this affects loading a model like imagenet or similar.\nCC @zdevito", "body": "PyTorch is known to have high start-up times, to a large part due to slow tensor initialization/filling when calling `normal_`. [Torch's `normal` function](https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THRandom.c#L253) uses the Box-Mueller transform to produce Gaussian distributed floats from uniform floats. I did some benchmarks and found that generating normal numbers took around 5 times longer than generating only uniform floats, suggesting that the current normal sampling code was the bottleneck.\r\n\r\nThis PR addresses this by introducing a vectorized version of the Box-Mueller transform that essentially does the same thing, but for 8 values at a time. This version is called only for floats, only if we have AVX2, only if there are more than 16 values (due to implementation) and only if the tensor is contiguous. However, this should cover like 90%-95% of real-world cases where it's currently slow.\r\n\r\nMy initial, small-scale benchmarks show a 5x-6x speed-up:\r\n\r\nBefore:\r\n```\r\nIn [1]: import torch\r\nIn [2]: x = torch.Tensor(10000, 10000)\r\nIn [3]: %time _ = x.normal_(0, 1)\r\nCPU times: user 3.45 s, sys: 111 ms, total: 3.57 s\r\nWall time: 3.57 s\r\n```\r\n\r\nAfter:\r\n```\r\nIn [1]: import torch\r\nIn [2]: x = torch.Tensor(10000, 10000)\r\nIn [3]: %time _ = x.normal_(0, 1)\r\nCPU times: user 611 ms, sys: 1.07 ms, total: 612 ms\r\nWall time: 613 ms\r\n```\r\n\r\nWhich looks pretty good. I will see how this affects loading a model like imagenet or similar.\r\n\r\nCC @zdevito "}