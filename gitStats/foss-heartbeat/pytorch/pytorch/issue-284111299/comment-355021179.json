{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355021179", "html_url": "https://github.com/pytorch/pytorch/pull/4312#issuecomment-355021179", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4312", "id": 355021179, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTAyMTE3OQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-03T14:13:51Z", "updated_at": "2018-01-03T14:14:02Z", "author_association": "MEMBER", "body_html": "<p>Alright, I looked into it and it seems that the test that's failing now is particularily flaky when using half (63 failures / 1000 trials). Reducing the scale of values used to test it makes the absolute errors smaller, and it succeeded 10000 times now. Here's the patch (just add <code>/ 2</code> in two places):</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-md\">--- a/test/test_nn.py                                                                    </span>\n<span class=\"pl-mi1\">+++ b/test/test_nn.py                                                                    </span>\n<span class=\"pl-mdr\">@@ -2132,9 +2132,9 @@</span> class TestNN(NNTestCase):                                          \n                 continue                                                                \n             for depth_multiplier in [1, 2]:                                             \n                 m = nn.Conv2d(2, 2 * depth_multiplier, kernel_size=3, groups=2).type(tp)\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                i = Variable(torch.randn(2, 2, 6, 6).type(tp), requires_grad=True)      </span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                i = Variable(torch.randn(2, 2, 6, 6).type(tp) / 2, requires_grad=True)  </span>\n                 output = m(i)                                                           \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                grad_output = torch.randn(2, 2 * depth_multiplier, 4, 4).type(tp)       </span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                grad_output = torch.randn(2, 2 * depth_multiplier, 4, 4).type(tp) / 2   </span>\n                 output.backward(grad_output)                                            \n                                                                                         \n                 offset = 1 * depth_multiplier                                                                                      </pre></div>", "body_text": "Alright, I looked into it and it seems that the test that's failing now is particularily flaky when using half (63 failures / 1000 trials). Reducing the scale of values used to test it makes the absolute errors smaller, and it succeeded 10000 times now. Here's the patch (just add / 2 in two places):\n--- a/test/test_nn.py                                                                    \n+++ b/test/test_nn.py                                                                    \n@@ -2132,9 +2132,9 @@ class TestNN(NNTestCase):                                          \n                 continue                                                                \n             for depth_multiplier in [1, 2]:                                             \n                 m = nn.Conv2d(2, 2 * depth_multiplier, kernel_size=3, groups=2).type(tp)\n-                i = Variable(torch.randn(2, 2, 6, 6).type(tp), requires_grad=True)      \n+                i = Variable(torch.randn(2, 2, 6, 6).type(tp) / 2, requires_grad=True)  \n                 output = m(i)                                                           \n-                grad_output = torch.randn(2, 2 * depth_multiplier, 4, 4).type(tp)       \n+                grad_output = torch.randn(2, 2 * depth_multiplier, 4, 4).type(tp) / 2   \n                 output.backward(grad_output)                                            \n                                                                                         \n                 offset = 1 * depth_multiplier", "body": "Alright, I looked into it and it seems that the test that's failing now is particularily flaky when using half (63 failures / 1000 trials). Reducing the scale of values used to test it makes the absolute errors smaller, and it succeeded 10000 times now. Here's the patch (just add `/ 2` in two places):\r\n```patch\r\n--- a/test/test_nn.py                                                                    \r\n+++ b/test/test_nn.py                                                                    \r\n@@ -2132,9 +2132,9 @@ class TestNN(NNTestCase):                                          \r\n                 continue                                                                \r\n             for depth_multiplier in [1, 2]:                                             \r\n                 m = nn.Conv2d(2, 2 * depth_multiplier, kernel_size=3, groups=2).type(tp)\r\n-                i = Variable(torch.randn(2, 2, 6, 6).type(tp), requires_grad=True)      \r\n+                i = Variable(torch.randn(2, 2, 6, 6).type(tp) / 2, requires_grad=True)  \r\n                 output = m(i)                                                           \r\n-                grad_output = torch.randn(2, 2 * depth_multiplier, 4, 4).type(tp)       \r\n+                grad_output = torch.randn(2, 2 * depth_multiplier, 4, 4).type(tp) / 2   \r\n                 output.backward(grad_output)                                            \r\n                                                                                         \r\n                 offset = 1 * depth_multiplier                                                                                      \r\n```\r\n  "}