{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11582", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11582/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11582/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11582/events", "html_url": "https://github.com/pytorch/pytorch/issues/11582", "id": 359579193, "node_id": "MDU6SXNzdWUzNTk1NzkxOTM=", "number": 11582, "title": "Some distributed tests are flaky", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-09-12T17:26:33Z", "updated_at": "2018-09-18T00:33:00Z", "closed_at": "2018-09-18T00:33:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>test_DistributedDataParallel:</p>\n<pre><code>15:44:47 MPI not available -- MPI backend tests will be skipped\n15:44:47 Running distributed tests for the gloo backend\n15:44:47 test_DistributedDataParallel (__main__.TestDistBackend) ... Process process 2:\n15:44:47 Traceback (most recent call last):\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n15:44:47     self.run()\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py\", line 114, in run\n15:44:47     self._target(*self._args, **self._kwargs)\n15:44:47   File \"test_distributed.py\", line 1268, in _run\n15:44:47     rank=self.rank\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:47     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:47 RuntimeError: Connection reset by peer\n15:44:47 FAIL\n\n...\n\n\n15:44:53 ======================================================================\n15:44:53 FAIL: test_DistributedDataParallel (__main__.TestDistBackend)\n15:44:53 ----------------------------------------------------------------------\n15:44:53 Traceback (most recent call last):\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\n15:44:53     self._join_and_reduce(fn)\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n15:44:53 AssertionError: 74 not less than or equal to 1e-05\n15:44:53 \n</code></pre>\n<p><a href=\"https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py2.7.9-test/17578/console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py2.7.9-test/17578/console</a></p>\n<p>test_broadcast_cuda:</p>\n<pre><code>15:44:01 test_broadcast_cuda (__main__.TestDistBackend) ... Process process 2:\n15:44:01 Traceback (most recent call last):\n15:44:01   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n15:44:01     self.run()\n15:44:01   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n15:44:01     self._target(*self._args, **self._kwargs)\n15:44:01   File \"test_distributed.py\", line 1268, in _run\n15:44:01     rank=self.rank\n15:44:01   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:01     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:01 RuntimeError: Connection reset by peer\n15:44:51 Process process 1:\n15:44:51 Traceback (most recent call last):\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n15:44:51     self.run()\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n15:44:51     self._target(*self._args, **self._kwargs)\n15:44:51   File \"test_distributed.py\", line 1268, in _run\n15:44:51     rank=self.rank\n15:44:51   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:51     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:51 RuntimeError: [/var/lib/jenkins/workspace/third_party/gloo/gloo/transport/tcp/pair.cc:834] Connect timeout [172.17.0.2]:55959\n15:44:51 FAIL\n\n...\n\n\n15:44:53 ======================================================================\n15:44:53 FAIL: test_broadcast_cuda (__main__.TestDistBackend)\n15:44:53 ----------------------------------------------------------------------\n15:44:53 Traceback (most recent call last):\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\n15:44:53     self._join_and_reduce(fn)\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n15:44:53 AssertionError: 74 not less than or equal to 1e-05 : \n15:44:53 \n</code></pre>\n<p><a href=\"https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console</a></p>\n<p>test_broadcast_multigpu:</p>\n<pre><code>15:44:51 test_broadcast_multigpu (__main__.TestDistBackend) ... Process process 2:\n15:44:51 Traceback (most recent call last):\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n15:44:51     self.run()\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n15:44:51     self._target(*self._args, **self._kwargs)\n15:44:51   File \"test_distributed.py\", line 1268, in _run\n15:44:51     rank=self.rank\n15:44:51   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:51     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:51 RuntimeError: Connection reset by peer\n15:44:51 FAIL\n\n...\n\n15:44:53 ======================================================================\n15:44:53 FAIL: test_broadcast_multigpu (__main__.TestDistBackend)\n15:44:53 ----------------------------------------------------------------------\n15:44:53 Traceback (most recent call last):\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\n15:44:53     self._join_and_reduce(fn)\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n15:44:53 AssertionError: 74 not less than or equal to 1e-05 : \n15:44:53 \n</code></pre>\n<p><a href=\"https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console</a></p>\n<p>test_all_reduce_sum_cuda:</p>\n<pre><code>17:05:23 ======================================================================\n17:05:23 FAIL: test_all_reduce_sum_cuda (__main__.TestDistBackend)\n17:05:23 ----------------------------------------------------------------------\n17:05:23 Traceback (most recent call last):\n17:05:23   File \"test_distributed.py\", line 1217, in wrapper\n17:05:23     self._join_and_reduce(fn)\n17:05:23   File \"test_distributed.py\", line 1294, in _join_and_reduce\n17:05:23     self.assertEqual(p.exitcode, first_process.exitcode)\n17:05:23   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n17:05:23     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n17:05:23 AssertionError: 74 not less than or equal to 1e-05 : \n17:05:23 \n17:05:23 ----------------------------------------------------------------------\n</code></pre>\n<p><a href=\"https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.6-gcc5.4-test/17697//console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.6-gcc5.4-test/17697//console</a></p>", "body_text": "test_DistributedDataParallel:\n15:44:47 MPI not available -- MPI backend tests will be skipped\n15:44:47 Running distributed tests for the gloo backend\n15:44:47 test_DistributedDataParallel (__main__.TestDistBackend) ... Process process 2:\n15:44:47 Traceback (most recent call last):\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n15:44:47     self.run()\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py\", line 114, in run\n15:44:47     self._target(*self._args, **self._kwargs)\n15:44:47   File \"test_distributed.py\", line 1268, in _run\n15:44:47     rank=self.rank\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:47     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:47 RuntimeError: Connection reset by peer\n15:44:47 FAIL\n\n...\n\n\n15:44:53 ======================================================================\n15:44:53 FAIL: test_DistributedDataParallel (__main__.TestDistBackend)\n15:44:53 ----------------------------------------------------------------------\n15:44:53 Traceback (most recent call last):\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\n15:44:53     self._join_and_reduce(fn)\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n15:44:53 AssertionError: 74 not less than or equal to 1e-05\n15:44:53 \n\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py2.7.9-test/17578/console\ntest_broadcast_cuda:\n15:44:01 test_broadcast_cuda (__main__.TestDistBackend) ... Process process 2:\n15:44:01 Traceback (most recent call last):\n15:44:01   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n15:44:01     self.run()\n15:44:01   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n15:44:01     self._target(*self._args, **self._kwargs)\n15:44:01   File \"test_distributed.py\", line 1268, in _run\n15:44:01     rank=self.rank\n15:44:01   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:01     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:01 RuntimeError: Connection reset by peer\n15:44:51 Process process 1:\n15:44:51 Traceback (most recent call last):\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n15:44:51     self.run()\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n15:44:51     self._target(*self._args, **self._kwargs)\n15:44:51   File \"test_distributed.py\", line 1268, in _run\n15:44:51     rank=self.rank\n15:44:51   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:51     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:51 RuntimeError: [/var/lib/jenkins/workspace/third_party/gloo/gloo/transport/tcp/pair.cc:834] Connect timeout [172.17.0.2]:55959\n15:44:51 FAIL\n\n...\n\n\n15:44:53 ======================================================================\n15:44:53 FAIL: test_broadcast_cuda (__main__.TestDistBackend)\n15:44:53 ----------------------------------------------------------------------\n15:44:53 Traceback (most recent call last):\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\n15:44:53     self._join_and_reduce(fn)\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n15:44:53 AssertionError: 74 not less than or equal to 1e-05 : \n15:44:53 \n\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console\ntest_broadcast_multigpu:\n15:44:51 test_broadcast_multigpu (__main__.TestDistBackend) ... Process process 2:\n15:44:51 Traceback (most recent call last):\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n15:44:51     self.run()\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n15:44:51     self._target(*self._args, **self._kwargs)\n15:44:51   File \"test_distributed.py\", line 1268, in _run\n15:44:51     rank=self.rank\n15:44:51   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\n15:44:51     _default_pg = ProcessGroupGloo(store, rank, world_size)\n15:44:51 RuntimeError: Connection reset by peer\n15:44:51 FAIL\n\n...\n\n15:44:53 ======================================================================\n15:44:53 FAIL: test_broadcast_multigpu (__main__.TestDistBackend)\n15:44:53 ----------------------------------------------------------------------\n15:44:53 Traceback (most recent call last):\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\n15:44:53     self._join_and_reduce(fn)\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n15:44:53 AssertionError: 74 not less than or equal to 1e-05 : \n15:44:53 \n\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console\ntest_all_reduce_sum_cuda:\n17:05:23 ======================================================================\n17:05:23 FAIL: test_all_reduce_sum_cuda (__main__.TestDistBackend)\n17:05:23 ----------------------------------------------------------------------\n17:05:23 Traceback (most recent call last):\n17:05:23   File \"test_distributed.py\", line 1217, in wrapper\n17:05:23     self._join_and_reduce(fn)\n17:05:23   File \"test_distributed.py\", line 1294, in _join_and_reduce\n17:05:23     self.assertEqual(p.exitcode, first_process.exitcode)\n17:05:23   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\n17:05:23     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n17:05:23 AssertionError: 74 not less than or equal to 1e-05 : \n17:05:23 \n17:05:23 ----------------------------------------------------------------------\n\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.6-gcc5.4-test/17697//console", "body": "test_DistributedDataParallel:\r\n```\r\n15:44:47 MPI not available -- MPI backend tests will be skipped\r\n15:44:47 Running distributed tests for the gloo backend\r\n15:44:47 test_DistributedDataParallel (__main__.TestDistBackend) ... Process process 2:\r\n15:44:47 Traceback (most recent call last):\r\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\r\n15:44:47     self.run()\r\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py\", line 114, in run\r\n15:44:47     self._target(*self._args, **self._kwargs)\r\n15:44:47   File \"test_distributed.py\", line 1268, in _run\r\n15:44:47     rank=self.rank\r\n15:44:47   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\r\n15:44:47     _default_pg = ProcessGroupGloo(store, rank, world_size)\r\n15:44:47 RuntimeError: Connection reset by peer\r\n15:44:47 FAIL\r\n\r\n...\r\n\r\n\r\n15:44:53 ======================================================================\r\n15:44:53 FAIL: test_DistributedDataParallel (__main__.TestDistBackend)\r\n15:44:53 ----------------------------------------------------------------------\r\n15:44:53 Traceback (most recent call last):\r\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\r\n15:44:53     self._join_and_reduce(fn)\r\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\r\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\r\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\r\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\n15:44:53 AssertionError: 74 not less than or equal to 1e-05\r\n15:44:53 \r\n```\r\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py2.7.9-test/17578/console\r\n\r\ntest_broadcast_cuda:\r\n```\r\n15:44:01 test_broadcast_cuda (__main__.TestDistBackend) ... Process process 2:\r\n15:44:01 Traceback (most recent call last):\r\n15:44:01   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\r\n15:44:01     self.run()\r\n15:44:01   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n15:44:01     self._target(*self._args, **self._kwargs)\r\n15:44:01   File \"test_distributed.py\", line 1268, in _run\r\n15:44:01     rank=self.rank\r\n15:44:01   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\r\n15:44:01     _default_pg = ProcessGroupGloo(store, rank, world_size)\r\n15:44:01 RuntimeError: Connection reset by peer\r\n15:44:51 Process process 1:\r\n15:44:51 Traceback (most recent call last):\r\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\r\n15:44:51     self.run()\r\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n15:44:51     self._target(*self._args, **self._kwargs)\r\n15:44:51   File \"test_distributed.py\", line 1268, in _run\r\n15:44:51     rank=self.rank\r\n15:44:51   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\r\n15:44:51     _default_pg = ProcessGroupGloo(store, rank, world_size)\r\n15:44:51 RuntimeError: [/var/lib/jenkins/workspace/third_party/gloo/gloo/transport/tcp/pair.cc:834] Connect timeout [172.17.0.2]:55959\r\n15:44:51 FAIL\r\n\r\n...\r\n\r\n\r\n15:44:53 ======================================================================\r\n15:44:53 FAIL: test_broadcast_cuda (__main__.TestDistBackend)\r\n15:44:53 ----------------------------------------------------------------------\r\n15:44:53 Traceback (most recent call last):\r\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\r\n15:44:53     self._join_and_reduce(fn)\r\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\r\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\r\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\r\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\n15:44:53 AssertionError: 74 not less than or equal to 1e-05 : \r\n15:44:53 \r\n```\r\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console\r\n\r\ntest_broadcast_multigpu:\r\n```\r\n15:44:51 test_broadcast_multigpu (__main__.TestDistBackend) ... Process process 2:\r\n15:44:51 Traceback (most recent call last):\r\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\r\n15:44:51     self.run()\r\n15:44:51   File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n15:44:51     self._target(*self._args, **self._kwargs)\r\n15:44:51   File \"test_distributed.py\", line 1268, in _run\r\n15:44:51     rank=self.rank\r\n15:44:51   File \"/opt/python/3.5/lib/python3.5/site-packages/torch/distributed/distributed_c10d.py\", line 224, in init_process_group\r\n15:44:51     _default_pg = ProcessGroupGloo(store, rank, world_size)\r\n15:44:51 RuntimeError: Connection reset by peer\r\n15:44:51 FAIL\r\n\r\n...\r\n\r\n15:44:53 ======================================================================\r\n15:44:53 FAIL: test_broadcast_multigpu (__main__.TestDistBackend)\r\n15:44:53 ----------------------------------------------------------------------\r\n15:44:53 Traceback (most recent call last):\r\n15:44:53   File \"test_distributed.py\", line 1217, in wrapper\r\n15:44:53     self._join_and_reduce(fn)\r\n15:44:53   File \"test_distributed.py\", line 1294, in _join_and_reduce\r\n15:44:53     self.assertEqual(p.exitcode, first_process.exitcode)\r\n15:44:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\r\n15:44:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\n15:44:53 AssertionError: 74 not less than or equal to 1e-05 : \r\n15:44:53 \r\n```\r\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.5-test/17631/console\r\n\r\ntest_all_reduce_sum_cuda:\r\n```\r\n17:05:23 ======================================================================\r\n17:05:23 FAIL: test_all_reduce_sum_cuda (__main__.TestDistBackend)\r\n17:05:23 ----------------------------------------------------------------------\r\n17:05:23 Traceback (most recent call last):\r\n17:05:23   File \"test_distributed.py\", line 1217, in wrapper\r\n17:05:23     self._join_and_reduce(fn)\r\n17:05:23   File \"test_distributed.py\", line 1294, in _join_and_reduce\r\n17:05:23     self.assertEqual(p.exitcode, first_process.exitcode)\r\n17:05:23   File \"/var/lib/jenkins/workspace/test/common.py\", line 390, in assertEqual\r\n17:05:23     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\n17:05:23 AssertionError: 74 not less than or equal to 1e-05 : \r\n17:05:23 \r\n17:05:23 ----------------------------------------------------------------------\r\n```\r\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-py3.6-gcc5.4-test/17697//console"}