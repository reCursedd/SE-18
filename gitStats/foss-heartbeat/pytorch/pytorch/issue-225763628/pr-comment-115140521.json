{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115140521", "pull_request_review_id": 36662426, "id": 115140521, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNTE0MDUyMQ==", "diff_hunk": "@@ -601,13 +611,168 @@ def forward(self, input):\n         return torch.cumsum(input, dim=self.dim)\n \n     def backward(self, grad_output):\n-        grad_input = torch.cumsum(-grad_output, dim=self.dim)\n+        return reverse_cumsum(grad_output, dim=self.dim)\n \n-        end_idx = grad_input.size(self.dim) - 1\n-        grad_sum = grad_input.narrow(self.dim, end_idx, 1)\n-        grad_input -= grad_sum.expand_as(grad_input)\n-        grad_input += grad_output\n-        return grad_input\n \n+class Cumprod(Function):\n+\n+    def __init__(self, dim):\n+        super(Cumprod, self).__init__()\n+        self.dim = dim\n+\n+    def forward(self, input):\n+        self.save_for_backward(input)\n+        return torch.cumprod(input, dim=self.dim)\n+\n+    def backward(self, grad_output):\n+        '''\n+        There are two algorithms to do this. The first one\n+        is very efficient, but works only when there are no\n+        nonzero elements in the input.\n+\n+        The second one is much more complex, but it doesn't\n+        assume anything on the input. The main downside is\n+        that it takes time O(n^2), where n = input.size(self.dim)\n+        (i.e. the length of the cumulative product). This is in\n+        contrast to the forward pass and the efficient algorithm,\n+        which are both O(n).\n+\n+        The second algorithm is a simple application of the chain\n+        rule. If x is an n-dimensional vector, and y = cumprod(x),\n+        and F is the final cost, then\n+\n+        dF / dx_k = sum_j (dF / dy_j) * (dy_j / dx_k)   (1)\n+\n+        The term dF / dy_j is just grad_output[j] (assuming again\n+        everything is one-dimensional).\n+\n+        The term (dy_j / dx_k) is easilly seen to be\n+\n+        if j >= k\n+            dy_j / dx_k = prod_{1 <= i <= j, i != k} x_i\n+        else:\n+            dy_j / dx_k = 0\n+\n+        Note that the indicator (j>=k) can be taken out\n+        by replacing the sum in (1) with a sum from\n+        j = k to n.\n+\n+        Thus,\n+        df / dx_k = sum_{k <= j <= n} grad_output[j] * (dy_j / dx_k)\n+\n+        with\n+        dy_j / dx_k = prod_{1 <= i <= j, i != k} x_i     (2)\n+\n+        Note that this last term is just the cumulative product\n+        with k omitted. Thus, if x_k (the input) is nonzero, we can\n+        just express this as\n+\n+        dy_j / dx_k = (prod_{1 <= i <= j} x_i) / x_k\n+                    = y_j / x_k\n+\n+        So therefore,\n+\n+        df / dx_k = sum_{k <= j <= n} grad_output[j] * y_j / x_k\n+\n+        so\n+\n+        grad_output = reverse_cumsum(grad_output * output) / input\n+\n+        If the input is nonzero, we need to calculate the dy_j / dx_k\n+        by using the formula (2), called in the code omitted_products.\n+\n+        The way the code calculates it is simply by noting that\n+\n+        prod_{1 <= i <= j, i != k} x_i\n+            = (prod_{1 <= i <= k} x_i) * (prod_{k + 1 <= i <= j} x_i)\n+\n+        the first term is calculated as prods_until_k, which since\n+        doesn't depend in j is easy to vectorize.\n+\n+        The second term (indexed by j) is the cumulative product of\n+        x_{k+1}, x_{k+2}, ..., x_n, and it's named in the code\n+        prods_from_k_pkus_1, and it's calculated as a cumprod.\n+\n+        In order to vectorize this properly, we need to add to\n+        omitted_products the dimensions where k > j, and therefore\n+        dy_j / dx_k = 0, which is done right after the assert.\n+        '''\n+\n+        input, = self.saved_tensors\n+        dim_size = input.size(self.dim)\n+        if dim_size == 1:\n+            return grad_output.clone()\n+\n+        #  Simple case with nonzero elements in the input\n+        if (input != 0).all():\n+            output = torch.cumprod(input, dim=self.dim)\n+            return reverse_cumsum(output * grad_output, dim=self.dim) / input\n+\n+        if input.is_cuda:\n+            idx = torch.cuda.LongTensor(range(dim_size))\n+        else:\n+            idx = torch.LongTensor(range(dim_size))\n+\n+        ones = input.select(self.dim, 0).unsqueeze(self.dim).clone().fill_(1)\n+        zeros = ones.clone().fill_(0)\n+        grad_input = grad_output.new(input.size()).zero_()\n+        for k in range(dim_size):\n+            if k == 0:\n+                prods_from_k_plus_1 = torch.cumprod(\n+                    input.index_select(self.dim, idx[k + 1:dim_size]),\n+                    dim=self.dim\n+                )\n+\n+                ommitted_products = torch.cat(\n+                    (ones, prods_from_k_plus_1),\n+                    self.dim\n+                )\n+\n+            elif k == dim_size - 1:\n+                prods_until_k = torch.prod(\n+                    input.index_select(self.dim, idx[:k]),\n+                    dim=self.dim\n+                )\n+\n+                ommitted_products = prods_until_k\n+\n+            else:\n+                prods_until_k = torch.prod(\n+                    input.index_select(self.dim, idx[:k]),\n+                    dim=self.dim\n+                )\n+\n+                prods_from_k_plus_1 = torch.cumprod(\n+                    input.index_select(self.dim, idx[k + 1:dim_size]),\n+                    dim=self.dim\n+                )\n+\n+                ommitted_products = prods_until_k.expand_as(\n+                    prods_from_k_plus_1) * prods_from_k_plus_1\n+\n+                ommitted_products = torch.cat(\n+                    (prods_until_k, ommitted_products), self.dim)\n+\n+            # At this point ommitted_products is the same size\n+            # as input, except on the dimension dim where it's\n+            # dim_size - k\n+            assert ommitted_products.size(self.dim) == dim_size - k\n+\n+            if k != 0:\n+                size_to_expand = list(input.size())\n+                size_to_expand[self.dim] = k  # Adding zeros to missing dimensions\n+                size_to_expand = torch.Size(size_to_expand)\n+                ommitted_products = torch.cat(\n+                    (zeros.expand(size_to_expand), ommitted_products),\n+                    self.dim\n+                )\n+\n+            grad_input.index_copy_(", "path": "torch/autograd/_functions/tensor.py", "position": null, "original_position": 183, "commit_id": "0764d73d6b2a2acf4e02984859a3f329fb908653", "original_commit_id": "b56318dc89069ef20949e87b9d6879e4591cdbd6", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think you could do it like that (regular `copy_` is likely to be faster than `index_copy_`):\r\n```python\r\ngrad_input.select(k, self.dim).copy_(torch.sum(...))\r\n```", "created_at": "2017-05-07T11:33:31Z", "updated_at": "2018-11-23T15:33:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/1439#discussion_r115140521", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1439", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115140521"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1439#discussion_r115140521"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1439"}}, "body_html": "<p>I think you could do it like that (regular <code>copy_</code> is likely to be faster than <code>index_copy_</code>):</p>\n<div class=\"highlight highlight-source-python\"><pre>grad_input.select(k, <span class=\"pl-c1\">self</span>.dim).copy_(torch.sum(<span class=\"pl-c1\">...</span>))</pre></div>", "body_text": "I think you could do it like that (regular copy_ is likely to be faster than index_copy_):\ngrad_input.select(k, self.dim).copy_(torch.sum(...))"}