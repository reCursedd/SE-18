{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/114443279", "pull_request_review_id": 35909313, "id": 114443279, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNDQ0MzI3OQ==", "diff_hunk": "@@ -601,13 +609,93 @@ def forward(self, input):\n         return torch.cumsum(input, dim=self.dim)\n \n     def backward(self, grad_output):\n-        grad_input = torch.cumsum(-grad_output, dim=self.dim)\n+        return reverse_cumsum(grad_output, dim=self.dim)\n+        \n+class Cumprod(Function):\n+    \n+    def __init__(self, dim):\n+        super(Cumprod, self).__init__()\n+        self.dim = dim\n \n-        end_idx = grad_input.size(self.dim) - 1\n-        grad_sum = grad_input.narrow(self.dim, end_idx, 1)\n-        grad_input -= grad_sum.expand_as(grad_input)\n-        grad_input += grad_output\n-        return grad_input\n+    def forward(self, input):\n+        self.save_for_backward(input)\n+        return torch.cumprod(input, dim=self.dim)\n+\n+    def backward(self, grad_output):\n+        '''\n+        There are two algorithms to do this, the naive one\n+        which works with all types of inputs is slow, namely\n+        O(n^2) where n = input.size(self.dim).\n+\n+        The second one is much faster, namely O(n), but\n+        requires nonzero inputs. This implementation checks\n+        for 0s in the inputs and then decides which one to use.\n+        '''\n+        input, = self.saved_tensors\n+        dim_size = input.size(self.dim)\n+        \n+        if dim_size == 1:\n+            return grad_output.clone()\n+\n+        if input.is_cuda:\n+            LT = torch.cuda.LongTensor\n+        else:\n+            LT = torch.LongTensor\n+\n+        if (input == 0).any():\n+            ones = input.index_select(self.dim,\n+                    LT(range(1))).fill_(1).clone()\n+            zeros = ones.clone().fill_(0)\n+            grad_input = grad_output.new(input.size()).zero_() \n+            for k in range(dim_size):\n+                if 0 < k and k < dim_size - 1:\n+                    prods_until_k = torch.prod(input.index_select(self.dim,\n+                        LT(range(k))), dim=self.dim)\n+\n+                    prods_from_k_plus_1 = torch.cumprod(input.index_select(\n+                        self.dim, LT(range(k+1, dim_size))), dim=self.dim)\n+\n+                    ommitted_products = prods_until_k.expand_as(\n+                        prods_from_k_plus_1) * prods_from_k_plus_1\n+\n+                    ommitted_products = torch.cat((prods_until_k,\n+                        ommitted_products), self.dim)\n+                elif k == dim_size - 1:\n+                    prods_until_k = torch.prod(input.index_select(self.dim,\n+                        LT(range(k))), dim=self.dim)\n+\n+                    ommitted_products = prods_until_k\n+                else: #k == 0\n+                    prods_from_k_plus_1 = torch.cumprod(input.index_select(\n+                        self.dim, LT(range(k+1, dim_size))), dim=self.dim)\n+\n+                    ommitted_products = torch.cat((ones, prods_from_k_plus_1),\n+                        self.dim)\n+                \n+                # At this point ommitted_products is the same size\n+                # as input, except on the dimension dim where it's\n+                # dim_size - k\n+                assert ommitted_products.size()[self.dim] == dim_size - k, \\\n+                    \"Dimension error\"\n+\n+                if k != 0:\n+                    size_to_expand = [l for l in input.size()]", "path": "torch/autograd/_functions/tensor.py", "position": null, "original_position": 95, "commit_id": "0764d73d6b2a2acf4e02984859a3f329fb908653", "original_commit_id": "fd08916198f13cd011f1af34830c17ec949771a7", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "```python\r\nsize_to_expand = list(input.size())\r\n```", "created_at": "2017-05-02T22:48:39Z", "updated_at": "2018-11-23T15:33:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/1439#discussion_r114443279", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1439", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/114443279"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1439#discussion_r114443279"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1439"}}, "body_html": "<div class=\"highlight highlight-source-python\"><pre>size_to_expand <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">input</span>.size())</pre></div>", "body_text": "size_to_expand = list(input.size())"}