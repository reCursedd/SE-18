{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422054666", "html_url": "https://github.com/pytorch/pytorch/issues/8856#issuecomment-422054666", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8856", "id": 422054666, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjA1NDY2Ng==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-17T15:09:06Z", "updated_at": "2018-09-17T15:09:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yes, it was because we weren't initializing CUDA on all codepaths that we should have. Basically, if you do some cuda operation before you invoke torch.cuda.sparse.FloatTensor, that will be sufficient to workaround.</p>", "body_text": "Yes, it was because we weren't initializing CUDA on all codepaths that we should have. Basically, if you do some cuda operation before you invoke torch.cuda.sparse.FloatTensor, that will be sufficient to workaround.", "body": "Yes, it was because we weren't initializing CUDA on all codepaths that we should have. Basically, if you do some cuda operation before you invoke torch.cuda.sparse.FloatTensor, that will be sufficient to workaround."}