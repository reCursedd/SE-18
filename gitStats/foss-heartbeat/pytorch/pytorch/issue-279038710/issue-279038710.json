{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4002", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4002/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4002/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4002/events", "html_url": "https://github.com/pytorch/pytorch/issues/4002", "id": 279038710, "node_id": "MDU6SXNzdWUyNzkwMzg3MTA=", "number": 4002, "title": "GRU behaviour varies for GPU and CPU", "user": {"login": "tejaswini", "id": 217152, "node_id": "MDQ6VXNlcjIxNzE1Mg==", "avatar_url": "https://avatars3.githubusercontent.com/u/217152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tejaswini", "html_url": "https://github.com/tejaswini", "followers_url": "https://api.github.com/users/tejaswini/followers", "following_url": "https://api.github.com/users/tejaswini/following{/other_user}", "gists_url": "https://api.github.com/users/tejaswini/gists{/gist_id}", "starred_url": "https://api.github.com/users/tejaswini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tejaswini/subscriptions", "organizations_url": "https://api.github.com/users/tejaswini/orgs", "repos_url": "https://api.github.com/users/tejaswini/repos", "events_url": "https://api.github.com/users/tejaswini/events{/privacy}", "received_events_url": "https://api.github.com/users/tejaswini/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-04T15:35:27Z", "updated_at": "2017-12-13T18:28:25Z", "closed_at": "2017-12-13T18:28:25Z", "author_association": "NONE", "body_html": "<p>When the number of layers in the GRU and the number of layers in the hidden size do not match, the GPU throws an error while the CPU does not.</p>\n<p>The following snippet to illustrates the same:</p>\n<pre lang=\"import\" data-meta=\"torch\"><code>      rnn = torch.nn.GRU(input_size=5,hidden_size=4)\n      x = torch.randn(3, 4, 5)\n      hidden = torch.autograd.Variable(torch.randn(2, 4, 4))\n      x = torch.autograd.Variable(x)\n      if torch.cuda.is_available():\n        rnn = rnn.cuda()\n        x = x.cuda()\n        hidden = hidden.cuda()\n      rnn(x, hidden)\n</code></pre>", "body_text": "When the number of layers in the GRU and the number of layers in the hidden size do not match, the GPU throws an error while the CPU does not.\nThe following snippet to illustrates the same:\n      rnn = torch.nn.GRU(input_size=5,hidden_size=4)\n      x = torch.randn(3, 4, 5)\n      hidden = torch.autograd.Variable(torch.randn(2, 4, 4))\n      x = torch.autograd.Variable(x)\n      if torch.cuda.is_available():\n        rnn = rnn.cuda()\n        x = x.cuda()\n        hidden = hidden.cuda()\n      rnn(x, hidden)", "body": "When the number of layers in the GRU and the number of layers in the hidden size do not match, the GPU throws an error while the CPU does not.\r\n\r\nThe following snippet to illustrates the same:\r\n```import torch\r\n      rnn = torch.nn.GRU(input_size=5,hidden_size=4)\r\n      x = torch.randn(3, 4, 5)\r\n      hidden = torch.autograd.Variable(torch.randn(2, 4, 4))\r\n      x = torch.autograd.Variable(x)\r\n      if torch.cuda.is_available():\r\n        rnn = rnn.cuda()\r\n        x = x.cuda()\r\n        hidden = hidden.cuda()\r\n      rnn(x, hidden)\r\n```\r\n\r\n"}