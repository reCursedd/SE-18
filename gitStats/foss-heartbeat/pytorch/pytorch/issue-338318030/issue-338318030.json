{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9168/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9168/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9168/events", "html_url": "https://github.com/pytorch/pytorch/issues/9168", "id": 338318030, "node_id": "MDU6SXNzdWUzMzgzMTgwMzA=", "number": 9168, "title": "Serialization of data within a tensor is slow", "user": {"login": "mrocklin", "id": 306380, "node_id": "MDQ6VXNlcjMwNjM4MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/306380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrocklin", "html_url": "https://github.com/mrocklin", "followers_url": "https://api.github.com/users/mrocklin/followers", "following_url": "https://api.github.com/users/mrocklin/following{/other_user}", "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions", "organizations_url": "https://api.github.com/users/mrocklin/orgs", "repos_url": "https://api.github.com/users/mrocklin/repos", "events_url": "https://api.github.com/users/mrocklin/events{/privacy}", "received_events_url": "https://api.github.com/users/mrocklin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2018-07-04T15:32:18Z", "updated_at": "2018-07-09T22:17:06Z", "closed_at": "2018-07-09T22:17:06Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>When naively serializing a pytorch tensor (or model) with pickle the process takes longer than it probably should when comparing it to Numpy serialization of the same data.  It appears that at some point we're converting things into a list rather than returning the buffers directly.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> numpy, torch, pickle\n\nIn [<span class=\"pl-c1\">2</span>]: x <span class=\"pl-k\">=</span> numpy.random.random((<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">10000</span>))\n\nIn [<span class=\"pl-c1\">3</span>]: t <span class=\"pl-k\">=</span> torch.tensor(x)\n\nIn [<span class=\"pl-c1\">4</span>]: <span class=\"pl-k\">%</span>time <span class=\"pl-c1\">len</span>(pickle.dumps(x)) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1e6</span>                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> around 1GB/s</span>\n<span class=\"pl-c1\">CPU</span> times: user <span class=\"pl-c1\">298</span> ms, sys: <span class=\"pl-c1\">415</span> ms, total: <span class=\"pl-c1\">713</span> ms\nWall time: <span class=\"pl-c1\">711</span> ms\nOut[<span class=\"pl-c1\">4</span>]: <span class=\"pl-c1\">800.000162</span>\n\nIn [<span class=\"pl-c1\">5</span>]: <span class=\"pl-k\">%</span>time <span class=\"pl-c1\">len</span>(pickle.dumps(t)) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1e6</span>                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> around 50MB/s</span>\n<span class=\"pl-c1\">CPU</span> times: user <span class=\"pl-c1\">14.6</span> s, sys: <span class=\"pl-c1\">1.03</span> s, total: <span class=\"pl-c1\">15.7</span> s\nWall time: <span class=\"pl-c1\">15.7</span> s\nOut[<span class=\"pl-c1\">5</span>]: <span class=\"pl-c1\">900.200098</span></pre></div>\n<p>The majority of this time is spent in converting the <code>t.storage()</code> object into a list</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">11</span>]: <span class=\"pl-k\">%</span>time _ <span class=\"pl-k\">=</span> t.storage().tolist()\n<span class=\"pl-c1\">CPU</span> times: user <span class=\"pl-c1\">12.3</span> s, sys: <span class=\"pl-c1\">891</span> ms, total: <span class=\"pl-c1\">13.2</span> s\nWall time: <span class=\"pl-c1\">13.2</span> s</pre></div>\n<p>Instead, we might consider passing around a numpy array, <code>buffer</code>, or <code>memoryview</code>, each of which will serialize much more quickly than converting to many Python objects</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> torch\n\nIn [<span class=\"pl-c1\">2</span>]: torch.<span class=\"pl-c1\">__version__</span>\nOut[<span class=\"pl-c1\">2</span>]: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>0.4.0<span class=\"pl-pds\">'</span></span></pre></div>", "body_text": "Issue description\nWhen naively serializing a pytorch tensor (or model) with pickle the process takes longer than it probably should when comparing it to Numpy serialization of the same data.  It appears that at some point we're converting things into a list rather than returning the buffers directly.\nCode example\nIn [1]: import numpy, torch, pickle\n\nIn [2]: x = numpy.random.random((10000, 10000))\n\nIn [3]: t = torch.tensor(x)\n\nIn [4]: %time len(pickle.dumps(x)) / 1e6                    # around 1GB/s\nCPU times: user 298 ms, sys: 415 ms, total: 713 ms\nWall time: 711 ms\nOut[4]: 800.000162\n\nIn [5]: %time len(pickle.dumps(t)) / 1e6                    # around 50MB/s\nCPU times: user 14.6 s, sys: 1.03 s, total: 15.7 s\nWall time: 15.7 s\nOut[5]: 900.200098\nThe majority of this time is spent in converting the t.storage() object into a list\nIn [11]: %time _ = t.storage().tolist()\nCPU times: user 12.3 s, sys: 891 ms, total: 13.2 s\nWall time: 13.2 s\nInstead, we might consider passing around a numpy array, buffer, or memoryview, each of which will serialize much more quickly than converting to many Python objects\nIn [1]: import torch\n\nIn [2]: torch.__version__\nOut[2]: '0.4.0'", "body": "## Issue description\r\n\r\nWhen naively serializing a pytorch tensor (or model) with pickle the process takes longer than it probably should when comparing it to Numpy serialization of the same data.  It appears that at some point we're converting things into a list rather than returning the buffers directly.\r\n\r\n## Code example\r\n\r\n```python\r\nIn [1]: import numpy, torch, pickle\r\n\r\nIn [2]: x = numpy.random.random((10000, 10000))\r\n\r\nIn [3]: t = torch.tensor(x)\r\n\r\nIn [4]: %time len(pickle.dumps(x)) / 1e6                    # around 1GB/s\r\nCPU times: user 298 ms, sys: 415 ms, total: 713 ms\r\nWall time: 711 ms\r\nOut[4]: 800.000162\r\n\r\nIn [5]: %time len(pickle.dumps(t)) / 1e6                    # around 50MB/s\r\nCPU times: user 14.6 s, sys: 1.03 s, total: 15.7 s\r\nWall time: 15.7 s\r\nOut[5]: 900.200098\r\n```\r\nThe majority of this time is spent in converting the `t.storage()` object into a list\r\n\r\n```python\r\nIn [11]: %time _ = t.storage().tolist()\r\nCPU times: user 12.3 s, sys: 891 ms, total: 13.2 s\r\nWall time: 13.2 s\r\n```\r\n\r\nInstead, we might consider passing around a numpy array, `buffer`, or `memoryview`, each of which will serialize much more quickly than converting to many Python objects\r\n\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.__version__\r\nOut[2]: '0.4.0'\r\n```"}