{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/330902952", "html_url": "https://github.com/pytorch/pytorch/issues/2753#issuecomment-330902952", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2753", "id": 330902952, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDkwMjk1Mg==", "user": {"login": "wickedfoo", "id": 1911637, "node_id": "MDQ6VXNlcjE5MTE2Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1911637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wickedfoo", "html_url": "https://github.com/wickedfoo", "followers_url": "https://api.github.com/users/wickedfoo/followers", "following_url": "https://api.github.com/users/wickedfoo/following{/other_user}", "gists_url": "https://api.github.com/users/wickedfoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/wickedfoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wickedfoo/subscriptions", "organizations_url": "https://api.github.com/users/wickedfoo/orgs", "repos_url": "https://api.github.com/users/wickedfoo/repos", "events_url": "https://api.github.com/users/wickedfoo/events{/privacy}", "received_events_url": "https://api.github.com/users/wickedfoo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-20T16:13:16Z", "updated_at": "2017-09-20T16:13:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This usage example is pretty slow too since the dimension for the top-k is not innermost, so all the reads are strided / non-coalesced (and it has to do N of them). I don't have an implementation that works well where the dimension is not innermost. If you could transpose your data such that the dimension along which you are gathering is innermost, this example would work faster. Or, as a fix (assuming that the temporary memory allocator is good enough), one could allocate temporary memory for the transposition, transpose it, operate on the transposed form, then produce results back out in cases where the top-k dimension is not innermost.</p>\n<p>The version I have in Faiss is fastest for k &lt;= 1024 (for k &lt;= 128, it does it in a single pass at up to 70-80% peak mem b/w):<br>\n<a href=\"https://github.com/facebookresearch/faiss/blob/master/gpu/utils/Select.cuh\">https://github.com/facebookresearch/faiss/blob/master/gpu/utils/Select.cuh</a></p>\n<p>I checked in a version for small top-k that is less fast in Caffe2, but still single-pass:<br>\n<a href=\"https://github.com/caffe2/caffe2/blob/master/caffe2/operators/top_k_heap_selection.cuh\">https://github.com/caffe2/caffe2/blob/master/caffe2/operators/top_k_heap_selection.cuh</a></p>", "body_text": "This usage example is pretty slow too since the dimension for the top-k is not innermost, so all the reads are strided / non-coalesced (and it has to do N of them). I don't have an implementation that works well where the dimension is not innermost. If you could transpose your data such that the dimension along which you are gathering is innermost, this example would work faster. Or, as a fix (assuming that the temporary memory allocator is good enough), one could allocate temporary memory for the transposition, transpose it, operate on the transposed form, then produce results back out in cases where the top-k dimension is not innermost.\nThe version I have in Faiss is fastest for k <= 1024 (for k <= 128, it does it in a single pass at up to 70-80% peak mem b/w):\nhttps://github.com/facebookresearch/faiss/blob/master/gpu/utils/Select.cuh\nI checked in a version for small top-k that is less fast in Caffe2, but still single-pass:\nhttps://github.com/caffe2/caffe2/blob/master/caffe2/operators/top_k_heap_selection.cuh", "body": "This usage example is pretty slow too since the dimension for the top-k is not innermost, so all the reads are strided / non-coalesced (and it has to do N of them). I don't have an implementation that works well where the dimension is not innermost. If you could transpose your data such that the dimension along which you are gathering is innermost, this example would work faster. Or, as a fix (assuming that the temporary memory allocator is good enough), one could allocate temporary memory for the transposition, transpose it, operate on the transposed form, then produce results back out in cases where the top-k dimension is not innermost.\r\n\r\nThe version I have in Faiss is fastest for k <= 1024 (for k <= 128, it does it in a single pass at up to 70-80% peak mem b/w):\r\nhttps://github.com/facebookresearch/faiss/blob/master/gpu/utils/Select.cuh\r\n\r\nI checked in a version for small top-k that is less fast in Caffe2, but still single-pass:\r\nhttps://github.com/caffe2/caffe2/blob/master/caffe2/operators/top_k_heap_selection.cuh\r\n\r\n"}