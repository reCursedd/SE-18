{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/334964027", "html_url": "https://github.com/pytorch/pytorch/pull/3016#issuecomment-334964027", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3016", "id": 334964027, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDk2NDAyNw==", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-07T20:34:37Z", "updated_at": "2017-10-07T20:34:37Z", "author_association": "COLLABORATOR", "body_html": "<p>I've just pushed a new commit:</p>\n<ul>\n<li>args to <code>__call__</code> in module are flattened (instead of <code>_first_var</code>)</li>\n<li>I introduced a <code>torch.jit.scope</code> context manager</li>\n</ul>\n<p>So, what was</p>\n<pre><code>x = Variable(torch.Tensor([0.4]), requires_grad=True)\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\n\ndef doit(x, y):\n    tracing_state = torch.jit.get_tracing_state(x,y)\n    if tracing_state:\n        tracing_state.push_scope('Foo')\n    z = Variable(torch.Tensor([0.7]), requires_grad=True)\n    out = torch.sigmoid(torch.tanh(x * (y + z)))\n    if tracing_state:\n        tracing_state.pop_scope()\n    return out\n\ntraced, _ = torch.jit.trace(doit, (x, y))\ng = torch._C._jit_get_graph(traced)\nprint(g)\n</code></pre>\n<p>becomes</p>\n<pre><code>x = Variable(torch.Tensor([0.4]), requires_grad=True)\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\n\ndef doit(x, y):\n    with torch.jit.scope('Foo', (x, y)):\n        z = Variable(torch.Tensor([0.7]), requires_grad=True)\n        out = torch.sigmoid(torch.tanh(x * (y + z)))\n    return out\n\ntraced, _ = torch.jit.trace(doit, (x, y))\ng = torch._C._jit_get_graph(traced)\nprint(g)\n</code></pre>", "body_text": "I've just pushed a new commit:\n\nargs to __call__ in module are flattened (instead of _first_var)\nI introduced a torch.jit.scope context manager\n\nSo, what was\nx = Variable(torch.Tensor([0.4]), requires_grad=True)\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\n\ndef doit(x, y):\n    tracing_state = torch.jit.get_tracing_state(x,y)\n    if tracing_state:\n        tracing_state.push_scope('Foo')\n    z = Variable(torch.Tensor([0.7]), requires_grad=True)\n    out = torch.sigmoid(torch.tanh(x * (y + z)))\n    if tracing_state:\n        tracing_state.pop_scope()\n    return out\n\ntraced, _ = torch.jit.trace(doit, (x, y))\ng = torch._C._jit_get_graph(traced)\nprint(g)\n\nbecomes\nx = Variable(torch.Tensor([0.4]), requires_grad=True)\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\n\ndef doit(x, y):\n    with torch.jit.scope('Foo', (x, y)):\n        z = Variable(torch.Tensor([0.7]), requires_grad=True)\n        out = torch.sigmoid(torch.tanh(x * (y + z)))\n    return out\n\ntraced, _ = torch.jit.trace(doit, (x, y))\ng = torch._C._jit_get_graph(traced)\nprint(g)", "body": "I've just pushed a new commit:\r\n* args to `__call__` in module are flattened (instead of `_first_var`)\r\n* I introduced a `torch.jit.scope` context manager\r\n\r\nSo, what was\r\n```\r\nx = Variable(torch.Tensor([0.4]), requires_grad=True)\r\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\r\n\r\ndef doit(x, y):\r\n    tracing_state = torch.jit.get_tracing_state(x,y)\r\n    if tracing_state:\r\n        tracing_state.push_scope('Foo')\r\n    z = Variable(torch.Tensor([0.7]), requires_grad=True)\r\n    out = torch.sigmoid(torch.tanh(x * (y + z)))\r\n    if tracing_state:\r\n        tracing_state.pop_scope()\r\n    return out\r\n\r\ntraced, _ = torch.jit.trace(doit, (x, y))\r\ng = torch._C._jit_get_graph(traced)\r\nprint(g)\r\n```\r\n\r\nbecomes\r\n\r\n```\r\nx = Variable(torch.Tensor([0.4]), requires_grad=True)\r\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\r\n\r\ndef doit(x, y):\r\n    with torch.jit.scope('Foo', (x, y)):\r\n        z = Variable(torch.Tensor([0.7]), requires_grad=True)\r\n        out = torch.sigmoid(torch.tanh(x * (y + z)))\r\n    return out\r\n\r\ntraced, _ = torch.jit.trace(doit, (x, y))\r\ng = torch._C._jit_get_graph(traced)\r\nprint(g)\r\n```"}