{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3016", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3016/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3016/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3016/events", "html_url": "https://github.com/pytorch/pytorch/pull/3016", "id": 263595888, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQ1MjYzNTE3", "number": 3016, "title": "Introduce scopes during tracing", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 44, "created_at": "2017-10-06T23:17:54Z", "updated_at": "2018-11-23T15:37:02Z", "closed_at": "2017-12-04T17:19:07Z", "author_association": "COLLABORATOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3016", "html_url": "https://github.com/pytorch/pytorch/pull/3016", "diff_url": "https://github.com/pytorch/pytorch/pull/3016.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3016.patch"}, "body_html": "<p>This PR introduces scopes (or namespaces) in order to group operations in the tracing IR, e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">0.4</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">0.7</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">doit</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>):\n    tracing_state <span class=\"pl-k\">=</span> torch.jit.get_tracing_state(x,y)\n    <span class=\"pl-k\">if</span> tracing_state:\n        tracing_state.push_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Foo<span class=\"pl-pds\">'</span></span>)\n    z <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">0.7</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    out <span class=\"pl-k\">=</span> torch.sigmoid(torch.tanh(x <span class=\"pl-k\">*</span> (y <span class=\"pl-k\">+</span> z)))\n    <span class=\"pl-k\">if</span> tracing_state:\n        tracing_state.pop_scope()\n    <span class=\"pl-k\">return</span> out\n\ntraced, _ <span class=\"pl-k\">=</span> torch.jit.trace(doit, (x, y))\ng <span class=\"pl-k\">=</span> torch._C._jit_get_graph(traced)\n<span class=\"pl-c1\">print</span>(g)</pre></div>\n<p>outputs</p>\n<pre><code>graph(%1 : Float(1)\n      %2 : Float(1)) {\n  %3 : Float(1) = Constant[value=&lt;Tensor&gt;](), uses = [%4.i1], scope: Foo;\n  %5 : Float(1) = ^Add(False)(%2, %3), uses = [[%6.i1]], scope: Foo;\n  %7 : Float(1) = ^Mul()(%1, %5), uses = [[%8.i0]], scope: Foo;\n  %9 : Float(1) = ^Tanh()(%7), uses = [[%10.i0]], scope: Foo;\n  %11 : Float(1) = ^Sigmoid()(%9), uses = [[%0.i0]], scope: Foo;\n  return (%11);\n}\n</code></pre>\n<p>Scopes work like a stack: they can be pushed and popped manually (as in the example above). Modules automatically push a scope during <code>__call__</code> and pops it before returning. The scope is named as <code>className$id</code>, where id is the value of the Python <code>id</code> function:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n            <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n            <span class=\"pl-c1\">self</span>.layer1 <span class=\"pl-k\">=</span> nn.Sequential(nn.Linear(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>), nn.ReLU())\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.layer1(x)\n\n    net <span class=\"pl-k\">=</span> Net()\n\n    t <span class=\"pl-k\">=</span> Variable(torch.ones(<span class=\"pl-c1\">2</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    traced, _ <span class=\"pl-k\">=</span> torch.jit.trace(net, (t, ))\n    g <span class=\"pl-k\">=</span> torch._C._jit_get_graph(traced)\n    <span class=\"pl-c1\">print</span>(g)</pre></div>\n<p>outputs</p>\n<pre><code>graph(%1 : Float(2)\n      %2 : Float(2, 2)\n      %3 : Float(2)) {\n  %5 : Float(2!, 2!) = ^Transpose(0, 1)(%2), uses = [[%10.i2]], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %7 : Float(1, 2), %8 : Handle = ^Unsqueeze(0)(%1), uses = [[%10.i1], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %9 : Float(1, 2) = Constant[value=&lt;Tensor&gt;](), uses = [%10.i0], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %11 : Float(1, 2), %12 : Handle = ^Addmm(0, 1, True)(%9, %7, %5), uses = [[%13.i0], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %14 : Float(2), %15 : Handle = ^Squeeze(0, True)(%11), uses = [[%16.i0], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %17 : Float(2) = ^Add(True)(%14, %3), uses = [[%18.i0]], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %19 : Float(2), %20 : Handle = ^Threshold(0, 0, False)(%17), uses = [[%0.i0], []], scope: Net$4569918736.Sequential$4569919312.ReLU$4569919184;\n  return (%19);\n}\n</code></pre>\n<p><em>Tests fail at the moment</em> because the expected output of traces differs, as I added a <code>scope</code> description. I'm not sure it belongs there, at the moment it's handy for debugging purposes. If we decide to keep them this way I'll update the expected output.</p>\n<p>Under the hood, scope names are implemented using interned strings.</p>\n<p>/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a></p>", "body_text": "This PR introduces scopes (or namespaces) in order to group operations in the tracing IR, e.g.\nx = Variable(torch.Tensor([0.4]), requires_grad=True)\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\n\ndef doit(x, y):\n    tracing_state = torch.jit.get_tracing_state(x,y)\n    if tracing_state:\n        tracing_state.push_scope('Foo')\n    z = Variable(torch.Tensor([0.7]), requires_grad=True)\n    out = torch.sigmoid(torch.tanh(x * (y + z)))\n    if tracing_state:\n        tracing_state.pop_scope()\n    return out\n\ntraced, _ = torch.jit.trace(doit, (x, y))\ng = torch._C._jit_get_graph(traced)\nprint(g)\noutputs\ngraph(%1 : Float(1)\n      %2 : Float(1)) {\n  %3 : Float(1) = Constant[value=<Tensor>](), uses = [%4.i1], scope: Foo;\n  %5 : Float(1) = ^Add(False)(%2, %3), uses = [[%6.i1]], scope: Foo;\n  %7 : Float(1) = ^Mul()(%1, %5), uses = [[%8.i0]], scope: Foo;\n  %9 : Float(1) = ^Tanh()(%7), uses = [[%10.i0]], scope: Foo;\n  %11 : Float(1) = ^Sigmoid()(%9), uses = [[%0.i0]], scope: Foo;\n  return (%11);\n}\n\nScopes work like a stack: they can be pushed and popped manually (as in the example above). Modules automatically push a scope during __call__ and pops it before returning. The scope is named as className$id, where id is the value of the Python id function:\nclass Net(nn.Module):\n\n        def __init__(self):\n            super(Net, self).__init__()\n            self.layer1 = nn.Sequential(nn.Linear(2,2), nn.ReLU())\n\n        def forward(self, x):\n            return self.layer1(x)\n\n    net = Net()\n\n    t = Variable(torch.ones(2), requires_grad=True)\n\n    traced, _ = torch.jit.trace(net, (t, ))\n    g = torch._C._jit_get_graph(traced)\n    print(g)\noutputs\ngraph(%1 : Float(2)\n      %2 : Float(2, 2)\n      %3 : Float(2)) {\n  %5 : Float(2!, 2!) = ^Transpose(0, 1)(%2), uses = [[%10.i2]], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %7 : Float(1, 2), %8 : Handle = ^Unsqueeze(0)(%1), uses = [[%10.i1], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %9 : Float(1, 2) = Constant[value=<Tensor>](), uses = [%10.i0], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %11 : Float(1, 2), %12 : Handle = ^Addmm(0, 1, True)(%9, %7, %5), uses = [[%13.i0], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %14 : Float(2), %15 : Handle = ^Squeeze(0, True)(%11), uses = [[%16.i0], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %17 : Float(2) = ^Add(True)(%14, %3), uses = [[%18.i0]], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\n  %19 : Float(2), %20 : Handle = ^Threshold(0, 0, False)(%17), uses = [[%0.i0], []], scope: Net$4569918736.Sequential$4569919312.ReLU$4569919184;\n  return (%19);\n}\n\nTests fail at the moment because the expected output of traces differs, as I added a scope description. I'm not sure it belongs there, at the moment it's handy for debugging purposes. If we decide to keep them this way I'll update the expected output.\nUnder the hood, scope names are implemented using interned strings.\n/cc @ezyang @fmassa", "body": "This PR introduces scopes (or namespaces) in order to group operations in the tracing IR, e.g.\r\n\r\n```python\r\nx = Variable(torch.Tensor([0.4]), requires_grad=True)\r\ny = Variable(torch.Tensor([0.7]), requires_grad=True)\r\n\r\ndef doit(x, y):\r\n    tracing_state = torch.jit.get_tracing_state(x,y)\r\n    if tracing_state:\r\n        tracing_state.push_scope('Foo')\r\n    z = Variable(torch.Tensor([0.7]), requires_grad=True)\r\n    out = torch.sigmoid(torch.tanh(x * (y + z)))\r\n    if tracing_state:\r\n        tracing_state.pop_scope()\r\n    return out\r\n\r\ntraced, _ = torch.jit.trace(doit, (x, y))\r\ng = torch._C._jit_get_graph(traced)\r\nprint(g)\r\n```\r\n\r\noutputs\r\n\r\n```\r\ngraph(%1 : Float(1)\r\n      %2 : Float(1)) {\r\n  %3 : Float(1) = Constant[value=<Tensor>](), uses = [%4.i1], scope: Foo;\r\n  %5 : Float(1) = ^Add(False)(%2, %3), uses = [[%6.i1]], scope: Foo;\r\n  %7 : Float(1) = ^Mul()(%1, %5), uses = [[%8.i0]], scope: Foo;\r\n  %9 : Float(1) = ^Tanh()(%7), uses = [[%10.i0]], scope: Foo;\r\n  %11 : Float(1) = ^Sigmoid()(%9), uses = [[%0.i0]], scope: Foo;\r\n  return (%11);\r\n}\r\n```\r\n\r\nScopes work like a stack: they can be pushed and popped manually (as in the example above). Modules automatically push a scope during `__call__` and pops it before returning. The scope is named as `className$id`, where id is the value of the Python `id` function:\r\n\r\n```python\r\nclass Net(nn.Module):\r\n\r\n        def __init__(self):\r\n            super(Net, self).__init__()\r\n            self.layer1 = nn.Sequential(nn.Linear(2,2), nn.ReLU())\r\n\r\n        def forward(self, x):\r\n            return self.layer1(x)\r\n\r\n    net = Net()\r\n\r\n    t = Variable(torch.ones(2), requires_grad=True)\r\n\r\n    traced, _ = torch.jit.trace(net, (t, ))\r\n    g = torch._C._jit_get_graph(traced)\r\n    print(g)\r\n```\r\n\r\noutputs\r\n\r\n```\r\ngraph(%1 : Float(2)\r\n      %2 : Float(2, 2)\r\n      %3 : Float(2)) {\r\n  %5 : Float(2!, 2!) = ^Transpose(0, 1)(%2), uses = [[%10.i2]], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\r\n  %7 : Float(1, 2), %8 : Handle = ^Unsqueeze(0)(%1), uses = [[%10.i1], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\r\n  %9 : Float(1, 2) = Constant[value=<Tensor>](), uses = [%10.i0], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\r\n  %11 : Float(1, 2), %12 : Handle = ^Addmm(0, 1, True)(%9, %7, %5), uses = [[%13.i0], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\r\n  %14 : Float(2), %15 : Handle = ^Squeeze(0, True)(%11), uses = [[%16.i0], []], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\r\n  %17 : Float(2) = ^Add(True)(%14, %3), uses = [[%18.i0]], scope: Net$4569918736.Sequential$4569919312.Linear$4569919120;\r\n  %19 : Float(2), %20 : Handle = ^Threshold(0, 0, False)(%17), uses = [[%0.i0], []], scope: Net$4569918736.Sequential$4569919312.ReLU$4569919184;\r\n  return (%19);\r\n}\r\n``` \r\n\r\n*Tests fail at the moment* because the expected output of traces differs, as I added a `scope` description. I'm not sure it belongs there, at the moment it's handy for debugging purposes. If we decide to keep them this way I'll update the expected output.\r\n\r\nUnder the hood, scope names are implemented using interned strings.\r\n\r\n/cc @ezyang @fmassa"}