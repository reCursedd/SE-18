{"url": "https://api.github.com/repos/pytorch/pytorch/issues/275", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/275/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/275/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/275/events", "html_url": "https://github.com/pytorch/pytorch/pull/275", "id": 192741962, "node_id": "MDExOlB1bGxSZXF1ZXN0OTYwMTc0MTE=", "number": 275, "title": "Enable caching allocator for pinned (page-locked) memory", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-12-01T02:54:57Z", "updated_at": "2018-11-23T15:31:58Z", "closed_at": "2016-12-02T06:33:57Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/275", "html_url": "https://github.com/pytorch/pytorch/pull/275", "diff_url": "https://github.com/pytorch/pytorch/pull/275.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/275.patch"}, "body_html": "<p>Adds a caching allocator for CUDA pinned (page-locked) memory. This avoid synchronization due to cudaFreeHost or cudaHostUnregister calls.</p>\n<p>To ensure read-after-write and write-after-read consistency, a CUDA event is recorded after every cudaMemcpyAsync between host and device involving pinned memory created by this allocator. Memory allocations are only re-used after they're freed and all associated CUDA events have completed.</p>\n<p>Unlike the caching device allocator, allocations are never split. This means that requests for small allocations may be filled by much larger cached buffers. I think this should be OK in practice.</p>\n<p>Also, CUDA events are processed in the order in which they're recorded, even though events may occur out-of-order between devices or streams. This does not affect correctness, but means that cached allocations may not be considered \"ready\" for re-use until a little later. In practice, I don't think this should matter.</p>\n<p>I'll send a PR to cutorch soon, but I want to make sure the continuous builds pass.</p>\n<p>I'm interested if <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> or <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3503919\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/thatguymike\">@thatguymike</a> have any comments.</p>\n<p>See <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"192166732\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/265\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/265/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/265\">#265</a></p>", "body_text": "Adds a caching allocator for CUDA pinned (page-locked) memory. This avoid synchronization due to cudaFreeHost or cudaHostUnregister calls.\nTo ensure read-after-write and write-after-read consistency, a CUDA event is recorded after every cudaMemcpyAsync between host and device involving pinned memory created by this allocator. Memory allocations are only re-used after they're freed and all associated CUDA events have completed.\nUnlike the caching device allocator, allocations are never split. This means that requests for small allocations may be filled by much larger cached buffers. I think this should be OK in practice.\nAlso, CUDA events are processed in the order in which they're recorded, even though events may occur out-of-order between devices or streams. This does not affect correctness, but means that cached allocations may not be considered \"ready\" for re-use until a little later. In practice, I don't think this should matter.\nI'll send a PR to cutorch soon, but I want to make sure the continuous builds pass.\nI'm interested if @ngimel or @thatguymike have any comments.\nSee #265", "body": "Adds a caching allocator for CUDA pinned (page-locked) memory. This avoid synchronization due to cudaFreeHost or cudaHostUnregister calls.\r\n\r\nTo ensure read-after-write and write-after-read consistency, a CUDA event is recorded after every cudaMemcpyAsync between host and device involving pinned memory created by this allocator. Memory allocations are only re-used after they're freed and all associated CUDA events have completed.\r\n\r\nUnlike the caching device allocator, allocations are never split. This means that requests for small allocations may be filled by much larger cached buffers. I think this should be OK in practice.\r\n\r\nAlso, CUDA events are processed in the order in which they're recorded, even though events may occur out-of-order between devices or streams. This does not affect correctness, but means that cached allocations may not be considered \"ready\" for re-use until a little later. In practice, I don't think this should matter.\r\n\r\nI'll send a PR to cutorch soon, but I want to make sure the continuous builds pass.\r\n\r\nI'm interested if @ngimel or @thatguymike have any comments.\r\n\r\nSee #265 "}