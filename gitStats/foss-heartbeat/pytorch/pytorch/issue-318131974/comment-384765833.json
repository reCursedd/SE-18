{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/384765833", "html_url": "https://github.com/pytorch/pytorch/issues/7002#issuecomment-384765833", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7002", "id": 384765833, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDc2NTgzMw==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-26T19:41:49Z", "updated_at": "2018-04-26T19:41:49Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Let\u2019s also bear in mind that this is just a single point in the entire\nfloat range. The likelihood that your training is going to land on this\npoint (okay two points) is very very very small. Even it does, it will\nmostly be just one time and the probablity that this causes the result to\ndiffer greatly is virtually zero.\n\nAutograd frameworks differ on points like this all the time. Taking the\nmathematical view (i.e. pretending computers have continuous numbers), they\nare all correct in that they all calculate subgradients. And probably more\nimportant to you, they are equal almost everywhere on the real line, which\nmeans that if you pick a point in R, the probablity that you land in such\npoints is 0.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Apr 27, 2018 at 3:31 AM Richard Zou ***@***.***&gt; wrote:\n Our users might rely on the edge case that the gradient does not exist. At\n any rate, you can define a custom autograd function in Python that has its\n own forward and backward. The forward can use clamp, while the backward\n should look something like:\n\n grad_output * (input &gt;= min).type_as(grad_output) * (input &lt;= max).type_as(grad_output)\n\n \u2014\n You are receiving this because you modified the open/close state.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"318131974\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7002\" href=\"https://github.com/pytorch/pytorch/issues/7002#issuecomment-384762981\">#7002 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AFaWZcWhzJll6oz0sq_EWrf0HdjRE0MGks5tsiB8gaJpZM4TmW_u\">https://github.com/notifications/unsubscribe-auth/AFaWZcWhzJll6oz0sq_EWrf0HdjRE0MGks5tsiB8gaJpZM4TmW_u</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Let\u2019s also bear in mind that this is just a single point in the entire\nfloat range. The likelihood that your training is going to land on this\npoint (okay two points) is very very very small. Even it does, it will\nmostly be just one time and the probablity that this causes the result to\ndiffer greatly is virtually zero.\n\nAutograd frameworks differ on points like this all the time. Taking the\nmathematical view (i.e. pretending computers have continuous numbers), they\nare all correct in that they all calculate subgradients. And probably more\nimportant to you, they are equal almost everywhere on the real line, which\nmeans that if you pick a point in R, the probablity that you land in such\npoints is 0.\n\u2026\nOn Fri, Apr 27, 2018 at 3:31 AM Richard Zou ***@***.***> wrote:\n Our users might rely on the edge case that the gradient does not exist. At\n any rate, you can define a custom autograd function in Python that has its\n own forward and backward. The forward can use clamp, while the backward\n should look something like:\n\n grad_output * (input >= min).type_as(grad_output) * (input <= max).type_as(grad_output)\n\n \u2014\n You are receiving this because you modified the open/close state.\n Reply to this email directly, view it on GitHub\n <#7002 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AFaWZcWhzJll6oz0sq_EWrf0HdjRE0MGks5tsiB8gaJpZM4TmW_u>\n .", "body": "Let\u2019s also bear in mind that this is just a single point in the entire\nfloat range. The likelihood that your training is going to land on this\npoint (okay two points) is very very very small. Even it does, it will\nmostly be just one time and the probablity that this causes the result to\ndiffer greatly is virtually zero.\n\nAutograd frameworks differ on points like this all the time. Taking the\nmathematical view (i.e. pretending computers have continuous numbers), they\nare all correct in that they all calculate subgradients. And probably more\nimportant to you, they are equal almost everywhere on the real line, which\nmeans that if you pick a point in R, the probablity that you land in such\npoints is 0.\n\nOn Fri, Apr 27, 2018 at 3:31 AM Richard Zou <notifications@github.com>\nwrote:\n\n> Our users might rely on the edge case that the gradient does not exist. At\n> any rate, you can define a custom autograd function in Python that has its\n> own forward and backward. The forward can use clamp, while the backward\n> should look something like:\n>\n> grad_output * (input >= min).type_as(grad_output) * (input <= max).type_as(grad_output)\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/7002#issuecomment-384762981>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZcWhzJll6oz0sq_EWrf0HdjRE0MGks5tsiB8gaJpZM4TmW_u>\n> .\n>\n"}