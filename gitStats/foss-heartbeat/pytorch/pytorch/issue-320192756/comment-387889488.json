{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/387889488", "html_url": "https://github.com/pytorch/pytorch/issues/7278#issuecomment-387889488", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7278", "id": 387889488, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Nzg4OTQ4OA==", "user": {"login": "jrwalsh1", "id": 9166900, "node_id": "MDQ6VXNlcjkxNjY5MDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9166900?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrwalsh1", "html_url": "https://github.com/jrwalsh1", "followers_url": "https://api.github.com/users/jrwalsh1/followers", "following_url": "https://api.github.com/users/jrwalsh1/following{/other_user}", "gists_url": "https://api.github.com/users/jrwalsh1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrwalsh1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrwalsh1/subscriptions", "organizations_url": "https://api.github.com/users/jrwalsh1/orgs", "repos_url": "https://api.github.com/users/jrwalsh1/repos", "events_url": "https://api.github.com/users/jrwalsh1/events{/privacy}", "received_events_url": "https://api.github.com/users/jrwalsh1/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T22:03:59Z", "updated_at": "2018-05-09T22:03:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I tested this for <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"321740251\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7440\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7440/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7440\">#7440</a>, but I found that the seg fault can happen even if the array memory is contiguous.</p>\n<p>Interestingly, <code>torch.tensor(seq[1:])</code> seg faults but <code>torch.tensor(seq[:1])</code> (taking from the head) does not. It seems that wrapping the data in a <code>pandas.Series</code> is causing the problem, it's fine without:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport torch\n\nx = np.random.rand(3)\ns = pd.Series(x)\n\ntorch.tensor(x[1:]) # OK\ntorch.tensor(s[1:]) # seg fault\n</code></pre>", "body_text": "I tested this for #7440, but I found that the seg fault can happen even if the array memory is contiguous.\nInterestingly, torch.tensor(seq[1:]) seg faults but torch.tensor(seq[:1]) (taking from the head) does not. It seems that wrapping the data in a pandas.Series is causing the problem, it's fine without:\nimport numpy as np\nimport pandas as pd\nimport torch\n\nx = np.random.rand(3)\ns = pd.Series(x)\n\ntorch.tensor(x[1:]) # OK\ntorch.tensor(s[1:]) # seg fault", "body": "I tested this for #7440, but I found that the seg fault can happen even if the array memory is contiguous.  \r\n\r\nInterestingly, `torch.tensor(seq[1:])` seg faults but `torch.tensor(seq[:1])` (taking from the head) does not. It seems that wrapping the data in a `pandas.Series` is causing the problem, it's fine without:\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\n\r\nx = np.random.rand(3)\r\ns = pd.Series(x)\r\n\r\ntorch.tensor(x[1:]) # OK\r\ntorch.tensor(s[1:]) # seg fault\r\n```"}