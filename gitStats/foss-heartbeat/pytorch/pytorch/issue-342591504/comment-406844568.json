{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/406844568", "html_url": "https://github.com/pytorch/pytorch/pull/9584#issuecomment-406844568", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9584", "id": 406844568, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjg0NDU2OA==", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-22T06:31:03Z", "updated_at": "2018-07-22T06:31:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This PR will generated a lot of follow-ups and unblock a lot of other changes. These are the known followups:</p>\n<ol>\n<li>Implement: <code>Graph::insert(Symbol name, at::ArrayRef&lt;Value*&gt; args, at::ArrayRef&lt;NamedValue&gt; kwargs, const SourceRange&amp; range)</code><br>\nReplace all create calls of aten::op things with <code>insert</code> rather than create, especially in SymbolicVariable.<br>\nConsider if there is a way to mix IValue's and Value* in args/kwargs so you can, e.g. write <code>g-&gt;emit(aten::add,{a, 1});</code></li>\n<li>(larger) Allow non-Tensor inputs to the graph executor. This requires re-design of ArgumentSpec and the specialization process.<br>\nThis change will require discussion.</li>\n<li>Add sane TensorToNum implicit conversions. This requires favoring operators with no-implicit conversions over operators with<br>\nthem, similar to C++ overloading rules. This will allow use of scalar tensors without using casts.</li>\n<li>Print should be able to print any IValue. We should also be able to print IValues for debugging purposes.<br>\nThis means adding operator&lt;&lt; logic to ivalues and modifying the print command.</li>\n<li>Change -&gt;expect and -&gt;cast on TypePtr to return <em>shared pointers</em> to the subtype rather than raw pointers.<br>\nAlso change isSubtypeOf to accept a TypePtr rather than a Type&amp;. This will make isSubtype cleaner:<br>\n<code>x-&gt;isSubtypeOf(NumberType::get())</code> rather than <code>x-&gt;isSubtypeOf(*NumberType::get())</code></li>\n<li>Allow int-&gt;float and float-&gt;int casts. Current we only allow <code>tensor-&gt;int</code> and <code>tensor-&gt;float</code> castws.</li>\n<li>(easy) shape prop formulat for NumToTensor</li>\n<li>(easy) move insertConstant back into Graph. Currently the optional SourceRange for insertConstant pulls the entire lexer into ir.h,<br>\nwe need to do some refactoring so that we can reference SourceRange in ir.h. This may be a very simple change.</li>\n<li>Remove 'varargs' behavior of cat and other things with tensor lists, and implement them with <em>actual lists of tensors</em>. This will be a difficult change<br>\nbecause some passes like fusion operator on cat nodes, and will need to continue to work to not slow down the LSTM examples.</li>\n</ol>", "body_text": "This PR will generated a lot of follow-ups and unblock a lot of other changes. These are the known followups:\n\nImplement: Graph::insert(Symbol name, at::ArrayRef<Value*> args, at::ArrayRef<NamedValue> kwargs, const SourceRange& range)\nReplace all create calls of aten::op things with insert rather than create, especially in SymbolicVariable.\nConsider if there is a way to mix IValue's and Value* in args/kwargs so you can, e.g. write g->emit(aten::add,{a, 1});\n(larger) Allow non-Tensor inputs to the graph executor. This requires re-design of ArgumentSpec and the specialization process.\nThis change will require discussion.\nAdd sane TensorToNum implicit conversions. This requires favoring operators with no-implicit conversions over operators with\nthem, similar to C++ overloading rules. This will allow use of scalar tensors without using casts.\nPrint should be able to print any IValue. We should also be able to print IValues for debugging purposes.\nThis means adding operator<< logic to ivalues and modifying the print command.\nChange ->expect and ->cast on TypePtr to return shared pointers to the subtype rather than raw pointers.\nAlso change isSubtypeOf to accept a TypePtr rather than a Type&. This will make isSubtype cleaner:\nx->isSubtypeOf(NumberType::get()) rather than x->isSubtypeOf(*NumberType::get())\nAllow int->float and float->int casts. Current we only allow tensor->int and tensor->float castws.\n(easy) shape prop formulat for NumToTensor\n(easy) move insertConstant back into Graph. Currently the optional SourceRange for insertConstant pulls the entire lexer into ir.h,\nwe need to do some refactoring so that we can reference SourceRange in ir.h. This may be a very simple change.\nRemove 'varargs' behavior of cat and other things with tensor lists, and implement them with actual lists of tensors. This will be a difficult change\nbecause some passes like fusion operator on cat nodes, and will need to continue to work to not slow down the LSTM examples.", "body": "This PR will generated a lot of follow-ups and unblock a lot of other changes. These are the known followups:\r\n1. Implement: `Graph::insert(Symbol name, at::ArrayRef<Value*> args, at::ArrayRef<NamedValue> kwargs, const SourceRange& range)`\r\n   Replace all create calls of aten::op things with `insert` rather than create, especially in SymbolicVariable.\r\n   Consider if there is a way to mix IValue's and Value* in args/kwargs so you can, e.g. write `g->emit(aten::add,{a, 1});`\r\n2. (larger) Allow non-Tensor inputs to the graph executor. This requires re-design of ArgumentSpec and the specialization process.\r\nThis change will require discussion.\r\n3. Add sane TensorToNum implicit conversions. This requires favoring operators with no-implicit conversions over operators with\r\n   them, similar to C++ overloading rules. This will allow use of scalar tensors without using casts.\r\n4. Print should be able to print any IValue. We should also be able to print IValues for debugging purposes.\r\n   This means adding operator<< logic to ivalues and modifying the print command.\r\n5. Change ->expect and ->cast on TypePtr to return _shared pointers_ to the subtype rather than raw pointers.\r\n   Also change isSubtypeOf to accept a TypePtr rather than a Type&. This will make isSubtype cleaner:\r\n     `x->isSubtypeOf(NumberType::get())` rather than `x->isSubtypeOf(*NumberType::get())`\r\n6. Allow int->float and float->int casts. Current we only allow `tensor->int` and `tensor->float` castws.\r\n7. (easy) shape prop formulat for NumToTensor\r\n8. (easy) move insertConstant back into Graph. Currently the optional SourceRange for insertConstant pulls the entire lexer into ir.h,\r\nwe need to do some refactoring so that we can reference SourceRange in ir.h. This may be a very simple change.\r\n9. Remove 'varargs' behavior of cat and other things with tensor lists, and implement them with _actual lists of tensors_. This will be a difficult change\r\nbecause some passes like fusion operator on cat nodes, and will need to continue to work to not slow down the LSTM examples.\r\n"}