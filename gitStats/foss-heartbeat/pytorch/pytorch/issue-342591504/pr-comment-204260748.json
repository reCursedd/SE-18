{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204260748", "pull_request_review_id": 139307370, "id": 204260748, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDI2MDc0OA==", "diff_hunk": "@@ -59,10 +65,11 @@ autograd::Variable getSizeOf(const autograd::Variable& var, int64_t dim) {\n   auto* value = getValueTrace(tracing_state, var);\n   auto* node = graph->create(aten::size, {value})\n                     ->i_(attr::dim, dim);\n-  node->output()->inferTypeFrom(size_var);\n+  node->output()->setType(jit::IntType::get());\n   graph->appendNode(node);\n-  setValueTrace(tracing_state, size_var, node->output());\n-\n+  auto ten =\n+      graph->appendNode(graph->createNumToTensor(node->output()))->output();", "path": "torch/csrc/jit/tracer.cpp", "position": null, "original_position": 25, "commit_id": "6a9e59de5c6c7e7821e48863cd70d8fee3fc1870", "original_commit_id": "d185b8f5c2bc76ff588b66d28a20e268cf4e91f5", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Yeah, I'm just not sure if this is something we really want to do. Basically all the scalar manipulation you might do on sizes will be traced as tensor operations, which are generally much more expensive. I guess we could have a \"tensor lowering\" pass that tries to prove that certain tensors are really scalars and optimize out known ops to ones on `double`/`int64_t` based `IValues`.", "created_at": "2018-07-23T00:16:46Z", "updated_at": "2018-11-23T15:47:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/9584#discussion_r204260748", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9584", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204260748"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9584#discussion_r204260748"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9584"}}, "body_html": "<p>Yeah, I'm just not sure if this is something we really want to do. Basically all the scalar manipulation you might do on sizes will be traced as tensor operations, which are generally much more expensive. I guess we could have a \"tensor lowering\" pass that tries to prove that certain tensors are really scalars and optimize out known ops to ones on <code>double</code>/<code>int64_t</code> based <code>IValues</code>.</p>", "body_text": "Yeah, I'm just not sure if this is something we really want to do. Basically all the scalar manipulation you might do on sizes will be traced as tensor operations, which are generally much more expensive. I guess we could have a \"tensor lowering\" pass that tries to prove that certain tensors are really scalars and optimize out known ops to ones on double/int64_t based IValues.", "in_reply_to_id": 203878477}