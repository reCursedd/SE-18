{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212415788", "pull_request_review_id": 149045065, "id": 212415788, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjQxNTc4OA==", "diff_hunk": "@@ -1,30 +1,92 @@\n+#include <utility>\n #include \"ATen/ATen.h\"\n \n namespace at { namespace native {\n \n-static inline void checkInputs(const Tensor& self, const Tensor& A) {\n-  if (A.size(-1) != A.size(-2)) {\n-    AT_ERROR(\"A must be batches of square matrices, \"\n-        \"but they are %lld by %lld matrices\",\n-        (long long)A.size(-1), (long long)A.size(-2));\n+static inline bool isTransposeContiguous(Tensor& self) {\n+  return self.dim() == 2 &&\n+          self.stride(0) == 1 &&\n+          self.stride(1) == self.size(0);\n+}\n+\n+/* gesv takes (self, A) and returns (sol, lu).\n+ * (i)  output tensors (sol, lu) may be same as input tensors (self, A)\n+ * (ii) for 2D matrices, .t_() represents their column-major format\n+ *\n+ * Cases:\n+ * 1) `out` has correct shape but elements do not form a contiguous\n+ * chunk of memory. Since shape is correct, we don't resize_ it. Instead, we\n+ * clone the input tensor into a buffer, use the buffer for Lapack and finally\n+ * copy the buffer to the output tensor.\n+ *\n+ * 2) out.t() is contiguous:\n+ *    (i)  &in == &out: use out.data() as is. Do nothing\n+ *    (ii) &in != &out: copy in.t() to out.t()\n+ * 3) out.t() is not contiguous:\n+ *    - resize_ should fix contiguity/size issues\n+ *    (i)  &in == &out: copy in.t().clone() to out (same tensor)\n+ *    (ii) &in != &out: copy in.t() to out\n+ */\n+static inline void prepareIOTensors(\n+    const Tensor& in, Tensor& out, Tensor& temp,\n+    int64_t& x, int64_t& y) {", "path": "aten/src/ATen/native/Gesv.h", "position": null, "original_position": 37, "commit_id": "6c573ec2fe05deacdb8541c2229f21fa51d96e30", "original_commit_id": "a01c96e6c01e093db3f27df4565e905cc2d1ee56", "user": {"login": "animesht", "id": 1777276, "node_id": "MDQ6VXNlcjE3NzcyNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1777276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/animesht", "html_url": "https://github.com/animesht", "followers_url": "https://api.github.com/users/animesht/followers", "following_url": "https://api.github.com/users/animesht/following{/other_user}", "gists_url": "https://api.github.com/users/animesht/gists{/gist_id}", "starred_url": "https://api.github.com/users/animesht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/animesht/subscriptions", "organizations_url": "https://api.github.com/users/animesht/orgs", "repos_url": "https://api.github.com/users/animesht/repos", "events_url": "https://api.github.com/users/animesht/events{/privacy}", "received_events_url": "https://api.github.com/users/animesht/received_events", "type": "User", "site_admin": false}, "body": "It's just easier to pass them by reference here so that I don't have to get (y, x) from the output tensor. I don't really use the values written for `ax, ay` (`A`'s dimensions), but pass `bx`, `by` to Lapack.", "created_at": "2018-08-23T18:40:15Z", "updated_at": "2018-11-23T15:49:51Z", "html_url": "https://github.com/pytorch/pytorch/pull/9742#discussion_r212415788", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9742", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212415788"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9742#discussion_r212415788"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9742"}}, "body_html": "<p>It's just easier to pass them by reference here so that I don't have to get (y, x) from the output tensor. I don't really use the values written for <code>ax, ay</code> (<code>A</code>'s dimensions), but pass <code>bx</code>, <code>by</code> to Lapack.</p>", "body_text": "It's just easier to pass them by reference here so that I don't have to get (y, x) from the output tensor. I don't really use the values written for ax, ay (A's dimensions), but pass bx, by to Lapack.", "in_reply_to_id": 212413933}