{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209035639", "pull_request_review_id": 144966796, "id": 209035639, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTAzNTYzOQ==", "diff_hunk": "@@ -72,6 +78,114 @@ static void applyGesv(Tensor& b, Tensor& A, std::vector<int64_t> infos) {\n   }\n }\n \n+std::tuple<Tensor&,Tensor&> _gesv_single_out_cpu(\n+    Tensor& sol, Tensor& lu,\n+    const Tensor& self, const Tensor& A) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"gesv: LAPACK library not found in compilation\");\n+#endif\n+  /* gesv takes two tensors (self, A) and returns (sol, lu).\n+   * The output Tensors (sol, lu) may be the same as input Tensors (self, A)\n+   *\n+   * Before passing pointers into Lapack, we need to ensure that:\n+   * (i)  self and A are represented in column major format\n+   * (ii) These pointers point to contiguous data for self and A.\n+   *\n+   * For 2D matrices, A.t() and self.t() represent their column major formats\n+   *\n+   * Case 1) The output tensor is of the correct shape, but it and its transpose\n+   *         are not contiguous. eg. torch.gesv(... , out=(n[::2], ...)):", "path": "aten/src/ATen/native/Gesv.cpp", "position": null, "original_position": 42, "commit_id": "6c573ec2fe05deacdb8541c2229f21fa51d96e30", "original_commit_id": "bfb323c3bcf95309f020c69ad16535aa8681d53e", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "This is still a bit confusing because what you really want to test if the elements form a contiguous chunk of memory. I would instead say something along the lines of:\r\n```\r\nThe output tensor is of the correct shape, but its elements do not form \r\na contiguous chunk of memory (i.e., it and its transpose are not contiguous). \r\nSince shape is correct, we must not resize_ it, but clone input into a \r\nbuffer, use the buffer for lapack, and copy to output afterwards.\r\n```", "created_at": "2018-08-09T18:33:58Z", "updated_at": "2018-11-23T15:49:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/9742#discussion_r209035639", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9742", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209035639"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9742#discussion_r209035639"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9742"}}, "body_html": "<p>This is still a bit confusing because what you really want to test if the elements form a contiguous chunk of memory. I would instead say something along the lines of:</p>\n<pre><code>The output tensor is of the correct shape, but its elements do not form \na contiguous chunk of memory (i.e., it and its transpose are not contiguous). \nSince shape is correct, we must not resize_ it, but clone input into a \nbuffer, use the buffer for lapack, and copy to output afterwards.\n</code></pre>", "body_text": "This is still a bit confusing because what you really want to test if the elements form a contiguous chunk of memory. I would instead say something along the lines of:\nThe output tensor is of the correct shape, but its elements do not form \na contiguous chunk of memory (i.e., it and its transpose are not contiguous). \nSince shape is correct, we must not resize_ it, but clone input into a \nbuffer, use the buffer for lapack, and copy to output afterwards."}