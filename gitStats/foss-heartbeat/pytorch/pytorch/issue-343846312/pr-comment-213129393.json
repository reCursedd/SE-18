{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213129393", "pull_request_review_id": 149900988, "id": 213129393, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMzEyOTM5Mw==", "diff_hunk": "@@ -1,30 +1,94 @@\n+#include <utility>\n #include \"ATen/ATen.h\"\n \n namespace at { namespace native {\n \n-static inline void checkInputs(const Tensor& self, const Tensor& A) {\n-  if (A.size(-1) != A.size(-2)) {\n-    AT_ERROR(\"A must be batches of square matrices, \"\n-        \"but they are %lld by %lld matrices\",\n-        (long long)A.size(-1), (long long)A.size(-2));\n+static inline bool isTransposeContiguous(Tensor& self) {\n+  return self.dim() == 2 &&\n+          self.stride(0) == 1 &&\n+          self.stride(1) == self.size(0);\n+}\n+\n+/* gesv takes (self, A) and returns (sol, lu).\n+ * (i)  output tensors (sol, lu) may be same as input tensors (self, A)\n+ * (ii) for 2D matrices, .t_() represents their column-major format\n+ *\n+ * Before passing pointers to Lapack, we need to ensure that these pointers\n+ * represent Fortran-contiguous tensors in column-major format\n+ *\n+ * Cases:\n+ * 1) `out` has correct shape but elements do not form a contiguous\n+ * chunk of memory. Since shape is correct, we don't resize_ it. Instead, we\n+ * clone the input tensor into a buffer, use the buffer for Lapack and finally\n+ * copy the buffer to the output tensor.\n+ *\n+ * 2) out.t() is contiguous:\n+ *    (i)  &in == &out: use out.data() as is. Do nothing", "path": "aten/src/ATen/native/Gesv.h", "position": 31, "original_position": 31, "commit_id": "6c573ec2fe05deacdb8541c2229f21fa51d96e30", "original_commit_id": "5341fee0443fab16c4386954242d6bb1e0b30cf2", "user": {"login": "animesht", "id": 1777276, "node_id": "MDQ6VXNlcjE3NzcyNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1777276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/animesht", "html_url": "https://github.com/animesht", "followers_url": "https://api.github.com/users/animesht/followers", "following_url": "https://api.github.com/users/animesht/following{/other_user}", "gists_url": "https://api.github.com/users/animesht/gists{/gist_id}", "starred_url": "https://api.github.com/users/animesht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/animesht/subscriptions", "organizations_url": "https://api.github.com/users/animesht/orgs", "repos_url": "https://api.github.com/users/animesht/repos", "events_url": "https://api.github.com/users/animesht/events{/privacy}", "received_events_url": "https://api.github.com/users/animesht/received_events", "type": "User", "site_admin": false}, "body": "The main reason for these optimizations is that the Lapack `gesv` interface takes two input tensors to operate on and the same tensors contain the outputs after the function is called, so it's nice to support that. Also, `test_gesv` explicitly checks for this case.", "created_at": "2018-08-27T21:58:55Z", "updated_at": "2018-11-23T15:50:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/9742#discussion_r213129393", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9742", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213129393"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9742#discussion_r213129393"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9742"}}, "body_html": "<p>The main reason for these optimizations is that the Lapack <code>gesv</code> interface takes two input tensors to operate on and the same tensors contain the outputs after the function is called, so it's nice to support that. Also, <code>test_gesv</code> explicitly checks for this case.</p>", "body_text": "The main reason for these optimizations is that the Lapack gesv interface takes two input tensors to operate on and the same tensors contain the outputs after the function is called, so it's nice to support that. Also, test_gesv explicitly checks for this case.", "in_reply_to_id": 213125036}