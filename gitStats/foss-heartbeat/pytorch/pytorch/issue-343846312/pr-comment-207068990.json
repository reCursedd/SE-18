{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207068990", "pull_request_review_id": 142607249, "id": 207068990, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNzA2ODk5MA==", "diff_hunk": "@@ -72,6 +78,63 @@ static void applyGesv(Tensor& b, Tensor& A, std::vector<int64_t> infos) {\n   }\n }\n \n+std::tuple<Tensor&,Tensor&> _gesv_single_out_cpu(\n+    Tensor& sol, Tensor& lu,\n+    const Tensor& self, const Tensor& A) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"gesv: LAPACK library not found in compilation\");\n+#endif\n+  int64_t bx = self.size(0);\n+  int64_t by = (self.dim() == 1) ? 1 : self.size(1);\n+  int64_t ax = A.size(0);\n+  int64_t ay = A.size(1);\n+  int info = 0;\n+\n+  /* Init to column major format. `sol` and `lu` need to be contiguous\n+   * since we pass sol.data() and lu.data() to Lapack */\n+  bool tc = sol.dim() == 2 && isTransposeContiguous(sol);\n+  if (tc) {\n+    // if transpose of output tensor is contiguous, use it\n+    sol.t_();\n+  }\n+\n+  sol.resize_({by, bx});\n+  if (self.data_ptr() == sol.data_ptr()) {", "path": "aten/src/ATen/native/Gesv.cpp", "position": null, "original_position": 47, "commit_id": "6c573ec2fe05deacdb8541c2229f21fa51d96e30", "original_commit_id": "a4ae8617d9d37adb709942b6fbace6af0d05842f", "user": {"login": "animesht", "id": 1777276, "node_id": "MDQ6VXNlcjE3NzcyNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1777276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/animesht", "html_url": "https://github.com/animesht", "followers_url": "https://api.github.com/users/animesht/followers", "following_url": "https://api.github.com/users/animesht/following{/other_user}", "gists_url": "https://api.github.com/users/animesht/gists{/gist_id}", "starred_url": "https://api.github.com/users/animesht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/animesht/subscriptions", "organizations_url": "https://api.github.com/users/animesht/orgs", "repos_url": "https://api.github.com/users/animesht/repos", "events_url": "https://api.github.com/users/animesht/events{/privacy}", "received_events_url": "https://api.github.com/users/animesht/received_events", "type": "User", "site_admin": false}, "body": "If I remove this optimization and always do `sol.copy_(self.view({bx, by}).t().clone()` it will work. This code handles three cases:\r\n1) When `sol` and `self` are the same Tensor (I'll revert to the previous address check)\r\n    a) `sol` is transpose contiguous, so we transpose it (need to transpose input B anyway for Lapack)\r\n    b) `sol`'s transpose is not contiguous, so it should be contiguous, but we need to copy the transpose of the input (`self`) into this contiguous tensor before passing it to Lapack. Since they share underlying storage, `self.view(...).t()` is cloned before copying to sol.\r\n2) When `sol` and `self` are different tensors: we can do the copy to `sol` from `self` without the extra `.clone()`\r\n\r\n\r\n    ", "created_at": "2018-08-02T00:10:52Z", "updated_at": "2018-11-23T15:48:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/9742#discussion_r207068990", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9742", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207068990"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9742#discussion_r207068990"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9742"}}, "body_html": "<p>If I remove this optimization and always do <code>sol.copy_(self.view({bx, by}).t().clone()</code> it will work. This code handles three cases:</p>\n<ol>\n<li>When <code>sol</code> and <code>self</code> are the same Tensor (I'll revert to the previous address check)<br>\na) <code>sol</code> is transpose contiguous, so we transpose it (need to transpose input B anyway for Lapack)<br>\nb) <code>sol</code>'s transpose is not contiguous, so it should be contiguous, but we need to copy the transpose of the input (<code>self</code>) into this contiguous tensor before passing it to Lapack. Since they share underlying storage, <code>self.view(...).t()</code> is cloned before copying to sol.</li>\n<li>When <code>sol</code> and <code>self</code> are different tensors: we can do the copy to <code>sol</code> from <code>self</code> without the extra <code>.clone()</code></li>\n</ol>", "body_text": "If I remove this optimization and always do sol.copy_(self.view({bx, by}).t().clone() it will work. This code handles three cases:\n\nWhen sol and self are the same Tensor (I'll revert to the previous address check)\na) sol is transpose contiguous, so we transpose it (need to transpose input B anyway for Lapack)\nb) sol's transpose is not contiguous, so it should be contiguous, but we need to copy the transpose of the input (self) into this contiguous tensor before passing it to Lapack. Since they share underlying storage, self.view(...).t() is cloned before copying to sol.\nWhen sol and self are different tensors: we can do the copy to sol from self without the extra .clone()", "in_reply_to_id": 207046012}