{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213491706", "pull_request_review_id": 150340372, "id": 213491706, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMzQ5MTcwNg==", "diff_hunk": "@@ -1,30 +1,94 @@\n+#include <utility>\n #include \"ATen/ATen.h\"\n \n namespace at { namespace native {\n \n-static inline void checkInputs(const Tensor& self, const Tensor& A) {\n-  if (A.size(-1) != A.size(-2)) {\n-    AT_ERROR(\"A must be batches of square matrices, \"\n-        \"but they are %lld by %lld matrices\",\n-        (long long)A.size(-1), (long long)A.size(-2));\n+static inline bool isTransposeContiguous(Tensor& self) {\n+  return self.dim() == 2 &&\n+          self.stride(0) == 1 &&\n+          self.stride(1) == self.size(0);\n+}\n+\n+/* gesv takes (self, A) and returns (sol, lu).\n+ * (i)  output tensors (sol, lu) may be same as input tensors (self, A)\n+ * (ii) for 2D matrices, .t_() represents their column-major format\n+ *\n+ * Before passing pointers to Lapack, we need to ensure that these pointers\n+ * represent Fortran-contiguous tensors in column-major format\n+ *\n+ * Cases:\n+ * 1) `out` has correct shape but elements do not form a contiguous\n+ * chunk of memory. Since shape is correct, we don't resize_ it. Instead, we\n+ * clone the input tensor into a buffer, use the buffer for Lapack and finally\n+ * copy the buffer to the output tensor.\n+ *\n+ * 2) out.t() is contiguous:\n+ *    (i)  &in == &out: use out.data() as is. Do nothing\n+ *    (ii) &in != &out: copy in.t() to out.t()\n+ * 3) out.t() is not contiguous:\n+ *    - resize_ should fix contiguity/size issues\n+ *    (i)  &in == &out: copy in.t().clone() to out (same tensor)\n+ *    (ii) &in != &out: copy in.t() to out\n+ */\n+static inline void prepareTensorsForLapack(\n+    const Tensor& in, Tensor& out, Tensor& temp) {\n+  int64_t x = in.size(0);\n+  int64_t y = (in.dim() == 1) ? 1 : in.size(1);\n+  bool out_tc = isTransposeContiguous(out);\n+  bool out_correct_shape =\n+    out.dim() == 2 && out.size(0) == x && out.size(1) == y;\n+\n+  // view potential 1D `in` as 2D\n+  auto in_t = in.view({x, y}).t_();\n+\n+  if (!out_tc && !out.is_contiguous() && out_correct_shape) {", "path": "aten/src/ATen/native/Gesv.h", "position": 49, "original_position": 49, "commit_id": "6c573ec2fe05deacdb8541c2229f21fa51d96e30", "original_commit_id": "5341fee0443fab16c4386954242d6bb1e0b30cf2", "user": {"login": "animesht", "id": 1777276, "node_id": "MDQ6VXNlcjE3NzcyNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1777276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/animesht", "html_url": "https://github.com/animesht", "followers_url": "https://api.github.com/users/animesht/followers", "following_url": "https://api.github.com/users/animesht/following{/other_user}", "gists_url": "https://api.github.com/users/animesht/gists{/gist_id}", "starred_url": "https://api.github.com/users/animesht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/animesht/subscriptions", "organizations_url": "https://api.github.com/users/animesht/orgs", "repos_url": "https://api.github.com/users/animesht/repos", "events_url": "https://api.github.com/users/animesht/events{/privacy}", "received_events_url": "https://api.github.com/users/animesht/received_events", "type": "User", "site_admin": false}, "body": "Basically, the only case where the Tensor falls through the ladder without being affected is when `out` is transpose contiguous and `&in == &out`. The third case is the generic case, where we resize, copy and transpose as usual if the two special cases above don't hold. Your suggestion would look something like this:\r\n\r\n```\r\nif (out_correct_shape) {\r\n  if (!out.is_contiguous() && !out_tc) {\r\n     temp = in_t.clone().t_();\r\n  } else if (out.is_contiguous()) {\r\n     // handle &in == &out\r\n     // copy into out\r\n  } else if (out_tc) {\r\n    if (&in != &out) {\r\n      // copy into out.t()\r\n    }\r\n  }\r\n} else {\r\n  if (out_tc) {\r\n    // resize out.t()\r\n    if (&in != &out) {\r\n      // copy into out.t()\r\n    }\r\n  } else {\r\n     // resize out -- we may need to resize even when &in == &out since `b` can be 1D\r\n     // handle &in == &out\r\n     // copy into out\r\n  }\r\n}", "created_at": "2018-08-28T22:22:57Z", "updated_at": "2018-11-23T15:50:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/9742#discussion_r213491706", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9742", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213491706"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9742#discussion_r213491706"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9742"}}, "body_html": "<p>Basically, the only case where the Tensor falls through the ladder without being affected is when <code>out</code> is transpose contiguous and <code>&amp;in == &amp;out</code>. The third case is the generic case, where we resize, copy and transpose as usual if the two special cases above don't hold. Your suggestion would look something like this:</p>\n<pre><code>if (out_correct_shape) {\n  if (!out.is_contiguous() &amp;&amp; !out_tc) {\n     temp = in_t.clone().t_();\n  } else if (out.is_contiguous()) {\n     // handle &amp;in == &amp;out\n     // copy into out\n  } else if (out_tc) {\n    if (&amp;in != &amp;out) {\n      // copy into out.t()\n    }\n  }\n} else {\n  if (out_tc) {\n    // resize out.t()\n    if (&amp;in != &amp;out) {\n      // copy into out.t()\n    }\n  } else {\n     // resize out -- we may need to resize even when &amp;in == &amp;out since `b` can be 1D\n     // handle &amp;in == &amp;out\n     // copy into out\n  }\n}\n</code></pre>", "body_text": "Basically, the only case where the Tensor falls through the ladder without being affected is when out is transpose contiguous and &in == &out. The third case is the generic case, where we resize, copy and transpose as usual if the two special cases above don't hold. Your suggestion would look something like this:\nif (out_correct_shape) {\n  if (!out.is_contiguous() && !out_tc) {\n     temp = in_t.clone().t_();\n  } else if (out.is_contiguous()) {\n     // handle &in == &out\n     // copy into out\n  } else if (out_tc) {\n    if (&in != &out) {\n      // copy into out.t()\n    }\n  }\n} else {\n  if (out_tc) {\n    // resize out.t()\n    if (&in != &out) {\n      // copy into out.t()\n    }\n  } else {\n     // resize out -- we may need to resize even when &in == &out since `b` can be 1D\n     // handle &in == &out\n     // copy into out\n  }\n}", "in_reply_to_id": 213126908}