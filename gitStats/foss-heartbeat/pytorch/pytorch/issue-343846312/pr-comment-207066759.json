{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207066759", "pull_request_review_id": 142604791, "id": 207066759, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNzA2Njc1OQ==", "diff_hunk": "@@ -72,6 +78,63 @@ static void applyGesv(Tensor& b, Tensor& A, std::vector<int64_t> infos) {\n   }\n }\n \n+std::tuple<Tensor&,Tensor&> _gesv_single_out_cpu(\n+    Tensor& sol, Tensor& lu,\n+    const Tensor& self, const Tensor& A) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"gesv: LAPACK library not found in compilation\");\n+#endif\n+  int64_t bx = self.size(0);\n+  int64_t by = (self.dim() == 1) ? 1 : self.size(1);\n+  int64_t ax = A.size(0);\n+  int64_t ay = A.size(1);\n+  int info = 0;\n+\n+  /* Init to column major format. `sol` and `lu` need to be contiguous\n+   * since we pass sol.data() and lu.data() to Lapack */\n+  bool tc = sol.dim() == 2 && isTransposeContiguous(sol);\n+  if (tc) {\n+    // if transpose of output tensor is contiguous, use it\n+    sol.t_();", "path": "aten/src/ATen/native/Gesv.cpp", "position": null, "original_position": 43, "commit_id": "6c573ec2fe05deacdb8541c2229f21fa51d96e30", "original_commit_id": "a4ae8617d9d37adb709942b6fbace6af0d05842f", "user": {"login": "animesht", "id": 1777276, "node_id": "MDQ6VXNlcjE3NzcyNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1777276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/animesht", "html_url": "https://github.com/animesht", "followers_url": "https://api.github.com/users/animesht/followers", "following_url": "https://api.github.com/users/animesht/following{/other_user}", "gists_url": "https://api.github.com/users/animesht/gists{/gist_id}", "starred_url": "https://api.github.com/users/animesht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/animesht/subscriptions", "organizations_url": "https://api.github.com/users/animesht/orgs", "repos_url": "https://api.github.com/users/animesht/repos", "events_url": "https://api.github.com/users/animesht/events{/privacy}", "received_events_url": "https://api.github.com/users/animesht/received_events", "type": "User", "site_admin": false}, "body": "<img width=\"511\" alt=\"screen shot 2018-08-01 at 4 46 01 pm\" src=\"https://user-images.githubusercontent.com/1777276/43554652-722cc238-95aa-11e8-93f7-01bd731a227c.png\">\r\nSo in the test cases and the documentation the Tensors are initialized and then transposed before being passed to `torch.gesv`. We have to transpose it internally as well for Lapack/Magma's column major format and we need the data at`A_ptr` and `b_ptr` to be contiguous before we pass it. If a user calls `torch.gesv(b, A, out=(b, A))` and has transposed the input Tensor since creating it, we can just transpose `sol` here and use it directly.  The resize will not have any effect if it is already the required size (eg. the above call) but it will take care of Tensors whose transpose is contiguous but size is incorrect. Also, this helps with reusing output tensors;\r\n\r\nfor example:\r\n\r\n    ta = torch.Tensor()\r\n    tb = torch.Tensor()\r\n    torch.gesv(b, a, out=(ta, tb)) # this will work\r\n    torch.gesv(b, a, out=(ta, tb)) # this will not\r\nThe second time `gesv` is called, ta and tb will have been generated as a result of Lapack/Magma and transposed before being returned (our convention is to always return the transpose according to documentation). So even though they may have the correct size (resize_ won't work as expected on a non-contiguous Tensor), we should use their transpose if it is contiguous since we need pointers to contiguous data.", "created_at": "2018-08-01T23:56:09Z", "updated_at": "2018-11-23T15:48:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/9742#discussion_r207066759", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9742", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207066759"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9742#discussion_r207066759"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9742"}}, "body_html": "<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1777276/43554652-722cc238-95aa-11e8-93f7-01bd731a227c.png\"><img width=\"511\" alt=\"screen shot 2018-08-01 at 4 46 01 pm\" src=\"https://user-images.githubusercontent.com/1777276/43554652-722cc238-95aa-11e8-93f7-01bd731a227c.png\" style=\"max-width:100%;\"></a></p>\nSo in the test cases and the documentation the Tensors are initialized and then transposed before being passed to `torch.gesv`. We have to transpose it internally as well for Lapack/Magma's column major format and we need the data at`A_ptr` and `b_ptr` to be contiguous before we pass it. If a user calls `torch.gesv(b, A, out=(b, A))` and has transposed the input Tensor since creating it, we can just transpose `sol` here and use it directly.  The resize will not have any effect if it is already the required size (eg. the above call) but it will take care of Tensors whose transpose is contiguous but size is incorrect. Also, this helps with reusing output tensors;\n<p>for example:</p>\n<pre><code>ta = torch.Tensor()\ntb = torch.Tensor()\ntorch.gesv(b, a, out=(ta, tb)) # this will work\ntorch.gesv(b, a, out=(ta, tb)) # this will not\n</code></pre>\n<p>The second time <code>gesv</code> is called, ta and tb will have been generated as a result of Lapack/Magma and transposed before being returned (our convention is to always return the transpose according to documentation). So even though they may have the correct size (resize_ won't work as expected on a non-contiguous Tensor), we should use their transpose if it is contiguous since we need pointers to contiguous data.</p>", "body_text": "So in the test cases and the documentation the Tensors are initialized and then transposed before being passed to `torch.gesv`. We have to transpose it internally as well for Lapack/Magma's column major format and we need the data at`A_ptr` and `b_ptr` to be contiguous before we pass it. If a user calls `torch.gesv(b, A, out=(b, A))` and has transposed the input Tensor since creating it, we can just transpose `sol` here and use it directly.  The resize will not have any effect if it is already the required size (eg. the above call) but it will take care of Tensors whose transpose is contiguous but size is incorrect. Also, this helps with reusing output tensors;\nfor example:\nta = torch.Tensor()\ntb = torch.Tensor()\ntorch.gesv(b, a, out=(ta, tb)) # this will work\ntorch.gesv(b, a, out=(ta, tb)) # this will not\n\nThe second time gesv is called, ta and tb will have been generated as a result of Lapack/Magma and transposed before being returned (our convention is to always return the transpose according to documentation). So even though they may have the correct size (resize_ won't work as expected on a non-contiguous Tensor), we should use their transpose if it is contiguous since we need pointers to contiguous data.", "in_reply_to_id": 207047692}