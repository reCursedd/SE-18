{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/408649297", "html_url": "https://github.com/pytorch/pytorch/pull/9742#issuecomment-408649297", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9742", "id": 408649297, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODY0OTI5Nw==", "user": {"login": "animesht", "id": 1777276, "node_id": "MDQ6VXNlcjE3NzcyNzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1777276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/animesht", "html_url": "https://github.com/animesht", "followers_url": "https://api.github.com/users/animesht/followers", "following_url": "https://api.github.com/users/animesht/following{/other_user}", "gists_url": "https://api.github.com/users/animesht/gists{/gist_id}", "starred_url": "https://api.github.com/users/animesht/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/animesht/subscriptions", "organizations_url": "https://api.github.com/users/animesht/orgs", "repos_url": "https://api.github.com/users/animesht/repos", "events_url": "https://api.github.com/users/animesht/events{/privacy}", "received_events_url": "https://api.github.com/users/animesht/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-29T03:27:49Z", "updated_at": "2018-07-30T23:03:08Z", "author_association": "NONE", "body_html": "<p><del>I handled re-use of output tensors and use of the input tensors as outputs, however, an example of one edge-case that I haven't handled yet is when <code>gesv</code> is called like this:</del></p>\n<pre><code>a = torch.Tensor([[2, -3], [-5, 5]]) \nb = torch.Tensor([-1, 20])   # b is 1D\ntorch.gesv(b, a, out=(b, a))\n</code></pre>\n<p><del>When output <code>X</code> is being written to the input variable <code>b</code>, and <code>b.dim() == 1</code>. I've tried using <code>.unsqueeze_(1)</code> and <code>.resize_(b.size(0), 1)</code> on the <code>b</code> Tensor to make it 2D before calling Lapack, but it produces incorrect values -- I think it might be because Lapack takes a pointer to the data, which can be non-contiguous after these calls. Any suggestions on how to make this work?</del></p>\n<p><del>Alternatively, we can remove support for 1D <code>b</code> tensors (the documentation currently says that <code>b</code> should be 2D), or not allow <code>gesv_out</code> to be called with them.</del></p>\n<p>edit: NVM, fixed this issue. The issue was related to the user passing in the transpose of a contiguous tensor as an output tensor</p>", "body_text": "I handled re-use of output tensors and use of the input tensors as outputs, however, an example of one edge-case that I haven't handled yet is when gesv is called like this:\na = torch.Tensor([[2, -3], [-5, 5]]) \nb = torch.Tensor([-1, 20])   # b is 1D\ntorch.gesv(b, a, out=(b, a))\n\nWhen output X is being written to the input variable b, and b.dim() == 1. I've tried using .unsqueeze_(1) and .resize_(b.size(0), 1) on the b Tensor to make it 2D before calling Lapack, but it produces incorrect values -- I think it might be because Lapack takes a pointer to the data, which can be non-contiguous after these calls. Any suggestions on how to make this work?\nAlternatively, we can remove support for 1D b tensors (the documentation currently says that b should be 2D), or not allow gesv_out to be called with them.\nedit: NVM, fixed this issue. The issue was related to the user passing in the transpose of a contiguous tensor as an output tensor", "body": "~~I handled re-use of output tensors and use of the input tensors as outputs, however, an example of one edge-case that I haven't handled yet is when `gesv` is called like this:~~\r\n\r\n    a = torch.Tensor([[2, -3], [-5, 5]]) \r\n    b = torch.Tensor([-1, 20])   # b is 1D\r\n    torch.gesv(b, a, out=(b, a))\r\n\r\n~~When output `X` is being written to the input variable `b`, and `b.dim() == 1`. I've tried using `.unsqueeze_(1)` and `.resize_(b.size(0), 1)` on the `b` Tensor to make it 2D before calling Lapack, but it produces incorrect values -- I think it might be because Lapack takes a pointer to the data, which can be non-contiguous after these calls. Any suggestions on how to make this work?~~\r\n\r\n~~Alternatively, we can remove support for 1D `b` tensors (the documentation currently says that `b` should be 2D), or not allow `gesv_out` to be called with them.~~\r\n\r\nedit: NVM, fixed this issue. The issue was related to the user passing in the transpose of a contiguous tensor as an output tensor\r\n"}