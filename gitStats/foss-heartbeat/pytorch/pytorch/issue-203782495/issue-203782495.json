{"url": "https://api.github.com/repos/pytorch/pytorch/issues/619", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/619/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/619/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/619/events", "html_url": "https://github.com/pytorch/pytorch/issues/619", "id": 203782495, "node_id": "MDU6SXNzdWUyMDM3ODI0OTU=", "number": 619, "title": "Backprop issue I can't figure out", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-01-28T02:36:00Z", "updated_at": "2017-01-28T13:37:32Z", "closed_at": "2017-01-28T13:37:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here's a straightforward one-layer bidirectional LSTM for sentiment classification -- but it only trains with <code>self.l_out(hn2)</code> while the loss stays above 0.6 with <code>self.l_out(hn1)</code> (yet the <code>assert</code> is never violated). The same is true if I used fixed embeddings with <code>.detach()</code> or pretrained GloVe embeddings.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> optim\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">from</span> torchtext <span class=\"pl-k\">import</span> data\n<span class=\"pl-k\">from</span> torchtext <span class=\"pl-k\">import</span> datasets\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\n\n<span class=\"pl-c1\">TEXT</span> <span class=\"pl-k\">=</span> data.Field()\n<span class=\"pl-c1\">LABEL</span> <span class=\"pl-k\">=</span> data.Field(<span class=\"pl-v\">sequential</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\ntrain, valid, test <span class=\"pl-k\">=</span> datasets.<span class=\"pl-c1\">SST</span>.splits(\n    <span class=\"pl-c1\">TEXT</span>, <span class=\"pl-c1\">LABEL</span>, <span class=\"pl-v\">fine_grained</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">train_subtrees</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">filter_pred</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">ex</span>: ex.label <span class=\"pl-k\">!=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>neutral<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c1\">TEXT</span>.build_vocab(train)\n<span class=\"pl-c1\">LABEL</span>.build_vocab(train)\n\ntrain_iter <span class=\"pl-k\">=</span> data.BucketIterator(train, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">LABEL</span>.vocab)\nnum_input_symbols <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">TEXT</span>.vocab)\nrnn_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">lstm_bi</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.embedding <span class=\"pl-k\">=</span> nn.Embedding(num_input_symbols, rnn_size)\n        <span class=\"pl-c1\">self</span>.rnn <span class=\"pl-k\">=</span> nn.LSTM(rnn_size, rnn_size, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bidirectional</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.l_out <span class=\"pl-k\">=</span> nn.Linear(rnn_size <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, num_classes)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        embeds <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embedding(x)\n        _batch_size <span class=\"pl-k\">=</span> embeds.size(<span class=\"pl-c1\">1</span>)\n        h0 <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, _batch_size, rnn_size).cuda())\n        c0 <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, _batch_size, rnn_size).cuda())\n        _, (hn, _) <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.rnn(embeds, (h0, c0))\n        hn1 <span class=\"pl-k\">=</span> hn.transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>).contiguous().view(_batch_size, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        hn2 <span class=\"pl-k\">=</span> torch.cat([hn[<span class=\"pl-c1\">0</span>], hn[<span class=\"pl-c1\">1</span>]], <span class=\"pl-c1\">1</span>)\n        diff <span class=\"pl-k\">=</span> (hn1 <span class=\"pl-k\">-</span> hn2).norm().data[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-k\">assert</span> diff <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.l_out(hn2)\n\nnet <span class=\"pl-k\">=</span> lstm_bi()\nnet.cuda()\ncriterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()\noptimizer <span class=\"pl-k\">=</span> optim.Adam(net.parameters())\n\n<span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> train_iter:\n    outputs <span class=\"pl-k\">=</span> net(batch.text)\n    cost <span class=\"pl-k\">=</span> criterion(outputs, batch.label)\n    <span class=\"pl-c1\">print</span>(cost.data[<span class=\"pl-c1\">0</span>])\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()</pre></div>\n<p>If I use just the formulas for <code>hn1</code> and <code>hn2</code> in isolation, <code>.backward()</code> gives the same results for <code>hn.grad</code> for each; the only gradient difference I've found is that if I iterate through the parameters after <code>cost.backward()</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> i, param <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(net.parameters()):\n   <span class=\"pl-c1\">print</span>(i, param.grad.data.norm())</pre></div>\n<p>some of the LSTM gradients are zero when using <code>hn2</code>. But none of them should be zero...and <code>hn2</code> is the one that works...</p>", "body_text": "Here's a straightforward one-layer bidirectional LSTM for sentiment classification -- but it only trains with self.l_out(hn2) while the loss stays above 0.6 with self.l_out(hn1) (yet the assert is never violated). The same is true if I used fixed embeddings with .detach() or pretrained GloVe embeddings.\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.autograd import Variable\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nbatch_size = 64\n\nTEXT = data.Field()\nLABEL = data.Field(sequential=False)\n\ntrain, valid, test = datasets.SST.splits(\n    TEXT, LABEL, fine_grained=False, train_subtrees=True,\n    filter_pred=lambda ex: ex.label != 'neutral')\n\nTEXT.build_vocab(train)\nLABEL.build_vocab(train)\n\ntrain_iter = data.BucketIterator(train, batch_size=batch_size, device=0)\n\nnum_classes = len(LABEL.vocab)\nnum_input_symbols = len(TEXT.vocab)\nrnn_size = 100\n\nclass lstm_bi(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(num_input_symbols, rnn_size)\n        self.rnn = nn.LSTM(rnn_size, rnn_size, 1, bidirectional=True)\n        self.l_out = nn.Linear(rnn_size * 2, num_classes)\n\n    def forward(self, x):\n        embeds = self.embedding(x)\n        _batch_size = embeds.size(1)\n        h0 = Variable(torch.zeros(1 * 2, _batch_size, rnn_size).cuda())\n        c0 = Variable(torch.zeros(1 * 2, _batch_size, rnn_size).cuda())\n        _, (hn, _) = self.rnn(embeds, (h0, c0))\n        hn1 = hn.transpose(0, 1).contiguous().view(_batch_size, -1)\n        hn2 = torch.cat([hn[0], hn[1]], 1)\n        diff = (hn1 - hn2).norm().data[0]\n        assert diff == 0\n        return self.l_out(hn2)\n\nnet = lstm_bi()\nnet.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters())\n\nfor batch in train_iter:\n    outputs = net(batch.text)\n    cost = criterion(outputs, batch.label)\n    print(cost.data[0])\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\nIf I use just the formulas for hn1 and hn2 in isolation, .backward() gives the same results for hn.grad for each; the only gradient difference I've found is that if I iterate through the parameters after cost.backward():\nfor i, param in enumerate(net.parameters()):\n   print(i, param.grad.data.norm())\nsome of the LSTM gradients are zero when using hn2. But none of them should be zero...and hn2 is the one that works...", "body": "Here's a straightforward one-layer bidirectional LSTM for sentiment classification -- but it only trains with `self.l_out(hn2)` while the loss stays above 0.6 with `self.l_out(hn1)` (yet the `assert` is never violated). The same is true if I used fixed embeddings with `.detach()` or pretrained GloVe embeddings.\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch import optim\r\nfrom torch.autograd import Variable\r\n\r\nfrom torchtext import data\r\nfrom torchtext import datasets\r\n\r\nbatch_size = 64\r\n\r\nTEXT = data.Field()\r\nLABEL = data.Field(sequential=False)\r\n\r\ntrain, valid, test = datasets.SST.splits(\r\n    TEXT, LABEL, fine_grained=False, train_subtrees=True,\r\n    filter_pred=lambda ex: ex.label != 'neutral')\r\n\r\nTEXT.build_vocab(train)\r\nLABEL.build_vocab(train)\r\n\r\ntrain_iter = data.BucketIterator(train, batch_size=batch_size, device=0)\r\n\r\nnum_classes = len(LABEL.vocab)\r\nnum_input_symbols = len(TEXT.vocab)\r\nrnn_size = 100\r\n\r\nclass lstm_bi(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.embedding = nn.Embedding(num_input_symbols, rnn_size)\r\n        self.rnn = nn.LSTM(rnn_size, rnn_size, 1, bidirectional=True)\r\n        self.l_out = nn.Linear(rnn_size * 2, num_classes)\r\n\r\n    def forward(self, x):\r\n        embeds = self.embedding(x)\r\n        _batch_size = embeds.size(1)\r\n        h0 = Variable(torch.zeros(1 * 2, _batch_size, rnn_size).cuda())\r\n        c0 = Variable(torch.zeros(1 * 2, _batch_size, rnn_size).cuda())\r\n        _, (hn, _) = self.rnn(embeds, (h0, c0))\r\n        hn1 = hn.transpose(0, 1).contiguous().view(_batch_size, -1)\r\n        hn2 = torch.cat([hn[0], hn[1]], 1)\r\n        diff = (hn1 - hn2).norm().data[0]\r\n        assert diff == 0\r\n        return self.l_out(hn2)\r\n\r\nnet = lstm_bi()\r\nnet.cuda()\r\ncriterion = nn.CrossEntropyLoss()\r\noptimizer = optim.Adam(net.parameters())\r\n\r\nfor batch in train_iter:\r\n    outputs = net(batch.text)\r\n    cost = criterion(outputs, batch.label)\r\n    print(cost.data[0])\r\n    optimizer.zero_grad()\r\n    cost.backward()\r\n    optimizer.step()\r\n```\r\nIf I use just the formulas for `hn1` and `hn2` in isolation, `.backward()` gives the same results for `hn.grad` for each; the only gradient difference I've found is that if I iterate through the parameters after `cost.backward()`:\r\n```python\r\nfor i, param in enumerate(net.parameters()):\r\n   print(i, param.grad.data.norm())\r\n```\r\nsome of the LSTM gradients are zero when using `hn2`. But none of them should be zero...and `hn2` is the one that works..."}