{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/147972100", "pull_request_review_id": 73127929, "id": 147972100, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0Nzk3MjEwMA==", "diff_hunk": "@@ -381,90 +378,104 @@ static void _transplant_var(VariableImpl& var, const std::shared_ptr<Function>&\n // do in this case.  After this method is run, t2var is extended with\n // mappings for output tensors as well.\n static void _wrap_outputs(THPFunction *self, t2var_type &t2var,\n-    std::unordered_set<PyObject *> &dirty_inputs, PyObject *raw_output,\n-    PyObject *outputs, bool is_volatile)\n+    std::unordered_set<PyObject *> &dirty_inputs,\n+    const t2var_type &shared_pairs,\n+    PyObject *raw_output, PyObject *outputs, bool is_volatile)\n {\n   auto cdata = is_volatile ? nullptr : THPFunction_asFunction(self);\n   Py_ssize_t num_outputs = PyTuple_GET_SIZE(raw_output);\n   if (self->cdata.is_executable) {\n     self->output_info = new std::vector<output_info_type>();\n     self->output_info->reserve(num_outputs);\n   }\n+\n+  // Given an output tensor, find the input Variable with which it shares storage\n+  auto get_shared_base = [&shared_pairs, &t2var](PyObject* tensor) -> Variable {\n+    auto input_it = t2var.find(tensor);\n+    if (input_it != t2var.end()) {\n+      // If the output is an input, it's a view on itself\n+      return input_it->second->cdata;\n+    }\n+    auto it = shared_pairs.find(tensor);\n+    if (it != shared_pairs.end()) {\n+      // It's explicitly marked as shared via mark_shared_storage\n+      return it->second->cdata;\n+    }\n+    return Variable();\n+  };\n+\n+  // Creates a new Variable from a PyObject* of type THPTensor\n+  auto new_variable = [&get_shared_base, is_volatile, cdata](PyObject* output) -> Variable {\n+    Variable var;\n+    auto base = get_shared_base(output);\n+    if (base.defined()) {\n+      var = make_variable_view(std::move(base), torch::createTensor(output));\n+    } else {\n+      var = make_variable(torch::createTensor(output));\n+    }\n+    var.is_volatile() = is_volatile;\n+    if (!is_volatile) {\n+      var.grad_fn() = cdata;\n+      var.requires_grad() = cdata->is_executable;\n+    }\n+    return var;\n+  };\n+\n   for (int i = 0; i < num_outputs; i++) {\n     PyObject *output = PyTuple_GET_ITEM(raw_output, i);\n-    THPVariable *output_var;\n     auto it = t2var.find(output);\n-    if (it == t2var.end()) {\n-      // A completely new tensor - just wrap it and continue\n-      if (is_volatile) {\n-        output_var = (THPVariable*)THPVariable_NewVolatile(output);\n-      } else {\n-        output_var = (THPVariable*)THPVariable_NewWithFunction(output, cdata);\n-      }\n-    } else {\n+    bool is_leaf = false;\n+\n+    // If the output is an input tensor, find the associated Variable\n+    Variable var;\n+    if (it != t2var.end()) {\n+      var = it->second->cdata;\n+      is_leaf = var.is_leaf();\n+    }\n+\n+    if (var.defined()) {\n       // If one of the outputs was also an input tensor it's a bit more complicated.\n-      THPVariable *input_var = it->second;\n-      auto& input_var_ = input_var->cdata;\n-      if (input_var_.grad_fn()) {\n-        Py_INCREF(input_var);\n-        output_var = input_var;\n-        // If it's not a leaf we want to move it in the graph so backprop\n-        // will be computed correctly, but only if it was modified. Otherwise\n-        // it's better to minimize the number of operations that mutate the graph.\n-        // grad_fn <- variable <- self  ==>  grad_fn <- self <- variable\n-        if (dirty_inputs.count(output) > 0) {\n-          _transplant_var(*input_var_.get(), cdata, i, is_volatile);\n-        }\n-      } else {\n-        // If the leaf Variable has been returned, we have to move it after the\n-        // current function to ensure the gradient is computed correctly.\n-        // There are two cases now:\n-        // 1. It has been modified in-place. If it didn't require_grad it's ok,\n-        // but if it does, then it's a clear error.\n-        // 2. It hasn't been modified. This means that it must have been\n-        // returned unchanged, and we can simply return a new Variable\n-        // referencing the same storage.\n-        if (dirty_inputs.count(output) > 0) {\n-          if (!input_var_.requires_grad()) {\n-            Py_INCREF(input_var);\n-            output_var = input_var;\n-            _transplant_var(*input_var_.get(), cdata, i, is_volatile);\n-          } else { // input_var_.requires_grad\n-            throw std::runtime_error(\"a leaf Variable that requires grad has been used in an in-place operation.\");\n-          }\n-        } else {\n-          // An input has been returned, but it wasn't modified. It's better\n-          // not to move the Variable, because there are some legitimate cases\n-          // where making it non-leaf would break stuff (e.g. broadcast). Also,\n-          // returning the input Variable is not a good option either,\n-          // because if someone registers hooks on it, they will fire with grads\n-          // from all usages, not only from usages of this output. This is why\n-          // we'll return a copy and join their version counters. This has\n-          // a side-effect of making in-place ops on any of these Variables an\n-          // immediate error, but it would be raised anyway once someone\n-          // calls backward.\n-          if (is_volatile) {\n-            output_var = (THPVariable*)THPVariable_NewVolatile(output);\n-          } else {\n-            output_var = (THPVariable*)THPVariable_NewWithFunction(output, cdata);\n-          }\n-          if (!output_var) throw python_error();\n-          output_var->cdata.version_counter() = input_var->cdata.version_counter();\n+      if (dirty_inputs.count(output) > 0) {\n+        if (is_leaf && var.requires_grad()) {\n+          throw std::runtime_error(\"a leaf Variable that requires grad has been used in an in-place operation.\");\n         }\n+        // If the input was modified, transplant the grad_fn in the graph:\n+        // grad_fn <- variable <- self  ==>  grad_fn <- self <- variable\n+        _transplant_var(var, cdata, i, is_volatile);\n+      } else if (is_leaf) {\n+        // An input has been returned, but it wasn't modified. It's better\n+        // not to move the Variable, because there are some legitimate cases\n+        // where making it non-leaf would break stuff (e.g. broadcast). Also,\n+        // returning the input Variable is not a good option either,\n+        // because if someone registers hooks on it, they will fire with grads\n+        // from all usages, not only from usages of this output. This is why\n+        // we return a view on the input Variable.\n+        var = new_variable(output);\n       }\n     }\n+\n+    // Wrap new tensors in Variables\n+    if (!var.defined()) {", "path": "torch/csrc/autograd/python_function.cpp", "position": null, "original_position": 172, "commit_id": "0faf2a8e86a5f3cc32aaf8640120aca134c10961", "original_commit_id": "924908320c84b00c09fdf3d94ac0291e8a513331", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "this could be an `else` of the `if` above", "created_at": "2017-10-31T12:05:53Z", "updated_at": "2018-11-23T15:35:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/3384#discussion_r147972100", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3384", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/147972100"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3384#discussion_r147972100"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3384"}}, "body_html": "<p>this could be an <code>else</code> of the <code>if</code> above</p>", "body_text": "this could be an else of the if above"}