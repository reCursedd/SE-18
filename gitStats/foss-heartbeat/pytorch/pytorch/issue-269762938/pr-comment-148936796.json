{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148936796", "pull_request_review_id": 74261011, "id": 148936796, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0ODkzNjc5Ng==", "diff_hunk": "@@ -382,87 +356,102 @@ static void _transplant_var(VariableImpl& var, const std::shared_ptr<Function>&\n // do in this case.  After this method is run, t2var is extended with\n // mappings for output tensors as well.\n static void _wrap_outputs(THPFunction *self, t2var_type &t2var,\n-    std::unordered_set<PyObject *> &dirty_inputs, PyObject *raw_output,\n-    PyObject *outputs, bool is_volatile)\n+    std::unordered_set<PyObject *> &dirty_inputs,\n+    const t2var_type &shared_pairs,\n+    PyObject *raw_output, PyObject *outputs, bool is_volatile)\n {\n-  auto cdata = is_volatile ? nullptr : THPFunction_asFunction(self);\n+  bool is_executable = self->cdata.is_executable;\n+  TORCH_ASSERT(!is_volatile || !is_executable);\n+  auto cdata = is_executable ? THPFunction_asFunction(self) : nullptr;\n   Py_ssize_t num_outputs = PyTuple_GET_SIZE(raw_output);\n   if (self->cdata.is_executable) {\n     self->output_info.clear();\n     self->output_info.reserve(num_outputs);\n   }\n+\n+  // Given an output tensor, find the input Variable with which it shares storage\n+  auto get_shared_base = [&](PyObject* tensor) -> Variable {\n+    auto input_it = t2var.find(tensor);\n+    if (input_it != t2var.end()) {\n+      // If the output is an input treat that as the base\n+      return input_it->second->cdata;\n+    }\n+    auto it = shared_pairs.find(tensor);\n+    if (it != shared_pairs.end()) {\n+      // It's explicitly marked as shared via mark_shared_storage\n+      return it->second->cdata;\n+    }\n+    return Variable();\n+  };\n+\n+  // Wraps an output Tensor in a Variable or returns the previous wrapper in\n+  // the case of in-place modification.\n+  auto wrap_output = [&](at::Tensor data, Variable prev, bool is_modified) -> Variable {\n+    if (!prev.defined()) {\n+      return make_variable(std::move(data));\n+    }\n+    if (is_modified) {\n+      if (prev.is_leaf() && prev.requires_grad()) {\n+        throw std::runtime_error(\"a leaf Variable that requires grad has been used in an in-place operation.\");\n+      }\n+      // If the input was modified, transplant the grad_fn in the graph:\n+      // grad_fn <- variable <- self  ==>  grad_fn <- self <- variable\n+      prev.get()->grad.reset();\n+      prev.get()->hooks.clear();\n+      if (auto grad_acc_fn = prev.get()->grad_accumulator.lock()) {\n+        auto grad_acc = dynamic_cast<AccumulateGrad*>(grad_acc_fn.get());\n+        grad_acc->variable.reset();\n+      }\n+      return prev;\n+    }\n+    // An input has been returned, but it wasn't modified. Return it as a view\n+    // so that we can attach a new grad_fn to the Variable.\n+    return make_variable_view(std::move(prev), std::move(data));\n+  };\n+\n+  t2var_type output2var;\n   for (int i = 0; i < num_outputs; i++) {\n     PyObject *output = PyTuple_GET_ITEM(raw_output, i);\n-    THPVariable *output_var;\n-    auto it = t2var.find(output);\n-    if (it == t2var.end()) {\n-      // A completely new tensor - just wrap it and continue\n-      if (is_volatile) {\n-        output_var = (THPVariable*)THPVariable_NewVolatile(output);\n-      } else {\n-        output_var = (THPVariable*)THPVariable_NewWithFunction(output, cdata);\n-      }\n+\n+    THPVariable* output_var;\n+    auto it = output2var.find(output);\n+    if (it != output2var.end()) {\n+      output_var = it->second;\n+      Py_INCREF(output_var);\n     } else {\n-      // If one of the outputs was also an input tensor it's a bit more complicated.\n-      THPVariable *input_var = it->second;\n-      auto& input_var_ = input_var->cdata;\n-      if (input_var_.grad_fn()) {\n-        Py_INCREF(input_var);\n-        output_var = input_var;\n-        // If it's not a leaf we want to move it in the graph so backprop\n-        // will be computed correctly, but only if it was modified. Otherwise\n-        // it's better to minimize the number of operations that mutate the graph.\n-        // grad_fn <- variable <- self  ==>  grad_fn <- self <- variable\n-        if (dirty_inputs.count(output) > 0) {\n-          _transplant_var(*input_var_.get(), cdata, i, is_volatile);\n-        }\n+      // Wrap the output in a Variable\n+      bool is_modified = dirty_inputs.count(output) > 0;\n+      Variable var = wrap_output(\n+          torch::createTensor(output),\n+          get_shared_base(output),\n+          is_modified);\n+      if (is_modified) {", "path": "torch/csrc/autograd/python_function.cpp", "position": null, "original_position": 138, "commit_id": "0faf2a8e86a5f3cc32aaf8640120aca134c10961", "original_commit_id": "790748e37d34eaf7b269cba610f3296db403ed8d", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Correctness of these lines depends on the code of `wrap_output` which is a bit far right now. Can you add a comment saying that it's safe to just call `set_history`, because the only case where we return the original variable is when it was an input and modified, so it will always enter the first case?", "created_at": "2017-11-04T17:45:19Z", "updated_at": "2018-11-23T15:36:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/3384#discussion_r148936796", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3384", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148936796"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3384#discussion_r148936796"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3384"}}, "body_html": "<p>Correctness of these lines depends on the code of <code>wrap_output</code> which is a bit far right now. Can you add a comment saying that it's safe to just call <code>set_history</code>, because the only case where we return the original variable is when it was an input and modified, so it will always enter the first case?</p>", "body_text": "Correctness of these lines depends on the code of wrap_output which is a bit far right now. Can you add a comment saying that it's safe to just call set_history, because the only case where we return the original variable is when it was an input and modified, so it will always enter the first case?"}