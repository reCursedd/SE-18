{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148828970", "pull_request_review_id": 74134948, "id": 148828970, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0ODgyODk3MA==", "diff_hunk": "@@ -159,12 +159,18 @@ auto CopySlices::apply(const variable_list& inputs) -> variable_list {\n   // it for the backward of res. We might be able to avoid the clone() if\n   // grad_slice is volatile.\n   auto res = (*fn)({ grad_slice.clone() });\n-  grad_slice.copy_(res[0]);\n \n   variable_list grad_inputs(next_functions.size());\n-  grad_inputs[0] = result;\n-  for (size_t i = 1; i < res.size(); i++) {\n-    grad_inputs[i] = std::move(res[i]);\n+  for (size_t i = 0; i < res.size(); i++) {\n+    if (should_compute_output(i)) {\n+      TORCH_ASSERT(res[i].defined());", "path": "torch/csrc/autograd/functions/tensor.cpp", "position": null, "original_position": 12, "commit_id": "0faf2a8e86a5f3cc32aaf8640120aca134c10961", "original_commit_id": "6d8314118a919969431defa5e73eb4624835d6d8", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think this assertion is too strict, mostly because `should_compute_output` is really an approximation, that might sometimes have false positives. Right now it's really a check of `is_executable` of `i`th next function. However, it is possible that a function is executable, but its `i`th output was non-differentiable (e.g. it was a max index), and it was used as a target for the loss. This loss will check that its input didn't require grad at runtime, and its backward won't return grad for it.", "created_at": "2017-11-03T16:17:51Z", "updated_at": "2018-11-23T15:35:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/3384#discussion_r148828970", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3384", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148828970"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3384#discussion_r148828970"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3384"}}, "body_html": "<p>I think this assertion is too strict, mostly because <code>should_compute_output</code> is really an approximation, that might sometimes have false positives. Right now it's really a check of <code>is_executable</code> of <code>i</code>th next function. However, it is possible that a function is executable, but its <code>i</code>th output was non-differentiable (e.g. it was a max index), and it was used as a target for the loss. This loss will check that its input didn't require grad at runtime, and its backward won't return grad for it.</p>", "body_text": "I think this assertion is too strict, mostly because should_compute_output is really an approximation, that might sometimes have false positives. Right now it's really a check of is_executable of ith next function. However, it is possible that a function is executable, but its ith output was non-differentiable (e.g. it was a max index), and it was used as a target for the loss. This loss will check that its input didn't require grad at runtime, and its backward won't return grad for it."}