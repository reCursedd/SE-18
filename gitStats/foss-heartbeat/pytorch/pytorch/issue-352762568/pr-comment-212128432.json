{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212128432", "pull_request_review_id": 148695045, "id": 212128432, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjEyODQzMg==", "diff_hunk": "@@ -40,62 +42,83 @@ class RNNImplBase : public torch::nn::Cloneable<Derived> {\n   // https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNMode_t\n   enum class CuDNNMode { RNN_RELU = 0, RNN_TANH = 1, LSTM = 2, GRU = 3 };\n \n-  RNNImplBase(\n+  explicit RNNImplBase(\n       RNNOptionsBase options_,\n       at::optional<CuDNNMode> cudnn_mode = at::nullopt,\n-      int64_t number_of_gates = 1,\n-      bool has_cell_state = false);\n-\n-  RNNOutput forward(Tensor input, Tensor state = {});\n+      int64_t number_of_gates = 1);\n \n+  /// Initializes the parameters of the RNN module.\n   void reset() override;\n \n-  /// Recursively casts all parameters to the given device and dtype.\n+  /// Overrides `nn::Module::to()` to call `flatten_parameters()` after the\n+  /// original operation.\n   void to(torch::Device device, torch::Dtype dtype, bool non_blocking = false)\n       override;\n-\n-  /// Recursively casts all parameters to the given dtype.\n   void to(torch::Dtype dtype, bool non_blocking = false) override;\n-\n-  /// Recursively moves all parameters to the given device.\n   void to(torch::Device device, bool non_blocking = false) override;\n \n-  /// Fills the internal flattened parameter buffers passed to cuDNN. Call this\n-  /// method if you mess around with the variable storages and want to use\n-  /// cuDNN.\n-  void flatten_parameters_for_cudnn();\n+  /// Modifies the internal storage of weights for optimization purposes.\n+  ///\n+  /// On CPU, this method should be called if any of the weight or bias vectors\n+  /// are changed. On GPU, it should be called __any time the storage of any\n+  /// parameter is modified__, e.g. any time a parameter is assigned a new\n+  /// value. This allows using the fast path in cuDNN implementations of\n+  /// respective RNN `forward()` methods. It is called once upon construction,\n+  /// inside `reset()`.\n+  void flatten_parameters();\n+\n+  /// Returns the cuDNN mode of the RNN subclass if it has one. See\n+  /// https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNMode_t\n+  /// for a list of all cuDNN modes.\n+  at::optional<CuDNNMode> cudnn_mode() const noexcept;\n \n   RNNOptionsBase options;\n \n+  /// The weights for `input x hidden` gates.\n   std::vector<Tensor> w_ih;\n+  /// The weights for `hidden x hidden` gates.\n   std::vector<Tensor> w_hh;\n+  /// The biases for `input x hidden` gates.\n   std::vector<Tensor> b_ih;\n+  /// The biases for `hidden x hidden` gates.\n   std::vector<Tensor> b_hh;\n \n-  Dropout dropout;\n+  /// The dropout module, if dropout is used.\n+  Dropout dropout{nullptr};\n \n  protected:\n-  virtual Tensor cell_forward(Tensor input, Tensor state, int64_t layer) = 0;\n-\n-  RNNOutput CUDNN_forward(Tensor input, Tensor state);\n-  RNNOutput autograd_forward(Tensor input, Tensor state);\n-\n+  /// The function signature of `at::lstm`, `at::rnn_relu`, `at::gru` etc.\n+  using RNNFunctionSignature = std::tuple<Tensor, Tensor>(\n+      /*input=*/const Tensor&,\n+      /*state=*/const Tensor&,\n+      /*params=*/TensorList,\n+      /*has_biases=*/bool,\n+      /*layers=*/int64_t,\n+      /*dropout=*/double,\n+      /*train=*/bool,\n+      /*bidirectional=*/bool,\n+      /*batch_first=*/bool);\n+\n+  RNNOutput generic_forward(\n+      std::function<RNNFunctionSignature> function,\n+      Tensor input,\n+      Tensor state);\n+\n+  /// Returns a flat vector of all weights, with layer weights following each\n+  /// other sequentially in (w_ih, w_hh, b_ih, b_hh) order.\n   std::vector<Tensor> flat_weights() const;\n-  bool use_cudnn(Tensor sample) const;\n-  Tensor create_dropout_state(Tensor input) const;\n \n+  /// Returns true if any of the parameters (weights, biases) alias each other.\n+  bool any_parameters_alias() const;\n+\n+  /// The number of gate weights/biases required by the RNN subclass.\n   int64_t number_of_gates_;\n-  bool has_cell_state_;\n+\n+  /// The cuDNN RNN mode, if this RNN subclass has any.\n   at::optional<CuDNNMode> cudnn_mode_;\n \n-  // This is copied from pytorch, to determine whether weights are flat for the\n-  // fast CUDNN route. Otherwise, we have to use non flattened weights, which\n-  // are much slower.\n-  // https://github.com/pytorch/pytorch/blob/1848cad10802db9fa0aa066d9de195958120d863/torch/nn/modules/rnn.py#L159-L165\n-  // TODO Actually since we are in C++ we can probably just actually check if\n-  // the parameters are flat, instead of relying on data pointers and stuff.\n-  std::vector<void*> data_ptrs_;\n-  Tensor flat_weights_;\n+  /// The cached result of the latest `flat_weights()` call.\n+  std::vector<Tensor> flat_weights_;", "path": "torch/csrc/api/include/torch/nn/modules/rnn.h", "position": 117, "original_position": 120, "commit_id": "1474faaf96f2466af81ee8bf5f4d9ab8a2eb095b", "original_commit_id": "f72a058c5a1d39e53f806688ab480701cb9d3b00", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This is unnecessary now. The calls to the RNN implementations will automatically figure out if the weights you passed in form a contiguous chunk in memory that's consistent with cuDNN layout.", "created_at": "2018-08-22T22:19:08Z", "updated_at": "2018-11-23T15:49:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/10761#discussion_r212128432", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10761", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212128432"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10761#discussion_r212128432"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10761"}}, "body_html": "<p>This is unnecessary now. The calls to the RNN implementations will automatically figure out if the weights you passed in form a contiguous chunk in memory that's consistent with cuDNN layout.</p>", "body_text": "This is unnecessary now. The calls to the RNN implementations will automatically figure out if the weights you passed in form a contiguous chunk in memory that's consistent with cuDNN layout."}