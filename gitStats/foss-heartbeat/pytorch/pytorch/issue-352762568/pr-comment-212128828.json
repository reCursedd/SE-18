{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212128828", "pull_request_review_id": 148695045, "id": 212128828, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjEyODgyOA==", "diff_hunk": "@@ -89,209 +84,116 @@ void RNNImplBase<Derived>::reset() {\n     }\n   }\n \n-  const auto stdv = 1.0 / std::sqrt(options.hidden_size_);\n-  NoGradGuard no_grad;;\n-  for (auto& p : this->parameters()) {\n-    p->uniform_(-stdv, stdv);\n+  {\n+    NoGradGuard no_grad;\n+    const auto stdv = 1.0 / std::sqrt(options.hidden_size_);\n+    for (auto& p : this->parameters()) {\n+      p->uniform_(-stdv, stdv);\n+    }\n   }\n-}\n \n-template <typename Derived>\n-RNNOutput RNNImplBase<Derived>::forward(Tensor input, Tensor state) {\n-  if (use_cudnn(/*sample=*/input)) {\n-    return CUDNN_forward(input, state);\n-  } else {\n-    return autograd_forward(input, state);\n-  }\n+  flatten_parameters();\n }\n \n template <typename Derived>\n-std::vector<Tensor> RNNImplBase<Derived>::flat_weights() const {\n-  std::vector<Tensor> flat;\n-  for (int64_t layer = 0; layer < options.layers_; layer++) {\n-    flat.push_back(w_ih[layer]);\n-    flat.push_back(w_hh[layer]);\n-    if (options.with_bias_) {\n-      flat.push_back(b_ih[layer]);\n-      flat.push_back(b_hh[layer]);\n-    }\n-  }\n-  return flat;\n+void RNNImplBase<Derived>::to(\n+    torch::Device device,\n+    torch::Dtype dtype,\n+    bool non_blocking) {\n+  nn::Module::to(device, dtype, non_blocking);\n+  flatten_parameters();\n }\n \n template <typename Derived>\n-bool RNNImplBase<Derived>::use_cudnn(Tensor sample) const {\n-  return cudnn_mode_.has_value() && sample.is_cuda() &&\n-      torch::cudnn_is_acceptable(sample);\n+void RNNImplBase<Derived>::to(torch::Dtype dtype, bool non_blocking) {\n+  nn::Module::to(dtype, non_blocking);\n+  flatten_parameters();\n }\n \n template <typename Derived>\n-Tensor RNNImplBase<Derived>::create_dropout_state(Tensor input) const {\n-  static const int64_t dropout_seed =\n-      torch::empty({}, torch::kInt64).random_().toCLong();\n-  if (options.dropout_ > 0) {\n-    torch::DeviceGuard guard(input.device());\n-    return torch::_cudnn_init_dropout_state(\n-        input.type().toScalarType(torch::kUInt8),\n-        options.dropout_,\n-        this->is_training(),\n-        dropout_seed);\n-  }\n-  return torch::empty({}, input.options());\n+void RNNImplBase<Derived>::to(torch::Device device, bool non_blocking) {\n+  nn::Module::to(device, non_blocking);\n+  flatten_parameters();\n }\n \n template <typename Derived>\n-RNNOutput RNNImplBase<Derived>::autograd_forward(Tensor input, Tensor state) {\n-  std::vector<Tensor> new_state;\n-  auto has_hidden = state.defined();\n-  auto layer_dimension = has_hidden ? state.ndimension() - 3 : -1;\n-  for (int64_t layer = 0; layer < options.layers_; layer++) {\n-    new_state.push_back(\n-        has_hidden ? state.select(layer_dimension, layer) : Tensor());\n-  }\n+void RNNImplBase<Derived>::flatten_parameters() {\n+  // Cache the flattened weight and bias vector.\n+  flat_weights_ = flat_weights();\n \n-  auto output = torch::zeros(\n-      {input.size(0), input.size(1), options.hidden_size_}, input.options());\n-  for (int64_t t = 0; t < input.size(0); t++) {\n-    auto x = input.select(0, t);\n-    for (int64_t i = 0; i < options.layers_; i++) {\n-      // cell_forward() returns a stacked tensor of one or more cell states.\n-      auto layer_output = cell_forward(x, new_state[i], i);\n-      // If there are multiple cell states, keep all. If there is only one,\n-      // the first dimension will be 1, so `.squeeze(0)` will unpack it.\n-      new_state[i] = layer_output.squeeze(0);\n-      // x should always be the hidden cell state h, assumed to be the zero-th.\n-      x = layer_output[0];\n-      output.select(0, t).copy_(x);\n-      if (options.dropout_ > 0 && i != options.layers_ - 1) {\n-        x = dropout->forward(x);\n-      }\n-    }\n+  if (!cudnn_mode_ || !torch::cudnn_is_acceptable(/*sample=*/w_ih.at(0)) ||\n+      any_parameters_alias()) {\n+    return;\n   }\n \n-  auto state_output = torch::stack(new_state);\n-  if (has_cell_state_) {\n-    state_output.transpose_(0, 1);\n-  }\n-  return {output, state_output};\n+  NoGradGuard no_grad;\n+  torch::_cudnn_rnn_flatten_weight(\n+      flat_weights_,\n+      /*weight_stride=*/options.with_bias_ ? 4 : 2,\n+      options.input_size_,\n+      static_cast<int64_t>(*cudnn_mode_),\n+      options.hidden_size_,\n+      options.layers_,\n+      /*batch_first=*/options.batch_first_,\n+      /*bidirectional=*/options.bidirectional_);\n }\n \n template <typename Derived>\n-void RNNImplBase<Derived>::flatten_parameters_for_cudnn() {\n-  data_ptrs_.clear();\n-  const auto any_parameter = w_ih.at(0);\n-  if (!use_cudnn(/*sample=*/w_ih.at(0))) {\n-    return;\n-  }\n-  std::unordered_set<void*> unique_data_ptrs;\n-  auto params = this->parameters();\n-  for (auto& p : params) {\n-    unique_data_ptrs.insert(p->data_ptr());\n-  }\n-  // TODO PyTorch says: If any parameters alias, we fall back to the slower,\n-  // copying code path. This is a sufficient check, because overlapping\n-  // parameter buffers that don't completely alias would break the assumptions\n-  // of the uniqueness check in Module.named_parameters(). But I'm not sure if\n-  // this is the case for us\n-  if (unique_data_ptrs.size() != params.size()) {\n-    return;\n-  }\n-\n-  {\n-    NoGradGuard no_grad;;\n-    flat_weights_ = torch::_cudnn_rnn_flatten_weight(\n-        flat_weights(),\n-        /*weight_stride=*/options.with_bias_ ? 4 : 2,\n-        options.input_size_,\n-        static_cast<int64_t>(*cudnn_mode_),\n-        options.hidden_size_,\n-        options.layers_,\n-        /*batch_first=*/false,\n-        /*bidirectional=*/false);\n-  }\n-  for (auto& p : params) {\n-    data_ptrs_.emplace_back(p->data_ptr());\n-  }\n+at::optional<typename RNNImplBase<Derived>::CuDNNMode> RNNImplBase<\n+    Derived>::cudnn_mode() const noexcept {\n+  return cudnn_mode_;\n }\n \n template <typename Derived>\n-RNNOutput RNNImplBase<Derived>::CUDNN_forward(Tensor input, Tensor state) {\n-  Tensor hx, cx;\n-  if (state.defined()) {\n-    if (has_cell_state_) {\n-      hx = state[0];\n-      cx = state[1];\n-    } else {\n-      hx = state;\n-    }\n-  } else {\n-    hx = torch::zeros(\n-        {options.layers_, input.size(1), options.hidden_size_},\n-        input.options());\n-    if (has_cell_state_) {\n-      cx = torch::zeros(\n-          {options.layers_, input.size(1), options.hidden_size_},\n-          input.options());\n-    }\n-  }\n-  std::vector<void*> weight_data_ptrs;\n-  for (auto& p : this->parameters()) {\n-    weight_data_ptrs.emplace_back(p->data_ptr());\n-  }\n-\n-  AT_CHECK(\n-      weight_data_ptrs == data_ptrs_,\n-      \"Parameters are unflattened! Code path might be super slow. \"\n-      \"Please call flatten_parameters_for_cudnn() when you muck \"\n-      \"around with storages!\")\n-\n-  // cudnn_output = std::tuple<output, hy, cy, reserve, new_weight_buf>\n-  auto cudnn_output = torch::_cudnn_rnn(\n-      /*input=*/input,\n-      /*weight=*/flat_weights(),\n-      /*weight_stride0=*/options.with_bias_ ? 4 : 2,\n-      /*weight_buf=*/flat_weights_,\n-      /*hx=*/hx,\n-      /*cx=*/cx,\n-      /*mode=*/static_cast<int64_t>(*cudnn_mode_),\n-      /*hidden_size=*/options.hidden_size_,\n-      /*num_layers=*/options.layers_,\n-      /*batch_first=*/false,\n-      /*dropout=*/options.dropout_,\n-      /*train=*/this->is_training(),\n-      /*bidirectional=*/false,\n-      /*batch_sizes=*/{},\n-      /*dropout_state=*/create_dropout_state(input));\n-\n-  Tensor hidden_output = std::get<1>(cudnn_output);\n-  if (has_cell_state_) {\n-    auto cy = std::get<2>(cudnn_output);\n-    hidden_output = torch::stack({hidden_output, cy});\n+RNNOutput RNNImplBase<Derived>::generic_forward(\n+    std::function<RNNFunctionSignature> function,\n+    Tensor input,\n+    Tensor state) {\n+  if (!state.defined()) {\n+    // #layers, batch size, state size\n+    const auto batch_size = input.size(options.batch_first_ ? 0 : 1);\n+    state = torch::zeros(\n+        {options.layers_, batch_size, options.hidden_size_}, input.options());\n   }\n-\n-  Tensor output = std::get<0>(cudnn_output);\n-  return {output, hidden_output};\n+  Tensor output, new_state;\n+  std::tie(output, new_state) = function(\n+      std::move(input),\n+      std::move(state),\n+      flat_weights_,\n+      options.with_bias_,\n+      options.layers_,\n+      options.dropout_,\n+      this->is_training(),\n+      options.bidirectional_,\n+      options.batch_first_);\n+  return {output, new_state};\n }\n \n template <typename Derived>\n-void RNNImplBase<Derived>::to(\n-    torch::Device device,\n-    torch::Dtype dtype,\n-    bool non_blocking) {\n-  nn::Module::to(device, dtype, non_blocking);\n-  flatten_parameters_for_cudnn();\n+std::vector<Tensor> RNNImplBase<Derived>::flat_weights() const {\n+  // Organize all weights in a flat vector in the order\n+  // (w_ih, w_hh, b_ih, b_hh), repeated for each layer (next to each other).\n+  std::vector<Tensor> flat;\n+  for (int64_t layer = 0; layer < options.layers_; layer++) {\n+    flat.push_back(w_ih[layer]);\n+    flat.push_back(w_hh[layer]);\n+    if (options.with_bias_) {\n+      flat.push_back(b_ih[layer]);\n+      flat.push_back(b_hh[layer]);\n+    }\n+  }\n+  return flat;\n }\n \n template <typename Derived>\n-void RNNImplBase<Derived>::to(torch::Dtype dtype, bool non_blocking) {\n-  nn::Module::to(dtype, non_blocking);\n-  flatten_parameters_for_cudnn();\n-}\n+bool RNNImplBase<Derived>::any_parameters_alias() const {\n+  std::unordered_set<void*> unique_data_ptrs;\n+  const auto params = this->parameters();\n+  params.map(\n+      std::inserter(unique_data_ptrs, unique_data_ptrs.end()),\n+      [](Tensor p) { return p.data_ptr(); });\n \n-template <typename Derived>\n-void RNNImplBase<Derived>::to(torch::Device device, bool non_blocking) {\n-  nn::Module::to(device, non_blocking);\n-  flatten_parameters_for_cudnn();\n+  return unique_data_ptrs.size() != params.size();", "path": "torch/csrc/api/src/nn/modules/rnn.cpp", "position": 330, "original_position": 309, "commit_id": "1474faaf96f2466af81ee8bf5f4d9ab8a2eb095b", "original_commit_id": "f72a058c5a1d39e53f806688ab480701cb9d3b00", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Yeah that's not really a robust check for aliasing.", "created_at": "2018-08-22T22:20:39Z", "updated_at": "2018-11-23T15:49:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/10761#discussion_r212128828", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10761", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212128828"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10761#discussion_r212128828"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10761"}}, "body_html": "<p>Yeah that's not really a robust check for aliasing.</p>", "body_text": "Yeah that's not really a robust check for aliasing."}