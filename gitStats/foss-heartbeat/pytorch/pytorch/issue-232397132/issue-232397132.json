{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1686", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1686/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1686/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1686/events", "html_url": "https://github.com/pytorch/pytorch/issues/1686", "id": 232397132, "node_id": "MDU6SXNzdWUyMzIzOTcxMzI=", "number": 1686, "title": "load_state_dict should take filenames", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-05-30T22:11:12Z", "updated_at": "2017-06-06T00:48:00Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>In high memory pressure situations, the following is a common occurrence:</p>\n<ol>\n<li>create model</li>\n<li>read state_dict from checkpoint file (loads on GPU)</li>\n<li>model.load_state_dict(s)</li>\n</ol>\n<p>Because of memory pressure, a common workaround is to first do:</p>\n<pre><code>s = torch.load('my_file.pt', map_location=lambda storage, loc: storage)\n</code></pre>\n<p>And then load <code>s</code> into <code>model</code>.</p>\n<p>This is a very common scenario that we should be able to avoid, and this scenario might have some pitfalls: what happens on part-GPU part-CPU models, what happens on multi-GPU models...</p>\n<p>if load_state_dict took a filename directly, it can delete it's existing parameter storages and set them to the new one on the fly, thereby requiring no extra memory.</p>", "body_text": "In high memory pressure situations, the following is a common occurrence:\n\ncreate model\nread state_dict from checkpoint file (loads on GPU)\nmodel.load_state_dict(s)\n\nBecause of memory pressure, a common workaround is to first do:\ns = torch.load('my_file.pt', map_location=lambda storage, loc: storage)\n\nAnd then load s into model.\nThis is a very common scenario that we should be able to avoid, and this scenario might have some pitfalls: what happens on part-GPU part-CPU models, what happens on multi-GPU models...\nif load_state_dict took a filename directly, it can delete it's existing parameter storages and set them to the new one on the fly, thereby requiring no extra memory.", "body": "In high memory pressure situations, the following is a common occurrence:\r\n\r\n1. create model\r\n2. read state_dict from checkpoint file (loads on GPU)\r\n3. model.load_state_dict(s)\r\n\r\nBecause of memory pressure, a common workaround is to first do:\r\n\r\n```\r\ns = torch.load('my_file.pt', map_location=lambda storage, loc: storage)\r\n```\r\n\r\nAnd then load `s` into `model`.\r\n\r\nThis is a very common scenario that we should be able to avoid, and this scenario might have some pitfalls: what happens on part-GPU part-CPU models, what happens on multi-GPU models...\r\n\r\nif load_state_dict took a filename directly, it can delete it's existing parameter storages and set them to the new one on the fly, thereby requiring no extra memory.\r\n\r\n"}