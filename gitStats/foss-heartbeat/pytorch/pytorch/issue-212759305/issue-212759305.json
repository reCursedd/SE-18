{"url": "https://api.github.com/repos/pytorch/pytorch/issues/957", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/957/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/957/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/957/events", "html_url": "https://github.com/pytorch/pytorch/issues/957", "id": 212759305, "node_id": "MDU6SXNzdWUyMTI3NTkzMDU=", "number": 957, "title": "Counter-intuitive behavior of expand and arithmetic op", "user": {"login": "MatthiasKohl", "id": 344856, "node_id": "MDQ6VXNlcjM0NDg1Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/344856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MatthiasKohl", "html_url": "https://github.com/MatthiasKohl", "followers_url": "https://api.github.com/users/MatthiasKohl/followers", "following_url": "https://api.github.com/users/MatthiasKohl/following{/other_user}", "gists_url": "https://api.github.com/users/MatthiasKohl/gists{/gist_id}", "starred_url": "https://api.github.com/users/MatthiasKohl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MatthiasKohl/subscriptions", "organizations_url": "https://api.github.com/users/MatthiasKohl/orgs", "repos_url": "https://api.github.com/users/MatthiasKohl/repos", "events_url": "https://api.github.com/users/MatthiasKohl/events{/privacy}", "received_events_url": "https://api.github.com/users/MatthiasKohl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-08T15:04:26Z", "updated_at": "2017-03-09T08:53:55Z", "closed_at": "2017-03-09T08:53:55Z", "author_association": "NONE", "body_html": "<p>I don't know if this is a bug (and it might have been reported already but I didn't find anything).<br>\nBut it could at least be added to the documentation for the <code>expand</code> operation on a tensor:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nT <span class=\"pl-k\">=</span> torch.Tensor([<span class=\"pl-c1\">2</span>]).expand(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>)\n<span class=\"pl-c1\">print</span>(T.mul_(<span class=\"pl-c1\">2</span>))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> expected a Tensor with [4 4 4] here, got [16, 16, 16]</span></pre></div>\n<p>Even if you are aware that expand doesn't copy data, I believe most will find this result quite counter-intuitive. Of course, this can be fixed atm either by replacing <code>expand</code> by <code>repeat</code> or <code>mul_</code> by <code>mul</code>, but that requires more memory. It would be awesome if the arithmetic ops like <code>mul</code> had a special case when the tensor's <code>stride</code> is 0 in some dimension</p>\n<p>Edit: My torch version string is <code>0.1.10_2</code></p>", "body_text": "I don't know if this is a bug (and it might have been reported already but I didn't find anything).\nBut it could at least be added to the documentation for the expand operation on a tensor:\nimport torch\nT = torch.Tensor([2]).expand(1,3)\nprint(T.mul_(2))  # expected a Tensor with [4 4 4] here, got [16, 16, 16]\nEven if you are aware that expand doesn't copy data, I believe most will find this result quite counter-intuitive. Of course, this can be fixed atm either by replacing expand by repeat or mul_ by mul, but that requires more memory. It would be awesome if the arithmetic ops like mul had a special case when the tensor's stride is 0 in some dimension\nEdit: My torch version string is 0.1.10_2", "body": "I don't know if this is a bug (and it might have been reported already but I didn't find anything).\r\nBut it could at least be added to the documentation for the `expand` operation on a tensor:\r\n```python\r\nimport torch\r\nT = torch.Tensor([2]).expand(1,3)\r\nprint(T.mul_(2))  # expected a Tensor with [4 4 4] here, got [16, 16, 16]\r\n```\r\nEven if you are aware that expand doesn't copy data, I believe most will find this result quite counter-intuitive. Of course, this can be fixed atm either by replacing `expand` by `repeat` or `mul_` by `mul`, but that requires more memory. It would be awesome if the arithmetic ops like `mul` had a special case when the tensor's `stride` is 0 in some dimension\r\n\r\nEdit: My torch version string is `0.1.10_2`"}