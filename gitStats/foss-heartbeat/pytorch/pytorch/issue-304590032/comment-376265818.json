{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/376265818", "html_url": "https://github.com/pytorch/pytorch/pull/5725#issuecomment-376265818", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5725", "id": 376265818, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjI2NTgxOA==", "user": {"login": "Lalaland", "id": 342233, "node_id": "MDQ6VXNlcjM0MjIzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/342233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lalaland", "html_url": "https://github.com/Lalaland", "followers_url": "https://api.github.com/users/Lalaland/followers", "following_url": "https://api.github.com/users/Lalaland/following{/other_user}", "gists_url": "https://api.github.com/users/Lalaland/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lalaland/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lalaland/subscriptions", "organizations_url": "https://api.github.com/users/Lalaland/orgs", "repos_url": "https://api.github.com/users/Lalaland/repos", "events_url": "https://api.github.com/users/Lalaland/events{/privacy}", "received_events_url": "https://api.github.com/users/Lalaland/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-26T18:29:22Z", "updated_at": "2018-03-26T18:29:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> It seems to be failing somewhat randomly. One issue here is that there is a lot of non-determinism in the order in which things get summed up. Note that the area where it's failing is not even code that I changed. It's failing in the sparse operations, which I did not touch.</p>\n<p>I think the core issue here is that static epsilon values are a bad idea. The problem is that larger floating point numbers need larger epsilons. We should have epsilon measurements relative to the magnitude of the values being compared. There are three main options here as I see them:</p>\n<ol>\n<li>Increase the epsilon for floats globally. Say to 1e-4 instead of 1e-5. This fixes things temporarily.</li>\n<li>Increase the epsilon for floats for that particular embedding bag test. This also fixes things temporarily.</li>\n<li>Properly implement an epsilon relative to the magnitude of the numbers being compared.</li>\n</ol>\n<p>Which do you want me to do?</p>", "body_text": "@ezyang It seems to be failing somewhat randomly. One issue here is that there is a lot of non-determinism in the order in which things get summed up. Note that the area where it's failing is not even code that I changed. It's failing in the sparse operations, which I did not touch.\nI think the core issue here is that static epsilon values are a bad idea. The problem is that larger floating point numbers need larger epsilons. We should have epsilon measurements relative to the magnitude of the values being compared. There are three main options here as I see them:\n\nIncrease the epsilon for floats globally. Say to 1e-4 instead of 1e-5. This fixes things temporarily.\nIncrease the epsilon for floats for that particular embedding bag test. This also fixes things temporarily.\nProperly implement an epsilon relative to the magnitude of the numbers being compared.\n\nWhich do you want me to do?", "body": "@ezyang It seems to be failing somewhat randomly. One issue here is that there is a lot of non-determinism in the order in which things get summed up. Note that the area where it's failing is not even code that I changed. It's failing in the sparse operations, which I did not touch.\r\n\r\nI think the core issue here is that static epsilon values are a bad idea. The problem is that larger floating point numbers need larger epsilons. We should have epsilon measurements relative to the magnitude of the values being compared. There are three main options here as I see them:\r\n\r\n1. Increase the epsilon for floats globally. Say to 1e-4 instead of 1e-5. This fixes things temporarily.\r\n2. Increase the epsilon for floats for that particular embedding bag test. This also fixes things temporarily. \r\n3. Properly implement an epsilon relative to the magnitude of the numbers being compared.\r\n\r\nWhich do you want me to do?"}