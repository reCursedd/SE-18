{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5041", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5041/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5041/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5041/events", "html_url": "https://github.com/pytorch/pytorch/issues/5041", "id": 294277684, "node_id": "MDU6SXNzdWUyOTQyNzc2ODQ=", "number": 5041, "title": "nn.DataParallel ignores requires_grad setting when running", "user": {"login": "jhlee525", "id": 6736676, "node_id": "MDQ6VXNlcjY3MzY2NzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/6736676?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhlee525", "html_url": "https://github.com/jhlee525", "followers_url": "https://api.github.com/users/jhlee525/followers", "following_url": "https://api.github.com/users/jhlee525/following{/other_user}", "gists_url": "https://api.github.com/users/jhlee525/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhlee525/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhlee525/subscriptions", "organizations_url": "https://api.github.com/users/jhlee525/orgs", "repos_url": "https://api.github.com/users/jhlee525/repos", "events_url": "https://api.github.com/users/jhlee525/events{/privacy}", "received_events_url": "https://api.github.com/users/jhlee525/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-02-05T05:27:19Z", "updated_at": "2018-02-06T04:38:36Z", "closed_at": "2018-02-06T04:38:36Z", "author_association": "NONE", "body_html": "<ul>\n<li>PyTorch version: 0.3.0</li>\n<li>Python version: 3.5</li>\n<li>GPU models and configuration: 8x NVIDIA Titan</li>\n</ul>\n<p>I found that requires_grad setting in module is ignored when module is wrapped with nn.DataParallel. For example,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> DataParallel\n<span class=\"pl-k\">from</span> torchvision.models.resnet <span class=\"pl-k\">import</span> resnet50\n\nmodule_ <span class=\"pl-k\">=</span> resnet50()\n<span class=\"pl-k\">for</span> name, param <span class=\"pl-k\">in</span> module_.named_parameters():\n    <span class=\"pl-k\">if</span> name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">or</span> name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn1<span class=\"pl-pds\">'</span></span>):\n        param.requires_grad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n    <span class=\"pl-k\">if</span> name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer1<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">or</span> name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer2<span class=\"pl-pds\">'</span></span>):\n        param.requires_grad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n    <span class=\"pl-k\">if</span> name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer3<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">or</span> name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer4<span class=\"pl-pds\">'</span></span>):\n        param.requires_grad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\nmodule_ <span class=\"pl-k\">=</span> DataParallel(module_).cuda()\n\nx <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1600</span>, <span class=\"pl-c1\">1600</span>)\nx <span class=\"pl-k\">=</span> torch.autograd.Variable(x)\nx <span class=\"pl-k\">=</span> module_(x)\n<span class=\"pl-c1\">print</span>(x)</pre></div>\n<p>This code should be executed because very small part of network activation (only fc layer) is stored for backward computation, but it results to run-time error:</p>\n<pre><code>RuntimeError: cuda runtime error (2) : out of memory at /tmp/pip-8pfswvat-build/torch/lib/THC/generic/THCStorage.cu:58\n</code></pre>\n<p>Internally, I found a crack in <code>replicate</code> function which is in <code>torch.nn.parallel.replicate</code>. In replicate function, it copies all parameter in module (# of replica times) with <code>Broadcast.apply</code>. In broadcasting code, it just defines new <code>torch.nn.Parameter</code> with default constructor <code>requires_grad</code> parameter, which is always set to True.</p>\n<p>I think there can be some choices to fix this issue.</p>\n<ol>\n<li>It is intended behavior for DataParallel, so we should use volatile (or torch.no_grad) to implement transfer learning in DataParallel</li>\n<li>Broadcast model should be fixed to handle requires_grad when copying parameters.</li>\n<li>There should be synchronizing <code>requires_grad</code> in <code>torch.nn.parallel.replicate</code></li>\n</ol>\n<p>I think any of solution doesn't need lots of effort to fix it.</p>", "body_text": "PyTorch version: 0.3.0\nPython version: 3.5\nGPU models and configuration: 8x NVIDIA Titan\n\nI found that requires_grad setting in module is ignored when module is wrapped with nn.DataParallel. For example,\nimport torch\nfrom torch.nn import DataParallel\nfrom torchvision.models.resnet import resnet50\n\nmodule_ = resnet50()\nfor name, param in module_.named_parameters():\n    if name.startswith('conv1') or name.startswith('bn1'):\n        param.requires_grad = False\n    if name.startswith('layer1') or name.startswith('layer2'):\n        param.requires_grad = False\n    if name.startswith('layer3') or name.startswith('layer4'):\n        param.requires_grad = False\n\nmodule_ = DataParallel(module_).cuda()\n\nx = torch.rand(32, 3, 1600, 1600)\nx = torch.autograd.Variable(x)\nx = module_(x)\nprint(x)\nThis code should be executed because very small part of network activation (only fc layer) is stored for backward computation, but it results to run-time error:\nRuntimeError: cuda runtime error (2) : out of memory at /tmp/pip-8pfswvat-build/torch/lib/THC/generic/THCStorage.cu:58\n\nInternally, I found a crack in replicate function which is in torch.nn.parallel.replicate. In replicate function, it copies all parameter in module (# of replica times) with Broadcast.apply. In broadcasting code, it just defines new torch.nn.Parameter with default constructor requires_grad parameter, which is always set to True.\nI think there can be some choices to fix this issue.\n\nIt is intended behavior for DataParallel, so we should use volatile (or torch.no_grad) to implement transfer learning in DataParallel\nBroadcast model should be fixed to handle requires_grad when copying parameters.\nThere should be synchronizing requires_grad in torch.nn.parallel.replicate\n\nI think any of solution doesn't need lots of effort to fix it.", "body": "- PyTorch version: 0.3.0\r\n- Python version: 3.5\r\n- GPU models and configuration: 8x NVIDIA Titan\r\n\r\nI found that requires_grad setting in module is ignored when module is wrapped with nn.DataParallel. For example,\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn import DataParallel\r\nfrom torchvision.models.resnet import resnet50\r\n\r\nmodule_ = resnet50()\r\nfor name, param in module_.named_parameters():\r\n    if name.startswith('conv1') or name.startswith('bn1'):\r\n        param.requires_grad = False\r\n    if name.startswith('layer1') or name.startswith('layer2'):\r\n        param.requires_grad = False\r\n    if name.startswith('layer3') or name.startswith('layer4'):\r\n        param.requires_grad = False\r\n\r\nmodule_ = DataParallel(module_).cuda()\r\n\r\nx = torch.rand(32, 3, 1600, 1600)\r\nx = torch.autograd.Variable(x)\r\nx = module_(x)\r\nprint(x)\r\n```\r\nThis code should be executed because very small part of network activation (only fc layer) is stored for backward computation, but it results to run-time error:\r\n\r\n```\r\nRuntimeError: cuda runtime error (2) : out of memory at /tmp/pip-8pfswvat-build/torch/lib/THC/generic/THCStorage.cu:58\r\n```\r\n\r\nInternally, I found a crack in ```replicate``` function which is in ```torch.nn.parallel.replicate```. In replicate function, it copies all parameter in module (# of replica times) with ```Broadcast.apply```. In broadcasting code, it just defines new ```torch.nn.Parameter``` with default constructor ```requires_grad``` parameter, which is always set to True.\r\n\r\nI think there can be some choices to fix this issue.\r\n\r\n1. It is intended behavior for DataParallel, so we should use volatile (or torch.no_grad) to implement transfer learning in DataParallel\r\n2. Broadcast model should be fixed to handle requires_grad when copying parameters.\r\n3. There should be synchronizing ```requires_grad``` in ```torch.nn.parallel.replicate```\r\n\r\nI think any of solution doesn't need lots of effort to fix it.\r\n"}