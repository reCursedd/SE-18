{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/188676694", "pull_request_review_id": 120703786, "id": 188676694, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4ODY3NjY5NA==", "diff_hunk": "@@ -113,25 +113,39 @@ class CloneableModule : public Module {\n  public:\n   using Module::Module;\n \n-  // should it also detach the gradients, like a deepcopy? Or maybe let's just\n-  // give clone() a boolean for this?\n-  std::unique_ptr<Module> clone() const override {\n-    auto ptr = std::unique_ptr<Module>(\n-        new Derived(*static_cast<const Derived*>(this)));\n+  virtual void reset() = 0;\n+\n+  std::shared_ptr<Derived> build() {\n+    auto module = std::make_shared<Derived>(static_cast<Derived&&>(*this));\n+    module->reset();\n+    return std::move(module);\n+  }\n+\n+  std::shared_ptr<Module> clone() const override {\n+    auto ptr = std::make_shared<Derived>(*static_cast<const Derived*>(this));\n+    ptr->parameters_.clear();\n+    ptr->reset();\n     for (auto& parameter : ptr->parameters_) {\n-      parameter.second = this->parameters_.at(parameter.first).clone();\n+      parameter.second.data().copy_(\n+          this->parameters_.at(parameter.first).data());\n     }\n     for (auto& child : ptr->children_) {\n       child.second = this->children_.at(child.first)->clone();\n     }\n     return ptr;\n   }\n };\n-\n-template <class Module>\n-std::shared_ptr<typename std::decay<Module>::type> make(Module&& module) {\n-  auto ptr = std::make_shared<typename std::decay<Module>::type>(\n-      std::forward<Module>(module));\n-  return ptr;\n-}\n }} // namespace torch::nn\n+\n+#define TORCH_PARAMETER(T, name)             \\\n+ public:                                     \\\n+  auto name(T new_##name)->decltype(*this) { \\\n+    this->name##_ = std::move(new_##name);   \\\n+    return *this;                            \\\n+  }                                          \\\n+  const T& name() const noexcept {           \\\n+    return this->name##_;                    \\\n+  }                                          \\\n+                                             \\\n+ protected:                                  \\", "path": "torch/csrc/api/include/torch/nn/module.h", "position": null, "original_position": 60, "commit_id": "994d74d21af6b1a2712a58c7ca15146c0166f1d3", "original_commit_id": "0028a7088f029ea74c4f8a96426a921bd926ea53", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "For example for conv, we'd switch from\r\n\r\n```\r\ntemplate <size_t D, typename Derived>\r\nclass Conv : public torch::nn::CloneableModule<Derived> {\r\n public:\r\n  struct ExpandingSize {\r\n    ExpandingSize(std::initializer_list<int64_t> list);\r\n    ExpandingSize(std::vector<int64_t> sizes);\r\n    ExpandingSize(int64_t single_size);\r\n    std::array<int64_t, D>& operator*();\r\n    std::array<int64_t, D>* operator->();\r\n    operator at::IntList();\r\n    std::array<int64_t, D> sizes_;\r\n  };\r\n\r\n  Conv(\r\n      int64_t input_channels,\r\n      int64_t output_channels,\r\n      ExpandingSize kernel_size);\r\n\r\n  void reset() override;\r\n\r\n  TORCH_PARAMETER(int64_t, input_channels);\r\n  TORCH_PARAMETER(int64_t, output_channels);\r\n  TORCH_PARAMETER(ExpandingSize, kernel_size);\r\n  TORCH_PARAMETER(ExpandingSize, stride) = 1;\r\n  TORCH_PARAMETER(ExpandingSize, padding) = 0;\r\n  TORCH_PARAMETER(ExpandingSize, dilation) = 1;\r\n  TORCH_PARAMETER(ExpandingSize, output_padding) = 0;\r\n  TORCH_PARAMETER(bool, transposed) = false;\r\n  TORCH_PARAMETER(bool, with_bias) = true;\r\n  TORCH_PARAMETER(int64_t, groups) = 1;\r\n\r\n protected:\r\n  Variable weights_;\r\n  Variable bias_;\r\n};\r\n```\r\n\r\nto\r\n\r\n```\r\ntemplate <size_t D, typename Derived>\r\nclass Conv : public torch::nn::CloneableModule<Derived> {\r\n public:\r\n  struct ExpandingSize {\r\n    ExpandingSize(std::initializer_list<int64_t> list);\r\n    ExpandingSize(std::vector<int64_t> sizes);\r\n    ExpandingSize(int64_t single_size);\r\n    std::array<int64_t, D>& operator*();\r\n    std::array<int64_t, D>* operator->();\r\n    operator at::IntList();\r\n    std::array<int64_t, D> sizes_;\r\n  };\r\n\r\n  Conv(\r\n      int64_t input_channels,\r\n      int64_t output_channels,\r\n      ExpandingSize kernel_size);\r\n\r\n  void reset() override;\r\n\r\n  TORCH_PARAMETER(int64_t, input_channels);\r\n  TORCH_PARAMETER(int64_t, output_channels);\r\n  TORCH_PARAMETER(ExpandingSize, kernel_size);\r\n  TORCH_PARAMETER(ExpandingSize, stride);\r\n  TORCH_PARAMETER(ExpandingSize, padding);\r\n  TORCH_PARAMETER(ExpandingSize, dilation);\r\n  TORCH_PARAMETER(ExpandingSize, output_padding);\r\n  TORCH_PARAMETER(bool, transposed);\r\n  TORCH_PARAMETER(bool, with_bias);\r\n  TORCH_PARAMETER(int64_t, groups);\r\n\r\n protected:\r\n  int64_t input_channels;\r\n  int64_t output_channels;\r\n  ExpandingSize kernel_size;\r\n  ExpandingSize stride = 1;\r\n  ExpandingSize padding = 0;\r\n  ExpandingSize dilation = 1;\r\n  ExpandingSize output_padding = 0;\r\n  bool transposed = false;\r\n  bool with_bias = true;\r\n  int64_t groups = 1;\r\n\r\n  Variable weights_;\r\n  Variable bias_;\r\n};\r\n```", "created_at": "2018-05-16T15:47:54Z", "updated_at": "2018-11-23T15:44:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/7597#discussion_r188676694", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7597", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/188676694"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7597#discussion_r188676694"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7597"}}, "body_html": "<p>For example for conv, we'd switch from</p>\n<pre><code>template &lt;size_t D, typename Derived&gt;\nclass Conv : public torch::nn::CloneableModule&lt;Derived&gt; {\n public:\n  struct ExpandingSize {\n    ExpandingSize(std::initializer_list&lt;int64_t&gt; list);\n    ExpandingSize(std::vector&lt;int64_t&gt; sizes);\n    ExpandingSize(int64_t single_size);\n    std::array&lt;int64_t, D&gt;&amp; operator*();\n    std::array&lt;int64_t, D&gt;* operator-&gt;();\n    operator at::IntList();\n    std::array&lt;int64_t, D&gt; sizes_;\n  };\n\n  Conv(\n      int64_t input_channels,\n      int64_t output_channels,\n      ExpandingSize kernel_size);\n\n  void reset() override;\n\n  TORCH_PARAMETER(int64_t, input_channels);\n  TORCH_PARAMETER(int64_t, output_channels);\n  TORCH_PARAMETER(ExpandingSize, kernel_size);\n  TORCH_PARAMETER(ExpandingSize, stride) = 1;\n  TORCH_PARAMETER(ExpandingSize, padding) = 0;\n  TORCH_PARAMETER(ExpandingSize, dilation) = 1;\n  TORCH_PARAMETER(ExpandingSize, output_padding) = 0;\n  TORCH_PARAMETER(bool, transposed) = false;\n  TORCH_PARAMETER(bool, with_bias) = true;\n  TORCH_PARAMETER(int64_t, groups) = 1;\n\n protected:\n  Variable weights_;\n  Variable bias_;\n};\n</code></pre>\n<p>to</p>\n<pre><code>template &lt;size_t D, typename Derived&gt;\nclass Conv : public torch::nn::CloneableModule&lt;Derived&gt; {\n public:\n  struct ExpandingSize {\n    ExpandingSize(std::initializer_list&lt;int64_t&gt; list);\n    ExpandingSize(std::vector&lt;int64_t&gt; sizes);\n    ExpandingSize(int64_t single_size);\n    std::array&lt;int64_t, D&gt;&amp; operator*();\n    std::array&lt;int64_t, D&gt;* operator-&gt;();\n    operator at::IntList();\n    std::array&lt;int64_t, D&gt; sizes_;\n  };\n\n  Conv(\n      int64_t input_channels,\n      int64_t output_channels,\n      ExpandingSize kernel_size);\n\n  void reset() override;\n\n  TORCH_PARAMETER(int64_t, input_channels);\n  TORCH_PARAMETER(int64_t, output_channels);\n  TORCH_PARAMETER(ExpandingSize, kernel_size);\n  TORCH_PARAMETER(ExpandingSize, stride);\n  TORCH_PARAMETER(ExpandingSize, padding);\n  TORCH_PARAMETER(ExpandingSize, dilation);\n  TORCH_PARAMETER(ExpandingSize, output_padding);\n  TORCH_PARAMETER(bool, transposed);\n  TORCH_PARAMETER(bool, with_bias);\n  TORCH_PARAMETER(int64_t, groups);\n\n protected:\n  int64_t input_channels;\n  int64_t output_channels;\n  ExpandingSize kernel_size;\n  ExpandingSize stride = 1;\n  ExpandingSize padding = 0;\n  ExpandingSize dilation = 1;\n  ExpandingSize output_padding = 0;\n  bool transposed = false;\n  bool with_bias = true;\n  int64_t groups = 1;\n\n  Variable weights_;\n  Variable bias_;\n};\n</code></pre>", "body_text": "For example for conv, we'd switch from\ntemplate <size_t D, typename Derived>\nclass Conv : public torch::nn::CloneableModule<Derived> {\n public:\n  struct ExpandingSize {\n    ExpandingSize(std::initializer_list<int64_t> list);\n    ExpandingSize(std::vector<int64_t> sizes);\n    ExpandingSize(int64_t single_size);\n    std::array<int64_t, D>& operator*();\n    std::array<int64_t, D>* operator->();\n    operator at::IntList();\n    std::array<int64_t, D> sizes_;\n  };\n\n  Conv(\n      int64_t input_channels,\n      int64_t output_channels,\n      ExpandingSize kernel_size);\n\n  void reset() override;\n\n  TORCH_PARAMETER(int64_t, input_channels);\n  TORCH_PARAMETER(int64_t, output_channels);\n  TORCH_PARAMETER(ExpandingSize, kernel_size);\n  TORCH_PARAMETER(ExpandingSize, stride) = 1;\n  TORCH_PARAMETER(ExpandingSize, padding) = 0;\n  TORCH_PARAMETER(ExpandingSize, dilation) = 1;\n  TORCH_PARAMETER(ExpandingSize, output_padding) = 0;\n  TORCH_PARAMETER(bool, transposed) = false;\n  TORCH_PARAMETER(bool, with_bias) = true;\n  TORCH_PARAMETER(int64_t, groups) = 1;\n\n protected:\n  Variable weights_;\n  Variable bias_;\n};\n\nto\ntemplate <size_t D, typename Derived>\nclass Conv : public torch::nn::CloneableModule<Derived> {\n public:\n  struct ExpandingSize {\n    ExpandingSize(std::initializer_list<int64_t> list);\n    ExpandingSize(std::vector<int64_t> sizes);\n    ExpandingSize(int64_t single_size);\n    std::array<int64_t, D>& operator*();\n    std::array<int64_t, D>* operator->();\n    operator at::IntList();\n    std::array<int64_t, D> sizes_;\n  };\n\n  Conv(\n      int64_t input_channels,\n      int64_t output_channels,\n      ExpandingSize kernel_size);\n\n  void reset() override;\n\n  TORCH_PARAMETER(int64_t, input_channels);\n  TORCH_PARAMETER(int64_t, output_channels);\n  TORCH_PARAMETER(ExpandingSize, kernel_size);\n  TORCH_PARAMETER(ExpandingSize, stride);\n  TORCH_PARAMETER(ExpandingSize, padding);\n  TORCH_PARAMETER(ExpandingSize, dilation);\n  TORCH_PARAMETER(ExpandingSize, output_padding);\n  TORCH_PARAMETER(bool, transposed);\n  TORCH_PARAMETER(bool, with_bias);\n  TORCH_PARAMETER(int64_t, groups);\n\n protected:\n  int64_t input_channels;\n  int64_t output_channels;\n  ExpandingSize kernel_size;\n  ExpandingSize stride = 1;\n  ExpandingSize padding = 0;\n  ExpandingSize dilation = 1;\n  ExpandingSize output_padding = 0;\n  bool transposed = false;\n  bool with_bias = true;\n  int64_t groups = 1;\n\n  Variable weights_;\n  Variable bias_;\n};", "in_reply_to_id": 188463619}