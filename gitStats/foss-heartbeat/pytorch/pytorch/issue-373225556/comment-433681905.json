{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/433681905", "html_url": "https://github.com/pytorch/pytorch/issues/13008#issuecomment-433681905", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13008", "id": 433681905, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzY4MTkwNQ==", "user": {"login": "bbartoldson", "id": 15717529, "node_id": "MDQ6VXNlcjE1NzE3NTI5", "avatar_url": "https://avatars3.githubusercontent.com/u/15717529?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bbartoldson", "html_url": "https://github.com/bbartoldson", "followers_url": "https://api.github.com/users/bbartoldson/followers", "following_url": "https://api.github.com/users/bbartoldson/following{/other_user}", "gists_url": "https://api.github.com/users/bbartoldson/gists{/gist_id}", "starred_url": "https://api.github.com/users/bbartoldson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bbartoldson/subscriptions", "organizations_url": "https://api.github.com/users/bbartoldson/orgs", "repos_url": "https://api.github.com/users/bbartoldson/repos", "events_url": "https://api.github.com/users/bbartoldson/events{/privacy}", "received_events_url": "https://api.github.com/users/bbartoldson/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-28T07:08:19Z", "updated_at": "2018-10-28T07:08:19Z", "author_association": "NONE", "body_html": "<p>The following approach seems to allow me to compute gradients with respect to inputs that have gone through a filter with <code>dilation</code>&gt;1.</p>\n<p>In the function <code>_grad_input_padding()</code> in the file <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/grad.py\">https://github.com/pytorch/pytorch/blob/master/torch/nn/grad.py</a>, change the definition of <code>dim_size()</code> to</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">dim_size</span>(<span class=\"pl-smi\">d</span>):\n     <span class=\"pl-k\">return</span> ((grad_output.size(d <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> stride[d] <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> padding[d] <span class=\"pl-k\">+</span> \n                kernel_size[d] <span class=\"pl-k\">+</span> (kernel_size[d] <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span>  (dilation[d] <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>))</pre></div>\n<p>and add <code>dilation</code> to the args of <code>_grad_input_padding()</code>.</p>\n<p>My approach to fixing this problem seems wrong, however, because I believe <code>_grad_input_padding()</code>will always return a tuple of 0s after my \"fix\".</p>", "body_text": "The following approach seems to allow me to compute gradients with respect to inputs that have gone through a filter with dilation>1.\nIn the function _grad_input_padding() in the file https://github.com/pytorch/pytorch/blob/master/torch/nn/grad.py, change the definition of dim_size() to\ndef dim_size(d):\n     return ((grad_output.size(d + 2) - 1) * stride[d] - 2 * padding[d] + \n                kernel_size[d] + (kernel_size[d] - 1) *  (dilation[d] - 1))\nand add dilation to the args of _grad_input_padding().\nMy approach to fixing this problem seems wrong, however, because I believe _grad_input_padding()will always return a tuple of 0s after my \"fix\".", "body": "The following approach seems to allow me to compute gradients with respect to inputs that have gone through a filter with ```dilation```>1.\r\n\r\nIn the function ```_grad_input_padding()``` in the file https://github.com/pytorch/pytorch/blob/master/torch/nn/grad.py, change the definition of ```dim_size()``` to\r\n```python\r\ndef dim_size(d):\r\n     return ((grad_output.size(d + 2) - 1) * stride[d] - 2 * padding[d] + \r\n                kernel_size[d] + (kernel_size[d] - 1) *  (dilation[d] - 1))\r\n```\r\nand add ```dilation``` to the args of ```_grad_input_padding()```.\r\n\r\nMy approach to fixing this problem seems wrong, however, because I believe ```_grad_input_padding()```will always return a tuple of 0s after my \"fix\"."}