{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7987", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7987/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7987/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7987/events", "html_url": "https://github.com/pytorch/pytorch/issues/7987", "id": 328163240, "node_id": "MDU6SXNzdWUzMjgxNjMyNDA=", "number": 7987, "title": "[Bug] Custom Implementation of Random Sampler", "user": {"login": "kris-singh", "id": 11256139, "node_id": "MDQ6VXNlcjExMjU2MTM5", "avatar_url": "https://avatars1.githubusercontent.com/u/11256139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kris-singh", "html_url": "https://github.com/kris-singh", "followers_url": "https://api.github.com/users/kris-singh/followers", "following_url": "https://api.github.com/users/kris-singh/following{/other_user}", "gists_url": "https://api.github.com/users/kris-singh/gists{/gist_id}", "starred_url": "https://api.github.com/users/kris-singh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kris-singh/subscriptions", "organizations_url": "https://api.github.com/users/kris-singh/orgs", "repos_url": "https://api.github.com/users/kris-singh/repos", "events_url": "https://api.github.com/users/kris-singh/events{/privacy}", "received_events_url": "https://api.github.com/users/kris-singh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-31T14:31:05Z", "updated_at": "2018-05-31T20:34:20Z", "closed_at": "2018-05-31T20:34:19Z", "author_association": "NONE", "body_html": "<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>I am trying to build some custom sampling functions. That need to be dynamic in nature. As a sanity check, I wanted to first implement RandomSampler. But if you run the code given below you would find that my implementation of the random sampler performs miserably compared to the build trainLoader.<br>\nCan anyone tell me what I am doing wrong here?</p>\n<h2>Code example</h2>\n<pre><code>import torch\nprint(\"torch.__version__\", torch.__version__)\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\n\n\n# Create a Random Sampler\nclass sampler():\n    def __init__(self, dataset, batch_size):\n        self.dataset = dataset\n        self.batch_size = batch_size\n    def __iter__(self):\n        indices = torch.randperm (len (self.dataset))\n        for i in range (0, self.dataset.shape[0] // self.batch_size):\n            yield i, indices[i * self.batch_size: (i + 1) * self.batch_size]\n\n\n# Create the Network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        \"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                m.bias.data.fill_(0.01)\n        \"\"\"\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        feat_x = x\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1), feat_x\n\n\ndef train(model, device, sampler, optimizer, epoch):\n    model.train()\n    for batch_idx, idx in sampler:\n        data, target = org_data[idx], org_target[idx]\n        data = data.to(device)\n        target = target.to(device)\n        optimizer.zero_grad()\n        output, feat = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 1000 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef train_normal(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n        optimizer.zero_grad()\n        output, feat = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 1000 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader, epoch):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data = data.to(device)\n            target = target.to(device)\n            output, _ = model(data)\n            test_loss += F.nll_loss (output, target, size_average=False).item ()  # sum up batch loss\n            pred = output.max (1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq (target.view_as (pred)).sum ().item ()\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main(normal):\n    # Read Dataset\n    dataset = datasets.MNIST(\"./mnist\", train=True, download=False,\n                         transform=transforms.Compose([\n                             transforms.ToTensor(),\n                             transforms.Normalize((0.1307,), (0.3081,))\n                         ]))\n    org_data, org_target = dataset.train_data, dataset.train_labels\n    org_data = org_data.unsqueeze(1).type(torch.FloatTensor)\n\n    test_dataset = datasets.MNIST(\"./mnist\", train=True, download=False,\n                                  transform=transforms.Compose([\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((0.1307,), (0.3081,))\n                                  ]))\n\n    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n    # Training loop\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    train_batch_size = 64\n    test_batch_size = 1000\n    learning_rate = 0.01\n    momentum = 0.5\n    epochs = 10\n    sampling_fnc = sampler(org_data, train_batch_size)\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n    for i in range(epochs):\n        if normal==1:\n            train_normal(model, device, train_loader, optimizer, i)\n        else:\n            train(model, device, sampling_fnc, optimizer, i)\n        test(model, device, test_loader, i)\n\nif __name__ == \"__main__\":\n    main(0)\n    main(1)\n</code></pre>\n<p>Please try to provide a minimal example to repro the bug.<br>\nError messages and stack traces are also helpful.</p>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch or Caffe2:</li>\n<li>How you installed PyTorch (conda, pip, source):</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS:</li>\n<li>PyTorch version:0.4.0</li>\n<li>Python version:</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "If you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nI am trying to build some custom sampling functions. That need to be dynamic in nature. As a sanity check, I wanted to first implement RandomSampler. But if you run the code given below you would find that my implementation of the random sampler performs miserably compared to the build trainLoader.\nCan anyone tell me what I am doing wrong here?\nCode example\nimport torch\nprint(\"torch.__version__\", torch.__version__)\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\n\n\n# Create a Random Sampler\nclass sampler():\n    def __init__(self, dataset, batch_size):\n        self.dataset = dataset\n        self.batch_size = batch_size\n    def __iter__(self):\n        indices = torch.randperm (len (self.dataset))\n        for i in range (0, self.dataset.shape[0] // self.batch_size):\n            yield i, indices[i * self.batch_size: (i + 1) * self.batch_size]\n\n\n# Create the Network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        \"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                m.bias.data.fill_(0.01)\n        \"\"\"\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        feat_x = x\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1), feat_x\n\n\ndef train(model, device, sampler, optimizer, epoch):\n    model.train()\n    for batch_idx, idx in sampler:\n        data, target = org_data[idx], org_target[idx]\n        data = data.to(device)\n        target = target.to(device)\n        optimizer.zero_grad()\n        output, feat = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 1000 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef train_normal(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n        optimizer.zero_grad()\n        output, feat = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 1000 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader, epoch):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data = data.to(device)\n            target = target.to(device)\n            output, _ = model(data)\n            test_loss += F.nll_loss (output, target, size_average=False).item ()  # sum up batch loss\n            pred = output.max (1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq (target.view_as (pred)).sum ().item ()\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main(normal):\n    # Read Dataset\n    dataset = datasets.MNIST(\"./mnist\", train=True, download=False,\n                         transform=transforms.Compose([\n                             transforms.ToTensor(),\n                             transforms.Normalize((0.1307,), (0.3081,))\n                         ]))\n    org_data, org_target = dataset.train_data, dataset.train_labels\n    org_data = org_data.unsqueeze(1).type(torch.FloatTensor)\n\n    test_dataset = datasets.MNIST(\"./mnist\", train=True, download=False,\n                                  transform=transforms.Compose([\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((0.1307,), (0.3081,))\n                                  ]))\n\n    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n    # Training loop\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    train_batch_size = 64\n    test_batch_size = 1000\n    learning_rate = 0.01\n    momentum = 0.5\n    epochs = 10\n    sampling_fnc = sampler(org_data, train_batch_size)\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n    for i in range(epochs):\n        if normal==1:\n            train_normal(model, device, train_loader, optimizer, i)\n        else:\n            train(model, device, sampling_fnc, optimizer, i)\n        test(model, device, test_loader, i)\n\nif __name__ == \"__main__\":\n    main(0)\n    main(1)\n\nPlease try to provide a minimal example to repro the bug.\nError messages and stack traces are also helpful.\nSystem Info\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch or Caffe2:\nHow you installed PyTorch (conda, pip, source):\nBuild command you used (if compiling from source):\nOS:\nPyTorch version:0.4.0\nPython version:\nCUDA/cuDNN version:\nGPU models and configuration:\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\n\r\nI am trying to build some custom sampling functions. That need to be dynamic in nature. As a sanity check, I wanted to first implement RandomSampler. But if you run the code given below you would find that my implementation of the random sampler performs miserably compared to the build trainLoader. \r\nCan anyone tell me what I am doing wrong here?\r\n\r\n## Code example\r\n```\r\nimport torch\r\nprint(\"torch.__version__\", torch.__version__)\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\n\r\n\r\n# Create a Random Sampler\r\nclass sampler():\r\n    def __init__(self, dataset, batch_size):\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n    def __iter__(self):\r\n        indices = torch.randperm (len (self.dataset))\r\n        for i in range (0, self.dataset.shape[0] // self.batch_size):\r\n            yield i, indices[i * self.batch_size: (i + 1) * self.batch_size]\r\n\r\n\r\n# Create the Network\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n        \"\"\"\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n            elif isinstance(m, nn.Linear):\r\n                nn.init.xavier_uniform_(m.weight)\r\n                m.bias.data.fill_(0.01)\r\n        \"\"\"\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        feat_x = x\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x, dim=1), feat_x\r\n\r\n\r\ndef train(model, device, sampler, optimizer, epoch):\r\n    model.train()\r\n    for batch_idx, idx in sampler:\r\n        data, target = org_data[idx], org_target[idx]\r\n        data = data.to(device)\r\n        target = target.to(device)\r\n        optimizer.zero_grad()\r\n        output, feat = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        if batch_idx % 1000 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                100. * batch_idx / len(train_loader), loss.item()))\r\n\r\ndef train_normal(model, device, train_loader, optimizer, epoch):\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.to(device)\r\n        target = target.to(device)\r\n        optimizer.zero_grad()\r\n        output, feat = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        if batch_idx % 1000 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                100. * batch_idx / len(train_loader), loss.item()))\r\n\r\ndef test(model, device, test_loader, epoch):\r\n    model.eval()\r\n    test_loss = 0\r\n    correct = 0\r\n    with torch.no_grad():\r\n        for data, target in test_loader:\r\n            data = data.to(device)\r\n            target = target.to(device)\r\n            output, _ = model(data)\r\n            test_loss += F.nll_loss (output, target, size_average=False).item ()  # sum up batch loss\r\n            pred = output.max (1, keepdim=True)[1]  # get the index of the max log-probability\r\n            correct += pred.eq (target.view_as (pred)).sum ().item ()\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n\r\ndef main(normal):\r\n    # Read Dataset\r\n    dataset = datasets.MNIST(\"./mnist\", train=True, download=False,\r\n                         transform=transforms.Compose([\r\n                             transforms.ToTensor(),\r\n                             transforms.Normalize((0.1307,), (0.3081,))\r\n                         ]))\r\n    org_data, org_target = dataset.train_data, dataset.train_labels\r\n    org_data = org_data.unsqueeze(1).type(torch.FloatTensor)\r\n\r\n    test_dataset = datasets.MNIST(\"./mnist\", train=True, download=False,\r\n                                  transform=transforms.Compose([\r\n                                      transforms.ToTensor(),\r\n                                      transforms.Normalize((0.1307,), (0.3081,))\r\n                                  ]))\r\n\r\n    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=False)\r\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\r\n    # Training loop\r\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n    train_batch_size = 64\r\n    test_batch_size = 1000\r\n    learning_rate = 0.01\r\n    momentum = 0.5\r\n    epochs = 10\r\n    sampling_fnc = sampler(org_data, train_batch_size)\r\n    model = Net().to(device)\r\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\r\n    for i in range(epochs):\r\n        if normal==1:\r\n            train_normal(model, device, train_loader, optimizer, i)\r\n        else:\r\n            train(model, device, sampling_fnc, optimizer, i)\r\n        test(model, device, test_loader, i)\r\n\r\nif __name__ == \"__main__\":\r\n    main(0)\r\n    main(1)\r\n```\r\nPlease try to provide a minimal example to repro the bug.\r\nError messages and stack traces are also helpful.\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n- PyTorch or Caffe2:\r\n- How you installed PyTorch (conda, pip, source):\r\n- Build command you used (if compiling from source):\r\n- OS:\r\n- PyTorch version:0.4.0\r\n- Python version:\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}