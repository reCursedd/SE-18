{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104961201", "pull_request_review_id": 25818944, "id": 104961201, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNDk2MTIwMQ==", "diff_hunk": "@@ -4,21 +4,35 @@\n class RMSprop(Optimizer):\n     \"\"\"Implements RMSprop algorithm.\n \n+    Proposed by G. Hinton in his `course <http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_.\n+\n+    The centered version first appears in `Generating Sequences\n+    With Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\n+\n     Arguments:\n         params (iterable): iterable of parameters to optimize or dicts defining\n             parameter groups\n         lr (float, optional): learning rate (default: 1e-2)\n+        momentum (float, optional): momentum factor (default: 0)\n         alpha (float, optional): smoothing constant (default: 0.99)\n         eps (float, optional): term added to the denominator to improve\n             numerical stability (default: 1e-8)\n+        centered (bool, optional) : if True, compute the centered RMSProp,\n+            the gradient is normalized by an estimation of its variance\n         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n \n     \"\"\"\n \n-    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0):\n-        defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay)\n+    def __init__(self, params, lr=1e-2, momentum=0, alpha=0.99, eps=1e-8, centered=False, weight_decay=0):", "path": "torch/optim/rmsprop.py", "position": null, "original_position": 25, "commit_id": "42cf15229ebbbb2bce7104c6c280b4d83c852a4b", "original_commit_id": "6291f5be8c2b94e26cdbfb8414c5073118ba408f", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Ugh, sorry but I just realised that you inserted additional arguments at the beginning of `__init__`. We can't do that, because if someone created the optimizer like that:\r\n```\r\noptimizer.RMSprop(model.parameters(), 1e-2, 1 - 1e-4)\r\n```\r\nthen the third argument will be interpreted as momentum, while it used to be `alpha`. Once that is change it's good to be merged.", "created_at": "2017-03-08T16:40:38Z", "updated_at": "2018-11-23T15:32:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/810#discussion_r104961201", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/810", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104961201"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/810#discussion_r104961201"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/810"}}, "body_html": "<p>Ugh, sorry but I just realised that you inserted additional arguments at the beginning of <code>__init__</code>. We can't do that, because if someone created the optimizer like that:</p>\n<pre><code>optimizer.RMSprop(model.parameters(), 1e-2, 1 - 1e-4)\n</code></pre>\n<p>then the third argument will be interpreted as momentum, while it used to be <code>alpha</code>. Once that is change it's good to be merged.</p>", "body_text": "Ugh, sorry but I just realised that you inserted additional arguments at the beginning of __init__. We can't do that, because if someone created the optimizer like that:\noptimizer.RMSprop(model.parameters(), 1e-2, 1 - 1e-4)\n\nthen the third argument will be interpreted as momentum, while it used to be alpha. Once that is change it's good to be merged."}