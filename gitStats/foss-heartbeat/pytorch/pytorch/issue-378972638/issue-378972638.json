{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13757", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13757/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13757/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13757/events", "html_url": "https://github.com/pytorch/pytorch/issues/13757", "id": 378972638, "node_id": "MDU6SXNzdWUzNzg5NzI2Mzg=", "number": 13757, "title": "Small CPU model forward pass extremely slow", "user": {"login": "p-morais", "id": 3759402, "node_id": "MDQ6VXNlcjM3NTk0MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3759402?v=4", "gravatar_id": "", "url": "https://api.github.com/users/p-morais", "html_url": "https://github.com/p-morais", "followers_url": "https://api.github.com/users/p-morais/followers", "following_url": "https://api.github.com/users/p-morais/following{/other_user}", "gists_url": "https://api.github.com/users/p-morais/gists{/gist_id}", "starred_url": "https://api.github.com/users/p-morais/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/p-morais/subscriptions", "organizations_url": "https://api.github.com/users/p-morais/orgs", "repos_url": "https://api.github.com/users/p-morais/repos", "events_url": "https://api.github.com/users/p-morais/events{/privacy}", "received_events_url": "https://api.github.com/users/p-morais/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/performance", "name": "performance", "color": "f9d0c4", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-09T00:20:17Z", "updated_at": "2018-11-09T20:09:32Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I have a 80x256x256x10 FC network I'm using for policy gradient, but when I do a forward pass on it it takes 70-100ms (!!) to execute. Puzzling-ly, after some condition that I can't identify, the forward pass suddenly speeds up permanently to &lt; 1-3ms (I couldn't get a reproducible example of that though). Exiting the code during execution seems to imply that it's spending a lot of time in <code>torch.addmm(bias, input, weight.t())</code>. Adding <code>pytorch.set_num_threads(1)</code> seems to fix it, but I don't know why. Sorry if this is a duplicate (I suspect it may be) but I looked and couldn't find anything in the issues</p>\n<h2>Code example</h2>\n<p>Here's a (sort of) minimal example that reproduces the slowness:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">GaussianMLP</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_inputs</span>, <span class=\"pl-smi\">action_dim</span>, <span class=\"pl-smi\">init_std</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">learn_std</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tanh<span class=\"pl-pds\">\"</span></span>):\n        <span class=\"pl-c1\">super</span>(GaussianMLP, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        actor_dims <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)\n        critic_dims <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> create actor network</span>\n        <span class=\"pl-c1\">self</span>.actor_layers <span class=\"pl-k\">=</span> nn.ModuleList()\n        <span class=\"pl-c1\">self</span>.actor_layers <span class=\"pl-k\">+=</span> [nn.Linear(num_inputs, actor_dims[<span class=\"pl-c1\">0</span>])]\n        <span class=\"pl-k\">for</span> l <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(actor_dims) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>):\n            in_dim <span class=\"pl-k\">=</span> actor_dims[l]\n            out_dim <span class=\"pl-k\">=</span> actor_dims[l <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>]\n            <span class=\"pl-c1\">self</span>.actor_layers <span class=\"pl-k\">+=</span> [nn.Linear(in_dim, out_dim)]\n        \n        <span class=\"pl-c1\">self</span>.mean <span class=\"pl-k\">=</span> nn.Linear(actor_dims[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], action_dim)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> create critic network</span>\n        <span class=\"pl-c1\">self</span>.critic_layers <span class=\"pl-k\">=</span> nn.ModuleList()\n        <span class=\"pl-c1\">self</span>.critic_layers <span class=\"pl-k\">+=</span> [nn.Linear(num_inputs, critic_dims[<span class=\"pl-c1\">0</span>])]\n        <span class=\"pl-k\">for</span> l <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(critic_dims) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>):\n            in_dim <span class=\"pl-k\">=</span> critic_dims[l]\n            out_dim <span class=\"pl-k\">=</span> critic_dims[l <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>]\n            <span class=\"pl-c1\">self</span>.critic_layers <span class=\"pl-k\">+=</span> [nn.Linear(in_dim, out_dim)]\n\n        <span class=\"pl-c1\">self</span>.vf <span class=\"pl-k\">=</span> nn.Linear(critic_dims[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">1</span>)\n\n        <span class=\"pl-k\">if</span> nonlinearity <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>:\n            <span class=\"pl-c1\">self</span>.nonlinearity <span class=\"pl-k\">=</span> F.relu\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">self</span>.nonlinearity <span class=\"pl-k\">=</span> torch.tanh\n        \n        <span class=\"pl-c1\">self</span>.train()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        start <span class=\"pl-k\">=</span> time.time()\n        x <span class=\"pl-k\">=</span> inputs\n        <span class=\"pl-k\">for</span> l <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.critic_layers:\n            x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.nonlinearity(l(x))\n        value <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.vf(x)\n\n        x <span class=\"pl-k\">=</span> inputs\n        <span class=\"pl-k\">for</span> l <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.actor_layers:\n            x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.nonlinearity(l(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.mean(x)\n\n        x <span class=\"pl-k\">=</span> torch.tanh(x)\n        <span class=\"pl-c1\">print</span>(time.time() <span class=\"pl-k\">-</span> start)\n\n        <span class=\"pl-k\">return</span> value, x\n\npolicy <span class=\"pl-k\">=</span> GaussianMLP(<span class=\"pl-c1\">80</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">init_std</span><span class=\"pl-k\">=</span>np.exp(<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>), <span class=\"pl-v\">learn_std</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n    s <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">80</span>)\n    policy(s)</pre></div>\n<p>And here's an iPython stacktrace from randomly exiting that code while it's running:</p>\n<pre lang=\"iPython\"><code>    61 for _ in range(100):\n     62     s = torch.rand(1, 80)\n---&gt; 63     policy(s)\n\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--&gt; 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n~/p-morais/cassie-sim-to-real/gaussian_mlp.py in forward(self, inputs)\n     49         x = inputs\n     50         for l in self.actor_layers:\n---&gt; 51             x = self.nonlinearity(l(x))\n     52         x = self.mean(x)\n     53 \n\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--&gt; 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)\n     53 \n     54     def forward(self, input):\n---&gt; 55         return F.linear(input, self.weight, self.bias)\n     56 \n     57     def extra_repr(self):\n\n~/.local/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias)\n   1022     if input.dim() == 2 and bias is not None:\n   1023         # fused op is marginally faster\n-&gt; 1024         return torch.addmm(bias, input, weight.t())\n   1025 \n   1026     output = input.matmul(weight.t()))\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 18.04.1 LTS<br>\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0<br>\nCMake version: version 3.10.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: TITAN X (Pascal)<br>\nNvidia driver version: 390.77<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.15.3)<br>\n[pip3] torch (1.0.0a0+c029c83)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch<br>\n[conda] pytorch-cpu               0.4.1                py36_cpu_1    pytorch<br>\n[conda] torchfile                 0.1.0                     <br>\n[conda] torchvision-cpu           0.2.1                    py36_1    pytorch</p>", "body_text": "Issue description\nI have a 80x256x256x10 FC network I'm using for policy gradient, but when I do a forward pass on it it takes 70-100ms (!!) to execute. Puzzling-ly, after some condition that I can't identify, the forward pass suddenly speeds up permanently to < 1-3ms (I couldn't get a reproducible example of that though). Exiting the code during execution seems to imply that it's spending a lot of time in torch.addmm(bias, input, weight.t()). Adding pytorch.set_num_threads(1) seems to fix it, but I don't know why. Sorry if this is a duplicate (I suspect it may be) but I looked and couldn't find anything in the issues\nCode example\nHere's a (sort of) minimal example that reproduces the slowness:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport time\n\nclass GaussianMLP(nn.Module):\n    def __init__(self, num_inputs, action_dim, init_std=1, learn_std=True, nonlinearity=\"tanh\"):\n        super(GaussianMLP, self).__init__()\n\n        actor_dims = (256, 256)\n        critic_dims = (256, 256)\n\n        # create actor network\n        self.actor_layers = nn.ModuleList()\n        self.actor_layers += [nn.Linear(num_inputs, actor_dims[0])]\n        for l in range(len(actor_dims) - 1):\n            in_dim = actor_dims[l]\n            out_dim = actor_dims[l + 1]\n            self.actor_layers += [nn.Linear(in_dim, out_dim)]\n        \n        self.mean = nn.Linear(actor_dims[-1], action_dim)\n\n        # create critic network\n        self.critic_layers = nn.ModuleList()\n        self.critic_layers += [nn.Linear(num_inputs, critic_dims[0])]\n        for l in range(len(critic_dims) - 1):\n            in_dim = critic_dims[l]\n            out_dim = critic_dims[l + 1]\n            self.critic_layers += [nn.Linear(in_dim, out_dim)]\n\n        self.vf = nn.Linear(critic_dims[-1], 1)\n\n        if nonlinearity == \"relu\":\n            self.nonlinearity = F.relu\n        else:\n            self.nonlinearity = torch.tanh\n        \n        self.train()\n\n    def forward(self, inputs):\n        start = time.time()\n        x = inputs\n        for l in self.critic_layers:\n            x = self.nonlinearity(l(x))\n        value = self.vf(x)\n\n        x = inputs\n        for l in self.actor_layers:\n            x = self.nonlinearity(l(x))\n        x = self.mean(x)\n\n        x = torch.tanh(x)\n        print(time.time() - start)\n\n        return value, x\n\npolicy = GaussianMLP(80, 10, nonlinearity=\"relu\", init_std=np.exp(-2), learn_std=False)\n\nfor _ in range(100):\n    s = torch.rand(1, 80)\n    policy(s)\nAnd here's an iPython stacktrace from randomly exiting that code while it's running:\n    61 for _ in range(100):\n     62     s = torch.rand(1, 80)\n---> 63     policy(s)\n\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--> 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n~/p-morais/cassie-sim-to-real/gaussian_mlp.py in forward(self, inputs)\n     49         x = inputs\n     50         for l in self.actor_layers:\n---> 51             x = self.nonlinearity(l(x))\n     52         x = self.mean(x)\n     53 \n\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--> 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)\n     53 \n     54     def forward(self, input):\n---> 55         return F.linear(input, self.weight, self.bias)\n     56 \n     57     def extra_repr(self):\n\n~/.local/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias)\n   1022     if input.dim() == 2 and bias is not None:\n   1023         # fused op is marginally faster\n-> 1024         return torch.addmm(bias, input, weight.t())\n   1025 \n   1026     output = input.matmul(weight.t()))\n\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\nCMake version: version 3.10.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: TITAN X (Pascal)\nNvidia driver version: 390.77\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip3] numpy (1.15.3)\n[pip3] torch (1.0.0a0+c029c83)\n[pip3] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] pytorch-cpu               0.4.1                py36_cpu_1    pytorch\n[conda] torchfile                 0.1.0                     \n[conda] torchvision-cpu           0.2.1                    py36_1    pytorch", "body": "## Issue description\r\n\r\nI have a 80x256x256x10 FC network I'm using for policy gradient, but when I do a forward pass on it it takes 70-100ms (!!) to execute. Puzzling-ly, after some condition that I can't identify, the forward pass suddenly speeds up permanently to < 1-3ms (I couldn't get a reproducible example of that though). Exiting the code during execution seems to imply that it's spending a lot of time in `torch.addmm(bias, input, weight.t())`. Adding `pytorch.set_num_threads(1)` seems to fix it, but I don't know why. Sorry if this is a duplicate (I suspect it may be) but I looked and couldn't find anything in the issues\r\n\r\n## Code example\r\nHere's a (sort of) minimal example that reproduces the slowness:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nimport time\r\n\r\nclass GaussianMLP(nn.Module):\r\n    def __init__(self, num_inputs, action_dim, init_std=1, learn_std=True, nonlinearity=\"tanh\"):\r\n        super(GaussianMLP, self).__init__()\r\n\r\n        actor_dims = (256, 256)\r\n        critic_dims = (256, 256)\r\n\r\n        # create actor network\r\n        self.actor_layers = nn.ModuleList()\r\n        self.actor_layers += [nn.Linear(num_inputs, actor_dims[0])]\r\n        for l in range(len(actor_dims) - 1):\r\n            in_dim = actor_dims[l]\r\n            out_dim = actor_dims[l + 1]\r\n            self.actor_layers += [nn.Linear(in_dim, out_dim)]\r\n        \r\n        self.mean = nn.Linear(actor_dims[-1], action_dim)\r\n\r\n        # create critic network\r\n        self.critic_layers = nn.ModuleList()\r\n        self.critic_layers += [nn.Linear(num_inputs, critic_dims[0])]\r\n        for l in range(len(critic_dims) - 1):\r\n            in_dim = critic_dims[l]\r\n            out_dim = critic_dims[l + 1]\r\n            self.critic_layers += [nn.Linear(in_dim, out_dim)]\r\n\r\n        self.vf = nn.Linear(critic_dims[-1], 1)\r\n\r\n        if nonlinearity == \"relu\":\r\n            self.nonlinearity = F.relu\r\n        else:\r\n            self.nonlinearity = torch.tanh\r\n        \r\n        self.train()\r\n\r\n    def forward(self, inputs):\r\n        start = time.time()\r\n        x = inputs\r\n        for l in self.critic_layers:\r\n            x = self.nonlinearity(l(x))\r\n        value = self.vf(x)\r\n\r\n        x = inputs\r\n        for l in self.actor_layers:\r\n            x = self.nonlinearity(l(x))\r\n        x = self.mean(x)\r\n\r\n        x = torch.tanh(x)\r\n        print(time.time() - start)\r\n\r\n        return value, x\r\n\r\npolicy = GaussianMLP(80, 10, nonlinearity=\"relu\", init_std=np.exp(-2), learn_std=False)\r\n\r\nfor _ in range(100):\r\n    s = torch.rand(1, 80)\r\n    policy(s)\r\n```\r\n\r\nAnd here's an iPython stacktrace from randomly exiting that code while it's running: \r\n\r\n```iPython     \r\n    61 for _ in range(100):\r\n     62     s = torch.rand(1, 80)\r\n---> 63     policy(s)\r\n\r\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    475             result = self._slow_forward(*input, **kwargs)\r\n    476         else:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         for hook in self._forward_hooks.values():\r\n    479             hook_result = hook(self, input, result)\r\n\r\n~/p-morais/cassie-sim-to-real/gaussian_mlp.py in forward(self, inputs)\r\n     49         x = inputs\r\n     50         for l in self.actor_layers:\r\n---> 51             x = self.nonlinearity(l(x))\r\n     52         x = self.mean(x)\r\n     53 \r\n\r\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    475             result = self._slow_forward(*input, **kwargs)\r\n    476         else:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         for hook in self._forward_hooks.values():\r\n    479             hook_result = hook(self, input, result)\r\n\r\n~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)\r\n     53 \r\n     54     def forward(self, input):\r\n---> 55         return F.linear(input, self.weight, self.bias)\r\n     56 \r\n     57     def extra_repr(self):\r\n\r\n~/.local/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias)\r\n   1022     if input.dim() == 2 and bias is not None:\r\n   1023         # fused op is marginally faster\r\n-> 1024         return torch.addmm(bias, input, weight.t())\r\n   1025 \r\n   1026     output = input.matmul(weight.t()))\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: TITAN X (Pascal)\r\nNvidia driver version: 390.77\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.15.3)\r\n[pip3] torch (1.0.0a0+c029c83)\r\n[pip3] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] pytorch-cpu               0.4.1                py36_cpu_1    pytorch\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision-cpu           0.2.1                    py36_1    pytorch\r\n"}