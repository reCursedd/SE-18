{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6003", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6003/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6003/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6003/events", "html_url": "https://github.com/pytorch/pytorch/issues/6003", "id": 308653027, "node_id": "MDU6SXNzdWUzMDg2NTMwMjc=", "number": 6003, "title": "numpy scala + cuda tensor -> non-cuda tensor", "user": {"login": "jh-jeong", "id": 9063308, "node_id": "MDQ6VXNlcjkwNjMzMDg=", "avatar_url": "https://avatars3.githubusercontent.com/u/9063308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jh-jeong", "html_url": "https://github.com/jh-jeong", "followers_url": "https://api.github.com/users/jh-jeong/followers", "following_url": "https://api.github.com/users/jh-jeong/following{/other_user}", "gists_url": "https://api.github.com/users/jh-jeong/gists{/gist_id}", "starred_url": "https://api.github.com/users/jh-jeong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jh-jeong/subscriptions", "organizations_url": "https://api.github.com/users/jh-jeong/orgs", "repos_url": "https://api.github.com/users/jh-jeong/repos", "events_url": "https://api.github.com/users/jh-jeong/events{/privacy}", "received_events_url": "https://api.github.com/users/jh-jeong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-26T16:52:43Z", "updated_at": "2018-05-14T19:12:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li>OS: Linux (Ubuntu 16.04)</li>\n<li>PyTorch version: 0.3.1</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 8.0 / 7</li>\n<li>GPU models and configuration: TITAN X (Pascal) x 8</li>\n<li>GCC version (if compiling from source): 5.4.0</li>\n</ul>\n<p>Hi,<br>\nIf we add scala to a cuda tensor, we typically expect a new cuda tensor on the same device:</p>\n<pre><code>&gt;&gt; torch.rand(5).cuda() + math.log(2)\n\n 1.6099\n 0.9674\n 0.9591\n 0.7931\n 1.1470\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\n\n&gt;&gt; math.log(2) + torch.rand(5).cuda()\n\n 0.7929\n 1.1955\n 0.7734\n 0.9757\n 1.0324\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\n</code></pre>\n<p>But with numpy scala, the things behave differently:</p>\n<pre><code>&gt;&gt; torch.rand(5).cuda() + math.log(2)\n\n 1.6099\n 0.9674\n 0.9591\n 0.7931\n 1.1470\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\n\n&gt;&gt; np.log(2) + torch.rand(5).cuda()\n\n 1.5042\n 1.6458\n 0.8886\n 1.3921\n 0.9682\n[torch.FloatTensor of size 5]\n</code></pre>\n<p>As we can see, the last line returned a non-cuda tensor, which seems to be inconsistent with the other equivalent lines. Is that an intended behavior?</p>", "body_text": "OS: Linux (Ubuntu 16.04)\nPyTorch version: 0.3.1\nHow you installed PyTorch (conda, pip, source): conda\nPython version: 3.6\nCUDA/cuDNN version: 8.0 / 7\nGPU models and configuration: TITAN X (Pascal) x 8\nGCC version (if compiling from source): 5.4.0\n\nHi,\nIf we add scala to a cuda tensor, we typically expect a new cuda tensor on the same device:\n>> torch.rand(5).cuda() + math.log(2)\n\n 1.6099\n 0.9674\n 0.9591\n 0.7931\n 1.1470\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\n\n>> math.log(2) + torch.rand(5).cuda()\n\n 0.7929\n 1.1955\n 0.7734\n 0.9757\n 1.0324\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\n\nBut with numpy scala, the things behave differently:\n>> torch.rand(5).cuda() + math.log(2)\n\n 1.6099\n 0.9674\n 0.9591\n 0.7931\n 1.1470\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\n\n>> np.log(2) + torch.rand(5).cuda()\n\n 1.5042\n 1.6458\n 0.8886\n 1.3921\n 0.9682\n[torch.FloatTensor of size 5]\n\nAs we can see, the last line returned a non-cuda tensor, which seems to be inconsistent with the other equivalent lines. Is that an intended behavior?", "body": "- OS: Linux (Ubuntu 16.04)\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 8.0 / 7\r\n- GPU models and configuration: TITAN X (Pascal) x 8\r\n- GCC version (if compiling from source): 5.4.0\r\n\r\nHi, \r\nIf we add scala to a cuda tensor, we typically expect a new cuda tensor on the same device:\r\n```\r\n>> torch.rand(5).cuda() + math.log(2)\r\n\r\n 1.6099\r\n 0.9674\r\n 0.9591\r\n 0.7931\r\n 1.1470\r\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\r\n\r\n>> math.log(2) + torch.rand(5).cuda()\r\n\r\n 0.7929\r\n 1.1955\r\n 0.7734\r\n 0.9757\r\n 1.0324\r\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\r\n```\r\n\r\nBut with numpy scala, the things behave differently: \r\n```\r\n>> torch.rand(5).cuda() + math.log(2)\r\n\r\n 1.6099\r\n 0.9674\r\n 0.9591\r\n 0.7931\r\n 1.1470\r\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\r\n\r\n>> np.log(2) + torch.rand(5).cuda()\r\n\r\n 1.5042\r\n 1.6458\r\n 0.8886\r\n 1.3921\r\n 0.9682\r\n[torch.FloatTensor of size 5]\r\n```\r\n\r\nAs we can see, the last line returned a non-cuda tensor, which seems to be inconsistent with the other equivalent lines. Is that an intended behavior? \r\n\r\n\r\n"}