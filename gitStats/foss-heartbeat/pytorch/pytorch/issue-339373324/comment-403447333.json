{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/403447333", "html_url": "https://github.com/pytorch/pytorch/issues/9260#issuecomment-403447333", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9260", "id": 403447333, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzQ0NzMzMw==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-09T11:26:58Z", "updated_at": "2018-07-09T11:26:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So I've tried to stare down the <code>THPModule_fromDLPack</code> but it seems that it's too clever for me.<br>\nMy intuition is that the tensor should really hold on to a reference of the dlpack, but that's not what the above function does. I tried to add it manually:</p>\n<pre><code>import cupy\nimport torch.utils.dlpack\nimport torch\n\nclass ManagedTensor(torch.Tensor):\n    def __new__(cls, dlpack=None, requires_grad=False):\n        data = torch.utils.dlpack.from_dlpack(dlpack)\n        t = torch.Tensor._make_subclass(cls, data, requires_grad)\n        t.dlpack = dlpack\n        t.dlpackdata = data\n        return t\n\n\nfor i in range(10000):\n   l = torch.nn.Linear(1000,1000).cuda()\n   b = l(torch.randn(1000,1000, device='cuda'))\n   a = ManagedTensor(cupy.random.randn(1000,1000, dtype=cupy.float32).toDlpack())\n   b.backward(a)\n</code></pre>\n<p>This takes away the segfault, seems to consume constant memory and run. But I haven't tested any further.<br>\nStill I wonder whether holding on to the dlpack reference might be a good strategy to implement at the C level.</p>\n<p>Best regards</p>\n<p>Thomas</p>", "body_text": "So I've tried to stare down the THPModule_fromDLPack but it seems that it's too clever for me.\nMy intuition is that the tensor should really hold on to a reference of the dlpack, but that's not what the above function does. I tried to add it manually:\nimport cupy\nimport torch.utils.dlpack\nimport torch\n\nclass ManagedTensor(torch.Tensor):\n    def __new__(cls, dlpack=None, requires_grad=False):\n        data = torch.utils.dlpack.from_dlpack(dlpack)\n        t = torch.Tensor._make_subclass(cls, data, requires_grad)\n        t.dlpack = dlpack\n        t.dlpackdata = data\n        return t\n\n\nfor i in range(10000):\n   l = torch.nn.Linear(1000,1000).cuda()\n   b = l(torch.randn(1000,1000, device='cuda'))\n   a = ManagedTensor(cupy.random.randn(1000,1000, dtype=cupy.float32).toDlpack())\n   b.backward(a)\n\nThis takes away the segfault, seems to consume constant memory and run. But I haven't tested any further.\nStill I wonder whether holding on to the dlpack reference might be a good strategy to implement at the C level.\nBest regards\nThomas", "body": "So I've tried to stare down the `THPModule_fromDLPack` but it seems that it's too clever for me.\r\nMy intuition is that the tensor should really hold on to a reference of the dlpack, but that's not what the above function does. I tried to add it manually:\r\n```\r\nimport cupy\r\nimport torch.utils.dlpack\r\nimport torch\r\n\r\nclass ManagedTensor(torch.Tensor):\r\n    def __new__(cls, dlpack=None, requires_grad=False):\r\n        data = torch.utils.dlpack.from_dlpack(dlpack)\r\n        t = torch.Tensor._make_subclass(cls, data, requires_grad)\r\n        t.dlpack = dlpack\r\n        t.dlpackdata = data\r\n        return t\r\n\r\n\r\nfor i in range(10000):\r\n   l = torch.nn.Linear(1000,1000).cuda()\r\n   b = l(torch.randn(1000,1000, device='cuda'))\r\n   a = ManagedTensor(cupy.random.randn(1000,1000, dtype=cupy.float32).toDlpack())\r\n   b.backward(a)\r\n```\r\nThis takes away the segfault, seems to consume constant memory and run. But I haven't tested any further.\r\nStill I wonder whether holding on to the dlpack reference might be a good strategy to implement at the C level.\r\n\r\nBest regards\r\n\r\nThomas"}