{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9260", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9260/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9260/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9260/events", "html_url": "https://github.com/pytorch/pytorch/issues/9260", "id": 339373324, "node_id": "MDU6SXNzdWUzMzkzNzMzMjQ=", "number": 9260, "title": "Segfault in CuPy -> dlpack -> PyTorch", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-07-09T09:31:14Z", "updated_at": "2018-07-14T14:47:24Z", "closed_at": "2018-07-09T21:12:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi,</p>\n<p>I'm not sure whether this is a PyTorch or a CuPy issue, but</p>\n<pre><code>import cupy\nimport torch.utils.dlpack\n\nfor i in range(100):\n   l = torch.nn.Linear(1000,1000).cuda()\n   b = l(torch.randn(1000,1000, device='cuda'))\n   a = torch.utils.dlpack.from_dlpack(cupy.random.randn(1000,1000, dtype=cupy.float32).toDlpack())\n   b.backward(a)\n</code></pre>\n<p>seems to sefault for me relative reliably (python 3.6, PyTorch recent master, CuPy recent master, both built with gcc 5).<br>\nI'm not entirely sue whether the module and backward is strictly necessary, but I got the repro down from 1000 to less then 10 lines already and it's relatively quick to fail.</p>\n<p>It could be a refcounting issue. Stacktrace:</p>\n<pre><code>0  0x0000000000000000 in ?? ()\n#1  0x00007fff8df01fec in at::call_deleter () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#2  0x00007fff8dff3c8f in THCStorage_free () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#3  0x00007fff8e00e3c5 in THCTensor_free () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#4  0x00007fff8df048b9 in at::CUDAFloatTensor::~CUDAFloatTensor() () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#5  0x00007fff9d053782 in at::Retainable::release (this=&lt;optimized out&gt;) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Retainable.h:16\n#6  at::detail::TensorBase::~TensorBase (this=0x3b7d5808, __in_chrg=&lt;optimized out&gt;) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:30\n#7  at::Tensor::~Tensor (this=0x3b7d5808, __in_chrg=&lt;optimized out&gt;) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Tensor.h:44\n#8  torch::autograd::Variable::Impl::~Impl (this=0x3b7d57d0, __in_chrg=&lt;optimized out&gt;) at /home/tv/pytorch/pytorch/torch/csrc/autograd/variable.h:282\n#9  0x00007fff9d0538b9 in torch::autograd::Variable::Impl::~Impl (this=0x3b7d57d0, __in_chrg=&lt;optimized out&gt;) at /home/tv/pytorch/pytorch/torch/csrc/autograd/variable.h:282\n#10 0x00007fff9d06a54e in at::Retainable::release (this=&lt;optimized out&gt;) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Retainable.h:16\n#11 at::detail::TensorBase::~TensorBase (this=&lt;optimized out&gt;, __in_chrg=&lt;optimized out&gt;) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:30\n#12 at::detail::TensorBase::reset (this=0x7fff827ad640) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:47\n#13 THPVariable_clear (self=self@entry=0x7fff827ad630) at torch/csrc/autograd/python_variable.cpp:114\n#14 0x00007fff9d06a631 in THPVariable_dealloc (self=0x7fff827ad630) at torch/csrc/autograd/python_variable.cpp:121\n#15 0x00000000004d8395 in subtype_dealloc (self=&lt;Tensor at remote 0x7fff827ad630&gt;) at ../Objects/typeobject.c:1222\nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \n#16 0x00000000004b5e55 in insertdict (value=&lt;Tensor at remote 0x7ffff69ebaf8&gt;, hash=-7244025631456083716, key=, mp=0x7ffff6b8c2d0) at ../Objects/dictobject.c:1181\nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \n#17 PyDict_SetItem (op=, key=, value=&lt;Tensor at remote 0x7ffff69ebaf8&gt;) at ../Objects/dictobject.c:1575\n#18 0x0000000000557859 in _PyEval_EvalFrameDefault (f=&lt;optimized out&gt;, throwflag=&lt;optimized out&gt;) at ../Python/ceval.c:2223\nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \n#19 0x000000000054fbe1 in PyEval_EvalFrameEx (throwflag=0, f=) at ../Python/ceval.c:753\n#20 _PyEval_EvalCodeWithName (_co=_co@entry=&lt;code at remote 0x7ffff6b48810&gt;, globals=globals@entry=&lt;code at remote 0x7ffff6b48810&gt;, \n    locals=locals@entry=&lt;error reading variable: Cannot access memory at address 0x3&gt;, args=args@entry=0x0, argcount=argcount@entry=0, kwnames=kwnames@entry=0x0, kwargs=0x0, kwcount=0, kwstep=2, defs=0x0, \n    defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at ../Python/ceval.c:4153\n#21 0x0000000000550b93 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \n    locals=locals@entry=&lt;error reading variable: Cannot access memory at address 0x3&gt;, globals=globals@entry=&lt;code at remote 0x7ffff6b48810&gt;, _co=_co@entry=&lt;code at remote 0x7ffff6b48810&gt;)\n    at ../Python/ceval.c:4174\nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \n#22 PyEval_EvalCode (co=co@entry=&lt;code at remote 0x7ffff6b48810&gt;, globals=globals@entry=, locals=locals@entry=) at ../Python/ceval.c:730\nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \n#23 0x000000000042b519 in run_mod (arena=0x7ffff6ba7270, flags=0x7fffffffe3cc, locals=, globals=, filename=, mod=0xb54778) at ../Python/pythonrun.c:1025\nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \nPython Exception &lt;class 'RuntimeError'&gt; Type does not have a target.: \n#24 PyRun_FileExFlags (fp=0xb3e8a0, filename_str=&lt;optimized out&gt;, start=&lt;optimized out&gt;, globals=, locals=, closeit=1, flags=0x7fffffffe3cc) at ../Python/pythonrun.c:978\n#25 0x000000000042b705 in PyRun_SimpleFileExFlags (fp=0xb3e8a0, filename=&lt;optimized out&gt;, closeit=1, flags=0x7fffffffe3cc) at ../Python/pythonrun.c:420\n#26 0x0000000000441fcb in run_file (p_cf=0x7fffffffe3cc, filename=0xa90650 L\"dlpack_fail.py\", fp=0xb3e8a0) at ../Modules/main.c:340\n#27 Py_Main (argc=argc@entry=2, argv=argv@entry=0xa8f260) at ../Modules/main.c:810\n#28 0x0000000000421ff4 in main (argc=2, argv=&lt;optimized out&gt;) at ../Programs/python.c:69\n</code></pre>\n<p>Best regards</p>\n<p>Thomas</p>", "body_text": "Hi,\nI'm not sure whether this is a PyTorch or a CuPy issue, but\nimport cupy\nimport torch.utils.dlpack\n\nfor i in range(100):\n   l = torch.nn.Linear(1000,1000).cuda()\n   b = l(torch.randn(1000,1000, device='cuda'))\n   a = torch.utils.dlpack.from_dlpack(cupy.random.randn(1000,1000, dtype=cupy.float32).toDlpack())\n   b.backward(a)\n\nseems to sefault for me relative reliably (python 3.6, PyTorch recent master, CuPy recent master, both built with gcc 5).\nI'm not entirely sue whether the module and backward is strictly necessary, but I got the repro down from 1000 to less then 10 lines already and it's relatively quick to fail.\nIt could be a refcounting issue. Stacktrace:\n0  0x0000000000000000 in ?? ()\n#1  0x00007fff8df01fec in at::call_deleter () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#2  0x00007fff8dff3c8f in THCStorage_free () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#3  0x00007fff8e00e3c5 in THCTensor_free () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#4  0x00007fff8df048b9 in at::CUDAFloatTensor::~CUDAFloatTensor() () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\n#5  0x00007fff9d053782 in at::Retainable::release (this=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Retainable.h:16\n#6  at::detail::TensorBase::~TensorBase (this=0x3b7d5808, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:30\n#7  at::Tensor::~Tensor (this=0x3b7d5808, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Tensor.h:44\n#8  torch::autograd::Variable::Impl::~Impl (this=0x3b7d57d0, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/csrc/autograd/variable.h:282\n#9  0x00007fff9d0538b9 in torch::autograd::Variable::Impl::~Impl (this=0x3b7d57d0, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/csrc/autograd/variable.h:282\n#10 0x00007fff9d06a54e in at::Retainable::release (this=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Retainable.h:16\n#11 at::detail::TensorBase::~TensorBase (this=<optimized out>, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:30\n#12 at::detail::TensorBase::reset (this=0x7fff827ad640) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:47\n#13 THPVariable_clear (self=self@entry=0x7fff827ad630) at torch/csrc/autograd/python_variable.cpp:114\n#14 0x00007fff9d06a631 in THPVariable_dealloc (self=0x7fff827ad630) at torch/csrc/autograd/python_variable.cpp:121\n#15 0x00000000004d8395 in subtype_dealloc (self=<Tensor at remote 0x7fff827ad630>) at ../Objects/typeobject.c:1222\nPython Exception <class 'RuntimeError'> Type does not have a target.: \n#16 0x00000000004b5e55 in insertdict (value=<Tensor at remote 0x7ffff69ebaf8>, hash=-7244025631456083716, key=, mp=0x7ffff6b8c2d0) at ../Objects/dictobject.c:1181\nPython Exception <class 'RuntimeError'> Type does not have a target.: \nPython Exception <class 'RuntimeError'> Type does not have a target.: \n#17 PyDict_SetItem (op=, key=, value=<Tensor at remote 0x7ffff69ebaf8>) at ../Objects/dictobject.c:1575\n#18 0x0000000000557859 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at ../Python/ceval.c:2223\nPython Exception <class 'RuntimeError'> Type does not have a target.: \n#19 0x000000000054fbe1 in PyEval_EvalFrameEx (throwflag=0, f=) at ../Python/ceval.c:753\n#20 _PyEval_EvalCodeWithName (_co=_co@entry=<code at remote 0x7ffff6b48810>, globals=globals@entry=<code at remote 0x7ffff6b48810>, \n    locals=locals@entry=<error reading variable: Cannot access memory at address 0x3>, args=args@entry=0x0, argcount=argcount@entry=0, kwnames=kwnames@entry=0x0, kwargs=0x0, kwcount=0, kwstep=2, defs=0x0, \n    defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at ../Python/ceval.c:4153\n#21 0x0000000000550b93 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \n    locals=locals@entry=<error reading variable: Cannot access memory at address 0x3>, globals=globals@entry=<code at remote 0x7ffff6b48810>, _co=_co@entry=<code at remote 0x7ffff6b48810>)\n    at ../Python/ceval.c:4174\nPython Exception <class 'RuntimeError'> Type does not have a target.: \nPython Exception <class 'RuntimeError'> Type does not have a target.: \n#22 PyEval_EvalCode (co=co@entry=<code at remote 0x7ffff6b48810>, globals=globals@entry=, locals=locals@entry=) at ../Python/ceval.c:730\nPython Exception <class 'RuntimeError'> Type does not have a target.: \nPython Exception <class 'RuntimeError'> Type does not have a target.: \nPython Exception <class 'RuntimeError'> Type does not have a target.: \n#23 0x000000000042b519 in run_mod (arena=0x7ffff6ba7270, flags=0x7fffffffe3cc, locals=, globals=, filename=, mod=0xb54778) at ../Python/pythonrun.c:1025\nPython Exception <class 'RuntimeError'> Type does not have a target.: \nPython Exception <class 'RuntimeError'> Type does not have a target.: \n#24 PyRun_FileExFlags (fp=0xb3e8a0, filename_str=<optimized out>, start=<optimized out>, globals=, locals=, closeit=1, flags=0x7fffffffe3cc) at ../Python/pythonrun.c:978\n#25 0x000000000042b705 in PyRun_SimpleFileExFlags (fp=0xb3e8a0, filename=<optimized out>, closeit=1, flags=0x7fffffffe3cc) at ../Python/pythonrun.c:420\n#26 0x0000000000441fcb in run_file (p_cf=0x7fffffffe3cc, filename=0xa90650 L\"dlpack_fail.py\", fp=0xb3e8a0) at ../Modules/main.c:340\n#27 Py_Main (argc=argc@entry=2, argv=argv@entry=0xa8f260) at ../Modules/main.c:810\n#28 0x0000000000421ff4 in main (argc=2, argv=<optimized out>) at ../Programs/python.c:69\n\nBest regards\nThomas", "body": "Hi,\r\n\r\nI'm not sure whether this is a PyTorch or a CuPy issue, but\r\n```\r\nimport cupy\r\nimport torch.utils.dlpack\r\n\r\nfor i in range(100):\r\n   l = torch.nn.Linear(1000,1000).cuda()\r\n   b = l(torch.randn(1000,1000, device='cuda'))\r\n   a = torch.utils.dlpack.from_dlpack(cupy.random.randn(1000,1000, dtype=cupy.float32).toDlpack())\r\n   b.backward(a)\r\n```\r\nseems to sefault for me relative reliably (python 3.6, PyTorch recent master, CuPy recent master, both built with gcc 5).\r\nI'm not entirely sue whether the module and backward is strictly necessary, but I got the repro down from 1000 to less then 10 lines already and it's relatively quick to fail.\r\n\r\nIt could be a refcounting issue. Stacktrace:\r\n```\r\n0  0x0000000000000000 in ?? ()\r\n#1  0x00007fff8df01fec in at::call_deleter () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\r\n#2  0x00007fff8dff3c8f in THCStorage_free () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\r\n#3  0x00007fff8e00e3c5 in THCTensor_free () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\r\n#4  0x00007fff8df048b9 in at::CUDAFloatTensor::~CUDAFloatTensor() () from /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2_gpu.so\r\n#5  0x00007fff9d053782 in at::Retainable::release (this=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Retainable.h:16\r\n#6  at::detail::TensorBase::~TensorBase (this=0x3b7d5808, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:30\r\n#7  at::Tensor::~Tensor (this=0x3b7d5808, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Tensor.h:44\r\n#8  torch::autograd::Variable::Impl::~Impl (this=0x3b7d57d0, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/csrc/autograd/variable.h:282\r\n#9  0x00007fff9d0538b9 in torch::autograd::Variable::Impl::~Impl (this=0x3b7d57d0, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/csrc/autograd/variable.h:282\r\n#10 0x00007fff9d06a54e in at::Retainable::release (this=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/Retainable.h:16\r\n#11 at::detail::TensorBase::~TensorBase (this=<optimized out>, __in_chrg=<optimized out>) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:30\r\n#12 at::detail::TensorBase::reset (this=0x7fff827ad640) at /home/tv/pytorch/pytorch/torch/lib/tmp_install/include/ATen/TensorBase.h:47\r\n#13 THPVariable_clear (self=self@entry=0x7fff827ad630) at torch/csrc/autograd/python_variable.cpp:114\r\n#14 0x00007fff9d06a631 in THPVariable_dealloc (self=0x7fff827ad630) at torch/csrc/autograd/python_variable.cpp:121\r\n#15 0x00000000004d8395 in subtype_dealloc (self=<Tensor at remote 0x7fff827ad630>) at ../Objects/typeobject.c:1222\r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\n#16 0x00000000004b5e55 in insertdict (value=<Tensor at remote 0x7ffff69ebaf8>, hash=-7244025631456083716, key=, mp=0x7ffff6b8c2d0) at ../Objects/dictobject.c:1181\r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\n#17 PyDict_SetItem (op=, key=, value=<Tensor at remote 0x7ffff69ebaf8>) at ../Objects/dictobject.c:1575\r\n#18 0x0000000000557859 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at ../Python/ceval.c:2223\r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\n#19 0x000000000054fbe1 in PyEval_EvalFrameEx (throwflag=0, f=) at ../Python/ceval.c:753\r\n#20 _PyEval_EvalCodeWithName (_co=_co@entry=<code at remote 0x7ffff6b48810>, globals=globals@entry=<code at remote 0x7ffff6b48810>, \r\n    locals=locals@entry=<error reading variable: Cannot access memory at address 0x3>, args=args@entry=0x0, argcount=argcount@entry=0, kwnames=kwnames@entry=0x0, kwargs=0x0, kwcount=0, kwstep=2, defs=0x0, \r\n    defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at ../Python/ceval.c:4153\r\n#21 0x0000000000550b93 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, \r\n    locals=locals@entry=<error reading variable: Cannot access memory at address 0x3>, globals=globals@entry=<code at remote 0x7ffff6b48810>, _co=_co@entry=<code at remote 0x7ffff6b48810>)\r\n    at ../Python/ceval.c:4174\r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\n#22 PyEval_EvalCode (co=co@entry=<code at remote 0x7ffff6b48810>, globals=globals@entry=, locals=locals@entry=) at ../Python/ceval.c:730\r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\n#23 0x000000000042b519 in run_mod (arena=0x7ffff6ba7270, flags=0x7fffffffe3cc, locals=, globals=, filename=, mod=0xb54778) at ../Python/pythonrun.c:1025\r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\nPython Exception <class 'RuntimeError'> Type does not have a target.: \r\n#24 PyRun_FileExFlags (fp=0xb3e8a0, filename_str=<optimized out>, start=<optimized out>, globals=, locals=, closeit=1, flags=0x7fffffffe3cc) at ../Python/pythonrun.c:978\r\n#25 0x000000000042b705 in PyRun_SimpleFileExFlags (fp=0xb3e8a0, filename=<optimized out>, closeit=1, flags=0x7fffffffe3cc) at ../Python/pythonrun.c:420\r\n#26 0x0000000000441fcb in run_file (p_cf=0x7fffffffe3cc, filename=0xa90650 L\"dlpack_fail.py\", fp=0xb3e8a0) at ../Modules/main.c:340\r\n#27 Py_Main (argc=argc@entry=2, argv=argv@entry=0xa8f260) at ../Modules/main.c:810\r\n#28 0x0000000000421ff4 in main (argc=2, argv=<optimized out>) at ../Programs/python.c:69\r\n```\r\n\r\nBest regards\r\n\r\nThomas"}