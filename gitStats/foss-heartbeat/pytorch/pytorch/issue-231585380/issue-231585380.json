{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1656", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1656/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1656/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1656/events", "html_url": "https://github.com/pytorch/pytorch/issues/1656", "id": 231585380, "node_id": "MDU6SXNzdWUyMzE1ODUzODA=", "number": 1656, "title": "GPUs crashed when running more than one parallel training processes on the same bunch of GPUs", "user": {"login": "dontloo", "id": 4488555, "node_id": "MDQ6VXNlcjQ0ODg1NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/4488555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dontloo", "html_url": "https://github.com/dontloo", "followers_url": "https://api.github.com/users/dontloo/followers", "following_url": "https://api.github.com/users/dontloo/following{/other_user}", "gists_url": "https://api.github.com/users/dontloo/gists{/gist_id}", "starred_url": "https://api.github.com/users/dontloo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dontloo/subscriptions", "organizations_url": "https://api.github.com/users/dontloo/orgs", "repos_url": "https://api.github.com/users/dontloo/repos", "events_url": "https://api.github.com/users/dontloo/events{/privacy}", "received_events_url": "https://api.github.com/users/dontloo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-26T10:19:35Z", "updated_at": "2017-11-02T15:43:06Z", "closed_at": "2017-11-02T15:42:32Z", "author_association": "NONE", "body_html": "<p>I accidentally ran two parallel training processes on the same bunch of GPUs, then those GPUs stopped responding and I could not <code>sudo kill</code> the training processes neither. How can I solve this?</p>\n<p>Many thanks.</p>\n<p>Here's the result from <code>nvidia-smi</code>, as it shows GPU 2 and 3 (which I ran the training processes on) are of 100% usage and I was not able to terminate the processes.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/4488555/28774061-86cc7e42-761e-11e7-8f3d-b931f118bb75.png\"><img src=\"https://user-images.githubusercontent.com/4488555/28774061-86cc7e42-761e-11e7-8f3d-b931f118bb75.png\" alt=\"screenshot from 2017-07-31 18 02 09\" style=\"max-width:100%;\"></a></p>", "body_text": "I accidentally ran two parallel training processes on the same bunch of GPUs, then those GPUs stopped responding and I could not sudo kill the training processes neither. How can I solve this?\nMany thanks.\nHere's the result from nvidia-smi, as it shows GPU 2 and 3 (which I ran the training processes on) are of 100% usage and I was not able to terminate the processes.", "body": "I accidentally ran two parallel training processes on the same bunch of GPUs, then those GPUs stopped responding and I could not `sudo kill` the training processes neither. How can I solve this? \r\n\r\nMany thanks.\r\n\r\nHere's the result from `nvidia-smi`, as it shows GPU 2 and 3 (which I ran the training processes on) are of 100% usage and I was not able to terminate the processes.\r\n![screenshot from 2017-07-31 18 02 09](https://user-images.githubusercontent.com/4488555/28774061-86cc7e42-761e-11e7-8f3d-b931f118bb75.png)\r\n\r\n"}