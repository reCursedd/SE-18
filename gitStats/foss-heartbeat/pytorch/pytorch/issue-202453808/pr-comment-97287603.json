{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/97287603", "pull_request_review_id": 17891112, "id": 97287603, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk3Mjg3NjAz", "diff_hunk": "@@ -225,6 +225,14 @@ void cudnn_convolution_forward(\n   size_t workspaceSize;\n   CHECK(cudnnGetConvolutionForwardWorkspaceSize(handle, idesc.desc, wdesc.desc,\n       cdesc.desc, odesc.desc, fwdAlg, &workspaceSize));\n+  size_t memFree;\n+  size_t memTotal;\n+  cudaMemGetInfo(&memFree, &memTotal);", "path": "torch/csrc/cudnn/Conv.cpp", "position": 6, "original_position": 6, "commit_id": "bcaceb4a8a692d28e2aa5e93802e98776d10d659", "original_commit_id": "bcaceb4a8a692d28e2aa5e93802e98776d10d659", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "It's a good point, yeah we should. There are some cases where free memory reported by the driver will be nearly 0, but the caching allocator will have plenty of free blocks to use.", "created_at": "2017-01-23T10:19:11Z", "updated_at": "2018-11-23T15:32:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/552#discussion_r97287603", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/552", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/97287603"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/552#discussion_r97287603"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/552"}}, "body_html": "<p>It's a good point, yeah we should. There are some cases where free memory reported by the driver will be nearly 0, but the caching allocator will have plenty of free blocks to use.</p>", "body_text": "It's a good point, yeah we should. There are some cases where free memory reported by the driver will be nearly 0, but the caching allocator will have plenty of free blocks to use.", "in_reply_to_id": 97278080}