{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/142271706", "pull_request_review_id": 66610070, "id": 142271706, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MjI3MTcwNg==", "diff_hunk": "@@ -331,14 +403,30 @@ def unpack_args():\n         body.extend(unpack_args())\n \n         i = 0\n+        added_derivative_tensor = False\n+        added_derivative_tensorlist = False\n         for arg in op['python_arguments']:\n             derivative = arg.get('derivative')\n             if derivative is None:\n                 continue\n-            body.append(DERIVATIVE.substitute({\n-                'i': i,\n-                'derivative': derivative,\n-            }))\n+\n+            if arg['type'] == 'TensorList':\n+                error_msg = \"derivatives don't support specifying both a TensorList and non-TensorList derivative yet\"", "path": "tools/autograd/gen_variable_type.py", "position": null, "original_position": 184, "commit_id": "73bdc946c7aafab133b3b7932e2687f502d1c579", "original_commit_id": "7ce054aea3dfcec98c6f2b8b79a50db9bf3a4bf3", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "Our DERIVATIVE_TENSORLIST template isn't smart enough yet; consider if you had a TensorList \"tl\" and Tensor \"t\" argument in that order.  The return value of the tl backward function needs to be assigned to the first tl.size() grad inputs (missing nice python slice syntax).  So instead we just assign the return to the entire grad_inputs variable_list.  Since this is never actually used (at least yet), punting made the most sense.", "created_at": "2017-10-02T22:25:17Z", "updated_at": "2018-11-23T15:34:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/2936#discussion_r142271706", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2936", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/142271706"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2936#discussion_r142271706"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2936"}}, "body_html": "<p>Our DERIVATIVE_TENSORLIST template isn't smart enough yet; consider if you had a TensorList \"tl\" and Tensor \"t\" argument in that order.  The return value of the tl backward function needs to be assigned to the first tl.size() grad inputs (missing nice python slice syntax).  So instead we just assign the return to the entire grad_inputs variable_list.  Since this is never actually used (at least yet), punting made the most sense.</p>", "body_text": "Our DERIVATIVE_TENSORLIST template isn't smart enough yet; consider if you had a TensorList \"tl\" and Tensor \"t\" argument in that order.  The return value of the tl backward function needs to be assigned to the first tl.size() grad inputs (missing nice python slice syntax).  So instead we just assign the return to the entire grad_inputs variable_list.  Since this is never actually used (at least yet), punting made the most sense.", "in_reply_to_id": 142263426}