{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/142270405", "pull_request_review_id": 66608604, "id": 142270405, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MjI3MDQwNQ==", "diff_hunk": "@@ -258,9 +297,31 @@ def get_type(arg_name):\n                         sizes_name = arg['name'] + '_sizes'\n                         formula = formula.replace(size_str, sizes_name)\n \n+                    # If x is a TensorList, turn x.sizes(y) into x_argsizes_y\n+                    def argsizes_repl(matchobj):\n+                        if arg['type'] != 'TensorList':\n+                            raise RuntimeError(\"sizes(argument) only supported on TensorList\")\n+                        argsizes_name = arg['name'] + \"_argsizes_\" + matchobj.group(1)\n+                        arg_sizes_found.append(argsizes_name + \".size()\")\n+                        return argsizes_name\n+                    formula = re.sub(arg['name'] + r\".sizes\\((\\w+)\\)\", argsizes_repl, formula)\n+\n+                    # If x is a Tensor, turn x.size(y) into x_argsize_y\n+                    def argsize_repl(matchobj):\n+                        if arg['type'] != 'Tensor':\n+                            raise RuntimeError(\"size(argument) only supported on Tensor\")\n+                        argsize_name = arg['name'] + \"_argsize_\" + matchobj.group(1)\n+                        return argsize_name\n+                    formula = re.sub(arg['name'] + r\".size\\((\\w+)\\)\", argsize_repl, formula)\n+\n                 derivatives.append(formula)\n                 arg['derivative'] = formula\n-                option['num_inputs'] += 1\n+                if arg['type'] != \"TensorList\":\n+                    option['num_inputs'] += 1\n+\n+        if arg_sizes_found:\n+            option['num_inputs'] = (\"+\".join(arg_sizes_found) +\n+                                    \"\" if option['num_inputs'] == 0 else \" + \" + str(option['num_inputs']))", "path": "tools/autograd/gen_variable_type.py", "position": null, "original_position": 146, "commit_id": "73bdc946c7aafab133b3b7932e2687f502d1c579", "original_commit_id": "7ce054aea3dfcec98c6f2b8b79a50db9bf3a4bf3", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "it is weird, but it's just dumped to code at the end anyway.  I'll just separate out the integer and string variables.", "created_at": "2017-10-02T22:18:02Z", "updated_at": "2018-11-23T15:34:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/2936#discussion_r142270405", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2936", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/142270405"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2936#discussion_r142270405"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2936"}}, "body_html": "<p>it is weird, but it's just dumped to code at the end anyway.  I'll just separate out the integer and string variables.</p>", "body_text": "it is weird, but it's just dumped to code at the end anyway.  I'll just separate out the integer and string variables.", "in_reply_to_id": 142263272}