{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434063646", "html_url": "https://github.com/pytorch/pytorch/pull/13137#issuecomment-434063646", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13137", "id": 434063646, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDA2MzY0Ng==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-29T20:19:20Z", "updated_at": "2018-10-29T20:19:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A few comments:</p>\n<ol>\n<li>This seems really brittle. In particular, if someone slightly changes the implementation of this test so that it no longer triggers timeouts, all the timeouts will come back, and it will be hard to tell when this occurred (because it's flaky.)</li>\n<li>I don't really like this strategy, because it's not really solving the root problem, it's just papering over the flakiness. I'd much rather prefer a solution that could have <em>plausibly</em> solved the root cause, even if we don't know exactly what the root cause is. (For that reason, I, for example, prefer solutions where we swap out Travis Python with different binary. Because maybe the root cause is the binary distribution.)</li>\n<li>You need to test if infinite retries occur. You're not checking for this right now, and I really don't want a PR that causes all tests to hang to cause CPU tests to be spinning on repeated retries.</li>\n</ol>", "body_text": "A few comments:\n\nThis seems really brittle. In particular, if someone slightly changes the implementation of this test so that it no longer triggers timeouts, all the timeouts will come back, and it will be hard to tell when this occurred (because it's flaky.)\nI don't really like this strategy, because it's not really solving the root problem, it's just papering over the flakiness. I'd much rather prefer a solution that could have plausibly solved the root cause, even if we don't know exactly what the root cause is. (For that reason, I, for example, prefer solutions where we swap out Travis Python with different binary. Because maybe the root cause is the binary distribution.)\nYou need to test if infinite retries occur. You're not checking for this right now, and I really don't want a PR that causes all tests to hang to cause CPU tests to be spinning on repeated retries.", "body": "A few comments:\r\n\r\n1. This seems really brittle. In particular, if someone slightly changes the implementation of this test so that it no longer triggers timeouts, all the timeouts will come back, and it will be hard to tell when this occurred (because it's flaky.)\r\n2. I don't really like this strategy, because it's not really solving the root problem, it's just papering over the flakiness. I'd much rather prefer a solution that could have *plausibly* solved the root cause, even if we don't know exactly what the root cause is. (For that reason, I, for example, prefer solutions where we swap out Travis Python with different binary. Because maybe the root cause is the binary distribution.)\r\n3. You need to test if infinite retries occur. You're not checking for this right now, and I really don't want a PR that causes all tests to hang to cause CPU tests to be spinning on repeated retries."}