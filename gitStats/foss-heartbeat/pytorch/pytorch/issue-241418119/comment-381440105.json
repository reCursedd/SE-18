{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/381440105", "html_url": "https://github.com/pytorch/pytorch/pull/2016#issuecomment-381440105", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2016", "id": 381440105, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTQ0MDEwNQ==", "user": {"login": "sean-adler", "id": 1624971, "node_id": "MDQ6VXNlcjE2MjQ5NzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1624971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sean-adler", "html_url": "https://github.com/sean-adler", "followers_url": "https://api.github.com/users/sean-adler/followers", "following_url": "https://api.github.com/users/sean-adler/following{/other_user}", "gists_url": "https://api.github.com/users/sean-adler/gists{/gist_id}", "starred_url": "https://api.github.com/users/sean-adler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sean-adler/subscriptions", "organizations_url": "https://api.github.com/users/sean-adler/orgs", "repos_url": "https://api.github.com/users/sean-adler/repos", "events_url": "https://api.github.com/users/sean-adler/events{/privacy}", "received_events_url": "https://api.github.com/users/sean-adler/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-15T21:38:47Z", "updated_at": "2018-04-15T21:38:47Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>You are right -- some scheduling methods require a different calling frequency. The main discrepancy would be the timing for <code>.step()</code> and the name of some methods/attributes/parameters. It might be helpful if there is another abstract class for batch-wise scheduler. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Any suggestion?</p>\n</blockquote>\n<p>Sorry to ask a newbie question - is it necessary to have a different API for updating the learning rate per-batch rather than per-epoch?</p>\n<p>If <code>CyclicLR</code> were rewritten to subclass <code>_LRScheduler</code> and implement <code>step()</code> instead of <code>batch_step()</code>, it kind of seems like doing this would be fine:</p>\n<pre><code>for epoch in range(num_epochs):\n    for batch in data_loader:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output, target)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()  #  &lt;- apply CLR per-batch\n</code></pre>\n<p>Because <code>step_size</code> is an argument to <code>CyclicLR</code> in this changeset, making cycles follow a schedule similar to those in <a href=\"https://arxiv.org/pdf/1506.01186.pdf\" rel=\"nofollow\">the paper</a> by e.g. setting <code>step_size</code> to a multiple of <code>len(data_loader)</code> seems like it'd make the above work (and be reasonably straightforward).</p>\n<p>(One downside of doing this is that <code>CyclicLR</code> would have a misleading <code>last_epoch</code> attribute, but I have no idea how big a deal that is.)</p>\n<p>I have very little pytorch-specific context here, so any feedback would be hugely appreciated.</p>", "body_text": "You are right -- some scheduling methods require a different calling frequency. The main discrepancy would be the timing for .step() and the name of some methods/attributes/parameters. It might be helpful if there is another abstract class for batch-wise scheduler. @apaszke Any suggestion?\n\nSorry to ask a newbie question - is it necessary to have a different API for updating the learning rate per-batch rather than per-epoch?\nIf CyclicLR were rewritten to subclass _LRScheduler and implement step() instead of batch_step(), it kind of seems like doing this would be fine:\nfor epoch in range(num_epochs):\n    for batch in data_loader:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output, target)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()  #  <- apply CLR per-batch\n\nBecause step_size is an argument to CyclicLR in this changeset, making cycles follow a schedule similar to those in the paper by e.g. setting step_size to a multiple of len(data_loader) seems like it'd make the above work (and be reasonably straightforward).\n(One downside of doing this is that CyclicLR would have a misleading last_epoch attribute, but I have no idea how big a deal that is.)\nI have very little pytorch-specific context here, so any feedback would be hugely appreciated.", "body": "> You are right -- some scheduling methods require a different calling frequency. The main discrepancy would be the timing for `.step()` and the name of some methods/attributes/parameters. It might be helpful if there is another abstract class for batch-wise scheduler. @apaszke Any suggestion?\r\n\r\nSorry to ask a newbie question - is it necessary to have a different API for updating the learning rate per-batch rather than per-epoch?\r\n\r\nIf `CyclicLR` were rewritten to subclass `_LRScheduler` and implement `step()` instead of `batch_step()`, it kind of seems like doing this would be fine:\r\n\r\n```\r\nfor epoch in range(num_epochs):\r\n    for batch in data_loader:\r\n        optimizer.zero_grad()\r\n        output = model(input)\r\n        loss = criterion(output, target)\r\n        loss.backward()\r\n\r\n        optimizer.step()\r\n        scheduler.step()  #  <- apply CLR per-batch\r\n```\r\n\r\nBecause `step_size` is an argument to `CyclicLR` in this changeset, making cycles follow a schedule similar to those in [the paper](https://arxiv.org/pdf/1506.01186.pdf) by e.g. setting `step_size` to a multiple of `len(data_loader)` seems like it'd make the above work (and be reasonably straightforward).\r\n\r\n(One downside of doing this is that `CyclicLR` would have a misleading `last_epoch` attribute, but I have no idea how big a deal that is.)\r\n\r\nI have very little pytorch-specific context here, so any feedback would be hugely appreciated."}