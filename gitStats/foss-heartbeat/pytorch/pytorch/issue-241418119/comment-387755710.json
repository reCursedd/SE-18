{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/387755710", "html_url": "https://github.com/pytorch/pytorch/pull/2016#issuecomment-387755710", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2016", "id": 387755710, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Nzc1NTcxMA==", "user": {"login": "AutuanLiu", "id": 15994006, "node_id": "MDQ6VXNlcjE1OTk0MDA2", "avatar_url": "https://avatars2.githubusercontent.com/u/15994006?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AutuanLiu", "html_url": "https://github.com/AutuanLiu", "followers_url": "https://api.github.com/users/AutuanLiu/followers", "following_url": "https://api.github.com/users/AutuanLiu/following{/other_user}", "gists_url": "https://api.github.com/users/AutuanLiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/AutuanLiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AutuanLiu/subscriptions", "organizations_url": "https://api.github.com/users/AutuanLiu/orgs", "repos_url": "https://api.github.com/users/AutuanLiu/repos", "events_url": "https://api.github.com/users/AutuanLiu/events{/privacy}", "received_events_url": "https://api.github.com/users/AutuanLiu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T14:22:19Z", "updated_at": "2018-05-09T14:27:23Z", "author_association": "NONE", "body_html": "<ol>\n<li>we can use <a href=\"https://github.com/AutuanLiu/pytorch/blob/master/torch/optim/lr_scheduler.py\">lr_scheduler.LambdaLR</a> to do this, here are the examples</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">cyclical_lr</span>(<span class=\"pl-smi\">step_sz</span>, <span class=\"pl-smi\">min_lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>, <span class=\"pl-smi\">max_lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>triangular<span class=\"pl-pds\">'</span></span>, <span class=\"pl-smi\">scale_func</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">scale_md</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cycles<span class=\"pl-pds\">'</span></span>, <span class=\"pl-smi\">gamma</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>.):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>implements a cyclical learning rate policy (CLR).</span>\n<span class=\"pl-s\">    Notes: the learning rate of optimizer should be 1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Parameters:</span>\n<span class=\"pl-s\">    ----------</span>\n<span class=\"pl-s\">    mode : str, optional</span>\n<span class=\"pl-s\">        one of {triangular, triangular2, exp_range}. </span>\n<span class=\"pl-s\">    scale_md : str, optional</span>\n<span class=\"pl-s\">        {'cycles', 'iterations'}.</span>\n<span class=\"pl-s\">    gamma : float, optional</span>\n<span class=\"pl-s\">        constant in 'exp_range' scaling function: gamma**(cycle iterations)</span>\n<span class=\"pl-s\">    </span>\n<span class=\"pl-s\">    Examples:</span>\n<span class=\"pl-s\">    --------</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span># the learning rate of optimizer should be 1</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>optimizer = optim.SGD(model.parameters(), lr=1.)</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>step_size = 2*len(train_loader)</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>clr = cyclical_lr(step_size, min_lr=0.001, max_lr=0.005)</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>scheduler = lr_scheduler.LambdaLR(optimizer, [clr])</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span># some other operations</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>scheduler.step()</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>optimizer.step()</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">if</span> scale_func <span class=\"pl-k\">==</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>triangular<span class=\"pl-pds\">'</span></span>:\n            scale_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: <span class=\"pl-c1\">1</span>.\n            scale_mode <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cycles<span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-k\">elif</span> mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>triangular2<span class=\"pl-pds\">'</span></span>:\n            scale_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: <span class=\"pl-c1\">1</span> <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">2</span>.<span class=\"pl-k\">**</span>(x <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>))\n            scale_mode <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cycles<span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-k\">elif</span> mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>exp_range<span class=\"pl-pds\">'</span></span>:\n            scale_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: gamma<span class=\"pl-k\">**</span>(x)\n            scale_mode <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>iterations<span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\">f</span><span class=\"pl-pds\">'</span><span class=\"pl-s\">The </span><span class=\"pl-c1\">{</span>mode<span class=\"pl-c1\">}</span><span class=\"pl-s\"> is not valid value!</span><span class=\"pl-pds\">'</span>)\n    <span class=\"pl-k\">else</span>:\n        scale_fn <span class=\"pl-k\">=</span> scale_func\n        scale_mode <span class=\"pl-k\">=</span> scale_md\n\n    lr_lambda <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">iters</span>: min_lr <span class=\"pl-k\">+</span> (max_lr <span class=\"pl-k\">-</span> min_lr) <span class=\"pl-k\">*</span> rel_val(iters, step_sz, scale_mode)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">rel_val</span>(<span class=\"pl-smi\">iteration</span>, <span class=\"pl-smi\">stepsize</span>, <span class=\"pl-smi\">mode</span>):\n        cycle <span class=\"pl-k\">=</span> math.floor(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">+</span> iteration <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> stepsize))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">abs</span>(iteration <span class=\"pl-k\">/</span> stepsize <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> cycle <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cycles<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">max</span>(<span class=\"pl-c1\">0</span>, (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> x)) <span class=\"pl-k\">*</span> scale_fn(cycle)\n        <span class=\"pl-k\">elif</span> mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>iterations<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">max</span>(<span class=\"pl-c1\">0</span>, (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> x)) <span class=\"pl-k\">*</span> scale_fn(iteration)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\">f</span><span class=\"pl-pds\">'</span><span class=\"pl-s\">The </span><span class=\"pl-c1\">{</span>scale_mode<span class=\"pl-c1\">}</span><span class=\"pl-s\"> is not valid value!</span><span class=\"pl-pds\">'</span>)\n\n    <span class=\"pl-k\">return</span> lr_lambda</pre></div>\n<ol start=\"2\">\n<li>example2</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>optimizer <span class=\"pl-k\">=</span> optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>.)\nclr <span class=\"pl-k\">=</span> cyclical_lr(step_size, <span class=\"pl-v\">min_lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>, <span class=\"pl-v\">max_lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>triangular2<span class=\"pl-pds\">'</span></span>)\nscheduler <span class=\"pl-k\">=</span> lr_scheduler.LambdaLR(optimizer, [clr])\nscheduler.step()\noptimizer.step()</pre></div>\n<ol start=\"3\">\n<li>reference\n<ol>\n<li><a href=\"https://github.com/bckenstler/CLR\">keras CLR</a></li>\n<li><a href=\"https://github.com/NVIDIA/nvvl/blob/master/examples/pytorch_superres/model/clr.py\">clr.py</a></li>\n</ol>\n</li>\n</ol>", "body_text": "we can use lr_scheduler.LambdaLR to do this, here are the examples\n\ndef cyclical_lr(step_sz, min_lr=0.001, max_lr=1, mode='triangular', scale_func=None, scale_md='cycles', gamma=1.):\n    \"\"\"implements a cyclical learning rate policy (CLR).\n    Notes: the learning rate of optimizer should be 1\n\n    Parameters:\n    ----------\n    mode : str, optional\n        one of {triangular, triangular2, exp_range}. \n    scale_md : str, optional\n        {'cycles', 'iterations'}.\n    gamma : float, optional\n        constant in 'exp_range' scaling function: gamma**(cycle iterations)\n    \n    Examples:\n    --------\n    >>> # the learning rate of optimizer should be 1\n    >>> optimizer = optim.SGD(model.parameters(), lr=1.)\n    >>> step_size = 2*len(train_loader)\n    >>> clr = cyclical_lr(step_size, min_lr=0.001, max_lr=0.005)\n    >>> scheduler = lr_scheduler.LambdaLR(optimizer, [clr])\n    >>> # some other operations\n    >>> scheduler.step()\n    >>> optimizer.step()\n    \"\"\"\n    if scale_func == None:\n        if mode == 'triangular':\n            scale_fn = lambda x: 1.\n            scale_mode = 'cycles'\n        elif mode == 'triangular2':\n            scale_fn = lambda x: 1 / (2.**(x - 1))\n            scale_mode = 'cycles'\n        elif mode == 'exp_range':\n            scale_fn = lambda x: gamma**(x)\n            scale_mode = 'iterations'\n        else:\n            raise ValueError(f'The {mode} is not valid value!')\n    else:\n        scale_fn = scale_func\n        scale_mode = scale_md\n\n    lr_lambda = lambda iters: min_lr + (max_lr - min_lr) * rel_val(iters, step_sz, scale_mode)\n\n    def rel_val(iteration, stepsize, mode):\n        cycle = math.floor(1 + iteration / (2 * stepsize))\n        x = abs(iteration / stepsize - 2 * cycle + 1)\n        if mode == 'cycles':\n            return max(0, (1 - x)) * scale_fn(cycle)\n        elif mode == 'iterations':\n            return max(0, (1 - x)) * scale_fn(iteration)\n        else:\n            raise ValueError(f'The {scale_mode} is not valid value!')\n\n    return lr_lambda\n\nexample2\n\noptimizer = optim.SGD(model.parameters(), lr=1.)\nclr = cyclical_lr(step_size, min_lr=0.001, max_lr=1, mode='triangular2')\nscheduler = lr_scheduler.LambdaLR(optimizer, [clr])\nscheduler.step()\noptimizer.step()\n\nreference\n\nkeras CLR\nclr.py", "body": "1. we can use [lr_scheduler.LambdaLR](https://github.com/AutuanLiu/pytorch/blob/master/torch/optim/lr_scheduler.py) to do this, here are the examples\r\n```python\r\ndef cyclical_lr(step_sz, min_lr=0.001, max_lr=1, mode='triangular', scale_func=None, scale_md='cycles', gamma=1.):\r\n    \"\"\"implements a cyclical learning rate policy (CLR).\r\n    Notes: the learning rate of optimizer should be 1\r\n\r\n    Parameters:\r\n    ----------\r\n    mode : str, optional\r\n        one of {triangular, triangular2, exp_range}. \r\n    scale_md : str, optional\r\n        {'cycles', 'iterations'}.\r\n    gamma : float, optional\r\n        constant in 'exp_range' scaling function: gamma**(cycle iterations)\r\n    \r\n    Examples:\r\n    --------\r\n    >>> # the learning rate of optimizer should be 1\r\n    >>> optimizer = optim.SGD(model.parameters(), lr=1.)\r\n    >>> step_size = 2*len(train_loader)\r\n    >>> clr = cyclical_lr(step_size, min_lr=0.001, max_lr=0.005)\r\n    >>> scheduler = lr_scheduler.LambdaLR(optimizer, [clr])\r\n    >>> # some other operations\r\n    >>> scheduler.step()\r\n    >>> optimizer.step()\r\n    \"\"\"\r\n    if scale_func == None:\r\n        if mode == 'triangular':\r\n            scale_fn = lambda x: 1.\r\n            scale_mode = 'cycles'\r\n        elif mode == 'triangular2':\r\n            scale_fn = lambda x: 1 / (2.**(x - 1))\r\n            scale_mode = 'cycles'\r\n        elif mode == 'exp_range':\r\n            scale_fn = lambda x: gamma**(x)\r\n            scale_mode = 'iterations'\r\n        else:\r\n            raise ValueError(f'The {mode} is not valid value!')\r\n    else:\r\n        scale_fn = scale_func\r\n        scale_mode = scale_md\r\n\r\n    lr_lambda = lambda iters: min_lr + (max_lr - min_lr) * rel_val(iters, step_sz, scale_mode)\r\n\r\n    def rel_val(iteration, stepsize, mode):\r\n        cycle = math.floor(1 + iteration / (2 * stepsize))\r\n        x = abs(iteration / stepsize - 2 * cycle + 1)\r\n        if mode == 'cycles':\r\n            return max(0, (1 - x)) * scale_fn(cycle)\r\n        elif mode == 'iterations':\r\n            return max(0, (1 - x)) * scale_fn(iteration)\r\n        else:\r\n            raise ValueError(f'The {scale_mode} is not valid value!')\r\n\r\n    return lr_lambda\r\n```\r\n2. example2\r\n```python\r\noptimizer = optim.SGD(model.parameters(), lr=1.)\r\nclr = cyclical_lr(step_size, min_lr=0.001, max_lr=1, mode='triangular2')\r\nscheduler = lr_scheduler.LambdaLR(optimizer, [clr])\r\nscheduler.step()\r\noptimizer.step()\r\n```\r\n3. reference\r\n    1. [keras CLR](https://github.com/bckenstler/CLR)\r\n    2. [clr.py](https://github.com/NVIDIA/nvvl/blob/master/examples/pytorch_superres/model/clr.py)"}