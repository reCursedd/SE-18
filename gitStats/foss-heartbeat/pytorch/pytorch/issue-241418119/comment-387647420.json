{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/387647420", "html_url": "https://github.com/pytorch/pytorch/pull/2016#issuecomment-387647420", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2016", "id": 387647420, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzY0NzQyMA==", "user": {"login": "EtienneDesticourt", "id": 8978248, "node_id": "MDQ6VXNlcjg5NzgyNDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/8978248?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EtienneDesticourt", "html_url": "https://github.com/EtienneDesticourt", "followers_url": "https://api.github.com/users/EtienneDesticourt/followers", "following_url": "https://api.github.com/users/EtienneDesticourt/following{/other_user}", "gists_url": "https://api.github.com/users/EtienneDesticourt/gists{/gist_id}", "starred_url": "https://api.github.com/users/EtienneDesticourt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EtienneDesticourt/subscriptions", "organizations_url": "https://api.github.com/users/EtienneDesticourt/orgs", "repos_url": "https://api.github.com/users/EtienneDesticourt/repos", "events_url": "https://api.github.com/users/EtienneDesticourt/events{/privacy}", "received_events_url": "https://api.github.com/users/EtienneDesticourt/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T07:29:24Z", "updated_at": "2018-05-09T07:29:24Z", "author_association": "NONE", "body_html": "<p>Very useful stuff, can't wait for this PR to be merged!</p>\n<p>I've been using it to train my models and I think the <code>step_size</code> parameter is confusing though. From its name you would expect it to be the amount of change in the learning rate at each step i.e: <code>step_size = (max_lr - min_lr) / num_steps_per_cycle</code>. Whereas in this implementation <code>step_size</code> is actually the number of steps per half cycle (I also think the \"number of training iterations per half cycle\" part of the description is needlessly cryptic, speaking in terms of steps makes it much clearer, the mental connection between training iterations and steps happens when you call batch_step either way.)</p>\n<p>So to sum up I propose something like this instead:<br>\n<code>num_steps(int): Number of steps per half cycle. Authors suggest setting num_steps 2-8 x training steps in epoch. Default: 2000</code></p>", "body_text": "Very useful stuff, can't wait for this PR to be merged!\nI've been using it to train my models and I think the step_size parameter is confusing though. From its name you would expect it to be the amount of change in the learning rate at each step i.e: step_size = (max_lr - min_lr) / num_steps_per_cycle. Whereas in this implementation step_size is actually the number of steps per half cycle (I also think the \"number of training iterations per half cycle\" part of the description is needlessly cryptic, speaking in terms of steps makes it much clearer, the mental connection between training iterations and steps happens when you call batch_step either way.)\nSo to sum up I propose something like this instead:\nnum_steps(int): Number of steps per half cycle. Authors suggest setting num_steps 2-8 x training steps in epoch. Default: 2000", "body": "Very useful stuff, can't wait for this PR to be merged!  \r\n\r\nI've been using it to train my models and I think the ``step_size`` parameter is confusing though. From its name you would expect it to be the amount of change in the learning rate at each step i.e: ``step_size = (max_lr - min_lr) / num_steps_per_cycle``. Whereas in this implementation ``step_size`` is actually the number of steps per half cycle (I also think the \"number of training iterations per half cycle\" part of the description is needlessly cryptic, speaking in terms of steps makes it much clearer, the mental connection between training iterations and steps happens when you call batch_step either way.)\r\n\r\nSo to sum up I propose something like this instead:  \r\n`\r\nnum_steps(int): Number of steps per half cycle. Authors suggest setting num_steps\r\n                         2-8 x training steps in epoch. Default: 2000\r\n`\r\n"}