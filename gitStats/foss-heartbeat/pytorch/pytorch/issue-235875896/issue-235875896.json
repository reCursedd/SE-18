{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1803", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1803/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1803/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1803/events", "html_url": "https://github.com/pytorch/pytorch/pull/1803", "id": 235875896, "node_id": "MDExOlB1bGxSZXF1ZXN0MTI1NTkzMDc4", "number": 1803, "title": "nn.EmbeddingBag to compute a bag of word embeddings (Embedding + Sum)", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-06-14T13:17:23Z", "updated_at": "2018-11-23T15:33:54Z", "closed_at": "2017-06-15T16:34:22Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1803", "html_url": "https://github.com/pytorch/pytorch/pull/1803", "diff_url": "https://github.com/pytorch/pytorch/pull/1803.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1803.patch"}, "body_html": "<p>Computes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.</p>\n<p>For bags of constant length,</p>\n<ul>\n<li>nn.EmbeddingBag with <code>mode=sum</code> is equivalent to nn.Embedding followed by <code>torch.sum(dim=1)</code></li>\n<li>with <code>mode=mean</code> is equivalent to nn.Embedding followed by <code>torch.mean(dim=1)</code></li>\n</ul>\n<p>However, nn.EmbeddingBag is much more time and memory efficient than using a chain of these operations.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> an Embedding module containing 10 tensors of size 3</span>\n    <span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> embedding_sum <span class=\"pl-k\">=</span> nn.EmbeddingBag(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> a batch of 2 samples of 4 indices each</span>\n    <span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.LongTensor([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">9</span>]))\n    <span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> offsets <span class=\"pl-k\">=</span> Variable(torch.LongTensor([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">4</span>]))\n    <span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> embedding_sum(<span class=\"pl-c1\">input</span>, offsets)\n\n    Variable containing:\n    <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.7296</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.6926</span>  <span class=\"pl-c1\">0.3295</span>\n    <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.5186</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.5631</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.2792</span>\n    [torch.FloatTensor of size <span class=\"pl-ii\">2x3</span>]</pre></div>\n<p>This diff introduces an efficient GPU implementation, and a slow/reference CPU implementation.<br>\nIn a subsequent diff, I'll introduce the fast CPU implementation as well.</p>\n<p>The initial part of this diff was written by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> . I changed the naming, introduced mean reduction and added additional tests / docs.</p>", "body_text": "Computes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.\nFor bags of constant length,\n\nnn.EmbeddingBag with mode=sum is equivalent to nn.Embedding followed by torch.sum(dim=1)\nwith mode=mean is equivalent to nn.Embedding followed by torch.mean(dim=1)\n\nHowever, nn.EmbeddingBag is much more time and memory efficient than using a chain of these operations.\n    >>> # an Embedding module containing 10 tensors of size 3\n    >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\n    >>> # a batch of 2 samples of 4 indices each\n    >>> input = Variable(torch.LongTensor([1,2,4,5,4,3,2,9]))\n    >>> offsets = Variable(torch.LongTensor([0,4]))\n    >>> embedding_sum(input, offsets)\n\n    Variable containing:\n    -0.7296 -4.6926  0.3295\n    -0.5186 -0.5631 -0.2792\n    [torch.FloatTensor of size 2x3]\nThis diff introduces an efficient GPU implementation, and a slow/reference CPU implementation.\nIn a subsequent diff, I'll introduce the fast CPU implementation as well.\nThe initial part of this diff was written by @adamlerer . I changed the naming, introduced mean reduction and added additional tests / docs.", "body": "Computes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.\r\n\r\nFor bags of constant length,\r\n\r\n* nn.EmbeddingBag with `mode=sum` is equivalent to nn.Embedding followed by `torch.sum(dim=1)`\r\n* with `mode=mean` is equivalent to nn.Embedding followed by `torch.mean(dim=1)`\r\n\r\nHowever, nn.EmbeddingBag is much more time and memory efficient than using a chain of these operations.\r\n\r\n```python\r\n    >>> # an Embedding module containing 10 tensors of size 3\r\n    >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\r\n    >>> # a batch of 2 samples of 4 indices each\r\n    >>> input = Variable(torch.LongTensor([1,2,4,5,4,3,2,9]))\r\n    >>> offsets = Variable(torch.LongTensor([0,4]))\r\n    >>> embedding_sum(input, offsets)\r\n\r\n    Variable containing:\r\n    -0.7296 -4.6926  0.3295\r\n    -0.5186 -0.5631 -0.2792\r\n    [torch.FloatTensor of size 2x3]\r\n```\r\n\r\nThis diff introduces an efficient GPU implementation, and a slow/reference CPU implementation.\r\nIn a subsequent diff, I'll introduce the fast CPU implementation as well.\r\n\r\nThe initial part of this diff was written by @adamlerer . I changed the naming, introduced mean reduction and added additional tests / docs."}