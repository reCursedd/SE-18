{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166092351", "pull_request_review_id": 94119732, "id": 166092351, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjA5MjM1MQ==", "diff_hunk": "@@ -0,0 +1,243 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Check.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include <cstring>\n+#include <iostream>\n+#include <memory>\n+#include <sstream>\n+#include <vector>\n+\n+#include <TH/THBlas.h>\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n+\n+namespace at {\n+namespace native {\n+\n+static void make_offset2bag(const Tensor &offsets, const Tensor &indices,\n+                            Tensor &offset2bag) {\n+  offset2bag.index_fill_(0, offsets, 1);    // offset2bag = [1 0 1 0 1]\n+  offset2bag[0] = 0;                        // offset2bag = [0 0 1 0 1]\n+  offset2bag = offset2bag.cumsum(0);        //  # offset2bag = [0 0 1 1 2]\n+}\n+\n+static void make_bag_size(const Tensor &offsets, const Tensor &indices,\n+                          const int64_t mode, Tensor &bag_size) {\n+  if (mode == 1) { // MODE_MEAN\n+    if (offsets.sizes()[0] != 1) {\n+      bag_size.slice(0, 0, bag_size.sizes()[0] - 1, 1) =\n+          offsets.slice(0, 1, offsets.sizes()[0], 1) -\n+          offsets.slice(0, 0, offsets.sizes()[0] - 1, 1);\n+      bag_size[-1] = indices.sizes()[0] - offsets[-1];\n+    }\n+  }\n+}\n+\n+static Tensor apply_bag_size(const Tensor &offsets, const Tensor &indices,\n+                             const int64_t mode, Tensor &output,\n+                             const Tensor &bag_size) {\n+  if (mode == 1) { // MODE_MEAN\n+    if (offsets.sizes()[0] == 1) {\n+      auto bag_size_ = indices.sizes()[0];\n+      output /= bag_size_;\n+    } else {\n+      auto bag_size_ =\n+          bag_size.toType(output.type()).unsqueeze(1).expand_as(output);\n+      output /= bag_size_;\n+    }\n+  }\n+  return output;\n+}\n+\n+static Tensor apply_bag_size_backward(const Tensor &offsets,\n+                                      const Tensor &indices, const int64_t mode,\n+                                      Tensor &output, const Tensor &offset2bag,\n+                                      const Tensor &bag_size) {\n+  if (mode == 1) { // MODE_MEAN\n+    if (offsets.sizes()[0] == 1) {\n+      auto bag_size = indices.sizes()[0];\n+      output /= bag_size;\n+    } else {\n+      auto bag_size_ = bag_size.toType(output.type())\n+                           .unsqueeze(1)\n+                           .index_select(0, offset2bag);\n+      output /= bag_size_;\n+    }\n+  }\n+  return output;\n+}\n+\n+std::tuple<Tensor, Tensor, Tensor>\n+embedding_bag_cpu(const Tensor &weight, const Tensor &indices__,\n+                  const Tensor &offsets__, const Tensor &offset2bag__,\n+                  const Tensor &bag_size_, const bool scale_grad_by_freq,\n+                  const int64_t mode, bool sparse) {\n+  auto indices_arg = TensorArg(indices__, \"indices__\", 1);\n+  checkScalarType(\"embedding_bag\", indices_arg, kLong);\n+  auto offsets_arg = TensorArg(offsets__, \"offsets__\", 1);\n+  checkScalarType(\"embedding_bag\", offsets_arg, kLong);\n+  auto offset2bag_arg = TensorArg(offset2bag__, \"offset2bag__\", 1);\n+  checkScalarType(\"embedding_bag\", offset2bag_arg, kLong);\n+  checkContiguous(\"embedding_bag\", offset2bag_arg);\n+  Tensor indices = indices__.contiguous();\n+  Tensor offsets = offsets__.contiguous();\n+  Tensor &offset2bag = const_cast<Tensor &>(offset2bag__);\n+  Tensor &bag_size = const_cast<Tensor &>(bag_size_);\n+  bag_size.resize_(offsets.sizes());\n+\n+  offset2bag.resize_({indices.sizes()[0]}); // offset2bag = [0 0 0 0 0]\n+  make_offset2bag(offsets, indices, offset2bag);\n+  auto output = weight.type().zeros({offsets.sizes()[0], weight.sizes()[1]});\n+  auto index_output = weight.index_select(0, indices);\n+  output.index_add_(0, offset2bag, index_output);\n+  make_bag_size(offsets, indices, mode, bag_size);\n+  auto ret = apply_bag_size(offsets, indices, mode, output, bag_size);\n+  return std::tuple<Tensor, Tensor, Tensor>(ret, offset2bag, bag_size);\n+}\n+\n+Tensor embedding_bag_backward(const Tensor &grad_, const Tensor &indices__,\n+                              const Tensor &offsets__,\n+                              const Tensor &offset2bag__,\n+                              const Tensor &bag_size_, int64_t num_weights,\n+                              bool scale_grad_by_freq, int64_t mode,\n+                              bool sparse) {\n+  auto indices_arg = TensorArg(indices__, \"indices__\", 1);\n+  checkScalarType(\"embedding_bag\", indices_arg, kLong);\n+  auto offsets_arg = TensorArg(offsets__, \"offsets__\", 1);\n+  checkScalarType(\"embedding_bag\", offsets_arg, kLong);\n+  auto offset2bag_arg = TensorArg(offset2bag__, \"offset2bag__\", 1);\n+  checkScalarType(\"embedding_bag\", offset2bag_arg, kLong);\n+  checkContiguous(\"embedding_bag\", offset2bag_arg);\n+  Tensor indices = indices__.contiguous();\n+  Tensor offsets = offsets__.contiguous();\n+\n+  if (sparse) {\n+    return at::embedding_bag_sparse_backward(\n+        grad_, indices, offsets, offset2bag__, bag_size_, num_weights,\n+        scale_grad_by_freq, mode);\n+  } else {\n+    return at::embedding_bag_dense_backward(\n+        grad_, indices, offsets, offset2bag__, bag_size_, num_weights,\n+        scale_grad_by_freq, mode);\n+  }\n+}\n+\n+Tensor embedding_bag_backward_cpu(const Tensor &grad_, const Tensor &indices__,\n+                                  const Tensor &offsets__,\n+                                  const Tensor &offset2bag__,\n+                                  const Tensor &bag_size_, int64_t num_weights,\n+                                  bool scale_grad_by_freq, int64_t mode) {\n+  auto grad = grad_.contiguous();\n+  auto indices_arg = TensorArg(indices__, \"indices__\", 1);\n+  checkScalarType(\"embedding_bag\", indices_arg, kLong);\n+  auto offsets_arg = TensorArg(offsets__, \"offsets__\", 1);\n+  checkScalarType(\"embedding_bag\", offsets_arg, kLong);\n+  auto offset2bag_arg = TensorArg(offset2bag__, \"offset2bag__\", 1);\n+  checkScalarType(\"embedding_bag\", offset2bag_arg, kLong);\n+  checkContiguous(\"embedding_bag\", offset2bag_arg);\n+  Tensor indices_ = indices__.contiguous();\n+  Tensor offsets_ = offsets__.contiguous();\n+\n+  Tensor &offset2bag_ = const_cast<Tensor &>(offset2bag__);\n+\n+  auto ind_sort_ = indices_.sort();\n+  auto indices = std::get<0>(ind_sort_);\n+  auto ind_sort = std::get<1>(ind_sort_);\n+  auto offset2bag = offset2bag_.index_select(0, ind_sort);\n+\n+  auto indices_data = indices.data<int64_t>();\n+  auto offsets_data = offsets_.data<int64_t>();\n+  auto offset2bag_data = offset2bag.data<int64_t>();\n+  int64_t numel = indices.numel();\n+\n+  std::vector<int64_t> counts(num_weights);\n+  for (int i = 0; i < numel; i++) {\n+    counts[indices_data[i]] = 0;\n+  }\n+  for (int i = 0; i < numel; i++) {\n+    counts[indices_data[i]]++;\n+  }\n+\n+  std::vector<int64_t> counts_uniq;\n+  counts_uniq.reserve(num_weights);\n+  int64_t o = 0;\n+  for (int64_t i = 0; i < numel; i += counts[indices_data[i]]) {\n+    counts_uniq.push_back(counts[indices_data[i]]);\n+    if (o > 0) {\n+      counts_uniq[o] += counts_uniq[o - 1];\n+    }\n+    o++;\n+  }\n+\n+  auto index_grad_weight =\n+      grad.type().zeros({num_weights, grad.sizes()[1]}).contiguous();\n+\n+#ifdef _OPENMP\n+#pragma omp parallel for if (numel > 1000)", "path": "aten/src/ATen/native/EmbeddingBag.cpp", "position": null, "original_position": 179, "commit_id": "0580481472d87117344fd61a60b0ee487a31ef56", "original_commit_id": "478ec5876cde3ee354cadd089d78aec929573265", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I don't think you need ifdef around the pragma", "created_at": "2018-02-05T19:49:56Z", "updated_at": "2018-11-23T15:39:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/4856#discussion_r166092351", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4856", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166092351"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4856#discussion_r166092351"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4856"}}, "body_html": "<p>I don't think you need ifdef around the pragma</p>", "body_text": "I don't think you need ifdef around the pragma"}