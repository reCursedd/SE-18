{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1997", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1997/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1997/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1997/events", "html_url": "https://github.com/pytorch/pytorch/issues/1997", "id": 241149989, "node_id": "MDU6SXNzdWUyNDExNDk5ODk=", "number": 1997, "title": "Several problems when training a convnet on mnist", "user": {"login": "cdluminate", "id": 5723047, "node_id": "MDQ6VXNlcjU3MjMwNDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5723047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cdluminate", "html_url": "https://github.com/cdluminate", "followers_url": "https://api.github.com/users/cdluminate/followers", "following_url": "https://api.github.com/users/cdluminate/following{/other_user}", "gists_url": "https://api.github.com/users/cdluminate/gists{/gist_id}", "starred_url": "https://api.github.com/users/cdluminate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cdluminate/subscriptions", "organizations_url": "https://api.github.com/users/cdluminate/orgs", "repos_url": "https://api.github.com/users/cdluminate/repos", "events_url": "https://api.github.com/users/cdluminate/events{/privacy}", "received_events_url": "https://api.github.com/users/cdluminate/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-07T03:49:35Z", "updated_at": "2017-07-07T16:59:21Z", "closed_at": "2017-07-07T16:59:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I wrote a script to train conv net on mnist, but there are some problems.<br>\nThe mnist dataset is from Kaggle (<code>train.csv</code>). The reference codes are<br>\nlisted in the head part of the code.</p>\n<p>Problems:</p>\n<p>(1) line <code>46</code>: Without this line, it will raise <code>RuntimeError: expected Double tensor (got Float tensor)</code>.<br>\nThe tutorial and example code don't do like this...</p>\n<pre><code>    46\tconvnet = convnet.double() #RuntimeError: expected Double tensor (got Float tensor)\n</code></pre>\n<p>(2) line <code>66</code>: will crash if I don't explicitly assign <code>True</code> to <code>model.trainning</code>. The totorial and example code don't do like this...</p>\n<pre><code>    66\t    model.trainning = True #AttributeError: 'MnistConvNet' object has no attribute 'trainning'\n</code></pre>\n<p>(3) The model seems not learning (test accuracy still 0.11 after 100 iterations), but I didn't find out why. The test accuracy should arise after merely several iterations (The TF example codes in Kaggle do so). Is the code wrong?</p>\n<p>Thanks in advance!</p>\n<p>Complete script:</p>\n<div class=\"highlight highlight-source-python\"><pre>     <span class=\"pl-c1\">1</span>\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> http://pytorch.org/tutorials/</span>\n     <span class=\"pl-c1\">2</span>\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</span>\n     <span class=\"pl-c1\">3</span>\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> https://github.com/pytorch/examples/blob/master/mnist/main.py</span>\n     <span class=\"pl-c1\">4</span>\t\n     <span class=\"pl-c1\">5</span>\t<span class=\"pl-k\">import</span> sys\n     <span class=\"pl-c1\">6</span>\t<span class=\"pl-k\">import</span> os\n     <span class=\"pl-c1\">7</span>\t\n     <span class=\"pl-c1\">8</span>\tos.putenv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>OPENBLAS_NUM_THREADS<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>4<span class=\"pl-pds\">'</span></span>)\n     <span class=\"pl-c1\">9</span>\t\n    <span class=\"pl-c1\">10</span>\t<span class=\"pl-k\">import</span> torch <span class=\"pl-k\">as</span> th\n    <span class=\"pl-c1\">11</span>\t<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> thnf\n    <span class=\"pl-c1\">12</span>\t<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n    <span class=\"pl-c1\">13</span>\t<span class=\"pl-k\">import</span> pandas <span class=\"pl-k\">as</span> pd\n    <span class=\"pl-c1\">14</span>\t<span class=\"pl-k\">from</span> sklearn.model_selection <span class=\"pl-k\">import</span> train_test_split\n    <span class=\"pl-c1\">15</span>\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-&gt; Using TH<span class=\"pl-pds\">'</span></span>, th.<span class=\"pl-c1\">__version__</span>)\n    <span class=\"pl-c1\">16</span>\t\n    <span class=\"pl-c1\">17</span>\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>## Read Train-Val data and split ###</span>\n    <span class=\"pl-c1\">18</span>\ttrainval <span class=\"pl-k\">=</span> pd.read_csv(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train.csv<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">19</span>\ttrainval_images <span class=\"pl-k\">=</span> trainval.iloc[:, <span class=\"pl-c1\">1</span>:].div(<span class=\"pl-c1\">255</span>)\n    <span class=\"pl-c1\">20</span>\ttrainval_labels <span class=\"pl-k\">=</span> trainval.iloc[:, :<span class=\"pl-c1\">1</span>]\n    <span class=\"pl-c1\">21</span>\ttrain_images, val_images, train_labels, val_labels <span class=\"pl-k\">=</span> train_test_split(\n    <span class=\"pl-c1\">22</span>\t        trainval_images, trainval_labels, <span class=\"pl-v\">train_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.8</span>, <span class=\"pl-v\">random_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    <span class=\"pl-c1\">23</span>\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-&gt; train set shape<span class=\"pl-pds\">'</span></span>, train_images.shape)\n    <span class=\"pl-c1\">24</span>\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-&gt; val   set shape<span class=\"pl-pds\">'</span></span>, val_images.shape)\n    <span class=\"pl-c1\">25</span>\t\n    <span class=\"pl-c1\">26</span>\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>## Model ###</span>\n    <span class=\"pl-c1\">27</span>\t<span class=\"pl-k\">class</span> <span class=\"pl-en\">MnistConvNet</span>(<span class=\"pl-e\">th</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-c1\">28</span>\t    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">29</span>\t        <span class=\"pl-c1\">super</span>(MnistConvNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">30</span>\t        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> th.nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n    <span class=\"pl-c1\">31</span>\t        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> th.nn.Conv2d(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n    <span class=\"pl-c1\">32</span>\t        <span class=\"pl-c1\">self</span>.conv2_drop <span class=\"pl-k\">=</span> th.nn.Dropout2d()\n    <span class=\"pl-c1\">33</span>\t        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> th.nn.Linear(<span class=\"pl-c1\">320</span>, <span class=\"pl-c1\">50</span>)\n    <span class=\"pl-c1\">34</span>\t        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> th.nn.Linear(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">10</span>)\n    <span class=\"pl-c1\">35</span>\t    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    <span class=\"pl-c1\">36</span>\t        x <span class=\"pl-k\">=</span> thnf.relu(thnf.max_pool2d(<span class=\"pl-c1\">self</span>.conv1(x), <span class=\"pl-c1\">2</span>))\n    <span class=\"pl-c1\">37</span>\t        x <span class=\"pl-k\">=</span> thnf.relu(thnf.max_pool2d(<span class=\"pl-c1\">self</span>.conv2_drop(<span class=\"pl-c1\">self</span>.conv2(x)), <span class=\"pl-c1\">2</span>))\n    <span class=\"pl-c1\">38</span>\t        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">320</span>)\n    <span class=\"pl-c1\">39</span>\t        x <span class=\"pl-k\">=</span> thnf.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n    <span class=\"pl-c1\">40</span>\t        x <span class=\"pl-k\">=</span> thnf.dropout(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.trainning)\n    <span class=\"pl-c1\">41</span>\t        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc2(x)\n    <span class=\"pl-c1\">42</span>\t        <span class=\"pl-k\">return</span> x\n    <span class=\"pl-c1\">43</span>\t        <span class=\"pl-c\"><span class=\"pl-c\">#</span>return thnf.log_softmax(x)</span>\n    <span class=\"pl-c1\">44</span>\t\n    <span class=\"pl-c1\">45</span>\tconvnet <span class=\"pl-k\">=</span> MnistConvNet()\n    <span class=\"pl-c1\">46</span>\tconvnet <span class=\"pl-k\">=</span> convnet.double() <span class=\"pl-c\"><span class=\"pl-c\">#</span>RuntimeError: expected Double tensor (got Float tensor)</span>\n    <span class=\"pl-c1\">47</span>\tcrit <span class=\"pl-k\">=</span> th.nn.CrossEntropyLoss()\n    <span class=\"pl-c1\">48</span>\toptimizer <span class=\"pl-k\">=</span> th.optim.Adam(convnet.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-2</span>)\n    <span class=\"pl-c1\">49</span>\t\n    <span class=\"pl-c1\">50</span>\t<span class=\"pl-c\"><span class=\"pl-c\">#</span>## Train and Val ###</span>\n    <span class=\"pl-c1\">51</span>\t\n    <span class=\"pl-c1\">52</span>\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">step_train</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">iteration</span>):\n    <span class=\"pl-c1\">53</span>\t    i <span class=\"pl-k\">=</span> iteration\n    <span class=\"pl-c1\">54</span>\t    batch_images <span class=\"pl-k\">=</span> train_images.iloc[\n    <span class=\"pl-c1\">55</span>\t        (i<span class=\"pl-k\">*</span><span class=\"pl-c1\">50</span>)<span class=\"pl-k\">%</span><span class=\"pl-c1\">33600</span>:\n    <span class=\"pl-c1\">56</span>\t        (i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">%</span><span class=\"pl-c1\">672</span><span class=\"pl-k\">==</span><span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> <span class=\"pl-c1\">33600</span> <span class=\"pl-k\">or</span> ((i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">*</span><span class=\"pl-c1\">50</span>)<span class=\"pl-k\">%</span><span class=\"pl-c1\">33600</span>].values\n    <span class=\"pl-c1\">57</span>\t    batch_labels <span class=\"pl-k\">=</span> train_labels.iloc[\n    <span class=\"pl-c1\">58</span>\t        (i<span class=\"pl-k\">*</span><span class=\"pl-c1\">50</span>)<span class=\"pl-k\">%</span><span class=\"pl-c1\">33600</span>:\n    <span class=\"pl-c1\">59</span>\t        (i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">%</span><span class=\"pl-c1\">672</span><span class=\"pl-k\">==</span><span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> <span class=\"pl-c1\">33600</span> <span class=\"pl-k\">or</span> ((i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">*</span><span class=\"pl-c1\">50</span>)<span class=\"pl-k\">%</span><span class=\"pl-c1\">33600</span>].values\n    <span class=\"pl-c1\">60</span>\t    batch_images <span class=\"pl-k\">=</span> th.autograd.Variable(th.from_numpy(batch_images))\n    <span class=\"pl-c1\">61</span>\t    batch_labels <span class=\"pl-k\">=</span> th.autograd.Variable(th.from_numpy(batch_labels))\n    <span class=\"pl-c1\">62</span>\t    batch_images <span class=\"pl-k\">=</span> batch_images.resize(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>)\n    <span class=\"pl-c1\">63</span>\t    batch_labels <span class=\"pl-k\">=</span> batch_labels.resize(<span class=\"pl-c1\">50</span>)\n    <span class=\"pl-c1\">64</span>\t\n    <span class=\"pl-c1\">65</span>\t    model.train()\n    <span class=\"pl-c1\">66</span>\t    model.trainning <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span>AttributeError: 'MnistConvNet' object has no attribute 'trainning'</span>\n    <span class=\"pl-c1\">67</span>\t    optimizer.zero_grad()\n    <span class=\"pl-c1\">68</span>\t    output <span class=\"pl-k\">=</span> model(batch_images)\n    <span class=\"pl-c1\">69</span>\t    loss <span class=\"pl-k\">=</span> crit(output, batch_labels)\n    <span class=\"pl-c1\">70</span>\t    loss.backward()\n    <span class=\"pl-c1\">71</span>\t    optimizer.step()\n    <span class=\"pl-c1\">72</span>\t\n    <span class=\"pl-c1\">73</span>\t    pred <span class=\"pl-k\">=</span> output.data.max(<span class=\"pl-c1\">1</span>)[<span class=\"pl-c1\">1</span>]\n    <span class=\"pl-c1\">74</span>\t    correct <span class=\"pl-k\">=</span> pred.eq(batch_labels.data).cpu().sum()\n    <span class=\"pl-c1\">75</span>\t    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-&gt; Iter <span class=\"pl-c1\">{<span class=\"pl-k\">:5d</span>}</span> |<span class=\"pl-pds\">'</span></span>.format(i), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss <span class=\"pl-c1\">{<span class=\"pl-k\">:7.3f</span>}</span> |<span class=\"pl-pds\">'</span></span>.format(loss.data[<span class=\"pl-c1\">0</span>]),\n    <span class=\"pl-c1\">76</span>\t            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Bch Train Accu <span class=\"pl-c1\">{<span class=\"pl-k\">:.2f</span>}</span><span class=\"pl-pds\">'</span></span>.format(correct <span class=\"pl-k\">/</span> output.size()[<span class=\"pl-c1\">1</span>]))\n    <span class=\"pl-c1\">77</span>\t\n    <span class=\"pl-c1\">78</span>\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">step_eval</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">iteration</span>):\n    <span class=\"pl-c1\">79</span>\t    correct <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-c1\">80</span>\t    total   <span class=\"pl-k\">=</span> val_images.shape[<span class=\"pl-c1\">0</span>]\n    <span class=\"pl-c1\">81</span>\t    lossaccum <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>.\n    <span class=\"pl-c1\">82</span>\t    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-&gt; TEST @ <span class=\"pl-c1\">{}</span> |<span class=\"pl-pds\">'</span></span>.format(iteration), <span class=\"pl-v\">end</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">83</span>\t    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>, val_images.shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">50</span>):\n    <span class=\"pl-c1\">84</span>\t        images <span class=\"pl-k\">=</span> val_images.iloc[i:i<span class=\"pl-k\">+</span><span class=\"pl-c1\">50</span>].values\n    <span class=\"pl-c1\">85</span>\t        labels <span class=\"pl-k\">=</span> val_labels.iloc[i:i<span class=\"pl-k\">+</span><span class=\"pl-c1\">50</span>].values\n    <span class=\"pl-c1\">86</span>\t        images <span class=\"pl-k\">=</span> th.autograd.Variable(th.from_numpy(images))\n    <span class=\"pl-c1\">87</span>\t        labels <span class=\"pl-k\">=</span> th.autograd.Variable(th.from_numpy(labels))\n    <span class=\"pl-c1\">88</span>\t        images <span class=\"pl-k\">=</span> images.resize(<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>)\n    <span class=\"pl-c1\">89</span>\t        labels <span class=\"pl-k\">=</span> labels.resize(<span class=\"pl-c1\">50</span>)\n    <span class=\"pl-c1\">90</span>\t\n    <span class=\"pl-c1\">91</span>\t        model.eval()\n    <span class=\"pl-c1\">92</span>\t        model.trainning <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n    <span class=\"pl-c1\">93</span>\t        output <span class=\"pl-k\">=</span> model(images)\n    <span class=\"pl-c1\">94</span>\t        loss <span class=\"pl-k\">=</span> thnf.nll_loss(output, labels)\n    <span class=\"pl-c1\">95</span>\t        lossaccum <span class=\"pl-k\">+=</span> loss.data[<span class=\"pl-c1\">0</span>]\n    <span class=\"pl-c1\">96</span>\t        pred <span class=\"pl-k\">=</span> output.data.max(<span class=\"pl-c1\">1</span>)[<span class=\"pl-c1\">1</span>]\n    <span class=\"pl-c1\">97</span>\t        correct <span class=\"pl-k\">+=</span> pred.eq(labels.data).cpu().sum()\n    <span class=\"pl-c1\">98</span>\t        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">end</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>); sys.stdout.flush()\n    <span class=\"pl-c1\">99</span>\t    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>|<span class=\"pl-pds\">'</span></span>)\n   <span class=\"pl-c1\">100</span>\t    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-&gt; TEST @ <span class=\"pl-c1\">{}</span> |<span class=\"pl-pds\">'</span></span>.format(iteration),\n   <span class=\"pl-c1\">101</span>\t            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loss <span class=\"pl-c1\">{<span class=\"pl-k\">:7.3f</span>}</span> |<span class=\"pl-pds\">'</span></span>.format(lossaccum),\n   <span class=\"pl-c1\">102</span>\t            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Accu <span class=\"pl-c1\">{<span class=\"pl-k\">:.2f</span>}</span>|<span class=\"pl-pds\">'</span></span>.format(correct <span class=\"pl-k\">/</span> total))\n   <span class=\"pl-c1\">103</span>\t    <span class=\"pl-c1\">exit</span>()\n   <span class=\"pl-c1\">104</span>\t\n   <span class=\"pl-c1\">105</span>\t<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">20000</span>):\n   <span class=\"pl-c1\">106</span>\t    step_train(convnet, i)\n   <span class=\"pl-c1\">107</span>\t    <span class=\"pl-k\">if</span> i<span class=\"pl-k\">&gt;</span><span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> i<span class=\"pl-k\">%</span><span class=\"pl-c1\">100</span><span class=\"pl-k\">==</span><span class=\"pl-c1\">0</span>:\n   <span class=\"pl-c1\">108</span>\t        step_eval(convnet, i)</pre></div>\n<p>part of output</p>\n<pre><code>-&gt; Using TH 0.1.12\n-&gt; train set shape (33600, 784)\n-&gt; val   set shape (8400, 784)\n-&gt; Iter     0 | loss   2.286 | Bch Train Accu 0.60\n-&gt; Iter     1 | loss   2.340 | Bch Train Accu 0.70\n</code></pre>", "body_text": "I wrote a script to train conv net on mnist, but there are some problems.\nThe mnist dataset is from Kaggle (train.csv). The reference codes are\nlisted in the head part of the code.\nProblems:\n(1) line 46: Without this line, it will raise RuntimeError: expected Double tensor (got Float tensor).\nThe tutorial and example code don't do like this...\n    46\tconvnet = convnet.double() #RuntimeError: expected Double tensor (got Float tensor)\n\n(2) line 66: will crash if I don't explicitly assign True to model.trainning. The totorial and example code don't do like this...\n    66\t    model.trainning = True #AttributeError: 'MnistConvNet' object has no attribute 'trainning'\n\n(3) The model seems not learning (test accuracy still 0.11 after 100 iterations), but I didn't find out why. The test accuracy should arise after merely several iterations (The TF example codes in Kaggle do so). Is the code wrong?\nThanks in advance!\nComplete script:\n     1\t# http://pytorch.org/tutorials/\n     2\t# http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n     3\t# https://github.com/pytorch/examples/blob/master/mnist/main.py\n     4\t\n     5\timport sys\n     6\timport os\n     7\t\n     8\tos.putenv('OPENBLAS_NUM_THREADS', '4')\n     9\t\n    10\timport torch as th\n    11\timport torch.nn.functional as thnf\n    12\timport numpy as np\n    13\timport pandas as pd\n    14\tfrom sklearn.model_selection import train_test_split\n    15\tprint('-> Using TH', th.__version__)\n    16\t\n    17\t### Read Train-Val data and split ###\n    18\ttrainval = pd.read_csv(\"train.csv\")\n    19\ttrainval_images = trainval.iloc[:, 1:].div(255)\n    20\ttrainval_labels = trainval.iloc[:, :1]\n    21\ttrain_images, val_images, train_labels, val_labels = train_test_split(\n    22\t        trainval_images, trainval_labels, train_size=0.8, random_state=0)\n    23\tprint('-> train set shape', train_images.shape)\n    24\tprint('-> val   set shape', val_images.shape)\n    25\t\n    26\t### Model ###\n    27\tclass MnistConvNet(th.nn.Module):\n    28\t    def __init__(self):\n    29\t        super(MnistConvNet, self).__init__()\n    30\t        self.conv1 = th.nn.Conv2d(1, 10, kernel_size=5)\n    31\t        self.conv2 = th.nn.Conv2d(10, 20, kernel_size=5)\n    32\t        self.conv2_drop = th.nn.Dropout2d()\n    33\t        self.fc1 = th.nn.Linear(320, 50)\n    34\t        self.fc2 = th.nn.Linear(50, 10)\n    35\t    def forward(self, x):\n    36\t        x = thnf.relu(thnf.max_pool2d(self.conv1(x), 2))\n    37\t        x = thnf.relu(thnf.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n    38\t        x = x.view(-1, 320)\n    39\t        x = thnf.relu(self.fc1(x))\n    40\t        x = thnf.dropout(x, training=self.trainning)\n    41\t        x = self.fc2(x)\n    42\t        return x\n    43\t        #return thnf.log_softmax(x)\n    44\t\n    45\tconvnet = MnistConvNet()\n    46\tconvnet = convnet.double() #RuntimeError: expected Double tensor (got Float tensor)\n    47\tcrit = th.nn.CrossEntropyLoss()\n    48\toptimizer = th.optim.Adam(convnet.parameters(), lr=1e-2)\n    49\t\n    50\t### Train and Val ###\n    51\t\n    52\tdef step_train(model, iteration):\n    53\t    i = iteration\n    54\t    batch_images = train_images.iloc[\n    55\t        (i*50)%33600:\n    56\t        (i+1)%672==0 and 33600 or ((i+1)*50)%33600].values\n    57\t    batch_labels = train_labels.iloc[\n    58\t        (i*50)%33600:\n    59\t        (i+1)%672==0 and 33600 or ((i+1)*50)%33600].values\n    60\t    batch_images = th.autograd.Variable(th.from_numpy(batch_images))\n    61\t    batch_labels = th.autograd.Variable(th.from_numpy(batch_labels))\n    62\t    batch_images = batch_images.resize(50, 1, 28, 28)\n    63\t    batch_labels = batch_labels.resize(50)\n    64\t\n    65\t    model.train()\n    66\t    model.trainning = True #AttributeError: 'MnistConvNet' object has no attribute 'trainning'\n    67\t    optimizer.zero_grad()\n    68\t    output = model(batch_images)\n    69\t    loss = crit(output, batch_labels)\n    70\t    loss.backward()\n    71\t    optimizer.step()\n    72\t\n    73\t    pred = output.data.max(1)[1]\n    74\t    correct = pred.eq(batch_labels.data).cpu().sum()\n    75\t    print('-> Iter {:5d} |'.format(i), 'loss {:7.3f} |'.format(loss.data[0]),\n    76\t            'Bch Train Accu {:.2f}'.format(correct / output.size()[1]))\n    77\t\n    78\tdef step_eval(model, iteration):\n    79\t    correct = 0\n    80\t    total   = val_images.shape[0]\n    81\t    lossaccum = 0.\n    82\t    print('-> TEST @ {} |'.format(iteration), end='')\n    83\t    for i in range(0, val_images.shape[0], 50):\n    84\t        images = val_images.iloc[i:i+50].values\n    85\t        labels = val_labels.iloc[i:i+50].values\n    86\t        images = th.autograd.Variable(th.from_numpy(images))\n    87\t        labels = th.autograd.Variable(th.from_numpy(labels))\n    88\t        images = images.resize(50, 1, 28, 28)\n    89\t        labels = labels.resize(50)\n    90\t\n    91\t        model.eval()\n    92\t        model.trainning = False\n    93\t        output = model(images)\n    94\t        loss = thnf.nll_loss(output, labels)\n    95\t        lossaccum += loss.data[0]\n    96\t        pred = output.data.max(1)[1]\n    97\t        correct += pred.eq(labels.data).cpu().sum()\n    98\t        print('.', end=''); sys.stdout.flush()\n    99\t    print('|')\n   100\t    print('-> TEST @ {} |'.format(iteration),\n   101\t            'Loss {:7.3f} |'.format(lossaccum),\n   102\t            'Accu {:.2f}|'.format(correct / total))\n   103\t    exit()\n   104\t\n   105\tfor i in range(20000):\n   106\t    step_train(convnet, i)\n   107\t    if i>0 and i%100==0:\n   108\t        step_eval(convnet, i)\npart of output\n-> Using TH 0.1.12\n-> train set shape (33600, 784)\n-> val   set shape (8400, 784)\n-> Iter     0 | loss   2.286 | Bch Train Accu 0.60\n-> Iter     1 | loss   2.340 | Bch Train Accu 0.70", "body": "I wrote a script to train conv net on mnist, but there are some problems.\r\nThe mnist dataset is from Kaggle (`train.csv`). The reference codes are\r\nlisted in the head part of the code.\r\n\r\nProblems:\r\n\r\n(1) line `46`: Without this line, it will raise `RuntimeError: expected Double tensor (got Float tensor)`.\r\nThe tutorial and example code don't do like this...\r\n\r\n```\r\n    46\tconvnet = convnet.double() #RuntimeError: expected Double tensor (got Float tensor)\r\n```\r\n\r\n(2) line `66`: will crash if I don't explicitly assign `True` to `model.trainning`. The totorial and example code don't do like this...\r\n\r\n```\r\n    66\t    model.trainning = True #AttributeError: 'MnistConvNet' object has no attribute 'trainning'\r\n```\r\n\r\n(3) The model seems not learning (test accuracy still 0.11 after 100 iterations), but I didn't find out why. The test accuracy should arise after merely several iterations (The TF example codes in Kaggle do so). Is the code wrong?\r\n\r\nThanks in advance!\r\n\r\nComplete script:\r\n\r\n```python\r\n     1\t# http://pytorch.org/tutorials/\r\n     2\t# http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\r\n     3\t# https://github.com/pytorch/examples/blob/master/mnist/main.py\r\n     4\t\r\n     5\timport sys\r\n     6\timport os\r\n     7\t\r\n     8\tos.putenv('OPENBLAS_NUM_THREADS', '4')\r\n     9\t\r\n    10\timport torch as th\r\n    11\timport torch.nn.functional as thnf\r\n    12\timport numpy as np\r\n    13\timport pandas as pd\r\n    14\tfrom sklearn.model_selection import train_test_split\r\n    15\tprint('-> Using TH', th.__version__)\r\n    16\t\r\n    17\t### Read Train-Val data and split ###\r\n    18\ttrainval = pd.read_csv(\"train.csv\")\r\n    19\ttrainval_images = trainval.iloc[:, 1:].div(255)\r\n    20\ttrainval_labels = trainval.iloc[:, :1]\r\n    21\ttrain_images, val_images, train_labels, val_labels = train_test_split(\r\n    22\t        trainval_images, trainval_labels, train_size=0.8, random_state=0)\r\n    23\tprint('-> train set shape', train_images.shape)\r\n    24\tprint('-> val   set shape', val_images.shape)\r\n    25\t\r\n    26\t### Model ###\r\n    27\tclass MnistConvNet(th.nn.Module):\r\n    28\t    def __init__(self):\r\n    29\t        super(MnistConvNet, self).__init__()\r\n    30\t        self.conv1 = th.nn.Conv2d(1, 10, kernel_size=5)\r\n    31\t        self.conv2 = th.nn.Conv2d(10, 20, kernel_size=5)\r\n    32\t        self.conv2_drop = th.nn.Dropout2d()\r\n    33\t        self.fc1 = th.nn.Linear(320, 50)\r\n    34\t        self.fc2 = th.nn.Linear(50, 10)\r\n    35\t    def forward(self, x):\r\n    36\t        x = thnf.relu(thnf.max_pool2d(self.conv1(x), 2))\r\n    37\t        x = thnf.relu(thnf.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n    38\t        x = x.view(-1, 320)\r\n    39\t        x = thnf.relu(self.fc1(x))\r\n    40\t        x = thnf.dropout(x, training=self.trainning)\r\n    41\t        x = self.fc2(x)\r\n    42\t        return x\r\n    43\t        #return thnf.log_softmax(x)\r\n    44\t\r\n    45\tconvnet = MnistConvNet()\r\n    46\tconvnet = convnet.double() #RuntimeError: expected Double tensor (got Float tensor)\r\n    47\tcrit = th.nn.CrossEntropyLoss()\r\n    48\toptimizer = th.optim.Adam(convnet.parameters(), lr=1e-2)\r\n    49\t\r\n    50\t### Train and Val ###\r\n    51\t\r\n    52\tdef step_train(model, iteration):\r\n    53\t    i = iteration\r\n    54\t    batch_images = train_images.iloc[\r\n    55\t        (i*50)%33600:\r\n    56\t        (i+1)%672==0 and 33600 or ((i+1)*50)%33600].values\r\n    57\t    batch_labels = train_labels.iloc[\r\n    58\t        (i*50)%33600:\r\n    59\t        (i+1)%672==0 and 33600 or ((i+1)*50)%33600].values\r\n    60\t    batch_images = th.autograd.Variable(th.from_numpy(batch_images))\r\n    61\t    batch_labels = th.autograd.Variable(th.from_numpy(batch_labels))\r\n    62\t    batch_images = batch_images.resize(50, 1, 28, 28)\r\n    63\t    batch_labels = batch_labels.resize(50)\r\n    64\t\r\n    65\t    model.train()\r\n    66\t    model.trainning = True #AttributeError: 'MnistConvNet' object has no attribute 'trainning'\r\n    67\t    optimizer.zero_grad()\r\n    68\t    output = model(batch_images)\r\n    69\t    loss = crit(output, batch_labels)\r\n    70\t    loss.backward()\r\n    71\t    optimizer.step()\r\n    72\t\r\n    73\t    pred = output.data.max(1)[1]\r\n    74\t    correct = pred.eq(batch_labels.data).cpu().sum()\r\n    75\t    print('-> Iter {:5d} |'.format(i), 'loss {:7.3f} |'.format(loss.data[0]),\r\n    76\t            'Bch Train Accu {:.2f}'.format(correct / output.size()[1]))\r\n    77\t\r\n    78\tdef step_eval(model, iteration):\r\n    79\t    correct = 0\r\n    80\t    total   = val_images.shape[0]\r\n    81\t    lossaccum = 0.\r\n    82\t    print('-> TEST @ {} |'.format(iteration), end='')\r\n    83\t    for i in range(0, val_images.shape[0], 50):\r\n    84\t        images = val_images.iloc[i:i+50].values\r\n    85\t        labels = val_labels.iloc[i:i+50].values\r\n    86\t        images = th.autograd.Variable(th.from_numpy(images))\r\n    87\t        labels = th.autograd.Variable(th.from_numpy(labels))\r\n    88\t        images = images.resize(50, 1, 28, 28)\r\n    89\t        labels = labels.resize(50)\r\n    90\t\r\n    91\t        model.eval()\r\n    92\t        model.trainning = False\r\n    93\t        output = model(images)\r\n    94\t        loss = thnf.nll_loss(output, labels)\r\n    95\t        lossaccum += loss.data[0]\r\n    96\t        pred = output.data.max(1)[1]\r\n    97\t        correct += pred.eq(labels.data).cpu().sum()\r\n    98\t        print('.', end=''); sys.stdout.flush()\r\n    99\t    print('|')\r\n   100\t    print('-> TEST @ {} |'.format(iteration),\r\n   101\t            'Loss {:7.3f} |'.format(lossaccum),\r\n   102\t            'Accu {:.2f}|'.format(correct / total))\r\n   103\t    exit()\r\n   104\t\r\n   105\tfor i in range(20000):\r\n   106\t    step_train(convnet, i)\r\n   107\t    if i>0 and i%100==0:\r\n   108\t        step_eval(convnet, i)\r\n```\r\n\r\npart of output\r\n```\r\n-> Using TH 0.1.12\r\n-> train set shape (33600, 784)\r\n-> val   set shape (8400, 784)\r\n-> Iter     0 | loss   2.286 | Bch Train Accu 0.60\r\n-> Iter     1 | loss   2.340 | Bch Train Accu 0.70\r\n```"}