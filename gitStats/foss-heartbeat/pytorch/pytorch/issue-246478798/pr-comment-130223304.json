{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/130223304", "pull_request_review_id": 53078906, "id": 130223304, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMDIyMzMwNA==", "diff_hunk": "@@ -644,67 +641,64 @@ static std::unique_ptr<Tensor> compute_grad_input(\n   auto dim = input->nDim();\n   auto dilated = params.is_dilated();\n \n-  if (dilated) {\n-    if (params.transposed) {\n-      /* dilated && transposed */\n-      /* NOT IMPLEMENTED */\n-    } else /* !transposed */ {\n-      /* dilated && !transposed */\n-      if (dim == 4) {\n-        SpatialDilatedConvolution_updateGradInput(\n+  if (params.transposed) {\n+    if (dim == 4) {\n+      SpatialFullDilatedConvolution_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns,\n             kernel_size[1], kernel_size[0],\n             params.stride[1], params.stride[0],\n             params.padding[1], params.padding[0],\n-            params.dilation[1], params.dilation[0]); goto done;\n-      } else if (dim == 5) {\n-        VolumetricDilatedConvolution_updateGradInput(\n-            input, grad_output, grad_input.get(), weight, columns,\n-            kernel_size[0], kernel_size[2], kernel_size[1],\n+            dilated ? params.dilation[1] : 1,\n+            dilated ? params.dilation[0] : 1,\n+            params.output_padding[1], params.output_padding[0]); goto done;\n+    } else if (dim == 5) {\n+      VolumetricFullDilatedConvolution_updateGradInput(\n+            input, grad_output, grad_input.get(), weight, columns, ones,\n             params.stride[0], params.stride[2], params.stride[1],\n             params.padding[0], params.padding[2], params.padding[1],\n-            params.dilation[0], params.dilation[2], params.dilation[1]); goto done;\n-      }\n+            dilated ? params.dilation[0] : 1,\n+            dilated ? params.dilation[2] : 1,\n+            dilated ? params.dilation[1] : 1,\n+            params.output_padding[0], params.output_padding[2], params.output_padding[1]); goto done;\n     }\n-  } else /* !dilated */ {\n-    if (params.transposed) {\n-      /* !dilated && transposed */\n-      if (dim == 4) {\n-        SpatialFullConvolution_updateGradInput(\n+  } else {  /* Not transposed */\n+    if (dim == 4) {\n+      if (dilated) {\n+        SpatialDilatedConvolution_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns,\n             kernel_size[1], kernel_size[0],\n             params.stride[1], params.stride[0],\n             params.padding[1], params.padding[0],\n-            params.output_padding[1], params.output_padding[0]); goto done;\n-      } else if (dim == 5) {\n-        VolumetricFullConvolution_updateGradInput(\n-            input, grad_output, grad_input.get(), weight, columns, ones,\n-            params.stride[0], params.stride[2], params.stride[1],\n-            params.padding[0], params.padding[2], params.padding[1],\n-            params.output_padding[0], params.output_padding[2], params.output_padding[1]); goto done;\n-      }\n-    } else /* !transposed */ {\n-      /* !dilated && !transposed */\n-      if (dim == 4) {\n+            params.dilation[1], params.dilation[0]); goto done;\n+      } else {\n+        /* CPU implementation has specialized MM kernels \n+           for non-dilated case here */\n         SpatialConvolutionMM_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns, ones,\n             kernel_size[1], kernel_size[0],\n             params.stride[1], params.stride[0],\n             params.padding[1], params.padding[0]); goto done;\n-      } else if (dim == 5 && input->isCuda()) {\n-        VolumetricConvolution_updateGradInput(\n+      }\n+    } else if (dim == 5 && (input->isCuda() || dilated)) {\n+        VolumetricDilatedConvolution_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns,\n+            kernel_size[0], kernel_size[2], kernel_size[1],\n             params.stride[0], params.stride[2], params.stride[1],\n-            params.padding[0], params.padding[2], params.padding[1]); goto done;\n-      } else if (dim == 5) {\n+            params.padding[0], params.padding[2], params.padding[1],\n+            dilated ? params.dilation[0] : 1,\n+            dilated ? params.dilation[2] : 1,\n+            dilated ? params.dilation[1] : 1); goto done;\n+    } else if (dim == 5) { /* dim == 5, CPU, non-dilated */\n+        /* CPU implementation has specialized MM kernels \n+           for non-dilated case here */\n         VolumetricConvolutionMM_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns, ones,\n             kernel_size[0], kernel_size[2], kernel_size[1],\n             params.stride[0], params.stride[2], params.stride[1],\n             params.padding[0], params.padding[2], params.padding[1]); goto done;\n-      }\n     }\n   }\n+", "path": "torch/csrc/autograd/functions/convolution.cpp", "position": 351, "original_position": 223, "commit_id": "6956b05b89e4b7ba04305184722703ee821c6878", "original_commit_id": "3751ac951bf4ef3fc96a1d6a72cce619570b5c6c", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "body": "i can revert this commit. we were duplicating a lot of code, so i refactored it, but it's only an additional 50 lines", "created_at": "2017-07-29T16:04:15Z", "updated_at": "2018-11-23T15:34:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/2238#discussion_r130223304", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2238", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/130223304"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2238#discussion_r130223304"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2238"}}, "body_html": "<p>i can revert this commit. we were duplicating a lot of code, so i refactored it, but it's only an additional 50 lines</p>", "body_text": "i can revert this commit. we were duplicating a lot of code, so i refactored it, but it's only an additional 50 lines", "in_reply_to_id": 130223000}