{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/130223000", "pull_request_review_id": 53078593, "id": 130223000, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMDIyMzAwMA==", "diff_hunk": "@@ -644,67 +641,64 @@ static std::unique_ptr<Tensor> compute_grad_input(\n   auto dim = input->nDim();\n   auto dilated = params.is_dilated();\n \n-  if (dilated) {\n-    if (params.transposed) {\n-      /* dilated && transposed */\n-      /* NOT IMPLEMENTED */\n-    } else /* !transposed */ {\n-      /* dilated && !transposed */\n-      if (dim == 4) {\n-        SpatialDilatedConvolution_updateGradInput(\n+  if (params.transposed) {\n+    if (dim == 4) {\n+      SpatialFullDilatedConvolution_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns,\n             kernel_size[1], kernel_size[0],\n             params.stride[1], params.stride[0],\n             params.padding[1], params.padding[0],\n-            params.dilation[1], params.dilation[0]); goto done;\n-      } else if (dim == 5) {\n-        VolumetricDilatedConvolution_updateGradInput(\n-            input, grad_output, grad_input.get(), weight, columns,\n-            kernel_size[0], kernel_size[2], kernel_size[1],\n+            dilated ? params.dilation[1] : 1,\n+            dilated ? params.dilation[0] : 1,\n+            params.output_padding[1], params.output_padding[0]); goto done;\n+    } else if (dim == 5) {\n+      VolumetricFullDilatedConvolution_updateGradInput(\n+            input, grad_output, grad_input.get(), weight, columns, ones,\n             params.stride[0], params.stride[2], params.stride[1],\n             params.padding[0], params.padding[2], params.padding[1],\n-            params.dilation[0], params.dilation[2], params.dilation[1]); goto done;\n-      }\n+            dilated ? params.dilation[0] : 1,\n+            dilated ? params.dilation[2] : 1,\n+            dilated ? params.dilation[1] : 1,\n+            params.output_padding[0], params.output_padding[2], params.output_padding[1]); goto done;\n     }\n-  } else /* !dilated */ {\n-    if (params.transposed) {\n-      /* !dilated && transposed */\n-      if (dim == 4) {\n-        SpatialFullConvolution_updateGradInput(\n+  } else {  /* Not transposed */\n+    if (dim == 4) {\n+      if (dilated) {\n+        SpatialDilatedConvolution_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns,\n             kernel_size[1], kernel_size[0],\n             params.stride[1], params.stride[0],\n             params.padding[1], params.padding[0],\n-            params.output_padding[1], params.output_padding[0]); goto done;\n-      } else if (dim == 5) {\n-        VolumetricFullConvolution_updateGradInput(\n-            input, grad_output, grad_input.get(), weight, columns, ones,\n-            params.stride[0], params.stride[2], params.stride[1],\n-            params.padding[0], params.padding[2], params.padding[1],\n-            params.output_padding[0], params.output_padding[2], params.output_padding[1]); goto done;\n-      }\n-    } else /* !transposed */ {\n-      /* !dilated && !transposed */\n-      if (dim == 4) {\n+            params.dilation[1], params.dilation[0]); goto done;\n+      } else {\n+        /* CPU implementation has specialized MM kernels \n+           for non-dilated case here */\n         SpatialConvolutionMM_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns, ones,\n             kernel_size[1], kernel_size[0],\n             params.stride[1], params.stride[0],\n             params.padding[1], params.padding[0]); goto done;\n-      } else if (dim == 5 && input->isCuda()) {\n-        VolumetricConvolution_updateGradInput(\n+      }\n+    } else if (dim == 5 && (input->isCuda() || dilated)) {\n+        VolumetricDilatedConvolution_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns,\n+            kernel_size[0], kernel_size[2], kernel_size[1],\n             params.stride[0], params.stride[2], params.stride[1],\n-            params.padding[0], params.padding[2], params.padding[1]); goto done;\n-      } else if (dim == 5) {\n+            params.padding[0], params.padding[2], params.padding[1],\n+            dilated ? params.dilation[0] : 1,\n+            dilated ? params.dilation[2] : 1,\n+            dilated ? params.dilation[1] : 1); goto done;\n+    } else if (dim == 5) { /* dim == 5, CPU, non-dilated */\n+        /* CPU implementation has specialized MM kernels \n+           for non-dilated case here */\n         VolumetricConvolutionMM_updateGradInput(\n             input, grad_output, grad_input.get(), weight, columns, ones,\n             kernel_size[0], kernel_size[2], kernel_size[1],\n             params.stride[0], params.stride[2], params.stride[1],\n             params.padding[0], params.padding[2], params.padding[1]); goto done;\n-      }\n     }\n   }\n+", "path": "torch/csrc/autograd/functions/convolution.cpp", "position": 351, "original_position": 223, "commit_id": "6956b05b89e4b7ba04305184722703ee821c6878", "original_commit_id": "3751ac951bf4ef3fc96a1d6a72cce619570b5c6c", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can you rearrange these conditions so that they form a full binary tree, like they used to? I did it specifically so we can be sure that we're not missing any cases (it has caused some bugs previously)", "created_at": "2017-07-29T15:43:59Z", "updated_at": "2018-11-23T15:34:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/2238#discussion_r130223000", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2238", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/130223000"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2238#discussion_r130223000"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2238"}}, "body_html": "<p>Can you rearrange these conditions so that they form a full binary tree, like they used to? I did it specifically so we can be sure that we're not missing any cases (it has caused some bugs previously)</p>", "body_text": "Can you rearrange these conditions so that they form a full binary tree, like they used to? I did it specifically so we can be sure that we're not missing any cases (it has caused some bugs previously)"}