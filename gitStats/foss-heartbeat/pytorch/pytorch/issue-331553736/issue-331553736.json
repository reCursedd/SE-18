{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8369", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8369/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8369/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8369/events", "html_url": "https://github.com/pytorch/pytorch/issues/8369", "id": 331553736, "node_id": "MDU6SXNzdWUzMzE1NTM3MzY=", "number": 8369, "title": "Something wrong with torch 0.4.0 in rnn?", "user": {"login": "shuxiaobo", "id": 11539889, "node_id": "MDQ6VXNlcjExNTM5ODg5", "avatar_url": "https://avatars2.githubusercontent.com/u/11539889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shuxiaobo", "html_url": "https://github.com/shuxiaobo", "followers_url": "https://api.github.com/users/shuxiaobo/followers", "following_url": "https://api.github.com/users/shuxiaobo/following{/other_user}", "gists_url": "https://api.github.com/users/shuxiaobo/gists{/gist_id}", "starred_url": "https://api.github.com/users/shuxiaobo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shuxiaobo/subscriptions", "organizations_url": "https://api.github.com/users/shuxiaobo/orgs", "repos_url": "https://api.github.com/users/shuxiaobo/repos", "events_url": "https://api.github.com/users/shuxiaobo/events{/privacy}", "received_events_url": "https://api.github.com/users/shuxiaobo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-06-12T11:49:42Z", "updated_at": "2018-07-25T08:56:00Z", "closed_at": "2018-06-20T15:18:35Z", "author_association": "NONE", "body_html": "<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>I got something wrong with torch 0.4.0 in rnn. it's like there are errors about the RAM.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre>        <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">emb_ranges</span>, <span class=\"pl-smi\">x3_dim</span>, <span class=\"pl-smi\">x12_dim</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>, <span class=\"pl-smi\">cell_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>,\n                 <span class=\"pl-smi\">num_layers</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">lr</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0002</span>):\n        <span class=\"pl-c1\">super</span>(SelfComputeModel, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.num_layers <span class=\"pl-k\">=</span> num_layers\n        <span class=\"pl-c1\">self</span>.cell_size <span class=\"pl-k\">=</span> cell_size\n\n        <span class=\"pl-c1\">self</span>.embeddings <span class=\"pl-k\">=</span> nn.ModuleList()\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> emb_ranges:\n            <span class=\"pl-c1\">self</span>.embeddings.append(nn.Embedding(i, x12_dim))\n\n        <span class=\"pl-c1\">self</span>.feat_dim <span class=\"pl-k\">=</span> x12_dim <span class=\"pl-k\">*</span> <span class=\"pl-c1\">len</span>(emb_ranges) <span class=\"pl-k\">+</span> x3_dim\n        <span class=\"pl-c1\">self</span>.x3_dim <span class=\"pl-k\">=</span> x3_dim\n        <span class=\"pl-c1\">self</span>.input_fc <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">self</span>.feat_dim, cell_size)\n        <span class=\"pl-c1\">self</span>.rnn <span class=\"pl-k\">=</span> nn.LSTM(cell_size, cell_size, <span class=\"pl-v\">num_layers</span> <span class=\"pl-k\">=</span> num_layers)\n        <span class=\"pl-c1\">self</span>.output_fc <span class=\"pl-k\">=</span> nn.Linear(cell_size, <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x_12</span>, <span class=\"pl-smi\">x_3</span>, <span class=\"pl-smi\">mask</span>, <span class=\"pl-smi\">label</span>, <span class=\"pl-smi\">is_h</span>):\n        x_12, x_3, mask, label <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.numpy_to_torch(x_12, x_3, mask, label)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Embedding</span>\n        x12_feat <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(x_12.shape[<span class=\"pl-c1\">2</span>]):\n            x12_feat.append(<span class=\"pl-c1\">self</span>.embeddings[i](x_12[:, :, i]))\n        feat <span class=\"pl-k\">=</span> torch.cat(x12_feat <span class=\"pl-k\">+</span> [x_3], <span class=\"pl-c1\">2</span>)\n\n        T, bs, _ <span class=\"pl-k\">=</span> feat.size()\n\n        feat <span class=\"pl-k\">=</span> feat.resize(T <span class=\"pl-k\">*</span> bs, <span class=\"pl-c1\">self</span>.feat_dim)\n        feat <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.input_fc(feat))\n\n        h0 <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">self</span>.num_layers, bs, <span class=\"pl-c1\">self</span>.cell_size)).cuda()\n        c0 <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">self</span>.num_layers, bs, <span class=\"pl-c1\">self</span>.cell_size)).cuda()\n        feat <span class=\"pl-k\">=</span> feat.resize(T, bs, <span class=\"pl-c1\">self</span>.cell_size)\n        outputs, _ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.rnn(feat, [h0, c0])\n\n        outputs <span class=\"pl-k\">=</span> outputs.resize(T <span class=\"pl-k\">*</span> bs, <span class=\"pl-c1\">self</span>.cell_size)\n        pred <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.output_fc(outputs)).squeeze(<span class=\"pl-c1\">1</span>)\n        pred <span class=\"pl-k\">=</span> pred.resize(T, bs)\n\n        pred <span class=\"pl-k\">=</span> (pred <span class=\"pl-k\">*</span> mask).sum(<span class=\"pl-c1\">0</span>).squeeze(<span class=\"pl-c1\">0</span>)\n\n        <span class=\"pl-k\">return</span> pred, FloatTensor(label)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">numpy_to_torch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x_12</span>, <span class=\"pl-smi\">x_3</span>, <span class=\"pl-smi\">mask</span>, <span class=\"pl-smi\">is_h</span>, <span class=\"pl-smi\">label</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>):\n        tensors <span class=\"pl-k\">=</span> []\n        x_12 <span class=\"pl-k\">=</span> x_12.transpose([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\n        tensors.append(Variable(torch.LongTensor(x_12)).cuda())\n        x_3 <span class=\"pl-k\">=</span> x_3.transpose([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\n        tensors.append(Variable(torch.FloatTensor(x_3)).cuda())\n        mask <span class=\"pl-k\">=</span> mask.transpose()\n        tensors.append(Variable(torch.FloatTensor(mask)).cuda())\n        <span class=\"pl-k\">if</span> label <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n            tensors.append(Variable(torch.FloatTensor(label)).cuda())\n        tensors.append(Variable(torch.FloatTensor(is_h)).cuda())\n        <span class=\"pl-k\">return</span> tensors</pre></div>\n<p>Please try to provide a minimal example to repro the bug.<br>\nError messages and stack traces are also helpful.</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">***</span> Error <span class=\"pl-k\">in</span> <span class=\"pl-s\"><span class=\"pl-pds\">`</span>/home/python3/bin/python3.6<span class=\"pl-s\"><span class=\"pl-pds\">'</span>: double free or corruption (fasttop): 0x00007fe73000aa50 ***</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">======= Backtrace: =========</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/lib64/libc.so.6(+0x7c619)[0x7fe86b938619]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/usr/local/nvidia/lib64/libcuda.so.1(+0x1dedcf)[0x7fe860e4ddcf]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/usr/local/nvidia/lib64/libcuda.so.1(+0xf6ebb)[0x7fe860d65ebb]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/usr/local/nvidia/lib64/libcuda.so.1(cuStreamCreate+0x5b)[0x7fe860e9672b]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x3258786)[0x7fe801f1c786]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x328dec4)[0x7fe801f51ec4]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN17RNNBackwardFilterIfffE4initEP12cudnnContextP14cudnnRNNStructi11PerfOptions+0x3b0)[0x7fe807792d10]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(cudnnRNNBackwardWeights+0xed1)[0x7fe807791f01]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native26_cudnn_rnn_backward_weightERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_+0xa7e)[0x7fe8000a0e5e]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x22f)[0x7fe8000a388f]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZNK2at4Type19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x9f)[0x7fe80030061f]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZNK5torch8autograd12VariableType19_cudnn_rnn_backwardERKN2at6TensorENS2_8ArrayRefIS3_EElS5_S5_S5_S5_S5_S5_S5_lllbdbbNS6_IlEES5_S5_St5arrayIbLm4EE+0x43b)[0x7fe824eead1b]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd9generated16CudnnRnnBackward5applyERKSt6vectorINS0_8VariableESaIS4_EE+0x6c6)[0x7fe824fb7f56]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine17evaluate_functionERNS0_12FunctionTaskE+0x3e2)[0x7fe824e46b62]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_mainEPNS0_9GraphTaskE+0xe5)[0x7fe824e47b75]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_initEi+0x5e)[0x7fe824e4402e]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6python12PythonEngine11thread_initEi+0x2a)[0x7fe824e71a8a]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/lib64/libstdc++.so.6(+0xb52b0)[0x7fe859a602b0]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/lib64/libpthread.so.0(+0x7e25)[0x7fe86c38fe25]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">/lib64/libc.so.6(clone+0x6d)[0x7fe86b9b434d]</span></span>\n<span class=\"pl-s\"><span class=\"pl-s\">======= Memory map: ========</span></span></pre></div>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our</p>\n<pre><code>ollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCMake version: version 2.8.12.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration: \nGPU 0: Tesla P40\nGPU 1: Tesla P40\nGPU 2: Tesla P40\nGPU 3: Tesla P40\n\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.2)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[conda] Could not collect\n</code></pre>\n<p>by the way, The dataset size is about 300+G.</p>\n<p>Is the cuDNN error or something wrong with pytorch?</p>", "body_text": "If you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nI got something wrong with torch 0.4.0 in rnn. it's like there are errors about the RAM.\nCode example\n        def __init__(self, emb_ranges, x3_dim, x12_dim = 20, cell_size = 128,\n                 num_layers = 1, lr = 0.0002):\n        super(SelfComputeModel, self).__init__()\n        self.num_layers = num_layers\n        self.cell_size = cell_size\n\n        self.embeddings = nn.ModuleList()\n        for i in emb_ranges:\n            self.embeddings.append(nn.Embedding(i, x12_dim))\n\n        self.feat_dim = x12_dim * len(emb_ranges) + x3_dim\n        self.x3_dim = x3_dim\n        self.input_fc = nn.Linear(self.feat_dim, cell_size)\n        self.rnn = nn.LSTM(cell_size, cell_size, num_layers = num_layers)\n        self.output_fc = nn.Linear(cell_size, 1)\n\n    def forward(self, x_12, x_3, mask, label, is_h):\n        x_12, x_3, mask, label = self.numpy_to_torch(x_12, x_3, mask, label)\n        # Embedding\n        x12_feat = []\n        for i in range(x_12.shape[2]):\n            x12_feat.append(self.embeddings[i](x_12[:, :, i]))\n        feat = torch.cat(x12_feat + [x_3], 2)\n\n        T, bs, _ = feat.size()\n\n        feat = feat.resize(T * bs, self.feat_dim)\n        feat = F.relu(self.input_fc(feat))\n\n        h0 = Variable(torch.zeros(self.num_layers, bs, self.cell_size)).cuda()\n        c0 = Variable(torch.zeros(self.num_layers, bs, self.cell_size)).cuda()\n        feat = feat.resize(T, bs, self.cell_size)\n        outputs, _ = self.rnn(feat, [h0, c0])\n\n        outputs = outputs.resize(T * bs, self.cell_size)\n        pred = F.relu(self.output_fc(outputs)).squeeze(1)\n        pred = pred.resize(T, bs)\n\n        pred = (pred * mask).sum(0).squeeze(0)\n\n        return pred, FloatTensor(label)\n\n    def numpy_to_torch(self, x_12, x_3, mask, is_h, label = None):\n        tensors = []\n        x_12 = x_12.transpose([1, 0, 2])\n        tensors.append(Variable(torch.LongTensor(x_12)).cuda())\n        x_3 = x_3.transpose([1, 0, 2])\n        tensors.append(Variable(torch.FloatTensor(x_3)).cuda())\n        mask = mask.transpose()\n        tensors.append(Variable(torch.FloatTensor(mask)).cuda())\n        if label is not None:\n            tensors.append(Variable(torch.FloatTensor(label)).cuda())\n        tensors.append(Variable(torch.FloatTensor(is_h)).cuda())\n        return tensors\nPlease try to provide a minimal example to repro the bug.\nError messages and stack traces are also helpful.\n*** Error in `/home/python3/bin/python3.6': double free or corruption (fasttop): 0x00007fe73000aa50 ***\n======= Backtrace: =========\n/lib64/libc.so.6(+0x7c619)[0x7fe86b938619]\n/usr/local/nvidia/lib64/libcuda.so.1(+0x1dedcf)[0x7fe860e4ddcf]\n/usr/local/nvidia/lib64/libcuda.so.1(+0xf6ebb)[0x7fe860d65ebb]\n/usr/local/nvidia/lib64/libcuda.so.1(cuStreamCreate+0x5b)[0x7fe860e9672b]\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x3258786)[0x7fe801f1c786]\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x328dec4)[0x7fe801f51ec4]\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN17RNNBackwardFilterIfffE4initEP12cudnnContextP14cudnnRNNStructi11PerfOptions+0x3b0)[0x7fe807792d10]\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(cudnnRNNBackwardWeights+0xed1)[0x7fe807791f01]\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native26_cudnn_rnn_backward_weightERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_+0xa7e)[0x7fe8000a0e5e]\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x22f)[0x7fe8000a388f]\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZNK2at4Type19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x9f)[0x7fe80030061f]\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZNK5torch8autograd12VariableType19_cudnn_rnn_backwardERKN2at6TensorENS2_8ArrayRefIS3_EElS5_S5_S5_S5_S5_S5_S5_lllbdbbNS6_IlEES5_S5_St5arrayIbLm4EE+0x43b)[0x7fe824eead1b]\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd9generated16CudnnRnnBackward5applyERKSt6vectorINS0_8VariableESaIS4_EE+0x6c6)[0x7fe824fb7f56]\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine17evaluate_functionERNS0_12FunctionTaskE+0x3e2)[0x7fe824e46b62]\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_mainEPNS0_9GraphTaskE+0xe5)[0x7fe824e47b75]\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_initEi+0x5e)[0x7fe824e4402e]\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6python12PythonEngine11thread_initEi+0x2a)[0x7fe824e71a8a]\n/lib64/libstdc++.so.6(+0xb52b0)[0x7fe859a602b0]\n/lib64/libpthread.so.0(+0x7e25)[0x7fe86c38fe25]\n/lib64/libc.so.6(clone+0x6d)[0x7fe86b9b434d]\n======= Memory map: ========\nSystem Info\nPlease copy and paste the output from our\nollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCMake version: version 2.8.12.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration: \nGPU 0: Tesla P40\nGPU 1: Tesla P40\nGPU 2: Tesla P40\nGPU 3: Tesla P40\n\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.2)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[conda] Could not collect\n\nby the way, The dataset size is about 300+G.\nIs the cuDNN error or something wrong with pytorch?", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\nI got something wrong with torch 0.4.0 in rnn. it's like there are errors about the RAM.\r\n\r\n\r\n## Code example\r\n```python\r\n        def __init__(self, emb_ranges, x3_dim, x12_dim = 20, cell_size = 128,\r\n                 num_layers = 1, lr = 0.0002):\r\n        super(SelfComputeModel, self).__init__()\r\n        self.num_layers = num_layers\r\n        self.cell_size = cell_size\r\n\r\n        self.embeddings = nn.ModuleList()\r\n        for i in emb_ranges:\r\n            self.embeddings.append(nn.Embedding(i, x12_dim))\r\n\r\n        self.feat_dim = x12_dim * len(emb_ranges) + x3_dim\r\n        self.x3_dim = x3_dim\r\n        self.input_fc = nn.Linear(self.feat_dim, cell_size)\r\n        self.rnn = nn.LSTM(cell_size, cell_size, num_layers = num_layers)\r\n        self.output_fc = nn.Linear(cell_size, 1)\r\n\r\n    def forward(self, x_12, x_3, mask, label, is_h):\r\n        x_12, x_3, mask, label = self.numpy_to_torch(x_12, x_3, mask, label)\r\n        # Embedding\r\n        x12_feat = []\r\n        for i in range(x_12.shape[2]):\r\n            x12_feat.append(self.embeddings[i](x_12[:, :, i]))\r\n        feat = torch.cat(x12_feat + [x_3], 2)\r\n\r\n        T, bs, _ = feat.size()\r\n\r\n        feat = feat.resize(T * bs, self.feat_dim)\r\n        feat = F.relu(self.input_fc(feat))\r\n\r\n        h0 = Variable(torch.zeros(self.num_layers, bs, self.cell_size)).cuda()\r\n        c0 = Variable(torch.zeros(self.num_layers, bs, self.cell_size)).cuda()\r\n        feat = feat.resize(T, bs, self.cell_size)\r\n        outputs, _ = self.rnn(feat, [h0, c0])\r\n\r\n        outputs = outputs.resize(T * bs, self.cell_size)\r\n        pred = F.relu(self.output_fc(outputs)).squeeze(1)\r\n        pred = pred.resize(T, bs)\r\n\r\n        pred = (pred * mask).sum(0).squeeze(0)\r\n\r\n        return pred, FloatTensor(label)\r\n\r\n    def numpy_to_torch(self, x_12, x_3, mask, is_h, label = None):\r\n        tensors = []\r\n        x_12 = x_12.transpose([1, 0, 2])\r\n        tensors.append(Variable(torch.LongTensor(x_12)).cuda())\r\n        x_3 = x_3.transpose([1, 0, 2])\r\n        tensors.append(Variable(torch.FloatTensor(x_3)).cuda())\r\n        mask = mask.transpose()\r\n        tensors.append(Variable(torch.FloatTensor(mask)).cuda())\r\n        if label is not None:\r\n            tensors.append(Variable(torch.FloatTensor(label)).cuda())\r\n        tensors.append(Variable(torch.FloatTensor(is_h)).cuda())\r\n        return tensors\r\n```\r\nPlease try to provide a minimal example to repro the bug.\r\nError messages and stack traces are also helpful.\r\n\r\n```bash\r\n*** Error in `/home/python3/bin/python3.6': double free or corruption (fasttop): 0x00007fe73000aa50 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x7c619)[0x7fe86b938619]\r\n/usr/local/nvidia/lib64/libcuda.so.1(+0x1dedcf)[0x7fe860e4ddcf]\r\n/usr/local/nvidia/lib64/libcuda.so.1(+0xf6ebb)[0x7fe860d65ebb]\r\n/usr/local/nvidia/lib64/libcuda.so.1(cuStreamCreate+0x5b)[0x7fe860e9672b]\r\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x3258786)[0x7fe801f1c786]\r\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x328dec4)[0x7fe801f51ec4]\r\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN17RNNBackwardFilterIfffE4initEP12cudnnContextP14cudnnRNNStructi11PerfOptions+0x3b0)[0x7fe807792d10]\r\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(cudnnRNNBackwardWeights+0xed1)[0x7fe807791f01]\r\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native26_cudnn_rnn_backward_weightERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_+0xa7e)[0x7fe8000a0e5e]\r\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x22f)[0x7fe8000a388f]\r\n/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZNK2at4Type19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x9f)[0x7fe80030061f]\r\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZNK5torch8autograd12VariableType19_cudnn_rnn_backwardERKN2at6TensorENS2_8ArrayRefIS3_EElS5_S5_S5_S5_S5_S5_S5_lllbdbbNS6_IlEES5_S5_St5arrayIbLm4EE+0x43b)[0x7fe824eead1b]\r\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd9generated16CudnnRnnBackward5applyERKSt6vectorINS0_8VariableESaIS4_EE+0x6c6)[0x7fe824fb7f56]\r\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine17evaluate_functionERNS0_12FunctionTaskE+0x3e2)[0x7fe824e46b62]\r\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_mainEPNS0_9GraphTaskE+0xe5)[0x7fe824e47b75]\r\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_initEi+0x5e)[0x7fe824e4402e]\r\n/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6python12PythonEngine11thread_initEi+0x2a)[0x7fe824e71a8a]\r\n/lib64/libstdc++.so.6(+0xb52b0)[0x7fe859a602b0]\r\n/lib64/libpthread.so.0(+0x7e25)[0x7fe86c38fe25]\r\n/lib64/libc.so.6(clone+0x6d)[0x7fe86b9b434d]\r\n======= Memory map: ========\r\n```\r\n## System Info\r\nPlease copy and paste the output from our\r\n\r\n```\r\nollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: \r\nGPU 0: Tesla P40\r\nGPU 1: Tesla P40\r\nGPU 2: Tesla P40\r\nGPU 3: Tesla P40\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] Could not collect\r\n```\r\n\r\n by the way, The dataset size is about 300+G. \r\n\r\nIs the cuDNN error or something wrong with pytorch?\r\n"}