{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1528", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1528/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1528/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1528/events", "html_url": "https://github.com/pytorch/pytorch/issues/1528", "id": 227701078, "node_id": "MDU6SXNzdWUyMjc3MDEwNzg=", "number": 1528, "title": "LongTensor indexing with duplication not propagating gradients", "user": {"login": "zkolter", "id": 2465474, "node_id": "MDQ6VXNlcjI0NjU0NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/2465474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zkolter", "html_url": "https://github.com/zkolter", "followers_url": "https://api.github.com/users/zkolter/followers", "following_url": "https://api.github.com/users/zkolter/following{/other_user}", "gists_url": "https://api.github.com/users/zkolter/gists{/gist_id}", "starred_url": "https://api.github.com/users/zkolter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zkolter/subscriptions", "organizations_url": "https://api.github.com/users/zkolter/orgs", "repos_url": "https://api.github.com/users/zkolter/repos", "events_url": "https://api.github.com/users/zkolter/events{/privacy}", "received_events_url": "https://api.github.com/users/zkolter/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-05-10T14:33:38Z", "updated_at": "2017-05-21T19:06:01Z", "closed_at": "2017-05-21T19:06:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It seems like if we use LongTensor indexing to duplicate entries of a Variable, this breaks autograd. Specifically, if any entry in a vector is duplicated, backward() only maintains gradients for the last copy of the element. Simple example:</p>\n<div class=\"highlight highlight-source-python\"><pre>y <span class=\"pl-k\">=</span> Variable(torch.ones(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny_dup <span class=\"pl-k\">=</span> y[torch.LongTensor([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>])]\ny_dup[<span class=\"pl-c1\">1</span>].backward() <span class=\"pl-c\"><span class=\"pl-c\">#</span> backprop on second element</span>\n<span class=\"pl-c1\">print</span>(y.grad)\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">Variable containing:</span>\n<span class=\"pl-s\"> 1</span>\n<span class=\"pl-s\">[torch.FloatTensor of size 1]<span class=\"pl-pds\">\"\"\"</span></span></pre></div>\n<p>However:</p>\n<div class=\"highlight highlight-source-python\"><pre>y <span class=\"pl-k\">=</span> Variable(torch.ones(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny_dup <span class=\"pl-k\">=</span> y[torch.LongTensor([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>])]\ny_dup[<span class=\"pl-c1\">0</span>].backward()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> backprop on first element, same exact value as second</span>\n<span class=\"pl-c1\">print</span>(y.grad)\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">Variable containing:</span>\n<span class=\"pl-s\"> 0</span>\n<span class=\"pl-s\">[torch.FloatTensor of size 1]<span class=\"pl-pds\">\"\"\"</span></span></pre></div>\n<p>This is version under version 0.1.11+fc48d2c, tested on 0.1.11+9f3119a as well.  This happens for both cuda and cpu tensors.</p>", "body_text": "It seems like if we use LongTensor indexing to duplicate entries of a Variable, this breaks autograd. Specifically, if any entry in a vector is duplicated, backward() only maintains gradients for the last copy of the element. Simple example:\ny = Variable(torch.ones(1), requires_grad=True)\ny_dup = y[torch.LongTensor([0,0])]\ny_dup[1].backward() # backprop on second element\nprint(y.grad)\n\"\"\"\nVariable containing:\n 1\n[torch.FloatTensor of size 1]\"\"\"\nHowever:\ny = Variable(torch.ones(1), requires_grad=True)\ny_dup = y[torch.LongTensor([0,0])]\ny_dup[0].backward()  # backprop on first element, same exact value as second\nprint(y.grad)\n\"\"\"\nVariable containing:\n 0\n[torch.FloatTensor of size 1]\"\"\"\nThis is version under version 0.1.11+fc48d2c, tested on 0.1.11+9f3119a as well.  This happens for both cuda and cpu tensors.", "body": "It seems like if we use LongTensor indexing to duplicate entries of a Variable, this breaks autograd. Specifically, if any entry in a vector is duplicated, backward() only maintains gradients for the last copy of the element. Simple example:\r\n\r\n```python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = y[torch.LongTensor([0,0])]\r\ny_dup[1].backward() # backprop on second element\r\nprint(y.grad)\r\n\"\"\"\r\nVariable containing:\r\n 1\r\n[torch.FloatTensor of size 1]\"\"\"\r\n```\r\n\r\nHowever:\r\n```python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = y[torch.LongTensor([0,0])]\r\ny_dup[0].backward()  # backprop on first element, same exact value as second\r\nprint(y.grad)\r\n\"\"\"\r\nVariable containing:\r\n 0\r\n[torch.FloatTensor of size 1]\"\"\"\r\n```\r\n\r\nThis is version under version 0.1.11+fc48d2c, tested on 0.1.11+9f3119a as well.  This happens for both cuda and cpu tensors.\r\n"}