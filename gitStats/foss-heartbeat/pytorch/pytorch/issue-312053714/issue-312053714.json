{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6353", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6353/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6353/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6353/events", "html_url": "https://github.com/pytorch/pytorch/issues/6353", "id": 312053714, "node_id": "MDU6SXNzdWUzMTIwNTM3MTQ=", "number": 6353, "title": "[jit cpp extensions] Possible file corruption in distributed setting", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}], "state": "closed", "locked": false, "assignee": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-04-06T17:21:13Z", "updated_at": "2018-04-18T22:07:04Z", "closed_at": "2018-04-18T22:07:04Z", "author_association": "MEMBER", "body_html": "<p>I believe there might be some sort of race condition that might happen when compiling jit cpp extensions in a distributed setting.</p>\n<p>Here a summary of what happens:</p>\n<ul>\n<li>I launch my code from multiple different processes (to use with DistributedDataParallel, via <code>torch.distributed.launch</code>)</li>\n<li>each process starts to compile the extension</li>\n<li>they all try to save the extension to the same file</li>\n<li>sometimes, the file gets corrupted because of that</li>\n</ul>\n<p>Here is part of the stack trace of the error I obtain (the rest is just the same error repeated):</p>\n<pre><code>  1 0: Traceback (most recent call last):\n  2 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/training_faster_rcnn.py\", line 190, in &lt;module&gt;\n  3 0:     from lib.faster_rcnn import fasterrcnn_resnet18, C2ResNetFasterRCNN\n  4 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/faster_rcnn.py\", line 13, in &lt;module&gt;\n  5 0:     from lib.layers import ROIAlign, ROIPool, FixedBatchNorm2d\n  6 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 28, in &lt;module&gt;\n  7 0:     _C = _load_C_extensions()\n  8 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 26, in _load_C_extensions\n  9 0:     '-gencode arch=compute_70,code=sm_70'])\n 10 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 330, in load\n 11 0:     build_directory = _get_build_directory(name, verbose)\n 12 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 392, in _get_build_directory\n 13 0:     os.makedirs(build_directory)\n 14 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/os.py\", line 220, in makedirs\n 15 0:     mkdir(name, mode)\n 16 0: FileExistsError: [Errno 17] File exists: '/tmp/torch_extensions/detectron_modules'\n 17 0: Traceback (most recent call last):\n 18 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/training_faster_rcnn.py\", line 190, in &lt;module&gt;\n 19 0:     from lib.faster_rcnn import fasterrcnn_resnet18, C2ResNetFasterRCNN\n 20 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/faster_rcnn.py\", line 13, in &lt;module&gt;\n 21 0:     from lib.layers import ROIAlign, ROIPool, FixedBatchNorm2d\n 22 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 28, in &lt;module&gt;\n 23 0:     _C = _load_C_extensions()\n 24 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 26, in _load_C_extensions\n 25 0:     '-gencode arch=compute_70,code=sm_70'])\n 26 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 361, in load\n 27 0:     return _import_module_from_library(name, build_directory)\n 28 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 414, in _import_module_from_library\n 29 0:     return imp.load_module(module_name, file, path, description)\n 30 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/imp.py\", line 243, in load_module\n 31 0:     return load_dynamic(name, filename, file)\n 32 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/imp.py\", line 343, in load_dynamic\n 33 0:     return _load(spec)\n 34 0: ImportError: /tmp/torch_extensions/detectron_modules/detectron_modules.so: file too short\n...\n</code></pre>\n<p>We can see that two types of errors pop out: <code>FileExistsError</code> and <code>ImportError: modules.so: file too short</code>.</p>\n<p>Let me know if you need a minimal reproducible example.</p>\n<p>pytorch version <code>'0.4.0a0+b21e135'</code></p>", "body_text": "I believe there might be some sort of race condition that might happen when compiling jit cpp extensions in a distributed setting.\nHere a summary of what happens:\n\nI launch my code from multiple different processes (to use with DistributedDataParallel, via torch.distributed.launch)\neach process starts to compile the extension\nthey all try to save the extension to the same file\nsometimes, the file gets corrupted because of that\n\nHere is part of the stack trace of the error I obtain (the rest is just the same error repeated):\n  1 0: Traceback (most recent call last):\n  2 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/training_faster_rcnn.py\", line 190, in <module>\n  3 0:     from lib.faster_rcnn import fasterrcnn_resnet18, C2ResNetFasterRCNN\n  4 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/faster_rcnn.py\", line 13, in <module>\n  5 0:     from lib.layers import ROIAlign, ROIPool, FixedBatchNorm2d\n  6 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 28, in <module>\n  7 0:     _C = _load_C_extensions()\n  8 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 26, in _load_C_extensions\n  9 0:     '-gencode arch=compute_70,code=sm_70'])\n 10 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 330, in load\n 11 0:     build_directory = _get_build_directory(name, verbose)\n 12 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 392, in _get_build_directory\n 13 0:     os.makedirs(build_directory)\n 14 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/os.py\", line 220, in makedirs\n 15 0:     mkdir(name, mode)\n 16 0: FileExistsError: [Errno 17] File exists: '/tmp/torch_extensions/detectron_modules'\n 17 0: Traceback (most recent call last):\n 18 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/training_faster_rcnn.py\", line 190, in <module>\n 19 0:     from lib.faster_rcnn import fasterrcnn_resnet18, C2ResNetFasterRCNN\n 20 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/faster_rcnn.py\", line 13, in <module>\n 21 0:     from lib.layers import ROIAlign, ROIPool, FixedBatchNorm2d\n 22 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 28, in <module>\n 23 0:     _C = _load_C_extensions()\n 24 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 26, in _load_C_extensions\n 25 0:     '-gencode arch=compute_70,code=sm_70'])\n 26 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 361, in load\n 27 0:     return _import_module_from_library(name, build_directory)\n 28 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 414, in _import_module_from_library\n 29 0:     return imp.load_module(module_name, file, path, description)\n 30 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/imp.py\", line 243, in load_module\n 31 0:     return load_dynamic(name, filename, file)\n 32 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/imp.py\", line 343, in load_dynamic\n 33 0:     return _load(spec)\n 34 0: ImportError: /tmp/torch_extensions/detectron_modules/detectron_modules.so: file too short\n...\n\nWe can see that two types of errors pop out: FileExistsError and ImportError: modules.so: file too short.\nLet me know if you need a minimal reproducible example.\npytorch version '0.4.0a0+b21e135'", "body": "I believe there might be some sort of race condition that might happen when compiling jit cpp extensions in a distributed setting.\r\n\r\nHere a summary of what happens:\r\n- I launch my code from multiple different processes (to use with DistributedDataParallel, via `torch.distributed.launch`)\r\n- each process starts to compile the extension\r\n- they all try to save the extension to the same file\r\n- sometimes, the file gets corrupted because of that\r\n\r\nHere is part of the stack trace of the error I obtain (the rest is just the same error repeated):\r\n```\r\n  1 0: Traceback (most recent call last):\r\n  2 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/training_faster_rcnn.py\", line 190, in <module>\r\n  3 0:     from lib.faster_rcnn import fasterrcnn_resnet18, C2ResNetFasterRCNN\r\n  4 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/faster_rcnn.py\", line 13, in <module>\r\n  5 0:     from lib.layers import ROIAlign, ROIPool, FixedBatchNorm2d\r\n  6 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 28, in <module>\r\n  7 0:     _C = _load_C_extensions()\r\n  8 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 26, in _load_C_extensions\r\n  9 0:     '-gencode arch=compute_70,code=sm_70'])\r\n 10 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 330, in load\r\n 11 0:     build_directory = _get_build_directory(name, verbose)\r\n 12 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 392, in _get_build_directory\r\n 13 0:     os.makedirs(build_directory)\r\n 14 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/os.py\", line 220, in makedirs\r\n 15 0:     mkdir(name, mode)\r\n 16 0: FileExistsError: [Errno 17] File exists: '/tmp/torch_extensions/detectron_modules'\r\n 17 0: Traceback (most recent call last):\r\n 18 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/training_faster_rcnn.py\", line 190, in <module>\r\n 19 0:     from lib.faster_rcnn import fasterrcnn_resnet18, C2ResNetFasterRCNN\r\n 20 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/faster_rcnn.py\", line 13, in <module>\r\n 21 0:     from lib.layers import ROIAlign, ROIPool, FixedBatchNorm2d\r\n 22 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 28, in <module>\r\n 23 0:     _C = _load_C_extensions()\r\n 24 0:   File \"/private/home/fmassa/github/detectron.pytorch/torch_detectron/lib/layers/__init__.py\", line 26, in _load_C_extensions\r\n 25 0:     '-gencode arch=compute_70,code=sm_70'])\r\n 26 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 361, in load\r\n 27 0:     return _import_module_from_library(name, build_directory)\r\n 28 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 414, in _import_module_from_library\r\n 29 0:     return imp.load_module(module_name, file, path, description)\r\n 30 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/imp.py\", line 243, in load_module\r\n 31 0:     return load_dynamic(name, filename, file)\r\n 32 0:   File \"/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n 33 0:     return _load(spec)\r\n 34 0: ImportError: /tmp/torch_extensions/detectron_modules/detectron_modules.so: file too short\r\n...\r\n```\r\nWe can see that two types of errors pop out: `FileExistsError` and `ImportError: modules.so: file too short`.\r\n\r\nLet me know if you need a minimal reproducible example.\r\n\r\npytorch version `'0.4.0a0+b21e135'`"}