{"url": "https://api.github.com/repos/pytorch/pytorch/issues/777", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/777/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/777/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/777/events", "html_url": "https://github.com/pytorch/pytorch/issues/777", "id": 208625899, "node_id": "MDU6SXNzdWUyMDg2MjU4OTk=", "number": 777, "title": "Failed for accumulation of gradients for RNN with cudnn", "user": {"login": "meijieru", "id": 9511136, "node_id": "MDQ6VXNlcjk1MTExMzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/9511136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/meijieru", "html_url": "https://github.com/meijieru", "followers_url": "https://api.github.com/users/meijieru/followers", "following_url": "https://api.github.com/users/meijieru/following{/other_user}", "gists_url": "https://api.github.com/users/meijieru/gists{/gist_id}", "starred_url": "https://api.github.com/users/meijieru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/meijieru/subscriptions", "organizations_url": "https://api.github.com/users/meijieru/orgs", "repos_url": "https://api.github.com/users/meijieru/repos", "events_url": "https://api.github.com/users/meijieru/events{/privacy}", "received_events_url": "https://api.github.com/users/meijieru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-02-18T08:52:18Z", "updated_at": "2017-02-18T10:26:32Z", "closed_at": "2017-02-18T10:26:31Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nlstm <span class=\"pl-k\">=</span> nn.LSTM(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bidirectional</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nlstm.cuda()\n\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">2</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [T, b, i]</span>\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.cuda()\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\noutput, _ <span class=\"pl-k\">=</span> lstm(<span class=\"pl-c1\">input</span>)\noutput.backward(torch.ones(output.size()).cuda())\noutput.backward(torch.ones(output.size()).cuda())</pre></div>\n<p>The desired behavior seems to accumulate the gradients for lstm's parameters, however it failed with error message</p>\n<div class=\"highlight highlight-source-shell\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test_lstm.py<span class=\"pl-pds\">\"</span></span>, line 18, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    <span class=\"pl-en\">output.backward(torch.ones(output.size()).cuda</span>())\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/jrmei/.local/lib/python2.7/site-packages/torch/autograd/variable.py<span class=\"pl-pds\">\"</span></span>, line 158, <span class=\"pl-k\">in</span> backward\n    self._execution_engine.run_backward<span class=\"pl-s\"><span class=\"pl-pds\">((</span>self<span class=\"pl-k\">,</span>)<span class=\"pl-k\">,</span> (gradient<span class=\"pl-k\">,</span>)<span class=\"pl-k\">,</span> retain_variables)</span>\n<span class=\"pl-s\">  File \"/home/jrmei/.local/lib/python<span class=\"pl-c1\">2</span>.<span class=\"pl-c1\">7</span>/site-packages/torch/autograd/function.py\", line <span class=\"pl-c1\">208</span>, in backward</span>\n<span class=\"pl-s\">    nested_gradients = _unflatten(gradients, self._nested_output)</span>\n<span class=\"pl-s\">AttributeError: 'CudnnRNN' object has no attribute '_nested_output'</span></pre></div>", "body_text": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nlstm = nn.LSTM(2, 3, 1, bidirectional=True)\nlstm.cuda()\n\ninput = torch.ones(4, 5, 2)  # [T, b, i]\ninput = input.cuda()\ninput = Variable(input, requires_grad=True)\noutput, _ = lstm(input)\noutput.backward(torch.ones(output.size()).cuda())\noutput.backward(torch.ones(output.size()).cuda())\nThe desired behavior seems to accumulate the gradients for lstm's parameters, however it failed with error message\nTraceback (most recent call last):\n  File \"test_lstm.py\", line 18, in <module>\n    output.backward(torch.ones(output.size()).cuda())\n  File \"/home/jrmei/.local/lib/python2.7/site-packages/torch/autograd/variable.py\", line 158, in backward\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\n  File \"/home/jrmei/.local/lib/python2.7/site-packages/torch/autograd/function.py\", line 208, in backward\n    nested_gradients = _unflatten(gradients, self._nested_output)\nAttributeError: 'CudnnRNN' object has no attribute '_nested_output'", "body": "```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nlstm = nn.LSTM(2, 3, 1, bidirectional=True)\r\nlstm.cuda()\r\n\r\ninput = torch.ones(4, 5, 2)  # [T, b, i]\r\ninput = input.cuda()\r\ninput = Variable(input, requires_grad=True)\r\noutput, _ = lstm(input)\r\noutput.backward(torch.ones(output.size()).cuda())\r\noutput.backward(torch.ones(output.size()).cuda())\r\n```\r\nThe desired behavior seems to accumulate the gradients for lstm's parameters, however it failed with error message\r\n```sh\r\nTraceback (most recent call last):\r\n  File \"test_lstm.py\", line 18, in <module>\r\n    output.backward(torch.ones(output.size()).cuda())\r\n  File \"/home/jrmei/.local/lib/python2.7/site-packages/torch/autograd/variable.py\", line 158, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n  File \"/home/jrmei/.local/lib/python2.7/site-packages/torch/autograd/function.py\", line 208, in backward\r\n    nested_gradients = _unflatten(gradients, self._nested_output)\r\nAttributeError: 'CudnnRNN' object has no attribute '_nested_output'\r\n```"}