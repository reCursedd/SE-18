{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6299", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6299/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6299/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6299/events", "html_url": "https://github.com/pytorch/pytorch/issues/6299", "id": 311448726, "node_id": "MDU6SXNzdWUzMTE0NDg3MjY=", "number": 6299, "title": "[feature request] make pad_packed_sequence work in DataParallel", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-04-05T02:02:42Z", "updated_at": "2018-04-09T00:25:49Z", "closed_at": "2018-04-09T00:25:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Currently users can not do <code>pack -&gt; RNN -&gt; unpack</code> in a module wrapped in <code>DataParallel</code> because the unpack operation (<code>pad_packed_sequence</code>) will only pad up to the longest input it sees, i.e., the longest on that particular device. Then the code breaks when it tries to gather the results into a single tensor afterwards. I propose to add a <code>total_length</code> option to <code>pad_packed_sequence</code> so that it can beyond the longest input. Then the following pattern should work with DataParallel:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">input_lengths</span>):\n    total_length <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    packed_input <span class=\"pl-k\">=</span> nn.utils.rnn.pack_padded_sequence(<span class=\"pl-c1\">input</span>, input_lengths)\n    packed_output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm(packed_input)\n    output, _ <span class=\"pl-k\">=</span> nn.utils.rnn.pad_packed_sequence(packed_output, <span class=\"pl-v\">total_length</span><span class=\"pl-k\">=</span>total_length)\n    <span class=\"pl-k\">return</span> output</pre></div>\n<p>The current suggested workaround is to manually add the padding. However, it becomes tricky when one wants to do ONNX export.</p>\n<p>Relevant post: <a href=\"https://discuss.pytorch.org/t/question-about-packed-rnn-with-dataparallel/2738\" rel=\"nofollow\">https://discuss.pytorch.org/t/question-about-packed-rnn-with-dataparallel/2738</a></p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6429851\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/goldsborough\">@goldsborough</a></p>", "body_text": "Currently users can not do pack -> RNN -> unpack in a module wrapped in DataParallel because the unpack operation (pad_packed_sequence) will only pad up to the longest input it sees, i.e., the longest on that particular device. Then the code breaks when it tries to gather the results into a single tensor afterwards. I propose to add a total_length option to pad_packed_sequence so that it can beyond the longest input. Then the following pattern should work with DataParallel:\ndef forward(self, input, input_lengths):\n    total_length = input.size(-1)\n    packed_input = nn.utils.rnn.pack_padded_sequence(input, input_lengths)\n    packed_output = self.lstm(packed_input)\n    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, total_length=total_length)\n    return output\nThe current suggested workaround is to manually add the padding. However, it becomes tricky when one wants to do ONNX export.\nRelevant post: https://discuss.pytorch.org/t/question-about-packed-rnn-with-dataparallel/2738\ncc @goldsborough", "body": "Currently users can not do `pack -> RNN -> unpack` in a module wrapped in `DataParallel` because the unpack operation (`pad_packed_sequence`) will only pad up to the longest input it sees, i.e., the longest on that particular device. Then the code breaks when it tries to gather the results into a single tensor afterwards. I propose to add a `total_length` option to `pad_packed_sequence` so that it can beyond the longest input. Then the following pattern should work with DataParallel:\r\n```python\r\ndef forward(self, input, input_lengths):\r\n    total_length = input.size(-1)\r\n    packed_input = nn.utils.rnn.pack_padded_sequence(input, input_lengths)\r\n    packed_output = self.lstm(packed_input)\r\n    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, total_length=total_length)\r\n    return output\r\n```\r\n\r\nThe current suggested workaround is to manually add the padding. However, it becomes tricky when one wants to do ONNX export.\r\n\r\nRelevant post: https://discuss.pytorch.org/t/question-about-packed-rnn-with-dataparallel/2738\r\n\r\ncc @goldsborough "}