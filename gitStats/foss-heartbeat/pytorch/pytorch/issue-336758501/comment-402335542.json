{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/402335542", "html_url": "https://github.com/pytorch/pytorch/pull/8997#issuecomment-402335542", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8997", "id": 402335542, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjMzNTU0Mg==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-04T01:22:28Z", "updated_at": "2018-07-04T01:22:28Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>cudaGetDevice() takes an int&amp;, not an int64_t&amp;. I have kept int64_t in the API for consistency with the current design but would like to recommend changing it. We could even add a \"gpu_index_type\" or \"gpu_size_type\" to ATen/Context to ensure consistency and make this type easy to change.</p>\n</blockquote>\n<p>The local convention I've seen is to use <code>int</code> when interacting with the CUDA APIs, but if you put it somewhere durable it turns into an <code>int64_t</code>. There are other good reasons to force people to use 64-bit ints (<code>int</code> on most systems is 32-bit even when the system is 64-bit.) The static casting is irritating, I agree, but I've mostly come to terms with it.</p>", "body_text": "cudaGetDevice() takes an int&, not an int64_t&. I have kept int64_t in the API for consistency with the current design but would like to recommend changing it. We could even add a \"gpu_index_type\" or \"gpu_size_type\" to ATen/Context to ensure consistency and make this type easy to change.\n\nThe local convention I've seen is to use int when interacting with the CUDA APIs, but if you put it somewhere durable it turns into an int64_t. There are other good reasons to force people to use 64-bit ints (int on most systems is 32-bit even when the system is 64-bit.) The static casting is irritating, I agree, but I've mostly come to terms with it.", "body": "> cudaGetDevice() takes an int&, not an int64_t&. I have kept int64_t in the API for consistency with the current design but would like to recommend changing it. We could even add a \"gpu_index_type\" or \"gpu_size_type\" to ATen/Context to ensure consistency and make this type easy to change.\r\n\r\nThe local convention I've seen is to use `int` when interacting with the CUDA APIs, but if you put it somewhere durable it turns into an `int64_t`. There are other good reasons to force people to use 64-bit ints (`int` on most systems is 32-bit even when the system is 64-bit.) The static casting is irritating, I agree, but I've mostly come to terms with it."}