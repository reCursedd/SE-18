{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3146", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3146/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3146/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3146/events", "html_url": "https://github.com/pytorch/pytorch/issues/3146", "id": 266070488, "node_id": "MDU6SXNzdWUyNjYwNzA0ODg=", "number": 3146, "title": "Better performance without OMP", "user": {"login": "loudinthecloud", "id": 3178431, "node_id": "MDQ6VXNlcjMxNzg0MzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/3178431?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loudinthecloud", "html_url": "https://github.com/loudinthecloud", "followers_url": "https://api.github.com/users/loudinthecloud/followers", "following_url": "https://api.github.com/users/loudinthecloud/following{/other_user}", "gists_url": "https://api.github.com/users/loudinthecloud/gists{/gist_id}", "starred_url": "https://api.github.com/users/loudinthecloud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loudinthecloud/subscriptions", "organizations_url": "https://api.github.com/users/loudinthecloud/orgs", "repos_url": "https://api.github.com/users/loudinthecloud/repos", "events_url": "https://api.github.com/users/loudinthecloud/events{/privacy}", "received_events_url": "https://api.github.com/users/loudinthecloud/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}, {"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/performance", "name": "performance", "color": "f9d0c4", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-17T10:20:17Z", "updated_at": "2018-08-14T20:09:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>EDIT (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>): inlined script into the issue</p>\n<hr>\n<p>The following script was used to generate the results below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> sys\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> optim\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n\n<span class=\"pl-c1\">NUM_INPUTS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n<span class=\"pl-c1\">NUM_OUTPUTS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.fc <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">NUM_INPUTS</span>, <span class=\"pl-c1\">NUM_OUTPUTS</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        o <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc(x)\n        <span class=\"pl-k\">return</span> F.sigmoid(o)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">dataloader</span>(<span class=\"pl-smi\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BATCH_SIZE</span>):\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        x <span class=\"pl-k\">=</span> Variable(torch.randn(batch_size, <span class=\"pl-c1\">NUM_INPUTS</span>))\n        y <span class=\"pl-k\">=</span> Variable(torch.randn(batch_size, <span class=\"pl-c1\">NUM_OUTPUTS</span>))\n        <span class=\"pl-k\">yield</span> x, y\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">getus</span>():\n    <span class=\"pl-k\">return</span> time.time() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000000</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-k\">if</span> (<span class=\"pl-c1\">len</span>(sys.argv) <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">and</span> (sys.argv[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>single<span class=\"pl-pds\">'</span></span>):\n        torch.set_num_threads(<span class=\"pl-c1\">1</span>)\n\n    net <span class=\"pl-k\">=</span> Model()\n    criterion <span class=\"pl-k\">=</span> nn.MSELoss()\n    optimizer <span class=\"pl-k\">=</span> optim.SGD(net.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0001</span>)\n\n    timings <span class=\"pl-k\">=</span> []\n\n    <span class=\"pl-k\">for</span> i, (x, y) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(dataloader()):\n        s <span class=\"pl-k\">=</span> getus()\n        optimizer.zero_grad()\n        o <span class=\"pl-k\">=</span> net(x)\n        loss <span class=\"pl-k\">=</span> criterion(o, y)\n        loss.backward()\n        optimizer.step()\n        e <span class=\"pl-k\">=</span> getus()\n\n        timings <span class=\"pl-k\">+=</span> [e <span class=\"pl-k\">-</span> s]\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            arr <span class=\"pl-k\">=</span> np.array(timings)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Time: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span> us (std: <span class=\"pl-c1\">{<span class=\"pl-k\">:.6f</span>}</span> us)<span class=\"pl-pds\">\"</span></span>.format(arr.mean(), arr.std()))\n            timings <span class=\"pl-k\">=</span> []\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<p>As seen below, 4 tests + performance measurements are listed. I tried running pytorch on single core but run into some problems. The tests below are running the benchmark script with/without specifying <code>MKL_NUM_THREADS=1 OMP_NUM_THREADS=1</code> and with/without calling <code>torch.set_num_threads(1)</code></p>\n<p>Problems arise from these tests:</p>\n<ol>\n<li>Running with <code>MKL_NUM_THREADS=1 OMP_NUM_THREADS=1</code> and with <code>torch.set_num_threads(1)</code> has different behavior</li>\n<li>Better performance without MKL/OMP</li>\n<li>Overall low CPU utilization for multi-threading</li>\n<li>High CPU utilization when calling <code>torch.set_num_threads(1)</code> but performance gain is not proportional (Utilization: 22.5% -&gt; 75%, Performance: 700us -&gt; 435us), i.e, overhead included</li>\n<li>No way to run pytorch on single thread</li>\n</ol>\n<hr>\n<p><strong>Test 1</strong><br>\nCommand: <code>./1.py</code><br>\nTime: 689.201600 us (std: 188.663304 us)<br>\nCPU Utilization: 22.5%<br>\n<a href=\"https://github.com/pytorch/pytorch/files/1390607/perf.hist.0.txt\">perf.hist.0.txt</a></p>\n<p><strong>Test 2</strong><br>\nCommand: <code>MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 ./1.py</code><br>\nTime: 541.709725 us (std: 139.295126 us)<br>\nCPU Utilization: 22.5%<br>\n<a href=\"https://github.com/pytorch/pytorch/files/1390608/perf.hist.1.txt\">perf.hist.1.txt</a></p>\n<p><strong>Test 3</strong><br>\nCommand: <code>./1.py single</code><br>\nTime: 435.998200 us (std: 192.966656 us)<br>\nCPU Utilization: 75%<br>\n<a href=\"https://github.com/pytorch/pytorch/files/1390609/perf.hist.2.txt\">perf.hist.2.txt</a></p>\n<p><strong>Test 4</strong><br>\nCommand: <code>MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 ./1.py single</code><br>\nTime: 511.281300 us (std: 66.547017 us)<br>\nUtilization: 24%<br>\n<a href=\"https://github.com/pytorch/pytorch/files/1390610/perf.hist.3.txt\">perf.hist.3.txt</a></p>", "body_text": "EDIT (@apaszke): inlined script into the issue\n\nThe following script was used to generate the results below:\n#!/usr/bin/env python\nimport time\nimport sys\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\n\n\nNUM_INPUTS = 100\nNUM_OUTPUTS = 100\nBATCH_SIZE = 4\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(NUM_INPUTS, NUM_OUTPUTS)\n\n    def forward(self, x):\n        o = self.fc(x)\n        return F.sigmoid(o)\n\ndef dataloader(batch_size=BATCH_SIZE):\n    while True:\n        x = Variable(torch.randn(batch_size, NUM_INPUTS))\n        y = Variable(torch.randn(batch_size, NUM_OUTPUTS))\n        yield x, y\n\n\ndef getus():\n    return time.time() * 1000000\n\ndef main():\n    if (len(sys.argv) > 1) and (sys.argv[1] == 'single'):\n        torch.set_num_threads(1)\n\n    net = Model()\n    criterion = nn.MSELoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.0001)\n\n    timings = []\n\n    for i, (x, y) in enumerate(dataloader()):\n        s = getus()\n        optimizer.zero_grad()\n        o = net(x)\n        loss = criterion(o, y)\n        loss.backward()\n        optimizer.step()\n        e = getus()\n\n        timings += [e - s]\n        if i % 10000 == 0:\n            arr = np.array(timings)\n            print(\"Time: {:.6f} us (std: {:.6f} us)\".format(arr.mean(), arr.std()))\n            timings = []\n\nif __name__ == '__main__':\n    main()\nAs seen below, 4 tests + performance measurements are listed. I tried running pytorch on single core but run into some problems. The tests below are running the benchmark script with/without specifying MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 and with/without calling torch.set_num_threads(1)\nProblems arise from these tests:\n\nRunning with MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 and with torch.set_num_threads(1) has different behavior\nBetter performance without MKL/OMP\nOverall low CPU utilization for multi-threading\nHigh CPU utilization when calling torch.set_num_threads(1) but performance gain is not proportional (Utilization: 22.5% -> 75%, Performance: 700us -> 435us), i.e, overhead included\nNo way to run pytorch on single thread\n\n\nTest 1\nCommand: ./1.py\nTime: 689.201600 us (std: 188.663304 us)\nCPU Utilization: 22.5%\nperf.hist.0.txt\nTest 2\nCommand: MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 ./1.py\nTime: 541.709725 us (std: 139.295126 us)\nCPU Utilization: 22.5%\nperf.hist.1.txt\nTest 3\nCommand: ./1.py single\nTime: 435.998200 us (std: 192.966656 us)\nCPU Utilization: 75%\nperf.hist.2.txt\nTest 4\nCommand: MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 ./1.py single\nTime: 511.281300 us (std: 66.547017 us)\nUtilization: 24%\nperf.hist.3.txt", "body": "\r\nEDIT (@apaszke): inlined script into the issue\r\n\r\n---\r\n\r\nThe following script was used to generate the results below:\r\n```python\r\n#!/usr/bin/env python\r\nimport time\r\nimport sys\r\n\r\nimport numpy as np\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch import optim\r\nimport torch.nn.functional as F\r\n\r\n\r\nNUM_INPUTS = 100\r\nNUM_OUTPUTS = 100\r\nBATCH_SIZE = 4\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.fc = nn.Linear(NUM_INPUTS, NUM_OUTPUTS)\r\n\r\n    def forward(self, x):\r\n        o = self.fc(x)\r\n        return F.sigmoid(o)\r\n\r\ndef dataloader(batch_size=BATCH_SIZE):\r\n    while True:\r\n        x = Variable(torch.randn(batch_size, NUM_INPUTS))\r\n        y = Variable(torch.randn(batch_size, NUM_OUTPUTS))\r\n        yield x, y\r\n\r\n\r\ndef getus():\r\n    return time.time() * 1000000\r\n\r\ndef main():\r\n    if (len(sys.argv) > 1) and (sys.argv[1] == 'single'):\r\n        torch.set_num_threads(1)\r\n\r\n    net = Model()\r\n    criterion = nn.MSELoss()\r\n    optimizer = optim.SGD(net.parameters(), lr=0.0001)\r\n\r\n    timings = []\r\n\r\n    for i, (x, y) in enumerate(dataloader()):\r\n        s = getus()\r\n        optimizer.zero_grad()\r\n        o = net(x)\r\n        loss = criterion(o, y)\r\n        loss.backward()\r\n        optimizer.step()\r\n        e = getus()\r\n\r\n        timings += [e - s]\r\n        if i % 10000 == 0:\r\n            arr = np.array(timings)\r\n            print(\"Time: {:.6f} us (std: {:.6f} us)\".format(arr.mean(), arr.std()))\r\n            timings = []\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nAs seen below, 4 tests + performance measurements are listed. I tried running pytorch on single core but run into some problems. The tests below are running the benchmark script with/without specifying `MKL_NUM_THREADS=1 OMP_NUM_THREADS=1` and with/without calling `torch.set_num_threads(1)`\r\n\r\nProblems arise from these tests:\r\n1. Running with `MKL_NUM_THREADS=1 OMP_NUM_THREADS=1` and with `torch.set_num_threads(1)` has different behavior\r\n2. Better performance without MKL/OMP\r\n3. Overall low CPU utilization for multi-threading\r\n4. High CPU utilization when calling `torch.set_num_threads(1)` but performance gain is not proportional (Utilization: 22.5% -> 75%, Performance: 700us -> 435us), i.e, overhead included\r\n5. No way to run pytorch on single thread\r\n\r\n***\r\n\r\n**Test 1**\r\nCommand: `./1.py`\r\nTime: 689.201600 us (std: 188.663304 us)\r\nCPU Utilization: 22.5%\r\n[perf.hist.0.txt](https://github.com/pytorch/pytorch/files/1390607/perf.hist.0.txt)\r\n\r\n**Test 2**\r\nCommand: `MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 ./1.py`\r\nTime: 541.709725 us (std: 139.295126 us)\r\nCPU Utilization: 22.5%\r\n[perf.hist.1.txt](https://github.com/pytorch/pytorch/files/1390608/perf.hist.1.txt)\r\n\r\n**Test 3**\r\nCommand: `./1.py single`\r\nTime: 435.998200 us (std: 192.966656 us)\r\nCPU Utilization: 75%\r\n[perf.hist.2.txt](https://github.com/pytorch/pytorch/files/1390609/perf.hist.2.txt)\r\n\r\n**Test 4**\r\nCommand: `MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 ./1.py single`\r\nTime: 511.281300 us (std: 66.547017 us)\r\nUtilization: 24%\r\n[perf.hist.3.txt](https://github.com/pytorch/pytorch/files/1390610/perf.hist.3.txt)\r\n"}