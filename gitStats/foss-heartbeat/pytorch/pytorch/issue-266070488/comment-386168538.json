{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386168538", "html_url": "https://github.com/pytorch/pytorch/issues/3146#issuecomment-386168538", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3146", "id": 386168538, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjE2ODUzOA==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T01:24:33Z", "updated_at": "2018-05-03T01:24:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3178431\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/loudinthecloud\">@loudinthecloud</a> a couple of questions linked to this issue here actually.</p>\n<p>first of all, if you intend to do benchmarking. The workload size needs to be big enough to <code>truly</code> reflect hardware performance, no matter cpu or gpu.<br>\ni change your script to</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">NUM_INPUTS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n<span class=\"pl-c1\">NUM_OUTPUTS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span></pre></div>\n<p>this is my test result on <a href=\"https://ark.intel.com/products/120496/Intel-Xeon-Platinum-8180-Processor-38_5M-Cache-2_50-GHz\" rel=\"nofollow\">Intel-Xeon-Platinum-8180-Processor</a>, totally 56 physical cores.<br>\n<code>OMP_NUM_THREADS=2</code> Time: 5804.872500 us (std: 267.272180 us)<br>\n<code>OMP_NUM_THREADS=4</code> Time: 3656.170000 us (std: 56.722756 us)<br>\n<code>OMP_NUM_THREADS=8</code> Time: 2340.300000 us (std: 52.732224 us)<br>\n<code>OMP_NUM_THREADS=16</code> Time: 1946.487500 us (std: 48.236752 us)<br>\n<code>OMP_NUM_THREADS=56</code> Time: 1291.955000 us (std: 15.144858 us)</p>\n<p>Second, cpu utilization is just a rough metric for cpu performance. Your cpu might be spinning and this is also recorded as busy but the ALUs are not working.</p>\n<p>Third, at current version, specify <code>KMP_AFFINITY</code> gives better and more stable performance<br>\n<code>export KMP_AFFINITY=granularity=fine,compact,1,0</code><br>\nthis is largely linked to that PyTorch spawns another set of OpenMP thread pool for backward path aside from forward path, which slows down the forward path also gives unstable cpu performance.<br>\nthis is a very tricky problem and we are close to solve this.<br>\nAnyway, you can have a try with <code>KMP_AFFINITY</code></p>\n<p>You don't have to set <code>MKL_NUM_THREADS</code>, it follows <code>OMP_NUM_THREADS</code> by default. <code>MKL_NUM_THREADS</code> is only needed when you intend to launch different number of threads for blas apart from non-blas calls.</p>", "body_text": "@loudinthecloud a couple of questions linked to this issue here actually.\nfirst of all, if you intend to do benchmarking. The workload size needs to be big enough to truly reflect hardware performance, no matter cpu or gpu.\ni change your script to\nNUM_INPUTS = 1000\nNUM_OUTPUTS = 1000\nBATCH_SIZE = 128\nthis is my test result on Intel-Xeon-Platinum-8180-Processor, totally 56 physical cores.\nOMP_NUM_THREADS=2 Time: 5804.872500 us (std: 267.272180 us)\nOMP_NUM_THREADS=4 Time: 3656.170000 us (std: 56.722756 us)\nOMP_NUM_THREADS=8 Time: 2340.300000 us (std: 52.732224 us)\nOMP_NUM_THREADS=16 Time: 1946.487500 us (std: 48.236752 us)\nOMP_NUM_THREADS=56 Time: 1291.955000 us (std: 15.144858 us)\nSecond, cpu utilization is just a rough metric for cpu performance. Your cpu might be spinning and this is also recorded as busy but the ALUs are not working.\nThird, at current version, specify KMP_AFFINITY gives better and more stable performance\nexport KMP_AFFINITY=granularity=fine,compact,1,0\nthis is largely linked to that PyTorch spawns another set of OpenMP thread pool for backward path aside from forward path, which slows down the forward path also gives unstable cpu performance.\nthis is a very tricky problem and we are close to solve this.\nAnyway, you can have a try with KMP_AFFINITY\nYou don't have to set MKL_NUM_THREADS, it follows OMP_NUM_THREADS by default. MKL_NUM_THREADS is only needed when you intend to launch different number of threads for blas apart from non-blas calls.", "body": "@loudinthecloud a couple of questions linked to this issue here actually.\r\n\r\nfirst of all, if you intend to do benchmarking. The workload size needs to be big enough to `truly` reflect hardware performance, no matter cpu or gpu. \r\ni change your script to \r\n```python\r\nNUM_INPUTS = 1000\r\nNUM_OUTPUTS = 1000\r\nBATCH_SIZE = 128\r\n```\r\nthis is my test result on [Intel-Xeon-Platinum-8180-Processor](https://ark.intel.com/products/120496/Intel-Xeon-Platinum-8180-Processor-38_5M-Cache-2_50-GHz), totally 56 physical cores.\r\n`OMP_NUM_THREADS=2` Time: 5804.872500 us (std: 267.272180 us)\r\n`OMP_NUM_THREADS=4` Time: 3656.170000 us (std: 56.722756 us)\r\n`OMP_NUM_THREADS=8` Time: 2340.300000 us (std: 52.732224 us)\r\n`OMP_NUM_THREADS=16` Time: 1946.487500 us (std: 48.236752 us)\r\n`OMP_NUM_THREADS=56` Time: 1291.955000 us (std: 15.144858 us)\r\n\r\nSecond, cpu utilization is just a rough metric for cpu performance. Your cpu might be spinning and this is also recorded as busy but the ALUs are not working.\r\n\r\nThird, at current version, specify `KMP_AFFINITY` gives better and more stable performance\r\n`export KMP_AFFINITY=granularity=fine,compact,1,0`\r\nthis is largely linked to that PyTorch spawns another set of OpenMP thread pool for backward path aside from forward path, which slows down the forward path also gives unstable cpu performance.\r\nthis is a very tricky problem and we are close to solve this.\r\nAnyway, you can have a try with `KMP_AFFINITY`\r\n\r\nYou don't have to set `MKL_NUM_THREADS`, it follows `OMP_NUM_THREADS` by default. `MKL_NUM_THREADS` is only needed when you intend to launch different number of threads for blas apart from non-blas calls."}