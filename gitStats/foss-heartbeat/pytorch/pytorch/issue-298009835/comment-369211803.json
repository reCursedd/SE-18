{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369211803", "html_url": "https://github.com/pytorch/pytorch/issues/5285#issuecomment-369211803", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5285", "id": 369211803, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTIxMTgwMw==", "user": {"login": "EKami", "id": 4030626, "node_id": "MDQ6VXNlcjQwMzA2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/4030626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EKami", "html_url": "https://github.com/EKami", "followers_url": "https://api.github.com/users/EKami/followers", "following_url": "https://api.github.com/users/EKami/following{/other_user}", "gists_url": "https://api.github.com/users/EKami/gists{/gist_id}", "starred_url": "https://api.github.com/users/EKami/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EKami/subscriptions", "organizations_url": "https://api.github.com/users/EKami/orgs", "repos_url": "https://api.github.com/users/EKami/repos", "events_url": "https://api.github.com/users/EKami/events{/privacy}", "received_events_url": "https://api.github.com/users/EKami/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-28T11:31:54Z", "updated_at": "2018-02-28T11:31:54Z", "author_association": "NONE", "body_html": "<p>Thanks a lot for these informations <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> !<br>\nWell it's very limited =/ . At a hight level that means I only have to use the <code>.cpu()</code> directive if I want to use nnpack? (considering my neural network is compatible). That's sad because I wanted to run inference on the cpu on aws lambda for my algorithm (with lambda being limited to 3gb of RAM usage) but now that this issue exist I'll have to look at another framework like TF :( .</p>", "body_text": "Thanks a lot for these informations @fmassa !\nWell it's very limited =/ . At a hight level that means I only have to use the .cpu() directive if I want to use nnpack? (considering my neural network is compatible). That's sad because I wanted to run inference on the cpu on aws lambda for my algorithm (with lambda being limited to 3gb of RAM usage) but now that this issue exist I'll have to look at another framework like TF :( .", "body": "Thanks a lot for these informations @fmassa !\r\nWell it's very limited =/ . At a hight level that means I only have to use the `.cpu()` directive if I want to use nnpack? (considering my neural network is compatible). That's sad because I wanted to run inference on the cpu on aws lambda for my algorithm (with lambda being limited to 3gb of RAM usage) but now that this issue exist I'll have to look at another framework like TF :( . "}