{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5285", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5285/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5285/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5285/events", "html_url": "https://github.com/pytorch/pytorch/issues/5285", "id": 298009835, "node_id": "MDU6SXNzdWUyOTgwMDk4MzU=", "number": 5285, "title": "[Bug] Memory leak on Convnet on CPU", "user": {"login": "EKami", "id": 4030626, "node_id": "MDQ6VXNlcjQwMzA2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/4030626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EKami", "html_url": "https://github.com/EKami", "followers_url": "https://api.github.com/users/EKami/followers", "following_url": "https://api.github.com/users/EKami/following{/other_user}", "gists_url": "https://api.github.com/users/EKami/gists{/gist_id}", "starred_url": "https://api.github.com/users/EKami/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EKami/subscriptions", "organizations_url": "https://api.github.com/users/EKami/orgs", "repos_url": "https://api.github.com/users/EKami/repos", "events_url": "https://api.github.com/users/EKami/events{/privacy}", "received_events_url": "https://api.github.com/users/EKami/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 19, "created_at": "2018-02-17T14:47:50Z", "updated_at": "2018-03-16T14:45:55Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.3.1b0+2b47480</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: CUDA 9.0/CuDnn 7</li>\n<li>GPU models and configuration: GTX 1080Ti (as well as GTX 1070)</li>\n<li>GCC version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609</li>\n</ul>\n<p>Hello,<br>\nWhile implementing the <a href=\"https://arxiv.org/abs/1609.04802\" rel=\"nofollow\">SRGan paper</a> I ran into a memory leak issue while doing the inference on my CPU.<br>\nThis issue should be easily reproductible from your side as my implementation is <a href=\"https://github.com/EKami/Torchlight/tree/showcase/memory-leak\">here</a>. All you have to do is to clone the repository with <code>git clone -b showcase/memory-leak git@github.com:EKami/Torchlite.git</code>, cd into the <code>examples</code> folder then run the script with <code>python srgan.py eval --on_cpu</code> to run the inference on the cpu. Then you should get a memory leak with the following message:</p>\n<pre><code>RuntimeError: $ Torch: not enough memory: you tried to allocate 116GB. Buy new RAM!\n</code></pre>\n<p>If you run the same script but with <code>python srgan.py eval</code> (defaults on cuda) then the memory leak vanishes. The exact line which cause the memory leak is <a href=\"https://github.com/EKami/Torchlight/blob/showcase/memory-leak/torchlight/nn/models/srgan.py#L42\">this one</a>. Remove that line to get:</p>\n<pre><code>        block_x2 = self.block_x2(block1 + block_x1)  # ElementWise sum\n\n        # TODO causes a memory leak on CPU\n        # block_x3 = self.block_x3(block_x2)\n\n        return (F.tanh(block_x2) + 1) / 2\n</code></pre>\n<p>and execute the script on the cpu again with <code>python srgan.py eval --on_cpu</code> and poof the memory leak vanishes.<br>\nI tried on 2 different computer each with the same software installed but for the hardware one has:</p>\n<ul>\n<li>AMD FX 8350</li>\n<li>GTX 1070<br>\nThe second:</li>\n<li>Intel i7 7700k</li>\n<li>GTX 1080Ti</li>\n</ul>\n<p>And I get the same memory leak on both machines.</p>", "body_text": "OS: Ubuntu 16.04\nPyTorch version: 0.3.1b0+2b47480\nHow you installed PyTorch (conda, pip, source): source\nPython version: 3.6.4\nCUDA/cuDNN version: CUDA 9.0/CuDnn 7\nGPU models and configuration: GTX 1080Ti (as well as GTX 1070)\nGCC version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\n\nHello,\nWhile implementing the SRGan paper I ran into a memory leak issue while doing the inference on my CPU.\nThis issue should be easily reproductible from your side as my implementation is here. All you have to do is to clone the repository with git clone -b showcase/memory-leak git@github.com:EKami/Torchlite.git, cd into the examples folder then run the script with python srgan.py eval --on_cpu to run the inference on the cpu. Then you should get a memory leak with the following message:\nRuntimeError: $ Torch: not enough memory: you tried to allocate 116GB. Buy new RAM!\n\nIf you run the same script but with python srgan.py eval (defaults on cuda) then the memory leak vanishes. The exact line which cause the memory leak is this one. Remove that line to get:\n        block_x2 = self.block_x2(block1 + block_x1)  # ElementWise sum\n\n        # TODO causes a memory leak on CPU\n        # block_x3 = self.block_x3(block_x2)\n\n        return (F.tanh(block_x2) + 1) / 2\n\nand execute the script on the cpu again with python srgan.py eval --on_cpu and poof the memory leak vanishes.\nI tried on 2 different computer each with the same software installed but for the hardware one has:\n\nAMD FX 8350\nGTX 1070\nThe second:\nIntel i7 7700k\nGTX 1080Ti\n\nAnd I get the same memory leak on both machines.", "body": "- OS: Ubuntu 16.04\r\n- PyTorch version: 0.3.1b0+2b47480\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: CUDA 9.0/CuDnn 7\r\n- GPU models and configuration: GTX 1080Ti (as well as GTX 1070)\r\n- GCC version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\r\n\r\nHello,\r\nWhile implementing the [SRGan paper](https://arxiv.org/abs/1609.04802) I ran into a memory leak issue while doing the inference on my CPU.\r\nThis issue should be easily reproductible from your side as my implementation is [here](https://github.com/EKami/Torchlight/tree/showcase/memory-leak). All you have to do is to clone the repository with `git clone -b showcase/memory-leak git@github.com:EKami/Torchlite.git`, cd into the `examples` folder then run the script with `python srgan.py eval --on_cpu` to run the inference on the cpu. Then you should get a memory leak with the following message:\r\n```\r\nRuntimeError: $ Torch: not enough memory: you tried to allocate 116GB. Buy new RAM!\r\n```\r\nIf you run the same script but with `python srgan.py eval` (defaults on cuda) then the memory leak vanishes. The exact line which cause the memory leak is [this one](https://github.com/EKami/Torchlight/blob/showcase/memory-leak/torchlight/nn/models/srgan.py#L42). Remove that line to get:\r\n```\r\n        block_x2 = self.block_x2(block1 + block_x1)  # ElementWise sum\r\n\r\n        # TODO causes a memory leak on CPU\r\n        # block_x3 = self.block_x3(block_x2)\r\n\r\n        return (F.tanh(block_x2) + 1) / 2\r\n```\r\nand execute the script on the cpu again with `python srgan.py eval --on_cpu` and poof the memory leak vanishes.\r\nI tried on 2 different computer each with the same software installed but for the hardware one has:\r\n - AMD FX 8350\r\n - GTX 1070\r\nThe second:\r\n - Intel i7 7700k\r\n - GTX 1080Ti\r\n\r\nAnd I get the same memory leak on both machines."}