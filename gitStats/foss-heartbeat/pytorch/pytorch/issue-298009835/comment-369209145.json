{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369209145", "html_url": "https://github.com/pytorch/pytorch/issues/5285#issuecomment-369209145", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5285", "id": 369209145, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTIwOTE0NQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-28T11:20:20Z", "updated_at": "2018-02-28T11:20:20Z", "author_association": "MEMBER", "body_html": "<p>The convolutions on the GPU uses cudnn, which does not use the same <code>unfold</code> technique so uses much less memory.</p>\n<p>For the moment, I'd say that the only way of reducing memory usage would be to either go through the <code>NNPack</code> binding, which in the master branch <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Convolution.cpp#L133-L139\">is enabled in the following cases</a>, or reducing the batch size / image size that you feed to your model.</p>\n<p>I've <a href=\"https://github.com/torch/nn/issues/501\" data-hovercard-type=\"issue\" data-hovercard-url=\"/torch/nn/issues/501/hovercard\">mentioned in the past</a> about the large memory requirements of convolutions on CPU, but we didn't reach an agreement</p>", "body_text": "The convolutions on the GPU uses cudnn, which does not use the same unfold technique so uses much less memory.\nFor the moment, I'd say that the only way of reducing memory usage would be to either go through the NNPack binding, which in the master branch is enabled in the following cases, or reducing the batch size / image size that you feed to your model.\nI've mentioned in the past about the large memory requirements of convolutions on CPU, but we didn't reach an agreement", "body": "The convolutions on the GPU uses cudnn, which does not use the same `unfold` technique so uses much less memory.\r\n\r\nFor the moment, I'd say that the only way of reducing memory usage would be to either go through the `NNPack` binding, which in the master branch [is enabled in the following cases](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Convolution.cpp#L133-L139), or reducing the batch size / image size that you feed to your model.\r\n\r\nI've [mentioned in the past](https://github.com/torch/nn/issues/501) about the large memory requirements of convolutions on CPU, but we didn't reach an agreement"}