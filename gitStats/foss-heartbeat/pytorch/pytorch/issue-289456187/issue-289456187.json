{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4714", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4714/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4714/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4714/events", "html_url": "https://github.com/pytorch/pytorch/issues/4714", "id": 289456187, "node_id": "MDU6SXNzdWUyODk0NTYxODc=", "number": 4714, "title": "Custom backward() not called", "user": {"login": "gabrielhuang", "id": 7798468, "node_id": "MDQ6VXNlcjc3OTg0Njg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7798468?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gabrielhuang", "html_url": "https://github.com/gabrielhuang", "followers_url": "https://api.github.com/users/gabrielhuang/followers", "following_url": "https://api.github.com/users/gabrielhuang/following{/other_user}", "gists_url": "https://api.github.com/users/gabrielhuang/gists{/gist_id}", "starred_url": "https://api.github.com/users/gabrielhuang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gabrielhuang/subscriptions", "organizations_url": "https://api.github.com/users/gabrielhuang/orgs", "repos_url": "https://api.github.com/users/gabrielhuang/repos", "events_url": "https://api.github.com/users/gabrielhuang/events{/privacy}", "received_events_url": "https://api.github.com/users/gabrielhuang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-17T23:54:13Z", "updated_at": "2018-01-17T23:59:39Z", "closed_at": "2018-01-17T23:59:39Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>I am having this issue where the <code>backward()</code> of a custom <code>torch.autograd.Function</code> is not called, when said function is not directly applied to a Leaf Variable.</p>\n<p>My pytorch version is <code>0.3.0.post4</code>.</p>\n<p>Thanks!<br>\n-Gabriel Huang</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.autograd <span class=\"pl-k\">as</span> autograd\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyFun</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> x\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_out</span>):\n        grad_input <span class=\"pl-k\">=</span> grad_out.clone()\n        <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Custom backward called!<span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-k\">return</span> grad_input <span class=\"pl-k\">*</span> <span class=\"pl-c1\">666</span>\n\n    \n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>***** MyFun(x) ***** <span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>\nx <span class=\"pl-k\">=</span> autograd.Variable(torch.Tensor([<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nz <span class=\"pl-k\">=</span> MyFun.apply(x)\nz.backward()\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Gradient<span class=\"pl-pds\">'</span></span>, x.grad  <span class=\"pl-c\"><span class=\"pl-c\">#</span> this is 666, which is correct.</span>\n\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>***** MyFun(x+1) ***** <span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>\nx <span class=\"pl-k\">=</span> autograd.Variable(torch.Tensor([<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nz <span class=\"pl-k\">=</span> MyFun.apply(x<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)\nz.backward()\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Gradient<span class=\"pl-pds\">'</span></span>, x.grad  <span class=\"pl-c\"><span class=\"pl-c\">#</span> this should be 666, not 1.</span></pre></div>\n<p>Output:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">*****</span> MyFun(x) <span class=\"pl-k\">*****</span> \n\nCustom backward called!\nGradient Variable containing:\n <span class=\"pl-c1\">666</span>\n[torch.FloatTensor of size <span class=\"pl-c1\">1</span>]\n\n<span class=\"pl-k\">*****</span> MyFun(x<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*****</span> \n\nGradient Variable containing:\n <span class=\"pl-c1\">1</span>\n[torch.FloatTensor of size <span class=\"pl-c1\">1</span>]</pre></div>", "body_text": "Hi all,\nI am having this issue where the backward() of a custom torch.autograd.Function is not called, when said function is not directly applied to a Leaf Variable.\nMy pytorch version is 0.3.0.post4.\nThanks!\n-Gabriel Huang\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\n\nclass MyFun(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        return x\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        grad_input = grad_out.clone()\n        print 'Custom backward called!'\n        return grad_input * 666\n\n    \nprint '***** MyFun(x) ***** \\n'\nx = autograd.Variable(torch.Tensor([1]), requires_grad=True)\nz = MyFun.apply(x)\nz.backward()\nprint 'Gradient', x.grad  # this is 666, which is correct.\n\nprint '***** MyFun(x+1) ***** \\n'\nx = autograd.Variable(torch.Tensor([1]), requires_grad=True)\nz = MyFun.apply(x+1)\nz.backward()\nprint 'Gradient', x.grad  # this should be 666, not 1.\nOutput:\n***** MyFun(x) ***** \n\nCustom backward called!\nGradient Variable containing:\n 666\n[torch.FloatTensor of size 1]\n\n***** MyFun(x+1) ***** \n\nGradient Variable containing:\n 1\n[torch.FloatTensor of size 1]", "body": "Hi all,\r\n\r\nI am having this issue where the `backward()` of a custom `torch.autograd.Function` is not called, when said function is not directly applied to a Leaf Variable.\r\n\r\nMy pytorch version is `0.3.0.post4`.\r\n\r\nThanks!\r\n-Gabriel Huang\r\n\r\n\r\n```python\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\n\r\nclass MyFun(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        return x\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        grad_input = grad_out.clone()\r\n        print 'Custom backward called!'\r\n        return grad_input * 666\r\n\r\n    \r\nprint '***** MyFun(x) ***** \\n'\r\nx = autograd.Variable(torch.Tensor([1]), requires_grad=True)\r\nz = MyFun.apply(x)\r\nz.backward()\r\nprint 'Gradient', x.grad  # this is 666, which is correct.\r\n\r\nprint '***** MyFun(x+1) ***** \\n'\r\nx = autograd.Variable(torch.Tensor([1]), requires_grad=True)\r\nz = MyFun.apply(x+1)\r\nz.backward()\r\nprint 'Gradient', x.grad  # this should be 666, not 1.\r\n```\r\n\r\nOutput:\r\n```python\r\n***** MyFun(x) ***** \r\n\r\nCustom backward called!\r\nGradient Variable containing:\r\n 666\r\n[torch.FloatTensor of size 1]\r\n\r\n***** MyFun(x+1) ***** \r\n\r\nGradient Variable containing:\r\n 1\r\n[torch.FloatTensor of size 1]\r\n```"}