{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396776297", "html_url": "https://github.com/pytorch/pytorch/issues/8062#issuecomment-396776297", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8062", "id": 396776297, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Njc3NjI5Nw==", "user": {"login": "li-roy", "id": 8813817, "node_id": "MDQ6VXNlcjg4MTM4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8813817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/li-roy", "html_url": "https://github.com/li-roy", "followers_url": "https://api.github.com/users/li-roy/followers", "following_url": "https://api.github.com/users/li-roy/following{/other_user}", "gists_url": "https://api.github.com/users/li-roy/gists{/gist_id}", "starred_url": "https://api.github.com/users/li-roy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/li-roy/subscriptions", "organizations_url": "https://api.github.com/users/li-roy/orgs", "repos_url": "https://api.github.com/users/li-roy/repos", "events_url": "https://api.github.com/users/li-roy/events{/privacy}", "received_events_url": "https://api.github.com/users/li-roy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-13T00:31:48Z", "updated_at": "2018-06-13T00:31:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think the output of both of your scripts is actually expected behavior. Let me know if I'm misunderstanding or if you disagree.</p>\n<p>In the first example, we're take an average weighted on the weights array, so (4000 + 0)/(20 + 10) = 133.333 which is expected.</p>\n<p>In the second example, I think you're not noticing a change due to weights because the third loss element ends up being zero, and it's the only one that has label 1 (thus the only one that would be affected by the 10 in your weights array). If you change the first element of your weights array, you'll be able to see a change reflected in your loss:</p>\n<pre><code>input = torch.tensor([\n    [-1,  1, -1],\n    [-1,  1, -1],\n    [-1,  1, -1],\n], dtype=torch.float) * 100\ntarget = torch.tensor([0,0,1], dtype=torch.long)\nweights = torch.tensor([10,10,1], dtype=torch.float)\n\nce_noweights = nn.CrossEntropyLoss(weight=None, reduce=False)\nce_weights = nn.CrossEntropyLoss(weight=weights, reduce=False)\n\nloss1 = ce_noweights(input, target)\nloss2 = ce_weights(input, target)\n\nprint(loss1.mean())\nprint(loss2.mean())\n</code></pre>\n<p>output:</p>\n<pre><code>tensor(133.3333)\ntensor(1333.3334)\n</code></pre>", "body_text": "I think the output of both of your scripts is actually expected behavior. Let me know if I'm misunderstanding or if you disagree.\nIn the first example, we're take an average weighted on the weights array, so (4000 + 0)/(20 + 10) = 133.333 which is expected.\nIn the second example, I think you're not noticing a change due to weights because the third loss element ends up being zero, and it's the only one that has label 1 (thus the only one that would be affected by the 10 in your weights array). If you change the first element of your weights array, you'll be able to see a change reflected in your loss:\ninput = torch.tensor([\n    [-1,  1, -1],\n    [-1,  1, -1],\n    [-1,  1, -1],\n], dtype=torch.float) * 100\ntarget = torch.tensor([0,0,1], dtype=torch.long)\nweights = torch.tensor([10,10,1], dtype=torch.float)\n\nce_noweights = nn.CrossEntropyLoss(weight=None, reduce=False)\nce_weights = nn.CrossEntropyLoss(weight=weights, reduce=False)\n\nloss1 = ce_noweights(input, target)\nloss2 = ce_weights(input, target)\n\nprint(loss1.mean())\nprint(loss2.mean())\n\noutput:\ntensor(133.3333)\ntensor(1333.3334)", "body": "I think the output of both of your scripts is actually expected behavior. Let me know if I'm misunderstanding or if you disagree.\r\n\r\nIn the first example, we're take an average weighted on the weights array, so (4000 + 0)/(20 + 10) = 133.333 which is expected.\r\n\r\nIn the second example, I think you're not noticing a change due to weights because the third loss element ends up being zero, and it's the only one that has label 1 (thus the only one that would be affected by the 10 in your weights array). If you change the first element of your weights array, you'll be able to see a change reflected in your loss:\r\n\r\n```\r\ninput = torch.tensor([\r\n    [-1,  1, -1],\r\n    [-1,  1, -1],\r\n    [-1,  1, -1],\r\n], dtype=torch.float) * 100\r\ntarget = torch.tensor([0,0,1], dtype=torch.long)\r\nweights = torch.tensor([10,10,1], dtype=torch.float)\r\n\r\nce_noweights = nn.CrossEntropyLoss(weight=None, reduce=False)\r\nce_weights = nn.CrossEntropyLoss(weight=weights, reduce=False)\r\n\r\nloss1 = ce_noweights(input, target)\r\nloss2 = ce_weights(input, target)\r\n\r\nprint(loss1.mean())\r\nprint(loss2.mean())\r\n```\r\noutput:\r\n```\r\ntensor(133.3333)\r\ntensor(1333.3334)\r\n```"}