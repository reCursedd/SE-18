{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434752659", "html_url": "https://github.com/pytorch/pytorch/issues/13023#issuecomment-434752659", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13023", "id": 434752659, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDc1MjY1OQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T16:22:15Z", "updated_at": "2018-10-31T16:22:15Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>Do you expect the iterator to resume from the beginning or from the particular place at which it was serialized? If you thought about the first case, why would be serialize the iterator instead of the data loader?</p>\n</blockquote>\n<p>The second. However, considering that we cache out-of-order batches, this may not work perfectly.</p>\n<blockquote>\n<p>I'm not sure if returning self from <strong>iter</strong> is a standard practice or not.</p>\n</blockquote>\n<p>It's used a lot, but not standard so we can't rely on that.</p>\n<blockquote>\n<p>NB implementing a flag that enforces strict consistency is possible too (we could flush the pipeline, and retry with new sample indices), but it's not all that simple, so I'd wait until it's really needed before we proceed.</p>\n</blockquote>\n<p>I agree.</p>\n<blockquote>\n<p>just accept an extra argument in the data loader, which would be a list of length equal to num_workers, where every element is a tuple of args to append to init_worker_fn.</p>\n</blockquote>\n<p>This solves only one of the two problems. The <code>worker_init_fn</code> you described still doesn't have access to the <code>dataset</code> object, which it may need to configure differently for different workers.</p>", "body_text": "Do you expect the iterator to resume from the beginning or from the particular place at which it was serialized? If you thought about the first case, why would be serialize the iterator instead of the data loader?\n\nThe second. However, considering that we cache out-of-order batches, this may not work perfectly.\n\nI'm not sure if returning self from iter is a standard practice or not.\n\nIt's used a lot, but not standard so we can't rely on that.\n\nNB implementing a flag that enforces strict consistency is possible too (we could flush the pipeline, and retry with new sample indices), but it's not all that simple, so I'd wait until it's really needed before we proceed.\n\nI agree.\n\njust accept an extra argument in the data loader, which would be a list of length equal to num_workers, where every element is a tuple of args to append to init_worker_fn.\n\nThis solves only one of the two problems. The worker_init_fn you described still doesn't have access to the dataset object, which it may need to configure differently for different workers.", "body": "> Do you expect the iterator to resume from the beginning or from the particular place at which it was serialized? If you thought about the first case, why would be serialize the iterator instead of the data loader?\r\n\r\nThe second. However, considering that we cache out-of-order batches, this may not work perfectly.\r\n\r\n> I'm not sure if returning self from __iter__ is a standard practice or not.\r\n\r\nIt's used a lot, but not standard so we can't rely on that.\r\n\r\n> NB implementing a flag that enforces strict consistency is possible too (we could flush the pipeline, and retry with new sample indices), but it's not all that simple, so I'd wait until it's really needed before we proceed.\r\n\r\nI agree.\r\n\r\n> just accept an extra argument in the data loader, which would be a list of length equal to num_workers, where every element is a tuple of args to append to init_worker_fn.\r\n\r\nThis solves only one of the two problems. The `worker_init_fn` you described still doesn't have access to the `dataset` object, which it may need to configure differently for different workers.\r\n"}