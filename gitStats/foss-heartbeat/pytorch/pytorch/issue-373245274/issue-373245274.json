{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13023", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13023/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13023/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13023/events", "html_url": "https://github.com/pytorch/pytorch/issues/13023", "id": 373245274, "node_id": "MDU6SXNzdWUzNzMyNDUyNzQ=", "number": 13023, "title": "Python dataloader Improvements", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-10-23T23:36:40Z", "updated_at": "2018-11-02T18:58:35Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6429851\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/goldsborough\">@goldsborough</a> and I are planning a series of improvements to dataloader in both C++ and Python API. This issue mainly focuses on the planned changes for the Python API.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Iterator Serialization</p>\n<p>Dataloader iterator is not picklable currently, due to the multiprocessing and multithreading attributes it has. We should make it picklable as long as the dataset and the Sampler iterator is picklable, e.g., the <code>__getstate__</code> could be</p>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getstate__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n      <span class=\"pl-k\">return</span> (<span class=\"pl-c1\">self</span>.loader, <span class=\"pl-c1\">self</span>.sampler_iter, <span class=\"pl-c1\">self</span>.base_seed)</pre></div>\n<p>We will also make the iterator of provided samplers serializable.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Examples of Bulk Loading</p>\n<p>The current dataloding API seems to suggest that dataloader is mainly suited for creating batches from random reads of the dataset. However, it supports bulk loading very well. For instance, this <a href=\"https://gist.github.com/SsnL/205a4cd2e4e631a42cc9d8e879a296dc\">gist</a> implements sharded/chunked bulk loading in just 40 lines. We will improve the documentation to include examples of such cases.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Worker Load Configurations</p>\n<p>We currently balance the load of workers by keeping the #tasks per worker balanced. This could be a problem if the workload is not very even for the tasks. We should make this optional instead. Additionally, the max task number (currently <code>2 * num_workers</code>) should also become configurable.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Expose Sampler iterator</p>\n<p>This would enable dynamic updates to the Sampler iterator states, e.g., dynamic reweighting of the samples. The API may be <code>loader_iter.get_sampler_iter()</code>. Since we always prefetch some number of batches, we also need to augment the existing document to reflect that this iterator may be ahead of the latest return value of the data loader iterator.</p>\n<p>Edit: As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> pointed out below, it is possible to allow for strict consistency by providing a interface to flush the pipeline and ask sampler iterator to give new indices basing on the updated state. But that design needs further consideration and we don't plan to do until there is immediate need.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> More Flexible <code>worker_init_fn</code></p>\n<p>Currently, <code>worker_init_fn</code> only takes in a single <code>worker_idx</code> argument, making it very difficult to initialize dataset object of each worker in a different way. Furthermore, it is impossible for the <code>dataset.__getitem__</code> in workers to communicate with the Sampler iterator to fetch more indices, or update the iterator state. We plan to augment it's input argument to include a wider range of objects it can access, without being BC breaking, and being future-proof.</p>\n<p>I haven't given much thought to the API design of this. But for a proof-of-concept, the API could be a <code>get_worker_init_fn_arg</code> argument which would be called in <strong>main</strong> process, takes in a <code>data_loader_iter_info</code> \"struct\", containing fields referencing the <code>dataset</code> and the <code>sampler_iter</code> (and maybe more), and returns a serializable to be fed in as an additional argument of <code>worker_init_fn</code> in <strong>worker</strong> processes. Please let me know if you have suggestions!</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Iterator-style Dataset</p>\n<p>We don't necessarily need to have a sampler. By allowing an iterator style Dataset (rather than a stateless mapping), the workers can do interesting things like backfilling. This is entirely supported as of today, but we will make it nicer.</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Bridging C++ and Python DataLoaders</p>\n<p>We will be providing a simple way to convert a C++ DataLoader into a Python one, with the same API as the existing Python DataLoader.</p>\n</li>\n</ul>\n<h1>Our Plan to Make These Happen</h1>\n<p>I (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a>) will be focusing on the first four items while <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6429851\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/goldsborough\">@goldsborough</a> will implement the fifth. In addition to these, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6429851\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/goldsborough\">@goldsborough</a> is also adding a bunch of exciting features into the C++ DataLoader to allow for greater flexibility (e.g., see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"372752949\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12960\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12960/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12960\">#12960</a> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"373153128\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12999\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/12999/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/12999\">#12999</a>).</p>\n<p>Let us know your thoughts and suggestions!</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>", "body_text": "@goldsborough and I are planning a series of improvements to dataloader in both C++ and Python API. This issue mainly focuses on the planned changes for the Python API.\n\n\n Iterator Serialization\nDataloader iterator is not picklable currently, due to the multiprocessing and multithreading attributes it has. We should make it picklable as long as the dataset and the Sampler iterator is picklable, e.g., the __getstate__ could be\n  def __getstate__(self):\n      return (self.loader, self.sampler_iter, self.base_seed)\nWe will also make the iterator of provided samplers serializable.\n\n\n Examples of Bulk Loading\nThe current dataloding API seems to suggest that dataloader is mainly suited for creating batches from random reads of the dataset. However, it supports bulk loading very well. For instance, this gist implements sharded/chunked bulk loading in just 40 lines. We will improve the documentation to include examples of such cases.\n\n\n Worker Load Configurations\nWe currently balance the load of workers by keeping the #tasks per worker balanced. This could be a problem if the workload is not very even for the tasks. We should make this optional instead. Additionally, the max task number (currently 2 * num_workers) should also become configurable.\n\n\n Expose Sampler iterator\nThis would enable dynamic updates to the Sampler iterator states, e.g., dynamic reweighting of the samples. The API may be loader_iter.get_sampler_iter(). Since we always prefetch some number of batches, we also need to augment the existing document to reflect that this iterator may be ahead of the latest return value of the data loader iterator.\nEdit: As @apaszke pointed out below, it is possible to allow for strict consistency by providing a interface to flush the pipeline and ask sampler iterator to give new indices basing on the updated state. But that design needs further consideration and we don't plan to do until there is immediate need.\n\n\n More Flexible worker_init_fn\nCurrently, worker_init_fn only takes in a single worker_idx argument, making it very difficult to initialize dataset object of each worker in a different way. Furthermore, it is impossible for the dataset.__getitem__ in workers to communicate with the Sampler iterator to fetch more indices, or update the iterator state. We plan to augment it's input argument to include a wider range of objects it can access, without being BC breaking, and being future-proof.\nI haven't given much thought to the API design of this. But for a proof-of-concept, the API could be a get_worker_init_fn_arg argument which would be called in main process, takes in a data_loader_iter_info \"struct\", containing fields referencing the dataset and the sampler_iter (and maybe more), and returns a serializable to be fed in as an additional argument of worker_init_fn in worker processes. Please let me know if you have suggestions!\n\n\n Iterator-style Dataset\nWe don't necessarily need to have a sampler. By allowing an iterator style Dataset (rather than a stateless mapping), the workers can do interesting things like backfilling. This is entirely supported as of today, but we will make it nicer.\n\n\n Bridging C++ and Python DataLoaders\nWe will be providing a simple way to convert a C++ DataLoader into a Python one, with the same API as the existing Python DataLoader.\n\n\nOur Plan to Make These Happen\nI (@SsnL) will be focusing on the first four items while @goldsborough will implement the fifth. In addition to these, @goldsborough is also adding a bunch of exciting features into the C++ DataLoader to allow for greater flexibility (e.g., see #12960 #12999).\nLet us know your thoughts and suggestions!\ncc @soumith @fmassa @apaszke", "body": "@goldsborough and I are planning a series of improvements to dataloader in both C++ and Python API. This issue mainly focuses on the planned changes for the Python API.\r\n\r\n- [ ] Iterator Serialization\r\n\r\n  Dataloader iterator is not picklable currently, due to the multiprocessing and multithreading attributes it has. We should make it picklable as long as the dataset and the Sampler iterator is picklable, e.g., the `__getstate__` could be\r\n  ```py\r\n    def __getstate__(self):\r\n        return (self.loader, self.sampler_iter, self.base_seed)\r\n  ```\r\n\r\n  We will also make the iterator of provided samplers serializable.\r\n\r\n- [ ] Examples of Bulk Loading\r\n\r\n  The current dataloding API seems to suggest that dataloader is mainly suited for creating batches from random reads of the dataset. However, it supports bulk loading very well. For instance, this [gist](https://gist.github.com/SsnL/205a4cd2e4e631a42cc9d8e879a296dc) implements sharded/chunked bulk loading in just 40 lines. We will improve the documentation to include examples of such cases.\r\n\r\n- [ ] Worker Load Configurations\r\n\r\n  We currently balance the load of workers by keeping the #tasks per worker balanced. This could be a problem if the workload is not very even for the tasks. We should make this optional instead. Additionally, the max task number (currently ``2 * num_workers``) should also become configurable.\r\n\r\n- [ ] Expose Sampler iterator\r\n\r\n  This would enable dynamic updates to the Sampler iterator states, e.g., dynamic reweighting of the samples. The API may be `loader_iter.get_sampler_iter()`. Since we always prefetch some number of batches, we also need to augment the existing document to reflect that this iterator may be ahead of the latest return value of the data loader iterator. \r\n\r\n  Edit: As @apaszke pointed out below, it is possible to allow for strict consistency by providing a interface to flush the pipeline and ask sampler iterator to give new indices basing on the updated state. But that design needs further consideration and we don't plan to do until there is immediate need.\r\n\r\n- [ ] More Flexible `worker_init_fn`\r\n  \r\n  Currently, `worker_init_fn` only takes in a single `worker_idx` argument, making it very difficult to initialize dataset object of each worker in a different way. Furthermore, it is impossible for the `dataset.__getitem__` in workers to communicate with the Sampler iterator to fetch more indices, or update the iterator state. We plan to augment it's input argument to include a wider range of objects it can access, without being BC breaking, and being future-proof. \r\n\r\n  I haven't given much thought to the API design of this. But for a proof-of-concept, the API could be a `get_worker_init_fn_arg` argument which would be called in **main** process, takes in a `data_loader_iter_info` \"struct\", containing fields referencing the `dataset` and the `sampler_iter` (and maybe more), and returns a serializable to be fed in as an additional argument of `worker_init_fn` in **worker** processes. Please let me know if you have suggestions!\r\n\r\n- [ ] Iterator-style Dataset\r\n\r\n  We don't necessarily need to have a sampler. By allowing an iterator style Dataset (rather than a stateless mapping), the workers can do interesting things like backfilling. This is entirely supported as of today, but we will make it nicer.\r\n\r\n- [ ] Bridging C++ and Python DataLoaders\r\n\r\n  We will be providing a simple way to convert a C++ DataLoader into a Python one, with the same API as the existing Python DataLoader. \r\n\r\n# Our Plan to Make These Happen\r\nI (@ssnl) will be focusing on the first four items while @goldsborough will implement the fifth. In addition to these, @goldsborough is also adding a bunch of exciting features into the C++ DataLoader to allow for greater flexibility (e.g., see https://github.com/pytorch/pytorch/pull/12960 https://github.com/pytorch/pytorch/pull/12999).\r\n\r\nLet us know your thoughts and suggestions! \r\n\r\n\r\ncc @soumith @fmassa @apaszke "}