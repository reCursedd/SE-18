{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195347180", "pull_request_review_id": 128695675, "id": 195347180, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTM0NzE4MA==", "diff_hunk": "@@ -3026,6 +3043,404 @@ def fn(x):\n         self.checkScript(fn, (torch.tensor(2),))\n \n \n+class TestEndToEndHybridFrontendModels(JitTestCase):\n+\n+    def test_dcgan_models(self):\n+        class DCGANGenerator(nn.Module):\n+            def __init__(self, nz, ngf, nc):\n+                super(DCGANGenerator, self).__init__()\n+                self.main = nn.Sequential(\n+                    # input is Z, going into a convolution\n+                    nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n+                    nn.BatchNorm2d(ngf * 8),\n+                    nn.ReLU(True),\n+                    # state size. (ngf*8) x 4 x 4\n+                    nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n+                    nn.BatchNorm2d(ngf * 4),\n+                    nn.ReLU(True),\n+                    # state size. (ngf*4) x 8 x 8\n+                    nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n+                    nn.BatchNorm2d(ngf * 2),\n+                    nn.ReLU(True),\n+                    # state size. (ngf*2) x 16 x 16\n+                    nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n+                    nn.BatchNorm2d(ngf),\n+                    nn.ReLU(True),\n+                    # state size. (ngf) x 32 x 32\n+                    nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n+                    nn.Tanh()\n+                    # state size. (nc) x 64 x 64\n+                )\n+\n+            def forward(self, input):\n+                return self.main(input)\n+\n+        class DCGANDiscriminator(nn.Module):\n+            def __init__(self, nc, ndf):\n+                super(DCGANDiscriminator, self).__init__()\n+                self.main = nn.Sequential(\n+                    # input is (nc) x 64 x 64\n+                    nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n+                    nn.LeakyReLU(0.2, inplace=True),\n+                    # state size. (ndf) x 32 x 32\n+                    nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n+                    nn.BatchNorm2d(ndf * 2),\n+                    nn.LeakyReLU(0.2, inplace=True),\n+                    # state size. (ndf*2) x 16 x 16\n+                    nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n+                    nn.BatchNorm2d(ndf * 4),\n+                    nn.LeakyReLU(0.2, inplace=True),\n+                    # state size. (ndf*4) x 8 x 8\n+                    nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n+                    nn.BatchNorm2d(ndf * 8),\n+                    nn.LeakyReLU(0.2, inplace=True),\n+                    # state size. (ndf*8) x 4 x 4\n+                    nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n+                    nn.Sigmoid()\n+                )\n+\n+            def forward(self, input):\n+                return self.main(input).view(-1, 1).squeeze(1)\n+\n+        bs, nz, ngf, nc, ndf = 5, 6, 9, 3, 10\n+        self.checkTrace(DCGANGenerator(nz, ngf, nc), (torch.rand(bs, nz, 1, 1),))\n+        example_input = DCGANGenerator(nz, ngf, nc)(torch.rand(bs, nz, 1, 1))\n+        self.checkTrace(DCGANDiscriminator(nc, ndf), (example_input,))\n+\n+    @unittest.skip('https://github.com/pytorch/pytorch/issues/8439 InstanceNormalization bug')\n+    def test_neural_style(self):\n+        class TransformerNet(torch.nn.Module):\n+            def __init__(self):\n+                super(TransformerNet, self).__init__()\n+                # Initial convolution layers\n+                self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n+                self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n+                self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n+                self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n+                self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n+                self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n+                # Residual layers\n+                self.res1 = ResidualBlock(128)\n+                self.res2 = ResidualBlock(128)\n+                self.res3 = ResidualBlock(128)\n+                self.res4 = ResidualBlock(128)\n+                self.res5 = ResidualBlock(128)\n+                # Upsampling Layers\n+                self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n+                self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n+                self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n+                self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n+                self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n+                # Non-linearities\n+                self.relu = torch.nn.ReLU()\n+\n+            def forward(self, X):\n+                y = self.relu(self.in1(self.conv1(X)))\n+                y = self.relu(self.in2(self.conv2(y)))\n+                y = self.relu(self.in3(self.conv3(y)))\n+                y = self.res1(y)\n+                y = self.res2(y)\n+                y = self.res3(y)\n+                y = self.res4(y)\n+                y = self.res5(y)\n+                y = self.relu(self.in4(self.deconv1(y)))\n+                y = self.relu(self.in5(self.deconv2(y)))\n+                y = self.deconv3(y)\n+                return y\n+\n+        class ConvLayer(torch.nn.Module):\n+            def __init__(self, in_channels, out_channels, kernel_size, stride):\n+                super(ConvLayer, self).__init__()\n+                reflection_padding = kernel_size // 2\n+                self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n+                self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n+\n+            def forward(self, x):\n+                out = self.reflection_pad(x)\n+                out = self.conv2d(out)\n+                return out\n+\n+        class ResidualBlock(torch.nn.Module):\n+            \"\"\"ResidualBlock\n+            introduced in: https://arxiv.org/abs/1512.03385\n+            recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n+            \"\"\"\n+\n+            def __init__(self, channels):\n+                super(ResidualBlock, self).__init__()\n+                self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n+                self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n+                self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n+                self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n+                self.relu = torch.nn.ReLU()\n+\n+            def forward(self, x):\n+                residual = x\n+                out = self.relu(self.in1(self.conv1(x)))\n+                out = self.in2(self.conv2(out))\n+                out = out + residual\n+                return out\n+\n+        class UpsampleConvLayer(torch.nn.Module):\n+            \"\"\"UpsampleConvLayer\n+            Upsamples the input and then does a convolution. This method gives better results\n+            compared to ConvTranspose2d.\n+            ref: http://distill.pub/2016/deconv-checkerboard/\n+            \"\"\"\n+\n+            def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n+                super(UpsampleConvLayer, self).__init__()\n+                self.upsample = upsample\n+                if upsample:\n+                    self.upsample_layer = torch.nn.Upsample(mode='nearest', scale_factor=upsample)\n+                reflection_padding = kernel_size // 2\n+                self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n+                self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n+\n+            def forward(self, x):\n+                x_in = x\n+                if self.upsample:\n+                    x_in = self.upsample_layer(x_in)\n+                out = self.reflection_pad(x_in)\n+                out = self.conv2d(out)\n+                return out\n+\n+        self.checkTrace(TransformerNet(), (torch.rand(5, 3, 224, 224),))\n+\n+    def test_mnist(self):\n+        class Net(nn.Module):\n+            def __init__(self):\n+                super(Net, self).__init__()\n+                self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n+                self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n+                self.conv2_drop = nn.Dropout2d()\n+                self.fc1 = nn.Linear(320, 50)\n+                self.fc2 = nn.Linear(50, 10)\n+\n+            def forward(self, x):\n+                x = F.relu(F.max_pool2d(self.conv1(x), 2))\n+                x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n+                x = x.view(-1, 320)\n+                x = F.relu(self.fc1(x))\n+                x = F.dropout(x, training=self.training)\n+                x = self.fc2(x)\n+                return F.log_softmax(x, dim=1)\n+\n+        # FIXME: eval() is present because it works around the issue described\n+        # in https://github.com/pytorch/pytorch/issues/8448\n+        self.checkTrace(Net().eval(), (torch.rand(5, 1, 28, 28),))\n+\n+    def test_reinforcement_learning(self):\n+        class Policy(nn.Module):\n+            def __init__(self):\n+                super(Policy, self).__init__()\n+                self.affine1 = nn.Linear(4, 128)\n+                self.affine2 = nn.Linear(128, 2)\n+\n+                self.saved_log_probs = []\n+                self.rewards = []", "path": "test/test_jit.py", "position": null, "original_position": 364, "commit_id": "5c5cca36d71dfe035f7d339541ad9994e757d6e5", "original_commit_id": "43719789fc2d69baf805e2cac16dffd0b46fab2c", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Those are unnecessary", "created_at": "2018-06-14T08:56:42Z", "updated_at": "2018-11-23T15:45:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/8451#discussion_r195347180", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8451", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195347180"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8451#discussion_r195347180"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8451"}}, "body_html": "<p>Those are unnecessary</p>", "body_text": "Those are unnecessary"}