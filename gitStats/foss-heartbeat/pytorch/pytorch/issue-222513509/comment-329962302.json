{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/329962302", "html_url": "https://github.com/pytorch/pytorch/pull/1283#issuecomment-329962302", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1283", "id": 329962302, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTk2MjMwMg==", "user": {"login": "mys007", "id": 5921083, "node_id": "MDQ6VXNlcjU5MjEwODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5921083?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mys007", "html_url": "https://github.com/mys007", "followers_url": "https://api.github.com/users/mys007/followers", "following_url": "https://api.github.com/users/mys007/following{/other_user}", "gists_url": "https://api.github.com/users/mys007/gists{/gist_id}", "starred_url": "https://api.github.com/users/mys007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mys007/subscriptions", "organizations_url": "https://api.github.com/users/mys007/orgs", "repos_url": "https://api.github.com/users/mys007/repos", "events_url": "https://api.github.com/users/mys007/events{/privacy}", "received_events_url": "https://api.github.com/users/mys007/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-16T11:15:19Z", "updated_at": "2017-09-16T11:15:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As <code>training=False</code> in fixed in calling <code>F.batch_norm()</code>, is it really necessary to:</p>\n<ol>\n<li>keep running stats at all? One could throw out their keeping as parameters (they could be allocated in `forward() each time anew) as well as repeating them and averaging them back.</li>\n<li>offer <code>momentum</code> in constructor?</li>\n</ol>", "body_text": "As training=False in fixed in calling F.batch_norm(), is it really necessary to:\n\nkeep running stats at all? One could throw out their keeping as parameters (they could be allocated in `forward() each time anew) as well as repeating them and averaging them back.\noffer momentum in constructor?", "body": "As `training=False` in fixed in calling `F.batch_norm()`, is it really necessary to:\r\n1) keep running stats at all? One could throw out their keeping as parameters (they could be allocated in `forward() each time anew) as well as repeating them and averaging them back.\r\n2) offer `momentum` in constructor?"}