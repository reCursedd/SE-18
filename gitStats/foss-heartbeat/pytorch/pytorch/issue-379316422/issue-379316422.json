{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13786", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13786/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13786/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13786/events", "html_url": "https://github.com/pytorch/pytorch/issues/13786", "id": 379316422, "node_id": "MDU6SXNzdWUzNzkzMTY0MjI=", "number": 13786, "title": "PyTorch streams are not cuda-memcheck clean", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-09T20:48:28Z", "updated_at": "2018-11-09T20:48:28Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>PyTorch streams are not cuda-memcheck clean</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<p>Create the following PyTorch script, which initializes our stream pool:</p>\n<pre><code>import torch\ntorch.cuda.Stream()\n</code></pre>\n<p>Run it with <code>cuda-memcheck</code>. You'll get 64 warnings. Here's a few:</p>\n<pre><code>========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \n=========     Saved host backtrace up to driver entry point at error\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\n=========     Host Frame:python [0x1c7773]\n=========\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \n=========     Saved host backtrace up to driver entry point at error\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\n=========     Host Frame:python [0x1c7773]\n=========\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \n=========     Saved host backtrace up to driver entry point at error\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\n=========     Host Frame:python [0x1c7773]\n=========\n========= ERROR SUMMARY: 64 errors\n</code></pre>\n<h2>Expected behavior</h2>\n<p>No errors</p>\n<h2>Environment</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 1.0.0a0+583731d\nIs debug build: No\nCUDA used to build PyTorch: 9.2.88\n\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\nCMake version: version 3.11.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.88\nGPU models and configuration: \nGPU 0: Tesla M40\nGPU 1: Tesla M40\nGPU 2: Tesla M40\nGPU 3: Tesla M40\nGPU 4: Tesla M40\nGPU 5: Tesla M40\nGPU 6: Tesla M40\nGPU 7: Tesla M40\n\nNvidia driver version: 396.26\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.2\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.4\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] onnx (1.2.1, /data/users/ezyang/pytorch-tmp/third_party/onnx)\n[pip] torch (1.0.0a0+583731d, /data/users/ezyang/pytorch-tmp)\n[pip] torch-complex (0.0.1, /data/users/ezyang/complex)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda91              2.3.0                         1    pytorch\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n</code></pre>\n<h2>Additional context</h2>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a></p>", "body_text": "\ud83d\udc1b Bug\nPyTorch streams are not cuda-memcheck clean\nTo Reproduce\nSteps to reproduce the behavior:\nCreate the following PyTorch script, which initializes our stream pool:\nimport torch\ntorch.cuda.Stream()\n\nRun it with cuda-memcheck. You'll get 64 warnings. Here's a few:\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \n=========     Saved host backtrace up to driver entry point at error\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\n=========     Host Frame:python [0x1c7773]\n=========\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \n=========     Saved host backtrace up to driver entry point at error\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\n=========     Host Frame:python [0x1c7773]\n=========\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \n=========     Saved host backtrace up to driver entry point at error\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\n=========     Host Frame:python [0x1c7773]\n=========\n========= ERROR SUMMARY: 64 errors\n\nExpected behavior\nNo errors\nEnvironment\nCollecting environment information...\nPyTorch version: 1.0.0a0+583731d\nIs debug build: No\nCUDA used to build PyTorch: 9.2.88\n\nOS: CentOS Linux 7 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\nCMake version: version 3.11.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.88\nGPU models and configuration: \nGPU 0: Tesla M40\nGPU 1: Tesla M40\nGPU 2: Tesla M40\nGPU 3: Tesla M40\nGPU 4: Tesla M40\nGPU 5: Tesla M40\nGPU 6: Tesla M40\nGPU 7: Tesla M40\n\nNvidia driver version: 396.26\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.2\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.4\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] onnx (1.2.1, /data/users/ezyang/pytorch-tmp/third_party/onnx)\n[pip] torch (1.0.0a0+583731d, /data/users/ezyang/pytorch-tmp)\n[pip] torch-complex (0.0.1, /data/users/ezyang/complex)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda91              2.3.0                         1    pytorch\n[conda] torchvision               0.2.1                     <pip>\n\nAdditional context\nCC @mruberry", "body": "## \ud83d\udc1b Bug\r\n\r\nPyTorch streams are not cuda-memcheck clean\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nCreate the following PyTorch script, which initializes our stream pool:\r\n\r\n```\r\nimport torch\r\ntorch.cuda.Stream()\r\n```\r\n\r\nRun it with `cuda-memcheck`. You'll get 64 warnings. Here's a few:\r\n\r\n```\r\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \r\n=========     Saved host backtrace up to driver entry point at error\r\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\r\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\r\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\r\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\r\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\r\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\r\n=========     Host Frame:python [0x1c7773]\r\n=========\r\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \r\n=========     Saved host backtrace up to driver entry point at error\r\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\r\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\r\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\r\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\r\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\r\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\r\n=========     Host Frame:python [0x1c7773]\r\n=========\r\n========= Program hit cudaErrorCudartUnloading (error 29) due to \"driver shutting down\" on CUDA API call to cudaStreamDestroy. \r\n=========     Saved host backtrace up to driver entry point at error\r\n=========     Host Frame:/usr/lib64/nvidia/libcuda.so.1 [0x3478e3]\r\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so [0x2ae05de]\r\n=========     Host Frame:/data/users/ezyang/pytorch-tmp/torch/lib/libcaffe2_gpu.so (_ZNSt6vectorISt5arrayI19CUDAStreamInternalsLm32EESaIS2_EED1Ev + 0x57) [0x2662317]\r\n=========     Host Frame:/lib64/libc.so.6 [0x39bd9]\r\n=========     Host Frame:/lib64/libc.so.6 [0x39c27]\r\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfc) [0x2244c]\r\n=========     Host Frame:python [0x1c7773]\r\n=========\r\n========= ERROR SUMMARY: 64 errors\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo errors\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+583731d\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: \r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\nGPU 2: Tesla M40\r\nGPU 3: Tesla M40\r\nGPU 4: Tesla M40\r\nGPU 5: Tesla M40\r\nGPU 6: Tesla M40\r\nGPU 7: Tesla M40\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.2\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] onnx (1.2.1, /data/users/ezyang/pytorch-tmp/third_party/onnx)\r\n[pip] torch (1.0.0a0+583731d, /data/users/ezyang/pytorch-tmp)\r\n[pip] torch-complex (0.0.1, /data/users/ezyang/complex)\r\n[pip] torchvision (0.2.1)\r\n[conda] magma-cuda91              2.3.0                         1    pytorch\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\n## Additional context\r\n\r\nCC @mruberry "}