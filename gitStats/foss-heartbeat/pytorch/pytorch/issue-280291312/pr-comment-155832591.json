{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155832591", "pull_request_review_id": 82221081, "id": 155832591, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NTgzMjU5MQ==", "diff_hunk": "@@ -0,0 +1,149 @@\n+#include \"tensor_new.h\"\n+\n+#include <ATen/ATen.h>\n+#include <Python.h>\n+\n+#include \"torch/csrc/Exceptions.h\"\n+#include \"torch/csrc/utils/auto_gil.h\"\n+#include \"torch/csrc/utils/auto_gpu.h\"\n+#include \"torch/csrc/utils/python_arg_parser.h\"\n+#include \"torch/csrc/utils/python_numbers.h\"\n+#include \"torch/csrc/utils/python_scalars.h\"\n+#include \"torch/csrc/utils/python_strings.h\"\n+#include \"torch/csrc/utils/tensor_numpy.h\"\n+\n+static const int MAX_DIMS = 128;\n+\n+using namespace at;\n+\n+namespace torch { namespace utils {\n+\n+static Tensor new_with_sizes(const Type& type, int device, IntList sizes) {\n+  AutoNoGIL no_gil;\n+  AutoGPU auto_gpu(device);\n+  return type.tensor(sizes);\n+}\n+\n+static Tensor new_with_storage(const Type& type, Storage& storage) {\n+  auto tensor = type.tensor();\n+  tensor.set_(storage);\n+  return tensor;\n+}\n+\n+static Tensor new_with_tensor(const Type& type, Tensor other) {\n+  if (other.type() != type) {\n+    throw TypeError(\"expected %s (got %s)\", type.toString(), other.type().toString());\n+  }\n+  return other.slice();\n+}\n+\n+static std::vector<int64_t> compute_sizes(PyObject* seq) {\n+  std::vector<int64_t> sizes;\n+  THPObjectPtr handle;\n+  do {\n+    auto length = PySequence_Length(seq);\n+    if (length < 0) throw python_error();\n+    sizes.push_back(length);\n+    if (sizes.size() > MAX_DIMS) {\n+      throw ValueError(\"too many dimensions '%s'\", Py_TYPE(seq)->tp_name);\n+    }\n+    if (length == 0) break;\n+    handle = THPObjectPtr(PySequence_GetItem(seq, 0));\n+    seq = handle.get();\n+  } while (PySequence_Check(seq));\n+\n+  return sizes;\n+}\n+\n+static void recursive_store(char* data, IntList sizes, IntList strides, int64_t dim,\n+                            ScalarType scalarType, int elementSize, PyObject* obj) {\n+  int64_t ndim = sizes.size();\n+  if (dim == ndim) {\n+    torch::utils::store_scalar(data, scalarType, obj);\n+    return;\n+  }\n+\n+  auto n = sizes[dim];\n+  auto seq = THPObjectPtr(PySequence_Fast(obj, \"not a sequence\"));\n+  if (!seq) throw python_error();\n+  auto seq_size = PySequence_Fast_GET_SIZE(seq.get());\n+  if (seq_size != n) {\n+    throw ValueError(\"expected sequence of length %lld at dim %lld (got %lld)\",\n+      (long long)n, (long long)dim, (long long)seq_size);\n+  }\n+\n+  PyObject** items = PySequence_Fast_ITEMS(seq.get());\n+  for (int64_t i = 0; i < n; i++) {\n+    recursive_store(data, sizes, strides, dim + 1, scalarType, elementSize, items[i]);\n+    data += strides[dim] * elementSize;\n+  }\n+}\n+\n+static Tensor new_from_sequence(ScalarType scalarType, PyObject* data) {\n+  if (!PySequence_Check(data)) {\n+    throw TypeError(\"new(): data must be a sequence (got %s)\", Py_TYPE(data)->tp_name);\n+  }\n+  if (THPUtils_checkString(data)) {\n+    throw TypeError(\"new(): invalid data type '%s'\", Py_TYPE(data)->tp_name);\n+  }\n+  if (PyArray_Check(data)) {\n+    return autograd::make_variable(tensor_from_numpy(data), false);\n+  }\n+\n+  auto sizes = compute_sizes(data);\n+  auto tensor = autograd::make_variable(CPU(scalarType).tensor(sizes), false);\n+  recursive_store(\n+      (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,\n+      scalarType, tensor.type().elementSizeInBytes(), data);\n+  return tensor;\n+}\n+\n+Tensor tensor_new(const Type& type, PyObject* args, PyObject* kwargs) {\n+  static PythonArgParser parser({\n+    \"new(*, int64_t device=-1)\",\n+    \"new(torch.Size size, *, int64_t device=-1)\",\n+    \"new(Storage storage)\",\n+    \"new(*, int64_t cdata)|hidden\",\n+    \"new(Tensor other)\",\n+    \"new(PyObject* data, *, int64_t device=-1)\",\n+  });\n+  static PythonArgParser parser2({\n+    \"new(IntList size, *, int64_t device=-1)\",\n+  });\n+\n+  PyObject* parsed_args[2];\n+\n+  // Handle new(int...). This signature would be ambiguous if combined with", "path": "torch/csrc/utils/tensor_new.cpp", "position": null, "original_position": 116, "commit_id": "bd9a8d484249831282b988d31b32114219b27fbc", "original_commit_id": "8819a410fecd8a2103422ec0a018b04af7986128", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "We need to match the semantics of `Tensor.new` to avoid breaking backwards compatibility. We can deprecate some of the overloads, but they all need to be implemented.", "created_at": "2017-12-08T17:39:32Z", "updated_at": "2018-11-23T15:37:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/4080#discussion_r155832591", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4080", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/155832591"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4080#discussion_r155832591"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4080"}}, "body_html": "<p>We need to match the semantics of <code>Tensor.new</code> to avoid breaking backwards compatibility. We can deprecate some of the overloads, but they all need to be implemented.</p>", "body_text": "We need to match the semantics of Tensor.new to avoid breaking backwards compatibility. We can deprecate some of the overloads, but they all need to be implemented.", "in_reply_to_id": 155686618}