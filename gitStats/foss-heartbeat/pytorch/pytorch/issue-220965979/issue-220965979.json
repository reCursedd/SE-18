{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1229", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1229/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1229/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1229/events", "html_url": "https://github.com/pytorch/pytorch/issues/1229", "id": 220965979, "node_id": "MDU6SXNzdWUyMjA5NjU5Nzk=", "number": 1229, "title": "Bug in legacy padding layer", "user": {"login": "arrybn", "id": 11784685, "node_id": "MDQ6VXNlcjExNzg0Njg1", "avatar_url": "https://avatars3.githubusercontent.com/u/11784685?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arrybn", "html_url": "https://github.com/arrybn", "followers_url": "https://api.github.com/users/arrybn/followers", "following_url": "https://api.github.com/users/arrybn/following{/other_user}", "gists_url": "https://api.github.com/users/arrybn/gists{/gist_id}", "starred_url": "https://api.github.com/users/arrybn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arrybn/subscriptions", "organizations_url": "https://api.github.com/users/arrybn/orgs", "repos_url": "https://api.github.com/users/arrybn/repos", "events_url": "https://api.github.com/users/arrybn/events{/privacy}", "received_events_url": "https://api.github.com/users/arrybn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-04-11T14:14:06Z", "updated_at": "2017-09-10T17:47:35Z", "closed_at": "2017-09-10T17:47:35Z", "author_association": "NONE", "body_html": "<p>I've found a bug in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/legacy/nn/Padding.py\">Padding.py</a>.<br>\nIt doesn't handle nInputDim original Torch Padding layer's parameter . As far as I've unterstood, you've rewritten lua code from <a href=\"https://github.com/torch/nn/blob/master/Padding.lua\">here</a>, but not fully. 19th line in this file contains code for dealing with nInputDim parameter.<br>\nTo reproduce issue, you need to download ENet model from <a href=\"https://www.dropbox.com/sh/dywzk3gyb12hpe5/AAD5YkUa8XgMpHs2gCRgmCVCa\" rel=\"nofollow\">here</a>(original model from ENet authors) and make inference:<br>\n<code>import numpy as np</code><br>\n<code>import torch</code><br>\n<code>from torch.utils.serialization import load_lua</code><br>\n<code>image = np.ones((1,3,1024,2048))</code><br>\n<code>tensor = torch.FloatTensor(image)</code><br>\n<code>net_torch = load_lua(torch_model)</code><br>\n<code>out_torch = net_torch.forward(tensor)</code></p>\n<p>Now I've got en exception:<br>\nTraceback (most recent call last):</p>\n<pre><code>  File \"/hdd/PycharmProjects/untitled/test.py\", line 110, in &lt;module&gt;\n    out_torch = net_torch.forward(tensor).numpy()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Module.py\", line 33, in forward\n    return self.updateOutput(input)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\n    currentOutput = module.updateOutput(currentOutput)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\n    currentOutput = module.updateOutput(currentOutput)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/CAddTable.py\", line 20, in updateOutput\n    self.output.add_(input[i])\nRuntimeError: inconsistent tensor size at /b/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:827\n</code></pre>\n<p>Padding is added to wrong axis, what's why CAddTable can't sum blobs with different shapes and throws exception.<br>\nIf I edit the code of Padding.py by adding one to self.dim variables (lines 21 and 23 in Padding.py), all works correctly.</p>", "body_text": "I've found a bug in Padding.py.\nIt doesn't handle nInputDim original Torch Padding layer's parameter . As far as I've unterstood, you've rewritten lua code from here, but not fully. 19th line in this file contains code for dealing with nInputDim parameter.\nTo reproduce issue, you need to download ENet model from here(original model from ENet authors) and make inference:\nimport numpy as np\nimport torch\nfrom torch.utils.serialization import load_lua\nimage = np.ones((1,3,1024,2048))\ntensor = torch.FloatTensor(image)\nnet_torch = load_lua(torch_model)\nout_torch = net_torch.forward(tensor)\nNow I've got en exception:\nTraceback (most recent call last):\n  File \"/hdd/PycharmProjects/untitled/test.py\", line 110, in <module>\n    out_torch = net_torch.forward(tensor).numpy()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Module.py\", line 33, in forward\n    return self.updateOutput(input)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\n    currentOutput = module.updateOutput(currentOutput)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\n    currentOutput = module.updateOutput(currentOutput)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/CAddTable.py\", line 20, in updateOutput\n    self.output.add_(input[i])\nRuntimeError: inconsistent tensor size at /b/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:827\n\nPadding is added to wrong axis, what's why CAddTable can't sum blobs with different shapes and throws exception.\nIf I edit the code of Padding.py by adding one to self.dim variables (lines 21 and 23 in Padding.py), all works correctly.", "body": "I've found a bug in [Padding.py](https://github.com/pytorch/pytorch/blob/master/torch/legacy/nn/Padding.py).\r\nIt doesn't handle nInputDim original Torch Padding layer's parameter . As far as I've unterstood, you've rewritten lua code from [here](https://github.com/torch/nn/blob/master/Padding.lua), but not fully. 19th line in this file contains code for dealing with nInputDim parameter.\r\nTo reproduce issue, you need to download ENet model from [here](https://www.dropbox.com/sh/dywzk3gyb12hpe5/AAD5YkUa8XgMpHs2gCRgmCVCa)(original model from ENet authors) and make inference:\r\n`import numpy as np`\r\n`import torch`\r\n`from torch.utils.serialization import load_lua`\r\n`image = np.ones((1,3,1024,2048))`\r\n`tensor = torch.FloatTensor(image)`\r\n`net_torch = load_lua(torch_model)`\r\n`out_torch = net_torch.forward(tensor)`\r\n\r\nNow I've got en exception: \r\nTraceback (most recent call last):\r\n```\r\n  File \"/hdd/PycharmProjects/untitled/test.py\", line 110, in <module>\r\n    out_torch = net_torch.forward(tensor).numpy()\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Module.py\", line 33, in forward\r\n    return self.updateOutput(input)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\r\n    currentOutput = module.updateOutput(currentOutput)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\r\n    currentOutput = module.updateOutput(currentOutput)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/CAddTable.py\", line 20, in updateOutput\r\n    self.output.add_(input[i])\r\nRuntimeError: inconsistent tensor size at /b/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:827\r\n```\r\n\r\nPadding is added to wrong axis, what's why CAddTable can't sum blobs with different shapes and throws exception.\r\nIf I edit the code of Padding.py by adding one to self.dim variables (lines 21 and 23 in Padding.py), all works correctly."}