{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/339629612", "html_url": "https://github.com/pytorch/pytorch/issues/3301#issuecomment-339629612", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3301", "id": 339629612, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTYyOTYxMg==", "user": {"login": "Nintorac", "id": 24326299, "node_id": "MDQ6VXNlcjI0MzI2Mjk5", "avatar_url": "https://avatars3.githubusercontent.com/u/24326299?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nintorac", "html_url": "https://github.com/Nintorac", "followers_url": "https://api.github.com/users/Nintorac/followers", "following_url": "https://api.github.com/users/Nintorac/following{/other_user}", "gists_url": "https://api.github.com/users/Nintorac/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nintorac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nintorac/subscriptions", "organizations_url": "https://api.github.com/users/Nintorac/orgs", "repos_url": "https://api.github.com/users/Nintorac/repos", "events_url": "https://api.github.com/users/Nintorac/events{/privacy}", "received_events_url": "https://api.github.com/users/Nintorac/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-26T10:58:44Z", "updated_at": "2017-10-26T10:58:44Z", "author_association": "CONTRIBUTOR", "body_html": "<pre><code>    def __init__(self, module, device_ids=None, output_device=None, dim=0):\n        super(DataParallel, self).__init__()\n\n        self.module = module\n\n        if not torch.cuda.is_available():\n            return\n\n        if device_ids is None:\n            device_ids = list(range(torch.cuda.device_count()))\n        if output_device is None:\n            output_device = device_ids[0]\n        self.dim = dim\n        self.device_ids = device_ids\n        self.output_device = output_device\n        if len(self.device_ids) == 1:\n            self.module.cuda(device_ids[0])\n\n    def forward(self, *inputs, **kwargs):\n\n        if not torch.cuda.is_available():\n            return self.module(*inputs, **kwargs)\n\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a><br>\ncont. from slack, made the change you recommended and simplified the init.. that final conditional you mentioned is part of the original code, should I leave it?</p>", "body_text": "def __init__(self, module, device_ids=None, output_device=None, dim=0):\n        super(DataParallel, self).__init__()\n\n        self.module = module\n\n        if not torch.cuda.is_available():\n            return\n\n        if device_ids is None:\n            device_ids = list(range(torch.cuda.device_count()))\n        if output_device is None:\n            output_device = device_ids[0]\n        self.dim = dim\n        self.device_ids = device_ids\n        self.output_device = output_device\n        if len(self.device_ids) == 1:\n            self.module.cuda(device_ids[0])\n\n    def forward(self, *inputs, **kwargs):\n\n        if not torch.cuda.is_available():\n            return self.module(*inputs, **kwargs)\n\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n\n@apaszke\ncont. from slack, made the change you recommended and simplified the init.. that final conditional you mentioned is part of the original code, should I leave it?", "body": "```\r\n    def __init__(self, module, device_ids=None, output_device=None, dim=0):\r\n        super(DataParallel, self).__init__()\r\n\r\n        self.module = module\r\n\r\n        if not torch.cuda.is_available():\r\n            return\r\n\r\n        if device_ids is None:\r\n            device_ids = list(range(torch.cuda.device_count()))\r\n        if output_device is None:\r\n            output_device = device_ids[0]\r\n        self.dim = dim\r\n        self.device_ids = device_ids\r\n        self.output_device = output_device\r\n        if len(self.device_ids) == 1:\r\n            self.module.cuda(device_ids[0])\r\n\r\n    def forward(self, *inputs, **kwargs):\r\n\r\n        if not torch.cuda.is_available():\r\n            return self.module(*inputs, **kwargs)\r\n\r\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n        if len(self.device_ids) == 1:\r\n            return self.module(*inputs[0], **kwargs[0])\r\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n        return self.gather(outputs, self.output_device)\r\n```\r\n@apaszke \r\ncont. from slack, made the change you recommended and simplified the init.. that final conditional you mentioned is part of the original code, should I leave it?"}