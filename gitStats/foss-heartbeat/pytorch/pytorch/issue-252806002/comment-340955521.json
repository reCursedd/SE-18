{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340955521", "html_url": "https://github.com/pytorch/pytorch/issues/2534#issuecomment-340955521", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2534", "id": 340955521, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDk1NTUyMQ==", "user": {"login": "PiotrDabkowski", "id": 4052440, "node_id": "MDQ6VXNlcjQwNTI0NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/4052440?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PiotrDabkowski", "html_url": "https://github.com/PiotrDabkowski", "followers_url": "https://api.github.com/users/PiotrDabkowski/followers", "following_url": "https://api.github.com/users/PiotrDabkowski/following{/other_user}", "gists_url": "https://api.github.com/users/PiotrDabkowski/gists{/gist_id}", "starred_url": "https://api.github.com/users/PiotrDabkowski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PiotrDabkowski/subscriptions", "organizations_url": "https://api.github.com/users/PiotrDabkowski/orgs", "repos_url": "https://api.github.com/users/PiotrDabkowski/repos", "events_url": "https://api.github.com/users/PiotrDabkowski/events{/privacy}", "received_events_url": "https://api.github.com/users/PiotrDabkowski/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T01:50:26Z", "updated_at": "2017-11-01T01:50:26Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>  I have the same problem. I thought that reducing the contribution of the gradient penalty may solve the problem, but no! Even setting the contribution to exactly 0 causes NaN:</p>\n<pre><code>gr = torch.autograd.grad(mid_scores, mids, create_graph=True)[0] + 0.0001\ngradient_penalty = ((gr.norm(dim=0) - 1.)**2).mean()\nloss = fake_scores - real_scores + self.drift_penalty_factor*(real_scores**2)  + 0.*gradient_penalty\n</code></pre>\n<p>Note that the gradient penalty is multiplied by 0 so should not have any impact on the optimisation. However, even when multiplied by 0 it causes some kind of huge numerical instability and the only solution is to not include it in the loss term at all.</p>", "body_text": "@soumith  I have the same problem. I thought that reducing the contribution of the gradient penalty may solve the problem, but no! Even setting the contribution to exactly 0 causes NaN:\ngr = torch.autograd.grad(mid_scores, mids, create_graph=True)[0] + 0.0001\ngradient_penalty = ((gr.norm(dim=0) - 1.)**2).mean()\nloss = fake_scores - real_scores + self.drift_penalty_factor*(real_scores**2)  + 0.*gradient_penalty\n\nNote that the gradient penalty is multiplied by 0 so should not have any impact on the optimisation. However, even when multiplied by 0 it causes some kind of huge numerical instability and the only solution is to not include it in the loss term at all.", "body": "@soumith  I have the same problem. I thought that reducing the contribution of the gradient penalty may solve the problem, but no! Even setting the contribution to exactly 0 causes NaN:\r\n\r\n```\r\ngr = torch.autograd.grad(mid_scores, mids, create_graph=True)[0] + 0.0001\r\ngradient_penalty = ((gr.norm(dim=0) - 1.)**2).mean()\r\nloss = fake_scores - real_scores + self.drift_penalty_factor*(real_scores**2)  + 0.*gradient_penalty\r\n```\r\n\r\nNote that the gradient penalty is multiplied by 0 so should not have any impact on the optimisation. However, even when multiplied by 0 it causes some kind of huge numerical instability and the only solution is to not include it in the loss term at all."}