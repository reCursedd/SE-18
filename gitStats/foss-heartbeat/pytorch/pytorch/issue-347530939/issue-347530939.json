{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10220", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10220/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10220/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10220/events", "html_url": "https://github.com/pytorch/pytorch/pull/10220", "id": 347530939, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA2MTM0NzE5", "number": 10220, "title": "Correctly share CUDA Parameters.", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-03T20:45:47Z", "updated_at": "2018-11-23T15:49:04Z", "closed_at": "2018-08-10T20:56:12Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10220", "html_url": "https://github.com/pytorch/pytorch/pull/10220", "diff_url": "https://github.com/pytorch/pytorch/pull/10220.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10220.patch"}, "body_html": "<pre><code>    Correctly share CUDA Parameters, requires_grad and hooks.\n    \n    Previously, the following was true:\n    \n    - If you put a Parameter for a CUDA tensor\n      in multiprocessing queue (or otherwise tried to transfer it),\n      this failed, saying that we cannot pickle CUDA storage.\n      This is issue #9996.\n    \n    - If you put a leaf Tensor that requires_grad=True through the\n      multiprocessing queue, it would come out the other end as\n      requires_grad=False (It should have come out the other end\n      as requires_grad=True).  Similarly, backwards hooks were\n      lost.\n    \n    - If you put a non-leaf Tensor that requires_grad=True through\n      the multiprocessing queue, it would come out the other end\n      as requires_grad=False.\n    \n    The root cause for the first issue was that implementation of\n    reductions for Parameter used the superclass implementation\n    (tensor) in __reduce_ex__, but this always picks up the\n    non-ForkingPickler reduction, which doesn't work with CUDA tensors.\n    So, we registered a new ForkingPickler specifically for Parameter,\n    and adjusted the code to correctly rewrap a Tensor in a Parameter\n    if it was originally a parameter.\n        \n    While working on this, we realized that requires_grad and backwards\n    hooks would not be preserved in the ForkingPickler reduction\n    implementation.  We fixed the reducer to save these parameters.\n    However, Adam Paszke pointed out that we shouldn't allow sending\n    requires_grad=True, non-leaf Tensors over a multiprocessing\n    queue, since we don't actually support autograd over process\n    boundar.  We now throw an error in this case; this may cause\n    previously working code to fail, but this is easy enough to fix;\n    just detach() the tensor before sending it.  The error message says\n    so.\n    \n    Fixes #9996.\n</code></pre>", "body_text": "Correctly share CUDA Parameters, requires_grad and hooks.\n    \n    Previously, the following was true:\n    \n    - If you put a Parameter for a CUDA tensor\n      in multiprocessing queue (or otherwise tried to transfer it),\n      this failed, saying that we cannot pickle CUDA storage.\n      This is issue #9996.\n    \n    - If you put a leaf Tensor that requires_grad=True through the\n      multiprocessing queue, it would come out the other end as\n      requires_grad=False (It should have come out the other end\n      as requires_grad=True).  Similarly, backwards hooks were\n      lost.\n    \n    - If you put a non-leaf Tensor that requires_grad=True through\n      the multiprocessing queue, it would come out the other end\n      as requires_grad=False.\n    \n    The root cause for the first issue was that implementation of\n    reductions for Parameter used the superclass implementation\n    (tensor) in __reduce_ex__, but this always picks up the\n    non-ForkingPickler reduction, which doesn't work with CUDA tensors.\n    So, we registered a new ForkingPickler specifically for Parameter,\n    and adjusted the code to correctly rewrap a Tensor in a Parameter\n    if it was originally a parameter.\n        \n    While working on this, we realized that requires_grad and backwards\n    hooks would not be preserved in the ForkingPickler reduction\n    implementation.  We fixed the reducer to save these parameters.\n    However, Adam Paszke pointed out that we shouldn't allow sending\n    requires_grad=True, non-leaf Tensors over a multiprocessing\n    queue, since we don't actually support autograd over process\n    boundar.  We now throw an error in this case; this may cause\n    previously working code to fail, but this is easy enough to fix;\n    just detach() the tensor before sending it.  The error message says\n    so.\n    \n    Fixes #9996.", "body": "```\r\n    Correctly share CUDA Parameters, requires_grad and hooks.\r\n    \r\n    Previously, the following was true:\r\n    \r\n    - If you put a Parameter for a CUDA tensor\r\n      in multiprocessing queue (or otherwise tried to transfer it),\r\n      this failed, saying that we cannot pickle CUDA storage.\r\n      This is issue #9996.\r\n    \r\n    - If you put a leaf Tensor that requires_grad=True through the\r\n      multiprocessing queue, it would come out the other end as\r\n      requires_grad=False (It should have come out the other end\r\n      as requires_grad=True).  Similarly, backwards hooks were\r\n      lost.\r\n    \r\n    - If you put a non-leaf Tensor that requires_grad=True through\r\n      the multiprocessing queue, it would come out the other end\r\n      as requires_grad=False.\r\n    \r\n    The root cause for the first issue was that implementation of\r\n    reductions for Parameter used the superclass implementation\r\n    (tensor) in __reduce_ex__, but this always picks up the\r\n    non-ForkingPickler reduction, which doesn't work with CUDA tensors.\r\n    So, we registered a new ForkingPickler specifically for Parameter,\r\n    and adjusted the code to correctly rewrap a Tensor in a Parameter\r\n    if it was originally a parameter.\r\n        \r\n    While working on this, we realized that requires_grad and backwards\r\n    hooks would not be preserved in the ForkingPickler reduction\r\n    implementation.  We fixed the reducer to save these parameters.\r\n    However, Adam Paszke pointed out that we shouldn't allow sending\r\n    requires_grad=True, non-leaf Tensors over a multiprocessing\r\n    queue, since we don't actually support autograd over process\r\n    boundar.  We now throw an error in this case; this may cause\r\n    previously working code to fail, but this is easy enough to fix;\r\n    just detach() the tensor before sending it.  The error message says\r\n    so.\r\n    \r\n    Fixes #9996.\r\n```"}