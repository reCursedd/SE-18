{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/330103591", "html_url": "https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/563", "id": 330103591, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDEwMzU5MQ==", "user": {"login": "karpathy", "id": 241138, "node_id": "MDQ6VXNlcjI0MTEzOA==", "avatar_url": "https://avatars3.githubusercontent.com/u/241138?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karpathy", "html_url": "https://github.com/karpathy", "followers_url": "https://api.github.com/users/karpathy/followers", "following_url": "https://api.github.com/users/karpathy/following{/other_user}", "gists_url": "https://api.github.com/users/karpathy/gists{/gist_id}", "starred_url": "https://api.github.com/users/karpathy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karpathy/subscriptions", "organizations_url": "https://api.github.com/users/karpathy/orgs", "repos_url": "https://api.github.com/users/karpathy/repos", "events_url": "https://api.github.com/users/karpathy/events{/privacy}", "received_events_url": "https://api.github.com/users/karpathy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-18T00:03:55Z", "updated_at": "2017-09-26T18:49:18Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=18602382\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/greydanus\">@greydanus</a>  since it's all PyTorch in the implementation it's fine to do this manually without any performance hits. In fact, my performance even increased quite a bit because the CrossEntropyLoss does additional heavy lifting for ignoring of certain label values - a feature that most people won't use, but will pay in performance for. This almost seems like a poor choice for the library.</p>\n<p>Anyway, assume <code>y_hat</code> is <code>(N,C)</code> class of raw scores (from a fully connected layer) and <code>y</code> are the true indices as <code>LongTensor</code>. We can do:</p>\n<div class=\"highlight highlight-source-python\"><pre>logp <span class=\"pl-k\">=</span> F.log_softmax(y_hat) <span class=\"pl-c\"><span class=\"pl-c\">#</span> get (N,C) log probabilities</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> prepare an (N,C) array of 1s at the locations of ground truth</span>\nymask <span class=\"pl-k\">=</span> logp.data.new(logp.size()).zero_() <span class=\"pl-c\"><span class=\"pl-c\">#</span> (N,C) all zero</span>\nymask.scatter_(<span class=\"pl-c1\">1</span>, y.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> have to make y into shape (N,1) for scatter_ to be happy</span>\nymask <span class=\"pl-k\">=</span> Variable(ymask)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pluck</span>\nlogpy <span class=\"pl-k\">=</span> (logp <span class=\"pl-k\">*</span> ymask).sum(<span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> this hurts in my heart</span></pre></div>\n<p><code>logpy</code> becomes <code>(N,)</code> Tensor of the log probabilities of the correct classes. This works because during backpropagation the sum distributes the gradients equally to all channels, and then the gradient will get blocked everywhere where <code>ymask == 0</code> in each row, and will only flow through the elements of the correct classes. And if we wanted to weigh each example differently we would do</p>\n<div class=\"highlight highlight-source-python\"><pre>negative_log_likelihood_loss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>(logpy <span class=\"pl-k\">*</span> per_example_weights).mean()</pre></div>\n<p>(or even pass the weights in during the <code>scatter_</code> call)</p>\n<hr>\n<p><strong>EDIT:</strong><br>\nsimple gather can be very fast:</p>\n<div class=\"highlight highlight-source-python\"><pre>logp <span class=\"pl-k\">=</span> F.log_softmax(y_hat)\nlogpy <span class=\"pl-k\">=</span> torch.gather(logp, <span class=\"pl-c1\">1</span>, y).view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)</pre></div>", "body_text": "@greydanus  since it's all PyTorch in the implementation it's fine to do this manually without any performance hits. In fact, my performance even increased quite a bit because the CrossEntropyLoss does additional heavy lifting for ignoring of certain label values - a feature that most people won't use, but will pay in performance for. This almost seems like a poor choice for the library.\nAnyway, assume y_hat is (N,C) class of raw scores (from a fully connected layer) and y are the true indices as LongTensor. We can do:\nlogp = F.log_softmax(y_hat) # get (N,C) log probabilities\n# prepare an (N,C) array of 1s at the locations of ground truth\nymask = logp.data.new(logp.size()).zero_() # (N,C) all zero\nymask.scatter_(1, y.view(-1,1), 1) # have to make y into shape (N,1) for scatter_ to be happy\nymask = Variable(ymask)\n# pluck\nlogpy = (logp * ymask).sum(1) # this hurts in my heart\nlogpy becomes (N,) Tensor of the log probabilities of the correct classes. This works because during backpropagation the sum distributes the gradients equally to all channels, and then the gradient will get blocked everywhere where ymask == 0 in each row, and will only flow through the elements of the correct classes. And if we wanted to weigh each example differently we would do\nnegative_log_likelihood_loss = -(logpy * per_example_weights).mean()\n(or even pass the weights in during the scatter_ call)\n\nEDIT:\nsimple gather can be very fast:\nlogp = F.log_softmax(y_hat)\nlogpy = torch.gather(logp, 1, y).view(-1)", "body": "@greydanus  since it's all PyTorch in the implementation it's fine to do this manually without any performance hits. In fact, my performance even increased quite a bit because the CrossEntropyLoss does additional heavy lifting for ignoring of certain label values - a feature that most people won't use, but will pay in performance for. This almost seems like a poor choice for the library.\r\n\r\nAnyway, assume `y_hat` is `(N,C)` class of raw scores (from a fully connected layer) and `y` are the true indices as `LongTensor`. We can do:\r\n\r\n```python\r\nlogp = F.log_softmax(y_hat) # get (N,C) log probabilities\r\n# prepare an (N,C) array of 1s at the locations of ground truth\r\nymask = logp.data.new(logp.size()).zero_() # (N,C) all zero\r\nymask.scatter_(1, y.view(-1,1), 1) # have to make y into shape (N,1) for scatter_ to be happy\r\nymask = Variable(ymask)\r\n# pluck\r\nlogpy = (logp * ymask).sum(1) # this hurts in my heart\r\n```\r\n\r\n`logpy` becomes `(N,)` Tensor of the log probabilities of the correct classes. This works because during backpropagation the sum distributes the gradients equally to all channels, and then the gradient will get blocked everywhere where `ymask == 0` in each row, and will only flow through the elements of the correct classes. And if we wanted to weigh each example differently we would do\r\n\r\n```python\r\nnegative_log_likelihood_loss = -(logpy * per_example_weights).mean()\r\n```\r\n\r\n(or even pass the weights in during the `scatter_` call)\r\n\r\n\r\n------------------------\r\n**EDIT:**\r\nsimple gather can be very fast:\r\n\r\n```python\r\nlogp = F.log_softmax(y_hat)\r\nlogpy = torch.gather(logp, 1, y).view(-1)\r\n```\r\n"}