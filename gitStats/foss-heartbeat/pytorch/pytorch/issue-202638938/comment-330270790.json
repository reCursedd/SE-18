{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/330270790", "html_url": "https://github.com/pytorch/pytorch/issues/563#issuecomment-330270790", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/563", "id": 330270790, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDI3MDc5MA==", "user": {"login": "aosokin", "id": 2099291, "node_id": "MDQ6VXNlcjIwOTkyOTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2099291?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aosokin", "html_url": "https://github.com/aosokin", "followers_url": "https://api.github.com/users/aosokin/followers", "following_url": "https://api.github.com/users/aosokin/following{/other_user}", "gists_url": "https://api.github.com/users/aosokin/gists{/gist_id}", "starred_url": "https://api.github.com/users/aosokin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aosokin/subscriptions", "organizations_url": "https://api.github.com/users/aosokin/orgs", "repos_url": "https://api.github.com/users/aosokin/repos", "events_url": "https://api.github.com/users/aosokin/events{/privacy}", "received_events_url": "https://api.github.com/users/aosokin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-18T16:02:58Z", "updated_at": "2017-09-18T16:02:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=241138\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/karpathy\">@karpathy</a> , <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=18602382\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/greydanus\">@greydanus</a> Hi, I have a question related to masking the cross-entropy. Is there an easy way to compute log softmax when some labels are masked out?</p>\n<p>For the moment, I had to implement in directly myself without using F.log_softmax. It seems that apart from performance losses this leads to some numerical instabilities, i.e., the training procedure starts to diverge from the path with F.log_softmax after 10-20 iterations. My current code:</p>\n<pre><code>def compute_argmax_masked(scores, mask):\n    scores_for_max = scores.clone()\n    scores_for_max.masked_fill_(mask == 0, float(\"-inf\"))\n    _, max_pos = torch.max(scores_for_max, 1, keepdim=True)\n    return max_pos\n\n\ndef compute_max_masked(scores, mask):\n    max_pos = compute_argmax_masked(scores, mask)\n    return torch.gather(scores, 1, max_pos)\n\n\ndef compute_sum_masked(scores, mask):\n    # not good: breaking the backprod chain (to get rid of infs):\n    scores.data.masked_fill_(mask.data == 0, 0.0)\n    # to get zero gradient in backprop, multiply by zero:\n    scores = torch.mul(scores, mask.float())  # if one of the non-masked element is inf, this produces nans\n    return scores.sum(1, keepdim=True)\n\n\ndef compute_log_softmax_masked(scores, mask):\n    # find max values\n    max_vals = compute_max_masked(scores, mask)\n    # exponentiate\n    scores_exp = torch.exp(scores - max_vals.expand_as(scores))\n    # compute sum\n    norm_vals = compute_sum_masked(scores_exp, mask)\n    # take log\n    norm_vals = torch.log(norm_vals) + max_vals\n    # subtract sum\n    scores = scores - norm_vals.expand_as(scores)\n    return scores\n</code></pre>", "body_text": "@karpathy , @greydanus Hi, I have a question related to masking the cross-entropy. Is there an easy way to compute log softmax when some labels are masked out?\nFor the moment, I had to implement in directly myself without using F.log_softmax. It seems that apart from performance losses this leads to some numerical instabilities, i.e., the training procedure starts to diverge from the path with F.log_softmax after 10-20 iterations. My current code:\ndef compute_argmax_masked(scores, mask):\n    scores_for_max = scores.clone()\n    scores_for_max.masked_fill_(mask == 0, float(\"-inf\"))\n    _, max_pos = torch.max(scores_for_max, 1, keepdim=True)\n    return max_pos\n\n\ndef compute_max_masked(scores, mask):\n    max_pos = compute_argmax_masked(scores, mask)\n    return torch.gather(scores, 1, max_pos)\n\n\ndef compute_sum_masked(scores, mask):\n    # not good: breaking the backprod chain (to get rid of infs):\n    scores.data.masked_fill_(mask.data == 0, 0.0)\n    # to get zero gradient in backprop, multiply by zero:\n    scores = torch.mul(scores, mask.float())  # if one of the non-masked element is inf, this produces nans\n    return scores.sum(1, keepdim=True)\n\n\ndef compute_log_softmax_masked(scores, mask):\n    # find max values\n    max_vals = compute_max_masked(scores, mask)\n    # exponentiate\n    scores_exp = torch.exp(scores - max_vals.expand_as(scores))\n    # compute sum\n    norm_vals = compute_sum_masked(scores_exp, mask)\n    # take log\n    norm_vals = torch.log(norm_vals) + max_vals\n    # subtract sum\n    scores = scores - norm_vals.expand_as(scores)\n    return scores", "body": "@karpathy , @greydanus Hi, I have a question related to masking the cross-entropy. Is there an easy way to compute log softmax when some labels are masked out?\r\n\r\nFor the moment, I had to implement in directly myself without using F.log_softmax. It seems that apart from performance losses this leads to some numerical instabilities, i.e., the training procedure starts to diverge from the path with F.log_softmax after 10-20 iterations. My current code:\r\n\r\n```\r\ndef compute_argmax_masked(scores, mask):\r\n    scores_for_max = scores.clone()\r\n    scores_for_max.masked_fill_(mask == 0, float(\"-inf\"))\r\n    _, max_pos = torch.max(scores_for_max, 1, keepdim=True)\r\n    return max_pos\r\n\r\n\r\ndef compute_max_masked(scores, mask):\r\n    max_pos = compute_argmax_masked(scores, mask)\r\n    return torch.gather(scores, 1, max_pos)\r\n\r\n\r\ndef compute_sum_masked(scores, mask):\r\n    # not good: breaking the backprod chain (to get rid of infs):\r\n    scores.data.masked_fill_(mask.data == 0, 0.0)\r\n    # to get zero gradient in backprop, multiply by zero:\r\n    scores = torch.mul(scores, mask.float())  # if one of the non-masked element is inf, this produces nans\r\n    return scores.sum(1, keepdim=True)\r\n\r\n\r\ndef compute_log_softmax_masked(scores, mask):\r\n    # find max values\r\n    max_vals = compute_max_masked(scores, mask)\r\n    # exponentiate\r\n    scores_exp = torch.exp(scores - max_vals.expand_as(scores))\r\n    # compute sum\r\n    norm_vals = compute_sum_masked(scores_exp, mask)\r\n    # take log\r\n    norm_vals = torch.log(norm_vals) + max_vals\r\n    # subtract sum\r\n    scores = scores - norm_vals.expand_as(scores)\r\n    return scores\r\n```"}