{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328423634", "html_url": "https://github.com/pytorch/pytorch/issues/563#issuecomment-328423634", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/563", "id": 328423634, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODQyMzYzNA==", "user": {"login": "karpathy", "id": 241138, "node_id": "MDQ6VXNlcjI0MTEzOA==", "avatar_url": "https://avatars3.githubusercontent.com/u/241138?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karpathy", "html_url": "https://github.com/karpathy", "followers_url": "https://api.github.com/users/karpathy/followers", "following_url": "https://api.github.com/users/karpathy/following{/other_user}", "gists_url": "https://api.github.com/users/karpathy/gists{/gist_id}", "starred_url": "https://api.github.com/users/karpathy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karpathy/subscriptions", "organizations_url": "https://api.github.com/users/karpathy/orgs", "repos_url": "https://api.github.com/users/karpathy/repos", "events_url": "https://api.github.com/users/karpathy/events{/privacy}", "received_events_url": "https://api.github.com/users/karpathy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T05:55:47Z", "updated_at": "2017-09-17T19:04:33Z", "author_association": "NONE", "body_html": "<p>I ran across this today too. Really what one wants is a tensor of the raw loss before any reduction (sum/mean as controlled by <code>size_average</code>). This is currently supported by TensorFlow's <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>, but not by PyTorch as far as I can tell.</p>\n<p>(update 9/17/2017):<br>\nI tracked the implementation of CrossEntropy loss to this function: <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/auto_double_backwards.py#L222\">nllloss_double_backward</a>. I had previously assumed that this had a low-level kernel implementation, but it looks like the loss is implemented directly in PyTorch. The full (log) probability is computed, and then a weight Tensor of the same size is created, filled with weights, and it's all simply summed up. Hence, it's straight-forward to implement this functionality manually and there is no expected hit on performance.</p>", "body_text": "I ran across this today too. Really what one wants is a tensor of the raw loss before any reduction (sum/mean as controlled by size_average). This is currently supported by TensorFlow's tf.nn.sparse_softmax_cross_entropy_with_logits, but not by PyTorch as far as I can tell.\n(update 9/17/2017):\nI tracked the implementation of CrossEntropy loss to this function: nllloss_double_backward. I had previously assumed that this had a low-level kernel implementation, but it looks like the loss is implemented directly in PyTorch. The full (log) probability is computed, and then a weight Tensor of the same size is created, filled with weights, and it's all simply summed up. Hence, it's straight-forward to implement this functionality manually and there is no expected hit on performance.", "body": "I ran across this today too. Really what one wants is a tensor of the raw loss before any reduction (sum/mean as controlled by `size_average`). This is currently supported by TensorFlow's `tf.nn.sparse_softmax_cross_entropy_with_logits`, but not by PyTorch as far as I can tell.\r\n\r\n(update 9/17/2017):\r\nI tracked the implementation of CrossEntropy loss to this function: [nllloss_double_backward](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/auto_double_backwards.py#L222). I had previously assumed that this had a low-level kernel implementation, but it looks like the loss is implemented directly in PyTorch. The full (log) probability is computed, and then a weight Tensor of the same size is created, filled with weights, and it's all simply summed up. Hence, it's straight-forward to implement this functionality manually and there is no expected hit on performance."}