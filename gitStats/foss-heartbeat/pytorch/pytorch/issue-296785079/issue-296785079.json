{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5213", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5213/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5213/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5213/events", "html_url": "https://github.com/pytorch/pytorch/issues/5213", "id": 296785079, "node_id": "MDU6SXNzdWUyOTY3ODUwNzk=", "number": 5213, "title": "CUDNN_STATUS_EXECUTION_FAILED with RNN on GPU", "user": {"login": "Diego999", "id": 1092464, "node_id": "MDQ6VXNlcjEwOTI0NjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1092464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Diego999", "html_url": "https://github.com/Diego999", "followers_url": "https://api.github.com/users/Diego999/followers", "following_url": "https://api.github.com/users/Diego999/following{/other_user}", "gists_url": "https://api.github.com/users/Diego999/gists{/gist_id}", "starred_url": "https://api.github.com/users/Diego999/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Diego999/subscriptions", "organizations_url": "https://api.github.com/users/Diego999/orgs", "repos_url": "https://api.github.com/users/Diego999/repos", "events_url": "https://api.github.com/users/Diego999/events{/privacy}", "received_events_url": "https://api.github.com/users/Diego999/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 806617721, "node_id": "MDU6TGFiZWw4MDY2MTc3MjE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cudnn", "name": "cudnn", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-02-13T15:47:35Z", "updated_at": "2018-02-15T20:19:26Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi everyone,</p>\n<p>I'm using the new torch.split function given a list of chunks as well as a LSTM/GRU network (both lead to the bug).</p>\n<p>On CPU, the code works perfectly.<br>\nOn GPU, If I do something else that a RNN forward during the iteration over torch.split, it's fine. otherwise it crashes.</p>\n<p><strong>StackTrace</strong><br>\n...<br>\nFile \"/home/diego/Github/DocAgg/pygcn_modified/models.py\", line 80, in forward<br>\noutput, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 357, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\", line 181, in forward<br>\noutput, hidden = func(input, self.all_weights, hx, batch_sizes)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\", line 315, in forward<br>\nreturn func(input, *fargs, **fkwargs)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\", line 284, in forward<br>\nVariable(dropout_desc.state) if dropout_desc.state is not None else None)<br>\nRuntimeError: CUDNN_STATUS_EXECUTION_FAILED</p>\n<p><strong>Code</strong><br>\nsentence_hidden_embeddings is a FloatTensor [657, 700]<br>\nnb_sentences_per_doc is a python list :[26, 13, 12, 20, 25, 26, 535]</p>\n<p>`</p>\n<pre><code>    all_sentence_embeddings_per_doc = torch.split(sentence_hidden_embeddings.unsqueeze(0), nb_sentences_per_doc, dim=1)[:-1]\n\n    document_embeddings = []\n    for sentence_embeddings_per_doc in all_sentence_embeddings_per_doc:\n        self.document_rnn_hidden = self.init_hidden()\n        output, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\n\n        # output[-1][-1] == hidden[-1][-1] (GRU) and output[-1][-1] == hidden[0][-1][-1] (LSTM)\n        doc_emb = hidden[-1] if self.mode == 'GRU' else (hidden[0][-1] if self.mode == 'LSTM' else None)\n        document_embeddings.append(doc_emb)\n\n        # TODO Remove. Doing only this perfectly works on GPU\n        #doc_emb = torch.mean(sentence_embeddings_per_doc, dim=1)\n        #document_embeddings.append(doc_emb)\n    cluster_embedding = torch.mean(torch.cat(document_embeddings), dim=0)`\n</code></pre>\n<p><strong>RNN</strong><br>\n`</p>\n<pre><code>    if self.mode == 'GRU':\n        self.document_rnn = nn.GRU(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n    elif self.mode == 'LSTM':\n        self.document_rnn = nn.LSTM(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n    self.document_rnn_hidden = self.init_hidden()\n</code></pre>\n<p>`</p>\n<p><strong>Hidden_init</strong><br>\n`</p>\n<pre><code>def init_hidden(self):\n    document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n    if self.mode == 'GRU':\n        return document_rnn_init_h\n    elif self.mode == 'LSTM':\n        document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n        return (document_rnn_init_h, document_rnn_init_c)\n</code></pre>\n<p>`</p>\n<ul>\n<li>OS: Linux Mint 18.2 Sonya</li>\n<li>PyTorch version: From source (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/2b2d56d8460d335daf5aa79774442a111d424f90/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/2b2d56d8460d335daf5aa79774442a111d424f90\"><tt>2b2d56d</tt></a>)</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version: 3.5</li>\n<li>CUDA/cuDNN version: 9.1/7.0.5 (latest versions)</li>\n<li>GPU models and configuration: Titan Xp 12Go (Driver 390.12)</li>\n<li>GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609</li>\n</ul>", "body_text": "Hi everyone,\nI'm using the new torch.split function given a list of chunks as well as a LSTM/GRU network (both lead to the bug).\nOn CPU, the code works perfectly.\nOn GPU, If I do something else that a RNN forward during the iteration over torch.split, it's fine. otherwise it crashes.\nStackTrace\n...\nFile \"/home/diego/Github/DocAgg/pygcn_modified/models.py\", line 80, in forward\noutput, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 357, in call\nresult = self.forward(*input, **kwargs)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\", line 181, in forward\noutput, hidden = func(input, self.all_weights, hx, batch_sizes)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\", line 315, in forward\nreturn func(input, *fargs, **fkwargs)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\", line 284, in forward\nVariable(dropout_desc.state) if dropout_desc.state is not None else None)\nRuntimeError: CUDNN_STATUS_EXECUTION_FAILED\nCode\nsentence_hidden_embeddings is a FloatTensor [657, 700]\nnb_sentences_per_doc is a python list :[26, 13, 12, 20, 25, 26, 535]\n`\n    all_sentence_embeddings_per_doc = torch.split(sentence_hidden_embeddings.unsqueeze(0), nb_sentences_per_doc, dim=1)[:-1]\n\n    document_embeddings = []\n    for sentence_embeddings_per_doc in all_sentence_embeddings_per_doc:\n        self.document_rnn_hidden = self.init_hidden()\n        output, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\n\n        # output[-1][-1] == hidden[-1][-1] (GRU) and output[-1][-1] == hidden[0][-1][-1] (LSTM)\n        doc_emb = hidden[-1] if self.mode == 'GRU' else (hidden[0][-1] if self.mode == 'LSTM' else None)\n        document_embeddings.append(doc_emb)\n\n        # TODO Remove. Doing only this perfectly works on GPU\n        #doc_emb = torch.mean(sentence_embeddings_per_doc, dim=1)\n        #document_embeddings.append(doc_emb)\n    cluster_embedding = torch.mean(torch.cat(document_embeddings), dim=0)`\n\nRNN\n`\n    if self.mode == 'GRU':\n        self.document_rnn = nn.GRU(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n    elif self.mode == 'LSTM':\n        self.document_rnn = nn.LSTM(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n    self.document_rnn_hidden = self.init_hidden()\n\n`\nHidden_init\n`\ndef init_hidden(self):\n    document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n    if self.mode == 'GRU':\n        return document_rnn_init_h\n    elif self.mode == 'LSTM':\n        document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n        return (document_rnn_init_h, document_rnn_init_c)\n\n`\n\nOS: Linux Mint 18.2 Sonya\nPyTorch version: From source (2b2d56d)\nHow you installed PyTorch (conda, pip, source): pip\nPython version: 3.5\nCUDA/cuDNN version: 9.1/7.0.5 (latest versions)\nGPU models and configuration: Titan Xp 12Go (Driver 390.12)\nGCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609", "body": "Hi everyone,\r\n\r\nI'm using the new torch.split function given a list of chunks as well as a LSTM/GRU network (both lead to the bug).\r\n\r\nOn CPU, the code works perfectly.\r\nOn GPU, If I do something else that a RNN forward during the iteration over torch.split, it's fine. otherwise it crashes.\r\n\r\n**StackTrace**\r\n ...\r\n  File \"/home/diego/Github/DocAgg/pygcn_modified/models.py\", line 80, in forward\r\n    output, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\", line 181, in forward\r\n    output, hidden = func(input, self.all_weights, hx, batch_sizes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\", line 315, in forward\r\n    return func(input, *fargs, **fkwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\", line 284, in forward\r\n    Variable(dropout_desc.state) if dropout_desc.state is not None else None)\r\nRuntimeError: CUDNN_STATUS_EXECUTION_FAILED\r\n\r\n**Code**\r\nsentence_hidden_embeddings is a FloatTensor [657, 700]\r\nnb_sentences_per_doc is a python list :[26, 13, 12, 20, 25, 26, 535]\r\n\r\n`\r\n\r\n        all_sentence_embeddings_per_doc = torch.split(sentence_hidden_embeddings.unsqueeze(0), nb_sentences_per_doc, dim=1)[:-1]\r\n\r\n        document_embeddings = []\r\n        for sentence_embeddings_per_doc in all_sentence_embeddings_per_doc:\r\n            self.document_rnn_hidden = self.init_hidden()\r\n            output, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\r\n\r\n            # output[-1][-1] == hidden[-1][-1] (GRU) and output[-1][-1] == hidden[0][-1][-1] (LSTM)\r\n            doc_emb = hidden[-1] if self.mode == 'GRU' else (hidden[0][-1] if self.mode == 'LSTM' else None)\r\n            document_embeddings.append(doc_emb)\r\n\r\n            # TODO Remove. Doing only this perfectly works on GPU\r\n            #doc_emb = torch.mean(sentence_embeddings_per_doc, dim=1)\r\n            #document_embeddings.append(doc_emb)\r\n        cluster_embedding = torch.mean(torch.cat(document_embeddings), dim=0)`\r\n\r\n__RNN__\r\n`\r\n\r\n        if self.mode == 'GRU':\r\n            self.document_rnn = nn.GRU(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\r\n        elif self.mode == 'LSTM':\r\n            self.document_rnn = nn.LSTM(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\r\n        self.document_rnn_hidden = self.init_hidden()\r\n\r\n`\r\n\r\n__Hidden_init__\r\n`\r\n\r\n    def init_hidden(self):\r\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\r\n        if self.mode == 'GRU':\r\n            return document_rnn_init_h\r\n        elif self.mode == 'LSTM':\r\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\r\n            return (document_rnn_init_h, document_rnn_init_c)\r\n\r\n`\r\n\r\n- OS: Linux Mint 18.2 Sonya\r\n- PyTorch version: From source (https://github.com/pytorch/pytorch/commit/2b2d56d8460d335daf5aa79774442a111d424f90)\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 9.1/7.0.5 (latest versions)\r\n- GPU models and configuration: Titan Xp 12Go (Driver 390.12)\r\n- GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609 \r\n"}