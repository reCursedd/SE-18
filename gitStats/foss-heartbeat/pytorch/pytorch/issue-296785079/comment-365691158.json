{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/365691158", "html_url": "https://github.com/pytorch/pytorch/issues/5213#issuecomment-365691158", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5213", "id": 365691158, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTY5MTE1OA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-14T17:56:30Z", "updated_at": "2018-02-14T17:56:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Well, I managed to reproduce it.</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self, mode, embedding_size):\n        super(Model, self).__init__()\n        self.mode = mode\n        self.embedding_size = embedding_size\n        self.nb_layers = 1\n        self.dropout = 0\n        self.batch_size = 1\n\n        if self.mode == 'GRU':\n            self.document_rnn = nn.GRU(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n        elif self.mode == 'LSTM':\n            self.document_rnn = nn.LSTM(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n        self.document_rnn_hidden = self.init_hidden()\n\n        self.init_hidden()\n\n    def init_hidden(self):\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n        if self.mode == 'GRU':\n            return document_rnn_init_h\n        elif self.mode == 'LSTM':\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n            return (document_rnn_init_h, document_rnn_init_c)\n\n    def forward(self, sentence_hidden_embeddings, nb_sentences_per_doc):\n        all_sentence_embeddings_per_doc = torch.split(sentence_hidden_embeddings.unsqueeze(0), nb_sentences_per_doc, dim=1)[:-1]\n\n        document_embeddings = []\n        for sentence_embeddings_per_doc in all_sentence_embeddings_per_doc:\n            self.document_rnn_hidden = self.init_hidden()\n            output, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\n\n            # output[-1][-1] == hidden[-1][-1] (GRU) and output[-1][-1] == hidden[0][-1][-1] (LSTM)\n            doc_emb = hidden[-1] if self.mode == 'GRU' else (hidden[0][-1] if self.mode == 'LSTM' else None)\n            document_embeddings.append(doc_emb)\n\n            # TODO Remove. Doing only this perfectly works on GPU\n            #doc_emb = torch.mean(sentence_embeddings_per_doc, dim=1)\n            #document_embeddings.append(doc_emb)\n        cluster_embedding = torch.mean(torch.cat(document_embeddings), dim=0)\n\n        return document_embeddings, cluster_embedding\n\n\n\nsentence_hidden_embeddings = Variable(torch.randn(657, 700).cuda())\nnb_sentences_per_doc = [26, 13, 12, 20, 25, 26, 535]\n\nmodel = Model('LSTM', 700)\nmodel = model.cuda()\nmodel(sentence_hidden_embeddings, nb_sentences_per_doc)\n</code></pre>", "body_text": "Well, I managed to reproduce it.\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    def __init__(self, mode, embedding_size):\n        super(Model, self).__init__()\n        self.mode = mode\n        self.embedding_size = embedding_size\n        self.nb_layers = 1\n        self.dropout = 0\n        self.batch_size = 1\n\n        if self.mode == 'GRU':\n            self.document_rnn = nn.GRU(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n        elif self.mode == 'LSTM':\n            self.document_rnn = nn.LSTM(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\n        self.document_rnn_hidden = self.init_hidden()\n\n        self.init_hidden()\n\n    def init_hidden(self):\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n        if self.mode == 'GRU':\n            return document_rnn_init_h\n        elif self.mode == 'LSTM':\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n            return (document_rnn_init_h, document_rnn_init_c)\n\n    def forward(self, sentence_hidden_embeddings, nb_sentences_per_doc):\n        all_sentence_embeddings_per_doc = torch.split(sentence_hidden_embeddings.unsqueeze(0), nb_sentences_per_doc, dim=1)[:-1]\n\n        document_embeddings = []\n        for sentence_embeddings_per_doc in all_sentence_embeddings_per_doc:\n            self.document_rnn_hidden = self.init_hidden()\n            output, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\n\n            # output[-1][-1] == hidden[-1][-1] (GRU) and output[-1][-1] == hidden[0][-1][-1] (LSTM)\n            doc_emb = hidden[-1] if self.mode == 'GRU' else (hidden[0][-1] if self.mode == 'LSTM' else None)\n            document_embeddings.append(doc_emb)\n\n            # TODO Remove. Doing only this perfectly works on GPU\n            #doc_emb = torch.mean(sentence_embeddings_per_doc, dim=1)\n            #document_embeddings.append(doc_emb)\n        cluster_embedding = torch.mean(torch.cat(document_embeddings), dim=0)\n\n        return document_embeddings, cluster_embedding\n\n\n\nsentence_hidden_embeddings = Variable(torch.randn(657, 700).cuda())\nnb_sentences_per_doc = [26, 13, 12, 20, 25, 26, 535]\n\nmodel = Model('LSTM', 700)\nmodel = model.cuda()\nmodel(sentence_hidden_embeddings, nb_sentences_per_doc)", "body": "Well, I managed to reproduce it.\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, mode, embedding_size):\r\n        super(Model, self).__init__()\r\n        self.mode = mode\r\n        self.embedding_size = embedding_size\r\n        self.nb_layers = 1\r\n        self.dropout = 0\r\n        self.batch_size = 1\r\n\r\n        if self.mode == 'GRU':\r\n            self.document_rnn = nn.GRU(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\r\n        elif self.mode == 'LSTM':\r\n            self.document_rnn = nn.LSTM(embedding_size, embedding_size, num_layers=self.nb_layers, bias=True, dropout=self.dropout, bidirectional=False, batch_first=True)\r\n        self.document_rnn_hidden = self.init_hidden()\r\n\r\n        self.init_hidden()\r\n\r\n    def init_hidden(self):\r\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\r\n        if self.mode == 'GRU':\r\n            return document_rnn_init_h\r\n        elif self.mode == 'LSTM':\r\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\r\n            return (document_rnn_init_h, document_rnn_init_c)\r\n\r\n    def forward(self, sentence_hidden_embeddings, nb_sentences_per_doc):\r\n        all_sentence_embeddings_per_doc = torch.split(sentence_hidden_embeddings.unsqueeze(0), nb_sentences_per_doc, dim=1)[:-1]\r\n\r\n        document_embeddings = []\r\n        for sentence_embeddings_per_doc in all_sentence_embeddings_per_doc:\r\n            self.document_rnn_hidden = self.init_hidden()\r\n            output, hidden = self.document_rnn(sentence_embeddings_per_doc, self.document_rnn_hidden)\r\n\r\n            # output[-1][-1] == hidden[-1][-1] (GRU) and output[-1][-1] == hidden[0][-1][-1] (LSTM)\r\n            doc_emb = hidden[-1] if self.mode == 'GRU' else (hidden[0][-1] if self.mode == 'LSTM' else None)\r\n            document_embeddings.append(doc_emb)\r\n\r\n            # TODO Remove. Doing only this perfectly works on GPU\r\n            #doc_emb = torch.mean(sentence_embeddings_per_doc, dim=1)\r\n            #document_embeddings.append(doc_emb)\r\n        cluster_embedding = torch.mean(torch.cat(document_embeddings), dim=0)\r\n\r\n        return document_embeddings, cluster_embedding\r\n\r\n\r\n\r\nsentence_hidden_embeddings = Variable(torch.randn(657, 700).cuda())\r\nnb_sentences_per_doc = [26, 13, 12, 20, 25, 26, 535]\r\n\r\nmodel = Model('LSTM', 700)\r\nmodel = model.cuda()\r\nmodel(sentence_hidden_embeddings, nb_sentences_per_doc)\r\n```"}