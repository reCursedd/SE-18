{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/365695242", "html_url": "https://github.com/pytorch/pytorch/issues/5213#issuecomment-365695242", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5213", "id": 365695242, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTY5NTI0Mg==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-14T18:10:42Z", "updated_at": "2018-02-14T18:10:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When I replace:</p>\n<pre><code>\n    def init_hidden(self):\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n        if self.mode == 'GRU':\n            return document_rnn_init_h\n        elif self.mode == 'LSTM':\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n            return (document_rnn_init_h, document_rnn_init_c)\n</code></pre>\n<p>with</p>\n<pre><code>    def init_hidden(self):\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n        if self.mode == 'GRU':\n            return document_rnn_init_h\n        elif self.mode == 'LSTM':\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n            return (document_rnn_init_h, document_rnn_init_c)\n</code></pre>\n<p>(that is, ensuring that the hidden parameters are put onto CUDA appropriately), it works. So it sounds like we're missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist.</p>", "body_text": "When I replace:\n\n    def init_hidden(self):\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n        if self.mode == 'GRU':\n            return document_rnn_init_h\n        elif self.mode == 'LSTM':\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n            return (document_rnn_init_h, document_rnn_init_c)\n\nwith\n    def init_hidden(self):\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n        if self.mode == 'GRU':\n            return document_rnn_init_h\n        elif self.mode == 'LSTM':\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n            return (document_rnn_init_h, document_rnn_init_c)\n\n(that is, ensuring that the hidden parameters are put onto CUDA appropriately), it works. So it sounds like we're missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist.", "body": "When I replace:\r\n\r\n```\r\n\r\n    def init_hidden(self):\r\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\r\n        if self.mode == 'GRU':\r\n            return document_rnn_init_h\r\n        elif self.mode == 'LSTM':\r\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\r\n            return (document_rnn_init_h, document_rnn_init_c)\r\n```\r\n\r\nwith\r\n\r\n```\r\n    def init_hidden(self):\r\n        document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\r\n        if self.mode == 'GRU':\r\n            return document_rnn_init_h\r\n        elif self.mode == 'LSTM':\r\n            document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\r\n            return (document_rnn_init_h, document_rnn_init_c)\r\n```\r\n\r\n(that is, ensuring that the hidden parameters are put onto CUDA appropriately), it works. So it sounds like we're missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist."}