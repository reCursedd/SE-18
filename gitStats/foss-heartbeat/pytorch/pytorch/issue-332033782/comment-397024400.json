{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397024400", "html_url": "https://github.com/pytorch/pytorch/pull/8425#issuecomment-397024400", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8425", "id": 397024400, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzAyNDQwMA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-13T17:41:56Z", "updated_at": "2018-06-13T17:41:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So, the point of this patch is to help avoid hapless end users who start using the shape propagation pass thinking that it is \"fast\" but are accidentally running CPU convolutions on big inputs when this happens (and they wonder why the shape propagator is slow...) I hate perf cliffs like that, because you spend a lot of time trying to figure out what's going on unless you actually read the shape propagator source code.</p>\n<p>Re your specific example, (1) reshape isn't necessarily cheap if you're dealing with big shapes (because you had to allocate the tensor in the first place) and (2) I'm fine with special casing to suppress the warning in this case.</p>", "body_text": "So, the point of this patch is to help avoid hapless end users who start using the shape propagation pass thinking that it is \"fast\" but are accidentally running CPU convolutions on big inputs when this happens (and they wonder why the shape propagator is slow...) I hate perf cliffs like that, because you spend a lot of time trying to figure out what's going on unless you actually read the shape propagator source code.\nRe your specific example, (1) reshape isn't necessarily cheap if you're dealing with big shapes (because you had to allocate the tensor in the first place) and (2) I'm fine with special casing to suppress the warning in this case.", "body": "So, the point of this patch is to help avoid hapless end users who start using the shape propagation pass thinking that it is \"fast\" but are accidentally running CPU convolutions on big inputs when this happens (and they wonder why the shape propagator is slow...) I hate perf cliffs like that, because you spend a lot of time trying to figure out what's going on unless you actually read the shape propagator source code.\r\n\r\nRe your specific example, (1) reshape isn't necessarily cheap if you're dealing with big shapes (because you had to allocate the tensor in the first place) and (2) I'm fine with special casing to suppress the warning in this case."}