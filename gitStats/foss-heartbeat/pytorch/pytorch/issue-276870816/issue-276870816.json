{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3890", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3890/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3890/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3890/events", "html_url": "https://github.com/pytorch/pytorch/issues/3890", "id": 276870816, "node_id": "MDU6SXNzdWUyNzY4NzA4MTY=", "number": 3890, "title": "requires_grad cannot be set to True for non-leaf nodes, but retain_grad needs requires_grad to be True", "user": {"login": "calebh", "id": 542102, "node_id": "MDQ6VXNlcjU0MjEwMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/542102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/calebh", "html_url": "https://github.com/calebh", "followers_url": "https://api.github.com/users/calebh/followers", "following_url": "https://api.github.com/users/calebh/following{/other_user}", "gists_url": "https://api.github.com/users/calebh/gists{/gist_id}", "starred_url": "https://api.github.com/users/calebh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/calebh/subscriptions", "organizations_url": "https://api.github.com/users/calebh/orgs", "repos_url": "https://api.github.com/users/calebh/repos", "events_url": "https://api.github.com/users/calebh/events{/privacy}", "received_events_url": "https://api.github.com/users/calebh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-11-27T00:21:01Z", "updated_at": "2018-08-09T03:09:39Z", "closed_at": "2017-11-27T09:21:04Z", "author_association": "NONE", "body_html": "<p>I am attempting to save the gradient of a non-leaf node. I attempt to set requires_grad to be true, and receive the following error:</p>\n<blockquote>\n<p>RuntimeError: you can only change requires_grad flags of leaf variables.</p>\n</blockquote>\n<p>Attempting to call retain_grad returns an error because requires_grad is False:</p>\n<blockquote>\n<p>RuntimeError: can't retain_grad on Variable that has requires_grad=False</p>\n</blockquote>\n<p>This makes saving the gradient of an already existing Variable by way of retain_grad impossible. Here's example code that produces these errors:</p>\n<pre><code>import torch\nimport numpy as np\nfrom torch.autograd import Variable\n\nx = Variable(torch.zeros(3,3), requires_grad=True)\ns = torch.sigmoid(x)\ns.requires_grad = True\ns.retain_grad()\ntotal = torch.sum(s)\ntotal.backward()\nassert(s.grad is not None)\n</code></pre>\n<p>I am using Python 3.5.2 and PyTorch 0.2.0.post3</p>", "body_text": "I am attempting to save the gradient of a non-leaf node. I attempt to set requires_grad to be true, and receive the following error:\n\nRuntimeError: you can only change requires_grad flags of leaf variables.\n\nAttempting to call retain_grad returns an error because requires_grad is False:\n\nRuntimeError: can't retain_grad on Variable that has requires_grad=False\n\nThis makes saving the gradient of an already existing Variable by way of retain_grad impossible. Here's example code that produces these errors:\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\n\nx = Variable(torch.zeros(3,3), requires_grad=True)\ns = torch.sigmoid(x)\ns.requires_grad = True\ns.retain_grad()\ntotal = torch.sum(s)\ntotal.backward()\nassert(s.grad is not None)\n\nI am using Python 3.5.2 and PyTorch 0.2.0.post3", "body": "I am attempting to save the gradient of a non-leaf node. I attempt to set requires_grad to be true, and receive the following error:\r\n> RuntimeError: you can only change requires_grad flags of leaf variables.\r\n\r\nAttempting to call retain_grad returns an error because requires_grad is False:\r\n> RuntimeError: can't retain_grad on Variable that has requires_grad=False\r\n\r\nThis makes saving the gradient of an already existing Variable by way of retain_grad impossible. Here's example code that produces these errors:\r\n```\r\nimport torch\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.zeros(3,3), requires_grad=True)\r\ns = torch.sigmoid(x)\r\ns.requires_grad = True\r\ns.retain_grad()\r\ntotal = torch.sum(s)\r\ntotal.backward()\r\nassert(s.grad is not None)\r\n```\r\n\r\nI am using Python 3.5.2 and PyTorch 0.2.0.post3"}