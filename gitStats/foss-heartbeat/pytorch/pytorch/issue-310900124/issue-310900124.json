{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6230", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6230/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6230/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6230/events", "html_url": "https://github.com/pytorch/pytorch/pull/6230", "id": 310900124, "node_id": "MDExOlB1bGxSZXF1ZXN0MTc5MTQ2NTU1", "number": 6230, "title": "Fix memory leak in maxpool3d backwards", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-04-03T16:05:28Z", "updated_at": "2018-04-03T19:47:30Z", "closed_at": "2018-04-03T19:47:30Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6230", "html_url": "https://github.com/pytorch/pytorch/pull/6230", "diff_url": "https://github.com/pytorch/pytorch/pull/6230.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6230.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #6222.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"310719640\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6222\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6222/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6222\">#6222</a></p>\n<p>We don't need to make sure gradInput is contiguous because it's always<br>\npassed in as an empty tensor (see CUDAFloatType.cpp after it gets<br>\ncodegen-ed). This was increasing the reference on gradInput and leaking<br>\nit.</p>\n<p>I'm not sure if there's a good way to test this. I put together a script<br>\nthat</p>\n<ol>\n<li>Prints out when a tensor is allocated and deallocated</li>\n<li>Checks allocations vs deallocations after running a python script<br>\nAnd verified that each allocation matches each deallocation.</li>\n</ol>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8813817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/li-roy\">@li-roy</a> maybe?</p>", "body_text": "Fixes #6222\nWe don't need to make sure gradInput is contiguous because it's always\npassed in as an empty tensor (see CUDAFloatType.cpp after it gets\ncodegen-ed). This was increasing the reference on gradInput and leaking\nit.\nI'm not sure if there's a good way to test this. I put together a script\nthat\n\nPrints out when a tensor is allocated and deallocated\nChecks allocations vs deallocations after running a python script\nAnd verified that each allocation matches each deallocation.\n\ncc @li-roy maybe?", "body": "Fixes #6222\r\n\r\nWe don't need to make sure gradInput is contiguous because it's always\r\npassed in as an empty tensor (see CUDAFloatType.cpp after it gets\r\ncodegen-ed). This was increasing the reference on gradInput and leaking\r\nit.\r\n\r\nI'm not sure if there's a good way to test this. I put together a script\r\nthat\r\n1) Prints out when a tensor is allocated and deallocated\r\n2) Checks allocations vs deallocations after running a python script\r\nAnd verified that each allocation matches each deallocation.\r\n\r\ncc @li-roy maybe?\r\n\r\n"}