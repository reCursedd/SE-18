{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13290", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13290/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13290/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13290/events", "html_url": "https://github.com/pytorch/pytorch/pull/13290", "id": 375302650, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI2Nzc4ODEx", "number": 13290, "title": "speed up torch.sparse_mask() cpu kernel", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-30T03:17:59Z", "updated_at": "2018-11-23T15:54:28Z", "closed_at": "2018-11-08T04:03:30Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13290", "html_url": "https://github.com/pytorch/pytorch/pull/13290", "diff_url": "https://github.com/pytorch/pytorch/pull/13290.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13290.patch"}, "body_html": "<ul>\n<li><code>sparse_mask(D, S)</code> is useful to implement backward for <code>sparse_addmm()</code></li>\n<li>previous <code>sparse_mask(D, S)</code> cpu kernel is not parallelized</li>\n<li>this PR speed up the cpu kernel for two separated cases:\n<ul>\n<li><code>D.dim == S.sparse_dim</code>: simply parallelize the kernel</li>\n<li><code>D.dim &gt; S.sparse_dim</code>: simply use CUDA kernel implementation</li>\n</ul>\n</li>\n<li>performance:</li>\n</ul>\n<p><code>D.dim == S.sparse_dim</code></p>\n<pre><code>&gt;&gt;&gt; nnz = 100000\n&gt;&gt;&gt; dims = [1000, 1000]\n&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \n               torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n&gt;&gt;&gt; V = torch.randn(nnz)\n&gt;&gt;&gt; size = torch.Size(dims)\n\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce()\n&gt;&gt;&gt; D = torch.randn(dims)\n\n&gt;&gt;&gt; %timeit D.sparse_mask(S)\n\n======= before change =======\n6.4 ms \u00b1 684 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n======= after change =======\n333 \u00b5s \u00b1 89.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<p><code>D.dim &gt; S.sparse_dim</code></p>\n<pre><code>&gt;&gt;&gt; nnz = 100000\n&gt;&gt;&gt; dims = [1000, 1000, 2, 2]\n&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \n               torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n&gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3])\n&gt;&gt;&gt; size = torch.Size(dims)\n\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce()\n&gt;&gt;&gt; D = torch.randn(dims)\n%timeit D.sparse_mask(S)\n\n======= before change =======\n495 ms \u00b1 41.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n======= after change =======\n594 \u00b5s \u00b1 68.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>", "body_text": "sparse_mask(D, S) is useful to implement backward for sparse_addmm()\nprevious sparse_mask(D, S) cpu kernel is not parallelized\nthis PR speed up the cpu kernel for two separated cases:\n\nD.dim == S.sparse_dim: simply parallelize the kernel\nD.dim > S.sparse_dim: simply use CUDA kernel implementation\n\n\nperformance:\n\nD.dim == S.sparse_dim\n>>> nnz = 100000\n>>> dims = [1000, 1000]\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \n               torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n>>> V = torch.randn(nnz)\n>>> size = torch.Size(dims)\n\n>>> S = torch.sparse_coo_tensor(I, V, size).coalesce()\n>>> D = torch.randn(dims)\n\n>>> %timeit D.sparse_mask(S)\n\n======= before change =======\n6.4 ms \u00b1 684 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n======= after change =======\n333 \u00b5s \u00b1 89.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nD.dim > S.sparse_dim\n>>> nnz = 100000\n>>> dims = [1000, 1000, 2, 2]\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \n               torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n>>> V = torch.randn(nnz, dims[2], dims[3])\n>>> size = torch.Size(dims)\n\n>>> S = torch.sparse_coo_tensor(I, V, size).coalesce()\n>>> D = torch.randn(dims)\n%timeit D.sparse_mask(S)\n\n======= before change =======\n495 ms \u00b1 41.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n======= after change =======\n594 \u00b5s \u00b1 68.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)", "body": "- `sparse_mask(D, S)` is useful to implement backward for `sparse_addmm()`\r\n- previous `sparse_mask(D, S)` cpu kernel is not parallelized\r\n- this PR speed up the cpu kernel for two separated cases:\r\n  - `D.dim == S.sparse_dim`: simply parallelize the kernel\r\n  - `D.dim > S.sparse_dim`: simply use CUDA kernel implementation\r\n- performance:\r\n\r\n`D.dim == S.sparse_dim`\r\n```\r\n>>> nnz = 100000\r\n>>> dims = [1000, 1000]\r\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \r\n               torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\r\n>>> V = torch.randn(nnz)\r\n>>> size = torch.Size(dims)\r\n\r\n>>> S = torch.sparse_coo_tensor(I, V, size).coalesce()\r\n>>> D = torch.randn(dims)\r\n\r\n>>> %timeit D.sparse_mask(S)\r\n\r\n======= before change =======\r\n6.4 ms \u00b1 684 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n======= after change =======\r\n333 \u00b5s \u00b1 89.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n`D.dim > S.sparse_dim`\r\n```\r\n>>> nnz = 100000\r\n>>> dims = [1000, 1000, 2, 2]\r\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)), \r\n               torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\r\n>>> V = torch.randn(nnz, dims[2], dims[3])\r\n>>> size = torch.Size(dims)\r\n\r\n>>> S = torch.sparse_coo_tensor(I, V, size).coalesce()\r\n>>> D = torch.randn(dims)\r\n%timeit D.sparse_mask(S)\r\n\r\n======= before change =======\r\n495 ms \u00b1 41.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n======= after change =======\r\n594 \u00b5s \u00b1 68.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```"}