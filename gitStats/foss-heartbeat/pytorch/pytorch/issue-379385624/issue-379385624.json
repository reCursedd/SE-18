{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13803", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13803/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13803/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13803/events", "html_url": "https://github.com/pytorch/pytorch/issues/13803", "id": 379385624, "node_id": "MDU6SXNzdWUzNzkzODU2MjQ=", "number": 13803, "title": "Cross-GPU copies failing silently", "user": {"login": "nikitakit", "id": 252225, "node_id": "MDQ6VXNlcjI1MjIyNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/252225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikitakit", "html_url": "https://github.com/nikitakit", "followers_url": "https://api.github.com/users/nikitakit/followers", "following_url": "https://api.github.com/users/nikitakit/following{/other_user}", "gists_url": "https://api.github.com/users/nikitakit/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikitakit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikitakit/subscriptions", "organizations_url": "https://api.github.com/users/nikitakit/orgs", "repos_url": "https://api.github.com/users/nikitakit/repos", "events_url": "https://api.github.com/users/nikitakit/events{/privacy}", "received_events_url": "https://api.github.com/users/nikitakit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-10T03:26:11Z", "updated_at": "2018-11-12T18:22:07Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>I'm trying to build a model where different layers are located on different GPU devices. However, any attempt to copy tensors between devices fails silently unless the copy uses the CPU as in intermediate.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.cuda.device_count()\n<span class=\"pl-c1\">2</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> cuda0 <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> cuda1 <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:1<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> t <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>cuda1)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> t\ntensor([<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.], <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:1<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> t.to(cuda0)\ntensor([<span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>.], <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> t.cpu().to(cuda0)\ntensor([<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.], <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>In general, any attempt to copy tensors by doing <code>.to(different_cuda_device)</code> results in a tensor with the correct device/shape/dtype but where all of the values are set to zero.</p>\n<h2>Expected behavior</h2>\n<p>I would expect <code>t.to(cuda0)</code> and <code>t.cpu().to(cuda0)</code> to both result in <code>tensor([1., 1., 1., 1., 1.], device='cuda:0')</code>.</p>\n<p>Alternatively, if there's something wrong with the software setup or I'm not using the API correctly, an error message would be more appropriate than the current behavior of populating the tensor with all-zero values.</p>\n<h2>Environment</h2>\n<p>PyTorch version: 0.4.1.post2<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Scientific Linux 7.4 (Nitrogen)<br>\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration:<br>\nGPU 0: Tesla K80<br>\nGPU 1: Tesla K80<br>\nGPU 2: Tesla K80<br>\nGPU 3: Tesla K80</p>\n<p>Nvidia driver version: 384.90<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[conda] cuda90                    1.0                  h6433d27_0    pytorch<br>\n[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch</p>", "body_text": "\ud83d\udc1b Bug\nI'm trying to build a model where different layers are located on different GPU devices. However, any attempt to copy tensors between devices fails silently unless the copy uses the CPU as in intermediate.\nTo Reproduce\nSteps to reproduce the behavior:\n>>> import torch\n>>> torch.cuda.device_count()\n2\n>>> cuda0 = torch.device('cuda:0')\n>>> cuda1 = torch.device('cuda:1')\n>>> t = torch.ones(5, device=cuda1)\n>>> t\ntensor([1., 1., 1., 1., 1.], device='cuda:1')\n>>> t.to(cuda0)\ntensor([0., 0., 0., 0., 0.], device='cuda:0')\n>>> t.cpu().to(cuda0)\ntensor([1., 1., 1., 1., 1.], device='cuda:0')\nIn general, any attempt to copy tensors by doing .to(different_cuda_device) results in a tensor with the correct device/shape/dtype but where all of the values are set to zero.\nExpected behavior\nI would expect t.to(cuda0) and t.cpu().to(cuda0) to both result in tensor([1., 1., 1., 1., 1.], device='cuda:0').\nAlternatively, if there's something wrong with the software setup or I'm not using the API correctly, an error message would be more appropriate than the current behavior of populating the tensor with all-zero values.\nEnvironment\nPyTorch version: 0.4.1.post2\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Scientific Linux 7.4 (Nitrogen)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCMake version: Could not collect\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration:\nGPU 0: Tesla K80\nGPU 1: Tesla K80\nGPU 2: Tesla K80\nGPU 3: Tesla K80\nNvidia driver version: 384.90\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\n[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch", "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to build a model where different layers are located on different GPU devices. However, any attempt to copy tensors between devices fails silently unless the copy uses the CPU as in intermediate.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\n>>> import torch\r\n>>> torch.cuda.device_count()\r\n2\r\n>>> cuda0 = torch.device('cuda:0')\r\n>>> cuda1 = torch.device('cuda:1')\r\n>>> t = torch.ones(5, device=cuda1)\r\n>>> t\r\ntensor([1., 1., 1., 1., 1.], device='cuda:1')\r\n>>> t.to(cuda0)\r\ntensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n>>> t.cpu().to(cuda0)\r\ntensor([1., 1., 1., 1., 1.], device='cuda:0')\r\n```\r\n\r\nIn general, any attempt to copy tensors by doing `.to(different_cuda_device)` results in a tensor with the correct device/shape/dtype but where all of the values are set to zero.\r\n\r\n## Expected behavior\r\n\r\nI would expect `t.to(cuda0)` and `t.cpu().to(cuda0)` to both result in `tensor([1., 1., 1., 1., 1.], device='cuda:0')`.\r\n\r\nAlternatively, if there's something wrong with the software setup or I'm not using the API correctly, an error message would be more appropriate than the current behavior of populating the tensor with all-zero values.\r\n\r\n## Environment\r\n\r\nPyTorch version: 0.4.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Scientific Linux 7.4 (Nitrogen)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla K80\r\nGPU 1: Tesla K80\r\nGPU 2: Tesla K80\r\nGPU 3: Tesla K80\r\n\r\nNvidia driver version: 384.90\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch\r\n\r\n"}