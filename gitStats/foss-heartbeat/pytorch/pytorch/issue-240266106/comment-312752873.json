{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/312752873", "html_url": "https://github.com/pytorch/pytorch/pull/1971#issuecomment-312752873", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1971", "id": 312752873, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjc1Mjg3Mw==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-04T00:14:42Z", "updated_at": "2017-07-04T00:14:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>While this does the job of getting rid of memcpy, perhaps the proper fix would be to fix the wrapper? In <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L25-L26\">https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L25-L26</a> add_matrix and output are still the same object for an inplace operation, they become different objects somewhere in the wrapper, which is certainly surprising. If beta !=0 as we've seen cudaMemcpyAsync will be called on overlapping memory of  output and add_matrix, which is UB. Yeah, most of the time beta==0, or even if it is not, it will work anyway, cause memory regions coincide and not just overlap, so not a big deal, but still having a proper fix would be nice.</p>", "body_text": "While this does the job of getting rid of memcpy, perhaps the proper fix would be to fix the wrapper? In https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L25-L26 add_matrix and output are still the same object for an inplace operation, they become different objects somewhere in the wrapper, which is certainly surprising. If beta !=0 as we've seen cudaMemcpyAsync will be called on overlapping memory of  output and add_matrix, which is UB. Yeah, most of the time beta==0, or even if it is not, it will work anyway, cause memory regions coincide and not just overlap, so not a big deal, but still having a proper fix would be nice.", "body": "While this does the job of getting rid of memcpy, perhaps the proper fix would be to fix the wrapper? In https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L25-L26 add_matrix and output are still the same object for an inplace operation, they become different objects somewhere in the wrapper, which is certainly surprising. If beta !=0 as we've seen cudaMemcpyAsync will be called on overlapping memory of  output and add_matrix, which is UB. Yeah, most of the time beta==0, or even if it is not, it will work anyway, cause memory regions coincide and not just overlap, so not a big deal, but still having a proper fix would be nice.  "}