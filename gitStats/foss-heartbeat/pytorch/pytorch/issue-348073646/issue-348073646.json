{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10273", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10273/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10273/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10273/events", "html_url": "https://github.com/pytorch/pytorch/pull/10273", "id": 348073646, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA2NTEwMjU3", "number": 10273, "title": "[ready] Move bernoulli into ATen", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-06T20:38:22Z", "updated_at": "2018-11-23T15:51:28Z", "closed_at": "2018-09-19T23:47:20Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10273", "html_url": "https://github.com/pytorch/pytorch/pull/10273", "diff_url": "https://github.com/pytorch/pytorch/pull/10273.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10273.patch"}, "body_html": "<h2>Fixes</h2>\n<ul>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"347659396\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10236\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10236/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/10236\">#10236</a> : torch.bernoulli's out kwarg is broken<br>\nfixed in moving <code>bernoulli_out</code> to ATen</li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345066733\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9917\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9917/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9917\">#9917</a> : BUG torch.bernoulli(p.expand(shape)) is broken<br>\nfixed in moving all <code>bernoulli</code> ops in ATen to use the modern apply utils methods</li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"348888555\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10357\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10357/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/10357\">#10357</a> : torch.bernoulli inconsistent gpu/cpu results<br>\nfixed by adding CUDA asserts</li>\n</ul>\n<h2>Notable changes:</h2>\n<p>In order to use <code>curand_uniform4</code>, I made some changes to <code>CUDAApplyUtils.cuh</code>. Specifically, I introduced an optional template parameter <code>int step</code> to the <code>CUDA_tensor_applyN</code> methods, representing that we want to process <code>step</code> values at each time for each of the <code>N</code> tensors.</p>\n<p>The calling convention for <code>step = 1</code> (default) isn't changed. But if <code>step &gt; 1</code>, the given lambda <code>op</code> must take in <code>int n</code> as its first argument, representing the number of valid values, because there may not be full <code>step</code> values at the boundary. E.g., here is what the <code>bernoulli(self, p_tensor)</code> call look like:</p>\n<div class=\"highlight highlight-source-c++\"><pre>  <span class=\"pl-c\"><span class=\"pl-c\">//</span> The template argument `4` below indicates that we want to operate on four</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> element at each time. See NOTE [ CUDA_tensor_applyN helpers ] for details.</span>\n  at::cuda::CUDA_tensor_apply2&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">prob_t</span>, <span class=\"pl-c1\">4</span>&gt;(\n      ret, p,\n      [seeds] __device__(\n          <span class=\"pl-k\">int</span> n, <span class=\"pl-c1\">scalar_t</span>&amp; v1, <span class=\"pl-c1\">scalar_t</span>&amp; v2, <span class=\"pl-c1\">scalar_t</span>&amp; v3, <span class=\"pl-c1\">scalar_t</span>&amp; v4,\n          <span class=\"pl-k\">const</span> <span class=\"pl-c1\">prob_t</span>&amp; p1, <span class=\"pl-k\">const</span> <span class=\"pl-c1\">prob_t</span>&amp; p2, <span class=\"pl-k\">const</span> <span class=\"pl-c1\">prob_t</span>&amp; p3, <span class=\"pl-k\">const</span> <span class=\"pl-c1\">prob_t</span>&amp; p4) {\n        curandStatePhilox4_32_10_t state;\n        <span class=\"pl-c1\">curand_init</span>(\n            seeds.<span class=\"pl-smi\">first</span>,\n            blockIdx.<span class=\"pl-smi\">x</span> * blockDim.<span class=\"pl-smi\">x</span> + threadIdx.<span class=\"pl-smi\">x</span>,\n            seeds.<span class=\"pl-smi\">second</span>,\n            &amp;state);\n        float4 <span class=\"pl-c1\">rand</span> = <span class=\"pl-c1\">curand_uniform4</span>(&amp;state);\n        <span class=\"pl-k\">switch</span> (n) {\n          <span class=\"pl-k\">case</span> <span class=\"pl-c1\">4</span>: {\n            <span class=\"pl-c1\">assert</span>(<span class=\"pl-c1\">0</span> &lt;= p4 &amp;&amp; p4 &lt;= <span class=\"pl-c1\">1</span>);\n            v4 = <span class=\"pl-k\">static_cast</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(<span class=\"pl-c1\">rand</span>.<span class=\"pl-smi\">w</span> &lt;= p4);\n          }\n          <span class=\"pl-k\">case</span> <span class=\"pl-c1\">3</span>: {\n            <span class=\"pl-c1\">assert</span>(<span class=\"pl-c1\">0</span> &lt;= p3 &amp;&amp; p3 &lt;= <span class=\"pl-c1\">1</span>);\n            v3 = <span class=\"pl-k\">static_cast</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(<span class=\"pl-c1\">rand</span>.<span class=\"pl-smi\">z</span> &lt;= p3);\n          }\n          <span class=\"pl-k\">case</span> <span class=\"pl-c1\">2</span>: {\n            <span class=\"pl-c1\">assert</span>(<span class=\"pl-c1\">0</span> &lt;= p2 &amp;&amp; p2 &lt;= <span class=\"pl-c1\">1</span>);\n            v2 = <span class=\"pl-k\">static_cast</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(<span class=\"pl-c1\">rand</span>.<span class=\"pl-smi\">y</span> &lt;= p2);\n          }\n          <span class=\"pl-k\">case</span> <span class=\"pl-c1\">1</span>: {\n            <span class=\"pl-c1\">assert</span>(<span class=\"pl-c1\">0</span> &lt;= p1 &amp;&amp; p1 &lt;= <span class=\"pl-c1\">1</span>);\n            v1 = <span class=\"pl-k\">static_cast</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(<span class=\"pl-c1\">rand</span>.<span class=\"pl-smi\">x</span> &lt;= p1);\n          }\n        }\n      }\n    );</pre></div>\n<h2>Benchmarking</h2>\n<p>Benchmarking on <code>torch.rand(200, 300, 400)</code> 20 times, each time with 20 loops:</p>\n<p>post patch</p>\n<pre><code>\u279c  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py\ntorch.bernoulli(x)\n6.841588497161865 +- 0.05413117632269859\ntorch.bernoulli(xc)\n0.05963418632745743 +- 0.0008014909108169377\nx.bernoulli_()\n0.4024486541748047 +- 0.0021550932433456182\nxc.bernoulli_()\n0.02167394384741783 +- 2.3818030967959203e-05\n\n</code></pre>\n<p>pre-patch</p>\n<pre><code>\u279c  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py\ntorch.bernoulli(x)\n12.394511222839355 +- 0.0966421514749527\ntorch.bernoulli(xc)\n0.08970972150564194 +- 0.0038722590543329716\nx.bernoulli_()\n1.654480218887329 +- 0.02364428900182247\nxc.bernoulli_()\n0.058352887630462646 +- 0.003094920190051198\n\n</code></pre>", "body_text": "Fixes\n\n#10236 : torch.bernoulli's out kwarg is broken\nfixed in moving bernoulli_out to ATen\n#9917 : BUG torch.bernoulli(p.expand(shape)) is broken\nfixed in moving all bernoulli ops in ATen to use the modern apply utils methods\n#10357 : torch.bernoulli inconsistent gpu/cpu results\nfixed by adding CUDA asserts\n\nNotable changes:\nIn order to use curand_uniform4, I made some changes to CUDAApplyUtils.cuh. Specifically, I introduced an optional template parameter int step to the CUDA_tensor_applyN methods, representing that we want to process step values at each time for each of the N tensors.\nThe calling convention for step = 1 (default) isn't changed. But if step > 1, the given lambda op must take in int n as its first argument, representing the number of valid values, because there may not be full step values at the boundary. E.g., here is what the bernoulli(self, p_tensor) call look like:\n  // The template argument `4` below indicates that we want to operate on four\n  // element at each time. See NOTE [ CUDA_tensor_applyN helpers ] for details.\n  at::cuda::CUDA_tensor_apply2<scalar_t, prob_t, 4>(\n      ret, p,\n      [seeds] __device__(\n          int n, scalar_t& v1, scalar_t& v2, scalar_t& v3, scalar_t& v4,\n          const prob_t& p1, const prob_t& p2, const prob_t& p3, const prob_t& p4) {\n        curandStatePhilox4_32_10_t state;\n        curand_init(\n            seeds.first,\n            blockIdx.x * blockDim.x + threadIdx.x,\n            seeds.second,\n            &state);\n        float4 rand = curand_uniform4(&state);\n        switch (n) {\n          case 4: {\n            assert(0 <= p4 && p4 <= 1);\n            v4 = static_cast<scalar_t>(rand.w <= p4);\n          }\n          case 3: {\n            assert(0 <= p3 && p3 <= 1);\n            v3 = static_cast<scalar_t>(rand.z <= p3);\n          }\n          case 2: {\n            assert(0 <= p2 && p2 <= 1);\n            v2 = static_cast<scalar_t>(rand.y <= p2);\n          }\n          case 1: {\n            assert(0 <= p1 && p1 <= 1);\n            v1 = static_cast<scalar_t>(rand.x <= p1);\n          }\n        }\n      }\n    );\nBenchmarking\nBenchmarking on torch.rand(200, 300, 400) 20 times, each time with 20 loops:\npost patch\n\u279c  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py\ntorch.bernoulli(x)\n6.841588497161865 +- 0.05413117632269859\ntorch.bernoulli(xc)\n0.05963418632745743 +- 0.0008014909108169377\nx.bernoulli_()\n0.4024486541748047 +- 0.0021550932433456182\nxc.bernoulli_()\n0.02167394384741783 +- 2.3818030967959203e-05\n\n\npre-patch\n\u279c  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py\ntorch.bernoulli(x)\n12.394511222839355 +- 0.0966421514749527\ntorch.bernoulli(xc)\n0.08970972150564194 +- 0.0038722590543329716\nx.bernoulli_()\n1.654480218887329 +- 0.02364428900182247\nxc.bernoulli_()\n0.058352887630462646 +- 0.003094920190051198", "body": "## Fixes \r\n+ https://github.com/pytorch/pytorch/issues/10236 : torch.bernoulli's out kwarg is broken\r\n  fixed in moving `bernoulli_out` to ATen\r\n+ https://github.com/pytorch/pytorch/issues/9917 : BUG torch.bernoulli(p.expand(shape)) is broken\r\n  fixed in moving all `bernoulli` ops in ATen to use the modern apply utils methods\r\n+ https://github.com/pytorch/pytorch/issues/10357 : torch.bernoulli inconsistent gpu/cpu results\r\n  fixed by adding CUDA asserts\r\n\r\n## Notable changes:\r\n\r\nIn order to use `curand_uniform4`, I made some changes to `CUDAApplyUtils.cuh`. Specifically, I introduced an optional template parameter `int step` to the `CUDA_tensor_applyN` methods, representing that we want to process `step` values at each time for each of the `N` tensors. \r\n\r\nThe calling convention for `step = 1` (default) isn't changed. But if `step > 1`, the given lambda `op` must take in `int n` as its first argument, representing the number of valid values, because there may not be full `step` values at the boundary. E.g., here is what the `bernoulli(self, p_tensor)` call look like:\r\n```cpp\r\n\r\n  // The template argument `4` below indicates that we want to operate on four\r\n  // element at each time. See NOTE [ CUDA_tensor_applyN helpers ] for details.\r\n  at::cuda::CUDA_tensor_apply2<scalar_t, prob_t, 4>(\r\n      ret, p,\r\n      [seeds] __device__(\r\n          int n, scalar_t& v1, scalar_t& v2, scalar_t& v3, scalar_t& v4,\r\n          const prob_t& p1, const prob_t& p2, const prob_t& p3, const prob_t& p4) {\r\n        curandStatePhilox4_32_10_t state;\r\n        curand_init(\r\n            seeds.first,\r\n            blockIdx.x * blockDim.x + threadIdx.x,\r\n            seeds.second,\r\n            &state);\r\n        float4 rand = curand_uniform4(&state);\r\n        switch (n) {\r\n          case 4: {\r\n            assert(0 <= p4 && p4 <= 1);\r\n            v4 = static_cast<scalar_t>(rand.w <= p4);\r\n          }\r\n          case 3: {\r\n            assert(0 <= p3 && p3 <= 1);\r\n            v3 = static_cast<scalar_t>(rand.z <= p3);\r\n          }\r\n          case 2: {\r\n            assert(0 <= p2 && p2 <= 1);\r\n            v2 = static_cast<scalar_t>(rand.y <= p2);\r\n          }\r\n          case 1: {\r\n            assert(0 <= p1 && p1 <= 1);\r\n            v1 = static_cast<scalar_t>(rand.x <= p1);\r\n          }\r\n        }\r\n      }\r\n    );\r\n``` \r\n\r\n## Benchmarking\r\n\r\nBenchmarking on `torch.rand(200, 300, 400)` 20 times, each time with 20 loops:\r\n\r\npost patch\r\n```\r\n\u279c  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py\r\ntorch.bernoulli(x)\r\n6.841588497161865 +- 0.05413117632269859\r\ntorch.bernoulli(xc)\r\n0.05963418632745743 +- 0.0008014909108169377\r\nx.bernoulli_()\r\n0.4024486541748047 +- 0.0021550932433456182\r\nxc.bernoulli_()\r\n0.02167394384741783 +- 2.3818030967959203e-05\r\n\r\n```\r\n\r\npre-patch\r\n```\r\n\u279c  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py\r\ntorch.bernoulli(x)\r\n12.394511222839355 +- 0.0966421514749527\r\ntorch.bernoulli(xc)\r\n0.08970972150564194 +- 0.0038722590543329716\r\nx.bernoulli_()\r\n1.654480218887329 +- 0.02364428900182247\r\nxc.bernoulli_()\r\n0.058352887630462646 +- 0.003094920190051198\r\n\r\n```"}