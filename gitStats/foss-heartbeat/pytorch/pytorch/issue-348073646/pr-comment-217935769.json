{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217935769", "pull_request_review_id": 155766199, "id": 217935769, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzkzNTc2OQ==", "diff_hunk": "@@ -251,20 +545,228 @@ inline dim3 getApplyBlock() {\n   return dim3(AT_APPLY_THREADS_PER_BLOCK);\n }\n \n-/*\n-  Apply a pointwise operator to two tensors.\n \n-  The calling convention for op is a function/functor that takes takes two references to\n-  type scalar; at least one of these references should be non-const in order to write the output.\n-  For example, to compute a = b^2, op would be of the form:\n-  [] __device__ (scalar &a_val, const scalar &b_val) { a_val = b_val * b_val; };\n+/*\n+  NOTE [ CUDA_tensor_applyN helpers ]\n+\n+  The following CUDA_tensor_applyN (where N currently can be 1, 2, 3, or 4)\n+  functions apply a pointwise operator to N tensor(s).\n+\n+  The calling convention is\n+\n+  1. The tempalte arguments should be, sequentially,\n+    - First N typename args specify the scalar types of each of the N tensors.\n+    - (Optional) `int step` arg specifies the number of elements processed\n+      together at the same time.\n+      Default is 1.\n+    - A usually omitted (i.e., inferred) typename arg specifies the type of the\n+      function/functor applied on `N * step` values  in each iteration of each\n+      CUDA thread.\n+  2. The arguments should be, sequentially,\n+    - N tensors\n+    - op: a function/functor that processes `N * step` values at the same time.\n+      - If `step == 1`, it must have signature\n+        `void(*)(scalar1_t&, scalar2_t&, ..., scalarN_t&)`, where\n+        `scalar*_t`s are the first N typename template args, and the inputs\n+        are the `N` values from the `N` tensors retrieved at a common index.\n+      - Otherwise, it must must have signature\n+          void(*)(int n, scalar1_t&, scalar1_t&, ..., scalar1_t&,  // repeat `step` times\n+                         scalar2_t&, scalar2_t&, ..., scalar2_t&,  // repeat `step` times\n+                         ...,\n+                         scalarN_t&, scalarN_t&, ..., scalarN_t&)  // repeat `step` times\n+        Different from `step == 1` case, it processes `N * step` values taken\n+        from `step` common indices. Moreover, the first input `n` represents the\n+        number of valid indices (it will always have `0 < n <= step`). It will\n+        almost always be `step`, but at the boundary we may not have full `step`\n+        elements and `n` can be a lesser value.\n+\n+        E.g., if `step == 4` and `N == 2`, `op` could be\n+\n+          [](int n, scalar1_t &u1, scalar1_t &u2, scalar1_t &u3, scalar1_t &u4,\n+                    scalar2_t &v1, scalar2_t &v2, scalar2_t &v3, scalar2_t &v4) {\n+            // Only process u1, ..., un and v1, ..., vn.\n+            // So if `n == 3`, `u4` and `v4` need not to be considered.\n+          }\n+\n+      In both cases, the references can actually be const, but at least one of\n+      them should be non-const in order to write the output.\n+    - (Optional, but recommended) N TensorArgType args that specify for each\n+      tensor whether `op` reads AND writes ] (i.e., TensorArgType::ReadWrite),\n+      or only reads (i.e., TensorArgType::ReadOnly).\n+      Default is TensorArgType::ReadWrite for first Tensor, and\n+                 TensorArgType::ReadOnly  for the rest.\n+\n+\n+  E.g.,\n+\n+  to compute a = b^2 for a and b of same dtype, we can call\n+\n+  CUDA_tensor_apply2<scalar, scalar>(\n+    a, b,\n+    [] __device__ (scalar &a_val, const scalar &b_val) { a_val = b_val * b_val; }\n+  );\n+\n+  to work on 2 values at the same time, we can call\n+\n+  CUDA_tensor_apply2<scalar1, scalar2, 2>(\n+    a, b,\n+    [] __device__ (int n, scalar1 &a_val1, scalar1 &a_val2,\n+                          const scalar2 &b_val1, const scalar2 &b_val2) {\n+      // call special vectorized op here, or just do elementwise and enjoy unrolling...\n+      // if n == 1, only process a_val1 and b_val1\n+    }\n+  );\n */\n-template <typename scalar1, typename scalar2, typename Op>\n-bool CUDA_tensor_apply2(at::Tensor a,\n-                        at::Tensor b,\n-                        Op op,\n-                        TensorArgType aType = TensorArgType::ReadWrite,\n-                        TensorArgType bType = TensorArgType::ReadOnly) {\n+\n+\n+template <typename scalar, int step, typename Op>\n+inline bool CUDA_tensor_apply1(at::Tensor a,", "path": "aten/src/ATen/cuda/CUDAApplyUtils.cuh", "position": null, "original_position": 586, "commit_id": "e4013d853c2a9a837f6d396f156a4a60eeff8119", "original_commit_id": "9986cd4b590828c858f0575ce6bcbad9664b84a5", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "I'm confused. You seem to be returning a boolean saying if the operation actually succeeds or not. But then you proceed to not check it at any of the call sites of this function. Perhaps you should replace it with an `AT_ERROR`?", "created_at": "2018-09-17T01:13:20Z", "updated_at": "2018-11-23T15:51:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/10273#discussion_r217935769", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10273", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217935769"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10273#discussion_r217935769"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10273"}}, "body_html": "<p>I'm confused. You seem to be returning a boolean saying if the operation actually succeeds or not. But then you proceed to not check it at any of the call sites of this function. Perhaps you should replace it with an <code>AT_ERROR</code>?</p>", "body_text": "I'm confused. You seem to be returning a boolean saying if the operation actually succeeds or not. But then you proceed to not check it at any of the call sites of this function. Perhaps you should replace it with an AT_ERROR?"}