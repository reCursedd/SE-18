{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217937254", "pull_request_review_id": 155767992, "id": 217937254, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzkzNzI1NA==", "diff_hunk": "@@ -251,20 +545,228 @@ inline dim3 getApplyBlock() {\n   return dim3(AT_APPLY_THREADS_PER_BLOCK);\n }\n \n-/*\n-  Apply a pointwise operator to two tensors.\n \n-  The calling convention for op is a function/functor that takes takes two references to\n-  type scalar; at least one of these references should be non-const in order to write the output.\n-  For example, to compute a = b^2, op would be of the form:\n-  [] __device__ (scalar &a_val, const scalar &b_val) { a_val = b_val * b_val; };\n+/*\n+  NOTE [ CUDA_tensor_applyN helpers ]\n+\n+  The following CUDA_tensor_applyN (where N currently can be 1, 2, 3, or 4)\n+  functions apply a pointwise operator to N tensor(s).\n+\n+  The calling convention is\n+\n+  1. The tempalte arguments should be, sequentially,\n+    - First N typename args specify the scalar types of each of the N tensors.\n+    - (Optional) `int step` arg specifies the number of elements processed\n+      together at the same time.\n+      Default is 1.\n+    - A usually omitted (i.e., inferred) typename arg specifies the type of the\n+      function/functor applied on `N * step` values  in each iteration of each\n+      CUDA thread.\n+  2. The arguments should be, sequentially,\n+    - N tensors\n+    - op: a function/functor that processes `N * step` values at the same time.\n+      - If `step == 1`, it must have signature\n+        `void(*)(scalar1_t&, scalar2_t&, ..., scalarN_t&)`, where\n+        `scalar*_t`s are the first N typename template args, and the inputs\n+        are the `N` values from the `N` tensors retrieved at a common index.\n+      - Otherwise, it must must have signature\n+          void(*)(int n, scalar1_t&, scalar1_t&, ..., scalar1_t&,  // repeat `step` times\n+                         scalar2_t&, scalar2_t&, ..., scalar2_t&,  // repeat `step` times\n+                         ...,\n+                         scalarN_t&, scalarN_t&, ..., scalarN_t&)  // repeat `step` times\n+        Different from `step == 1` case, it processes `N * step` values taken\n+        from `step` common indices. Moreover, the first input `n` represents the\n+        number of valid indices (it will always have `0 < n <= step`). It will\n+        almost always be `step`, but at the boundary we may not have full `step`\n+        elements and `n` can be a lesser value.\n+\n+        E.g., if `step == 4` and `N == 2`, `op` could be\n+\n+          [](int n, scalar1_t &u1, scalar1_t &u2, scalar1_t &u3, scalar1_t &u4,\n+                    scalar2_t &v1, scalar2_t &v2, scalar2_t &v3, scalar2_t &v4) {\n+            // Only process u1, ..., un and v1, ..., vn.\n+            // So if `n == 3`, `u4` and `v4` need not to be considered.\n+          }\n+\n+      In both cases, the references can actually be const, but at least one of\n+      them should be non-const in order to write the output.\n+    - (Optional, but recommended) N TensorArgType args that specify for each\n+      tensor whether `op` reads AND writes ] (i.e., TensorArgType::ReadWrite),\n+      or only reads (i.e., TensorArgType::ReadOnly).\n+      Default is TensorArgType::ReadWrite for first Tensor, and\n+                 TensorArgType::ReadOnly  for the rest.\n+\n+\n+  E.g.,\n+\n+  to compute a = b^2 for a and b of same dtype, we can call\n+\n+  CUDA_tensor_apply2<scalar, scalar>(\n+    a, b,\n+    [] __device__ (scalar &a_val, const scalar &b_val) { a_val = b_val * b_val; }\n+  );\n+\n+  to work on 2 values at the same time, we can call\n+\n+  CUDA_tensor_apply2<scalar1, scalar2, 2>(\n+    a, b,\n+    [] __device__ (int n, scalar1 &a_val1, scalar1 &a_val2,\n+                          const scalar2 &b_val1, const scalar2 &b_val2) {\n+      // call special vectorized op here, or just do elementwise and enjoy unrolling...\n+      // if n == 1, only process a_val1 and b_val1\n+    }\n+  );\n */\n-template <typename scalar1, typename scalar2, typename Op>\n-bool CUDA_tensor_apply2(at::Tensor a,\n-                        at::Tensor b,\n-                        Op op,\n-                        TensorArgType aType = TensorArgType::ReadWrite,\n-                        TensorArgType bType = TensorArgType::ReadOnly) {\n+\n+\n+template <typename scalar, int step, typename Op>\n+inline bool CUDA_tensor_apply1(at::Tensor a,", "path": "aten/src/ATen/cuda/CUDAApplyUtils.cuh", "position": null, "original_position": 586, "commit_id": "e4013d853c2a9a837f6d396f156a4a60eeff8119", "original_commit_id": "9986cd4b590828c858f0575ce6bcbad9664b84a5", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Very good point. I copied the return type from `CUDA_tensor_apply2` without thinking much. I'll update them all.", "created_at": "2018-09-17T01:43:48Z", "updated_at": "2018-11-23T15:51:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/10273#discussion_r217937254", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10273", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217937254"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10273#discussion_r217937254"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10273"}}, "body_html": "<p>Very good point. I copied the return type from <code>CUDA_tensor_apply2</code> without thinking much. I'll update them all.</p>", "body_text": "Very good point. I copied the return type from CUDA_tensor_apply2 without thinking much. I'll update them all.", "in_reply_to_id": 217935769}