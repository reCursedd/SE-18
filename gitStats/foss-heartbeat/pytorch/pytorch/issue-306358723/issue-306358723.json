{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5868", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5868/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5868/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5868/events", "html_url": "https://github.com/pytorch/pytorch/issues/5868", "id": 306358723, "node_id": "MDU6SXNzdWUzMDYzNTg3MjM=", "number": 5868, "title": "[Feature request]: support mpi backend in DistributedDataParallel", "user": {"login": "xhzhao", "id": 17486215, "node_id": "MDQ6VXNlcjE3NDg2MjE1", "avatar_url": "https://avatars1.githubusercontent.com/u/17486215?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xhzhao", "html_url": "https://github.com/xhzhao", "followers_url": "https://api.github.com/users/xhzhao/followers", "following_url": "https://api.github.com/users/xhzhao/following{/other_user}", "gists_url": "https://api.github.com/users/xhzhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/xhzhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xhzhao/subscriptions", "organizations_url": "https://api.github.com/users/xhzhao/orgs", "repos_url": "https://api.github.com/users/xhzhao/repos", "events_url": "https://api.github.com/users/xhzhao/events{/privacy}", "received_events_url": "https://api.github.com/users/xhzhao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-19T07:36:11Z", "updated_at": "2018-03-19T20:13:37Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>AFAIK, PyTorch <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel\" rel=\"nofollow\">DDP</a> only support nccl and gloo backend, and i think it would be great to support mpi backend when the user get CPU only, especially for the researcher with supper computer access.</p>\n<p>I tried to build a proto in this <a href=\"https://github.com/xhzhao/PyTorch-MPI-DDP-example\">github</a>, and my example passed the accuracy validation on MNIST+LeNet and AN4+DeepSpeech2. Besides, this <a href=\"https://github.com/xhzhao/PyTorch-MPI-DDP-example/blob/master/data/distributed.py\">distributed.py</a> has exactly the same usage as the PyTorch DDP, so i'm here to check if i should propose a PR to merge the code to the PyTorch <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py\">distributed.py</a>. If yes, the PyTorch DDP will support both CPU and GPU with exactly the same interface. There maybe a bug in model saving, and i will get it fixed in the following days.</p>\n<p>BTW, we found the async SGD is also very popular in multi-node training, and we have a plan to enable async SGD( say <a href=\"https://arxiv.org/abs/1412.6651\" rel=\"nofollow\">elastic SGD</a>) on CPU side .</p>", "body_text": "AFAIK, PyTorch DDP only support nccl and gloo backend, and i think it would be great to support mpi backend when the user get CPU only, especially for the researcher with supper computer access.\nI tried to build a proto in this github, and my example passed the accuracy validation on MNIST+LeNet and AN4+DeepSpeech2. Besides, this distributed.py has exactly the same usage as the PyTorch DDP, so i'm here to check if i should propose a PR to merge the code to the PyTorch distributed.py. If yes, the PyTorch DDP will support both CPU and GPU with exactly the same interface. There maybe a bug in model saving, and i will get it fixed in the following days.\nBTW, we found the async SGD is also very popular in multi-node training, and we have a plan to enable async SGD( say elastic SGD) on CPU side .", "body": "AFAIK, PyTorch [DDP](http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel) only support nccl and gloo backend, and i think it would be great to support mpi backend when the user get CPU only, especially for the researcher with supper computer access.\r\n\r\nI tried to build a proto in this [github](https://github.com/xhzhao/PyTorch-MPI-DDP-example), and my example passed the accuracy validation on MNIST+LeNet and AN4+DeepSpeech2. Besides, this [distributed.py](https://github.com/xhzhao/PyTorch-MPI-DDP-example/blob/master/data/distributed.py) has exactly the same usage as the PyTorch DDP, so i'm here to check if i should propose a PR to merge the code to the PyTorch [distributed.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py). If yes, the PyTorch DDP will support both CPU and GPU with exactly the same interface. There maybe a bug in model saving, and i will get it fixed in the following days.\r\n\r\nBTW, we found the async SGD is also very popular in multi-node training, and we have a plan to enable async SGD( say [elastic SGD](https://arxiv.org/abs/1412.6651)) on CPU side .\r\n"}