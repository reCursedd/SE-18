{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8815", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8815/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8815/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8815/events", "html_url": "https://github.com/pytorch/pytorch/pull/8815", "id": 335048352, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk2ODY3Nzcy", "number": 8815, "title": "[C++ API]  Rework optimization package", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-22T23:53:40Z", "updated_at": "2018-06-26T17:13:18Z", "closed_at": "2018-06-26T17:13:15Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8815", "html_url": "https://github.com/pytorch/pytorch/pull/8815", "diff_url": "https://github.com/pytorch/pytorch/pull/8815.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8815.patch"}, "body_html": "<p>This PR is a re-work of the final part of autogradpp I had not yet touched: the optimization parts.</p>\n<p>Things I did:</p>\n<ol>\n<li>Split up <code>torch/csrc/api/include/optimizers.h</code> into a folder with the <code>Optimizer</code> base class and the concrete optimizers in separate files,</li>\n<li>Optimizers now take a vector of parameters instead of a model. This is in line with PyTorch.</li>\n<li>Differentiate between optimizers that require a loss closure (e.g. LBFGS) and those that do not (e.g. SGD, Adam, all others)</li>\n<li>Wrote a Python script to run the equivalent PyTorch optimization algorithms on a certain model setup and dump the parameter values every 100 steps to a file. Then wrote the equivalent model setup in C++ and now am comparing the exact parameter values every 100 steps. This is the safest way to ensure the correctness of these algorithms. Happy!!! I'll have to see how flaky this is.</li>\n</ol>\n<p>Question: Should optimizers also have reference semantics like modules? Optimizers will hardly ever be subclassed, and would likely be instantiated at the point of use. Maybe not necessary, but happy to hear thoughts.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3605224\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebetica\">@ebetica</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>", "body_text": "This PR is a re-work of the final part of autogradpp I had not yet touched: the optimization parts.\nThings I did:\n\nSplit up torch/csrc/api/include/optimizers.h into a folder with the Optimizer base class and the concrete optimizers in separate files,\nOptimizers now take a vector of parameters instead of a model. This is in line with PyTorch.\nDifferentiate between optimizers that require a loss closure (e.g. LBFGS) and those that do not (e.g. SGD, Adam, all others)\nWrote a Python script to run the equivalent PyTorch optimization algorithms on a certain model setup and dump the parameter values every 100 steps to a file. Then wrote the equivalent model setup in C++ and now am comparing the exact parameter values every 100 steps. This is the safest way to ensure the correctness of these algorithms. Happy!!! I'll have to see how flaky this is.\n\nQuestion: Should optimizers also have reference semantics like modules? Optimizers will hardly ever be subclassed, and would likely be instantiated at the point of use. Maybe not necessary, but happy to hear thoughts.\n@ebetica @apaszke @ezyang", "body": "This PR is a re-work of the final part of autogradpp I had not yet touched: the optimization parts.\r\n\r\nThings I did:\r\n\r\n1. Split up `torch/csrc/api/include/optimizers.h` into a folder with the `Optimizer` base class and the concrete optimizers in separate files,\r\n2. Optimizers now take a vector of parameters instead of a model. This is in line with PyTorch.\r\n3. Differentiate between optimizers that require a loss closure (e.g. LBFGS) and those that do not (e.g. SGD, Adam, all others)\r\n4. Wrote a Python script to run the equivalent PyTorch optimization algorithms on a certain model setup and dump the parameter values every 100 steps to a file. Then wrote the equivalent model setup in C++ and now am comparing the exact parameter values every 100 steps. This is the safest way to ensure the correctness of these algorithms. Happy!!! I'll have to see how flaky this is.\r\n\r\nQuestion: Should optimizers also have reference semantics like modules? Optimizers will hardly ever be subclassed, and would likely be instantiated at the point of use. Maybe not necessary, but happy to hear thoughts.\r\n\r\n@ebetica @apaszke @ezyang "}