{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9178", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9178/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9178/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9178/events", "html_url": "https://github.com/pytorch/pytorch/issues/9178", "id": 338437951, "node_id": "MDU6SXNzdWUzMzg0Mzc5NTE=", "number": 9178, "title": " subfunction can not use new parameters cover old parameters\uff01\uff01\uff01", "user": {"login": "SongFGH", "id": 28194863, "node_id": "MDQ6VXNlcjI4MTk0ODYz", "avatar_url": "https://avatars3.githubusercontent.com/u/28194863?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SongFGH", "html_url": "https://github.com/SongFGH", "followers_url": "https://api.github.com/users/SongFGH/followers", "following_url": "https://api.github.com/users/SongFGH/following{/other_user}", "gists_url": "https://api.github.com/users/SongFGH/gists{/gist_id}", "starred_url": "https://api.github.com/users/SongFGH/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SongFGH/subscriptions", "organizations_url": "https://api.github.com/users/SongFGH/orgs", "repos_url": "https://api.github.com/users/SongFGH/repos", "events_url": "https://api.github.com/users/SongFGH/events{/privacy}", "received_events_url": "https://api.github.com/users/SongFGH/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-07-05T06:09:45Z", "updated_at": "2018-07-06T01:30:14Z", "closed_at": "2018-07-05T14:32:24Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>When we use a subfunction in a model, however this subfunction is not included in a model, if we repeatedly delivery parameters into this subfunction, this subfunction can not use new parameters cover old parameters. Maybe this is a bug</p>\n<h2>Code example</h2>\n<pre><code>  #subfunction\n def obtain_idx_of_tree(tree, idx_tree = [], idx_idy = [], y = 0):\n     if tree.num_children == 0:\n        tree.idy = y\n        idx_idy.append(tree.idy) #\u8282\u70b9\u5728\u6811\u4e2d\u7684\u6307\u6807\n        idx_tree.append(tree.idx) #\u8282\u70b9\u5728\u6570\u636e\u96c6\u4e2d\u7684\u6307\u6807\n        return idx_tree, idx_idy, y        \n    else:\n        for i in range(tree.num_children):\n            temp_tree, temp_idy, y = obtain_idx_of_tree(tree.children[i], idx_tree, idx_idy, y)\n            idx_tree + temp_tree\n            idx_idy + temp_idy\n            y = y + 1\n        tree.idy = y\n        idx_idy.append(tree.idy)\n        idx_tree.append(tree.idx)\n        return idx_tree, idx_idy, y\n          \nclass SentimentTrainer(object):\n    def __init__(self, args, model, criterion, optimizer, embedding_model = None):\n        super(SentimentTrainer, self).__init__()\n        self.args       = args\n        self.model      = model\n        if embedding_model:\n            self.feature_embedding = embedding_model[0]\n            self.label_embedding = embedding_model[1]\n            self.stru_embedding = embedding_model[2]\n        self.criterion  = criterion\n        self.optimizer  = optimizer\n        self.epoch      = 0\n\n    # helper function for training\n    def train(self, dataset, alltrees):\n        self.model.train()\n        self.optimizer.zero_grad()\n        loss, k = 0.0, 0\n        accuracies = torch.zeros(len(dataset[0]))\n        pred_label = []\n        true_label = []\n        for idx in tqdm(range(len(dataset[0])),desc='Training epoch '+str(self.epoch+1)+''):\n            double_tree = dataset[0][idx]\n            label = dataset[1][idx]\n            tree0 = alltrees[double_tree[0]]\n            idx_tree0, idx_idy0, y0 = obtain_idx_of_tree(tree0)\n</code></pre>\n<h2>System Info</h2>\n<p>Windows,<br>\nAnaconda 3.6<br>\nPytorch 0.4.0<br>\ncuda 0.9</p>\n<h2></h2>\n<p>when we delivery parameters into obtain_idx_of_tree(()), this function will use old parameters but not the new parameters</p>", "body_text": "Issue description\nWhen we use a subfunction in a model, however this subfunction is not included in a model, if we repeatedly delivery parameters into this subfunction, this subfunction can not use new parameters cover old parameters. Maybe this is a bug\nCode example\n  #subfunction\n def obtain_idx_of_tree(tree, idx_tree = [], idx_idy = [], y = 0):\n     if tree.num_children == 0:\n        tree.idy = y\n        idx_idy.append(tree.idy) #\u8282\u70b9\u5728\u6811\u4e2d\u7684\u6307\u6807\n        idx_tree.append(tree.idx) #\u8282\u70b9\u5728\u6570\u636e\u96c6\u4e2d\u7684\u6307\u6807\n        return idx_tree, idx_idy, y        \n    else:\n        for i in range(tree.num_children):\n            temp_tree, temp_idy, y = obtain_idx_of_tree(tree.children[i], idx_tree, idx_idy, y)\n            idx_tree + temp_tree\n            idx_idy + temp_idy\n            y = y + 1\n        tree.idy = y\n        idx_idy.append(tree.idy)\n        idx_tree.append(tree.idx)\n        return idx_tree, idx_idy, y\n          \nclass SentimentTrainer(object):\n    def __init__(self, args, model, criterion, optimizer, embedding_model = None):\n        super(SentimentTrainer, self).__init__()\n        self.args       = args\n        self.model      = model\n        if embedding_model:\n            self.feature_embedding = embedding_model[0]\n            self.label_embedding = embedding_model[1]\n            self.stru_embedding = embedding_model[2]\n        self.criterion  = criterion\n        self.optimizer  = optimizer\n        self.epoch      = 0\n\n    # helper function for training\n    def train(self, dataset, alltrees):\n        self.model.train()\n        self.optimizer.zero_grad()\n        loss, k = 0.0, 0\n        accuracies = torch.zeros(len(dataset[0]))\n        pred_label = []\n        true_label = []\n        for idx in tqdm(range(len(dataset[0])),desc='Training epoch '+str(self.epoch+1)+''):\n            double_tree = dataset[0][idx]\n            label = dataset[1][idx]\n            tree0 = alltrees[double_tree[0]]\n            idx_tree0, idx_idy0, y0 = obtain_idx_of_tree(tree0)\n\nSystem Info\nWindows,\nAnaconda 3.6\nPytorch 0.4.0\ncuda 0.9\n\nwhen we delivery parameters into obtain_idx_of_tree(()), this function will use old parameters but not the new parameters", "body": "## Issue description\r\nWhen we use a subfunction in a model, however this subfunction is not included in a model, if we repeatedly delivery parameters into this subfunction, this subfunction can not use new parameters cover old parameters. Maybe this is a bug\r\n\r\n## Code example\r\n```\r\n  #subfunction\r\n def obtain_idx_of_tree(tree, idx_tree = [], idx_idy = [], y = 0):\r\n     if tree.num_children == 0:\r\n        tree.idy = y\r\n        idx_idy.append(tree.idy) #\u8282\u70b9\u5728\u6811\u4e2d\u7684\u6307\u6807\r\n        idx_tree.append(tree.idx) #\u8282\u70b9\u5728\u6570\u636e\u96c6\u4e2d\u7684\u6307\u6807\r\n        return idx_tree, idx_idy, y        \r\n    else:\r\n        for i in range(tree.num_children):\r\n            temp_tree, temp_idy, y = obtain_idx_of_tree(tree.children[i], idx_tree, idx_idy, y)\r\n            idx_tree + temp_tree\r\n            idx_idy + temp_idy\r\n            y = y + 1\r\n        tree.idy = y\r\n        idx_idy.append(tree.idy)\r\n        idx_tree.append(tree.idx)\r\n        return idx_tree, idx_idy, y\r\n          \r\nclass SentimentTrainer(object):\r\n    def __init__(self, args, model, criterion, optimizer, embedding_model = None):\r\n        super(SentimentTrainer, self).__init__()\r\n        self.args       = args\r\n        self.model      = model\r\n        if embedding_model:\r\n            self.feature_embedding = embedding_model[0]\r\n            self.label_embedding = embedding_model[1]\r\n            self.stru_embedding = embedding_model[2]\r\n        self.criterion  = criterion\r\n        self.optimizer  = optimizer\r\n        self.epoch      = 0\r\n\r\n    # helper function for training\r\n    def train(self, dataset, alltrees):\r\n        self.model.train()\r\n        self.optimizer.zero_grad()\r\n        loss, k = 0.0, 0\r\n        accuracies = torch.zeros(len(dataset[0]))\r\n        pred_label = []\r\n        true_label = []\r\n        for idx in tqdm(range(len(dataset[0])),desc='Training epoch '+str(self.epoch+1)+''):\r\n            double_tree = dataset[0][idx]\r\n            label = dataset[1][idx]\r\n            tree0 = alltrees[double_tree[0]]\r\n            idx_tree0, idx_idy0, y0 = obtain_idx_of_tree(tree0)\r\n```\r\n\r\n## System Info\r\nWindows, \r\nAnaconda 3.6\r\nPytorch 0.4.0\r\ncuda 0.9\r\n\r\n##\r\nwhen we delivery parameters into obtain_idx_of_tree(()), this function will use old parameters but not the new parameters\r\n"}