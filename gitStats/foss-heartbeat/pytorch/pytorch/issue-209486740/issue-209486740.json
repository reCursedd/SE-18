{"url": "https://api.github.com/repos/pytorch/pytorch/issues/825", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/825/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/825/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/825/events", "html_url": "https://github.com/pytorch/pytorch/issues/825", "id": 209486740, "node_id": "MDU6SXNzdWUyMDk0ODY3NDA=", "number": 825, "title": "Easy way of creating your own custom cuda kernels", "user": {"login": "bordingj", "id": 7777521, "node_id": "MDQ6VXNlcjc3Nzc1MjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/7777521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bordingj", "html_url": "https://github.com/bordingj", "followers_url": "https://api.github.com/users/bordingj/followers", "following_url": "https://api.github.com/users/bordingj/following{/other_user}", "gists_url": "https://api.github.com/users/bordingj/gists{/gist_id}", "starred_url": "https://api.github.com/users/bordingj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bordingj/subscriptions", "organizations_url": "https://api.github.com/users/bordingj/orgs", "repos_url": "https://api.github.com/users/bordingj/repos", "events_url": "https://api.github.com/users/bordingj/events{/privacy}", "received_events_url": "https://api.github.com/users/bordingj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2017-02-22T15:25:48Z", "updated_at": "2018-07-23T17:15:43Z", "closed_at": "2017-02-22T15:42:50Z", "author_association": "NONE", "body_html": "<p>In chainer/cupy, creating your own custom kernel is as easy as:</p>\n<pre><code>@cp.util.memoize(for_each_device=True)\ndef _get_fill_c_next_and_h_kernel():\n    cuda_lib = cp.cuda.compile_with_cache(\"\"\"\n    \n        extern \"C\" __global__ void fill_c_next_and_h(\n                       const float* a, const float* i, const float* f, const float* o,\n                       const float* c_prev, float* c_next, float* h, \n                       int num_batches,\n                       int num_samples,\n                       int hidden_size) \n        {\n            int batch_idx  = threadIdx.x + blockIdx.x * blockDim.x;\n            int sample_idx = threadIdx.y + blockIdx.y * blockDim.y;\n            int hidden_idx = threadIdx.z + blockIdx.z * blockDim.z;\n\n            if (batch_idx &lt; num_batches &amp;&amp; sample_idx &lt; num_samples &amp;&amp; hidden_idx &lt; hidden_size){\n                \n                int idx = (batch_idx*num_samples*hidden_size\n                           + sample_idx*hidden_size\n                           + hidden_idx);\n                \n                c_next[idx] = i[idx]*a[idx]+c_prev[idx]*f[idx];\n                h[idx]      = o[idx]*tanhf(c_next[idx]);\n            }\n        }\n       \"\"\")\n    return cuda_lib.get_function('fill_c_next_and_h')\n</code></pre>\n<p>And then calling it with:</p>\n<pre><code>def get_memory_cell_and_output_batched(a, i, f, o, c_prev):\n        \n    num_batches = a.shape[0]\n    num_samples = a.shape[1]\n    hidden_size = a.shape[2]\n    c_next = cp.empty_like(c_prev)\n    h = cp.empty_like(c_prev)\n    \n    fill_c_next_and_h = _get_fill_c_next_and_h_kernel()\n    \n    bdim = (4,16,8)\n    gdim = (int(math.ceil(num_batches/4.0)), \n            int(math.ceil(num_samples/16.0)),\n            int(math.ceil(hidden_size/8.0))\n            )\n    fill_c_next_and_h(grid=gdim, block=bdim,\n                      args = (\n                              a, i, f, o,\n                              c_prev, c_next, h,\n                              num_batches,\n                              num_samples,\n                              hidden_size\n                              )\n                      )\n\n    return c_next, h\n</code></pre>\n<p>It could be cool if something similar was possible in pytorch.</p>", "body_text": "In chainer/cupy, creating your own custom kernel is as easy as:\n@cp.util.memoize(for_each_device=True)\ndef _get_fill_c_next_and_h_kernel():\n    cuda_lib = cp.cuda.compile_with_cache(\"\"\"\n    \n        extern \"C\" __global__ void fill_c_next_and_h(\n                       const float* a, const float* i, const float* f, const float* o,\n                       const float* c_prev, float* c_next, float* h, \n                       int num_batches,\n                       int num_samples,\n                       int hidden_size) \n        {\n            int batch_idx  = threadIdx.x + blockIdx.x * blockDim.x;\n            int sample_idx = threadIdx.y + blockIdx.y * blockDim.y;\n            int hidden_idx = threadIdx.z + blockIdx.z * blockDim.z;\n\n            if (batch_idx < num_batches && sample_idx < num_samples && hidden_idx < hidden_size){\n                \n                int idx = (batch_idx*num_samples*hidden_size\n                           + sample_idx*hidden_size\n                           + hidden_idx);\n                \n                c_next[idx] = i[idx]*a[idx]+c_prev[idx]*f[idx];\n                h[idx]      = o[idx]*tanhf(c_next[idx]);\n            }\n        }\n       \"\"\")\n    return cuda_lib.get_function('fill_c_next_and_h')\n\nAnd then calling it with:\ndef get_memory_cell_and_output_batched(a, i, f, o, c_prev):\n        \n    num_batches = a.shape[0]\n    num_samples = a.shape[1]\n    hidden_size = a.shape[2]\n    c_next = cp.empty_like(c_prev)\n    h = cp.empty_like(c_prev)\n    \n    fill_c_next_and_h = _get_fill_c_next_and_h_kernel()\n    \n    bdim = (4,16,8)\n    gdim = (int(math.ceil(num_batches/4.0)), \n            int(math.ceil(num_samples/16.0)),\n            int(math.ceil(hidden_size/8.0))\n            )\n    fill_c_next_and_h(grid=gdim, block=bdim,\n                      args = (\n                              a, i, f, o,\n                              c_prev, c_next, h,\n                              num_batches,\n                              num_samples,\n                              hidden_size\n                              )\n                      )\n\n    return c_next, h\n\nIt could be cool if something similar was possible in pytorch.", "body": "In chainer/cupy, creating your own custom kernel is as easy as:\r\n\r\n```\r\n@cp.util.memoize(for_each_device=True)\r\ndef _get_fill_c_next_and_h_kernel():\r\n    cuda_lib = cp.cuda.compile_with_cache(\"\"\"\r\n    \r\n        extern \"C\" __global__ void fill_c_next_and_h(\r\n                       const float* a, const float* i, const float* f, const float* o,\r\n                       const float* c_prev, float* c_next, float* h, \r\n                       int num_batches,\r\n                       int num_samples,\r\n                       int hidden_size) \r\n        {\r\n            int batch_idx  = threadIdx.x + blockIdx.x * blockDim.x;\r\n            int sample_idx = threadIdx.y + blockIdx.y * blockDim.y;\r\n            int hidden_idx = threadIdx.z + blockIdx.z * blockDim.z;\r\n\r\n            if (batch_idx < num_batches && sample_idx < num_samples && hidden_idx < hidden_size){\r\n                \r\n                int idx = (batch_idx*num_samples*hidden_size\r\n                           + sample_idx*hidden_size\r\n                           + hidden_idx);\r\n                \r\n                c_next[idx] = i[idx]*a[idx]+c_prev[idx]*f[idx];\r\n                h[idx]      = o[idx]*tanhf(c_next[idx]);\r\n            }\r\n        }\r\n       \"\"\")\r\n    return cuda_lib.get_function('fill_c_next_and_h')\r\n```\r\nAnd then calling it with:\r\n\r\n```\r\ndef get_memory_cell_and_output_batched(a, i, f, o, c_prev):\r\n        \r\n    num_batches = a.shape[0]\r\n    num_samples = a.shape[1]\r\n    hidden_size = a.shape[2]\r\n    c_next = cp.empty_like(c_prev)\r\n    h = cp.empty_like(c_prev)\r\n    \r\n    fill_c_next_and_h = _get_fill_c_next_and_h_kernel()\r\n    \r\n    bdim = (4,16,8)\r\n    gdim = (int(math.ceil(num_batches/4.0)), \r\n            int(math.ceil(num_samples/16.0)),\r\n            int(math.ceil(hidden_size/8.0))\r\n            )\r\n    fill_c_next_and_h(grid=gdim, block=bdim,\r\n                      args = (\r\n                              a, i, f, o,\r\n                              c_prev, c_next, h,\r\n                              num_batches,\r\n                              num_samples,\r\n                              hidden_size\r\n                              )\r\n                      )\r\n\r\n    return c_next, h\r\n```\r\n\r\nIt could be cool if something similar was possible in pytorch.\r\n\r\n"}