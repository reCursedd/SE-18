{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6742", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6742/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6742/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6742/events", "html_url": "https://github.com/pytorch/pytorch/issues/6742", "id": 315708571, "node_id": "MDU6SXNzdWUzMTU3MDg1NzE=", "number": 6742, "title": "incorrect doc for torch.nn.functional.relu", "user": {"login": "JasonQSY", "id": 15360639, "node_id": "MDQ6VXNlcjE1MzYwNjM5", "avatar_url": "https://avatars1.githubusercontent.com/u/15360639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JasonQSY", "html_url": "https://github.com/JasonQSY", "followers_url": "https://api.github.com/users/JasonQSY/followers", "following_url": "https://api.github.com/users/JasonQSY/following{/other_user}", "gists_url": "https://api.github.com/users/JasonQSY/gists{/gist_id}", "starred_url": "https://api.github.com/users/JasonQSY/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JasonQSY/subscriptions", "organizations_url": "https://api.github.com/users/JasonQSY/orgs", "repos_url": "https://api.github.com/users/JasonQSY/repos", "events_url": "https://api.github.com/users/JasonQSY/events{/privacy}", "received_events_url": "https://api.github.com/users/JasonQSY/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-19T02:47:15Z", "updated_at": "2018-04-19T09:26:14Z", "closed_at": "2018-04-19T09:26:14Z", "author_association": "NONE", "body_html": "<p>In line 613-621 of <code>torch/nn/functional.py</code>, it states</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">relu</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-s\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"\"\"</span>relu(input, threshold, value, inplace=False) -&gt; Tensor</span>\n<span class=\"pl-s\">    Applies the rectified linear unit function element-wise. See</span>\n<span class=\"pl-s\">    :class:`~torch.nn.ReLU` for more details.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">if</span> inplace:\n        <span class=\"pl-k\">return</span> torch.relu_(<span class=\"pl-c1\">input</span>)\n    <span class=\"pl-k\">return</span> torch.relu(<span class=\"pl-c1\">input</span>)</pre></div>\n<p>It looks like the doc is incorrect. I think it should something like</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">relu</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-s\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"\"\"</span>relu(input, inplace=False) -&gt; Tensor</span>\n<span class=\"pl-s\">    Applies the rectified linear unit function element-wise. See</span>\n<span class=\"pl-s\">    :class:`~torch.nn.ReLU` for more details.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">if</span> inplace:\n        <span class=\"pl-k\">return</span> torch.relu_(<span class=\"pl-c1\">input</span>)\n    <span class=\"pl-k\">return</span> torch.relu(<span class=\"pl-c1\">input</span>)</pre></div>\n<p>which causes the docs on the website to be incorrect.</p>", "body_text": "In line 613-621 of torch/nn/functional.py, it states\ndef relu(input, inplace=False):\n    r\"\"\"relu(input, threshold, value, inplace=False) -> Tensor\n    Applies the rectified linear unit function element-wise. See\n    :class:`~torch.nn.ReLU` for more details.\n    \"\"\"\n    if inplace:\n        return torch.relu_(input)\n    return torch.relu(input)\nIt looks like the doc is incorrect. I think it should something like\ndef relu(input, inplace=False):\n    r\"\"\"relu(input, inplace=False) -> Tensor\n    Applies the rectified linear unit function element-wise. See\n    :class:`~torch.nn.ReLU` for more details.\n    \"\"\"\n    if inplace:\n        return torch.relu_(input)\n    return torch.relu(input)\nwhich causes the docs on the website to be incorrect.", "body": "In line 613-621 of `torch/nn/functional.py`, it states\r\n\r\n```python\r\ndef relu(input, inplace=False):\r\n    r\"\"\"relu(input, threshold, value, inplace=False) -> Tensor\r\n    Applies the rectified linear unit function element-wise. See\r\n    :class:`~torch.nn.ReLU` for more details.\r\n    \"\"\"\r\n    if inplace:\r\n        return torch.relu_(input)\r\n    return torch.relu(input)\r\n```\r\n\r\nIt looks like the doc is incorrect. I think it should something like\r\n```python\r\ndef relu(input, inplace=False):\r\n    r\"\"\"relu(input, inplace=False) -> Tensor\r\n    Applies the rectified linear unit function element-wise. See\r\n    :class:`~torch.nn.ReLU` for more details.\r\n    \"\"\"\r\n    if inplace:\r\n        return torch.relu_(input)\r\n    return torch.relu(input)\r\n```\r\n\r\nwhich causes the docs on the website to be incorrect.\r\n"}