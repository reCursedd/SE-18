{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436632186", "html_url": "https://github.com/pytorch/pytorch/issues/13246#issuecomment-436632186", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13246", "id": 436632186, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjYzMjE4Ng==", "user": {"login": "bfreskura", "id": 9371112, "node_id": "MDQ6VXNlcjkzNzExMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/9371112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bfreskura", "html_url": "https://github.com/bfreskura", "followers_url": "https://api.github.com/users/bfreskura/followers", "following_url": "https://api.github.com/users/bfreskura/following{/other_user}", "gists_url": "https://api.github.com/users/bfreskura/gists{/gist_id}", "starred_url": "https://api.github.com/users/bfreskura/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bfreskura/subscriptions", "organizations_url": "https://api.github.com/users/bfreskura/orgs", "repos_url": "https://api.github.com/users/bfreskura/repos", "events_url": "https://api.github.com/users/bfreskura/events{/privacy}", "received_events_url": "https://api.github.com/users/bfreskura/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T14:01:51Z", "updated_at": "2018-11-07T21:05:04Z", "author_association": "NONE", "body_html": "<p>After some more investigation, I have found an exact scenario when the leak occurs. Consider the code example below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> Dataset, DataLoader\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DataIter</span>(<span class=\"pl-e\">Dataset</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>.data_np <span class=\"pl-k\">=</span> np.array([x <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">24000000</span>)])\n        <span class=\"pl-c1\">self</span>.data <span class=\"pl-k\">=</span> [x <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">24000000</span>)]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.data)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n        data <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.data[idx]\n        data <span class=\"pl-k\">=</span> np.array([data], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int64)\n        <span class=\"pl-k\">return</span> torch.tensor(data)\n\n\ntrain_data <span class=\"pl-k\">=</span> DataIter()\ntrain_loader <span class=\"pl-k\">=</span> DataLoader(train_data, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">300</span>,\n                          <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                          <span class=\"pl-v\">drop_last</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                          <span class=\"pl-v\">pin_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                          <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">18</span>)\n\n<span class=\"pl-k\">for</span> i, item <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">1000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-c1\">print</span>(i)</pre></div>\n<p>If we use the <code>self.data</code> variable which is a standard Python list of ints, the data leak will occur. However, if the <code>self.data_np</code> variable is used, which holds the same data but in a form of a Numpy array, the leak will not occur.<br>\nAnother observation is that the leakage is significantly less severe if the <code>shuffle=False</code> in the <code>DataLoader</code>.</p>", "body_text": "After some more investigation, I have found an exact scenario when the leak occurs. Consider the code example below:\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport torch\n\n\nclass DataIter(Dataset):\n    def __init__(self):\n        self.data_np = np.array([x for x in range(24000000)])\n        self.data = [x for x in range(24000000)]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data[idx]\n        data = np.array([data], dtype=np.int64)\n        return torch.tensor(data)\n\n\ntrain_data = DataIter()\ntrain_loader = DataLoader(train_data, batch_size=300,\n                          shuffle=True,\n                          drop_last=True,\n                          pin_memory=False,\n                          num_workers=18)\n\nfor i, item in enumerate(train_loader):\n    if i % 1000 == 0:\n        print(i)\nIf we use the self.data variable which is a standard Python list of ints, the data leak will occur. However, if the self.data_np variable is used, which holds the same data but in a form of a Numpy array, the leak will not occur.\nAnother observation is that the leakage is significantly less severe if the shuffle=False in the DataLoader.", "body": "After some more investigation, I have found an exact scenario when the leak occurs. Consider the code example below:\r\n\r\n```python\r\nfrom torch.utils.data import Dataset, DataLoader\r\nimport numpy as np\r\nimport torch\r\n\r\n\r\nclass DataIter(Dataset):\r\n    def __init__(self):\r\n        self.data_np = np.array([x for x in range(24000000)])\r\n        self.data = [x for x in range(24000000)]\r\n\r\n    def __len__(self):\r\n        return len(self.data)\r\n\r\n    def __getitem__(self, idx):\r\n        data = self.data[idx]\r\n        data = np.array([data], dtype=np.int64)\r\n        return torch.tensor(data)\r\n\r\n\r\ntrain_data = DataIter()\r\ntrain_loader = DataLoader(train_data, batch_size=300,\r\n                          shuffle=True,\r\n                          drop_last=True,\r\n                          pin_memory=False,\r\n                          num_workers=18)\r\n\r\nfor i, item in enumerate(train_loader):\r\n    if i % 1000 == 0:\r\n        print(i)\r\n```\r\n\r\nIf we use the `self.data` variable which is a standard Python list of ints, the data leak will occur. However, if the `self.data_np` variable is used, which holds the same data but in a form of a Numpy array, the leak will not occur. \r\nAnother observation is that the leakage is significantly less severe if the `shuffle=False` in the `DataLoader`."}