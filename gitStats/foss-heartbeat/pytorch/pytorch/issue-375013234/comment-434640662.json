{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434640662", "html_url": "https://github.com/pytorch/pytorch/issues/13246#issuecomment-434640662", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13246", "id": 434640662, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDY0MDY2Mg==", "user": {"login": "samgd", "id": 5612252, "node_id": "MDQ6VXNlcjU2MTIyNTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5612252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samgd", "html_url": "https://github.com/samgd", "followers_url": "https://api.github.com/users/samgd/followers", "following_url": "https://api.github.com/users/samgd/following{/other_user}", "gists_url": "https://api.github.com/users/samgd/gists{/gist_id}", "starred_url": "https://api.github.com/users/samgd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samgd/subscriptions", "organizations_url": "https://api.github.com/users/samgd/orgs", "repos_url": "https://api.github.com/users/samgd/repos", "events_url": "https://api.github.com/users/samgd/events{/privacy}", "received_events_url": "https://api.github.com/users/samgd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T10:43:21Z", "updated_at": "2018-10-31T10:44:27Z", "author_association": "NONE", "body_html": "<p>I've been experiencing something similar where memory usage continuously climbs until a OOM is triggered when using a <code>batch_sampler</code> with <code>num_workers&gt;0</code>.</p>\n<h2>To Reproduce</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> math\n\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Sampler</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">n</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100000</span>, <span class=\"pl-smi\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>):\n        <span class=\"pl-c1\">self</span>.n <span class=\"pl-k\">=</span> n\n        <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> batch_size\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> math.ceil(<span class=\"pl-c1\">float</span>(<span class=\"pl-c1\">self</span>.n)<span class=\"pl-k\">/</span><span class=\"pl-c1\">self</span>.batch_size)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__iter__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        batch <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">self</span>.n):\n            batch.append(i)\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(batch) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">self</span>.batch_size:\n                <span class=\"pl-k\">yield</span> batch\n                batch <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">if</span> batch:\n            <span class=\"pl-k\">yield</span> batch\n\n            \nN <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100000000</span>\ntrain_data <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(N))\n\n            \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">ok</span>():\n    train_sampler <span class=\"pl-k\">=</span> Sampler(<span class=\"pl-c1\">len</span>(train_data))\n    train_loader <span class=\"pl-k\">=</span> DataLoader(train_data,\n                              <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>,\n                              <span class=\"pl-v\">batch_sampler</span><span class=\"pl-k\">=</span>train_sampler)\n    \n    <span class=\"pl-k\">for</span> i, item <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span>(i)\n            \n            \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">leaky</span>():\n    train_sampler <span class=\"pl-k\">=</span> Sampler(<span class=\"pl-c1\">len</span>(train_data))\n    train_loader <span class=\"pl-k\">=</span> DataLoader(train_data,\n                              <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">8</span>,\n                              <span class=\"pl-v\">batch_sampler</span><span class=\"pl-k\">=</span>train_sampler)\n\n    <span class=\"pl-k\">for</span> i, item <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span>(i)\n            \n            \n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Starting ok<span class=\"pl-pds\">'</span></span>)\nok()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>ok done, starting leaky()<span class=\"pl-pds\">'</span></span>)\nleaky()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>leaky done<span class=\"pl-pds\">'</span></span>)</pre></div>\n<h2>Environment</h2>\n<pre><code>$ python3 collect_env.py\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\nNvidia driver version: 390.77\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n</code></pre>", "body_text": "I've been experiencing something similar where memory usage continuously climbs until a OOM is triggered when using a batch_sampler with num_workers>0.\nTo Reproduce\nimport math\n\nfrom torch.utils.data import DataLoader\n\n\nclass Sampler:\n    def __init__(self, n=100000, batch_size=32):\n        self.n = n\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return math.ceil(float(self.n)/self.batch_size)\n\n    def __iter__(self):\n        batch = []\n        for i in range(self.n):\n            batch.append(i)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n\n            \nN = 100000000\ntrain_data = list(range(N))\n\n            \ndef ok():\n    train_sampler = Sampler(len(train_data))\n    train_loader = DataLoader(train_data,\n                              num_workers=0,\n                              batch_sampler=train_sampler)\n    \n    for i, item in enumerate(train_loader):\n        if i % 10000 == 0:\n            print(i)\n            \n            \ndef leaky():\n    train_sampler = Sampler(len(train_data))\n    train_loader = DataLoader(train_data,\n                              num_workers=8,\n                              batch_sampler=train_sampler)\n\n    for i, item in enumerate(train_loader):\n        if i % 10000 == 0:\n            print(i)\n            \n            \nprint('Starting ok')\nok()\nprint('ok done, starting leaky()')\nleaky()\nprint('leaky done')\nEnvironment\n$ python3 collect_env.py\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\nNvidia driver version: 390.77\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect", "body": "I've been experiencing something similar where memory usage continuously climbs until a OOM is triggered when using a `batch_sampler` with `num_workers>0`.  \r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport math\r\n\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\nclass Sampler:\r\n    def __init__(self, n=100000, batch_size=32):\r\n        self.n = n\r\n        self.batch_size = batch_size\r\n\r\n    def __len__(self):\r\n        return math.ceil(float(self.n)/self.batch_size)\r\n\r\n    def __iter__(self):\r\n        batch = []\r\n        for i in range(self.n):\r\n            batch.append(i)\r\n            if len(batch) == self.batch_size:\r\n                yield batch\r\n                batch = []\r\n        if batch:\r\n            yield batch\r\n\r\n            \r\nN = 100000000\r\ntrain_data = list(range(N))\r\n\r\n            \r\ndef ok():\r\n    train_sampler = Sampler(len(train_data))\r\n    train_loader = DataLoader(train_data,\r\n                              num_workers=0,\r\n                              batch_sampler=train_sampler)\r\n    \r\n    for i, item in enumerate(train_loader):\r\n        if i % 10000 == 0:\r\n            print(i)\r\n            \r\n            \r\ndef leaky():\r\n    train_sampler = Sampler(len(train_data))\r\n    train_loader = DataLoader(train_data,\r\n                              num_workers=8,\r\n                              batch_sampler=train_sampler)\r\n\r\n    for i, item in enumerate(train_loader):\r\n        if i % 10000 == 0:\r\n            print(i)\r\n            \r\n            \r\nprint('Starting ok')\r\nok()\r\nprint('ok done, starting leaky()')\r\nleaky()\r\nprint('leaky done')\r\n```\r\n\r\n## Environment\r\n\r\n```\r\n$ python3 collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 390.77\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```"}