{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13246", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13246/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13246/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13246/events", "html_url": "https://github.com/pytorch/pytorch/issues/13246", "id": 375013234, "node_id": "MDU6SXNzdWUzNzUwMTMyMzQ=", "number": 13246, "title": "CPU memory gradually leaks when num_workers > 0 in the DataLoader", "user": {"login": "bfreskura", "id": 9371112, "node_id": "MDQ6VXNlcjkzNzExMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/9371112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bfreskura", "html_url": "https://github.com/bfreskura", "followers_url": "https://api.github.com/users/bfreskura/followers", "following_url": "https://api.github.com/users/bfreskura/following{/other_user}", "gists_url": "https://api.github.com/users/bfreskura/gists{/gist_id}", "starred_url": "https://api.github.com/users/bfreskura/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bfreskura/subscriptions", "organizations_url": "https://api.github.com/users/bfreskura/orgs", "repos_url": "https://api.github.com/users/bfreskura/repos", "events_url": "https://api.github.com/users/bfreskura/events{/privacy}", "received_events_url": "https://api.github.com/users/bfreskura/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-10-29T13:23:59Z", "updated_at": "2018-11-12T18:39:40Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>CPU memory will leak if the DataLoader <code>num_workers &gt; 0</code>.</p>\n<h2>To Reproduce</h2>\n<p>Run the following snippet:</p>\n<pre><code>from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport os\n\nclass DataIter(Dataset):\n    def __init__(self):\n        path = \"path/to/data\"\n        self.data = []\n\n        for cls in os.listdir(path):\n            for img in os.listdir(os.path.join(path, cls)):\n                self.data.append(os.path.join(path, cls, img))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        with Image.open(self.data[idx]) as img:\n            img = img.convert('RGB')\n            return transforms.functional.to_tensor(img)\n\n\ntrain_data = DataIter()\ntrain_loader = DataLoader(train_data, batch_size=300,\n                          shuffle=True,\n                          drop_last=True,\n                          pin_memory=False,\n                          num_workers=18)\n\nfor i, item in enumerate(train_loader):\n    if i % 200 == 0:\n        print(i)\n</code></pre>\n<h2>Expected behavior</h2>\n<p>CPU memory will gradually start increasing, eventually filling up the whole RAM. E.g., the process starts with around 15GB and fills up the whole 128GB available on the system.<br>\nWhen the <code>num_workers=0</code>, RAM usage is constant.</p>\n<h2>Environment</h2>\n<pre><code>PyTorch version: 1.0.0.dev20181028\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\n\nNvidia driver version: 390.67\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n\nPIL.__version__\n'5.3.0'\n</code></pre>\n<h2>Additional info</h2>\n<p>There are around 24 million images in the dataset and all image paths are loaded into a single list as presented in the above code snippet.</p>\n<p>I have also tried multiple Pytorch (0.4.0 and 0.4.1) versions and the effect is the same.</p>", "body_text": "\ud83d\udc1b Bug\nCPU memory will leak if the DataLoader num_workers > 0.\nTo Reproduce\nRun the following snippet:\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport os\n\nclass DataIter(Dataset):\n    def __init__(self):\n        path = \"path/to/data\"\n        self.data = []\n\n        for cls in os.listdir(path):\n            for img in os.listdir(os.path.join(path, cls)):\n                self.data.append(os.path.join(path, cls, img))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        with Image.open(self.data[idx]) as img:\n            img = img.convert('RGB')\n            return transforms.functional.to_tensor(img)\n\n\ntrain_data = DataIter()\ntrain_loader = DataLoader(train_data, batch_size=300,\n                          shuffle=True,\n                          drop_last=True,\n                          pin_memory=False,\n                          num_workers=18)\n\nfor i, item in enumerate(train_loader):\n    if i % 200 == 0:\n        print(i)\n\nExpected behavior\nCPU memory will gradually start increasing, eventually filling up the whole RAM. E.g., the process starts with around 15GB and fills up the whole 128GB available on the system.\nWhen the num_workers=0, RAM usage is constant.\nEnvironment\nPyTorch version: 1.0.0.dev20181028\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: \nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\n\nNvidia driver version: 390.67\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n\nPIL.__version__\n'5.3.0'\n\nAdditional info\nThere are around 24 million images in the dataset and all image paths are loaded into a single list as presented in the above code snippet.\nI have also tried multiple Pytorch (0.4.0 and 0.4.1) versions and the effect is the same.", "body": "## \ud83d\udc1b Bug\r\n\r\nCPU memory will leak if the DataLoader `num_workers > 0`.\r\n\r\n## To Reproduce\r\nRun the following snippet:\r\n\r\n```\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom PIL import Image\r\nfrom torchvision import transforms\r\nimport os\r\n\r\nclass DataIter(Dataset):\r\n    def __init__(self):\r\n        path = \"path/to/data\"\r\n        self.data = []\r\n\r\n        for cls in os.listdir(path):\r\n            for img in os.listdir(os.path.join(path, cls)):\r\n                self.data.append(os.path.join(path, cls, img))\r\n\r\n    def __len__(self):\r\n        return len(self.data)\r\n\r\n    def __getitem__(self, idx):\r\n        with Image.open(self.data[idx]) as img:\r\n            img = img.convert('RGB')\r\n            return transforms.functional.to_tensor(img)\r\n\r\n\r\ntrain_data = DataIter()\r\ntrain_loader = DataLoader(train_data, batch_size=300,\r\n                          shuffle=True,\r\n                          drop_last=True,\r\n                          pin_memory=False,\r\n                          num_workers=18)\r\n\r\nfor i, item in enumerate(train_loader):\r\n    if i % 200 == 0:\r\n        print(i)\r\n```\r\n## Expected behavior\r\n\r\nCPU memory will gradually start increasing, eventually filling up the whole RAM. E.g., the process starts with around 15GB and fills up the whole 128GB available on the system. \r\nWhen the `num_workers=0`, RAM usage is constant.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0.dev20181028\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.67\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\nPIL.__version__\r\n'5.3.0'\r\n```\r\n\r\n## Additional info\r\nThere are around 24 million images in the dataset and all image paths are loaded into a single list as presented in the above code snippet.\r\n\r\nI have also tried multiple Pytorch (0.4.0 and 0.4.1) versions and the effect is the same."}