{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3064", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3064/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3064/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3064/events", "html_url": "https://github.com/pytorch/pytorch/issues/3064", "id": 264465398, "node_id": "MDU6SXNzdWUyNjQ0NjUzOTg=", "number": 3064, "title": "double backward for Conv2d failing with CUDNN_STATUS_NOT_SUPPORTED", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-11T05:56:26Z", "updated_at": "2017-10-24T13:51:53Z", "closed_at": "2017-10-24T13:51:53Z", "author_association": "MEMBER", "body_html": "<p>Reported at <a href=\"https://discuss.pytorch.org/t/cudnn-status-not-supported-error-occurs-when-apply-autograd-grad-to-compute-high-order-differentiation/8256\" rel=\"nofollow\">https://discuss.pytorch.org/t/cudnn-status-not-supported-error-occurs-when-apply-autograd-grad-to-compute-high-order-differentiation/8256</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable, grad\n<span class=\"pl-k\">import</span> torch.utils.data <span class=\"pl-k\">as</span> Data\n<span class=\"pl-k\">import</span> torchvision\n\ntrain_data <span class=\"pl-k\">=</span> torchvision.datasets.MNIST(\n    <span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./mnist/<span class=\"pl-pds\">'</span></span>,\n    <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>torchvision.transforms.ToTensor(),\n    <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n)\n\ntrain_loader <span class=\"pl-k\">=</span> Data.DataLoader(\n    <span class=\"pl-v\">dataset</span><span class=\"pl-k\">=</span>train_data, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">50</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CNN</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">CNN</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Sequential(nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>))\n        <span class=\"pl-c1\">self</span>.out <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">16</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">28</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        x <span class=\"pl-k\">=</span> x.view(x.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.out(x)\n        <span class=\"pl-k\">return</span> output, x\n\n\ncnn <span class=\"pl-k\">=</span> CNN()\ncnn.cuda()\n\nloss_func <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()\n\n<span class=\"pl-k\">for</span> step, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(data).cuda()\n    target <span class=\"pl-k\">=</span> Variable(label).cuda()\n\n    output <span class=\"pl-k\">=</span> cnn(<span class=\"pl-c1\">input</span>)[<span class=\"pl-c1\">0</span>]\n    loss <span class=\"pl-k\">=</span> loss_func(output, target)\n\n    params <span class=\"pl-k\">=</span> cnn.parameters()\n    g <span class=\"pl-k\">=</span> grad(loss, params, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    g_sum <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">for</span> g_para <span class=\"pl-k\">in</span> g:\n        g_sum <span class=\"pl-k\">+=</span> g_para.sum()\n\n    params <span class=\"pl-k\">=</span> cnn.parameters()\n    hv <span class=\"pl-k\">=</span> grad(g_sum, params, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    <span class=\"pl-k\">break</span></pre></div>", "body_text": "Reported at https://discuss.pytorch.org/t/cudnn-status-not-supported-error-occurs-when-apply-autograd-grad-to-compute-high-order-differentiation/8256\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable, grad\nimport torch.utils.data as Data\nimport torchvision\n\ntrain_data = torchvision.datasets.MNIST(\n    root='./mnist/',\n    train=True,\n    transform=torchvision.transforms.ToTensor(),\n    download=True,\n)\n\ntrain_loader = Data.DataLoader(\n    dataset=train_data, batch_size=50, shuffle=True, num_workers=2)\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(1, 16, 5, 1, 2))\n        self.out = nn.Linear(16 * 28 * 28, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = x.view(x.size(0), -1)\n        output = self.out(x)\n        return output, x\n\n\ncnn = CNN()\ncnn.cuda()\n\nloss_func = nn.CrossEntropyLoss()\n\nfor step, (data, label) in enumerate(train_loader):\n    input = Variable(data).cuda()\n    target = Variable(label).cuda()\n\n    output = cnn(input)[0]\n    loss = loss_func(output, target)\n\n    params = cnn.parameters()\n    g = grad(loss, params, create_graph=True)\n\n    g_sum = 0\n    for g_para in g:\n        g_sum += g_para.sum()\n\n    params = cnn.parameters()\n    hv = grad(g_sum, params, create_graph=True)\n\n    break", "body": "Reported at https://discuss.pytorch.org/t/cudnn-status-not-supported-error-occurs-when-apply-autograd-grad-to-compute-high-order-differentiation/8256\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable, grad\r\nimport torch.utils.data as Data\r\nimport torchvision\r\n\r\ntrain_data = torchvision.datasets.MNIST(\r\n    root='./mnist/',\r\n    train=True,\r\n    transform=torchvision.transforms.ToTensor(),\r\n    download=True,\r\n)\r\n\r\ntrain_loader = Data.DataLoader(\r\n    dataset=train_data, batch_size=50, shuffle=True, num_workers=2)\r\n\r\n\r\nclass CNN(nn.Module):\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        self.conv1 = nn.Sequential(nn.Conv2d(1, 16, 5, 1, 2))\r\n        self.out = nn.Linear(16 * 28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = x.view(x.size(0), -1)\r\n        output = self.out(x)\r\n        return output, x\r\n\r\n\r\ncnn = CNN()\r\ncnn.cuda()\r\n\r\nloss_func = nn.CrossEntropyLoss()\r\n\r\nfor step, (data, label) in enumerate(train_loader):\r\n    input = Variable(data).cuda()\r\n    target = Variable(label).cuda()\r\n\r\n    output = cnn(input)[0]\r\n    loss = loss_func(output, target)\r\n\r\n    params = cnn.parameters()\r\n    g = grad(loss, params, create_graph=True)\r\n\r\n    g_sum = 0\r\n    for g_para in g:\r\n        g_sum += g_para.sum()\r\n\r\n    params = cnn.parameters()\r\n    hv = grad(g_sum, params, create_graph=True)\r\n\r\n    break\r\n```"}