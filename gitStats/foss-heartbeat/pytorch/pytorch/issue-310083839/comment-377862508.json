{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377862508", "html_url": "https://github.com/pytorch/pytorch/pull/6132#issuecomment-377862508", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6132", "id": 377862508, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Nzg2MjUwOA==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-02T06:03:14Z", "updated_at": "2018-04-02T06:03:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a>  i tested <a href=\"https://github.com/pytorch/examples/tree/perftests/mnist\">https://github.com/pytorch/examples/tree/perftests/mnist</a>  on my machine, Xeon(R) Platinum 8180 CPU @ 2.50GHz, <code>OMP_NUM_THREADS=4 python main.py --epochs 1</code>. It did run slower with mkldnn,</p>\n<ul>\n<li>mnist without mkldnn: 9.63s per epoch</li>\n<li>mnist with mkldnn: 10.99s per epoch</li>\n</ul>\n<p>however, mnist is a very <code>small</code> workload which is unsuitable for performance measurement. <a href=\"https://github.com/mingfeima/convnet-benchmarks/tree/master/pytorch\">Convnet</a> is a better alternative from performance perspective.<br>\nbelow is convnet-alexnet performance, also OMP_NUM_THREADS=4</p>\n<ul>\n<li>convnet-alexnet without mkldnn: 3.22s per iteration</li>\n<li>convnet-alexnet with mkldnn: 1.45s per iteration</li>\n</ul>\n<p>And also the mkldnn integration is just a kickoff, the current performance is roughly 20% of the best number. We will continue work on that.</p>\n<p>i got a few questions,<br>\na) what is the CPU type you are using for benchmarking?<br>\nb) any special reason whey setting OMP_NUM_THREADS to be 4, why not use up all the cores?</p>\n<p>if you want to use the same number of threads for mkl, no need to set MKL_NUM_THREADS, it will follow OMP_NUM_THREADS. MKL_NUM_THREADS is only needed when you want to use different number threads from OMP_NUM_THREADS.</p>\n<p>also KMP_AFFINITY has big impact on CPU performance. if you are testing using only 4 cores, this doesn't really make any difference. but if you use all the cores on CPU, <code>export KMP_AFFINITY=granularity=fine,compact,1,0</code> will give you faster result.</p>", "body_text": "@yf225  i tested https://github.com/pytorch/examples/tree/perftests/mnist  on my machine, Xeon(R) Platinum 8180 CPU @ 2.50GHz, OMP_NUM_THREADS=4 python main.py --epochs 1. It did run slower with mkldnn,\n\nmnist without mkldnn: 9.63s per epoch\nmnist with mkldnn: 10.99s per epoch\n\nhowever, mnist is a very small workload which is unsuitable for performance measurement. Convnet is a better alternative from performance perspective.\nbelow is convnet-alexnet performance, also OMP_NUM_THREADS=4\n\nconvnet-alexnet without mkldnn: 3.22s per iteration\nconvnet-alexnet with mkldnn: 1.45s per iteration\n\nAnd also the mkldnn integration is just a kickoff, the current performance is roughly 20% of the best number. We will continue work on that.\ni got a few questions,\na) what is the CPU type you are using for benchmarking?\nb) any special reason whey setting OMP_NUM_THREADS to be 4, why not use up all the cores?\nif you want to use the same number of threads for mkl, no need to set MKL_NUM_THREADS, it will follow OMP_NUM_THREADS. MKL_NUM_THREADS is only needed when you want to use different number threads from OMP_NUM_THREADS.\nalso KMP_AFFINITY has big impact on CPU performance. if you are testing using only 4 cores, this doesn't really make any difference. but if you use all the cores on CPU, export KMP_AFFINITY=granularity=fine,compact,1,0 will give you faster result.", "body": "@yf225  i tested https://github.com/pytorch/examples/tree/perftests/mnist  on my machine, Xeon(R) Platinum 8180 CPU @ 2.50GHz, `OMP_NUM_THREADS=4 python main.py --epochs 1`. It did run slower with mkldnn,\r\n\r\n- mnist without mkldnn: 9.63s per epoch\r\n- mnist with mkldnn: 10.99s per epoch\r\n\r\nhowever, mnist is a very `small` workload which is unsuitable for performance measurement. [Convnet](https://github.com/mingfeima/convnet-benchmarks/tree/master/pytorch) is a better alternative from performance perspective.\r\nbelow is convnet-alexnet performance, also OMP_NUM_THREADS=4\r\n\r\n- convnet-alexnet without mkldnn: 3.22s per iteration\r\n- convnet-alexnet with mkldnn: 1.45s per iteration\r\n\r\nAnd also the mkldnn integration is just a kickoff, the current performance is roughly 20% of the best number. We will continue work on that.\r\n\r\ni got a few questions,\r\na) what is the CPU type you are using for benchmarking?\r\nb) any special reason whey setting OMP_NUM_THREADS to be 4, why not use up all the cores?\r\n\r\nif you want to use the same number of threads for mkl, no need to set MKL_NUM_THREADS, it will follow OMP_NUM_THREADS. MKL_NUM_THREADS is only needed when you want to use different number threads from OMP_NUM_THREADS.\r\n\r\nalso KMP_AFFINITY has big impact on CPU performance. if you are testing using only 4 cores, this doesn't really make any difference. but if you use all the cores on CPU, `export KMP_AFFINITY=granularity=fine,compact,1,0` will give you faster result."}