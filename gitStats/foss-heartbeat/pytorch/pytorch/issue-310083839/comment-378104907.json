{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/378104907", "html_url": "https://github.com/pytorch/pytorch/pull/6132#issuecomment-378104907", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6132", "id": 378104907, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODEwNDkwNw==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-03T02:08:29Z", "updated_at": "2018-04-03T02:08:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a> ah, looks we have a huge difference in testing cpu performance. :(</p>\n<p>First of all, Intel has quite a long product line, i5/i7 for desktop and Atom (low power low performance) for mobile. In AI, Intel is promoting Xeon (high power high performance) which is mostly used in data centers, CSP(cloud service provider). Inside Intel, <code>CPU</code> refers to <code>Xeon</code> in the context of AI. The latest generation of Xeon is called <code>Skylake</code>, AWS C5 instance is Xeon skylake. And we test on <a href=\"https://ark.intel.com/products/120496/Intel-Xeon-Platinum-8180-Processor-38_5M-Cache-2_50-GHz\" rel=\"nofollow\">Xeon Skylake 8180</a>, which has 56 cores @ 2.5GHz, 512-bit instruction set, providing roughly 9T flops/s, good enough to train large topology such as Inception_v3 on ImageNet.</p>\n<p>Presumably, MKLDNN should provide speedup for both i5/i7 and atom, but actually MKLDNN is designed for Xeon. We highly recommend to include Xeon in CPU performance testing, better skylake.</p>\n<p>As for the second question, it must be guaranteed that NO other process interrupts the performance benchmarking, otherwise the result is useless. This is also true for GPU performance benchmarking. We have a 32-node skylake cluster, managed by <code>slurm</code> which will guaranteed the submitted test job takes the machine exclusively.</p>\n<p>Also use the metric of <code>instruction per second</code> is probably not a good idea, the result is going to be missing leading since modern CPU is using super instruction set (256 bit or 512 bit SIMD) which generates fewer instructions but much faster.</p>", "body_text": "@yf225 ah, looks we have a huge difference in testing cpu performance. :(\nFirst of all, Intel has quite a long product line, i5/i7 for desktop and Atom (low power low performance) for mobile. In AI, Intel is promoting Xeon (high power high performance) which is mostly used in data centers, CSP(cloud service provider). Inside Intel, CPU refers to Xeon in the context of AI. The latest generation of Xeon is called Skylake, AWS C5 instance is Xeon skylake. And we test on Xeon Skylake 8180, which has 56 cores @ 2.5GHz, 512-bit instruction set, providing roughly 9T flops/s, good enough to train large topology such as Inception_v3 on ImageNet.\nPresumably, MKLDNN should provide speedup for both i5/i7 and atom, but actually MKLDNN is designed for Xeon. We highly recommend to include Xeon in CPU performance testing, better skylake.\nAs for the second question, it must be guaranteed that NO other process interrupts the performance benchmarking, otherwise the result is useless. This is also true for GPU performance benchmarking. We have a 32-node skylake cluster, managed by slurm which will guaranteed the submitted test job takes the machine exclusively.\nAlso use the metric of instruction per second is probably not a good idea, the result is going to be missing leading since modern CPU is using super instruction set (256 bit or 512 bit SIMD) which generates fewer instructions but much faster.", "body": "@yf225 ah, looks we have a huge difference in testing cpu performance. :(\r\n\r\nFirst of all, Intel has quite a long product line, i5/i7 for desktop and Atom (low power low performance) for mobile. In AI, Intel is promoting Xeon (high power high performance) which is mostly used in data centers, CSP(cloud service provider). Inside Intel, `CPU` refers to `Xeon` in the context of AI. The latest generation of Xeon is called `Skylake`, AWS C5 instance is Xeon skylake. And we test on [Xeon Skylake 8180](https://ark.intel.com/products/120496/Intel-Xeon-Platinum-8180-Processor-38_5M-Cache-2_50-GHz), which has 56 cores @ 2.5GHz, 512-bit instruction set, providing roughly 9T flops/s, good enough to train large topology such as Inception_v3 on ImageNet.\r\n\r\nPresumably, MKLDNN should provide speedup for both i5/i7 and atom, but actually MKLDNN is designed for Xeon. We highly recommend to include Xeon in CPU performance testing, better skylake.\r\n\r\nAs for the second question, it must be guaranteed that NO other process interrupts the performance benchmarking, otherwise the result is useless. This is also true for GPU performance benchmarking. We have a 32-node skylake cluster, managed by `slurm` which will guaranteed the submitted test job takes the machine exclusively. \r\n\r\nAlso use the metric of `instruction per second` is probably not a good idea, the result is going to be missing leading since modern CPU is using super instruction set (256 bit or 512 bit SIMD) which generates fewer instructions but much faster.\r\n"}