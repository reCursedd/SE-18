{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5613", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5613/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5613/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5613/events", "html_url": "https://github.com/pytorch/pytorch/issues/5613", "id": 303206893, "node_id": "MDU6SXNzdWUzMDMyMDY4OTM=", "number": 5613, "title": "Feature request: named dimensions", "user": {"login": "shekhovt", "id": 2486893, "node_id": "MDQ6VXNlcjI0ODY4OTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2486893?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shekhovt", "html_url": "https://github.com/shekhovt", "followers_url": "https://api.github.com/users/shekhovt/followers", "following_url": "https://api.github.com/users/shekhovt/following{/other_user}", "gists_url": "https://api.github.com/users/shekhovt/gists{/gist_id}", "starred_url": "https://api.github.com/users/shekhovt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shekhovt/subscriptions", "organizations_url": "https://api.github.com/users/shekhovt/orgs", "repos_url": "https://api.github.com/users/shekhovt/repos", "events_url": "https://api.github.com/users/shekhovt/events{/privacy}", "received_events_url": "https://api.github.com/users/shekhovt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-07T18:19:08Z", "updated_at": "2018-03-12T20:16:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Feature:<br>\nMake all Tensors have an optional list of dimension names like ['B','C','H','W']<br>\nThen the broadcasting in binary operations like<br>\na + b<br>\ncan be done according to the names (if they both have named dimensions). For example, if Tensor a has a list ['B','C','H','W'] and b has list ['C'] , then the expected result is efficiently equivalent to<br>\na + b.view([1,-1,1,1])</p>\n<p>This will make the broadcasting safe and very convenient to use.<br>\nCurrently, because broadcasting is implicit, if something goes wrong it is a luck if a dimension mismatch happens somewhen later on or there is an out of memory error thrown.</p>\n<p>I think it could further ease the writing of a generic code that has to work with different number of spatial dimensions (e.g. fully connected layers and conv layers), debug problems, etc. Maybe the named dimensions method be also adopted in functions to allow for the syntax like<br>\nlog_softmax(axis='C')</p>\n<p>Why the usual axis number is not sufficient? The axes are hardwired to the dimensionality. For example, the affine weight in BatchNorm is 1D, and its dimension 0 is by default not matching to the dimension 0 in e.g. the input data. So there going to be lots of these .view() commands and if you expect the input with or without spatial dimensions it is going to be more pain.</p>\n<p>Now consider the code using named dimensions. It will pretty much work if you change the data layout (how it is linearized in the memory). Or if you increase the number of dimensions. For example, you want to extend to a siamese (or 4-stream) network processing, for which it is convenient to use 5D Tensors with dimensions  'instance', 'stream', 'channels','width','height'. Suddenly 'channels' becomes dimension number 2 instead of 1, but if dimensions are matched by names there is no problem.</p>", "body_text": "Feature:\nMake all Tensors have an optional list of dimension names like ['B','C','H','W']\nThen the broadcasting in binary operations like\na + b\ncan be done according to the names (if they both have named dimensions). For example, if Tensor a has a list ['B','C','H','W'] and b has list ['C'] , then the expected result is efficiently equivalent to\na + b.view([1,-1,1,1])\nThis will make the broadcasting safe and very convenient to use.\nCurrently, because broadcasting is implicit, if something goes wrong it is a luck if a dimension mismatch happens somewhen later on or there is an out of memory error thrown.\nI think it could further ease the writing of a generic code that has to work with different number of spatial dimensions (e.g. fully connected layers and conv layers), debug problems, etc. Maybe the named dimensions method be also adopted in functions to allow for the syntax like\nlog_softmax(axis='C')\nWhy the usual axis number is not sufficient? The axes are hardwired to the dimensionality. For example, the affine weight in BatchNorm is 1D, and its dimension 0 is by default not matching to the dimension 0 in e.g. the input data. So there going to be lots of these .view() commands and if you expect the input with or without spatial dimensions it is going to be more pain.\nNow consider the code using named dimensions. It will pretty much work if you change the data layout (how it is linearized in the memory). Or if you increase the number of dimensions. For example, you want to extend to a siamese (or 4-stream) network processing, for which it is convenient to use 5D Tensors with dimensions  'instance', 'stream', 'channels','width','height'. Suddenly 'channels' becomes dimension number 2 instead of 1, but if dimensions are matched by names there is no problem.", "body": "Feature:\r\nMake all Tensors have an optional list of dimension names like ['B','C','H','W']\r\nThen the broadcasting in binary operations like\r\na + b\r\ncan be done according to the names (if they both have named dimensions). For example, if Tensor a has a list ['B','C','H','W'] and b has list ['C'] , then the expected result is efficiently equivalent to\r\na + b.view([1,-1,1,1])\r\n\r\nThis will make the broadcasting safe and very convenient to use.\r\nCurrently, because broadcasting is implicit, if something goes wrong it is a luck if a dimension mismatch happens somewhen later on or there is an out of memory error thrown.\r\n\r\nI think it could further ease the writing of a generic code that has to work with different number of spatial dimensions (e.g. fully connected layers and conv layers), debug problems, etc. Maybe the named dimensions method be also adopted in functions to allow for the syntax like\r\nlog_softmax(axis='C')\r\n\r\nWhy the usual axis number is not sufficient? The axes are hardwired to the dimensionality. For example, the affine weight in BatchNorm is 1D, and its dimension 0 is by default not matching to the dimension 0 in e.g. the input data. So there going to be lots of these .view() commands and if you expect the input with or without spatial dimensions it is going to be more pain.\r\n\r\nNow consider the code using named dimensions. It will pretty much work if you change the data layout (how it is linearized in the memory). Or if you increase the number of dimensions. For example, you want to extend to a siamese (or 4-stream) network processing, for which it is convenient to use 5D Tensors with dimensions  'instance', 'stream', 'channels','width','height'. Suddenly 'channels' becomes dimension number 2 instead of 1, but if dimensions are matched by names there is no problem.\r\n\r\n"}