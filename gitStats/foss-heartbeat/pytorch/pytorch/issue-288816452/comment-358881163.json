{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358881163", "html_url": "https://github.com/pytorch/pytorch/issues/4679#issuecomment-358881163", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4679", "id": 358881163, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODg4MTE2Mw==", "user": {"login": "Zrachel", "id": 4532062, "node_id": "MDQ6VXNlcjQ1MzIwNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4532062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zrachel", "html_url": "https://github.com/Zrachel", "followers_url": "https://api.github.com/users/Zrachel/followers", "following_url": "https://api.github.com/users/Zrachel/following{/other_user}", "gists_url": "https://api.github.com/users/Zrachel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zrachel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zrachel/subscriptions", "organizations_url": "https://api.github.com/users/Zrachel/orgs", "repos_url": "https://api.github.com/users/Zrachel/repos", "events_url": "https://api.github.com/users/Zrachel/events{/privacy}", "received_events_url": "https://api.github.com/users/Zrachel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-19T06:46:25Z", "updated_at": "2018-01-19T06:46:25Z", "author_association": "NONE", "body_html": "<p>With your code, the first process hangs here:</p>\n<pre><code>$ python reduce_sum_tengli.py --rank 0\nbefore init\n</code></pre>\n<p>I've merely added two print lines:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch                                                           \n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> argparse\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Simple point-to-point communication. <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>begin rank<span class=\"pl-pds\">'</span></span>, rank)\n    group <span class=\"pl-k\">=</span> dist.new_group([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n    tensor <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>).cuda(<span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>rank)\n    dist.all_reduce(tensor, <span class=\"pl-v\">op</span><span class=\"pl-k\">=</span>dist.reduce_op.<span class=\"pl-c1\">SUM</span>, <span class=\"pl-v\">group</span><span class=\"pl-k\">=</span>group)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Rank <span class=\"pl-pds\">'</span></span>, rank, <span class=\"pl-s\"><span class=\"pl-pds\">'</span> has data <span class=\"pl-pds\">'</span></span>, tensor[<span class=\"pl-c1\">0</span>])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">init_processes</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">fn</span>, <span class=\"pl-smi\">backend</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>nccl<span class=\"pl-pds\">'</span></span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Initialize the distributed environment. <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>before init<span class=\"pl-pds\">'</span></span>)\n    dist.init_process_group(backend,\n                            <span class=\"pl-v\">rank</span><span class=\"pl-k\">=</span>rank,\n                            <span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span>size,\n                            <span class=\"pl-v\">init_method</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tcp://127.0.0.1:29500<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>after init<span class=\"pl-pds\">'</span></span>)\n    fn(rank, size)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--rank<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>rank<span class=\"pl-pds\">'</span></span>)\n    args <span class=\"pl-k\">=</span> parser.parse_args()\n    init_processes(args.rank, size, run, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>nccl<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "With your code, the first process hangs here:\n$ python reduce_sum_tengli.py --rank 0\nbefore init\n\nI've merely added two print lines:\nimport torch                                                           \nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    print('begin rank', rank)\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1).cuda(device=rank)\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend='nccl'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('before init')\n    dist.init_process_group(backend,\n                            rank=rank,\n                            world_size=size,\n                            init_method=\"tcp://127.0.0.1:29500\")\n    print('after init')\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,\n                        help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')", "body": "With your code, the first process hangs here:\r\n```\r\n$ python reduce_sum_tengli.py --rank 0\r\nbefore init\r\n```\r\n\r\nI've merely added two print lines:\r\n```python\r\nimport torch                                                           \r\nimport torch.distributed as dist\r\nimport argparse\r\n\r\ndef run(rank, size):\r\n    \"\"\" Simple point-to-point communication. \"\"\"\r\n    print('begin rank', rank)\r\n    group = dist.new_group([0, 1])\r\n    tensor = torch.ones(1).cuda(device=rank)\r\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\r\n    print('Rank ', rank, ' has data ', tensor[0])\r\n\r\ndef init_processes(rank, size, fn, backend='nccl'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    print('before init')\r\n    dist.init_process_group(backend,\r\n                            rank=rank,\r\n                            world_size=size,\r\n                            init_method=\"tcp://127.0.0.1:29500\")\r\n    print('after init')\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--rank', default=-1, type=int,\r\n                        help='rank')\r\n    args = parser.parse_args()\r\n    init_processes(args.rank, size, run, 'nccl')\r\n```"}