{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358877853", "html_url": "https://github.com/pytorch/pytorch/issues/4679#issuecomment-358877853", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4679", "id": 358877853, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODg3Nzg1Mw==", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-19T06:22:49Z", "updated_at": "2018-01-19T07:06:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry I missed out one more thing,  <code>tensor = torch.ones(1).cuda()</code> should be modified to <code>tensor = torch.ones(1).cuda(device=rank)</code> since you would like to allreduce from two different GPUs.</p>\n<p>Instead of using multiprocessing, do you mind opening up two python process, one with rank 0 and the other one with rank 1 to see if the problem persists.  This is currently the recommended way of launching distributed training code.</p>\n<p>I modified your code to</p>\n<pre><code>import torch\nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1).cuda(device=rank)\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend='nccl'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    dist.init_process_group(backend,\n                            rank=rank,\n                            world_size=size,\n                            init_method=\"tcp://127.0.0.1:29500\")\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,\n                        help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')\n</code></pre>\n<p>open up two terminals and run</p>\n<pre><code>python ~/test.py --rank 0\npython ~/test.py --rank 1\n</code></pre>\n<p>If this still doesn't work, could you attach GDB to both processes and paste stack traces of both processes here?</p>", "body_text": "Sorry I missed out one more thing,  tensor = torch.ones(1).cuda() should be modified to tensor = torch.ones(1).cuda(device=rank) since you would like to allreduce from two different GPUs.\nInstead of using multiprocessing, do you mind opening up two python process, one with rank 0 and the other one with rank 1 to see if the problem persists.  This is currently the recommended way of launching distributed training code.\nI modified your code to\nimport torch\nimport torch.distributed as dist\nimport argparse\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1).cuda(device=rank)\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend='nccl'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    dist.init_process_group(backend,\n                            rank=rank,\n                            world_size=size,\n                            init_method=\"tcp://127.0.0.1:29500\")\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--rank', default=-1, type=int,\n                        help='rank')\n    args = parser.parse_args()\n    init_processes(args.rank, size, run, 'nccl')\n\nopen up two terminals and run\npython ~/test.py --rank 0\npython ~/test.py --rank 1\n\nIf this still doesn't work, could you attach GDB to both processes and paste stack traces of both processes here?", "body": "Sorry I missed out one more thing,  `tensor = torch.ones(1).cuda()` should be modified to `tensor = torch.ones(1).cuda(device=rank)` since you would like to allreduce from two different GPUs.\r\n\r\nInstead of using multiprocessing, do you mind opening up two python process, one with rank 0 and the other one with rank 1 to see if the problem persists.  This is currently the recommended way of launching distributed training code. \r\n\r\nI modified your code to\r\n\r\n```\r\nimport torch\r\nimport torch.distributed as dist\r\nimport argparse\r\n\r\ndef run(rank, size):\r\n    \"\"\" Simple point-to-point communication. \"\"\"\r\n    group = dist.new_group([0, 1])\r\n    tensor = torch.ones(1).cuda(device=rank)\r\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\r\n    print('Rank ', rank, ' has data ', tensor[0])\r\n\r\ndef init_processes(rank, size, fn, backend='nccl'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    dist.init_process_group(backend,\r\n                            rank=rank,\r\n                            world_size=size,\r\n                            init_method=\"tcp://127.0.0.1:29500\")\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--rank', default=-1, type=int,\r\n                        help='rank')\r\n    args = parser.parse_args()\r\n    init_processes(args.rank, size, run, 'nccl')\r\n```\r\nopen up two terminals and run\r\n```\r\npython ~/test.py --rank 0\r\npython ~/test.py --rank 1\r\n```\r\n\r\nIf this still doesn't work, could you attach GDB to both processes and paste stack traces of both processes here?\r\n"}