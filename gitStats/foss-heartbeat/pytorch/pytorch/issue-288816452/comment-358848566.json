{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358848566", "html_url": "https://github.com/pytorch/pytorch/issues/4679#issuecomment-358848566", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4679", "id": 358848566, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODg0ODU2Ng==", "user": {"login": "Zrachel", "id": 4532062, "node_id": "MDQ6VXNlcjQ1MzIwNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4532062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zrachel", "html_url": "https://github.com/Zrachel", "followers_url": "https://api.github.com/users/Zrachel/followers", "following_url": "https://api.github.com/users/Zrachel/following{/other_user}", "gists_url": "https://api.github.com/users/Zrachel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zrachel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zrachel/subscriptions", "organizations_url": "https://api.github.com/users/Zrachel/orgs", "repos_url": "https://api.github.com/users/Zrachel/repos", "events_url": "https://api.github.com/users/Zrachel/events{/privacy}", "received_events_url": "https://api.github.com/users/Zrachel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-19T02:31:13Z", "updated_at": "2018-01-19T03:09:55Z", "author_association": "NONE", "body_html": "<p>Thank you <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a><br>\nModified <code>reduce_sum.py</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">from</span> torch.multiprocessing <span class=\"pl-k\">import</span> Process\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Simple point-to-point communication. <span class=\"pl-pds\">\"\"\"</span></span>\n    group <span class=\"pl-k\">=</span> dist.new_group([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n    t<span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>).cuda()\n    <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>typeof tensor:<span class=\"pl-c1\">{}</span>, <span class=\"pl-c1\">\\</span></span>\n<span class=\"pl-s\">            typeof dist.reduce_op.SUM:<span class=\"pl-c1\">{}</span>, <span class=\"pl-c1\">\\</span></span>\n<span class=\"pl-s\">            typeof group:<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">type</span>(t), <span class=\"pl-c1\">type</span>(dist.reduce_op.<span class=\"pl-c1\">SUM</span>), <span class=\"pl-c1\">type</span>(group)))\n    dist.all_reduce(t, <span class=\"pl-v\">op</span><span class=\"pl-k\">=</span>dist.reduce_op.<span class=\"pl-c1\">SUM</span>, <span class=\"pl-v\">group</span><span class=\"pl-k\">=</span>group)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Rank <span class=\"pl-pds\">'</span></span>, rank, <span class=\"pl-s\"><span class=\"pl-pds\">'</span> has data <span class=\"pl-pds\">'</span></span>, tensor[<span class=\"pl-c1\">0</span>])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">init_processes</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">fn</span>, <span class=\"pl-smi\">backend</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>tcp<span class=\"pl-pds\">'</span></span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Initialize the distributed environment. <span class=\"pl-pds\">\"\"\"</span></span>\n    os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MASTER_ADDR<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>127.0.0.1<span class=\"pl-pds\">'</span></span>\n    os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MASTER_PORT<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>29500<span class=\"pl-pds\">'</span></span>\n    dist.init_process_group(backend, <span class=\"pl-v\">rank</span><span class=\"pl-k\">=</span>rank, <span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span>size)\n    fn(rank, size)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n    processes <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> rank <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(size):\n        p <span class=\"pl-k\">=</span> Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>init_processes, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(rank, size, run, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>nccl<span class=\"pl-pds\">'</span></span>))           \n        p.start()\n        processes.append(p)\n\n    <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> processes:\n        p.join()                                                                       </pre></div>\n<p>Result still hangs here:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">$</span> python reduce_sum.py \n<span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>zhangruiqing01<span class=\"pl-k\">/</span>tools<span class=\"pl-k\">/</span>anaconda3_torch_distributed<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>distributed<span class=\"pl-k\">/</span><span class=\"pl-c1\">__init__</span>.py:<span class=\"pl-c1\">105</span>: <span class=\"pl-c1\">UserWarning</span>: \n        <span class=\"pl-k\">================================================================================</span>\n                                            <span class=\"pl-c1\">WARNING</span>\n        <span class=\"pl-k\">================================================================================</span>\n        <span class=\"pl-c1\">NCCL</span> backend <span class=\"pl-k\">is</span> still experimental. The APIs will change without\n        notice <span class=\"pl-k\">and</span> we<span class=\"pl-s\"><span class=\"pl-pds\">'</span>re can<span class=\"pl-pds\">'</span></span>t guarantee full correctness <span class=\"pl-k\">and</span> expected performance yet.\n        We<span class=\"pl-s\"><span class=\"pl-pds\">'</span>ll announce it once it<span class=\"pl-pds\">'</span></span>s ready.\n        \n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>)</span>\n<span class=\"pl-s\">/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/site-packages/torch/distributed/__init__.py:105: UserWarning: </span>\n<span class=\"pl-s\">        ================================================================================</span>\n<span class=\"pl-s\">                                            WARNING</span>\n<span class=\"pl-s\">        ================================================================================</span>\n<span class=\"pl-s\">        NCCL backend is still experimental. The APIs will change without</span>\n<span class=\"pl-s\">        notice and we're can't guarantee full correctness and expected performance yet.</span>\n<span class=\"pl-s\">        We'll announce it once it's ready.</span>\n<span class=\"pl-s\">        </span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>)\ntypeof tensor:<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.cuda.FloatTensor<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>,             typeof dist.reduce_op.<span class=\"pl-c1\">SUM</span>:<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>,             typeof group:<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>int<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>\ntypeof tensor:<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.cuda.FloatTensor<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>,             typeof dist.reduce_op.<span class=\"pl-c1\">SUM</span>:<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>,             typeof group:<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>int<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>\n\n<span class=\"pl-k\">^</span>CTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>reduce_sum.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">48</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    p.join()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/process.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">123</span>, <span class=\"pl-k\">in</span> join\n    res <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._popen.wait(timeout)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">55</span>, <span class=\"pl-k\">in</span> wait\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.poll(os.<span class=\"pl-c1\">WNOHANG</span> <span class=\"pl-k\">if</span> timeout <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0.0</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">0</span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">31</span>, <span class=\"pl-k\">in</span> poll\n    pid, sts <span class=\"pl-k\">=</span> os.waitpid(<span class=\"pl-c1\">self</span>.pid, flag)\n<span class=\"pl-c1\">KeyboardInterrupt</span>\n<span class=\"pl-k\">^</span>CError <span class=\"pl-k\">in</span> atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">31</span>, <span class=\"pl-k\">in</span> poll\n    pid, sts <span class=\"pl-k\">=</span> os.waitpid(<span class=\"pl-c1\">self</span>.pid, flag)\n<span class=\"pl-c1\">KeyboardInterrupt</span></pre></div>", "body_text": "Thank you @teng-li\nModified reduce_sum.py:\nimport torch\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\n\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    group = dist.new_group([0, 1])\n    t= torch.ones(1).cuda()\n    print (\"typeof tensor:{}, \\\n            typeof dist.reduce_op.SUM:{}, \\\n            typeof group:{}\".format(type(t), type(dist.reduce_op.SUM), type(group)))\n    dist.all_reduce(t, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n\ndef init_processes(rank, size, fn, backend='tcp'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_processes, args=(rank, size, run, 'nccl'))           \n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()                                                                       \nResult still hangs here:\n$ python reduce_sum.py \n/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/site-packages/torch/distributed/__init__.py:105: UserWarning: \n        ================================================================================\n                                            WARNING\n        ================================================================================\n        NCCL backend is still experimental. The APIs will change without\n        notice and we're can't guarantee full correctness and expected performance yet.\n        We'll announce it once it's ready.\n        \n  \"\"\")\n/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/site-packages/torch/distributed/__init__.py:105: UserWarning: \n        ================================================================================\n                                            WARNING\n        ================================================================================\n        NCCL backend is still experimental. The APIs will change without\n        notice and we're can't guarantee full correctness and expected performance yet.\n        We'll announce it once it's ready.\n        \n  \"\"\")\ntypeof tensor:<class 'torch.cuda.FloatTensor'>,             typeof dist.reduce_op.SUM:<class 'object'>,             typeof group:<class 'int'>\ntypeof tensor:<class 'torch.cuda.FloatTensor'>,             typeof dist.reduce_op.SUM:<class 'object'>,             typeof group:<class 'int'>\n\n^CTraceback (most recent call last):\n  File \"reduce_sum.py\", line 48, in <module>\n    p.join()\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/process.py\", line 123, in join\n    res = self._popen.wait(timeout)\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py\", line 55, in wait\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py\", line 31, in poll\n    pid, sts = os.waitpid(self.pid, flag)\nKeyboardInterrupt\n^CError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py\", line 31, in poll\n    pid, sts = os.waitpid(self.pid, flag)\nKeyboardInterrupt", "body": "Thank you @teng-li \r\nModified `reduce_sum.py`:\r\n```python\r\nimport torch\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nfrom torch.multiprocessing import Process\r\n\r\ndef run(rank, size):\r\n    \"\"\" Simple point-to-point communication. \"\"\"\r\n    group = dist.new_group([0, 1])\r\n    t= torch.ones(1).cuda()\r\n    print (\"typeof tensor:{}, \\\r\n            typeof dist.reduce_op.SUM:{}, \\\r\n            typeof group:{}\".format(type(t), type(dist.reduce_op.SUM), type(group)))\r\n    dist.all_reduce(t, op=dist.reduce_op.SUM, group=group)\r\n    print('Rank ', rank, ' has data ', tensor[0])\r\n\r\ndef init_processes(rank, size, fn, backend='tcp'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_processes, args=(rank, size, run, 'nccl'))           \r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()                                                                       \r\n```\r\n\r\n\r\nResult still hangs here: \r\n```python\r\n$ python reduce_sum.py \r\n/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/site-packages/torch/distributed/__init__.py:105: UserWarning: \r\n        ================================================================================\r\n                                            WARNING\r\n        ================================================================================\r\n        NCCL backend is still experimental. The APIs will change without\r\n        notice and we're can't guarantee full correctness and expected performance yet.\r\n        We'll announce it once it's ready.\r\n        \r\n  \"\"\")\r\n/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/site-packages/torch/distributed/__init__.py:105: UserWarning: \r\n        ================================================================================\r\n                                            WARNING\r\n        ================================================================================\r\n        NCCL backend is still experimental. The APIs will change without\r\n        notice and we're can't guarantee full correctness and expected performance yet.\r\n        We'll announce it once it's ready.\r\n        \r\n  \"\"\")\r\ntypeof tensor:<class 'torch.cuda.FloatTensor'>,             typeof dist.reduce_op.SUM:<class 'object'>,             typeof group:<class 'int'>\r\ntypeof tensor:<class 'torch.cuda.FloatTensor'>,             typeof dist.reduce_op.SUM:<class 'object'>,             typeof group:<class 'int'>\r\n\r\n^CTraceback (most recent call last):\r\n  File \"reduce_sum.py\", line 48, in <module>\r\n    p.join()\r\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/process.py\", line 123, in join\r\n    res = self._popen.wait(timeout)\r\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py\", line 55, in wait\r\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\r\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py\", line 31, in poll\r\n    pid, sts = os.waitpid(self.pid, flag)\r\nKeyboardInterrupt\r\n^CError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/home/zhangruiqing01/tools/anaconda3_torch_distributed/lib/python3.6/multiprocessing/popen_fork.py\", line 31, in poll\r\n    pid, sts = os.waitpid(self.pid, flag)\r\nKeyboardInterrupt\r\n```\r\n"}