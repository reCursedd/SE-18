{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/336189476", "html_url": "https://github.com/pytorch/pytorch/issues/3085#issuecomment-336189476", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3085", "id": 336189476, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjE4OTQ3Ng==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-12T16:21:29Z", "updated_at": "2017-10-12T16:21:29Z", "author_association": "MEMBER", "body_html": "<p>Yes, that's expected. It's a weird interplay of Python and PyTorch semantics. The first case is obviously expected to fail, because advanced indexing always returns a tensor that doesn't share memory with the source.</p>\n<p>The second case is more complicated. These in-place operators are really a syntactic sugar.</p>\n<pre><code>&lt;A expression&gt; -= &lt;B expression&gt;\n</code></pre>\n<p>is actually rewritten as</p>\n<pre><code>&lt;A expression&gt; = &lt;A expression&gt;.__isub__(&lt;B expression&gt;)\n</code></pre>\n<p>The important part is that it's the entire expression so it includes indexing as well. In your case, it will expand to (note that <code>__isub__</code> just calls <code>sub_</code>, so you could replace one with another):</p>\n<div class=\"highlight highlight-source-python\"><pre>x2[idx] <span class=\"pl-k\">=</span> x2[idx].<span class=\"pl-c1\">__isub__</span>(<span class=\"pl-c1\">0.1</span> <span class=\"pl-k\">*</span> y)</pre></div>\n<p>This works fine, because you'll slice <code>x2</code>, get a new tensor with new data, mutate it, and do <code>x2[idx] = new_tensor</code>, which will correctly copy its data back into <code>x2</code>.</p>", "body_text": "Yes, that's expected. It's a weird interplay of Python and PyTorch semantics. The first case is obviously expected to fail, because advanced indexing always returns a tensor that doesn't share memory with the source.\nThe second case is more complicated. These in-place operators are really a syntactic sugar.\n<A expression> -= <B expression>\n\nis actually rewritten as\n<A expression> = <A expression>.__isub__(<B expression>)\n\nThe important part is that it's the entire expression so it includes indexing as well. In your case, it will expand to (note that __isub__ just calls sub_, so you could replace one with another):\nx2[idx] = x2[idx].__isub__(0.1 * y)\nThis works fine, because you'll slice x2, get a new tensor with new data, mutate it, and do x2[idx] = new_tensor, which will correctly copy its data back into x2.", "body": "Yes, that's expected. It's a weird interplay of Python and PyTorch semantics. The first case is obviously expected to fail, because advanced indexing always returns a tensor that doesn't share memory with the source.\r\n\r\nThe second case is more complicated. These in-place operators are really a syntactic sugar.\r\n\r\n```\r\n<A expression> -= <B expression>\r\n```\r\nis actually rewritten as\r\n```\r\n<A expression> = <A expression>.__isub__(<B expression>)\r\n```\r\n\r\nThe important part is that it's the entire expression so it includes indexing as well. In your case, it will expand to (note that `__isub__` just calls `sub_`, so you could replace one with another):\r\n```python\r\nx2[idx] = x2[idx].__isub__(0.1 * y)\r\n```\r\nThis works fine, because you'll slice `x2`, get a new tensor with new data, mutate it, and do `x2[idx] = new_tensor`, which will correctly copy its data back into `x2`."}