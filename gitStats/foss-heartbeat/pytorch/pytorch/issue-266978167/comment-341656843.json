{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341656843", "html_url": "https://github.com/pytorch/pytorch/issues/3185#issuecomment-341656843", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3185", "id": 341656843, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTY1Njg0Mw==", "user": {"login": "gokceneraslan", "id": 1140359, "node_id": "MDQ6VXNlcjExNDAzNTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1140359?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gokceneraslan", "html_url": "https://github.com/gokceneraslan", "followers_url": "https://api.github.com/users/gokceneraslan/followers", "following_url": "https://api.github.com/users/gokceneraslan/following{/other_user}", "gists_url": "https://api.github.com/users/gokceneraslan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gokceneraslan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gokceneraslan/subscriptions", "organizations_url": "https://api.github.com/users/gokceneraslan/orgs", "repos_url": "https://api.github.com/users/gokceneraslan/repos", "events_url": "https://api.github.com/users/gokceneraslan/events{/privacy}", "received_events_url": "https://api.github.com/users/gokceneraslan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T09:38:18Z", "updated_at": "2017-11-12T08:00:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>lgamma is already in (i.e. <code>torch.lgamma()</code>) but without the derivative, so I had to use this to do backward() on it:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Function\n<span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">from</span> scipy.special <span class=\"pl-k\">import</span> digamma\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Lgamma</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-c1\">self</span>.save_for_backward(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-k\">return</span> torch.lgamma(<span class=\"pl-c1\">input</span>)\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        <span class=\"pl-c1\">input</span>, <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.saved_tensors\n        res <span class=\"pl-k\">=</span> torch.from_numpy(digamma(<span class=\"pl-c1\">input</span>.numpy())).type_as(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-k\">return</span> grad_output<span class=\"pl-k\">*</span>res\n\nlgamma <span class=\"pl-k\">=</span> Lgamma.apply</pre></div>", "body_text": "lgamma is already in (i.e. torch.lgamma()) but without the derivative, so I had to use this to do backward() on it:\nfrom torch.autograd import Function\nimport torch\n\nfrom scipy.special import digamma\n\nclass Lgamma(Function):\n    @staticmethod\n    def forward(self, input):\n        self.save_for_backward(input)\n        return torch.lgamma(input)\n\n    @staticmethod\n    def backward(self, grad_output):\n        input, = self.saved_tensors\n        res = torch.from_numpy(digamma(input.numpy())).type_as(input)\n        return grad_output*res\n\nlgamma = Lgamma.apply", "body": "lgamma is already in (i.e. `torch.lgamma()`) but without the derivative, so I had to use this to do backward() on it:\r\n\r\n```python\r\nfrom torch.autograd import Function\r\nimport torch\r\n\r\nfrom scipy.special import digamma\r\n\r\nclass Lgamma(Function):\r\n    @staticmethod\r\n    def forward(self, input):\r\n        self.save_for_backward(input)\r\n        return torch.lgamma(input)\r\n\r\n    @staticmethod\r\n    def backward(self, grad_output):\r\n        input, = self.saved_tensors\r\n        res = torch.from_numpy(digamma(input.numpy())).type_as(input)\r\n        return grad_output*res\r\n\r\nlgamma = Lgamma.apply\r\n```"}