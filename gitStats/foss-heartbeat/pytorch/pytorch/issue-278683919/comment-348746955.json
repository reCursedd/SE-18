{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348746955", "html_url": "https://github.com/pytorch/pytorch/issues/3979#issuecomment-348746955", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3979", "id": 348746955, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODc0Njk1NQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-03T07:51:36Z", "updated_at": "2017-12-03T07:51:36Z", "author_association": "MEMBER", "body_html": "<p>I'm not exactly sure what's the problem here, but doesn't look related to DataLoader. Data loading workers usually don't even initialize CUDA contexts (in fact, it is often an error to do it in those processes). The symptoms look as if you launched multiple multi-GPU training scripts that used the same GPUs. This is known to deadlock because of NVIDIA's NCCL library that doesn't handle these cases well.  Can you confirm that this was the case?</p>", "body_text": "I'm not exactly sure what's the problem here, but doesn't look related to DataLoader. Data loading workers usually don't even initialize CUDA contexts (in fact, it is often an error to do it in those processes). The symptoms look as if you launched multiple multi-GPU training scripts that used the same GPUs. This is known to deadlock because of NVIDIA's NCCL library that doesn't handle these cases well.  Can you confirm that this was the case?", "body": "I'm not exactly sure what's the problem here, but doesn't look related to DataLoader. Data loading workers usually don't even initialize CUDA contexts (in fact, it is often an error to do it in those processes). The symptoms look as if you launched multiple multi-GPU training scripts that used the same GPUs. This is known to deadlock because of NVIDIA's NCCL library that doesn't handle these cases well.  Can you confirm that this was the case?"}