{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/315243168", "html_url": "https://github.com/pytorch/pytorch/pull/1987#issuecomment-315243168", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1987", "id": 315243168, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTI0MzE2OA==", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-14T01:17:19Z", "updated_at": "2017-07-14T01:17:19Z", "author_association": "CONTRIBUTOR", "body_html": "<ol>\n<li>hows this looking?</li>\n<li>how to run the doctests? (I tried <code>python torch/nn/functional.py</code> but didnt seem to work. I tried <code>grep -r doctest</code>, and found a conftest.py in sphinx, that contains that. So, maybe need to run some tests in sphinx somehow?</li>\n</ol>\n<p>Note that I'm not sure that <code>padding_idx</code> makes sense in fucntional version? I think we just need to zero out the appropriate embedding_matrix rows ourselves? You can see how I think we should do that in the doctest bit:</p>\n<pre><code>        &gt;&gt;&gt; # example with padding_idx\n        &gt;&gt;&gt; embedding_matrix = Variable(torch.rand(10, 3))\n        &gt;&gt;&gt; embedding_matrix[0].zero_()\n        &gt;&gt;&gt; input = Variable(torch.LongTensor([[0,2,0,5]]))\n        &gt;&gt;&gt; torch.nn.functional.embedding(input, embedding_matrix)\n\n        Variable containing:\n        (0 ,.,.) =\n          0.0000  0.0000  0.0000\n          0.3452  0.4937 -0.9361\n          0.0000  0.0000  0.0000\n          0.0706 -2.1962 -0.6276\n        [torch.FloatTensor of size 1x4x3]\n</code></pre>\n<p>Thoughts?</p>", "body_text": "hows this looking?\nhow to run the doctests? (I tried python torch/nn/functional.py but didnt seem to work. I tried grep -r doctest, and found a conftest.py in sphinx, that contains that. So, maybe need to run some tests in sphinx somehow?\n\nNote that I'm not sure that padding_idx makes sense in fucntional version? I think we just need to zero out the appropriate embedding_matrix rows ourselves? You can see how I think we should do that in the doctest bit:\n        >>> # example with padding_idx\n        >>> embedding_matrix = Variable(torch.rand(10, 3))\n        >>> embedding_matrix[0].zero_()\n        >>> input = Variable(torch.LongTensor([[0,2,0,5]]))\n        >>> torch.nn.functional.embedding(input, embedding_matrix)\n\n        Variable containing:\n        (0 ,.,.) =\n          0.0000  0.0000  0.0000\n          0.3452  0.4937 -0.9361\n          0.0000  0.0000  0.0000\n          0.0706 -2.1962 -0.6276\n        [torch.FloatTensor of size 1x4x3]\n\nThoughts?", "body": "1. hows this looking?\r\n2. how to run the doctests? (I tried `python torch/nn/functional.py` but didnt seem to work. I tried `grep -r doctest`, and found a conftest.py in sphinx, that contains that. So, maybe need to run some tests in sphinx somehow?\r\n\r\nNote that I'm not sure that `padding_idx` makes sense in fucntional version? I think we just need to zero out the appropriate embedding_matrix rows ourselves? You can see how I think we should do that in the doctest bit:\r\n\r\n```\r\n        >>> # example with padding_idx\r\n        >>> embedding_matrix = Variable(torch.rand(10, 3))\r\n        >>> embedding_matrix[0].zero_()\r\n        >>> input = Variable(torch.LongTensor([[0,2,0,5]]))\r\n        >>> torch.nn.functional.embedding(input, embedding_matrix)\r\n\r\n        Variable containing:\r\n        (0 ,.,.) =\r\n          0.0000  0.0000  0.0000\r\n          0.3452  0.4937 -0.9361\r\n          0.0000  0.0000  0.0000\r\n          0.0706 -2.1962 -0.6276\r\n        [torch.FloatTensor of size 1x4x3]\r\n```\r\nThoughts?"}