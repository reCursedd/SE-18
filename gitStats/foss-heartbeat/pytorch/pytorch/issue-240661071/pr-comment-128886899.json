{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/128886899", "pull_request_review_id": 51613038, "id": 128886899, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyODg4Njg5OQ==", "diff_hunk": "@@ -557,6 +557,73 @@ def bilinear(input1, input2, weight, bias=None):\n         return Bilinear.apply(input1, input2, weight, bias)\n \n \n+def embedding(input, embedding_matrix,\n+              max_norm=None, norm_type=2, scale_grad_by_freq=False,\n+              sparse=False):\n+    r\"\"\"A simple lookup table that looks up embeddings in a fixed dictionary and size.\n+\n+    This module is often used to retrieve word embeddings using indices.\n+    The input to the module is a list of indices, and the embedding matrix,\n+    and the output is the corresponding word embeddings.\n+\n+    Args:\n+        input: tensor, containing indices into the embedding matrix\n+        embedding_matrix:\n+                Number of rows should correspond to the maximum possible index + 1,\n+                number of columns is the embedding size\n+        max_norm (float, optional): If given, will renormalize the embeddings to always have a norm lesser than this\n+        norm_type (float, optional): The p of the p-norm to compute for the max_norm option\n+        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the frequency of\n+                                                the words in the mini-batch.\n+\n+    Shape:\n+        - Input: LongTensor `(N, W)`, N = mini-batch, W = number of indices to extract per mini-batch\n+        - Embedding_matrix: FloatTensor `(V, embedding_dim)`, V = maximum index + 1, embedding_dim = embedding size\n+        - Output: `(N, W, embedding_dim)`\n+\n+    Examples::\n+\n+        >>> # a batch of 2 samples of 4 indices each\n+        >>> input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))\n+        >>> # an embedding matrix containing 10 tensors of size 3\n+        >>> embedding_matrix = Variable(torch.rand(10, 3))\n+        >>> torch.nn.functional.embedding(input, embedding_matrix)\n+\n+        Variable containing:\n+        (0 ,.,.) =\n+         -1.0822  1.2522  0.2434\n+          0.8393 -0.6062 -0.3348\n+          0.6597  0.0350  0.0837\n+          0.5521  0.9447  0.0498\n+\n+        (1 ,.,.) =\n+          0.6597  0.0350  0.0837\n+         -0.1527  0.0877  0.4260\n+          0.8393 -0.6062 -0.3348\n+         -0.8738 -0.9054  0.4281\n+        [torch.FloatTensor of size 2x4x3]\n+\n+        >>> # example with padding_idx\n+        >>> embedding_matrix = Variable(torch.rand(10, 3))\n+        >>> embedding_matrix[0].zero_()\n+        >>> input = Variable(torch.LongTensor([[0,2,0,5]]))\n+        >>> torch.nn.functional.embedding(input, embedding_matrix)\n+\n+        Variable containing:\n+        (0 ,.,.) =\n+          0.0000  0.0000  0.0000\n+          0.3452  0.4937 -0.9361\n+          0.0000  0.0000  0.0000\n+          0.0706 -2.1962 -0.6276\n+        [torch.FloatTensor of size 1x4x3]\n+\n+    \"\"\"\n+    return torch.nn.backends.thnn.backend.Embedding(", "path": "torch/nn/functional.py", "position": 65, "original_position": 65, "commit_id": "80cb0c4370c9d9954aaa61f0df21fc470cecdf6b", "original_commit_id": "80cb0c4370c9d9954aaa61f0df21fc470cecdf6b", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "body": "Well, I've never used padding_idx, but how were you imagining one would use padding_idx in the functional version?", "created_at": "2017-07-22T02:21:52Z", "updated_at": "2018-11-23T15:34:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/1987#discussion_r128886899", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1987", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/128886899"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1987#discussion_r128886899"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1987"}}, "body_html": "<p>Well, I've never used padding_idx, but how were you imagining one would use padding_idx in the functional version?</p>", "body_text": "Well, I've never used padding_idx, but how were you imagining one would use padding_idx in the functional version?", "in_reply_to_id": 128886504}