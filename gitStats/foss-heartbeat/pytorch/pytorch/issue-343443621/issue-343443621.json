{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9696", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9696/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9696/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9696/events", "html_url": "https://github.com/pytorch/pytorch/issues/9696", "id": 343443621, "node_id": "MDU6SXNzdWUzNDM0NDM2MjE=", "number": 9696, "title": "Possible deadlock in dist.init_process_group", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-07-22T22:13:02Z", "updated_at": "2018-09-21T00:20:48Z", "closed_at": "2018-09-21T00:20:48Z", "author_association": "NONE", "body_html": "<p>I'm observing hangs in <code>dist.init_process_group</code>. This happens reliably (100% of the time for me) when launching PyTorch distributed training runs on AWS using official DLAMI.</p>\n<p>It goes away when I pre-warm the volume. This workaround makes PyTorch startup much faster, hence I suspect the failure is caused by some handshake logic not being robust to variability in distributed worker timings.</p>\n<p>Using NCCL version 2.1.15+cuda9.1, and Amazon Deep Learning AMI v11</p>\n<p>Looking at strace, I see some workers stuck in <code>recvfrom</code>, while others are waiting on <code>accept4</code></p>\n<pre><code>[pid 10047] 21:59:49 accept4(32,  &lt;unfinished ...&gt;\n[pid  9976] 21:59:49 recvfrom(34,\n</code></pre>\n<pre><code>[pid  9985] 21:59:12 recvfrom(34,  &lt;unfinished ...&gt;\n[pid 10058] 21:59:12 accept4(32,\n</code></pre>", "body_text": "I'm observing hangs in dist.init_process_group. This happens reliably (100% of the time for me) when launching PyTorch distributed training runs on AWS using official DLAMI.\nIt goes away when I pre-warm the volume. This workaround makes PyTorch startup much faster, hence I suspect the failure is caused by some handshake logic not being robust to variability in distributed worker timings.\nUsing NCCL version 2.1.15+cuda9.1, and Amazon Deep Learning AMI v11\nLooking at strace, I see some workers stuck in recvfrom, while others are waiting on accept4\n[pid 10047] 21:59:49 accept4(32,  <unfinished ...>\n[pid  9976] 21:59:49 recvfrom(34,\n\n[pid  9985] 21:59:12 recvfrom(34,  <unfinished ...>\n[pid 10058] 21:59:12 accept4(32,", "body": "I'm observing hangs in `dist.init_process_group`. This happens reliably (100% of the time for me) when launching PyTorch distributed training runs on AWS using official DLAMI.\r\n\r\nIt goes away when I pre-warm the volume. This workaround makes PyTorch startup much faster, hence I suspect the failure is caused by some handshake logic not being robust to variability in distributed worker timings.\r\n\r\nUsing NCCL version 2.1.15+cuda9.1, and Amazon Deep Learning AMI v11\r\n\r\nLooking at strace, I see some workers stuck in `recvfrom`, while others are waiting on `accept4`\r\n\r\n```\r\n[pid 10047] 21:59:49 accept4(32,  <unfinished ...>\r\n[pid  9976] 21:59:49 recvfrom(34,\r\n```\r\n```\r\n[pid  9985] 21:59:12 recvfrom(34,  <unfinished ...>\r\n[pid 10058] 21:59:12 accept4(32,\r\n```\r\n"}