{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4984", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4984/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4984/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4984/events", "html_url": "https://github.com/pytorch/pytorch/issues/4984", "id": 293403624, "node_id": "MDU6SXNzdWUyOTM0MDM2MjQ=", "number": 4984, "title": "Implement AVX2 vectorized tanh function.", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-02-01T04:54:13Z", "updated_at": "2018-02-01T04:54:13Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>PyTorch includes avx_mathfun.h, which includes an 8-way vectorized implementation of <code>exp</code> (exp256_ps). We'd like to use that to implement a vectorized tanh function (accessible from torch using <code>torch.tanh</code>). Currently the tanh function is not vectorized.</p>\n<p>To get started, look into how other vectorized functions, like cadd, are implemented:</p>\n<p>Actual implementation:<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/vector/AVX2.c\">https://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/vector/AVX2.c</a><br>\nCode to figure out whether AVX2 is available dynamically and dispatch:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THVectorDispatch.c#L47\">pytorch/aten/src/TH/generic/THVectorDispatch.c</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 47\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472\">77c792e</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L47\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"47\"></td>\n          <td id=\"LC47\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">static</span> <span class=\"pl-en\">void</span> (*<span class=\"pl-en\">THVector_</span>(cadd_DISPATCHPTR))(real *, <span class=\"pl-k\">const</span> real *, <span class=\"pl-k\">const</span> real *, <span class=\"pl-k\">const</span> real, <span class=\"pl-k\">const</span> <span class=\"pl-c1\">ptrdiff_t</span>) = &amp;THVector_(cadd_DEFAULT); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>Current place where tanh (non-vectorized) is created (you will need to modify this to do correct AVX2 dispatch, and may not be able to use the macro that currently defines it):<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THTensorMath.c#L3081\">pytorch/aten/src/TH/generic/THTensorMath.c</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 3081\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472\">77c792e</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L3081\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"3081\"></td>\n          <td id=\"LC3081\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> LAB_IMPLEMENT_BASIC_FUNCTION(tanh,TH_MATH_NAME(tanh)) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=885359\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tschrager\">@tschrager</a></p>", "body_text": "PyTorch includes avx_mathfun.h, which includes an 8-way vectorized implementation of exp (exp256_ps). We'd like to use that to implement a vectorized tanh function (accessible from torch using torch.tanh). Currently the tanh function is not vectorized.\nTo get started, look into how other vectorized functions, like cadd, are implemented:\nActual implementation:\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/vector/AVX2.c\nCode to figure out whether AVX2 is available dynamically and dispatch:\n\n  \n    \n      pytorch/aten/src/TH/generic/THVectorDispatch.c\n    \n    \n         Line 47\n      in\n      77c792e\n    \n    \n    \n    \n\n        \n          \n           static void (*THVector_(cadd_DISPATCHPTR))(real *, const real *, const real *, const real, const ptrdiff_t) = &THVector_(cadd_DEFAULT); \n        \n    \n  \n\n\nCurrent place where tanh (non-vectorized) is created (you will need to modify this to do correct AVX2 dispatch, and may not be able to use the macro that currently defines it):\n\n  \n    \n      pytorch/aten/src/TH/generic/THTensorMath.c\n    \n    \n         Line 3081\n      in\n      77c792e\n    \n    \n    \n    \n\n        \n          \n           LAB_IMPLEMENT_BASIC_FUNCTION(tanh,TH_MATH_NAME(tanh)) \n        \n    \n  \n\n\n@tschrager", "body": "PyTorch includes avx_mathfun.h, which includes an 8-way vectorized implementation of `exp` (exp256_ps). We'd like to use that to implement a vectorized tanh function (accessible from torch using `torch.tanh`). Currently the tanh function is not vectorized.\r\n\r\nTo get started, look into how other vectorized functions, like cadd, are implemented:\r\n\r\nActual implementation:\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/vector/AVX2.c\r\nCode to figure out whether AVX2 is available dynamically and dispatch:\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THVectorDispatch.c#L47\r\n\r\nCurrent place where tanh (non-vectorized) is created (you will need to modify this to do correct AVX2 dispatch, and may not be able to use the macro that currently defines it):\r\nhttps://github.com/pytorch/pytorch/blob/77c792ec276ee8bf9e279ce34ecb8dac5ecbf472/aten/src/TH/generic/THTensorMath.c#L3081\r\n\r\n@tschrager\r\n\r\n\r\n"}