{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166154346", "pull_request_review_id": 94102327, "id": 166154346, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjE1NDM0Ng==", "diff_hunk": "@@ -319,107 +273,105 @@ static void lambdaLiftReverse(Graph& graph,\n   //   [original outputs], [temporaries]\n   //\n   // Reverse inputs:\n-  //   [captured primal values, in topological order],\n   //   [output vjps (aka grad_outputs)], [temporary vjps]\n-\n-  // -- Simple cases -----------------------------------------------------------\n-  value_list primal_inputs   = filter(graph.inputs(),  is_stage_0);\n-  value_list reverse_outputs = filter(graph.outputs(), is_stage_1);\n+  //   [captured primal values, in topological order],\n \n   // -- Construct primal_outputs, df_input_captures, f_real_outputs ----\n-  value_list primal_outputs  = filter(graph.outputs(), is_stage_0);\n-  grad_desc.f_real_outputs = primal_outputs.size();\n+  grad_desc.f_real_outputs = graph.outputs().size();\n \n   std::unordered_map<Value*, std::size_t> orig_primal_outputs_idx;\n   std::unordered_map<Value*, std::size_t> orig_primal_inputs_idx;\n   // NOTE: we use emplace to avoid replacing an existing index if an output is repeated\n-  for (std::size_t i = 0, num_outputs = primal_outputs.size(); i < num_outputs; ++i)\n-    orig_primal_outputs_idx.emplace(primal_outputs[i], i);\n-  for (std::size_t i = 0, num_inputs = primal_inputs.size(); i < num_inputs; ++i)\n-    orig_primal_inputs_idx[primal_inputs[i]] = i;\n+  for (std::size_t i = 0, num_outputs = graph.outputs().size(); i < num_outputs; ++i)\n+    orig_primal_outputs_idx.emplace(graph.outputs()[i], i);\n+  for (std::size_t i = 0, num_inputs = graph.inputs().size(); i < num_inputs; ++i)\n+    orig_primal_inputs_idx[graph.inputs()[i]] = i;\n \n   // NB: reverse_captures are already deduplicated, and in topo order\n   for (Value * capture_val : reverse_captures) {\n     // If it's already an output we don't have to add anything,\n     // but register the fact that it needs to be captured.\n     if (orig_primal_outputs_idx.count(capture_val) > 0) {\n-      grad_desc.df_input_captures.emplace_back(Capture::Kind::Output,\n-                                              orig_primal_outputs_idx[capture_val]);\n+      grad_desc.df_input_captured_outputs.push_back(orig_primal_outputs_idx[capture_val]);\n     // If it's an input, we could add it as an output but in fact it's\n     // more efficient to use a special kind of capture.\n     } else if (orig_primal_inputs_idx.count(capture_val) > 0) {\n-      grad_desc.df_input_captures.emplace_back(Capture::Kind::Input,\n-                                               orig_primal_inputs_idx.at(capture_val));\n+      grad_desc.df_input_captured_inputs.push_back(orig_primal_inputs_idx.at(capture_val));\n     // Otherwise it's just a regular intermediate value that we need to add as an output\n     } else {\n-      primal_outputs.emplace_back(capture_val);\n-      grad_desc.df_input_captures.emplace_back(Capture::Kind::Output,\n-                                               primal_outputs.size() - 1);\n+      // we need to create a new temporary output for this capture because it wasn't availiable.\n+      graph.registerOutput(capture_val);\n+      grad_desc.df_input_captured_outputs.emplace_back(graph.outputs().size() - 1);\n     }\n   }\n \n   // -- Add VJPs for temporaries, adjust df_input_vjps -------------------------\n   // NB [possible optimization]: use the newly added vjp input as soon as the first\n   // vjp for that value is generated, to reduce the lifespan of this input\n   // (currently we add it to the final vjp after all adds).\n-  JIT_ASSERT(graph.stage() == 1); // We will be adding inputs to stage 1\n-  for (std::size_t i = grad_desc.f_real_outputs; i < primal_outputs.size(); ++i) {\n-    Value * tmp = primal_outputs.at(i);\n+  for (std::size_t i = grad_desc.f_real_outputs; i < graph.outputs().size(); ++i) {\n+    Value * tmp = graph.outputs().at(i);\n     // Add VJP inputs only for intermediates that actually required grad.\n     if (rev_info.requires_grad_set.count(tmp) == 0) continue;\n-    Value * tmp_vjp_in = graph.addInput()->setType(tmp->typeOption());\n+    Value * tmp_vjp_in = reverse_block->addInput()->setType(tmp->typeOption());\n     Value * tmp_vjp_prev = rev_info.grad_map.at(tmp);\n-    auto zeroes = createZerosLike(tmp);\n-    tmp_vjp_in = createUndefGuard(tmp_vjp_in, zeroes);\n-    // make sure createUndefGuard happens before the addition but inside stage 1\n-    zeroes->node()->moveBefore(tmp_vjp_prev->node());\n-    tmp_vjp_in->node()->moveBefore(tmp_vjp_prev->node());\n-\n+    {\n+      WithInsertPoint guard(graph, tmp_vjp_prev->node());\n+      auto zeroes = createZerosLike(tmp);\n+      tmp_vjp_in = createUndefGuard(tmp_vjp_in, zeroes);\n+    }\n     // This is quite weird because we can't first make a sum and then replace all uses\n     // of tmp_vjp_prev (that would replace its use in the sum too!), so we create an\n     // incorrect sum that doesn't use prev vjp, replace uses, and fix the sum.\n-    Value * new_vjp = addValues(tmp_vjp_in, tmp_vjp_in, tmp_vjp_prev->node());\n+    Value * new_vjp = toVar(tmp_vjp_in) + toVar(tmp_vjp_in);\n+    new_vjp->node()->moveAfter(tmp_vjp_prev->node());\n     tmp_vjp_prev->replaceAllUsesWith(new_vjp);\n     new_vjp->node()->replaceInput(1, tmp_vjp_prev);\n     grad_desc.df_input_vjps.emplace_back(i);\n   }\n \n-  // -- Construct reverse_inputs -----------------------------------------------\n-  // Quick reference:\n-  //   [captured primal values, in topological order],            1st loop below\n-  //   [output vjps (aka grad_outputs)], [temporary vjps]         2nd loop below\n-  value_list reverse_inputs;\n-  for (Capture capture : grad_desc.df_input_captures) {\n-    auto & source = capture.kind == Capture::Kind::Input ? primal_inputs : primal_outputs;\n-    reverse_inputs.push_back(source[capture.offset]);\n-  }\n-  // These are the vjps computed by differentiate + the code above\n-  for (Value * reverse_vjp : filter(graph.inputs(), is_stage_1))\n-    reverse_inputs.push_back(reverse_vjp);\n-\n-  // Finally, we can split the graph into two parts.\n-  grad_desc.f  = splitOffStage(graph, 0, primal_inputs, primal_outputs);\n-  grad_desc.df = splitOffStage(graph, 1, reverse_inputs, reverse_outputs);\n+  // add the captures as formal arguments to the reverse_block\n+  // afterward inputs: [output vjps][temporary vjps][captures]\n+  // construct a map from captured 'value' to the index in the input list\n+  // used to extract this block into its own function\n+  std::unordered_map<Value*, size_t> capture_to_formal_index;\n+  const auto & add_capture = [&](Value * captured) {\n+    capture_to_formal_index[captured] = reverse_block->inputs().size();\n+    reverse_block->addInput()->copyMetadata(captured);\n+  };\n+  for(auto & offset : grad_desc.df_input_captured_inputs)\n+    add_capture(graph.inputs()[offset]);\n+  for(auto & offset : grad_desc.df_input_captured_outputs)\n+    add_capture(graph.outputs()[offset]);\n+\n+  grad_desc.df = std::make_shared<Graph>();\n+  grad_desc.df->block()->cloneFrom(reverse_block, [&](Value* v) {\n+    return grad_desc.df->inputs()[capture_to_formal_index.at(v)];\n+  });\n+  // reverse_node was just to hold onto reverse_block in a debuggable way\n+  // we can remove it now.\n+  reverse_block->owningNode()->destroy();\n }\n \n Gradient differentiate(std::shared_ptr<Graph>& _graph, const std::vector<bool>& requires_grad) {\n+  Gradient grad_desc;\n   // Take ownership of the graph\n-  std::shared_ptr<Graph> graph;\n   JIT_ASSERTM(_graph.use_count() == 1,\n               \"differentiate will mutate and destroy the graph, so it requires \"\n               \"graph.use_count() == 1\");\n-  std::swap(_graph, graph);\n+  std::swap(_graph, grad_desc.f);\n   // XXX: Take care when handling outputs - they can be duplicated!\n-  Gradient grad_desc;\n+\n+  WithInsertPoint guard(*grad_desc.f, grad_desc.f->block());", "path": "torch/csrc/jit/autodiff.cpp", "position": 382, "original_position": 382, "commit_id": "6f27a41a86b8da8934947f2ad37d0c8efff17d6d", "original_commit_id": "6f27a41a86b8da8934947f2ad37d0c8efff17d6d", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why is this insert point necessary? Isn't the main block a default insert point?", "created_at": "2018-02-06T00:20:29Z", "updated_at": "2018-11-23T15:39:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/5036#discussion_r166154346", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5036", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166154346"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5036#discussion_r166154346"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5036"}}, "body_html": "<p>Why is this insert point necessary? Isn't the main block a default insert point?</p>", "body_text": "Why is this insert point necessary? Isn't the main block a default insert point?"}