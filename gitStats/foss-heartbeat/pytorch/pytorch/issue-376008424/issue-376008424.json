{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13390", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13390/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13390/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13390/events", "html_url": "https://github.com/pytorch/pytorch/issues/13390", "id": 376008424, "node_id": "MDU6SXNzdWUzNzYwMDg0MjQ=", "number": 13390, "title": "_jit_differentiate can not process aten::cat", "user": {"login": "ozzzp", "id": 2487585, "node_id": "MDQ6VXNlcjI0ODc1ODU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2487585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ozzzp", "html_url": "https://github.com/ozzzp", "followers_url": "https://api.github.com/users/ozzzp/followers", "following_url": "https://api.github.com/users/ozzzp/following{/other_user}", "gists_url": "https://api.github.com/users/ozzzp/gists{/gist_id}", "starred_url": "https://api.github.com/users/ozzzp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ozzzp/subscriptions", "organizations_url": "https://api.github.com/users/ozzzp/orgs", "repos_url": "https://api.github.com/users/ozzzp/repos", "events_url": "https://api.github.com/users/ozzzp/events{/privacy}", "received_events_url": "https://api.github.com/users/ozzzp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-31T15:04:07Z", "updated_at": "2018-10-31T16:51:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>code:</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">fun</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>):\n        <span class=\"pl-k\">return</span> torch.cat([x, y], <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\n    fun <span class=\"pl-k\">=</span> torch.jit.trace(fun, (torch.ones([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n                                torch.zeros([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)))\n    out <span class=\"pl-k\">=</span> torch._C._jit_differentiate(fun.graph)\n    <span class=\"pl-k\">return</span> out\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<h2>output:</h2>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>playground.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">14</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    main()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>playground.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">9</span>, <span class=\"pl-k\">in</span> main\n    out <span class=\"pl-k\">=</span> torch._C._jit_differentiate(fun.graph)\n<span class=\"pl-c1\">RuntimeError</span>: differentiation of aten::cat <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> supported, <span class=\"pl-k\">or</span> it <span class=\"pl-k\">is</span> missing necessary <span class=\"pl-c1\">type</span> information</pre></div>\n<h2>Environment</h2>\n<div class=\"highlight highlight-source-shell\"><pre>Collecting environment information...\nPyTorch version: 1.0.0.dev20181029\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.14\nGCC version: Could not collect\nCMake version: version 3.12.2\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.3)\n[pip] pytorchviz (0.0.1)\n[pip] torch (1.0.0.dev20181029)\n[pip] torchvision (0.2.1)\n[conda] pytorch-nightly           1.0.0.dev20181029         py3.6_0    \n[conda] pytorchviz                0.0.1                     <span class=\"pl-k\">&lt;</span>pip<span class=\"pl-k\">&gt;</span>\n[conda] torchvision               0.2.1                     <span class=\"pl-k\">&lt;</span>pip<span class=\"pl-k\">&gt;</span></pre></div>\n<p>It can be located in <code>pytorch/torch/csrc/jit/autodiff.cpp</code> line 357-361</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">if</span> (!isDifferentiable(node)) {\n    <span class=\"pl-k\">throw</span> <span class=\"pl-smi\">std::runtime_error</span>(<span class=\"pl-c1\">std::string</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>differentiation of <span class=\"pl-pds\">\"</span></span>) + node-&gt;<span class=\"pl-c1\">kind</span>().<span class=\"pl-c1\">toDisplayString</span>() + <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> <span class=\"pl-pds\">\"</span></span>\n                             <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>is not supported, or it is missing necessary type information<span class=\"pl-pds\">\"</span></span>);\n  }\n  <span class=\"pl-k\">auto</span> sym_grads = build_sym_grad(fmap&lt;SymbolicVariable&gt;(grad_values));</pre></div>\n<p>but curiously, from my perspective, it seems that it has the ability to process <code>\"aten::cat(Tensor[] tensors, int dim) -&gt; Tensor\"</code> but simply blocked by <code>isDifferentiable</code>.</p>\n<p>Any Idea?</p>", "body_text": "code:\nimport torch\n\ndef main():\n    def fun(x, y):\n        return torch.cat([x, y], dim=0)\n\n    fun = torch.jit.trace(fun, (torch.ones([4, 4], requires_grad=True),\n                                torch.zeros([4, 4], requires_grad=True)))\n    out = torch._C._jit_differentiate(fun.graph)\n    return out\n\n\nif __name__ == '__main__':\n    main()\noutput:\nTraceback (most recent call last):\n  File \"playground.py\", line 14, in <module>\n    main()\n  File \"playground.py\", line 9, in main\n    out = torch._C._jit_differentiate(fun.graph)\nRuntimeError: differentiation of aten::cat is not supported, or it is missing necessary type information\nEnvironment\nCollecting environment information...\nPyTorch version: 1.0.0.dev20181029\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.14\nGCC version: Could not collect\nCMake version: version 3.12.2\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.3)\n[pip] pytorchviz (0.0.1)\n[pip] torch (1.0.0.dev20181029)\n[pip] torchvision (0.2.1)\n[conda] pytorch-nightly           1.0.0.dev20181029         py3.6_0    \n[conda] pytorchviz                0.0.1                     <pip>\n[conda] torchvision               0.2.1                     <pip>\nIt can be located in pytorch/torch/csrc/jit/autodiff.cpp line 357-361\nif (!isDifferentiable(node)) {\n    throw std::runtime_error(std::string(\"differentiation of \") + node->kind().toDisplayString() + \" \"\n                             \"is not supported, or it is missing necessary type information\");\n  }\n  auto sym_grads = build_sym_grad(fmap<SymbolicVariable>(grad_values));\nbut curiously, from my perspective, it seems that it has the ability to process \"aten::cat(Tensor[] tensors, int dim) -> Tensor\" but simply blocked by isDifferentiable.\nAny Idea?", "body": "## code:\r\n\r\n```python\r\nimport torch\r\n\r\ndef main():\r\n    def fun(x, y):\r\n        return torch.cat([x, y], dim=0)\r\n\r\n    fun = torch.jit.trace(fun, (torch.ones([4, 4], requires_grad=True),\r\n                                torch.zeros([4, 4], requires_grad=True)))\r\n    out = torch._C._jit_differentiate(fun.graph)\r\n    return out\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n## output:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"playground.py\", line 14, in <module>\r\n    main()\r\n  File \"playground.py\", line 9, in main\r\n    out = torch._C._jit_differentiate(fun.graph)\r\nRuntimeError: differentiation of aten::cat is not supported, or it is missing necessary type information\r\n```\r\n\r\n## Environment\r\n```bash\r\nCollecting environment information...\r\nPyTorch version: 1.0.0.dev20181029\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14\r\nGCC version: Could not collect\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.3)\r\n[pip] pytorchviz (0.0.1)\r\n[pip] torch (1.0.0.dev20181029)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch-nightly           1.0.0.dev20181029         py3.6_0    \r\n[conda] pytorchviz                0.0.1                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\nIt can be located in `pytorch/torch/csrc/jit/autodiff.cpp` line 357-361\r\n```C++\r\nif (!isDifferentiable(node)) {\r\n    throw std::runtime_error(std::string(\"differentiation of \") + node->kind().toDisplayString() + \" \"\r\n                             \"is not supported, or it is missing necessary type information\");\r\n  }\r\n  auto sym_grads = build_sym_grad(fmap<SymbolicVariable>(grad_values));\r\n``` \r\nbut curiously, from my perspective, it seems that it has the ability to process `\"aten::cat(Tensor[] tensors, int dim) -> Tensor\"` but simply blocked by `isDifferentiable`.\r\n\r\nAny Idea? "}