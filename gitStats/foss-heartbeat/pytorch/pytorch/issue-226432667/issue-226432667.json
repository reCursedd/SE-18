{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1478", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1478/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1478/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1478/events", "html_url": "https://github.com/pytorch/pytorch/issues/1478", "id": 226432667, "node_id": "MDU6SXNzdWUyMjY0MzI2Njc=", "number": 1478, "title": "GPU Memory climbs with torch.nn.LSTM or torch.nn.LSTMCell", "user": {"login": "NaimKabir", "id": 4506277, "node_id": "MDQ6VXNlcjQ1MDYyNzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/4506277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NaimKabir", "html_url": "https://github.com/NaimKabir", "followers_url": "https://api.github.com/users/NaimKabir/followers", "following_url": "https://api.github.com/users/NaimKabir/following{/other_user}", "gists_url": "https://api.github.com/users/NaimKabir/gists{/gist_id}", "starred_url": "https://api.github.com/users/NaimKabir/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NaimKabir/subscriptions", "organizations_url": "https://api.github.com/users/NaimKabir/orgs", "repos_url": "https://api.github.com/users/NaimKabir/repos", "events_url": "https://api.github.com/users/NaimKabir/events{/privacy}", "received_events_url": "https://api.github.com/users/NaimKabir/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-04T23:05:23Z", "updated_at": "2017-05-04T23:42:36Z", "closed_at": "2017-05-04T23:42:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Whenever I use an LSTM to munch on batches of sequences, GPU memory usage seems to climb steadily upwards until I hit an Out Of Memory error.</p>\n<p><strong>Some System Info:</strong><br>\nTITAN X (Pascal)<br>\nDriver Version: 367.48<br>\nPython 2.7.12 |Anaconda 4.2.0 (64-bit)|</p>\n<p>Small code snippet of a dumb LSTM usage that causes the memory problem:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Function to make dummy data.</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">datagen</span>(<span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">seq_length</span>,  <span class=\"pl-smi\">vector_dim</span>):\n    <span class=\"pl-k\">return</span> torch.rand(seq_length, batch_size,  vector_dim)\n\nsamples <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100000</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nsequence_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\nvector_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\nlstm <span class=\"pl-k\">=</span>  nn.LSTM(vector_dim, vector_dim).cuda()\n\n<span class=\"pl-k\">for</span> sample <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(samples):\n\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(datagen(batch_size, sequence_len, vector_dim).cuda(), <span class=\"pl-v\">requires_grad</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Instantiating new c0 and h0 on every forward...I hope</span>\n    c0 <span class=\"pl-k\">=</span> Variable(torch.rand(lstm.num_layers, batch_size, lstm.hidden_size).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    h0 <span class=\"pl-k\">=</span> Variable(torch.rand(lstm.num_layers, batch_size, lstm.hidden_size).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n    hidden, output <span class=\"pl-k\">=</span> lstm(<span class=\"pl-c1\">input</span>, (h0, c0))</pre></div>\n<p>Stepping with a debugger and keeping my eye on the nvidia-smi, I can see that memory jumps up on every forward pass.</p>\n<p>The problem is replicable with an LSTMCell as well:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Function to make dummy data.</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">datagen</span>(<span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">seq_length</span>,  <span class=\"pl-smi\">vector_dim</span>):\n    <span class=\"pl-k\">return</span> torch.rand(seq_length, batch_size,  vector_dim)\n\nsamples <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100000</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nsequence_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\nvector_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\nlstm <span class=\"pl-k\">=</span>  nn.LSTMCell(vector_dim, vector_dim).cuda()\n\n<span class=\"pl-k\">for</span> sample <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(samples):\n\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(datagen(batch_size, sequence_len, vector_dim).cuda(), <span class=\"pl-v\">requires_grad</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Instantiating new c0 and h0 on every forward...I hope</span>\n    c0 <span class=\"pl-k\">=</span> Variable(torch.rand(batch_size, lstm.hidden_size).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    h0 <span class=\"pl-k\">=</span> Variable(torch.rand(batch_size, lstm.hidden_size).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Passing sequence through Cell</span>\n    <span class=\"pl-k\">for</span> sequence_element <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>)):\n        hidden, output <span class=\"pl-k\">=</span> lstm(<span class=\"pl-c1\">input</span>[sequence_element, :, :], (h0, c0))</pre></div>", "body_text": "Whenever I use an LSTM to munch on batches of sequences, GPU memory usage seems to climb steadily upwards until I hit an Out Of Memory error.\nSome System Info:\nTITAN X (Pascal)\nDriver Version: 367.48\nPython 2.7.12 |Anaconda 4.2.0 (64-bit)|\nSmall code snippet of a dumb LSTM usage that causes the memory problem:\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\n\n#Function to make dummy data.\ndef datagen(batch_size, seq_length,  vector_dim):\n    return torch.rand(seq_length, batch_size,  vector_dim)\n\nsamples = 100000\nbatch_size = 32\nsequence_len = 20\nvector_dim = 10\n\nlstm =  nn.LSTM(vector_dim, vector_dim).cuda()\n\nfor sample in range(samples):\n\n    input = Variable(datagen(batch_size, sequence_len, vector_dim).cuda(), requires_grad = True)\n\n    # Instantiating new c0 and h0 on every forward...I hope\n    c0 = Variable(torch.rand(lstm.num_layers, batch_size, lstm.hidden_size).cuda(), requires_grad=False)\n    h0 = Variable(torch.rand(lstm.num_layers, batch_size, lstm.hidden_size).cuda(), requires_grad=False)\n\n    hidden, output = lstm(input, (h0, c0))\nStepping with a debugger and keeping my eye on the nvidia-smi, I can see that memory jumps up on every forward pass.\nThe problem is replicable with an LSTMCell as well:\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\n\n#Function to make dummy data.\ndef datagen(batch_size, seq_length,  vector_dim):\n    return torch.rand(seq_length, batch_size,  vector_dim)\n\nsamples = 100000\nbatch_size = 32\nsequence_len = 20\nvector_dim = 10\n\nlstm =  nn.LSTMCell(vector_dim, vector_dim).cuda()\n\nfor sample in range(samples):\n\n    input = Variable(datagen(batch_size, sequence_len, vector_dim).cuda(), requires_grad = True)\n\n\n    # Instantiating new c0 and h0 on every forward...I hope\n    c0 = Variable(torch.rand(batch_size, lstm.hidden_size).cuda(), requires_grad=False)\n    h0 = Variable(torch.rand(batch_size, lstm.hidden_size).cuda(), requires_grad=False)\n\n    #Passing sequence through Cell\n    for sequence_element in range(input.size(0)):\n        hidden, output = lstm(input[sequence_element, :, :], (h0, c0))", "body": "Whenever I use an LSTM to munch on batches of sequences, GPU memory usage seems to climb steadily upwards until I hit an Out Of Memory error.\r\n\r\n**Some System Info:**\r\nTITAN X (Pascal)  \r\nDriver Version: 367.48 \r\nPython 2.7.12 |Anaconda 4.2.0 (64-bit)| \r\n\r\nSmall code snippet of a dumb LSTM usage that causes the memory problem:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\n\r\n#Function to make dummy data.\r\ndef datagen(batch_size, seq_length,  vector_dim):\r\n    return torch.rand(seq_length, batch_size,  vector_dim)\r\n\r\nsamples = 100000\r\nbatch_size = 32\r\nsequence_len = 20\r\nvector_dim = 10\r\n\r\nlstm =  nn.LSTM(vector_dim, vector_dim).cuda()\r\n\r\nfor sample in range(samples):\r\n\r\n    input = Variable(datagen(batch_size, sequence_len, vector_dim).cuda(), requires_grad = True)\r\n\r\n    # Instantiating new c0 and h0 on every forward...I hope\r\n    c0 = Variable(torch.rand(lstm.num_layers, batch_size, lstm.hidden_size).cuda(), requires_grad=False)\r\n    h0 = Variable(torch.rand(lstm.num_layers, batch_size, lstm.hidden_size).cuda(), requires_grad=False)\r\n\r\n    hidden, output = lstm(input, (h0, c0))\r\n```\r\n\r\nStepping with a debugger and keeping my eye on the nvidia-smi, I can see that memory jumps up on every forward pass.\r\n\r\nThe problem is replicable with an LSTMCell as well:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\n\r\n#Function to make dummy data.\r\ndef datagen(batch_size, seq_length,  vector_dim):\r\n    return torch.rand(seq_length, batch_size,  vector_dim)\r\n\r\nsamples = 100000\r\nbatch_size = 32\r\nsequence_len = 20\r\nvector_dim = 10\r\n\r\nlstm =  nn.LSTMCell(vector_dim, vector_dim).cuda()\r\n\r\nfor sample in range(samples):\r\n\r\n    input = Variable(datagen(batch_size, sequence_len, vector_dim).cuda(), requires_grad = True)\r\n\r\n\r\n    # Instantiating new c0 and h0 on every forward...I hope\r\n    c0 = Variable(torch.rand(batch_size, lstm.hidden_size).cuda(), requires_grad=False)\r\n    h0 = Variable(torch.rand(batch_size, lstm.hidden_size).cuda(), requires_grad=False)\r\n\r\n    #Passing sequence through Cell\r\n    for sequence_element in range(input.size(0)):\r\n        hidden, output = lstm(input[sequence_element, :, :], (h0, c0))\r\n```\r\n\r\n"}