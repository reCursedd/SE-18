{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6107", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6107/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6107/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6107/events", "html_url": "https://github.com/pytorch/pytorch/issues/6107", "id": 309716937, "node_id": "MDU6SXNzdWUzMDk3MTY5Mzc=", "number": 6107, "title": "[Proposal] Using ONNX initializers as a safe way to serialize model parameters", "user": {"login": "arogozhnikov", "id": 6318811, "node_id": "MDQ6VXNlcjYzMTg4MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6318811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arogozhnikov", "html_url": "https://github.com/arogozhnikov", "followers_url": "https://api.github.com/users/arogozhnikov/followers", "following_url": "https://api.github.com/users/arogozhnikov/following{/other_user}", "gists_url": "https://api.github.com/users/arogozhnikov/gists{/gist_id}", "starred_url": "https://api.github.com/users/arogozhnikov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arogozhnikov/subscriptions", "organizations_url": "https://api.github.com/users/arogozhnikov/orgs", "repos_url": "https://api.github.com/users/arogozhnikov/repos", "events_url": "https://api.github.com/users/arogozhnikov/events{/privacy}", "received_events_url": "https://api.github.com/users/arogozhnikov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-03-29T11:22:37Z", "updated_at": "2018-05-29T12:59:26Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi, I propose to add a safe way to store/load model weights (safe = can safely use weights downloaded from the internet). My vision:</p>\n<ul>\n<li>a serialization mechanism that is currently officially supported uses pickle (protocol=2) to store/load weights. Usage described <a href=\"http://pytorch.org/docs/master/notes/serialization.html\" rel=\"nofollow\">here</a></li>\n<li>pickle is unsafe</li>\n<li>pytorch supports exporting to ONNX, but not importing</li>\n</ul>\n<h1>Proposal</h1>\n<p>Use ONNX format, but use only initializers field to store a model's state.</p>\n<h2>Pros</h2>\n<ul>\n<li>Safe</li>\n<li>Relies on already existing format with good tooling; needed protobuf-code is already in pytorch source. If needed, weights can be read from other languages</li>\n<li>Proposed implementation (see later) stores parameters that are met in the model state several times, loading does not break this situation (if parameters used the same tensor, they use the same tensor after loading)</li>\n<li>Proposed implementation was checked to normally import / export all (largest) torchvision models: inception_v3, alexnet, densenet201, resnet152, squeezenet1_1, vgg19_bn.</li>\n</ul>\n<h2>Cons</h2>\n<ul>\n<li>No support for serialization of sparse tensors. AFAIK, ONNX has no support for SparseTensors so far. Hacks possible to save indices and values, but it is better to wait for official support of sparse tensors in ONNX.</li>\n</ul>\n<h2>Proposed implementation</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> onnx <span class=\"pl-k\">import</span> ModelProto, GraphProto, numpy_helper, load_from_string\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">save_model_state</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">filename</span>):\n    graph <span class=\"pl-k\">=</span> GraphProto()\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> exporting only once</span>\n    exported_ids <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>()\n    <span class=\"pl-k\">for</span> name, tensor <span class=\"pl-k\">in</span> model.state_dict().items():\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">id</span>(tensor) <span class=\"pl-k\">in</span> exported_ids:\n            <span class=\"pl-k\">continue</span> \n        <span class=\"pl-k\">else</span>:\n            exported_ids.add(<span class=\"pl-c1\">id</span>(tensor))\n        <span class=\"pl-k\">try</span>:\n            numpy_value <span class=\"pl-k\">=</span> tensor.clone().cpu().numpy()\n        <span class=\"pl-k\">except</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Parameter <span class=\"pl-c1\">{}</span>, tensor: <span class=\"pl-c1\">{}</span> can't be dumped. <span class=\"pl-c1\">\\</span></span>\n<span class=\"pl-s\">                               Sparse tensors can't be saved<span class=\"pl-pds\">\"</span></span>.format(name, tensor))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> cloning to avoid moving tensor from/to GPU</span>\n        initializer <span class=\"pl-k\">=</span> numpy_helper.from_array(numpy_value, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name)\n        graph.initializer.extend([initializer])\n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(filename, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>wb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n        f.write(ModelProto(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph).SerializeToString())\n        \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">load_model_state</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">filename</span>, <span class=\"pl-smi\">strict</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(filename, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>rb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n        graph_loaded <span class=\"pl-k\">=</span> load_from_string(f.read()).graph\n        \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> exporting only once</span>\n    imported_ids <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>()\n    own_state <span class=\"pl-k\">=</span> model.state_dict()\n    <span class=\"pl-k\">for</span> initializer <span class=\"pl-k\">in</span> graph_loaded.initializer:\n        name <span class=\"pl-k\">=</span> initializer.name\n        <span class=\"pl-k\">if</span> name <span class=\"pl-k\">in</span> own_state:\n            <span class=\"pl-k\">try</span>:\n                tensor <span class=\"pl-k\">=</span> own_state[name]\n                numpy_value <span class=\"pl-k\">=</span> numpy_helper.to_array(initializer)\n                tensor.copy_(tensor.new(numpy_value)[:], <span class=\"pl-v\">broadcast</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n                imported_ids.add(<span class=\"pl-c1\">id</span>(tensor))\n            <span class=\"pl-k\">except</span> <span class=\"pl-c1\">Exception</span>:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>While copying the parameter named <span class=\"pl-c1\">{}</span>, <span class=\"pl-pds\">'</span></span>\n                                   <span class=\"pl-s\"><span class=\"pl-pds\">'</span>whose dimensions in the model are <span class=\"pl-c1\">{}</span> and <span class=\"pl-pds\">'</span></span>\n                                   <span class=\"pl-s\"><span class=\"pl-pds\">'</span>whose dimensions in the checkpoint are <span class=\"pl-c1\">{}</span>.<span class=\"pl-pds\">'</span></span>\n                                   .format(name, own_state[name].size(), numpy_value.shape))\n        <span class=\"pl-k\">elif</span> strict:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">KeyError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>unexpected key \"<span class=\"pl-c1\">{}</span>\" in state_dict<span class=\"pl-pds\">'</span></span>.format(name))\n    <span class=\"pl-k\">if</span> strict:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> checking that all tensors were covered</span>\n        <span class=\"pl-k\">for</span> name, tensor <span class=\"pl-k\">in</span> own_state.items():\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">id</span>(tensor) <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> imported_ids:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">KeyError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>missing keys in state_dict: \"<span class=\"pl-c1\">{}</span>\"<span class=\"pl-pds\">'</span></span>.format(name))</pre></div>\n<p>Comment on implementation:</p>\n<ul>\n<li>wasn't able to use pytorch's ONNX classes, so used onnx package. Probably, worth fixing</li>\n</ul>", "body_text": "Hi, I propose to add a safe way to store/load model weights (safe = can safely use weights downloaded from the internet). My vision:\n\na serialization mechanism that is currently officially supported uses pickle (protocol=2) to store/load weights. Usage described here\npickle is unsafe\npytorch supports exporting to ONNX, but not importing\n\nProposal\nUse ONNX format, but use only initializers field to store a model's state.\nPros\n\nSafe\nRelies on already existing format with good tooling; needed protobuf-code is already in pytorch source. If needed, weights can be read from other languages\nProposed implementation (see later) stores parameters that are met in the model state several times, loading does not break this situation (if parameters used the same tensor, they use the same tensor after loading)\nProposed implementation was checked to normally import / export all (largest) torchvision models: inception_v3, alexnet, densenet201, resnet152, squeezenet1_1, vgg19_bn.\n\nCons\n\nNo support for serialization of sparse tensors. AFAIK, ONNX has no support for SparseTensors so far. Hacks possible to save indices and values, but it is better to wait for official support of sparse tensors in ONNX.\n\nProposed implementation\nimport torch\nfrom onnx import ModelProto, GraphProto, numpy_helper, load_from_string\n\ndef save_model_state(model, filename):\n    graph = GraphProto()\n    # exporting only once\n    exported_ids = set()\n    for name, tensor in model.state_dict().items():\n        if id(tensor) in exported_ids:\n            continue \n        else:\n            exported_ids.add(id(tensor))\n        try:\n            numpy_value = tensor.clone().cpu().numpy()\n        except:\n            raise RuntimeError(\"Parameter {}, tensor: {} can't be dumped. \\\n                               Sparse tensors can't be saved\".format(name, tensor))\n        # cloning to avoid moving tensor from/to GPU\n        initializer = numpy_helper.from_array(numpy_value, name=name)\n        graph.initializer.extend([initializer])\n    with open(filename, mode='wb') as f:\n        f.write(ModelProto(graph=graph).SerializeToString())\n        \ndef load_model_state(model, filename, strict=True):\n    with open(filename, mode='rb') as f:\n        graph_loaded = load_from_string(f.read()).graph\n        \n    # exporting only once\n    imported_ids = set()\n    own_state = model.state_dict()\n    for initializer in graph_loaded.initializer:\n        name = initializer.name\n        if name in own_state:\n            try:\n                tensor = own_state[name]\n                numpy_value = numpy_helper.to_array(initializer)\n                tensor.copy_(tensor.new(numpy_value)[:], broadcast=False)\n                imported_ids.add(id(tensor))\n            except Exception:\n                raise RuntimeError('While copying the parameter named {}, '\n                                   'whose dimensions in the model are {} and '\n                                   'whose dimensions in the checkpoint are {}.'\n                                   .format(name, own_state[name].size(), numpy_value.shape))\n        elif strict:\n            raise KeyError('unexpected key \"{}\" in state_dict'.format(name))\n    if strict:\n        # checking that all tensors were covered\n        for name, tensor in own_state.items():\n            if id(tensor) not in imported_ids:\n                raise KeyError('missing keys in state_dict: \"{}\"'.format(name))\nComment on implementation:\n\nwasn't able to use pytorch's ONNX classes, so used onnx package. Probably, worth fixing", "body": "Hi, I propose to add a safe way to store/load model weights (safe = can safely use weights downloaded from the internet). My vision:\r\n- a serialization mechanism that is currently officially supported uses pickle (protocol=2) to store/load weights. Usage described [here](http://pytorch.org/docs/master/notes/serialization.html)\r\n- pickle is unsafe \r\n- pytorch supports exporting to ONNX, but not importing\r\n\r\n# Proposal \r\n\r\nUse ONNX format, but use only initializers field to store a model's state.\r\n\r\n## Pros\r\n\r\n- Safe\r\n- Relies on already existing format with good tooling; needed protobuf-code is already in pytorch source. If needed, weights can be read from other languages\r\n- Proposed implementation (see later) stores parameters that are met in the model state several times, loading does not break this situation (if parameters used the same tensor, they use the same tensor after loading)\r\n- Proposed implementation was checked to normally import / export all (largest) torchvision models: inception_v3, alexnet, densenet201, resnet152, squeezenet1_1, vgg19_bn.\r\n\r\n## Cons \r\n\r\n- No support for serialization of sparse tensors. AFAIK, ONNX has no support for SparseTensors so far. Hacks possible to save indices and values, but it is better to wait for official support of sparse tensors in ONNX.\r\n\r\n## Proposed implementation\r\n\r\n```python\r\nimport torch\r\nfrom onnx import ModelProto, GraphProto, numpy_helper, load_from_string\r\n\r\ndef save_model_state(model, filename):\r\n    graph = GraphProto()\r\n    # exporting only once\r\n    exported_ids = set()\r\n    for name, tensor in model.state_dict().items():\r\n        if id(tensor) in exported_ids:\r\n            continue \r\n        else:\r\n            exported_ids.add(id(tensor))\r\n        try:\r\n            numpy_value = tensor.clone().cpu().numpy()\r\n        except:\r\n            raise RuntimeError(\"Parameter {}, tensor: {} can't be dumped. \\\r\n                               Sparse tensors can't be saved\".format(name, tensor))\r\n        # cloning to avoid moving tensor from/to GPU\r\n        initializer = numpy_helper.from_array(numpy_value, name=name)\r\n        graph.initializer.extend([initializer])\r\n    with open(filename, mode='wb') as f:\r\n        f.write(ModelProto(graph=graph).SerializeToString())\r\n        \r\ndef load_model_state(model, filename, strict=True):\r\n    with open(filename, mode='rb') as f:\r\n        graph_loaded = load_from_string(f.read()).graph\r\n        \r\n    # exporting only once\r\n    imported_ids = set()\r\n    own_state = model.state_dict()\r\n    for initializer in graph_loaded.initializer:\r\n        name = initializer.name\r\n        if name in own_state:\r\n            try:\r\n                tensor = own_state[name]\r\n                numpy_value = numpy_helper.to_array(initializer)\r\n                tensor.copy_(tensor.new(numpy_value)[:], broadcast=False)\r\n                imported_ids.add(id(tensor))\r\n            except Exception:\r\n                raise RuntimeError('While copying the parameter named {}, '\r\n                                   'whose dimensions in the model are {} and '\r\n                                   'whose dimensions in the checkpoint are {}.'\r\n                                   .format(name, own_state[name].size(), numpy_value.shape))\r\n        elif strict:\r\n            raise KeyError('unexpected key \"{}\" in state_dict'.format(name))\r\n    if strict:\r\n        # checking that all tensors were covered\r\n        for name, tensor in own_state.items():\r\n            if id(tensor) not in imported_ids:\r\n                raise KeyError('missing keys in state_dict: \"{}\"'.format(name))\r\n```\r\n\r\nComment on implementation:\r\n- wasn't able to use pytorch's ONNX classes, so used onnx package. Probably, worth fixing"}