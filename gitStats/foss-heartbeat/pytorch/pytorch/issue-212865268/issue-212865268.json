{"url": "https://api.github.com/repos/pytorch/pytorch/issues/961", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/961/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/961/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/961/events", "html_url": "https://github.com/pytorch/pytorch/pull/961", "id": 212865268, "node_id": "MDExOlB1bGxSZXF1ZXN0MTA5NzkwNjY3", "number": 961, "title": "Docs(RNN|GRU|LSTM): Note dropout applies to all layers *except* the last layer", "user": {"login": "Smerity", "id": 32325, "node_id": "MDQ6VXNlcjMyMzI1", "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Smerity", "html_url": "https://github.com/Smerity", "followers_url": "https://api.github.com/users/Smerity/followers", "following_url": "https://api.github.com/users/Smerity/following{/other_user}", "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}", "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions", "organizations_url": "https://api.github.com/users/Smerity/orgs", "repos_url": "https://api.github.com/users/Smerity/repos", "events_url": "https://api.github.com/users/Smerity/events{/privacy}", "received_events_url": "https://api.github.com/users/Smerity/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-08T21:41:29Z", "updated_at": "2017-03-08T23:09:15Z", "closed_at": "2017-03-08T23:09:12Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/961", "html_url": "https://github.com/pytorch/pytorch/pull/961", "diff_url": "https://github.com/pytorch/pytorch/pull/961.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/961.patch"}, "body_html": "<p>This is an important clarification to make as otherwise users are misled as to where they may need to add dropout and to clarify the situation would need to delve into the backend implementation.</p>\n<p>This may not even be obvious to the user if they don't have a model with an expected behaviour that they're trying to match.</p>\n<p>The specific implementation detail this refers to is below:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/4647f753bc22d3fc4b57f326608663173109aeb0/torch/nn/_functions/rnn.py#L73\">pytorch/torch/nn/_functions/rnn.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 73\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/4647f753bc22d3fc4b57f326608663173109aeb0\">4647f75</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L73\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"73\"></td>\n          <td id=\"LC73\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> dropout <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> i <span class=\"pl-k\">&lt;</span> num_layers <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>: </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>", "body_text": "This is an important clarification to make as otherwise users are misled as to where they may need to add dropout and to clarify the situation would need to delve into the backend implementation.\nThis may not even be obvious to the user if they don't have a model with an expected behaviour that they're trying to match.\nThe specific implementation detail this refers to is below:\n\n  \n    \n      pytorch/torch/nn/_functions/rnn.py\n    \n    \n         Line 73\n      in\n      4647f75\n    \n    \n    \n    \n\n        \n          \n           if dropout != 0 and i < num_layers - 1:", "body": "This is an important clarification to make as otherwise users are misled as to where they may need to add dropout and to clarify the situation would need to delve into the backend implementation.\r\n\r\nThis may not even be obvious to the user if they don't have a model with an expected behaviour that they're trying to match.\r\n\r\nThe specific implementation detail this refers to is below:\r\nhttps://github.com/pytorch/pytorch/blob/4647f753bc22d3fc4b57f326608663173109aeb0/torch/nn/_functions/rnn.py#L73"}