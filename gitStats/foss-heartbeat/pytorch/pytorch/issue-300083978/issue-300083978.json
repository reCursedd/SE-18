{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5406", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5406/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5406/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5406/events", "html_url": "https://github.com/pytorch/pytorch/issues/5406", "id": 300083978, "node_id": "MDU6SXNzdWUzMDAwODM5Nzg=", "number": 5406, "title": "BatchNorm behaves different in train() and eval()", "user": {"login": "yuandong-tian", "id": 2973937, "node_id": "MDQ6VXNlcjI5NzM5Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2973937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuandong-tian", "html_url": "https://github.com/yuandong-tian", "followers_url": "https://api.github.com/users/yuandong-tian/followers", "following_url": "https://api.github.com/users/yuandong-tian/following{/other_user}", "gists_url": "https://api.github.com/users/yuandong-tian/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuandong-tian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuandong-tian/subscriptions", "organizations_url": "https://api.github.com/users/yuandong-tian/orgs", "repos_url": "https://api.github.com/users/yuandong-tian/repos", "events_url": "https://api.github.com/users/yuandong-tian/events{/privacy}", "received_events_url": "https://api.github.com/users/yuandong-tian/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-02-26T01:38:08Z", "updated_at": "2018-05-10T19:55:08Z", "closed_at": "2018-02-26T04:03:16Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c1\">print</span>(torch.<span class=\"pl-c1\">__version__</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_data</span>(<span class=\"pl-smi\">cuda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-k\">if</span> cuda:\n        <span class=\"pl-k\">return</span> torch.cuda.FloatTensor(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">19</span>, <span class=\"pl-c1\">19</span>)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> torch.FloatTensor(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">19</span>, <span class=\"pl-c1\">19</span>)\n\nm <span class=\"pl-k\">=</span> nn.Sequential(\n    nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n    nn.BatchNorm2d(<span class=\"pl-c1\">3</span>),\n    nn.ReLU()\n)\n\ncuda <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n<span class=\"pl-k\">if</span> cuda:\n    m.cuda()\n\nm.eval()\n<span class=\"pl-c1\">print</span>(m._modules[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1<span class=\"pl-pds\">'</span></span>].training)\n\nx <span class=\"pl-k\">=</span> Variable(get_data(cuda))\ny1 <span class=\"pl-k\">=</span> m.forward(x)\n\nm.train()\n<span class=\"pl-c1\">print</span>(m._modules[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1<span class=\"pl-pds\">'</span></span>].training)\ny2 <span class=\"pl-k\">=</span> m.forward(x)\n\nerr <span class=\"pl-k\">=</span> (y1 <span class=\"pl-k\">-</span> y2).norm().data[<span class=\"pl-c1\">0</span>]\n<span class=\"pl-k\">if</span> err <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1e-5</span>:\n    <span class=\"pl-c1\">print</span>(err)</pre></div>\n<p>Sample output:</p>\n<pre><code>0.3.0.post4\nFalse\nTrue\n33.47104263305664\n</code></pre>\n<p>Also in master:</p>\n<pre><code>0.4.0a0+0ef1038\nFalse\nTrue\n6.883443355560303\n</code></pre>", "body_text": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nprint(torch.__version__)\n\ndef get_data(cuda=False):\n    if cuda:\n        return torch.cuda.FloatTensor(32, 1, 19, 19)\n    else:\n        return torch.FloatTensor(32, 1, 19, 19)\n\nm = nn.Sequential(\n    nn.Conv2d(1, 3, kernel_size=3, padding=1),\n    nn.BatchNorm2d(3),\n    nn.ReLU()\n)\n\ncuda = True\nif cuda:\n    m.cuda()\n\nm.eval()\nprint(m._modules['1'].training)\n\nx = Variable(get_data(cuda))\ny1 = m.forward(x)\n\nm.train()\nprint(m._modules['1'].training)\ny2 = m.forward(x)\n\nerr = (y1 - y2).norm().data[0]\nif err > 1e-5:\n    print(err)\nSample output:\n0.3.0.post4\nFalse\nTrue\n33.47104263305664\n\nAlso in master:\n0.4.0a0+0ef1038\nFalse\nTrue\n6.883443355560303", "body": "```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nprint(torch.__version__)\r\n\r\ndef get_data(cuda=False):\r\n    if cuda:\r\n        return torch.cuda.FloatTensor(32, 1, 19, 19)\r\n    else:\r\n        return torch.FloatTensor(32, 1, 19, 19)\r\n\r\nm = nn.Sequential(\r\n    nn.Conv2d(1, 3, kernel_size=3, padding=1),\r\n    nn.BatchNorm2d(3),\r\n    nn.ReLU()\r\n)\r\n\r\ncuda = True\r\nif cuda:\r\n    m.cuda()\r\n\r\nm.eval()\r\nprint(m._modules['1'].training)\r\n\r\nx = Variable(get_data(cuda))\r\ny1 = m.forward(x)\r\n\r\nm.train()\r\nprint(m._modules['1'].training)\r\ny2 = m.forward(x)\r\n\r\nerr = (y1 - y2).norm().data[0]\r\nif err > 1e-5:\r\n    print(err)\r\n```\r\n\r\nSample output:\r\n```\r\n0.3.0.post4\r\nFalse\r\nTrue\r\n33.47104263305664\r\n```\r\nAlso in master:\r\n```\r\n0.4.0a0+0ef1038\r\nFalse\r\nTrue\r\n6.883443355560303\r\n```"}