{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/387377448", "html_url": "https://github.com/pytorch/pytorch/issues/7250#issuecomment-387377448", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7250", "id": 387377448, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzM3NzQ0OA==", "user": {"login": "Ravenwater", "id": 466641, "node_id": "MDQ6VXNlcjQ2NjY0MQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/466641?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ravenwater", "html_url": "https://github.com/Ravenwater", "followers_url": "https://api.github.com/users/Ravenwater/followers", "following_url": "https://api.github.com/users/Ravenwater/following{/other_user}", "gists_url": "https://api.github.com/users/Ravenwater/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ravenwater/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ravenwater/subscriptions", "organizations_url": "https://api.github.com/users/Ravenwater/orgs", "repos_url": "https://api.github.com/users/Ravenwater/repos", "events_url": "https://api.github.com/users/Ravenwater/events{/privacy}", "received_events_url": "https://api.github.com/users/Ravenwater/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-08T11:52:25Z", "updated_at": "2018-05-08T11:52:25Z", "author_association": "NONE", "body_html": "<p>Just adding a tidbit of information that hopefully steers this outline in the right direction: please, do not organize the interface like OpenCL. Assume an RDMA-type device that can be an autonomous process/collaborator with the CPU that communicates through shared memory data structures and function calls. With that model, high-performance hw accelerators are free to organize the work as their internal architecture requires. For example, a data flow machine would not know what to do with an OpenCL kernel. Furthermore, with an RDMA architecture you could conceivably drive remote clusters of AI supercomputers that are optimized for specific capabilities. It would also make it possible to envision external hw accelerators that you plug into a USB3 bus on your laptop so you can carry around a 10TB data set into the field for transfer learning or other types of mash-ups.</p>\n<p>So, please, no OpenCL-type interface: data structures and function calls. For inspiration, you can take a look at the high-performance distributed computing libraries like PETSc and Trilinos, which solved (part of) this problem 20 years ago.</p>", "body_text": "Just adding a tidbit of information that hopefully steers this outline in the right direction: please, do not organize the interface like OpenCL. Assume an RDMA-type device that can be an autonomous process/collaborator with the CPU that communicates through shared memory data structures and function calls. With that model, high-performance hw accelerators are free to organize the work as their internal architecture requires. For example, a data flow machine would not know what to do with an OpenCL kernel. Furthermore, with an RDMA architecture you could conceivably drive remote clusters of AI supercomputers that are optimized for specific capabilities. It would also make it possible to envision external hw accelerators that you plug into a USB3 bus on your laptop so you can carry around a 10TB data set into the field for transfer learning or other types of mash-ups.\nSo, please, no OpenCL-type interface: data structures and function calls. For inspiration, you can take a look at the high-performance distributed computing libraries like PETSc and Trilinos, which solved (part of) this problem 20 years ago.", "body": "Just adding a tidbit of information that hopefully steers this outline in the right direction: please, do not organize the interface like OpenCL. Assume an RDMA-type device that can be an autonomous process/collaborator with the CPU that communicates through shared memory data structures and function calls. With that model, high-performance hw accelerators are free to organize the work as their internal architecture requires. For example, a data flow machine would not know what to do with an OpenCL kernel. Furthermore, with an RDMA architecture you could conceivably drive remote clusters of AI supercomputers that are optimized for specific capabilities. It would also make it possible to envision external hw accelerators that you plug into a USB3 bus on your laptop so you can carry around a 10TB data set into the field for transfer learning or other types of mash-ups.\r\n\r\nSo, please, no OpenCL-type interface: data structures and function calls. For inspiration, you can take a look at the high-performance distributed computing libraries like PETSc and Trilinos, which solved (part of) this problem 20 years ago.\r\n"}