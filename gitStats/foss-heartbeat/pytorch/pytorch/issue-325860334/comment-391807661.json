{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391807661", "html_url": "https://github.com/pytorch/pytorch/issues/7795#issuecomment-391807661", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7795", "id": 391807661, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTgwNzY2MQ==", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-24T18:06:14Z", "updated_at": "2018-05-24T18:06:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Early in the distributions design we discussed adding a <code>.cuda()</code> method. At the time we decided against it, but things may have changed since then so maybe it's worth reconsidering. Here was some of our original rationale:</p>\n<ol>\n<li>If we were to support <code>.cuda()</code> it would require significant extra complexity, since some distributions create temporary objects in <code>__init__</code> (e.g. <code>Beta</code>) and in <code>@lazy_property</code> attrs.</li>\n<li>Distributions are flyweight objects and should not persist (and do not have the internal logic to handle mutation). Therefore it is preferable to call <code>.cuda()</code> (or <code>.to()</code>) on all input args to each distribution. This is easy since all input args usually tensors anyway.</li>\n<li>If you're initializing with floats, then you can instead simply\n<div class=\"highlight highlight-source-python\"><pre>set_default_tensor_type(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.cuda.FloatTensor<span class=\"pl-pds\">'</span></span>)\nd <span class=\"pl-k\">=</span> dist.Normal(<span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">1</span>.)\n<span class=\"pl-k\">assert</span> d.loc.is_cuda</pre></div>\n</li>\n</ol>", "body_text": "Early in the distributions design we discussed adding a .cuda() method. At the time we decided against it, but things may have changed since then so maybe it's worth reconsidering. Here was some of our original rationale:\n\nIf we were to support .cuda() it would require significant extra complexity, since some distributions create temporary objects in __init__ (e.g. Beta) and in @lazy_property attrs.\nDistributions are flyweight objects and should not persist (and do not have the internal logic to handle mutation). Therefore it is preferable to call .cuda() (or .to()) on all input args to each distribution. This is easy since all input args usually tensors anyway.\nIf you're initializing with floats, then you can instead simply\nset_default_tensor_type('torch.cuda.FloatTensor')\nd = dist.Normal(0., 1.)\nassert d.loc.is_cuda", "body": "Early in the distributions design we discussed adding a `.cuda()` method. At the time we decided against it, but things may have changed since then so maybe it's worth reconsidering. Here was some of our original rationale:\r\n1. If we were to support `.cuda()` it would require significant extra complexity, since some distributions create temporary objects in `__init__` (e.g. `Beta`) and in `@lazy_property` attrs.\r\n2. Distributions are flyweight objects and should not persist (and do not have the internal logic to handle mutation). Therefore it is preferable to call `.cuda()` (or `.to()`) on all input args to each distribution. This is easy since all input args usually tensors anyway.\r\n3. If you're initializing with floats, then you can instead simply\r\n    ```py\r\n    set_default_tensor_type('torch.cuda.FloatTensor')\r\n    d = dist.Normal(0., 1.)\r\n    assert d.loc.is_cuda\r\n    ```"}