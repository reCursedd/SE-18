{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/391849369", "html_url": "https://github.com/pytorch/pytorch/issues/7795#issuecomment-391849369", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7795", "id": 391849369, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTg0OTM2OQ==", "user": {"login": "Balandat", "id": 1605878, "node_id": "MDQ6VXNlcjE2MDU4Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1605878?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Balandat", "html_url": "https://github.com/Balandat", "followers_url": "https://api.github.com/users/Balandat/followers", "following_url": "https://api.github.com/users/Balandat/following{/other_user}", "gists_url": "https://api.github.com/users/Balandat/gists{/gist_id}", "starred_url": "https://api.github.com/users/Balandat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Balandat/subscriptions", "organizations_url": "https://api.github.com/users/Balandat/orgs", "repos_url": "https://api.github.com/users/Balandat/repos", "events_url": "https://api.github.com/users/Balandat/events{/privacy}", "received_events_url": "https://api.github.com/users/Balandat/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-24T20:30:31Z", "updated_at": "2018-05-24T20:30:31Z", "author_association": "NONE", "body_html": "<p>Here's an alternative idea: Make distributions modules that register their parameters as buffers.</p>\n<p>See the example below for how I'm currently wrapping distributions to define <code>Prior</code> objects over a multi-dimensional parameter (the one below assumes independence, but you can easily do this also using a MVN). Basically, if you attach this as a child to a module, then calling <code>.cuda()</code> on the top-level module will appropriately cudaify the <code>Prior</code> object by re-instantiating the attached distributions with the cudaified tensors. It seems like something like this could be a blueprint for Distributions as well.</p>\n<pre><code>from torch.distributions.normal import Normal\nfrom torch import nn\n\n\nclass Prior(nn.Module):\n\n    def _apply(self, fn):\n        Module._apply(self, fn)\n        self._initialize_distributions()\n\n\nclass NormalPrior(Prior):\n\n    def __init__(self, loc, scale):\n        super(NormalPrior, self).__init__()\n        self.register_buffer(\"loc\", loc.view(-1).clone())\n        self.register_buffer(\"scale\", scale.view(-1).clone())\n        self._initialize_distributions()\n\n    def _initialize_distributions(self):\n        self._distributions = [\n            Normal(loc=l, scale=s, validate_args=True)\n            for l, s in zip(self.loc, self.scale)\n        ]\n\n</code></pre>\n<p><em>Aside:</em> You could also think of distributions that register loc and scale parameters as torch parameters rather than buffers, that would give you an immediate way to optimize parameters using hyper-priors....</p>", "body_text": "Here's an alternative idea: Make distributions modules that register their parameters as buffers.\nSee the example below for how I'm currently wrapping distributions to define Prior objects over a multi-dimensional parameter (the one below assumes independence, but you can easily do this also using a MVN). Basically, if you attach this as a child to a module, then calling .cuda() on the top-level module will appropriately cudaify the Prior object by re-instantiating the attached distributions with the cudaified tensors. It seems like something like this could be a blueprint for Distributions as well.\nfrom torch.distributions.normal import Normal\nfrom torch import nn\n\n\nclass Prior(nn.Module):\n\n    def _apply(self, fn):\n        Module._apply(self, fn)\n        self._initialize_distributions()\n\n\nclass NormalPrior(Prior):\n\n    def __init__(self, loc, scale):\n        super(NormalPrior, self).__init__()\n        self.register_buffer(\"loc\", loc.view(-1).clone())\n        self.register_buffer(\"scale\", scale.view(-1).clone())\n        self._initialize_distributions()\n\n    def _initialize_distributions(self):\n        self._distributions = [\n            Normal(loc=l, scale=s, validate_args=True)\n            for l, s in zip(self.loc, self.scale)\n        ]\n\n\nAside: You could also think of distributions that register loc and scale parameters as torch parameters rather than buffers, that would give you an immediate way to optimize parameters using hyper-priors....", "body": "Here's an alternative idea: Make distributions modules that register their parameters as buffers. \r\n\r\nSee the example below for how I'm currently wrapping distributions to define `Prior` objects over a multi-dimensional parameter (the one below assumes independence, but you can easily do this also using a MVN). Basically, if you attach this as a child to a module, then calling `.cuda()` on the top-level module will appropriately cudaify the `Prior` object by re-instantiating the attached distributions with the cudaified tensors. It seems like something like this could be a blueprint for Distributions as well.\r\n\r\n\r\n```\r\nfrom torch.distributions.normal import Normal\r\nfrom torch import nn\r\n\r\n\r\nclass Prior(nn.Module):\r\n\r\n    def _apply(self, fn):\r\n        Module._apply(self, fn)\r\n        self._initialize_distributions()\r\n\r\n\r\nclass NormalPrior(Prior):\r\n\r\n    def __init__(self, loc, scale):\r\n        super(NormalPrior, self).__init__()\r\n        self.register_buffer(\"loc\", loc.view(-1).clone())\r\n        self.register_buffer(\"scale\", scale.view(-1).clone())\r\n        self._initialize_distributions()\r\n\r\n    def _initialize_distributions(self):\r\n        self._distributions = [\r\n            Normal(loc=l, scale=s, validate_args=True)\r\n            for l, s in zip(self.loc, self.scale)\r\n        ]\r\n\r\n```\r\n\r\n\r\n*Aside:* You could also think of distributions that register loc and scale parameters as torch parameters rather than buffers, that would give you an immediate way to optimize parameters using hyper-priors...."}